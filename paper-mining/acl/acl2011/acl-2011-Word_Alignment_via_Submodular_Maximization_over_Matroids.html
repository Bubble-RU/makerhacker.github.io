<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>340 acl-2011-Word Alignment via Submodular Maximization over Matroids</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-340" href="#">acl2011-340</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>340 acl-2011-Word Alignment via Submodular Maximization over Matroids</h1>
<br/><p>Source: <a title="acl-2011-340-pdf" href="http://aclweb.org/anthology//P/P11/P11-2030.pdf">pdf</a></p><p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We cast the word alignment problem as maximizing a submodular function under matroid constraints. Our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. We show that submodularity naturally arises when modeling word fertility. Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.</p><p>Reference: <a title="acl-2011-340-reference" href="../acl2011_reference/acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hl in@ ee washingt on edu Abstract We cast the word alignment problem as maximizing a submodular function under matroid constraints. [sent-4, score-1.426]
</p><p>2 Our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. [sent-5, score-0.737]
</p><p>3 We show that submodularity naturally arises when modeling word fertility. [sent-6, score-0.204]
</p><p>4 Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches. [sent-7, score-0.525]
</p><p>5 1 Introduction  Word alignment is a key component in most statistical machine translation systems. [sent-8, score-0.218]
</p><p>6 While classical approaches for word alignment are based on generative models (e. [sent-9, score-0.218]
</p><p>7 , 1996)), word alignment can also be viewed as a matching problem, where each word pair is associated with a score reflecting the desirability of aligning that pair, and the alignment is then the highest scored matching under some constraints. [sent-13, score-0.703]
</p><p>8 Melamed (2000) introduces the competitive linking algorithm which greedily constructs matchings under the one-to-one mapping assumption. [sent-15, score-0.129]
</p><p>9 , 2004), matchings are found using an algorithm for constructing a maximum weighted bipartite graph matching (Schrijver, 2003), where word pair scores come from alignment posteriors of generative models. [sent-17, score-0.374]
</p><p>10 (2005) cast word alignment as a maximum weighted matching problem and propose a 170 Jeff Bilmes Dept. [sent-19, score-0.335]
</p><p>11 of Electrical Engineering University of Washington  Seattle, WA 98195, USA bilme s @ ee . [sent-20, score-0.059]
</p><p>12 edu framework for learning word pair scores as a function of arbitrary features of that pair. [sent-22, score-0.058]
</p><p>13 These approaches, however, have two potentially substantial limitations: words have fertility of at most one, and interactions between alignment decisions are not representable. [sent-23, score-0.497]
</p><p>14 (2006) address this issue by formulating the alignment problem as a quadratic assignment problem, and off-the-shelf integer linear programming (ILP) solvers are used to solve to optimization problem. [sent-25, score-0.273]
</p><p>15 While efficient for some median scale problems, ILP-based approaches are limited since when modeling more sophisticated interactions, the number of variables (and/or constraints) required grows polynomially, or even exponentially, making the resultant optimization impractical to solve. [sent-26, score-0.06]
</p><p>16 In this paper, we treat the word alignment problem as maximizing a submodular function subject to matroid constraints (to be defined in Section 2). [sent-27, score-1.423]
</p><p>17 Submodular objective functions can represent complex interactions among alignment decisions, and essentially extend the modular (linear) objectives  used in the aforementioned approaches. [sent-28, score-0.309]
</p><p>18 This is because maximizing a monotone submodular function under a matroid constraint can be solved efficiently using a simple greedy algorithm. [sent-30, score-1.456]
</p><p>19 The greedy algorithm, moreover, is a constant factor approximation algorithm that guarantees a near-optimal solution. [sent-31, score-0.298]
</p><p>20 In this paper, we moreover show that submodularity naturally arises in word alignment problems when modeling word fertility (see Section 4). [sent-32, score-0.647]
</p><p>21 Experiment results on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to the maximum weighted matching approach, while being at least 50 times Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o. [sent-33, score-0.525]
</p><p>22 2  Background  Matroids and submodularity both play important roles in combinatorial optimization. [sent-36, score-0.174]
</p><p>23 Matroids are combinatorial structures that generalize the notion of linear independence in matrices. [sent-38, score-0.085]
</p><p>24 A pair (V, I) is called a matroid if V is a finite ground set a(nVd, II)is a nonempty collection of subsets of V that are independent. [sent-39, score-0.568]
</p><p>25 In particular, I must satisfy (i) if X ⊂ Y and Y ∈ I X ∈ I, (ii) if X, Y ∈ I then andX |X| < |Y | tYhen ∈ X I∪ {e} ∈ I∈ f Ior some e ∈ Y \X. [sent-40, score-0.073]
</p><p>26 We typically r |efer toX a ∪m{aet}ro ∈id I by listing iets ∈ ground set and its family of independent sets: M = (V, I). [sent-41, score-0.096]
</p><p>27 lar (Edmonds, 1970) if it →sati Rsfies the property of diminishing returns: for any X Y V \ v, a submodular function f must satisfy f(X + v) −f(X) ≥ f(Y + v) − f(Y ). [sent-43, score-0.707]
</p><p>28 That is, the fin(cXre+mev)n−talf “(vXa)lue ≥” fof( v d +ec vre)a −ses f as t )he context in which v is considered grows from X to Y. [sent-44, score-0.033]
</p><p>29 If this is satisfied everywhere with equality, then the function f is called modular. [sent-45, score-0.085]
</p><p>30 A set function f is monotone nondecreasing if ∀X Y, f(X) ≤ f(Y ). [sent-46, score-0.283]
</p><p>31 As shorthand, in this paper, ⊆mo Ynotofn(eX nondecreasing submodular functions will simply be referred to as monotone submodular. [sent-47, score-0.727]
</p><p>32 Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. [sent-48, score-0.557]
</p><p>33 More recently, submodular functions have started receiving attention in the  ⊆ ⊆  ⊆  ×  machine learning and computer vision community (Kempe et al. [sent-49, score-0.502]
</p><p>34 3 Approach We are given a source language (English) string e1I = e1, · · · , ei, · · · , eI and a target language (French) string ,fe1J = f1, · · · , fj , · · · , fJ that have to be aligned. [sent-52, score-0.134]
</p><p>35 Define t,h·e· ·w ,ofrd, positions in the English 171 string as set E {1, · · · , I} and positions in the French string as ,set {F1 {1, · · · , J}. [sent-53, score-0.162]
</p><p>36 An alignment A between the two wFor ,d strings can then be seen as a subset of the Cartesian product of the word positions, i. [sent-54, score-0.218]
</p><p>37 , A ⊆ {(i, j) : i ∈ E, j ∈ F} V, and V = E FA Ai s⊆ ⊆th {e( ground s ∈et. [sent-56, score-0.066]
</p><p>38 EF,ojr convenience, we rVefe =r tEo e× le Fment (i, j) ∈ A as an edge that connects i and j in alignment jA). [sent-57, score-0.264]
</p><p>39 ,  ,··  ,  ×  Restricting the fertility of word fj to be at most kj is mathematically equivalent to having |A ∩ PjE| ≤ kj, where A ⊆ V is an alignment and PjE = E×{j}. [sent-58, score-0.672]
</p><p>40 Intuitively, PjE is the set of all possible edges in the ground set that connect to j, and the cardinality of the intersection between A and PjE indicates how many edges in A are connected to j. [sent-59, score-0.066]
</p><p>41 Similarly, we can impose constraints on the fertility of English words by constraining the alignment A to satisfy |A ∩ PiF| ≤ ki for i ∈ E where PiF = {i} F. [sent-60, score-0.662]
</p><p>42 |NAo t∩e Pthat| |e ≤ithe kr of {PjE : j ∈ F} or {P=iF : i } ∈ E} constitute a partition Pof V:. [sent-61, score-0.04]
</p><p>43 Suppose we have a set function f : 2V → R+ that measures quality (or scores) of an alignment AR ⊆ V, then when also considering fertility constraints, we  can treat the word alignment problem as maximizing a set function subject to matroid constraint: Problem 1. [sent-63, score-1.415]
</p><p>44 maxA⊆V f(A) , subject to: A ∈ I, where Ithe set of independent sets of a matroid (or is it might be the set of independent sets simultaneously in two matroids, as we shall see later). [sent-64, score-0.575]
</p><p>45 Independence in partition matroids generalizes the typical matching constraints for word alignment, where each word aligns to at most one word (kj = 1, ∀j) in the other sentence (Matusov et al. [sent-65, score-0.31]
</p><p>46 Our matroid generalizations provide flexibility in modeling fertility, and also strategies for solving the word alignment problem efficiently and near-optimally. [sent-68, score-0.748]
</p><p>47 In particular, when f is monotone submodular, near-optimal solutions for Problem 1can be efficiently guaranteed. [sent-69, score-0.196]
</p><p>48 , 1978), a simple greedy algorithm for monotone submodular function maximization with a matroid constraint is shown to have a constant approximation factor. [sent-71, score-1.504]
</p><p>49 Precisely, the greedy algorithm finds a solution A such that f(A) ≥ m+11f(A∗) where A∗ is the optimal solution afn(dA m ≥is number of matroid constraints. [sent-72, score-0.706]
</p><p>50 When there is only one matroid constraint, we get an approxima-  21  tion factor . [sent-73, score-0.504]
</p><p>51 Constant factor approximation algorithms are particularly attractive since the quality of the solution does not depend on the size of the problem, so even very large size problems do well. [sent-74, score-0.123]
</p><p>52 It is also important to note that this is a worst case bound, and in most cases the quality of the solution obtained will be much better than this bound suggests. [sent-75, score-0.031]
</p><p>53 Vondra´k (2008) shows a continuous greedy algorithm followed by pipage rounding with approximation factor 1 − 1/e (≈ 0. [sent-76, score-0.264]
</p><p>54 63) for maximizing a monotone sub1m −od 1u/lear f≈un 0ct. [sent-77, score-0.261]
</p><p>55 (2009) improve the approximation result in (Fisher et al. [sent-80, score-0.06]
</p><p>56 , 1978) by showing a local-search algorithm has approximation guarantee of m+1? [sent-81, score-0.135]
</p><p>57 for the problem of maximizing a monotone submodular function subject to m matroid constraints (m ≥ 2 and ? [sent-82, score-1.371]
</p><p>58 In this paper, however, we use thme simple greedy algorithm for the sake of efficiency. [sent-84, score-0.172]
</p><p>59 We outline our greedy algorithm for Problem 1in Algorithm 1, which is slightly different from the one in (Fisher et al. [sent-85, score-0.172]
</p><p>60 , 1978) as in line 4 of Algorithm 1, we have an additional requirement on a such that the increment of adding a is strictly greater than zero. [sent-86, score-0.03]
</p><p>61 This additional requirement is to maintain a higher precision word alignment solution. [sent-87, score-0.248]
</p><p>62 The  m+11-  theoretical guarantee still holds as f is monotone i. [sent-88, score-0.194]
</p><p>63 , Algorithm 1is a 21-approximation algorithm for Problem 1 (only one matroid constraint) when f is monotone submodular. [sent-90, score-0.685]
</p><p>64 —  Algorithm 1:A greedy algorithm for Problem 1. [sent-91, score-0.172]
</p><p>65 In practice, the argmax in Algorithm 1can be efficient implemented with priority queue when f is submodular (Minoux, 1978), which brings the complexity down to O( |V | log |V |) oracle function calls. [sent-96, score-0.523]
</p><p>66 4  Submodular Fertility  We begin this section by demonstrating that submodularity arises naturally when modeling word fertility. [sent-97, score-0.204]
</p><p>67 To do so, we borrow an example of fertility from (Melamed, 2000). [sent-98, score-0.225]
</p><p>68 01, where s(ei, fj) represents the score of aligning ei and fj. [sent-102, score-0.197]
</p><p>69 To find the correct alignment (e1, f1) and (e2, f2), the competitive linking algorithm in (Melamed, 2000) poses a one-to-one assumption to prevent choosing (e1, f2) over (e2, f2) . [sent-103, score-0.296]
</p><p>70 The one-to-one assumption, however, limits the algorithm’s capability of handling models with fertility larger than one. [sent-104, score-0.225]
</p><p>71 The scores estimated from the data for aligning word pairs (the, le), (the, de) and (of, de) are 0. [sent-107, score-0.121]
</p><p>72 44, regardless the fact that the is already aligned with le. [sent-115, score-0.028]
</p><p>73 Now if we use a submodular function to model the score of aligning an English word to a set of French words, we might obtain the correct alignments (the, le) and (of, de) by incorporating the diminishing returns property (i. [sent-116, score-0.823]
</p><p>74 Formally, for each iin E, we define a mapping δi : 2V  →  2F with δi(A) = {j ∈ F|(i, j) ∈ A},  (1)  i. [sent-121, score-0.045]
</p><p>75 , δi(A) is the set of positions in F that are aligned with position iin alignment A. [sent-123, score-0.342]
</p><p>76 We use function fi : 2F → R+ to represent the benefit of aligning position i∈ →E R to a set of positions  in F. [sent-124, score-0.23]
</p><p>77 Given score have, for S ⊆ F,  si,j  of aligning iand j,we could  fi(S) =jX∈Ssi,jα,  (2)  where 0 < α ≤ 1, i. [sent-125, score-0.121]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('matroid', 0.472), ('submodular', 0.465), ('pje', 0.236), ('fertility', 0.225), ('alignment', 0.218), ('matroids', 0.168), ('monotone', 0.166), ('bilmes', 0.152), ('pif', 0.135), ('kj', 0.128), ('greedy', 0.125), ('aligning', 0.121), ('submodularity', 0.119), ('maximizing', 0.095), ('narasimhan', 0.089), ('hansards', 0.082), ('diminishing', 0.082), ('ei', 0.076), ('fj', 0.074), ('satisfy', 0.073), ('schrijver', 0.067), ('ki', 0.067), ('melamed', 0.067), ('ground', 0.066), ('approximation', 0.06), ('krause', 0.059), ('nondecreasing', 0.059), ('ithe', 0.059), ('fisher', 0.058), ('matching', 0.058), ('function', 0.058), ('combinatorial', 0.055), ('matusov', 0.055), ('interactions', 0.054), ('electrical', 0.051), ('matchings', 0.051), ('positions', 0.051), ('arises', 0.051), ('algorithm', 0.047), ('le', 0.046), ('constraint', 0.045), ('iin', 0.045), ('constraints', 0.044), ('subject', 0.043), ('de', 0.041), ('partition', 0.04), ('functions', 0.037), ('returns', 0.037), ('impose', 0.035), ('french', 0.035), ('constant', 0.034), ('naturally', 0.034), ('wa', 0.034), ('grows', 0.033), ('factor', 0.032), ('maximization', 0.032), ('seattle', 0.032), ('linking', 0.031), ('cast', 0.031), ('rates', 0.031), ('alignments', 0.031), ('solution', 0.031), ('independent', 0.03), ('washington', 0.03), ('requirement', 0.03), ('independence', 0.03), ('bilme', 0.03), ('jegelka', 0.03), ('minoux', 0.03), ('washingt', 0.03), ('zabin', 0.03), ('desirability', 0.03), ('ownership', 0.03), ('pthat', 0.03), ('ior', 0.03), ('vre', 0.03), ('iith', 0.03), ('cdc', 0.03), ('lue', 0.03), ('anr', 0.03), ('nonempty', 0.03), ('maxa', 0.03), ('comme', 0.03), ('wfor', 0.03), ('polynomially', 0.03), ('nao', 0.03), ('nvd', 0.03), ('string', 0.03), ('efficiently', 0.03), ('ee', 0.029), ('property', 0.029), ('guarantee', 0.028), ('aligned', 0.028), ('problem', 0.028), ('optimization', 0.027), ('concave', 0.027), ('everywhere', 0.027), ('kolmogorov', 0.027), ('mathematically', 0.027), ('diminishes', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="340-tfidf-1" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We cast the word alignment problem as maximizing a submodular function under matroid constraints. Our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. We show that submodularity naturally arises when modeling word fertility. Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.</p><p>2 0.49651638 <a title="340-tfidf-2" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</p><p>3 0.1403815 <a title="340-tfidf-3" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>4 0.13135934 <a title="340-tfidf-4" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>5 0.10773331 <a title="340-tfidf-5" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>6 0.097744524 <a title="340-tfidf-6" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>7 0.093742907 <a title="340-tfidf-7" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>8 0.091146447 <a title="340-tfidf-8" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>9 0.088930525 <a title="340-tfidf-9" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>10 0.08042907 <a title="340-tfidf-10" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>11 0.075614028 <a title="340-tfidf-11" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>12 0.069413744 <a title="340-tfidf-12" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>13 0.06920746 <a title="340-tfidf-13" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>14 0.062737726 <a title="340-tfidf-14" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>15 0.058698226 <a title="340-tfidf-15" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>16 0.055400066 <a title="340-tfidf-16" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>17 0.052984271 <a title="340-tfidf-17" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>18 0.047403913 <a title="340-tfidf-18" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>19 0.046323813 <a title="340-tfidf-19" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>20 0.046234496 <a title="340-tfidf-20" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, -0.055), (2, 0.025), (3, 0.078), (4, 0.001), (5, -0.019), (6, -0.01), (7, 0.078), (8, -0.013), (9, 0.058), (10, 0.122), (11, 0.123), (12, -0.032), (13, 0.119), (14, -0.224), (15, 0.032), (16, 0.093), (17, -0.03), (18, -0.101), (19, 0.022), (20, -0.053), (21, -0.072), (22, -0.018), (23, 0.036), (24, -0.037), (25, -0.054), (26, 0.049), (27, -0.095), (28, 0.086), (29, -0.212), (30, 0.043), (31, -0.046), (32, 0.115), (33, -0.066), (34, 0.077), (35, 0.254), (36, -0.124), (37, 0.009), (38, 0.031), (39, -0.039), (40, -0.068), (41, -0.097), (42, 0.062), (43, -0.025), (44, 0.155), (45, 0.308), (46, -0.055), (47, 0.082), (48, -0.106), (49, -0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91972202 <a title="340-lsi-1" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We cast the word alignment problem as maximizing a submodular function under matroid constraints. Our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. We show that submodularity naturally arises when modeling word fertility. Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.</p><p>2 0.77385831 <a title="340-lsi-2" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</p><p>3 0.41373894 <a title="340-lsi-3" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>4 0.40984511 <a title="340-lsi-4" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>Author: Kapil Thadani ; Kathleen McKeown</p><p>Abstract: The task of aligning corresponding phrases across two related sentences is an important component of approaches for natural language problems such as textual inference, paraphrase detection and text-to-text generation. In this work, we examine a state-of-the-art structured prediction model for the alignment task which uses a phrase-based representation and is forced to decode alignments using an approximate search approach. We propose instead a straightforward exact decoding technique based on integer linear programming that yields order-of-magnitude improvements in decoding speed. This ILP-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which significantly increase the precision of the model.</p><p>5 0.37884954 <a title="340-lsi-5" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>6 0.37631604 <a title="340-lsi-6" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>7 0.37300503 <a title="340-lsi-7" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>8 0.32530585 <a title="340-lsi-8" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>9 0.32412744 <a title="340-lsi-9" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>10 0.32202947 <a title="340-lsi-10" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>11 0.31310034 <a title="340-lsi-11" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>12 0.30616817 <a title="340-lsi-12" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>13 0.28756991 <a title="340-lsi-13" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>14 0.28306541 <a title="340-lsi-14" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>15 0.27725929 <a title="340-lsi-15" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>16 0.27285585 <a title="340-lsi-16" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>17 0.26310202 <a title="340-lsi-17" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>18 0.25989023 <a title="340-lsi-18" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>19 0.24018277 <a title="340-lsi-19" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>20 0.22990213 <a title="340-lsi-20" href="./acl-2011-Tier-based_Strictly_Local_Constraints_for_Phonology.html">303 acl-2011-Tier-based Strictly Local Constraints for Phonology</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.023), (17, 0.041), (25, 0.369), (26, 0.016), (37, 0.066), (39, 0.024), (41, 0.064), (50, 0.019), (55, 0.029), (59, 0.041), (62, 0.012), (72, 0.038), (91, 0.031), (96, 0.14)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75956345 <a title="340-lda-1" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We cast the word alignment problem as maximizing a submodular function under matroid constraints. Our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. We show that submodularity naturally arises when modeling word fertility. Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.</p><p>2 0.63480616 <a title="340-lda-2" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>Author: Tara McIntosh ; Lars Yencken ; James R. Curran ; Timothy Baldwin</p><p>Abstract: State-of-the-art bootstrapping systems rely on expert-crafted semantic constraints such as negative categories to reduce semantic drift. Unfortunately, their use introduces a substantial amount of supervised knowledge. We present the Relation Guided Bootstrapping (RGB) algorithm, which simultaneously extracts lexicons and open relationships to guide lexicon growth and reduce semantic drift. This removes the necessity for manually crafting category and relationship constraints, and manually generating negative categories.</p><p>3 0.5349232 <a title="340-lda-3" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>Author: Anders Sgaard</p><p>Abstract: We consider a very simple, yet effective, approach to cross language adaptation of dependency parsers. We first remove lexical items from the treebanks and map part-of-speech tags into a common tagset. We then train a language model on tag sequences in otherwise unlabeled target data and rank labeled source data by perplexity per word of tag sequences from less similar to most similar to the target. We then train our target language parser on the most similar data points in the source labeled data. The strategy achieves much better results than a non-adapted baseline and stateof-the-art unsupervised dependency parsing, and results are comparable to more complex projection-based cross language adaptation algorithms.</p><p>4 0.51115656 <a title="340-lda-4" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</p><p>5 0.45012945 <a title="340-lda-5" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>Author: Raphael Hoffmann ; Congle Zhang ; Xiao Ling ; Luke Zettlemoyer ; Daniel S. Weld</p><p>Abstract: Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded ( Jobs Apple ) and CEO-o f ( Jobs Apple ) . , , This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extrac- , tion model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</p><p>6 0.44913679 <a title="340-lda-6" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>7 0.4488138 <a title="340-lda-7" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>8 0.44702524 <a title="340-lda-8" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>9 0.44693586 <a title="340-lda-9" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>10 0.44684619 <a title="340-lda-10" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>11 0.44566602 <a title="340-lda-11" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>12 0.44534653 <a title="340-lda-12" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>13 0.44478515 <a title="340-lda-13" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>14 0.4443976 <a title="340-lda-14" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>15 0.44425598 <a title="340-lda-15" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>16 0.44380721 <a title="340-lda-16" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>17 0.44308376 <a title="340-lda-17" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>18 0.44301131 <a title="340-lda-18" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>19 0.44271654 <a title="340-lda-19" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>20 0.44271171 <a title="340-lda-20" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
