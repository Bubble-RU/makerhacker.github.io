<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-29" href="#">acl2011-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</h1>
<br/><p>Source: <a title="acl-2011-29-pdf" href="http://aclweb.org/anthology//P/P11/P11-1001.pdf">pdf</a></p><p>Author: Andreas Zollmann ; Stephan Vogel</p><p>Abstract: In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.</p><p>Reference: <a title="acl-2011-29-reference" href="../acl2011_reference/acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  ,  Abstract In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. [sent-3, score-0.31]
</p><p>2 The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. [sent-4, score-0.418]
</p><p>3 Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. [sent-5, score-0.319]
</p><p>4 As in monolingual parsing, nonterminal symbols in translation rules are used to generalize beyond purely lexical operations. [sent-8, score-0.538]
</p><p>5 Labels on these nonterminal symbols are often used to enforce syntactic constraints in the generation of bilingual sentences and imply conditional independence assumptions in the translation model. [sent-9, score-0.463]
</p><p>6 1 While all of these techniques rely on wordalignments to suggest lexical relationships, they differ in the way in which they assign labels to nonterminal symbols of PSCFG rules. [sent-14, score-0.342]
</p><p>7 (2006), target language parse trees are used to identify rules and label their nonterminal symbols, while Liu et al. [sent-19, score-0.471]
</p><p>8 Zollmann and Venugopal (2006) directly extend the rule extraction procedure from Chiang (2005) to heuristically label any phrase pair based on target language parse trees. [sent-21, score-0.53]
</p><p>9 In this work, we propose a labeling approach that is based merely on part-of-speech analysis of the source or target language (or even both). [sent-25, score-0.32]
</p><p>10 Towards the ultimate goal of building end-to-end machine translation systems without any human annotations, we also experiment with automatically inferred word classes using distributional clustering (Kneser and Ney, 1993). [sent-26, score-0.427]
</p><p>11 Since the number ofclasses is a parameter of the clustering method and the resulting nonterminal size of our grammar is a func-  tion of the number of word classes, the PSCFG grammar complexity can be adjusted to the specific translation task at hand. [sent-27, score-0.889]
</p><p>12 ec A20s 1o1ci Aastiso nci faotrio Cno fomrp Cuotamtipounta lti Loin aglu Lisitnigcsu,is patigces 1–1 , the incorporation of an arbitrary number of wordclass based features, including phrasal contexts, can make use of multiple tagging schemes, and also allows non-class features such as phrase sizes. [sent-30, score-0.245]
</p><p>13 Chiang (2005) learns a single-nonterminal PSCFG from a bilingual corpus by first identifying initial phrase pairs using the technique from Koehn et al. [sent-33, score-0.382]
</p><p>14 (2003), and then performing a generalization operation to generate phrase pairs with gaps, which can be viewed as PSCFG rules with generic ‘X’ nonterminal left-hand-sides and substitution sites. [sent-34, score-0.674]
</p><p>15 Bilingual features φi that judge the quality of each rule are estimated based on rule extraction frequency counts. [sent-35, score-0.3]
</p><p>16 3  Hard rule labeling from word classes  We now describe a simple method of inducing a multi-nonterminal PSCFG from a parallel corpus with word-tagged target side sentences. [sent-36, score-0.532]
</p><p>17 (2003) to provide us with a set of phrase pairs for each sentence pair in the training corpus, annotated with their respective start and end positions in the source and target sentences. [sent-40, score-0.455]
</p><p>18 Let f = f1 · · · fm be the current source sentence, e = e1 · · · en t·hfe current target sentence, and t = 2 t1 · · · tn its corresponding target tag sequence. [sent-41, score-0.461]
</p><p>19 The creation of complex rules based on all initial rules obtained from the current sentence now proceeds just as in Chiang’s model. [sent-43, score-0.378]
</p><p>20 Tn to denote that tags were o·m··iTtted from the phrase’s tag sequence. [sent-50, score-0.188]
</p><p>21 The resulting number of grammar nonterminals based on a tag vocabulary of size t is thus given by 2t2 + t. [sent-51, score-0.391]
</p><p>22 An alternative way of accounting for phrase size  is presented by Chiang et al. [sent-52, score-0.379]
</p><p>23 (2008), who introduce structural distortion features into a hierarchical phrase-based model, aimed at modeling nonterminal reordering given source span length. [sent-53, score-0.475]
</p><p>24 Extension to a bilingually tagged corpus While the availability of syntactic annotations for both source and target language is unlikely in most translation scenarios, some form of word tags, be it partof-speech tags or learned word clusters (cf. [sent-55, score-0.555]
</p><p>25 In this case, our grammar extraction procedure can be easily extended to impose both source and target constraints on the eligible substitutions simultaneously. [sent-57, score-0.372]
</p><p>26 Let Nf be the nonterminal label that would be assigned to a given initial rule when utilizing the source-side tag sequence, and Ne the assigned la-  bel according to the target-side tag sequence. [sent-58, score-0.631]
</p><p>27 The number of nonterminals in this model, based on a source tag vocabulary of size s and a target tag vocabulary of size t, is thus given by s2t2 for the regular labeling method and (2s2 + s) (2t2 + t) when accounting for phrase size. [sent-61, score-0.946]
</p><p>28 VDB →N+ gVeBseDh-ePnRP | →w habe ihn gesehen | saw him g5:e PheRnP. [sent-64, score-0.388]
</p><p>29 Abstracting-out rule 2 from rule 4, for instance, leads to the complex rule: AUX. [sent-70, score-0.3]
</p><p>30 Vn B| Ns+awV PBRDP-+PPRRPP →1 Unsupervised word class assignment by clustering As an alternative to POS tags, we experiment with unsupervised word clustering methods based on the exchange algorithm (Kneser and Ney, 1993). [sent-74, score-0.34]
</p><p>31 As training data we use the respective side of the  parallel training data for the translation system. [sent-92, score-0.205]
</p><p>32 4 Clustering phrase pairs directly using the K-means algorithm Even though we have only made use of the first and last words’ classes in the labeling methods described so far, the number of resulting grammar nonterminals quickly explodes. [sent-95, score-0.717]
</p><p>33 Using a scheme based on source and target phrases with accounting for phrase size, with 36 word classes (the size of the Penn English POS tag set) for both languages, yields a grammar with (36 + 2 ∗ 362)2 = 6. [sent-96, score-0.911]
</p><p>34 Quite plausibly, phrase labeling should be informed by more than just the classes of the first and last words of the phrase. [sent-98, score-0.412]
</p><p>35 Taking phrase context into account, for example, can aid the learning of syntactic properties: a phrase beginning with a deter-  miner and ending with a noun, with a verb as right context, is more likely to be a noun phrase than the same phrase with another noun as right context. [sent-99, score-0.84]
</p><p>36 Similarly, it is conceivable that using non-boundary words inside the phrase might aid the labeling process. [sent-101, score-0.351]
</p><p>37 A smaller number of word clusters will result in smaller number of grammar nonterminals, and thus more reliable feature estimation, while a larger number has the potential to discover more subtle syntactic properties. [sent-103, score-0.305]
</p><p>38 Using multiple word clusterings simultaneously, each based on a different number of classes, could turn this global, hard trade-off into a local, soft one, informed by the number of phrase pair instances available for a given granularity. [sent-104, score-0.285]
</p><p>39 Lastly, our method of accounting for phrase size is somewhat displeasing: While there is a hard partitioning of one-word and two-word phrases, no distinction is made between phrases of length greater than two. [sent-105, score-0.444]
</p><p>40 Marking phrase sizes greater than two explicitly by length, however, would create many  sparse, low-frequency rules, and one of the strengths of PSCFG-based translation is the ability to substitute flexible-length spans into nonterminals of a derivation. [sent-106, score-0.417]
</p><p>41 A partitioning where phrase size is instead merely a feature informing the labeling process seems more desirable. [sent-107, score-0.424]
</p><p>42 We thus propose to represent each phrase pair instance (including its bilingual one-word contexts) as feature vectors, i. [sent-108, score-0.306]
</p><p>43 We 4 then use these data points to partition the space into clusters, and subsequently assign each phrase pair instance the cluster of its corresponding feature vector as label. [sent-111, score-0.343]
</p><p>44 The feature mapping stance (f0)f1 · · · fm(fm+1)  Consider the phrase pair in-  | (e0)e1 · · · en(en+1)  (where f0, fm+1 , e0, en+1 are the left and right, source and target side contexts, respectively). [sent-112, score-0.457]
</p><p>45 We now map our phrase pair instance to the real-valued vector (where 1[P] is the indicator function defined as 1if property P is true, and 0 otherwise):  D1[e1=c0],. [sent-122, score-0.251]
</p><p>46 The elements in the first line represent the phrase boundary word classes, the next two lines the classes of the second and penultimate word, followed by a line representing the accumulated contents ofthe whole phrase, followed by  two lines pertaining to the context word classes. [sent-144, score-0.377]
</p><p>47 The final element of the vector is proportional to the logarithm ofthe phrase length. [sent-145, score-0.21]
</p><p>48 Thus, all other features being equal, the distance between a two-word and a four-word phrase is √  1The √N + 1 factor serves to make the feature’s influence independent of the number of word classes by yielding the same distance (under L2) as N + 1identical copies of the feature. [sent-147, score-0.346]
</p><p>49 We will mainly use the Euclidean (L2) distance to compare points for clustering purposes. [sent-149, score-0.17]
</p><p>50 Given an initial mapping from the data points to K clusters, the procedure alternates between (i) computing the centroid of each cluster and (ii) reallocating each data point to the closest cluster centroid, until convergence. [sent-157, score-0.197]
</p><p>51 Accordingly, we use Chiang’s hierarchical phrase based translation model (Chiang, 2007) as a base line, and the syntax-augmented MT model (Zollmann and Venugopal, 2006) as a ‘target line’, a model that would not be applicable for language pairs without linguistic resources. [sent-166, score-0.486]
</p><p>52 We perform PSCFG rule extraction and decoding using the open-source “SAMT” system (Venugopal and Zollmann, 2009), using the provided implementations for the hierarchical and syntax-augmented grammars. [sent-167, score-0.264]
</p><p>53 2 Further, to mitigate badly estimated PSCFG derivations based on low-frequency rules of the much sparser syntax model, the syntax grammar also contains the hierarchical grammar as a backbone (cf. [sent-169, score-0.672]
</p><p>54 We implemented our rule labeling approach within the SAMT rule extraction pipeline, resulting in comparable features across all systems. [sent-171, score-0.441]
</p><p>55 For all systems, we use the bottom-up chart parsing decoder implemented in the SAMT toolkit with a reordering limit of 15 source words, and correspondingly extract rules from initial phrase pairs of maximum source length 15. [sent-172, score-0.635]
</p><p>56 All rules have at most two nonterminal symbols, which must be non-consecutive on the source side, and rules must contain at least one source-side terminal symbol. [sent-173, score-0.612]
</p><p>57 3 Due to memory limitations, the multi-nonterminal grammars have to be pruned more harshly: We al2Penalization or reward of purely-lexical rules can be indirectly learned by trading off these features with the rule counter feature. [sent-175, score-0.382]
</p><p>58 Each system is trained separately to adapt the parameters to its specific properties (size of nonterminal set, grammar complexity, features sparseness, reliance on the language model, etc. [sent-182, score-0.383]
</p><p>59 The source and target language parses for the syntax-augmented grammar, as well as the POS tags for our POS-based grammars were generated by the Stanford parser (Klein and Manning, 2003). [sent-186, score-0.333]
</p><p>60 Our approach, using target POS tags (‘POS-tgt (no phr. [sent-190, score-0.199]
</p><p>61 )’), outperforms the hierarchical system on all three tests sets, and gains further improvements when accounting for phrase size (‘POS-tgt’). [sent-192, score-0.493]
</p><p>62 In line with previous findings for syntax-augmented grammars (Zollmann and Vogel, 2010), the source-side-based grammar does not reach the translation quality of its target-based counterpart; however, the model still outperforms the hi4As shown in Zollmann et al. [sent-199, score-0.316]
</p><p>63 (2008), the impact of these rules on translation quality is negligible. [sent-200, score-0.246]
</p><p>64 At the same time, this demonstrates that there is hence less of a role for the nonterminal labels to resolve translational ambiguity in the source based  model than in the target based model. [sent-204, score-0.458]
</p><p>65 Figure 1 (left) shows the performance of the distributional clustering model (‘Clust’) and its morphology-sensitive extension (‘Clust-morph’) according to this score for varying values of N =  ×  1, . [sent-211, score-0.215]
</p><p>66 Looking back at Table 1, we now compare the clustering models chosen by the procedure above—  36). [sent-219, score-0.208]
</p><p>67 12683 for Chinese-English NIST-large translation tasks, comparing baseline Hierarchical and Syntax systems with POS and clustering based approaches proposed in this work. [sent-252, score-0.286]
</p><p>68 ‘Clust-7-tgt’ improves over the hierarchical base line on all three test sets and is on par with the corresponding Syntax and POS target lines. [sent-256, score-0.254]
</p><p>69 We also experimented with a model variant based on seven source and seven target language clusters (‘Clust-7-src&tgt;’) and a source-only labeled model (‘Clust-7-src’)—both performing worse. [sent-258, score-0.288]
</p><p>70 Surprisingly, the morphology-sensitive clustering model (‘Clust-7-morph-tgt’), while still improving over the hierarchical system, performs worse than the morphology-unaware model. [sent-259, score-0.284]
</p><p>71 While these subtle distinctions make for good partitionings when the number of clusters 7  is large, they appear to lead to inferior results for our task that relies on coarse-grained partitionings of the vocabulary. [sent-263, score-0.268]
</p><p>72 K-means clustering based models To establish suitable values for the α parameters and investigate the impact of the number of clusters, we looked at the development performance over various parameter combinations for a K-means model based on source and/or target part-of-speech tags. [sent-265, score-0.328]
</p><p>73 The optimal ratio of weighting source and target classes is 0. [sent-268, score-0.254]
</p><p>74 when giving contexts 1/4 the influence of the phrase boundary words. [sent-274, score-0.368]
</p><p>75 Figure 1: Left: Performance of the distributional clustering model ‘Clust’ and its morphology-sensitive extension ‘Clust-morph’ according to L0-penalized development set BLEU score for varying numbers N of word classes. [sent-278, score-0.215]
</p><p>76 nonterminals of the induced grammar is stated in parentheses. [sent-281, score-0.224]
</p><p>77 While beating the hierarchical baseline, it is only minimally better than the much simpler target-based hard labeling method ‘POS-tgt’ . [sent-291, score-0.22]
</p><p>78 Unfortunately, these features appear to deteriorate performance, presumably because given a fixed number of clusters, accounting  for contents inside the phrase comes at the cost of neglect of boundary words, which are more relevant to producing correctly reordered translations. [sent-295, score-0.433]
</p><p>79 (2007) improve the statistical phrasebased MT model by injecting supertags, lexical information such as the POS tag of the word and its  subcategorization information, into the phrase table, resulting in generalized phrases with placeholders in them. [sent-304, score-0.468]
</p><p>80 Our approach also generates phrase labels and placeholders based on word tags (albeit in a different manner and without the use of subcategorization information), but produces PSCFG rules for use in a parsing-based decoding system. [sent-306, score-0.576]
</p><p>81 Unsupervised synchronous grammar induction, apart from the contribution of Chiang (2005) discussed earlier, has been proposed by Wu (1997) for inversion transduction grammars, but as Chiang’s model only uses a single generic nonterminal label. [sent-307, score-0.468]
</p><p>82 (2009) present a nonparametric PSCFG translation model that directly induces a grammar from parallel sentences without the use of or constraints from a word-alignment model, and Cohn and Blunsom (2009) achieve the same for tree-to-string grammars, with encouraging results on small data. [sent-309, score-0.29]
</p><p>83 Our phrase pair clustering approach is similar in spirit to the work of Lin and Wu (2009), who use Kmeans to cluster (monolingual) phrases and use the resulting clusters as features in discriminative classifiers for a named-entity-recognition and a query classification task. [sent-311, score-0.695]
</p><p>84 Phrases are represented in terms of their contexts, which can be more than one word long; words within the phrase are not considered. [sent-312, score-0.21]
</p><p>85 Another distinction is that Lin and Wu (2009) work with phrase types instead of phrase instances, obtaining a phrase type’s contexts by averaging the contexts of all its phrase instances. [sent-314, score-0.934]
</p><p>86 (2006) present a reordering model for machine translation, and make use of clustered phrase pairs to cope with data sparseness in the model. [sent-316, score-0.374]
</p><p>87 They achieve the clustering by reducing phrases to their head words and then applying the MKCLS tool to these pseudo-words. [sent-317, score-0.235]
</p><p>88 (2010) cluster the phrase pairs of an SMT phrase table based on their co-occurrence  counts and edit distances in order to arrive at semantically similar phrases for the purpose ofphrase table smoothing. [sent-319, score-0.575]
</p><p>89 The clustering proceeds in a bottom-up fashion, gradually merging similar phrases while alternating back and forth between the two languages. [sent-320, score-0.282]
</p><p>90 7  Conclusion and discussion  In this work we proposed methods oflabeling phrase pairs to create automatically learned PSCFG rules for machine translation. [sent-321, score-0.386]
</p><p>91 Evaluated on a Chinese-to-English translation task, our approach improves translation quality over a popular PSCFG baseline—the hierarchical model of Chiang (2005) —and performs on par 9  with the model of Zollmann and Venugopal (2006), using heuristically generated labels from parse trees. [sent-323, score-0.445]
</p><p>92 Using automatically obtained word clusters instead of POS tags yields essentially the same results, thus making our methods applicable to all languages pairs with parallel corpora, whether syntactic resources are available for them or not. [sent-324, score-0.325]
</p><p>93 We also propose a more flexible way of obtaining  the phrase labels from word classes using K-means clustering. [sent-325, score-0.356]
</p><p>94 When considering the constraints and independence relationships implied by each labeling approach, we can distinguish between approaches that label rules differently within the context of the sentence that they were extracted from, and those that do not. [sent-327, score-0.236]
</p><p>95 On the other extreme, the clustering based approach labels phrases based on the contained words alone. [sent-330, score-0.285]
</p><p>96 8 The POS grammar represents an intermediate point on this spectrum, since POS tags can change based on surrounding words in the sentence; and the position of the K-means model depends on the influence of the phrase contexts on the clustering process. [sent-331, score-0.708]
</p><p>97 Context insensitive labeling has the advantage that there are less alternative lefthand-side labels for initial rules, producing grammars with less rules, whose weights can be more  accurately estimated. [sent-332, score-0.294]
</p><p>98 Phrase clustering for smoothing TM probabilities - or, how to extract paraphrases from phrase tables. [sent-395, score-0.38]
</p><p>99 SPMT: Statistical machine translation with syntactified target language phrases. [sent-418, score-0.207]
</p><p>100 A clustered global phrase  reordering model for statistical machine translation. [sent-426, score-0.291]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pscfg', 0.354), ('zollmann', 0.257), ('nonterminal', 0.25), ('phrase', 0.21), ('venugopal', 0.171), ('clustering', 0.17), ('rule', 0.15), ('habe', 0.145), ('samt', 0.145), ('chiang', 0.133), ('grammar', 0.133), ('clusters', 0.13), ('rules', 0.13), ('accounting', 0.117), ('translation', 0.116), ('hierarchical', 0.114), ('tags', 0.108), ('labeling', 0.106), ('ich', 0.105), ('gesehen', 0.097), ('classes', 0.096), ('src', 0.092), ('target', 0.091), ('nonterminals', 0.091), ('cn', 0.09), ('mkcls', 0.085), ('ihn', 0.085), ('syntax', 0.081), ('tag', 0.08), ('en', 0.078), ('clust', 0.073), ('forgy', 0.073), ('pprrpp', 0.073), ('boundary', 0.071), ('initial', 0.071), ('pos', 0.07), ('source', 0.067), ('grammars', 0.067), ('phrases', 0.065), ('pscfgs', 0.064), ('wi', 0.064), ('saw', 0.061), ('ashish', 0.06), ('merely', 0.056), ('bilingual', 0.055), ('fm', 0.054), ('size', 0.052), ('labels', 0.05), ('par', 0.049), ('cntxt', 0.048), ('gnes', 0.048), ('heinm', 0.048), ('ihim', 0.048), ('iichnh', 0.048), ('partitionings', 0.048), ('phrsize', 0.048), ('ins', 0.048), ('partition', 0.048), ('side', 0.048), ('proceeds', 0.047), ('synchronous', 0.047), ('contexts', 0.047), ('pairs', 0.046), ('distributional', 0.045), ('vogel', 0.045), ('cluster', 0.044), ('reordering', 0.044), ('kneser', 0.044), ('andreas', 0.043), ('euclidean', 0.043), ('prp', 0.043), ('bilingually', 0.043), ('eligible', 0.043), ('placeholders', 0.043), ('symbols', 0.042), ('subtle', 0.042), ('pair', 0.041), ('parallel', 0.041), ('influence', 0.04), ('cnt', 0.039), ('cell', 0.039), ('generic', 0.038), ('bleu', 0.038), ('schemes', 0.038), ('procedure', 0.038), ('blunsom', 0.037), ('sparseness', 0.037), ('ih', 0.037), ('clustered', 0.037), ('terminal', 0.035), ('phrasal', 0.035), ('supertags', 0.035), ('subcategorization', 0.035), ('trading', 0.035), ('resulting', 0.035), ('inside', 0.035), ('slow', 0.035), ('cohn', 0.034), ('clusterings', 0.034), ('pin', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="29-tfidf-1" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>Author: Andreas Zollmann ; Stephan Vogel</p><p>Abstract: In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.</p><p>2 0.31018722 <a title="29-tfidf-2" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>3 0.20313028 <a title="29-tfidf-3" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>Author: Mark-Jan Nederhof ; Giorgio Satta</p><p>Abstract: We present a method for the computation of prefix probabilities for synchronous contextfree grammars. Our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms.</p><p>4 0.19799025 <a title="29-tfidf-4" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>5 0.19734371 <a title="29-tfidf-5" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Eiichiro Sumita ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: We present an unsupervised model for joint phrase alignment and extraction using nonparametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size.</p><p>6 0.18851902 <a title="29-tfidf-6" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>7 0.18522175 <a title="29-tfidf-7" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>8 0.1666497 <a title="29-tfidf-8" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>9 0.16390704 <a title="29-tfidf-9" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>10 0.1619494 <a title="29-tfidf-10" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>11 0.15694788 <a title="29-tfidf-11" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>12 0.14656278 <a title="29-tfidf-12" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>13 0.14101852 <a title="29-tfidf-13" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>14 0.12971078 <a title="29-tfidf-14" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>15 0.12805501 <a title="29-tfidf-15" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>16 0.1225302 <a title="29-tfidf-16" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>17 0.12008516 <a title="29-tfidf-17" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>18 0.11933178 <a title="29-tfidf-18" href="./acl-2011-Terminal-Aware_Synchronous_Binarization.html">296 acl-2011-Terminal-Aware Synchronous Binarization</a></p>
<p>19 0.11854336 <a title="29-tfidf-19" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>20 0.1112935 <a title="29-tfidf-20" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.339), (1, -0.202), (2, 0.076), (3, -0.019), (4, 0.043), (5, -0.008), (6, -0.123), (7, -0.033), (8, -0.056), (9, -0.031), (10, -0.023), (11, -0.04), (12, 0.014), (13, 0.1), (14, 0.043), (15, -0.093), (16, -0.107), (17, 0.077), (18, 0.023), (19, 0.046), (20, -0.023), (21, -0.039), (22, -0.019), (23, -0.166), (24, -0.02), (25, 0.083), (26, 0.036), (27, -0.104), (28, 0.038), (29, -0.036), (30, 0.032), (31, 0.001), (32, -0.011), (33, -0.055), (34, 0.009), (35, -0.049), (36, 0.137), (37, -0.026), (38, 0.016), (39, 0.105), (40, -0.026), (41, 0.104), (42, -0.046), (43, 0.004), (44, 0.003), (45, -0.054), (46, 0.024), (47, 0.076), (48, 0.009), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95694596 <a title="29-lsi-1" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>Author: Andreas Zollmann ; Stephan Vogel</p><p>Abstract: In this work we propose methods to label probabilistic synchronous context-free grammar (PSCFG) rules using only word tags, generated by either part-of-speech analysis or unsupervised word class induction. The proposals range from simple tag-combination schemes to a phrase clustering model that can incorporate an arbitrary number of features. Our models improve translation quality over the single generic label approach of Chiang (2005) and perform on par with the syntactically motivated approach from Zollmann and Venugopal (2006) on the NIST large Chineseto-English translation task. These results persist when using automatically learned word tags, suggesting broad applicability of our technique across diverse language pairs for which syntactic resources are not available.</p><p>2 0.80421191 <a title="29-lsi-2" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>3 0.80293328 <a title="29-lsi-3" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>Author: Mark-Jan Nederhof ; Giorgio Satta</p><p>Abstract: We present a method for the computation of prefix probabilities for synchronous contextfree grammars. Our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms.</p><p>4 0.76466978 <a title="29-lsi-4" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>Author: Michael Subotin</p><p>Abstract: This paper presents an exponential model for translation into highly inflected languages which can be scaled to very large datasets. As in other recent proposals, it predicts targetside phrases and can be conditioned on sourceside context. However, crucially for the task of modeling morphological generalizations, it estimates feature parameters from the entire training set rather than as a collection of separate classifiers. We apply it to English-Czech translation, using a variety of features capturing potential predictors for case, number, and gender, and one of the largest publicly available parallel data sets. We also describe generation and modeling of inflected forms unobserved in training data and decoding procedures for a model with non-local target-side feature dependencies.</p><p>5 0.76022053 <a title="29-lsi-5" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>Author: Tagyoung Chung ; Licheng Fang ; Daniel Gildea</p><p>Abstract: We discuss some of the practical issues that arise from decoding with general synchronous context-free grammars. We examine problems caused by unary rules and we also examine how virtual nonterminals resulting from binarization can best be handled. We also investigate adding more flexibility to synchronous context-free grammars by adding glue rules and phrases.</p><p>6 0.75123185 <a title="29-lsi-6" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>7 0.71796989 <a title="29-lsi-7" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>8 0.70165116 <a title="29-lsi-8" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>9 0.67265445 <a title="29-lsi-9" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>10 0.67171407 <a title="29-lsi-10" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>11 0.65758365 <a title="29-lsi-11" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>12 0.6513595 <a title="29-lsi-12" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>13 0.65080374 <a title="29-lsi-13" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>14 0.61782348 <a title="29-lsi-14" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>15 0.59695345 <a title="29-lsi-15" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>16 0.59029824 <a title="29-lsi-16" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>17 0.58865023 <a title="29-lsi-17" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>18 0.58271337 <a title="29-lsi-18" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>19 0.5782221 <a title="29-lsi-19" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>20 0.57572299 <a title="29-lsi-20" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.027), (17, 0.068), (26, 0.018), (37, 0.087), (39, 0.391), (41, 0.053), (55, 0.031), (59, 0.042), (72, 0.028), (91, 0.023), (96, 0.161)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97906756 <a title="29-lda-1" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>Author: Yoav Goldberg ; Michael Elhadad</p><p>Abstract: We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.</p><p>2 0.96748316 <a title="29-lda-2" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>Author: Jey Han Lau ; Karl Grieser ; David Newman ; Timothy Baldwin</p><p>Abstract: We propose a method for automatically labelling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.</p><p>3 0.96020186 <a title="29-lda-3" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>Author: Muhua Zhu ; Jingbo Zhu ; Minghan Hu</p><p>Abstract: For the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard. Experiments on two Chinese treebanks show that our approach improves conversion accuracy by 1.31% over a strong baseline.</p><p>4 0.92971623 <a title="29-lda-4" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>Author: Jacob Eisenstein ; Noah A. Smith ; Eric P. Xing</p><p>Abstract: We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. By imposing a composite ‘1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. Next, we conjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.</p><p>5 0.89110154 <a title="29-lda-5" href="./acl-2011-%2811-06-spirl%29.html">1 acl-2011-(11-06-spirl)</a></p>
<p>Author: (hal)</p><p>Abstract: unkown-abstract</p><p>6 0.88975477 <a title="29-lda-6" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>same-paper 7 0.88111889 <a title="29-lda-7" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>8 0.84671903 <a title="29-lda-8" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>9 0.76940858 <a title="29-lda-9" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>10 0.71015579 <a title="29-lda-10" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>11 0.70896554 <a title="29-lda-11" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>12 0.70656252 <a title="29-lda-12" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>13 0.69964558 <a title="29-lda-13" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>14 0.69878757 <a title="29-lda-14" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>15 0.69867039 <a title="29-lda-15" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>16 0.69783258 <a title="29-lda-16" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>17 0.69703484 <a title="29-lda-17" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>18 0.69376463 <a title="29-lda-18" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>19 0.68607098 <a title="29-lda-19" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>20 0.6837424 <a title="29-lda-20" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
