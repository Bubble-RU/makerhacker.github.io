<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-23" href="#">acl2011-23</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</h1>
<br/><p>Source: <a title="acl-2011-23-pdf" href="http://aclweb.org/anthology//P/P11/P11-1117.pdf">pdf</a></p><p>Author: Dingcheng Li ; Tim Miller ; William Schuler</p><p>Abstract: and Wellner, This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.</p><p>Reference: <a title="acl-2011-23-reference" href="../acl2011_reference/acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu l Abstract and Wellner, This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). [sent-4, score-0.804]
</p><p>2 The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. [sent-5, score-0.507]
</p><p>3 Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. [sent-6, score-0.706]
</p><p>4 1 Introduction Pronoun anaphora resolution is the task of finding the correct antecedent for a given pronominal anaphor in a document. [sent-8, score-0.447]
</p><p>5 It is a subtask of coreference resolution, which is the process of determin-  ing whether two or more linguistic expressions in a document refer to the same entity. [sent-9, score-0.373]
</p><p>6 Each mention is a reference to some entity in the domain of discourse. [sent-11, score-0.214]
</p><p>7 Mentions usually fall into three categories proper mentions (proper names), nominal mentions (descriptions), and pronominal mentions (pronouns). [sent-12, score-0.483]
</p><p>8 edu 2003) decompose the task into a col-  lection of pairwise or mention set coreference decisions. [sent-18, score-0.506]
</p><p>9 One line of research aiming at overcoming the limitation of pairwise models is to learn a mentionranking model to rank preceding mentions for a given anaphor (Denis and Baldridge, 2007) This approach results in more coherent coreference chains. [sent-24, score-0.654]
</p><p>10 Haghighi and Klein (2007), proposed an unsupervised nonparametric Bayesian model for coreference resolution. [sent-26, score-0.441]
</p><p>11 In contrast to pairwise models, this fully gener-  ative model produces each mention from a combination of global entity properties and local attentional state. [sent-27, score-0.299]
</p><p>12 Another unsupervised generative model was recently presented to tackle only pronoun anaphora ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. [sent-29, score-0.453]
</p><p>13 This model generates a pronoun’s person, number and gender features along with the governor of the pronoun and the syntactic relation between the pronoun and the governor. [sent-34, score-0.606]
</p><p>14 They report the best results to date on the complete end-to-end coreference task. [sent-38, score-0.373]
</p><p>15 Namely, the system identifies mentions from a parse tree and resolves resolution with a left-to-right sequential beam search. [sent-40, score-0.379]
</p><p>16 In this paper, we present a supervised pronoun resolution system based on Factorial Hidden Markov Models (FHMMs). [sent-42, score-0.4]
</p><p>17 According to Clark and Sengul (1979), anaphoric definite NPs are much faster retrieved if the antecedent of a pronoun is in immediately previous sentence. [sent-44, score-0.267]
</p><p>18 In order to construct an operable model, we also measured the average distance between pronouns and their antecedents as discussed in next sections and used distances as important salience fea-  tures in the model. [sent-46, score-0.204]
</p><p>19 Although the system described here is evaluated for pronoun resolution, the framework we describe can be extended to more general coreference resolution in a fairly straightforward manner. [sent-51, score-0.773]
</p><p>20 The FHMM-based pronoun resolution system does a better job than the global ranking technique and other approaches. [sent-55, score-0.4]
</p><p>21 This is a promising start for this novel FHMM-based pronoun resolution system. [sent-56, score-0.366]
</p><p>22 Unlike the more commonly known Hidden Markov Model (HMM), in an FHMM the hidden state at each time step is expanded to contain more than one random variable (as shown in Figure 1). [sent-58, score-0.222]
</p><p>23 This allows for the use of more complex hidden states by taking advantage of conditional independence between substates. [sent-59, score-0.221]
</p><p>24 This conditional independence allows complex hidden states to be learned with limited training data. [sent-60, score-0.221]
</p><p>25 HMMs represent sequential data as a sequence of hidden states generating observation states (words in this case) at corresponding time steps t. [sent-63, score-0.326]
</p><p>26 TaxtY=1PΘT(ht|ht−1) · PΘO(ot|ht) (3) For a simple HMM, the hidden state corresponding to each observation state only involves one variable. [sent-83, score-0.31]
</p><p>27 An FHMM contains more than one hidden variable  in the hidden state. [sent-84, score-0.282]
</p><p>28 As Figure 1 shows, the hidden states include three sub-states, op, cr and pos which are short forms of operation, coreference feature and part-of-speech. [sent-87, score-0.671]
</p><p>29 PΘO(ot |  ht)d=ef P(ot  |post,crt)  (5)  The observation state depends on more than one hidden state at each time step in an FHMM. [sent-90, score-0.31]
</p><p>30 After coreference is resolved, the coreferring chain can then be expanded to the whole phrase with NP chunker tools. [sent-96, score-0.373]
</p><p>31 1171 In this system, hidden states are composed of three main variables: a referent operation (OP), coreference features (CR) and part of speech tags (POS) as displayed in Figure 1. [sent-97, score-0.686]
</p><p>32 Figure 1: Factorial HMM CR Model The starting point for the hidden state at each time step is the OP variable, which determines which kind of referent operations will occur at the current word. [sent-99, score-0.241]
</p><p>33 All previous hidden state  values (the list of previous mentions) will be passed deterministically (with probability 1) to the current time step without any changes. [sent-102, score-0.261]
</p><p>34 In this event, a new mention will be added to the entity set, as represented by its set of feature values and position in the coreference table. [sent-104, score-0.617]
</p><p>35 The old state indicates that there is a mention in the present time state and that this mention refers back to some antecedent mention. [sent-105, score-0.484]
</p><p>36 In such a case, the list of entities in the buffer will be reordered deterministically, moving the currently mentioned entity to the top of the list. [sent-106, score-0.371]
</p><p>37 Notice that opt is defined to depend on opt−1 and post−1 . [sent-107, score-0.293]
</p><p>38 This dependency can be useful, for example, if opt−1 is new, in which case opt has a higher probability of being none or old. [sent-109, score-0.293]
</p><p>39 If post−1 is a verb or preposition, opt has more probability of being old or new. [sent-110, score-0.366]
</p><p>40 One may wonder why opt generates post, and not the other way around. [sent-111, score-0.332]
</p><p>41 This model only roughly models the process of (new and old) entity generation, and either direction of causality might be con-  sistent with a model of human entity generation, but this direction of causality is chosen to represent the effect of semantics (referents) generating syntax (POS tags). [sent-112, score-0.332]
</p><p>42 In addition, this is a joint model in which POS tagging and coreference resolution are integrated together, so the best combination of those hidden states will be computed in either case. [sent-113, score-0.784]
</p><p>43 In this paper, they mainly include index (I), named entity type (E), number (N) and gender (G). [sent-116, score-0.307]
</p><p>44 The index feature represents the order that a mention was encountered relative to the other mentions in the buffer. [sent-117, score-0.323]
</p><p>45 The features are just a shorthand way of representing some well known essential aspects of a referent (as pertains to anaphora resolution) in a discourse model. [sent-120, score-0.266]
</p><p>46 This model must then have a mechanism for jointly considering pronouns in tandem with previous mentions, as well as the features of those mentions that might be used to find matches between pronouns and antecedents. [sent-122, score-0.495]
</p><p>47 This is especially true for coreference resolution because pronouns often refer back to mentions that are far away from the present state. [sent-124, score-0.883]
</p><p>48 In this case, we would need to know information about mentions which are  at least two mentions before the present one. [sent-125, score-0.322]
</p><p>49 In this sense, a higher order HMM may seem ideal for coreference resolution. [sent-126, score-0.373]
</p><p>50 The entity buffer is intended to model the set of ‘activated’ entities in the discourse those which could plausibly be referred to with a pronoun. [sent-130, score-0.401]
</p><p>51 Fortunately, human short term memory faces effectively similar limitations and thus pronouns usually refer back to –  mentions not very far away. [sent-133, score-0.363]
</p><p>52 Since the buffer of our system will carry forward a few previous groups of coreference features plus op and pos, the computational complexity will be exorbitantly high if we keep high beam size and meanwhile if each feature interacts with others. [sent-135, score-0.711]
</p><p>53 First, we estimate the size of buffer with a simple count of average distances between pronouns and their antecedents in the corpus. [sent-137, score-0.37]
</p><p>54 Secondly, the coreference features we have used have the nice property of being independent from one another. [sent-140, score-0.408]
</p><p>55 Thus, we treat each coreference feature as an independent event. [sent-148, score-0.403]
</p><p>56 Hence, we can safely split coreference features into separate parts. [sent-149, score-0.408]
</p><p>57 The probability of the new state of the coreference table P(crt | crt−1 , opt) is defined to be the product of probabilities of the individual feature transitions. [sent-152, score-0.464]
</p><p>58 4 Feature Passing Equation 7 is correct and complete, but in fact the switching variable for operation type results in three different cases which simplifies the calculation of the transition probabilities for the coreference feature table. [sent-156, score-0.547]
</p><p>59 Note the following observations about coreference features: it only needs a probabilistic model when opt is old in other words, only when the model must choose between several antecedents to re-refer to. [sent-157, score-0.886]
</p><p>60 gt, et and nt are deterministic except 1173 when opt is new, when gender, entity type, and number information must be generated for the new entity being introduced. [sent-158, score-0.625]
</p><p>61 When opt is none, all coreference variables (entity features) will be copied over from the previous –  time step to the current time step, and the probability of this transition is 1. [sent-159, score-0.708]
</p><p>62 When opt is new, it is changed deterministically by adding the new entity to the first position in the list and moving every other entity down one position. [sent-161, score-0.63]
</p><p>63 If the list of entities is full, the least recently mentioned entity will be discarded. [sent-162, score-0.234]
</p><p>64 When opt is old, it will probabilistically select a value 1. [sent-164, score-0.327]
</p><p>65 The reorder model actually implements the list reordering for each independent feature by moving the feature value corresponding to the selected entity in the index model to the top of that feature’s list. [sent-172, score-0.346]
</p><p>66 Note that post is used in both hidden states and observation states. [sent-177, score-0.378]
</p><p>67 While it is not considered a coreference feature as such, it can still play an important role in the resolving process. [sent-178, score-0.439]
</p><p>68 Basically, the system tags parts of speech incrementally while simultaneously resolving pronoun anaphora. [sent-179, score-0.252]
</p><p>69 In training, pronouns are sub-categorised into personal pronouns, reflexive and other-pronoun. [sent-185, score-0.239]
</p><p>70 We then define a variable loct whose value is how far back in the list of antecedents the current hypothesis must have gone to arrive at the current value of it. [sent-186, score-0.342]
</p><p>71 If we have the syntax annotations or parsed trees, then, the part of speech model can be defined when opt is old as Pbinding (post | loct, sloct ). [sent-187, score-0.474]
</p><p>72 For example, if post ∈ reflexive, P(post | loct, sloct ) where loct has sm∈al rleerf vlaexluievse (implying c|llooscer mentions to post) and sloct = subject should have higher values since reflexive pronouns always re–  fer back to subjects within its governing domains. [sent-188, score-0.82]
</p><p>73 For example, suppose the buffer size is 6 and loct = 5, posloct = noun. [sent-199, score-0.396]
</p><p>74 Then, P(post = reflexive | loct, posloct) is usually higher than P(post = pronoun | loct, posloct ), since the reflexive has a higher probability of referring back to the noun located in position 5 than the  pronoun. [sent-200, score-0.473]
</p><p>75 In future work expanding to coreference resolution between any noun phrases we intend to integrate syntax into this framework as a joint model of coreference resolution and parsing. [sent-201, score-1.193]
</p><p>76 To expand that equation in detail, the observation state, the word, depends on its part of speech and its coreference features as well. [sent-203, score-0.519]
</p><p>77 Since FHMMs are generative, we can say part of speech and coreference features generate the word. [sent-204, score-0.408]
</p><p>78 In actual implementation, the observed model will be very sparse, since crt will be split into more variables according to how many coreference features it is composed of. [sent-205, score-0.674]
</p><p>79 Instead, the observation model generates hidden states, which is more a combination of discriminative and generative approaches. [sent-209, score-0.295]
</p><p>80 This way facilitates building likelihood model files offeatures for given mentions from the training data. [sent-210, score-0.234]
</p><p>81 The hidden state transition model represents prior probabilities of coreference features associated with each while this observation model factors in the probability given a pronoun. [sent-211, score-0.773]
</p><p>82 Instead, we follow Bergsma and Lin (2005) to get the distribution of gender from their gender/number data and then predict the gender for unknown words. [sent-223, score-0.298]
</p><p>83 The main point of comparison is Denis and Baldridge (2007), which was similar in that it described a new type of coreference resolver using simple features. [sent-243, score-0.373]
</p><p>84 Namely, pronouns associated with named entity types could be used in this system. [sent-245, score-0.26]
</p><p>85 Firstly, emPronouns is a publicly available system with high accuracy in pronoun resolution. [sent-252, score-0.216]
</p><p>86 During training, besides coreference annotation itself, the part of speech, dependencies between words and named entities, gender, number and index are extracted using relative frequency estimation to train models for the coreference resolution system. [sent-255, score-0.977]
</p><p>87 The entity buffer used in these experiments kept track of only the six most recent men-  tions. [sent-257, score-0.295]
</p><p>88 In addition, this system does not do anaphoricity detection, so the antecedent operation for non-anaphora pronoun it is set to be none. [sent-259, score-0.33]
</p><p>89 This shows that number and gender agreements play an important role in pronoun anaphora resolution. [sent-273, score-0.456]
</p><p>90 In last section, we mention that articles in NPAPER are longer than other genres and also have denser coreference chains while articles in BENEWS are shorter and have sparer chains. [sent-307, score-0.539]
</p><p>91 Long-distance anaphora resolution may pose a problem since the buffer size cannot be too long considering the complexity of tracking a large number of mentions through time. [sent-330, score-0.654]
</p><p>92 Future work will explore optimizations that will allow for larger or variable buffer sizes so that longer distance anaphora can be detected. [sent-333, score-0.378]
</p><p>93 This is not a problem for singular pronouns since gender features can tell whether pronouns are personal or not. [sent-336, score-0.464]
</p><p>94 In this example, the pronoun they corefers with the noun phrase President Barack Obama and his wife Michelle Obama. [sent-345, score-0.255]
</p><p>95 Finally, while the coreference feature annotations of the ACE are valuable for learning feature models, the model training may still give some misleading results. [sent-348, score-0.47]
</p><p>96 5  Conclusion and Future Work  This paper has presented a pronoun anaphora resolution system based on FHMMs. [sent-351, score-0.543]
</p><p>97 This generative system incrementally resolves pronoun anaphora with  an entity buffer carrying forward mention features. [sent-352, score-0.77]
</p><p>98 Simple coreference resolution with rich syntactic and semantic features. [sent-405, score-0.557]
</p><p>99 A public reference implementation of the rap anaphora resolution algorithm. [sent-470, score-0.327]
</p><p>100 Improving pronoun resolution by incorporating coreferential information of candidates. [sent-479, score-0.366]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.373), ('opt', 0.293), ('fhmm', 0.247), ('crt', 0.229), ('resolution', 0.184), ('pronoun', 0.182), ('buffer', 0.166), ('mentions', 0.161), ('loct', 0.159), ('bnews', 0.143), ('anaphora', 0.143), ('fhmms', 0.141), ('factorial', 0.14), ('pronouns', 0.131), ('gender', 0.131), ('gt', 0.129), ('entity', 0.129), ('ace', 0.126), ('npaper', 0.124), ('empronouns', 0.124), ('hidden', 0.121), ('post', 0.121), ('nwire', 0.1), ('ot', 0.089), ('antecedent', 0.085), ('mention', 0.085), ('ht', 0.077), ('denis', 0.077), ('baldridge', 0.075), ('nt', 0.074), ('old', 0.073), ('op', 0.073), ('antecedents', 0.073), ('reflexive', 0.072), ('posloct', 0.071), ('sloct', 0.071), ('states', 0.069), ('observation', 0.067), ('state', 0.061), ('elsner', 0.061), ('referent', 0.059), ('hmms', 0.058), ('rk', 0.057), ('haghighi', 0.053), ('preorder', 0.053), ('tcc', 0.053), ('genres', 0.052), ('charniak', 0.049), ('pairwise', 0.048), ('index', 0.047), ('scc', 0.047), ('equation', 0.044), ('ef', 0.044), ('deterministically', 0.043), ('transition', 0.042), ('noun', 0.042), ('hmm', 0.041), ('cr', 0.041), ('entities', 0.04), ('variable', 0.04), ('generates', 0.039), ('markov', 0.039), ('binding', 0.038), ('model', 0.037), ('pos', 0.037), ('memory', 0.037), ('unknown', 0.036), ('plural', 0.036), ('resolving', 0.036), ('files', 0.036), ('personal', 0.036), ('list', 0.036), ('anaphor', 0.035), ('hasler', 0.035), ('ppo', 0.035), ('substates', 0.035), ('unkword', 0.035), ('demands', 0.035), ('features', 0.035), ('system', 0.034), ('probabilistically', 0.034), ('back', 0.034), ('switching', 0.033), ('generative', 0.031), ('independence', 0.031), ('unsupervised', 0.031), ('devtest', 0.031), ('markables', 0.031), ('michelle', 0.031), ('pnew', 0.031), ('twin', 0.031), ('wife', 0.031), ('wisconsin', 0.031), ('feature', 0.03), ('bergsma', 0.03), ('klein', 0.03), ('longer', 0.029), ('operation', 0.029), ('recently', 0.029), ('discourse', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="23-tfidf-1" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>Author: Dingcheng Li ; Tim Miller ; William Schuler</p><p>Abstract: and Wellner, This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.</p><p>2 0.30995381 <a title="23-tfidf-2" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>Author: Sameer Singh ; Amarnag Subramanya ; Fernando Pereira ; Andrew McCallum</p><p>Abstract: Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach.</p><p>3 0.29982129 <a title="23-tfidf-3" href="./acl-2011-A_Cross-Lingual_ILP_Solution_to_Zero_Anaphora_Resolution.html">9 acl-2011-A Cross-Lingual ILP Solution to Zero Anaphora Resolution</a></p>
<p>Author: Ryu Iida ; Massimo Poesio</p><p>Abstract: We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. We show that this new model outperforms several baselines and competing models, as well as a direct translation of the Denis / Baldridge model, for both Italian and Japanese zero anaphora. We incorporate our model in complete anaphoric resolvers for both Italian and Japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classifiers are used for zeros and for ex- plicitly realized anaphors.</p><p>4 0.24985683 <a title="23-tfidf-4" href="./acl-2011-Extending_the_Entity_Grid_with_Entity-Specific_Features.html">129 acl-2011-Extending the Entity Grid with Entity-Specific Features</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We extend the popular entity grid representation for local coherence modeling. The grid abstracts away information about the entities it models; we add discourse prominence, named entity type and coreference features to distinguish between important and unimportant entities. We improve the best result for WSJ document discrimination by 6%.</p><p>5 0.22986092 <a title="23-tfidf-5" href="./acl-2011-Bootstrapping_coreference_resolution_using_word_associations.html">63 acl-2011-Bootstrapping coreference resolution using word associations</a></p>
<p>Author: Hamidreza Kobdani ; Hinrich Schuetze ; Michael Schiehlen ; Hans Kamp</p><p>Abstract: In this paper, we present an unsupervised framework that bootstraps a complete coreference resolution (CoRe) system from word associations mined from a large unlabeled corpus. We show that word associations are useful for CoRe – e.g., the strong association between Obama and President is an indicator of likely coreference. Association information has so far not been used in CoRe because it is sparse and difficult to learn from small labeled corpora. Since unlabeled text is readily available, our unsupervised approach addresses the sparseness problem. In a self-training framework, we train a decision tree on a corpus that is automatically labeled using word associations. We show that this unsupervised system has better CoRe performance than other learning approaches that do not use manually labeled data. .</p><p>6 0.20756209 <a title="23-tfidf-6" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>7 0.17378096 <a title="23-tfidf-7" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>8 0.14458652 <a title="23-tfidf-8" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>9 0.10889732 <a title="23-tfidf-9" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>10 0.10435433 <a title="23-tfidf-10" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>11 0.10193389 <a title="23-tfidf-11" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>12 0.099808834 <a title="23-tfidf-12" href="./acl-2011-Disentangling_Chat_with_Local_Coherence_Models.html">101 acl-2011-Disentangling Chat with Local Coherence Models</a></p>
<p>13 0.094876483 <a title="23-tfidf-13" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>14 0.090435319 <a title="23-tfidf-14" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>15 0.090238541 <a title="23-tfidf-15" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>16 0.07518734 <a title="23-tfidf-16" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>17 0.07425227 <a title="23-tfidf-17" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>18 0.07154467 <a title="23-tfidf-18" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>19 0.067963958 <a title="23-tfidf-19" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>20 0.064984828 <a title="23-tfidf-20" href="./acl-2011-Automatically_Evaluating_Text_Coherence_Using_Discourse_Relations.html">53 acl-2011-Automatically Evaluating Text Coherence Using Discourse Relations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, 0.034), (2, -0.167), (3, -0.014), (4, 0.117), (5, 0.075), (6, 0.029), (7, -0.092), (8, -0.285), (9, 0.1), (10, 0.066), (11, 0.001), (12, -0.159), (13, -0.09), (14, 0.046), (15, 0.09), (16, -0.049), (17, 0.108), (18, 0.01), (19, 0.096), (20, -0.033), (21, 0.043), (22, 0.06), (23, 0.124), (24, -0.1), (25, 0.048), (26, -0.055), (27, -0.088), (28, -0.191), (29, -0.188), (30, 0.135), (31, -0.122), (32, -0.059), (33, 0.004), (34, -0.133), (35, -0.045), (36, 0.001), (37, 0.032), (38, 0.085), (39, 0.041), (40, 0.077), (41, -0.043), (42, -0.036), (43, 0.027), (44, 0.006), (45, -0.018), (46, -0.022), (47, -0.096), (48, 0.024), (49, -0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92571938 <a title="23-lsi-1" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>Author: Dingcheng Li ; Tim Miller ; William Schuler</p><p>Abstract: and Wellner, This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.</p><p>2 0.89814037 <a title="23-lsi-2" href="./acl-2011-A_Cross-Lingual_ILP_Solution_to_Zero_Anaphora_Resolution.html">9 acl-2011-A Cross-Lingual ILP Solution to Zero Anaphora Resolution</a></p>
<p>Author: Ryu Iida ; Massimo Poesio</p><p>Abstract: We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. We show that this new model outperforms several baselines and competing models, as well as a direct translation of the Denis / Baldridge model, for both Italian and Japanese zero anaphora. We incorporate our model in complete anaphoric resolvers for both Italian and Japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classifiers are used for zeros and for ex- plicitly realized anaphors.</p><p>3 0.81345743 <a title="23-lsi-3" href="./acl-2011-Bootstrapping_coreference_resolution_using_word_associations.html">63 acl-2011-Bootstrapping coreference resolution using word associations</a></p>
<p>Author: Hamidreza Kobdani ; Hinrich Schuetze ; Michael Schiehlen ; Hans Kamp</p><p>Abstract: In this paper, we present an unsupervised framework that bootstraps a complete coreference resolution (CoRe) system from word associations mined from a large unlabeled corpus. We show that word associations are useful for CoRe – e.g., the strong association between Obama and President is an indicator of likely coreference. Association information has so far not been used in CoRe because it is sparse and difficult to learn from small labeled corpora. Since unlabeled text is readily available, our unsupervised approach addresses the sparseness problem. In a self-training framework, we train a decision tree on a corpus that is automatically labeled using word associations. We show that this unsupervised system has better CoRe performance than other learning approaches that do not use manually labeled data. .</p><p>4 0.80663341 <a title="23-lsi-4" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>Author: Altaf Rahman ; Vincent Ng</p><p>Abstract: While world knowledge has been shown to improve learning-based coreference resolvers, the improvements were typically obtained by incorporating world knowledge into a fairly weak baseline resolver. Hence, it is not clear whether these benefits can carry over to a stronger baseline. Moreover, since there has been no attempt to apply different sources of world knowledge in combination to coreference resolution, it is not clear whether they offer complementary benefits to a resolver. We systematically compare commonly-used and under-investigated sources of world knowledge for coreference resolution by applying them to two learning-based coreference models and evaluating them on documents annotated with two different annotation schemes.</p><p>5 0.7716558 <a title="23-lsi-5" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>Author: Sameer Singh ; Amarnag Subramanya ; Fernando Pereira ; Andrew McCallum</p><p>Abstract: Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach.</p><p>6 0.63997543 <a title="23-lsi-6" href="./acl-2011-Extending_the_Entity_Grid_with_Entity-Specific_Features.html">129 acl-2011-Extending the Entity Grid with Entity-Specific Features</a></p>
<p>7 0.4478178 <a title="23-lsi-7" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>8 0.42318007 <a title="23-lsi-8" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>9 0.42292935 <a title="23-lsi-9" href="./acl-2011-Disentangling_Chat_with_Local_Coherence_Models.html">101 acl-2011-Disentangling Chat with Local Coherence Models</a></p>
<p>10 0.4197875 <a title="23-lsi-10" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>11 0.39175448 <a title="23-lsi-11" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>12 0.31348822 <a title="23-lsi-12" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>13 0.30239126 <a title="23-lsi-13" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>14 0.30032611 <a title="23-lsi-14" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>15 0.28588784 <a title="23-lsi-15" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>16 0.28480369 <a title="23-lsi-16" href="./acl-2011-Exploiting_Morphology_in_Turkish_Named_Entity_Recognition_System.html">124 acl-2011-Exploiting Morphology in Turkish Named Entity Recognition System</a></p>
<p>17 0.28236645 <a title="23-lsi-17" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>18 0.28084362 <a title="23-lsi-18" href="./acl-2011-That%27s_What_She_Said%3A_Double_Entendre_Identification.html">297 acl-2011-That's What She Said: Double Entendre Identification</a></p>
<p>19 0.27785712 <a title="23-lsi-19" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>20 0.27766982 <a title="23-lsi-20" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.028), (16, 0.028), (17, 0.071), (26, 0.027), (37, 0.103), (38, 0.227), (39, 0.05), (41, 0.089), (55, 0.02), (59, 0.035), (72, 0.038), (91, 0.035), (96, 0.134)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78085387 <a title="23-lda-1" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>Author: Dingcheng Li ; Tim Miller ; William Schuler</p><p>Abstract: and Wellner, This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.</p><p>2 0.77916837 <a title="23-lda-2" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>Author: Shane Bergsma ; David Yarowsky ; Kenneth Church</p><p>Abstract: Resolving coordination ambiguity is a classic hard problem. This paper looks at coordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don’t do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via achieves data and pervised tations. co-training. The co-trained classifier close to 96% accuracy on Treebank makes 20% fewer errors than a susystem trained with Treebank anno-</p><p>3 0.74540007 <a title="23-lda-3" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>Author: Vahed Qazvinian ; Dragomir R. Radev</p><p>Abstract: We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a net- work of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.</p><p>4 0.67945606 <a title="23-lda-4" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>5 0.67858636 <a title="23-lda-5" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>Author: Shasha Liao ; Ralph Grishman</p><p>Abstract: Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 1</p><p>6 0.66909176 <a title="23-lda-6" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>7 0.66838706 <a title="23-lda-7" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>8 0.66785371 <a title="23-lda-8" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>9 0.66577309 <a title="23-lda-9" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>10 0.66356891 <a title="23-lda-10" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>11 0.66355699 <a title="23-lda-11" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>12 0.66344941 <a title="23-lda-12" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>13 0.66300362 <a title="23-lda-13" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>14 0.66240585 <a title="23-lda-14" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>15 0.66210836 <a title="23-lda-15" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>16 0.66190481 <a title="23-lda-16" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>17 0.66025287 <a title="23-lda-17" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>18 0.65926403 <a title="23-lda-18" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>19 0.6585803 <a title="23-lda-19" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>20 0.65792286 <a title="23-lda-20" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
