<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-272" href="#">acl2011-272</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</h1>
<br/><p>Source: <a title="acl-2011-272-pdf" href="http://aclweb.org/anthology//P/P11/P11-2106.pdf">pdf</a></p><p>Author: Wei-Bin Liang ; Chung-Hsien Wu ; Chia-Ping Chen</p><p>Abstract: In this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. First, partial sentence trees are proposed to represent a speech recognition output sentence. Semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. The constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. The proposed approach is implemented and evaluated in a Mandarin spoken dialogue system for tour-guiding service. Combined with scores derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84.3% detection accuracy, an absolute improvement of 34.7% over the baseline of the semantic slot-based method with 49.6% detection accuracy.</p><p>Reference: <a title="acl-2011-272-reference" href="../acl2011_reference/acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System Wei-Bin Liang1 Chung-Hsien Wu2 Chia-Ping Chen  Department of Computer Science and Information Engineering National Cheng Kung University Tainan, Taiwan  1l angnet @ gmai l. [sent-1, score-0.029]
</p><p>2 com Abstract In this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. [sent-3, score-1.335]
</p><p>3 First, partial sentence trees are proposed to represent a speech recognition output sentence. [sent-4, score-0.262]
</p><p>4 Semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. [sent-5, score-1.062]
</p><p>5 The constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. [sent-6, score-0.802]
</p><p>6 The proposed approach is implemented and evaluated in a Mandarin spoken dialogue system for tour-guiding service. [sent-7, score-0.509]
</p><p>7 Combined with scores  derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84. [sent-8, score-0.486]
</p><p>8 1 Introduction An intuitive framework for spoken dialogue system (SDS) can be regarded as a chain process. [sent-12, score-0.509]
</p><p>9 Specifically, the automatic speech recognition (ASR) module accepts the user’s utterance Ut and returns a string of words Wt The spoken language understanding (SLU) module converts Wt to an abstract representation of the user’s dialogue act (DA). [sent-13, score-1.02]
</p><p>10 The dialogue management (DM) module determines the user’s dialogue act At∗ and accordingly decides the current act of the system. [sent-14, score-1.297]
</p><p>11 The system DA is converted to a surface representation by natural lanDepartment of Computer Science and Engineering National Sun Yat-sen University Kaohsiung, Taiwan cpchen @mai l c s e . [sent-15, score-0.027]
</p><p>12 guage generation in the textual form, which is passed to a text-to-speech synthesizer for speech waveform generation. [sent-20, score-0.055]
</p><p>13 The cycle repeats when the user responds with a new utterance. [sent-21, score-0.064]
</p><p>14 Clearly, one can see that the inference of the user’s overall intention via DA detection is an important task in SDS. [sent-22, score-0.124]
</p><p>15 Figure 1 depicts the training and test phases of the SLU module and the DM module in our system. [sent-23, score-0.222]
</p><p>16 The dataflow for training and testing are indicated by blue arrows and red arrows, respectively. [sent-24, score-0.035]
</p><p>17 The input word sequences are converted to partial sen-  tence trees (PST) (Wu and Chen, 2004) in the PST Construction block. [sent-25, score-0.13]
</p><p>18 The derivation rule (DR) Generation block extracts derivation rules from the training text. [sent-26, score-0.447]
</p><p>19 The DR-DA matrix is created after clustering the sentences into different dialogue acts (DAs), counting the occurrences the DRs in DA, and introducing an entropy-based weighting scheme (Bellegarda, 2000). [sent-27, score-0.605]
</p><p>20 This matrix is pivotal in the computation of the lexical score. [sent-28, score-0.108]
</p><p>21 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 603–608, optimal dialogue act, and a proper action by the system is taken. [sent-31, score-0.443]
</p><p>22 Furthermore, a predefined keyword list is used and the keyword tokens are replaced by the corresponding named entity classes (NEC) in order to obtain a compact feature set. [sent-33, score-0.235]
</p><p>23 2 Models for Dialogue Act Detection Referring to the SDS depicted in Figure 1, the DA detection can be formulated as follows. [sent-34, score-0.124]
</p><p>24 At turn t,  the most likely DA is determined by At∗ = argmaxPr(A|Ut,Ht),  (1)  where Ut is the user’s utterance, Ht is the dialogue historical information, and Ω = {A1, . [sent-35, score-0.443]
</p><p>25 Using tohne, manadxi Ωm =um { approximation fthoer summation, (1) can be written as  At∗= argmax ≈  XWPr(A,W|Ut,Ht)  argmaxmWaxPr(A,W|Ut,Ht)  = aArg∈Ωm,WaxPr(W|Ut,Ht)Pr(A|W,Ut,Ht), (2) where W is the ASR output. [sent-39, score-0.027]
</p><p>26 Since the ASR output is independent of Ht given Ut, the ASR-related first term in (2) can be re-written as Pr(W|Ut, Ht) = Pr(W|Ut)  ∝  f(W, Ut),  (3)  where the function f(W, Ut) is introduced as the ASR score function. [sent-40, score-0.046]
</p><p>27 604  Figure 2: An example of a dialogue management module using n-gram model for dialogue act sequence in the domain of historic spot. [sent-44, score-1.163]
</p><p>28 3  ASR Score and History Score  For the ASR score, we use the conventional recognition probability of the ASR recognition model. [sent-45, score-0.086]
</p><p>29 For the history score, similar to the schemes used in (Hori et al. [sent-46, score-0.094]
</p><p>30 , 2009a), a back-off bi-gram model for DA sequence is estimated from the data collected by the SDS. [sent-49, score-0.029]
</p><p>31 The estimated bi-gram model is used to calculate the history score. [sent-50, score-0.123]
</p><p>32 (6)  Essentially, (6) is based on a Markov model assumption for the chain of the dialogue acts. [sent-52, score-0.443]
</p><p>33 Figure 2 shows an example of dialogue controlling model of an SDS. [sent-53, score-0.443]
</p><p>34 A dialogue begins with the greeting state and ends with the ending state. [sent-55, score-0.443]
</p><p>35 During a session, a user can inquire the system about the provided services and then choose one service to continue (e. [sent-56, score-0.064]
</p><p>36 4  The Lexical Score Function  The main challenge of this system is the computation of the lexical score g(A, W). [sent-59, score-0.046]
</p><p>37 1 Construction of Partial Sentence Tree In an SDS, it is often beneficial to define a set of keywords K, and a set of non-keywords N. [sent-62, score-0.071]
</p><p>38 Each kweoyrwd w ∈ ,K a sdho aul sde b oef ninodnic-kaetiyvwe orfd sth Ne . [sent-63, score-0.042]
</p><p>39 Tshhoeu lsdet oef nsdenitceanticvees oSf containing at least one keyword itn o Kf ,s can bcee represented as aSt = a Nst∗ (K N ke∗y)w+,o wrdh einre K KK,+ c means a string otefd one or more w(Kor Nds in K, . [sent-65, score-0.103]
</p><p>40 w Ghievreen K a sentence s ∈ S, a parotiarl m soenrete wncored iss ifno rKm. [sent-66, score-0.061]
</p><p>41 e Gd by keeping naclle t she ∈ keywords in s and some of the non-keywords in s. [sent-67, score-0.071]
</p><p>42 These  Figure 3: Construction of the partial sentence tree for the sentence Where is the Anping-Fort. [sent-68, score-0.161]
</p><p>43 partial sentences can be compiled in a tree, called  the partial sentence tree (PST) and denoted as T (s). [sent-69, score-0.256]
</p><p>44 t DA detection as the ASR module could be errorprone in adverse environments. [sent-71, score-0.235]
</p><p>45 In addition, words that are not confidently recognized are replaced by a special non-keyword token called Filler. [sent-72, score-0.064]
</p><p>46 There are two keywords Where and Anping-Fort and two non-keywords is and the. [sent-75, score-0.071]
</p><p>47 Note that with 2 non-keywords in the original sentence s, we have 22 = 4 partial sentences in the PST T (s). [sent-76, score-0.127]
</p><p>48 2 Extraction of the Derivation Rules After text processing, a sentence s is parsed by the statistical Stanford parser (S-parser) (Levy and Manning, 2003). [sent-78, score-0.034]
</p><p>49 AP derivation rule is defined to be a derivation of the form A → B → w where A, B ∈ V and w ∈ Σ. [sent-80, score-0.406]
</p><p>50 fTohrem parsing rB esu →lt →o fw wth weh exemplar s ∈en Vten acned s represented in the parenthesized expression is shown in Figure 4. [sent-81, score-0.093]
</p><p>51 For example, v(s) = [10 10]T means that there are four derivation rules, of which R1 and R3 are used in T (s). [sent-90, score-0.18]
</p><p>52 Moreover, the probabilistic nature of the S-parser renders the DRs extracted from the parsing results quite robust and consistent, even for the error-prone ASR output sentences. [sent-94, score-0.027]
</p><p>53 3 Generation of Dialogue Acts The basic idea of data-driven DA is to cluster sentences in the set and identify the clusters as formed by the sentences of the same DA. [sent-96, score-0.028]
</p><p>54 In this work, the spectral clustering algorithm (von Luxburg, 2007) is employed for sentence clustering. [sent-97, score-0.095]
</p><p>55 Specifically, suppose we have n vectors represented as C = {vk v(sk) , ke = 1, . [sent-98, score-0.034]
</p><p>56 , n} cso rnevperertesedn tfredom a sse Cnt =enc {evs according kto = (7). [sent-101, score-0.027]
</p><p>57 ro,mn} C, we ecrotnedst frurocmt an n n ssi amc-ilarity gm taot(r 7ix) M, min Cw,h wiceh c oenaschtr ecltem anen nt × Mkk′ misa symmetric nonnegative distance measure between vk and vk′ . [sent-105, score-0.061]
</p><p>58 The matrix M can be regarded as the adjacency matrix of a graph G with node set N and edge set E, wtrihxer oef N a g girsa 1p-hto G-1 correspondent to tahned set gCe, asentd EE, corresponds 1t-ot ot-h1e non-zero deentnrtie tso tihn eM se. [sent-107, score-0.258]
</p><p>59 TC ,he a nor-  ,  ,  malized Laplacian matrix of M is L  − D−21MD−21,  I  where D is a diagonal matrix with entries Xn  Dkk′  = δkk′XMkj. [sent-108, score-0.216]
</p><p>60 In our implementation, we find the q eigenvectors of the normalized Laplacian matrix of M of the smallest eigenvalues. [sent-110, score-0.154]
</p><p>61 mWa treix p Q, ahnesde c eliugsetenrv tehcteo row vectors to q clusters. [sent-112, score-0.037]
</p><p>62 Each cluster correspond to a data-driven DA Aj , and the n sentences are classified according to the cluster they belong to. [sent-113, score-0.056]
</p><p>63 In order to use the DRs in a PST as a knowledge source for DA detection, we essentially need to model the relationship between the random DA and the random DR. [sent-114, score-0.028]
</p><p>64 Given a text corpus, let nij be the accumulated count that Ri occurs in a sentence  labeled as Aj. [sent-116, score-0.095]
</p><p>65 (11)  From (10) and (11), a matrix Φ can be constructed by Φij = (1 − ǫi)γij. [sent-122, score-0.108]
</p><p>66 We call Φ the derivationrule dialogue-act (DR-DA) matrix, in which each row corresponds to a derivation rule and each column corresponds to a dialogue act. [sent-123, score-0.743]
</p><p>67 4 Distance Measure In our system, the lexical score g(A, W) in (5) is further broken into two terms g(A, W)  ≈  gR(A, s)gN(A, W)  (12)  where gR(A, s) is called the DR score and gN(A, W) is called the named entity score. [sent-125, score-0.249]
</p><p>68 Note that s denotes the sentence after text processing. [sent-126, score-0.034]
</p><p>69 For the named entity score, we use the approximation gN(A,W) = (14)  Yν(A,αk) Yk  606  NEC/SCName entities/Words  TableGE1r:nCSeEpditoxnyagmpleTAWsahnoeiflpnca konasgm,-TFeBa,doiyHrpet, nSlit,uoKyna-cMolhasoieunsgL(NaEkeC)and semantic classes (SC)  where  αk  is the kth named entity in W. [sent-128, score-0.17]
</p><p>70 Note that  ν(A, α) is estimated from a training corpus by relative frequencies. [sent-129, score-0.029]
</p><p>71 The first type of data, called A-data, is a travel information data set harvested from the databases available on the web, e. [sent-133, score-0.036]
</p><p>72 The second type of data, called Q-data, is the edited transcription of a speech data set simulating human-computer dialogues in a lab environment. [sent-137, score-0.202]
</p><p>73 It consists of 144 dialogues with 1, 586 utterances. [sent-141, score-0.036]
</p><p>74 From the Q-data, 28 named entity classes  and 796 derivation rules were obtained from the Sparser. [sent-142, score-0.306]
</p><p>75 1 Experimental Conditions A Mandarin speech recognition engine was realized using the HTK (Young et al. [sent-145, score-0.098]
</p><p>76 For speech features, 39 dimensions were used, including 12 dimensions of mel-frequency cepstral coefficients (MFCCs), one dimension of log energy, and their delta and acceleration features. [sent-147, score-0.111]
</p><p>77 , EN, MA, OU) based  number of DA types373839 detection accuracy82. [sent-150, score-0.124]
</p><p>78 The average word accuracy ofthe ASR module is 86. [sent-156, score-0.111]
</p><p>79 As shown in Table 2, one can see that 38 DA types achieve the best performance for the proposed detection model. [sent-160, score-0.124]
</p><p>80 Note that some exemplar DAs are shown in Figure 2. [sent-162, score-0.066]
</p><p>81 In this table, the third column (ASR) represents the results of the experiment using the ASR transcripts directly. [sent-166, score-0.082]
</p><p>82 The fourth column (REF) uses the reference transcripts, so it represents the case with perfect ASR. [sent-167, score-0.037]
</p><p>83 The first (40%-sim) and second (60%sim) column represents the simulation where 40% and 60% of the words in the reference transcripts are retained, respectively. [sent-168, score-0.082]
</p><p>84 There are five sets of ex-  periments summarized in this table. [sent-169, score-0.027]
</p><p>85 For the baseline, each keyword corresponds to a coordinate in the vector representation for a sentence. [sent-170, score-0.088]
</p><p>86 The results are shown in the first row (baseline). [sent-171, score-0.037]
</p><p>87 In the second set ofexperiments (NEC), the keywords are replaced by their NEC. [sent-172, score-0.126]
</p><p>88 In the third set of experiments (PST), the PST representation for a sentence is used. [sent-173, score-0.061]
</p><p>89 In the fourth set of experiments (DR), the derivation rule representation of a sentence is used. [sent-174, score-0.287]
</p><p>90 Finally, the entropy-normalized DR-DA matrix is used to represent sentences, and the results are shown in the last row (DR-DA). [sent-175, score-0.145]
</p><p>91 9 Table 4: Evaluation on different weighted product fusion the DR and DR-DA representations also lead to significant improvements, achieving 81. [sent-194, score-0.035]
</p><p>92 Using DR-DA, however, suffers from performance degradation when the keywords are randomly discarded. [sent-198, score-0.071]
</p><p>93 Table 4 shows the results that history information will effect on the DA detection,  because it was estimated by the dialogue turns that captured the user behaviors. [sent-201, score-0.63]
</p><p>94 6  Conclusions  In this paper, a noise-robust dialogue act detection using named entity classes, partial sentence trees, derivation rules, and entropy-based dialogue actderivation rule matrix is investigated. [sent-202, score-1.69]
</p><p>95 Data-driven dialogue acts are created by the spectral clustering algorithm, which is applied on the vectors of sentences represented by the derivation rules. [sent-203, score-0.738]
</p><p>96 Our spoken dialogue system benefits when the proposed components are integrated incrementally. [sent-204, score-0.509]
</p><p>97 Recovery from false rejection using statistical partial pattern trees for sentence verification. [sent-288, score-0.164]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dialogue', 0.443), ('asr', 0.349), ('pst', 0.268), ('da', 0.262), ('drs', 0.213), ('hori', 0.182), ('derivation', 0.18), ('act', 0.134), ('detection', 0.124), ('dr', 0.122), ('sds', 0.122), ('ut', 0.12), ('module', 0.111), ('matrix', 0.108), ('nec', 0.107), ('history', 0.094), ('partial', 0.093), ('kashioka', 0.091), ('misu', 0.091), ('ohtake', 0.091), ('slu', 0.091), ('ht', 0.083), ('keywords', 0.071), ('spoken', 0.066), ('exemplar', 0.066), ('aj', 0.065), ('user', 0.064), ('gr', 0.063), ('dialog', 0.063), ('keyword', 0.061), ('aarg', 0.061), ('htk', 0.061), ('luxburg', 0.061), ('nij', 0.061), ('vk', 0.061), ('spectral', 0.061), ('gn', 0.061), ('von', 0.059), ('pr', 0.059), ('speech', 0.055), ('acts', 0.054), ('larsen', 0.054), ('dm', 0.052), ('laplacian', 0.049), ('eigenvectors', 0.046), ('score', 0.046), ('entity', 0.046), ('rule', 0.046), ('transcripts', 0.045), ('simulating', 0.044), ('ref', 0.044), ('recognition', 0.043), ('oef', 0.042), ('rules', 0.041), ('mandarin', 0.041), ('taiwan', 0.041), ('kk', 0.041), ('named', 0.039), ('levy', 0.038), ('row', 0.037), ('column', 0.037), ('trees', 0.037), ('nnp', 0.037), ('ij', 0.036), ('dialogues', 0.036), ('called', 0.036), ('arrows', 0.035), ('fusion', 0.035), ('rb', 0.035), ('sentence', 0.034), ('ri', 0.034), ('ke', 0.034), ('young', 0.033), ('management', 0.032), ('fraser', 0.032), ('lab', 0.031), ('das', 0.03), ('utterance', 0.03), ('estimated', 0.029), ('gmai', 0.029), ('replaced', 0.028), ('essentially', 0.028), ('cluster', 0.028), ('dimensions', 0.028), ('wt', 0.028), ('robust', 0.027), ('representation', 0.027), ('ta', 0.027), ('fthoer', 0.027), ('kto', 0.027), ('bellegarda', 0.027), ('twhoerd', 0.027), ('periments', 0.027), ('enc', 0.027), ('tehnet', 0.027), ('ifno', 0.027), ('esu', 0.027), ('gilbert', 0.027), ('ofexperiments', 0.027), ('nds', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="272-tfidf-1" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>Author: Wei-Bin Liang ; Chung-Hsien Wu ; Chia-Ping Chen</p><p>Abstract: In this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. First, partial sentence trees are proposed to represent a speech recognition output sentence. Semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. The constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. The proposed approach is implemented and evaluated in a Mandarin spoken dialogue system for tour-guiding service. Combined with scores derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84.3% detection accuracy, an absolute improvement of 34.7% over the baseline of the semantic slot-based method with 49.6% detection accuracy.</p><p>2 0.40452284 <a title="272-tfidf-2" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>3 0.30777049 <a title="272-tfidf-3" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>Author: Paul Piwek ; Svetlana Stoyanchev</p><p>Abstract: This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.</p><p>4 0.30493084 <a title="272-tfidf-4" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>5 0.20721497 <a title="272-tfidf-5" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>Author: Staffan Larsson ; Alexander Berman ; Jessica Villing</p><p>Abstract: Alexander Berman Jessica Villing Talkamatic AB University of Gothenburg Sweden Sweden alex@ t alkamat i . se c jessi ca@ l ing .gu . s e 2 In-vehicle dialogue systems This paper describes Dico II+, an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogueand a “speech cursor” which enables menu navigation as well as browsing long list using haptic input and spoken output.</p><p>6 0.159141 <a title="272-tfidf-6" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>7 0.15413919 <a title="272-tfidf-7" href="./acl-2011-N-Best_Rescoring_Based_on_Pitch-accent_Patterns.html">228 acl-2011-N-Best Rescoring Based on Pitch-accent Patterns</a></p>
<p>8 0.14039846 <a title="272-tfidf-8" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>9 0.12047806 <a title="272-tfidf-9" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>10 0.11837313 <a title="272-tfidf-10" href="./acl-2011-Turn-Taking_Cues_in_a_Human_Tutoring_Corpus.html">312 acl-2011-Turn-Taking Cues in a Human Tutoring Corpus</a></p>
<p>11 0.10240337 <a title="272-tfidf-11" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>12 0.10199284 <a title="272-tfidf-12" href="./acl-2011-Question_Detection_in_Spoken_Conversations_Using_Textual_Conversations.html">257 acl-2011-Question Detection in Spoken Conversations Using Textual Conversations</a></p>
<p>13 0.092564344 <a title="272-tfidf-13" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>14 0.086844057 <a title="272-tfidf-14" href="./acl-2011-Entrainment_in_Speech_Preceding_Backchannels..html">118 acl-2011-Entrainment in Speech Preceding Backchannels.</a></p>
<p>15 0.08322449 <a title="272-tfidf-15" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>16 0.077184811 <a title="272-tfidf-16" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>17 0.074526161 <a title="272-tfidf-17" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>18 0.066908024 <a title="272-tfidf-18" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>19 0.059715152 <a title="272-tfidf-19" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>20 0.057703186 <a title="272-tfidf-20" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, 0.028), (2, -0.029), (3, -0.007), (4, -0.342), (5, 0.346), (6, -0.141), (7, -0.038), (8, -0.03), (9, 0.024), (10, 0.133), (11, 0.015), (12, 0.098), (13, -0.004), (14, 0.07), (15, 0.001), (16, 0.036), (17, 0.034), (18, -0.002), (19, 0.046), (20, -0.059), (21, -0.048), (22, 0.092), (23, -0.061), (24, 0.019), (25, -0.055), (26, -0.046), (27, 0.011), (28, 0.022), (29, 0.004), (30, -0.012), (31, -0.053), (32, -0.032), (33, -0.0), (34, 0.012), (35, -0.043), (36, 0.018), (37, -0.064), (38, -0.047), (39, -0.0), (40, -0.021), (41, -0.022), (42, -0.022), (43, 0.069), (44, 0.033), (45, -0.005), (46, 0.011), (47, -0.001), (48, 0.096), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94894069 <a title="272-lsi-1" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>Author: Wei-Bin Liang ; Chung-Hsien Wu ; Chia-Ping Chen</p><p>Abstract: In this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. First, partial sentence trees are proposed to represent a speech recognition output sentence. Semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. The constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. The proposed approach is implemented and evaluated in a Mandarin spoken dialogue system for tour-guiding service. Combined with scores derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84.3% detection accuracy, an absolute improvement of 34.7% over the baseline of the semantic slot-based method with 49.6% detection accuracy.</p><p>2 0.92360502 <a title="272-lsi-2" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>3 0.91424817 <a title="272-lsi-3" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>4 0.89400548 <a title="272-lsi-4" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>Author: Staffan Larsson ; Alexander Berman ; Jessica Villing</p><p>Abstract: Alexander Berman Jessica Villing Talkamatic AB University of Gothenburg Sweden Sweden alex@ t alkamat i . se c jessi ca@ l ing .gu . s e 2 In-vehicle dialogue systems This paper describes Dico II+, an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogueand a “speech cursor” which enables menu navigation as well as browsing long list using haptic input and spoken output.</p><p>5 0.88316965 <a title="272-lsi-5" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>Author: Paul Piwek ; Svetlana Stoyanchev</p><p>Abstract: This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.</p><p>6 0.68575186 <a title="272-lsi-6" href="./acl-2011-Turn-Taking_Cues_in_a_Human_Tutoring_Corpus.html">312 acl-2011-Turn-Taking Cues in a Human Tutoring Corpus</a></p>
<p>7 0.62998503 <a title="272-lsi-7" href="./acl-2011-Entrainment_in_Speech_Preceding_Backchannels..html">118 acl-2011-Entrainment in Speech Preceding Backchannels.</a></p>
<p>8 0.59710634 <a title="272-lsi-8" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>9 0.46580219 <a title="272-lsi-9" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>10 0.44215891 <a title="272-lsi-10" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>11 0.43424079 <a title="272-lsi-11" href="./acl-2011-Question_Detection_in_Spoken_Conversations_Using_Textual_Conversations.html">257 acl-2011-Question Detection in Spoken Conversations Using Textual Conversations</a></p>
<p>12 0.37488294 <a title="272-lsi-12" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>13 0.34809333 <a title="272-lsi-13" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>14 0.30806324 <a title="272-lsi-14" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>15 0.29702026 <a title="272-lsi-15" href="./acl-2011-N-Best_Rescoring_Based_on_Pitch-accent_Patterns.html">228 acl-2011-N-Best Rescoring Based on Pitch-accent Patterns</a></p>
<p>16 0.28992641 <a title="272-lsi-16" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>17 0.2666961 <a title="272-lsi-17" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>18 0.26626125 <a title="272-lsi-18" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>19 0.26307276 <a title="272-lsi-19" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>20 0.25675523 <a title="272-lsi-20" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.043), (26, 0.014), (37, 0.053), (39, 0.026), (41, 0.046), (55, 0.022), (59, 0.024), (72, 0.02), (91, 0.011), (96, 0.653)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99909115 <a title="272-lda-1" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>Author: Daniel Emilio Beck</p><p>Abstract: In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</p><p>2 0.99905324 <a title="272-lda-2" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>Author: Anselmo Penas ; Alvaro Rodrigo</p><p>Abstract: There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.</p><p>3 0.99833888 <a title="272-lda-3" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>Author: Maoxi Li ; Chengqing Zong ; Hwee Tou Ng</p><p>Abstract: Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 1</p><p>4 0.99773991 <a title="272-lda-4" href="./acl-2011-Improving_On-line_Handwritten_Recognition_using_Translation_Models_in_Multimodal_Interactive_Machine_Translation.html">168 acl-2011-Improving On-line Handwritten Recognition using Translation Models in Multimodal Interactive Machine Translation</a></p>
<p>Author: Vicent Alabau ; Alberto Sanchis ; Francisco Casacuberta</p><p>Abstract: In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work.</p><p>5 0.9974618 <a title="272-lda-5" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>6 0.99742484 <a title="272-lda-6" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>same-paper 7 0.99741924 <a title="272-lda-7" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>8 0.99221683 <a title="272-lda-8" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>9 0.98788685 <a title="272-lda-9" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>10 0.97770053 <a title="272-lda-10" href="./acl-2011-An_Interactive_Machine_Translation_System_with_Online_Learning.html">41 acl-2011-An Interactive Machine Translation System with Online Learning</a></p>
<p>11 0.9738211 <a title="272-lda-11" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>12 0.96684343 <a title="272-lda-12" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>13 0.95787686 <a title="272-lda-13" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>14 0.95406646 <a title="272-lda-14" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>15 0.95162642 <a title="272-lda-15" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>16 0.95025545 <a title="272-lda-16" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>17 0.94785929 <a title="272-lda-17" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>18 0.94623542 <a title="272-lda-18" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>19 0.94607961 <a title="272-lda-19" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>20 0.94178581 <a title="272-lda-20" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
