<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-176" href="#">acl2011-176</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</h1>
<br/><p>Source: <a title="acl-2011-176-pdf" href="http://aclweb.org/anthology//P/P11/P11-1106.pdf">pdf</a></p><p>Author: Roger Levy</p><p>Abstract: A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.</p><p>Reference: <a title="acl-2011-176-reference" href="../acl2011_reference/acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results Roger Levy Department of Linguistics University of California at San Diego 9500 Gilman Drive # 0108 La Jolla, CA 92093-0108 rlevy@uc sd . [sent-1, score-0.41]
</p><p>2 edu  Abstract A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. [sent-2, score-0.547]
</p><p>3 Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. [sent-3, score-0.376]
</p><p>4 We also present a behavioral experiment confirming the key empirical predictions of the theory. [sent-5, score-0.081]
</p><p>5 If this is the case, then subsequent input strongly disconfirming this “hallucinated” garden-path analysis might be expected to induce the same effects as seen in classic cases of garden-path disambiguation traditionally studied in the psycholinguistic literature. [sent-12, score-0.291]
</p><p>6 Section 2 reviews surprisal theory and how it accounts for traditional gardenpath effects. [sent-14, score-0.44]
</p><p>7 Section 4 defines and estimates parameters for a model instantiating the general theory, and describes the predictions of the model for the experiment described in Section 3 (along with the inference procedures required to determine those predictions). [sent-18, score-0.081]
</p><p>8 i−1,Ctxt) (In the rest of this paper, we consider isolatedsentence comprehension and ignore Ctxt. [sent-27, score-0.157]
</p><p>9 ) The theory derives empirical support not only from controlled experiments manipulating grammatical context but also from broad-coverage studies of reading times for naturalistic text (Demberg and Keller, 2008; Boston et al. [sent-28, score-0.288]
</p><p>10 , 2009), including demonstration that the shape of the relationship between word probability and reading time is indeed log-linear (Smith and Levy, 2008). [sent-30, score-0.128]
</p><p>11 The most famous example is (1) below (Bever, 1970): (1) The horse raced past the barn fell. [sent-32, score-0.305]
</p><p>12 Letting Tj range over the possible incremental syntactic analyses of words w1. [sent-34, score-0.134]
</p><p>13 6 preceding fell, under surprisal the conditional probability of the disambiguating continuation fell can be approximated as P(fell|w1. [sent-37, score-0.539]
</p><p>14 6) Xj (I)  For all possible predisambiguation analyses Tj, either the analysis is disfavored by the context (P(Tj |w1. [sent-46, score-0.144]
</p><p>15 bSiginucaet every srudm umnlainkedl yin tPhe(f marginalization of Equation (I) has a very small term in it, the total marginal probability is thus small and the surprisal is high. [sent-53, score-0.36]
</p><p>16 Hale (2001) demonstrated that surprisal thus predicts strong garden-pathing effects in the classic sentence The horse raced past the barn fell on basis of the overall rarity of reduced relative clauses alone. [sent-54, score-0.955]
</p><p>17 1 3  Garden-pathing and input uncertainty  We now move on to cases where garden-pathing can apparently be blocked by only small changes to the surface input, which we will take as a starting point for developing an integrated theory of uncertaininput inference and surprisal. [sent-57, score-0.269]
</p><p>18 S  S  NP DT  VP  NN  VBD  The horse  raced  NP  PP IN  . [sent-59, score-0.227]
</p><p>19 NP  past DT  DT  NN  VP RRC  The horse  S  NN  VP  the barn  VBN  (a) MV interpretation  PP  raced INNP past DT  NN  the barn  (b) RR interpretation Figure 1: Classic garden pathing (2) While Mary was mending the socks fell off her lap. [sent-61, score-0.854]
</p><p>20 The main-clause verb fell disambiguates, ruling out the initially favored NP analysis. [sent-63, score-0.206]
</p><p>21 It has been known since Frazier and Rayner (1982) that this effect of garden-path disambiguation can be measured in reading times on the main-clause verb (see also Mitchell, 1987; Ferreira and Henderson, 1993; Adams et al. [sent-64, score-0.201]
</p><p>22 Small changes to the context can have huge effects on comprehenders’ initial interpretations, however. [sent-68, score-0.059]
</p><p>23 Understanding sentences easier, and reading times are reliably lower when (2002) summarized the cinctly:  like (3) is intuitively much at the disambiguating verb compared with (2). [sent-70, score-0.247]
</p><p>24 Fodor power of this effect suc-  [w]ith a comma after mending, there would be no syntactic garden path left to be studied. [sent-71, score-0.185]
</p><p>25 2 When uncertainty about surface input is introduced, however— due to visual noise, imperfect memory representations, and/or beliefs about possible speaker error— analyses come into play in which some parts of the true string are treated as if they were absent. [sent-73, score-0.495]
</p><p>26 In particular, because the two sentences are perceptual neighbors, the pre-disambiguation garden-path analysis of (2) may be entertained in (3). [sent-74, score-0.135]
</p><p>27 We can get a tighter handle on the effect of input uncertainty by extending Levy (2008b)’s analysis of the expected beliefs of a comprehender about the sequence of words constituting an input sentence to joint inference over both sentence identity and sen-  tence structure. [sent-75, score-0.892]
</p><p>28 ]); none involve true direct objects of the type in (3). [sent-90, score-0.063]
</p><p>29 3This assumption is effectively saying that noise processes are syntax-insensitive, which is clearly sensible for environmental noise but would need to be relaxed for some types of speaker error. [sent-91, score-0.247]
</p><p>30 At the same time, the uncertain-input theory predicts that if we manipulate the balance of prior grammatical probabilities PC(T, w) strongly enough (term (i) in Equation (II)), it may shift the comprehender’s beliefs toward a garden-path interpretation. [sent-95, score-0.43]
</p><p>31 This observation sets the stage for our experimental manipulation, illustrated below: (4) As the soldiers marched, toward the tank lurched an injured enemy combatant. [sent-96, score-0.56]
</p><p>32 First, there has been LOCATIVE INVERSION (Bolinger, 1971; Bresnan, 1994) in the main clause: a locative PP has been fronted before the verb, and the subject NP is realized postverbally. [sent-98, score-0.145]
</p><p>33 Locative inversion is a low-frequency construction, hence it is crucially disfavored by the comprehender’s prior over possible grammatical structures. [sent-99, score-0.199]
</p><p>34 Second, the subordinate-clause verb is no longer transitive, as in (3); instead it is intran-  sitive but could itself take the main-clause fronted PP as a dependent. [sent-100, score-0.141]
</p><p>35 If comprehenders do indeed seriously entertain such interpretations, then we should be able to find the empirical hallmarks (e. [sent-102, score-0.177]
</p><p>36 , elevated reading times) of garden-path disambiguation at the mainclause verb lurched, which is incompatible with the “hallucinated” garden-path interpretation. [sent-104, score-0.201]
</p><p>37 Empirically, however, it is important to disentangle these empirical hallmarks of garden-path disambiguation from more general disruption that may be induced by encountering locative inversion itself. [sent-105, score-0.121]
</p><p>38 We address this issue by introducing a control condition in which a postverbal PP is placed within the subor-  dinate clause: (5) As the soldiers marched into the bunker, toward the tank lurched an injured enemy combatant. [sent-106, score-0.719]
</p><p>39 Finally, to ensure that sentence length itself does not create a confound driving any observed processing-time difference, we cross presence/absence of the subordinate-clause PP with inversion in the main clause: (6) a. [sent-108, score-0.094]
</p><p>40 As the soldiers marched, the tank lurched toward an injured enemy combatant. [sent-109, score-0.56]
</p><p>41 nt o[U tnhien beurtnekde,−r, PtPhe] tank lurched toward an injured enemy combatant. [sent-112, score-0.507]
</p><p>42 [Uninverted,+PP]  4  Model instantiation and predictions  To determine  the predictions  of our uncertain-  input/surprisal model for the above sentence types, we extracted a small grammar from the parsed TOP → S . [sent-113, score-0.212]
</p><p>43 , 1994), covering sentence-initial clause and locative-inversion non-terminal  subordinate  constructions. [sent-138, score-0.09]
</p><p>44 The handful of true locative PPs (5 in total) are all parentheticals intervening between  ,  the verb and a complement strongly selected by the verb (e. [sent-150, score-0.286]
</p><p>45 1059 certain input is represented as a weighted finite-state automaton (WFSA), allowing us to represent the incremental inferences of the comprehender through intersection of the input WFSA with the PCFG above (Bar-Hillel et al. [sent-153, score-0.727]
</p><p>46 1 Uncertain-input representations Levy (2008a) introduced the LEVENSHTEINDISTANCE KERNEL as a model of the average effect of noise in uncertain-input probabilistic sentence comprehension; this corresponds to term (ii) in our Equation (II). [sent-156, score-0.155]
</p><p>47 The resulting weighted finite-state representation of noisy input for a true sentence prefix w∗ = w1. [sent-160, score-0.238]
</p><p>48 j is a j + 1-state automaton with arcs as follows: • For each i∈ 1, . [sent-163, score-0.09]
</p><p>49 0 0 Figure 2: Noisy WFSA for partial input it hit. [sent-179, score-0.125]
</p><p>50 with lexicon {it,hit,him}, noise parameter γ=1 the lexicon, with zero cost (value 1 in the real semiring); • State j is a zero-cost final state; no other states are efin jal i. [sent-182, score-0.143]
</p><p>51 In order to ensure that the distribution over already-seen input is proper, we normalize the costs on outgoing arcs from all states but j. [sent-184, score-0.178]
</p><p>52 6 Figure 2 gives an example of a simple WFSA representation for a short partial input with a small lexicon. [sent-185, score-0.125]
</p><p>53 2 Inference Computing the surprisal incurred by the disambiguating element given an uncertain-input representation of the sentence involves a standard application of the definition of conditional probability (Hale, 2001):  logP(I1. [sent-187, score-0.493]
</p><p>54 k) is equal to the partition function of the intersection of this WFSA with the PCFG given in Table 1. [sent-205, score-0.127]
</p><p>55 7 PCFGs are a special class of weighted context-free grammars (WCFGs), 6If a state’s total unnormalized cost of insertion arcs is α and that of deletion and insertion arcs is β, its normalizing constant is β. [sent-206, score-0.144]
</p><p>56 7Using the WFSA representation of average noise effects here actually involves one simplifying assumption, that the av1060 which are closed under intersection with WFSAs; a constructive procedure exists for finding the intersection (Bar-Hillel et al. [sent-208, score-0.32]
</p><p>57 After completion of this bottom-up parse, every rule  that will have non-zero probability in the intersection PCFG will be identifiable with a set of entries in the chart, but not all entries in this chart will have non-zero probability, since some are not connected to the root. [sent-214, score-0.078]
</p><p>58 We can then include in the intersection grammar only those rules from the classic construction that can be identified with a set of surviving entries in the final parse chart. [sent-216, score-0.126]
</p><p>59 8 The partition functions for each category in this intersection grammar can then be computed; we used a fixed-point method preceded by a topological sort on the grammar’s ruleset, as described by Nederhof and Satta (2008). [sent-217, score-0.127]
</p><p>60 To obtain the surprisal of the input deriving from a word wi in its context, we can thus com-  hlogPC(Ii|I11. [sent-218, score-0.532]
</p><p>61 i−1)i, is well ap-  erage surprisal of Ii, or EPT proximated by the log of the ratio of the expected probabilities of the noisy inputs I1. [sent-221, score-0.4]
</p><p>62 This simplifying assumption has the advantage of bypassing commitment to a specific representation of perceptual input and should be justifiable for rea-  sonable noise functions, but the issue is worth further scrutiny. [sent-234, score-0.365]
</p><p>63 Noise level γ (high=noisy)  Figure 3: Model predictions for (4)–(6) pute the partition functions for noisy inputs I1. [sent-236, score-0.17]
</p><p>64 3  Predictions  The noise level γ is a free parameter in this model, so  we plot model predictions—the expected surprisal of input from the main-clause verb for each variant of the target sentence in (4)–(6)—over a wide range of its possible values (Figure 3). [sent-250, score-0.713]
</p><p>65 The far left of the graph asymptotes toward the predictions ofclean surprisal, or noise-free input. [sent-251, score-0.167]
</p><p>66 With little to no input uncertainty, the presence of the comma rules out the garden-path analysis of the fronted PP toward the tank, and the surprisal at the main-clause verb is the same across condition (here reflecting only the uncertainty of verb identity for this small grammar). [sent-252, score-1.059]
</p><p>67 As input uncertainty increases, however, surprisal in the [Inverted, −PP] condition increases, reflecting hthee [ stronger , b −eliPefP given preceding caosentse,x rte iflne an input-unfaithful interpretation. [sent-253, score-0.602]
</p><p>68 Forty monolingual native-English speaker  participants read twenty-four sentence quadruplets (“items”) on the pattern of (4)–(6), with a Latinsquare design so that each participant saw an equal 1061 InvertedUninverted -PP0. [sent-255, score-0.05]
</p><p>69 92 Table 2: Question-answering accuracy number of sentences in each condition and saw each item only once. [sent-259, score-0.053]
</p><p>70 Every sentence was followed by a yes/no comprehension question (e. [sent-262, score-0.207]
</p><p>71 , Did the tank lurch toward an injured enemy combatant? [sent-264, score-0.418]
</p><p>72 The same pattern persists and remains significant through to the end of the sentence, indicating considerable processing disruption, and is also observed in question-answering accuracies for experimental sentences, which are superadditively lowest in the [Inverted, −PP] condition (Table 2). [sent-271, score-0.053]
</p><p>73 nTvheer eind,fla −tePdP reading ntim (Teasb lfeo r2 . [sent-272, score-0.128]
</p><p>74 the [Inverted,  −PP] condition beginning at the main-clause v−ePrbP ]co cnofnirdmit tnhe predictions tof t ethe m uaninc-ecrlatauinseinput/surprisal theory. [sent-273, score-0.134]
</p><p>75 Crucially, the input that would on our theory induce the comprehender to question the comma (the fronted main-clause PP)  As  htes oldeirs marched(),  bi unnotk ethre,  otward  htet ank  ulrched  otward  an  enemy  combaatn. [sent-274, score-0.758]
</p><p>76 t  Figure 4: Average reading times for each part of the sentence, broken down by experimental condition is not seen until after the comma is no longer visible (and presumably has been integrated into beliefs about syntactic analysis on veridical-input theories). [sent-275, score-0.432]
</p><p>77 This empirical result is hence difficult to accommodate in accounts which do not share our theory’s cru-  cial property that comprehenders can revise their belief in previous input on the basis of current input. [sent-276, score-0.389]
</p><p>78 6  Conclusion  Language is redundant: the content of one part of a sentence carries predictive value both for what will precede and what will follow it. [sent-277, score-0.05]
</p><p>79 If subsequent input strongly disconfirms this incorrect in1062 terpretation, we should see behavioral signatures of classic garden-path disambiguation. [sent-281, score-0.173]
</p><p>80 Within the theory, the size of this “hallucinated” garden-path effect is indexed by the surprisal value under uncertain input, marginalizing over the actual sentence observed. [sent-282, score-0.487]
</p><p>81 As predicted by the theory, reading times at the word disambiguating the “hallucinated” garden-path were inflated relative to control conditions. [sent-284, score-0.174]
</p><p>82 These results contribute to the theory of uncertain-input effects in online sentence processing by suggesting that comprehenders may be in-  duced not only to entertain but to adopt relatively strong beliefs in grammatical analyses that require modification of the surface input itself. [sent-285, score-0.771]
</p><p>83 Our results also bring a new degree of nuance to surprisal theory, demonstrating that perceptual neighbors of true preceding input may need to be taken into account in order to estimate how surprising a comprehender will find subsequent input to be. [sent-286, score-1.092]
</p><p>84 Beyond the domain of psycholinguistics, the methods employed here might also be usefully applied to practical problems such as parsing of degraded or fragmentary sentence input, allowing joint constraint derived from grammar and available input to fill in gaps (Lang, 1988). [sent-287, score-0.175]
</p><p>85 Of course, practical applications of this sort would raise challenges of their own, such as extending the grammar to broader coverage, which is delicate here since the surface input places a weaker check on overgeneration from the grammar than in traditional probabilistic parsing. [sent-288, score-0.125]
</p><p>86 Larger grammars also impose a technical burden since parsing uncertain input is in practice more computationally intensive than parsing clean input, raising the question of what approximate-inference algorithms might be well-suited to processing un-  certain input with grammatical knowledge. [sent-289, score-0.407]
</p><p>87 Answers to this question might in turn be of interest for sentence processing, since the exhaustive-parsing idealization employed here is not psychologically plausible. [sent-290, score-0.05]
</p><p>88 It seems likely that human comprehension involves approximate inference with severely limited memory that is nonetheless highly optimized to recover something close to the intended meaning of an utterance, even when the recovered meaning is not completely faithful to the input itself. [sent-291, score-0.325]
</p><p>89 Iam grateful to Natalie Katz and Henry Lu for assistance in preparing materials and collecting data for the self-paced reading experiment described here. [sent-294, score-0.128]
</p><p>90 Mixed-effects modeling with crossed random effects for subjects and items. [sent-312, score-0.059]
</p><p>91 Parsing costs as predictors of reading difficulty: An evaluation using the Pots-  dam sentence corpus. [sent-339, score-0.178]
</p><p>92 Effects of subsequent sentence context in auditory word recognition: Temporal and linguistic constraints. [sent-359, score-0.05]
</p><p>93 Surprisal-based comparison between a symbolic and a connectionist model of sentence processing. [sent-380, score-0.05]
</p><p>94 Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences. [sent-389, score-0.05]
</p><p>95 Commas and spaces: Effects of punctuation on eye movements and sentence parsing. [sent-408, score-0.097]
</p><p>96 A noisy-channel model of rational human sentence comprehension under uncertain input. [sent-435, score-0.33]
</p><p>97 Eye movement evidence that readers maintain and act on uncertainty about past linguistic input. [sent-447, score-0.064]
</p><p>98 An evaluation of subjectpaced reading tasks and other methods for investigating immediate processes in reading. [sent-460, score-0.165]
</p><p>99 A Bayesian model predicts human parse preference and reading time in sentence processing. [sent-481, score-0.178]
</p><p>100 Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. [sent-500, score-0.137]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('surprisal', 0.36), ('comprehender', 0.284), ('comprehenders', 0.177), ('wfsa', 0.177), ('levy', 0.177), ('tank', 0.16), ('comprehension', 0.157), ('beliefs', 0.144), ('raced', 0.141), ('perceptual', 0.135), ('fell', 0.133), ('hale', 0.13), ('reading', 0.128), ('vp', 0.127), ('input', 0.125), ('pp', 0.111), ('comma', 0.107), ('hallucinated', 0.106), ('marched', 0.106), ('vvbbdd', 0.106), ('noise', 0.105), ('enemy', 0.094), ('nederhof', 0.094), ('lurched', 0.089), ('mending', 0.089), ('horse', 0.086), ('toward', 0.086), ('predictions', 0.081), ('grammatical', 0.08), ('theory', 0.08), ('intersection', 0.078), ('incremental', 0.078), ('barn', 0.078), ('garden', 0.078), ('injured', 0.078), ('np', 0.078), ('uncertain', 0.077), ('locative', 0.077), ('satta', 0.074), ('verb', 0.073), ('fodor', 0.071), ('frazier', 0.071), ('fronted', 0.068), ('uncertainty', 0.064), ('cognitive', 0.063), ('true', 0.063), ('interpretation', 0.062), ('effects', 0.059), ('psycholinguistic', 0.059), ('inverted', 0.058), ('analyses', 0.056), ('condition', 0.053), ('ferreira', 0.053), ('predisambiguation', 0.053), ('rayner', 0.053), ('soldiers', 0.053), ('uninverted', 0.053), ('arcs', 0.053), ('sentence', 0.05), ('ii', 0.05), ('identity', 0.05), ('partition', 0.049), ('clause', 0.049), ('classic', 0.048), ('cognition', 0.048), ('eye', 0.047), ('socks', 0.047), ('pnpp', 0.047), ('wi', 0.047), ('tj', 0.047), ('psychology', 0.046), ('rational', 0.046), ('disambiguating', 0.046), ('belief', 0.044), ('inversion', 0.044), ('revise', 0.043), ('pickering', 0.043), ('psycholinguistics', 0.043), ('memory', 0.043), ('subordinate', 0.041), ('inputs', 0.04), ('prior', 0.04), ('pcfg', 0.04), ('mitchell', 0.04), ('jurafsky', 0.039), ('fidelity', 0.039), ('button', 0.039), ('adams', 0.039), ('cost', 0.038), ('processes', 0.037), ('narayanan', 0.037), ('semiring', 0.037), ('incurred', 0.037), ('automaton', 0.037), ('arc', 0.037), ('bever', 0.035), ('christianson', 0.035), ('connine', 0.035), ('demberg', 0.035), ('disfavored', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="176-tfidf-1" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>Author: Roger Levy</p><p>Abstract: A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.</p><p>2 0.10525808 <a title="176-tfidf-2" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>3 0.093844347 <a title="176-tfidf-3" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>Author: Y. Albert Park ; Roger Levy</p><p>Abstract: Automated grammar correction techniques have seen improvement over the years, but there is still much room for increased performance. Current correction techniques mainly focus on identifying and correcting a specific type of error, such as verb form misuse or preposition misuse, which restricts the corrections to a limited scope. We introduce a novel technique, based on a noisy channel model, which can utilize the whole sentence context to determine proper corrections. We show how to use the EM algorithm to learn the parameters of the noise model, using only a data set of erroneous sentences, given the proper language model. This frees us from the burden of acquiring a large corpora of corrected sentences. We also present a cheap and efficient way to provide automated evaluation re- sults for grammar corrections by using BLEU and METEOR, in contrast to the commonly used manual evaluations.</p><p>4 0.088861451 <a title="176-tfidf-4" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>5 0.080684438 <a title="176-tfidf-5" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>6 0.079850271 <a title="176-tfidf-6" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>7 0.069253191 <a title="176-tfidf-7" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>8 0.063336797 <a title="176-tfidf-8" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>9 0.062547065 <a title="176-tfidf-9" href="./acl-2011-Lexicographic_Semirings_for_Exact_Automata_Encoding_of_Sequence_Models.html">210 acl-2011-Lexicographic Semirings for Exact Automata Encoding of Sequence Models</a></p>
<p>10 0.062071253 <a title="176-tfidf-10" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>11 0.062070601 <a title="176-tfidf-11" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>12 0.059287321 <a title="176-tfidf-12" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>13 0.059184007 <a title="176-tfidf-13" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>14 0.056464832 <a title="176-tfidf-14" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>15 0.055807188 <a title="176-tfidf-15" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>16 0.055804934 <a title="176-tfidf-16" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>17 0.052906364 <a title="176-tfidf-17" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>18 0.052681953 <a title="176-tfidf-18" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>19 0.052108083 <a title="176-tfidf-19" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>20 0.051027969 <a title="176-tfidf-20" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.161), (1, -0.034), (2, -0.023), (3, -0.075), (4, -0.043), (5, 0.009), (6, -0.03), (7, -0.013), (8, -0.028), (9, -0.016), (10, -0.047), (11, -0.002), (12, -0.0), (13, 0.069), (14, -0.015), (15, 0.045), (16, -0.013), (17, -0.017), (18, 0.019), (19, 0.001), (20, 0.103), (21, -0.004), (22, -0.013), (23, -0.027), (24, 0.008), (25, -0.03), (26, -0.006), (27, 0.016), (28, -0.039), (29, -0.071), (30, 0.026), (31, 0.005), (32, -0.011), (33, 0.022), (34, 0.008), (35, 0.033), (36, 0.01), (37, -0.056), (38, 0.053), (39, -0.004), (40, 0.006), (41, -0.112), (42, -0.045), (43, -0.013), (44, -0.063), (45, -0.011), (46, -0.068), (47, 0.022), (48, 0.087), (49, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9307248 <a title="176-lsi-1" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>Author: Roger Levy</p><p>Abstract: A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.</p><p>2 0.66239095 <a title="176-lsi-2" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: unkown-abstract</p><p>3 0.65165788 <a title="176-lsi-3" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>Author: Daniel de Kok ; Barbara Plank ; Gertjan van Noord</p><p>Abstract: An attractive property of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking.</p><p>4 0.63439947 <a title="176-lsi-4" href="./acl-2011-Underspecifying_and_Predicting_Voice_for_Surface_Realisation_Ranking.html">317 acl-2011-Underspecifying and Predicting Voice for Surface Realisation Ranking</a></p>
<p>Author: Sina Zarriess ; Aoife Cahill ; Jonas Kuhn</p><p>Abstract: This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice. 1</p><p>5 0.62310141 <a title="176-lsi-5" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>Author: Antske Fokkens</p><p>Abstract: When designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. Moreover, because analyses interact, the choices made by the engineer influence the possibilities available in further grammar development. The order in which phenomena are treated may therefore have a major impact on the resulting grammar. This paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. Iargue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. The idea is illustrated through a comparative study of auxiliary structures in HPSG-based grammars for German and Dutch. Auxiliaries form a central phenomenon of German and Dutch and are likely to influence many components of the grammar. This study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation.</p><p>6 0.62137365 <a title="176-lsi-6" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>7 0.61424953 <a title="176-lsi-7" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>8 0.6060819 <a title="176-lsi-8" href="./acl-2011-Lexicographic_Semirings_for_Exact_Automata_Encoding_of_Sequence_Models.html">210 acl-2011-Lexicographic Semirings for Exact Automata Encoding of Sequence Models</a></p>
<p>9 0.57889938 <a title="176-lsi-9" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>10 0.56660748 <a title="176-lsi-10" href="./acl-2011-Tier-based_Strictly_Local_Constraints_for_Phonology.html">303 acl-2011-Tier-based Strictly Local Constraints for Phonology</a></p>
<p>11 0.56228817 <a title="176-lsi-11" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>12 0.55335712 <a title="176-lsi-12" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>13 0.54448533 <a title="176-lsi-13" href="./acl-2011-Generalized_Interpolation_in_Decision_Tree_LM.html">142 acl-2011-Generalized Interpolation in Decision Tree LM</a></p>
<p>14 0.53638542 <a title="176-lsi-14" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>15 0.53517574 <a title="176-lsi-15" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>16 0.53321195 <a title="176-lsi-16" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<p>17 0.52841264 <a title="176-lsi-17" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>18 0.52396226 <a title="176-lsi-18" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>19 0.51826036 <a title="176-lsi-19" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>20 0.51711172 <a title="176-lsi-20" href="./acl-2011-Subjective_Natural_Language_Problems%3A_Motivations%2C_Applications%2C_Characterizations%2C_and_Implications.html">288 acl-2011-Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.041), (17, 0.062), (26, 0.025), (31, 0.014), (37, 0.073), (39, 0.048), (41, 0.066), (45, 0.261), (53, 0.018), (55, 0.031), (59, 0.046), (72, 0.037), (88, 0.014), (91, 0.047), (96, 0.107), (97, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80248469 <a title="176-lda-1" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>Author: Roger Levy</p><p>Abstract: A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.</p><p>2 0.6489048 <a title="176-lda-2" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.</p><p>3 0.63137507 <a title="176-lda-3" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>Author: Thomas Mueller ; Hinrich Schuetze</p><p>Abstract: We present a class-based language model that clusters rare words of similar morphology together. The model improves the prediction of words after histories containing outof-vocabulary words. The morphological features used are obtained without the use of labeled data. The perplexity improvement compared to a state of the art Kneser-Ney model is 4% overall and 81% on unknown histories.</p><p>4 0.62079859 <a title="176-lda-4" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>Author: Omar F. Zaidan ; Chris Callison-Burch</p><p>Abstract: Naively collecting translations by crowdsourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-toEnglish evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional trans- lators. The total cost is more than an order of magnitude lower than professional translation.</p><p>5 0.55540633 <a title="176-lda-5" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>6 0.55270875 <a title="176-lda-6" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>7 0.55195981 <a title="176-lda-7" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>8 0.55132091 <a title="176-lda-8" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>9 0.54938626 <a title="176-lda-9" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>10 0.54937762 <a title="176-lda-10" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>11 0.54751718 <a title="176-lda-11" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>12 0.54687899 <a title="176-lda-12" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>13 0.54684359 <a title="176-lda-13" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>14 0.54636496 <a title="176-lda-14" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>15 0.54615158 <a title="176-lda-15" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>16 0.54519105 <a title="176-lda-16" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>17 0.54308587 <a title="176-lda-17" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>18 0.54294097 <a title="176-lda-18" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>19 0.54279506 <a title="176-lda-19" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>20 0.54199207 <a title="176-lda-20" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
