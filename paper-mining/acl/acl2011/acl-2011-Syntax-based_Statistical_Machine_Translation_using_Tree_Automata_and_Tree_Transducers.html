<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-290" href="#">acl2011-290</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</h1>
<br/><p>Source: <a title="acl-2011-290-pdf" href="http://aclweb.org/anthology//P/P11/P11-3007.pdf">pdf</a></p><p>Author: Daniel Emilio Beck</p><p>Abstract: In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</p><p>Reference: <a title="acl-2011-290-reference" href="../acl2011_reference/acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers Daniel Emilio Beck Computer Science Department Federal University of S a˜o Carlos daniel beck@ dc . [sent-1, score-0.053]
</p><p>2 br  Abstract In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. [sent-3, score-0.149]
</p><p>3 Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. [sent-4, score-0.038]
</p><p>4 Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. [sent-5, score-0.074]
</p><p>5 These formalisms have important representational properties that makes them well-suited for syntax modeling. [sent-6, score-0.114]
</p><p>6 Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese. [sent-7, score-0.08]
</p><p>7 However, since the advent of PB-SMT by Koehn et al. [sent-10, score-0.037]
</p><p>8 (2003) and Och and Ney (2004), purely statistical MT systems have not achieved considerable improvements. [sent-11, score-0.04]
</p><p>9 This Master’s thesis proposal aims to improve SMT systems by including syntactic information in the first and second steps. [sent-14, score-0.223]
</p><p>10 Thereas  1For the remainder of this proposal, Iwill refer to this step simply translation model. [sent-15, score-0.143]
</p><p>11 36 fore, I plan to investigate two approaches: the Treeto-String (TTS) and the Tree-to-Tree (TTT) models. [sent-16, score-0.042]
</p><p>12 In the former, syntactic information is provided only for the source language while in the latter, it is provided for both source and target languages. [sent-17, score-0.074]
</p><p>13 There are many formal theories to represent syntax in a language, like Context-free Grammars (CFGs), Tree Substitution Grammars (TSGs),  Tree Adjoining Grammars (TAGs) and all its synchronous counterparts. [sent-18, score-0.118]
</p><p>14 In this work, I represent each sentence as a constituent tree and use Tree Automata (TAs) and Tree Transducers (TTs) in the language and translation models. [sent-19, score-0.31]
</p><p>15 Although this work is mainly language independent, proof-of-concept experiments will be executed on the English and Brazilian Portuguese (en-ptBR) language pair. [sent-20, score-0.036]
</p><p>16 Previous research on factored translation for this pair (using morphological information) showed that it improved the results in terms of BLEU (Papineni et al. [sent-21, score-0.196]
</p><p>17 However, even factored translation models have limitations: many languages (and Brazilian Portuguese is not an exception) have relatively loose word order constraints and present longdistance agreements that cannot be efficiently represented by those models. [sent-23, score-0.234]
</p><p>18 Such phenomena motivate the use of more powerful models that take syntactic information into account. [sent-24, score-0.112]
</p><p>19 (2008) uses synchronous CFGs rules and Liu et al. [sent-29, score-0.106]
</p><p>20 (2006) also uses transducer rules but extract them from parse trees in target language instead (the string-to-tree approach - STT). [sent-32, score-0.2]
</p><p>21 All those works also include methods and algorithms for efficient rule extraction since it’s unfeasible to extract all possible rules from a parsed corpus due to exponential cost. [sent-35, score-0.215]
</p><p>22 These works mainly try to incorporate non-syntatic phrases into a syntax-based model: while Liu et al. [sent-37, score-0.074]
</p><p>23 (2008) uses an algorithm to convert leaves in a parse tree to phrases before rule extraction. [sent-39, score-0.304]
</p><p>24 Language models that take into account syntactic aspects have also been an active research subject. [sent-40, score-0.112]
</p><p>25 While works like Post and Gildea (2009) and Vandeghinste (2009) focus solely on language modeling itself, Graham and van Genabith (2010) shows an experiment that incorporates a syntax-based model into an PB-SMT system. [sent-41, score-0.079]
</p><p>26 3  Tree automata and tree transducers  Tree Automata are similar to Finite-state Automata (FSA), except they recognize trees instead of strings (or sequences of words). [sent-42, score-0.632]
</p><p>27 Formally, FSA can only represent Regular Languages and thus, cannot efficiently model several syntactic features, including long-distance agreement. [sent-43, score-0.074]
</p><p>28 TA recognize the so-  called Regular Tree Languages (RTLs), which can represent Context-free Languages (CFLs) since a set of all syntactic trees of a CFL is an RTL (Comon et al. [sent-44, score-0.134]
</p><p>29 Figure 1 shows such an RTL, composed of two trees. [sent-47, score-0.04]
</p><p>30 If we extract an CFG from this RTL it would have the recursive rule S → SS, mw thhiicsh R wTLou iltd w generate an hinef irneictuer sseivte eo rfu syntactic trees. [sent-48, score-0.172]
</p><p>31 In other words, there isn’t an CFG capable to generate only the syntactic trees contained in the RTL shown in Figure 1. [sent-49, score-0.134]
</p><p>32 This feature implies that RTLs have more representational power than CFLs. [sent-50, score-0.06]
</p><p>33 An FST is composed by an input RTL, an output RTL and a set of transformation rules. [sent-52, score-0.04]
</p><p>34 Top-down (T) transducers processes input trees starting from its root and descending through its nodes until it reaches the leaves, in contrast to bottom-up transducers, which do the opposite. [sent-56, score-0.274]
</p><p>35 Figure 2 shows a T rule, where uppercase letters (NP) represent symbols, lowercase letters (q, r, s) represent states and x1 and x2 are variables (formal definitions can be found in Comon et al. [sent-57, score-0.074]
</p><p>36 Default top-down transducers must have only one symbol on the left-hand sides and thus cannot model some syntactic transformations (like local reordering, for example) without relying on copy and delete  operations (Maletti et al. [sent-59, score-0.346]
</p><p>37 Extended topdown transducers allow multiple symbols on lefthand sides, making them more suited for syntax modeling. [sent-61, score-0.331]
</p><p>38 Tree-to-string transducers simply drop the tree structure on righthand sides, which makes them adequate for translation models wihtout syntactic information in one of the languages. [sent-64, score-0.673]
</p><p>39 q NP x1  x2  NP q q −→  x2  x1  Figure 2: Example of a T rule 4  SMT Model  The systems will be implemented using a discriminative, log-linear model (Och and Ney, 2002), using the language and translation models as feature  functions. [sent-66, score-0.279]
</p><p>40 Settings that uses more features besides those two models will also be built. [sent-67, score-0.038]
</p><p>41 (2008) The translation models will be weighted TTs (Graehl et al. [sent-70, score-0.181]
</p><p>42 (2004) but Ialso plan to investigate the approaches used by Liu et al. [sent-75, score-0.042]
</p><p>43 For TTT rule extraction, Iwill use a method similar to the one described in Zhang et al. [sent-78, score-0.098]
</p><p>44 Ialso plan to use language models which takes into account syntactic properties. [sent-80, score-0.154]
</p><p>45 Although most works in syntactic language models uses tree grammars like TSGs and TAGs, these can be simulated by TAs and TTs (Shieber, 2004; Maletti, 2010). [sent-81, score-0.381]
</p><p>46 This property can help the systems implementation because it’s possible to unite language and translation modeling in one TT toolkit. [sent-82, score-0.143]
</p><p>47 5  Methods  In this section, I present the experiments proposed in my thesis and the materials required, along with the metrics used for evaluation. [sent-83, score-0.14]
</p><p>48 38  q S x1 SINV x2  q  x3  S q VP x3 q q −→  x2  S r VP  x1  S x1  x2  x2 s q  r SINV x1  x2  s SINV  x2  −→  x1  q −→  x2  q  x1 x2 −→ x1 Figure 3: Example of a xT rule and its corresponding T rules 5. [sent-85, score-0.14]
</p><p>49 1 Materials To implement and evaluate the techniques described, a parallel corpus with syntactic annotation is required. [sent-86, score-0.074]
</p><p>50 As the focus of this thesis is the English and Brazilian Portuguese language pair, Iwill use the  PesquisaFAPESP corpus2 in my experiments. [sent-87, score-0.088]
</p><p>51 This corpus is composed of 646 scientific papers, originally written in Brazilian Portuguese and manually translated into English, resulting in about 17,000 parallel sentences. [sent-88, score-0.04]
</p><p>52 As for syntactic annotation, I will use the Berkeley parser (Petrov and Klein, 2007) for 2http : / / revi st ape squi s a . [sent-89, score-0.148]
</p><p>53 br  Sq x1  VP  V x2 was  −→  x1 foi x2  Figure 4: Example of a xTS rule (for the en-ptBR language pair)  English and the PALAVRAS parser (Bick, 2000) for Brazilian Portuguese. [sent-91, score-0.098]
</p><p>54 In addition to the corpora and parsers, the following tools will be used: •  •  •  GIZA++3 (Och and Ney, 2000) for lexical alignment Tiburon4 (May and Knight, 2006) for transdTuibcuerro training in both TTS and TTT systems Moses5 (Koehn et al. [sent-92, score-0.043]
</p><p>55 For the TTS systems (one for each translation direction), the training set will be lexically aligned using GIZA++ and for the TTT system, its syntactic trees will be aligned using techniques similar to the ones proposed by Gildea (2003) and by Zhang et al. [sent-96, score-0.277]
</p><p>56 The baseline will be the score for fac-  tored translation, shown in Table 1. [sent-100, score-0.037]
</p><p>57 6  Contributions  After its conclusion, this thesis will have brought the following contributions: 3http : //www . [sent-101, score-0.088]
</p><p>58 org/moses  •  •  •  39 Language-independent SMT models which incorporates syntactic nitn fSoMrmTa tmioond eilns bwohtihc hla inn-guage and translation models. [sent-108, score-0.292]
</p><p>59 Implementations of these models, using the tIomoplsle dmesencrtiabtieodn isn oSfec tthioense e5. [sent-109, score-0.04]
</p><p>60 Technical reports will be written during this thesis progress and made publicly available. [sent-111, score-0.088]
</p><p>61 Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. [sent-132, score-0.143]
</p><p>62 Scalable inference and training of context-rich syntactic translation models. [sent-140, score-0.217]
</p><p>63 Discriminative training and maximum entropy models for statistical machine translation. [sent-192, score-0.078]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tts', 0.317), ('rtl', 0.291), ('ttt', 0.291), ('brazilian', 0.22), ('transducers', 0.214), ('automata', 0.191), ('tree', 0.167), ('maletti', 0.146), ('translation', 0.143), ('smt', 0.134), ('portuguese', 0.134), ('graehl', 0.128), ('iwill', 0.127), ('cfl', 0.125), ('comon', 0.125), ('rtls', 0.125), ('xts', 0.125), ('ialso', 0.11), ('sinv', 0.11), ('rule', 0.098), ('transducer', 0.098), ('fsa', 0.095), ('thesis', 0.088), ('nguyen', 0.087), ('caseli', 0.083), ('palavras', 0.083), ('tiburon', 0.083), ('och', 0.081), ('syntactic', 0.074), ('tas', 0.073), ('tsgs', 0.073), ('beck', 0.073), ('jonathan', 0.073), ('kevin', 0.07), ('galley', 0.069), ('knight', 0.068), ('josef', 0.067), ('synchronous', 0.064), ('grammars', 0.064), ('topdown', 0.063), ('proposal', 0.061), ('trees', 0.06), ('graham', 0.06), ('fst', 0.06), ('representational', 0.06), ('sides', 0.058), ('liu', 0.057), ('zhang', 0.056), ('cfgs', 0.055), ('syntax', 0.054), ('master', 0.053), ('factored', 0.053), ('daniel', 0.053), ('bleu', 0.052), ('materials', 0.052), ('gildea', 0.05), ('xt', 0.049), ('vp', 0.047), ('franz', 0.046), ('nist', 0.046), ('koehn', 0.044), ('alignment', 0.043), ('yamada', 0.043), ('plan', 0.042), ('cfg', 0.042), ('rules', 0.042), ('van', 0.041), ('isn', 0.04), ('composed', 0.04), ('hermann', 0.04), ('statistical', 0.04), ('leaves', 0.039), ('works', 0.038), ('tt', 0.038), ('models', 0.038), ('ney', 0.037), ('letters', 0.037), ('ta', 0.037), ('akira', 0.037), ('tored', 0.037), ('vandeghinste', 0.037), ('dauchet', 0.037), ('stt', 0.037), ('righthand', 0.037), ('atica', 0.037), ('planned', 0.037), ('helena', 0.037), ('ape', 0.037), ('unfeasible', 0.037), ('ipropose', 0.037), ('advent', 0.037), ('eilns', 0.037), ('revi', 0.037), ('mainly', 0.036), ('extended', 0.036), ('post', 0.036), ('hopkins', 0.035), ('petrov', 0.035), ('np', 0.034), ('michel', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="290-tfidf-1" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>Author: Daniel Emilio Beck</p><p>Abstract: In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</p><p>2 0.17253357 <a title="290-tfidf-2" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>3 0.17018911 <a title="290-tfidf-3" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>4 0.16454519 <a title="290-tfidf-4" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>5 0.15803131 <a title="290-tfidf-5" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>6 0.15757641 <a title="290-tfidf-6" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>7 0.15528002 <a title="290-tfidf-7" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>8 0.15099625 <a title="290-tfidf-8" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>9 0.13409676 <a title="290-tfidf-9" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>10 0.13207579 <a title="290-tfidf-10" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>11 0.12859571 <a title="290-tfidf-11" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>12 0.11831298 <a title="290-tfidf-12" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>13 0.10812224 <a title="290-tfidf-13" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>14 0.10581459 <a title="290-tfidf-14" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>15 0.10025903 <a title="290-tfidf-15" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>16 0.098493159 <a title="290-tfidf-16" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>17 0.098351754 <a title="290-tfidf-17" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>18 0.094136052 <a title="290-tfidf-18" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>19 0.09375751 <a title="290-tfidf-19" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>20 0.089941062 <a title="290-tfidf-20" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.216), (1, -0.207), (2, 0.119), (3, 0.019), (4, 0.053), (5, 0.029), (6, -0.11), (7, -0.041), (8, -0.017), (9, -0.009), (10, -0.027), (11, -0.038), (12, -0.012), (13, -0.033), (14, 0.022), (15, -0.02), (16, 0.005), (17, 0.001), (18, -0.012), (19, 0.025), (20, -0.02), (21, 0.013), (22, -0.006), (23, 0.012), (24, 0.069), (25, -0.016), (26, 0.025), (27, -0.016), (28, -0.032), (29, -0.011), (30, 0.008), (31, -0.039), (32, -0.012), (33, -0.025), (34, -0.015), (35, 0.023), (36, -0.012), (37, -0.038), (38, -0.014), (39, 0.016), (40, -0.07), (41, 0.009), (42, -0.0), (43, -0.012), (44, -0.099), (45, 0.066), (46, -0.001), (47, -0.07), (48, 0.067), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95604497 <a title="290-lsi-1" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>Author: Daniel Emilio Beck</p><p>Abstract: In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</p><p>2 0.86362547 <a title="290-lsi-2" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>3 0.8200143 <a title="290-lsi-3" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>4 0.81888115 <a title="290-lsi-4" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>5 0.81497592 <a title="290-lsi-5" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>6 0.79418766 <a title="290-lsi-6" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>7 0.7937603 <a title="290-lsi-7" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>8 0.78548902 <a title="290-lsi-8" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>9 0.78476334 <a title="290-lsi-9" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>10 0.77704602 <a title="290-lsi-10" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>11 0.6948486 <a title="290-lsi-11" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>12 0.69460917 <a title="290-lsi-12" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>13 0.68973237 <a title="290-lsi-13" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>14 0.68732285 <a title="290-lsi-14" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>15 0.65658921 <a title="290-lsi-15" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>16 0.65577573 <a title="290-lsi-16" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>17 0.63203502 <a title="290-lsi-17" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>18 0.63178968 <a title="290-lsi-18" href="./acl-2011-Translating_from_Morphologically_Complex_Languages%3A_A_Paraphrase-Based_Approach.html">310 acl-2011-Translating from Morphologically Complex Languages: A Paraphrase-Based Approach</a></p>
<p>19 0.63057899 <a title="290-lsi-19" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>20 0.62329739 <a title="290-lsi-20" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.015), (17, 0.041), (26, 0.011), (37, 0.045), (39, 0.03), (41, 0.033), (55, 0.013), (59, 0.024), (72, 0.019), (91, 0.026), (96, 0.672)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99969441 <a title="290-lda-1" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>Author: Anselmo Penas ; Alvaro Rodrigo</p><p>Abstract: There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.</p><p>2 0.99850267 <a title="290-lda-2" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>Author: Maoxi Li ; Chengqing Zong ; Hwee Tou Ng</p><p>Abstract: Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 1</p><p>same-paper 3 0.9982022 <a title="290-lda-3" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>Author: Daniel Emilio Beck</p><p>Abstract: In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</p><p>4 0.99716282 <a title="290-lda-4" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>5 0.99634391 <a title="290-lda-5" href="./acl-2011-Improving_On-line_Handwritten_Recognition_using_Translation_Models_in_Multimodal_Interactive_Machine_Translation.html">168 acl-2011-Improving On-line Handwritten Recognition using Translation Models in Multimodal Interactive Machine Translation</a></p>
<p>Author: Vicent Alabau ; Alberto Sanchis ; Francisco Casacuberta</p><p>Abstract: In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work.</p><p>6 0.99587762 <a title="290-lda-6" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>7 0.99510443 <a title="290-lda-7" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>8 0.99045277 <a title="290-lda-8" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>9 0.98635811 <a title="290-lda-9" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>10 0.97579706 <a title="290-lda-10" href="./acl-2011-An_Interactive_Machine_Translation_System_with_Online_Learning.html">41 acl-2011-An Interactive Machine Translation System with Online Learning</a></p>
<p>11 0.97155863 <a title="290-lda-11" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>12 0.96286649 <a title="290-lda-12" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>13 0.95425206 <a title="290-lda-13" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>14 0.95140833 <a title="290-lda-14" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>15 0.94779938 <a title="290-lda-15" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>16 0.94631416 <a title="290-lda-16" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>17 0.94526792 <a title="290-lda-17" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>18 0.9432745 <a title="290-lda-18" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>19 0.94094735 <a title="290-lda-19" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>20 0.93779176 <a title="290-lda-20" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
