<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-336" href="#">acl2011-336</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</h1>
<br/><p>Source: <a title="acl-2011-336-pdf" href="http://aclweb.org/anthology//P/P11/P11-2085.pdf">pdf</a></p><p>Author: Yabin Zheng ; Lixing Xie ; Zhiyuan Liu ; Maosong Sun ; Yang Zhang ; Liyun Ru</p><p>Abstract: Chinese Pinyin input method is very important for Chinese language information processing. Users may make errors when they are typing in Chinese words. In this paper, we are concerned with the reasons that cause the errors. Inspired by the observation that pressing backspace is one of the most common user behaviors to modify the errors, we collect 54, 309, 334 error-correction pairs from a realworld data set that contains 2, 277, 786 users via backspace operations. In addition, we present a comparative analysis of the data to achieve a better understanding of users’ input behaviors. Comparisons with English typos suggest that some language-specific properties result in a part of Chinese input errors. 1</p><p>Reference: <a title="acl-2011-336-reference" href="../acl2011_reference/acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Users may make errors when they are typing in Chinese words. [sent-9, score-0.347]
</p><p>2 Inspired by the observation that pressing backspace is one of the most common user behaviors to modify the errors, we collect 54, 309, 334 error-correction pairs from a realworld data set that contains 2, 277, 786 users via backspace operations. [sent-11, score-0.999]
</p><p>3 Comparisons with English typos suggest that some language-specific properties result in a part of Chinese input errors. [sent-13, score-0.222]
</p><p>4 Chinese users cannot directly type in Chinese words using a QWERTY keyboard. [sent-15, score-0.172]
</p><p>5 Based on this transcription system, Pinyin input methods have been proposed to assist users to type in Chinese words (Chen, 1997). [sent-17, score-0.247]
</p><p>6 Assume users want to type in the Chinese word “什 么(what)”. [sent-20, score-0.172]
</p><p>7 Figure 2: Typical Chinese Pinyin input method for a mistyped Pinyin (Sogou-Pinyin). [sent-25, score-0.217]
</p><p>8 The last two steps do not exist in typing process of English words, which indicates that it is more complicated for Chinese users to type in Chinese words. [sent-27, score-0.379]
</p><p>9 Chinese users may make errors when they are typing in Chinese words. [sent-28, score-0.488]
</p><p>10 Typical Chinese Pinyin input method can not return the right word. [sent-31, score-0.101]
</p><p>11 This greatly limits user experience since users have to identify errors and modify them, or cannot get the right word. [sent-33, score-0.46]
</p><p>12 In this paper, we analyze the reasons that cause errors in Chinese Pinyin input method. [sent-34, score-0.281]
</p><p>13 This analysis is helpful in enhancing the user experience and  the performance of Chinese Pinyin input method. [sent-35, score-0.183]
</p><p>14 In practice, users press backspace on the keyboard to modify the errors, they delete the mistyped word and re-type in the correct word. [sent-36, score-0.737]
</p><p>15 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 485–490, servation, we can extract error-correction pairs from backspace operations. [sent-39, score-0.315]
</p><p>16 These error-correction pairs are of great importance in Chinese spelling correction task which generally relies on sets of confusing words. [sent-40, score-0.398]
</p><p>17 We extract 54, 309, 334 error-correction pairs from user input behaviors and further study them. [sent-41, score-0.356]
</p><p>18 Our comparative analysis of Chinese and English typos suggests that some language-specific properties of Chinese lead to a part of input errors. [sent-42, score-0.222]
</p><p>19 To the best of our knowledge, this paper is the first one which analyzes user input behaviors in Chinese Pinyin input method. [sent-43, score-0.382]
</p><p>20 Section 3 introduces how we collect errors in Chinese Pinyin input method. [sent-46, score-0.215]
</p><p>21 2  Previous Work  For English spelling correction (Kukich, 1992; Ahmad and Kondrak, 2005; Chen et al. [sent-49, score-0.328]
</p><p>22 Context features (Rozovskaya and Roth, 2010) of words provide useful evidences for spelling correction. [sent-53, score-0.194]
</p><p>23 Phonetic features (Toutanova and Moore, 2002; Atkinson, 2008) are proved to be useful in English spelling correction. [sent-56, score-0.194]
</p><p>24 A spelling correction system is trained using these features by a noisy channel model (Kernighan et al. [sent-57, score-0.357]
</p><p>25 Chang (1994) first proposes a representative approach for Chinese spelling correction, which relies on sets of confusing characters. [sent-60, score-0.233]
</p><p>26 (2000) propose an approximate word-matching algorithm for Chinese to solve Chinese spell detec-  tion and correction task. [sent-62, score-0.134]
</p><p>27 (1999) present a winnow-based approach for Chinese spelling correction which takes both local language features and wide-scope semantic features into account. [sent-64, score-0.328]
</p><p>28 (2009) show that about 80% of the errors are related to pronunciations. [sent-68, score-0.14]
</p><p>29 Visual and phonological features are used in Chinese spelling correction (Liu et al. [sent-69, score-0.328]
</p><p>30 Instead of proposing a method for spelling correction, we mainly investigate the reasons that cause typing errors in both English and Chinese. [sent-71, score-0.607]
</p><p>31 Some errors are caused by specific properties in Chinese such as the phonetic difference between Mandarin and dialects spoken in southern China. [sent-72, score-0.256]
</p><p>32 Meanwhile, confusion sets of Chinese words play an important role in Chinese spelling correction. [sent-73, score-0.221]
</p><p>33 We extract a large scale of error-correction pairs from real user input behaviors. [sent-74, score-0.213]
</p><p>34 These pairs contain important evidence about confusing Pinyins and Chinese words which are helpful in Chinese spelling correction. [sent-75, score-0.264]
</p><p>35 3  User Input Behaviors Analysis  We analyze user input behaviors from anonymous user typing records in a Chinese input method. [sent-76, score-0.709]
</p><p>36 It contains 2, 277, 786 users’ typing records in 15 days. [sent-78, score-0.238]
</p><p>37 3, we can see the typing process of a Chinese sentence “这 是 什么” (What is this). [sent-105, score-0.207]
</p><p>38 Each line represents an input segment or a backspace operation. [sent-106, score-0.341]
</p><p>39 The Pinyin he/she he has placed  user made a mistake to type in the third (“shenme” is mistyped as “shenem”). [sent-108, score-0.288]
</p><p>40 Then, pressed the backspace to modify the errors made. [sent-109, score-0.469]
</p><p>41 As a result, we compare the typedin Pinyins before and after backspace operations. [sent-113, score-0.266]
</p><p>42 Threshold is set to 2 in this paper, as Damerau (1964) shows that about 80% of typos are caused by a single edit operation. [sent-115, score-0.177]
</p><p>43 Furthermore, we can extract corresponding Chinese word-correction pairs “什恶魔-什么” from this typing record. [sent-117, score-0.256]
</p><p>44 Most of the mistyped Chinese words are  meaningless. [sent-120, score-0.142]
</p><p>45 Some correct Pinyins are labeled as errors because we only take edit distance into consideration. [sent-126, score-0.168]
</p><p>46 We choose 15 typical mistyped Pinyins to evaluate the recall of our method. [sent-128, score-0.174]
</p><p>47 The total occurrences of these mistyped Pinyins are 259, 051. [sent-129, score-0.142]
</p><p>48 Some errors are not found because sometimes users do not modify the errors, especially when they are using Chinese input method under instant messenger softwares. [sent-132, score-0.401]
</p><p>49 487 4  Comparisons of Pinyin typos and English Typos  In this section, we compare the Pinyin typos and English typos. [sent-133, score-0.224]
</p><p>50 As shown in (Cooper, 1983), typing errors can be classified into four categories: deletions, insertions, substitutions, and transpositions. [sent-134, score-0.347]
</p><p>51 We aim at studying the reasons that result in these four kinds of typing errors in Chinese Pinyin and English, respectively. [sent-135, score-0.407]
</p><p>52 For English typos, we generate mistyped wordcorrection pairs from Wikipedia2 and SpellGood. [sent-136, score-0.213]
</p><p>53 As shown in Table 2, we reach the first conclusion: about half  of the typing errors in Pinyin and English are caused by deletions, which indicates that users are more possible to omit some letters than other three edit operations. [sent-138, score-0.678]
</p><p>54 Table 3 and Table 4 list Top 5 letters that produce deletion errors (users forget to type in some letters) and insertion errors (users type in extra letters) in Pinyin and English. [sent-148, score-0.569]
</p><p>55 Pinuhgeypsxnihe aEgnmxyzaeoi-mpsyxhienla ngszmyaoieuEngctialsh omadbcpiEhaltgsvxiehadns-ma cdpbghlmiaeosnmtivepsldih  Table 3: Deletion errors in Pinyin and English. [sent-149, score-0.14]
</p><p>56 PinagohysxiuhntaEbgiheuxwjba einmo- sxpytheilu bniswuejaniEgsenrilhab nlodrag sEonieux dnaesmdn- ap bsnlrieoadsulnoedgonuesd  Table 4: Insertion errors in Pinyin and English. [sent-150, score-0.14]
</p><p>57 (2) some specific properties in Chinese lead to insertion and deletion errors. [sent-156, score-0.119]
</p><p>58 Many users in southern China cannot distinguish the front and the back nasal sound (‘ang’ ‘an’, ‘ing’ - ‘in’, ‘eng’ - ‘en’) as well as the retroflex and the blade-alveolar (‘zh’ - ‘z’, ‘sh’ - ‘s’, ‘ch’ ‘c’). [sent-157, score-0.213]
</p><p>59 (3) the same letters can occur continuously in English, such as “acomplish-accomplish” and “admited-admitted” in our examples. [sent-159, score-0.125]
</p><p>60 English users sometimes make in-  sertion or deletion errors in these cases. [sent-160, score-0.329]
</p><p>61 We also observe this kind of errors in Chinese Pinyin, such as “yingai-yinggai”, “liange-liangge” and “dianaodiannao”. [sent-161, score-0.14]
</p><p>62 For transposition errors, Table 5 lists Top 10 patterns that produce transposition errors in Pinyin and English. [sent-162, score-0.282]
</p><p>63 We classify the letters of the keyboard into two categories, i. [sent-164, score-0.251]
</p><p>64 Letter ‘e’ is controlled by left hand while ‘m’ is controlled by right hand. [sent-167, score-0.189]
</p><p>65 Users mistype “shenme” as “shenem” because they mistake the typing order of ‘m’ and ‘e’ . [sent-168, score-0.314]
</p><p>66 4, we reach the second conclusion: most of the transposition errors are caused by mistaking the typing orders across left and right hands. [sent-173, score-0.502]
</p><p>67 For instance, users intend to type in a letter (‘m’) controlled by right hand. [sent-174, score-0.332]
</p><p>68 But they type in a letter (‘e’) controlled by left hand instead. [sent-175, score-0.186]
</p><p>69 Table 5: Transpositions errors in Pinyin and English. [sent-176, score-0.14]
</p><p>70 488  Letters Controlled by Left Hand  Letters Controlled by Right Hand  Figure 4: Transpositions errors on the keyboard. [sent-177, score-0.14]
</p><p>71 For substitution errors, we study the reason why users mistype one letter for another. [sent-178, score-0.315]
</p><p>72 In the Pinyincorrection pairs, users always mistype ‘a’ as ‘e’ and vice versa. [sent-179, score-0.222]
</p><p>73 The reason is that they have similar pronunciations in Chinese. [sent-180, score-0.112]
</p><p>74 Some letters are mistyped for each other because they are adjacent on the keyboard although they do not share similar pronunciations, such as ‘g’ and ‘f’. [sent-183, score-0.42]
</p><p>75 We summarize the substitution errors in English  in Fig. [sent-184, score-0.17]
</p><p>76 However, the three letters are not connected in Fig. [sent-187, score-0.145]
</p><p>77 5, which indicates that users can easily distinguish them in Pinyin. [sent-188, score-0.141]
</p><p>78 For example, letters ‘m’ and ‘n’ have similar pronunciations in both Chinese and English. [sent-192, score-0.237]
</p><p>79 6, letters ‘b’ and ‘p’ are connected to each other because they have similar pronunciations in English, although they are not adjacent on the keyboard. [sent-198, score-0.284]
</p><p>80 Finally, we summarize the third conclusion: substitution errors are caused by language specific similarities (similar pronunciations) or keyboard neighborhood (adjacent on the keyboard). [sent-199, score-0.333]
</p><p>81 All in all, we generally classify typing errors in English and Chinese into four categories and investigate the reasons that result in these errors respectively. [sent-200, score-0.529]
</p><p>82 Some language specific properties, such as pronunciations in English and Chinese, lead to substitution, insertion and deletion errors. [sent-201, score-0.196]
</p><p>83 489 5  Conclusions and Future Works  In this paper, we study user input behaviors in Chinese Pinyin input method from backspace operations. [sent-203, score-0.648]
</p><p>84 Users signal that they are very likely to make errors if they press backspace on the keyboard. [sent-205, score-0.423]
</p><p>85 Then they modify the errors and type in the correct words they want. [sent-206, score-0.216]
</p><p>86 Different from the previous research, we extract abundant Pinyin-correction and Chinese word-correction pairs from backspace operations. [sent-207, score-0.315]
</p><p>87 For example, it is less likely that users make errors in the first letter of an input Pinyin. [sent-214, score-0.419]
</p><p>88 (3) we want to build a Chinese spelling correction system based on extracted error-correction pairs. [sent-217, score-0.328]
</p><p>89 Learning a spelling error model from search query logs. [sent-224, score-0.229]
</p><p>90 An improved error model for noisy channel spelling correction. [sent-240, score-0.241]
</p><p>91 A pilot study on automatic Chinese spelling error correction. [sent-246, score-0.212]
</p><p>92 Spelling correction as an iterative process that exploits the collective knowledge of web users. [sent-277, score-0.134]
</p><p>93 A technique for computer detection and correction of spelling errors. [sent-283, score-0.328]
</p><p>94 A large scale ranker-based system for search query spelling correction. [sent-292, score-0.211]
</p><p>95 A spelling correction program based on a noisy channel model. [sent-314, score-0.357]
</p><p>96 Visually and phonologically similar characters in incorrect simplified chinese words. [sent-353, score-0.387]
</p><p>97 Chinese input with keyboard and eye-tracking: an anatomical study. [sent-383, score-0.201]
</p><p>98 Real-word spelling correction with trigrams: A reconsideration of the Mays, Damerau, and Mercer model. [sent-399, score-0.328]
</p><p>99 Multifeature-based approach to automatic error detection and correction of Chinese text. [sent-406, score-0.152]
</p><p>100 Automatic detecting/correcting errors in Chinese text by an approximate word-matching algorithm. [sent-414, score-0.14]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pinyin', 0.621), ('chinese', 0.367), ('backspace', 0.266), ('typing', 0.207), ('spelling', 0.194), ('behaviors', 0.143), ('mistyped', 0.142), ('users', 0.141), ('shenme', 0.141), ('errors', 0.14), ('correction', 0.134), ('keyboard', 0.126), ('letters', 0.125), ('typos', 0.112), ('pronunciations', 0.112), ('pinyins', 0.101), ('zheng', 0.09), ('user', 0.089), ('mistype', 0.081), ('shenem', 0.081), ('input', 0.075), ('controlled', 0.071), ('transposition', 0.071), ('letter', 0.063), ('deletion', 0.048), ('modify', 0.045), ('reasons', 0.042), ('chuang', 0.04), ('hearn', 0.04), ('wordcorrection', 0.04), ('confusing', 0.039), ('hirst', 0.038), ('caused', 0.037), ('insertion', 0.036), ('kernighan', 0.036), ('spe', 0.036), ('transpositions', 0.036), ('yabin', 0.036), ('properties', 0.035), ('brill', 0.034), ('english', 0.034), ('islam', 0.033), ('ristad', 0.033), ('damerau', 0.033), ('lai', 0.033), ('substitutions', 0.032), ('typical', 0.032), ('liu', 0.032), ('pairs', 0.031), ('records', 0.031), ('whitelaw', 0.031), ('tsinghua', 0.031), ('alike', 0.031), ('type', 0.031), ('sound', 0.03), ('substitution', 0.03), ('channel', 0.029), ('rozovskaya', 0.029), ('ru', 0.028), ('visually', 0.028), ('edit', 0.028), ('adjacent', 0.027), ('confusion', 0.027), ('select', 0.027), ('ahmad', 0.027), ('cucerzan', 0.027), ('chen', 0.026), ('mistake', 0.026), ('right', 0.026), ('china', 0.025), ('cause', 0.024), ('southern', 0.024), ('numeric', 0.024), ('deletions', 0.023), ('pronunciation', 0.023), ('moore', 0.022), ('left', 0.021), ('deleted', 0.021), ('characters', 0.02), ('phonetic', 0.02), ('connected', 0.02), ('correcting', 0.019), ('accepted', 0.019), ('experience', 0.019), ('discusses', 0.018), ('error', 0.018), ('zhang', 0.018), ('toutanova', 0.018), ('extract', 0.018), ('hutchinson', 0.018), ('spellchecking', 0.018), ('pressed', 0.018), ('logographic', 0.018), ('layouts', 0.018), ('realworld', 0.018), ('nasal', 0.018), ('aim', 0.018), ('list', 0.018), ('query', 0.017), ('press', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="336-tfidf-1" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>Author: Yabin Zheng ; Lixing Xie ; Zhiyuan Liu ; Maosong Sun ; Yang Zhang ; Liyun Ru</p><p>Abstract: Chinese Pinyin input method is very important for Chinese language information processing. Users may make errors when they are typing in Chinese words. In this paper, we are concerned with the reasons that cause the errors. Inspired by the observation that pressing backspace is one of the most common user behaviors to modify the errors, we collect 54, 309, 334 error-correction pairs from a realworld data set that contains 2, 277, 786 users via backspace operations. In addition, we present a comparative analysis of the data to achieve a better understanding of users’ input behaviors. Comparisons with English typos suggest that some language-specific properties result in a part of Chinese input errors. 1</p><p>2 0.16517127 <a title="336-tfidf-2" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>Author: Maoxi Li ; Chengqing Zong ; Hwee Tou Ng</p><p>Abstract: Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 1</p><p>3 0.13306893 <a title="336-tfidf-3" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>Author: Xiaojun Wan</p><p>Abstract: Cross-language document summarization is defined as the task of producing a summary in a target language (e.g. Chinese) for a set of documents in a source language (e.g. English). Existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language. In this study, we propose to use the bilingual information from both the source and translated documents for this task. Two summarization methods (SimFusion and CoRank) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction. Experimental results on the DUC2001 dataset with manually translated reference Chinese summaries show the effectiveness of the proposed methods. 1</p><p>4 0.13036138 <a title="336-tfidf-4" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>Author: Y. Albert Park ; Roger Levy</p><p>Abstract: Automated grammar correction techniques have seen improvement over the years, but there is still much room for increased performance. Current correction techniques mainly focus on identifying and correcting a specific type of error, such as verb form misuse or preposition misuse, which restricts the corrections to a limited scope. We introduce a novel technique, based on a noisy channel model, which can utilize the whole sentence context to determine proper corrections. We show how to use the EM algorithm to learn the parameters of the noise model, using only a data set of erroneous sentences, given the proper language model. This frees us from the burden of acquiring a large corpora of corrected sentences. We also present a cheap and efficient way to provide automated evaluation re- sults for grammar corrections by using BLEU and METEOR, in contrast to the commonly used manual evaluations.</p><p>5 0.12503397 <a title="336-tfidf-5" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>6 0.11930981 <a title="336-tfidf-6" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>7 0.10924271 <a title="336-tfidf-7" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>8 0.095518231 <a title="336-tfidf-8" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>9 0.0928936 <a title="336-tfidf-9" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>10 0.090566032 <a title="336-tfidf-10" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>11 0.090224452 <a title="336-tfidf-11" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>12 0.089840017 <a title="336-tfidf-12" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>13 0.069522671 <a title="336-tfidf-13" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>14 0.06837824 <a title="336-tfidf-14" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>15 0.065484092 <a title="336-tfidf-15" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>16 0.063906789 <a title="336-tfidf-16" href="./acl-2011-Latent_Class_Transliteration_based_on_Source_Language_Origin.html">197 acl-2011-Latent Class Transliteration based on Source Language Origin</a></p>
<p>17 0.062765978 <a title="336-tfidf-17" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>18 0.061280709 <a title="336-tfidf-18" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>19 0.05589414 <a title="336-tfidf-19" href="./acl-2011-Using_Deep_Morphology_to_Improve_Automatic_Error_Detection_in_Arabic_Handwriting_Recognition.html">329 acl-2011-Using Deep Morphology to Improve Automatic Error Detection in Arabic Handwriting Recognition</a></p>
<p>20 0.053681981 <a title="336-tfidf-20" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, -0.006), (2, 0.002), (3, 0.015), (4, -0.073), (5, -0.04), (6, 0.026), (7, -0.071), (8, 0.056), (9, 0.021), (10, -0.12), (11, -0.028), (12, -0.027), (13, -0.001), (14, -0.045), (15, 0.129), (16, 0.06), (17, -0.072), (18, 0.181), (19, 0.145), (20, 0.002), (21, 0.058), (22, -0.027), (23, 0.041), (24, 0.028), (25, -0.103), (26, -0.036), (27, 0.073), (28, 0.105), (29, -0.051), (30, 0.024), (31, 0.036), (32, 0.036), (33, -0.003), (34, -0.073), (35, -0.144), (36, -0.024), (37, 0.026), (38, 0.044), (39, 0.034), (40, 0.048), (41, 0.034), (42, 0.011), (43, 0.15), (44, 0.023), (45, 0.032), (46, -0.067), (47, 0.056), (48, 0.049), (49, 0.097)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97009206 <a title="336-lsi-1" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>Author: Yabin Zheng ; Lixing Xie ; Zhiyuan Liu ; Maosong Sun ; Yang Zhang ; Liyun Ru</p><p>Abstract: Chinese Pinyin input method is very important for Chinese language information processing. Users may make errors when they are typing in Chinese words. In this paper, we are concerned with the reasons that cause the errors. Inspired by the observation that pressing backspace is one of the most common user behaviors to modify the errors, we collect 54, 309, 334 error-correction pairs from a realworld data set that contains 2, 277, 786 users via backspace operations. In addition, we present a comparative analysis of the data to achieve a better understanding of users’ input behaviors. Comparisons with English typos suggest that some language-specific properties result in a part of Chinese input errors. 1</p><p>2 0.59679604 <a title="336-lsi-2" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>3 0.58879715 <a title="336-lsi-3" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>4 0.55224586 <a title="336-lsi-4" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>Author: Xiaojun Wan</p><p>Abstract: Cross-language document summarization is defined as the task of producing a summary in a target language (e.g. Chinese) for a set of documents in a source language (e.g. English). Existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language. In this study, we propose to use the bilingual information from both the source and translated documents for this task. Two summarization methods (SimFusion and CoRank) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction. Experimental results on the DUC2001 dataset with manually translated reference Chinese summaries show the effectiveness of the proposed methods. 1</p><p>5 0.55067807 <a title="336-lsi-5" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>Author: Maoxi Li ; Chengqing Zong ; Hwee Tou Ng</p><p>Abstract: Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 1</p><p>6 0.50526875 <a title="336-lsi-6" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>7 0.5043112 <a title="336-lsi-7" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>8 0.50287247 <a title="336-lsi-8" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>9 0.50017452 <a title="336-lsi-9" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>10 0.49755654 <a title="336-lsi-10" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>11 0.45441127 <a title="336-lsi-11" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>12 0.42986017 <a title="336-lsi-12" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>13 0.42341998 <a title="336-lsi-13" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>14 0.38707566 <a title="336-lsi-14" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>15 0.38380444 <a title="336-lsi-15" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>16 0.3757731 <a title="336-lsi-16" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>17 0.36878091 <a title="336-lsi-17" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>18 0.3669734 <a title="336-lsi-18" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>19 0.36361045 <a title="336-lsi-19" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>20 0.35889667 <a title="336-lsi-20" href="./acl-2011-Grammatical_Error_Correction_with_Alternating_Structure_Optimization.html">147 acl-2011-Grammatical Error Correction with Alternating Structure Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.014), (17, 0.045), (26, 0.037), (37, 0.04), (39, 0.019), (41, 0.067), (55, 0.023), (59, 0.02), (72, 0.042), (91, 0.039), (96, 0.157), (97, 0.395)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82221031 <a title="336-lda-1" href="./acl-2011-Types_of_Common-Sense_Knowledge_Needed_for_Recognizing_Textual_Entailment.html">315 acl-2011-Types of Common-Sense Knowledge Needed for Recognizing Textual Entailment</a></p>
<p>Author: Peter LoBue ; Alexander Yates</p><p>Abstract: Understanding language requires both linguistic knowledge and knowledge about how the world works, also known as common-sense knowledge. We attempt to characterize the kinds of common-sense knowledge most often involved in recognizing textual entailments. We identify 20 categories of common-sense knowledge that are prevalent in textual entailment, many of which have received scarce attention from researchers building collections of knowledge.</p><p>same-paper 2 0.75655806 <a title="336-lda-2" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>Author: Yabin Zheng ; Lixing Xie ; Zhiyuan Liu ; Maosong Sun ; Yang Zhang ; Liyun Ru</p><p>Abstract: Chinese Pinyin input method is very important for Chinese language information processing. Users may make errors when they are typing in Chinese words. In this paper, we are concerned with the reasons that cause the errors. Inspired by the observation that pressing backspace is one of the most common user behaviors to modify the errors, we collect 54, 309, 334 error-correction pairs from a realworld data set that contains 2, 277, 786 users via backspace operations. In addition, we present a comparative analysis of the data to achieve a better understanding of users’ input behaviors. Comparisons with English typos suggest that some language-specific properties result in a part of Chinese input errors. 1</p><p>3 0.70047462 <a title="336-lda-3" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>Author: Eneko Agirre ; Kepa Bengoetxea ; Koldo Gojenola ; Joakim Nivre</p><p>Abstract: This paper presents the introduction of WordNet semantic classes in a dependency parser, obtaining improvements on the full Penn Treebank for the first time. We tried different combinations of some basic semantic classes and word sense disambiguation algorithms. Our experiments show that selecting the adequate combination of semantic features on development data is key for success. Given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements. 1</p><p>4 0.67060047 <a title="336-lda-4" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>Author: John Lee ; Jason Naradowsky ; David A. Smith</p><p>Abstract: Most previous studies of morphological disambiguation and dependency parsing have been pursued independently. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the “pipeline” approach, assuming that morphological information has been separately obtained. However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.</p><p>5 0.63077414 <a title="336-lda-5" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>Author: Yves Petinot ; Kathleen McKeown ; Kapil Thadani</p><p>Abstract: We investigate the relevance of hierarchical topic models to represent the content of Web gists. We focus our attention on DMOZ, a popular Web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories. Our first approach, based on information-theoretic grounds, uses an algorithm similar to recursive feature selection. Our second approach is fully Bayesian and derived from the more general model, hierarchical LDA. We evaluate the performance of both models against a flat 1-gram baseline and show improvements in terms of perplexity over held-out data.</p><p>6 0.53277731 <a title="336-lda-6" href="./acl-2011-Identification_of_Domain-Specific_Senses_in_a_Machine-Readable_Dictionary.html">158 acl-2011-Identification of Domain-Specific Senses in a Machine-Readable Dictionary</a></p>
<p>7 0.50309789 <a title="336-lda-7" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>8 0.50255555 <a title="336-lda-8" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>9 0.49292356 <a title="336-lda-9" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>10 0.48922771 <a title="336-lda-10" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>11 0.48864251 <a title="336-lda-11" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>12 0.48184589 <a title="336-lda-12" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>13 0.47778633 <a title="336-lda-13" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>14 0.47396559 <a title="336-lda-14" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>15 0.46613106 <a title="336-lda-15" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>16 0.46411502 <a title="336-lda-16" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>17 0.46338096 <a title="336-lda-17" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>18 0.46167362 <a title="336-lda-18" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>19 0.46031469 <a title="336-lda-19" href="./acl-2011-Unsupervised_Decomposition_of_a_Document_into_Authorial_Components.html">319 acl-2011-Unsupervised Decomposition of a Document into Authorial Components</a></p>
<p>20 0.45883277 <a title="336-lda-20" href="./acl-2011-A_Corpus_for_Modeling_Morpho-Syntactic_Agreement_in_Arabic%3A_Gender%2C_Number_and_Rationality.html">7 acl-2011-A Corpus for Modeling Morpho-Syntactic Agreement in Arabic: Gender, Number and Rationality</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
