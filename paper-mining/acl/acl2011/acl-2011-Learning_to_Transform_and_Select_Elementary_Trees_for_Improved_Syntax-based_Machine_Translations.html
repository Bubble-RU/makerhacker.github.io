<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-206" href="#">acl2011-206</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</h1>
<br/><p>Source: <a title="acl-2011-206-pdf" href="http://aclweb.org/anthology//P/P11/P11-1085.pdf">pdf</a></p><p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>Reference: <a title="acl-2011-206-reference" href="../acl2011_reference/acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. [sent-7, score-0.65]
</p><p>2 We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. [sent-8, score-0.466]
</p><p>3 1 Introduction Most syntax-based machine translation models with synchronous context free grammar (SCFG) have been re-  lying on the off-the-shelf monolingual parse structures to learn the translation equivalences for string-to-tree, tree-to-string or tree-to-tree grammars. [sent-13, score-0.512]
</p><p>4 1% of NP-SBJs are mapped with human alignment and parse trees as in § 2. [sent-17, score-0.345]
</p><p>5 (2006) used broken syntactic fragments to augment their grammars to increase the rule coverage; while we learn optimal tree fragments transformed from the original ones via a generative framework, they enumerate the fragments –  available from the original trees without learning process. [sent-25, score-0.482]
</p><p>6 Mi and Huang (2008) introduced parse forests to blur the chunking decisions to a certain degree, to expand search space and reduce parsing errors from 1-best trees (Mi et al. [sent-26, score-0.269]
</p><p>7 , 2008); others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero (Marton and Resnik, 2008; Chiang, 2010; Huang et al. [sent-27, score-0.39]
</p><p>8 , 2008) based approaches were carried out by adding pseudo nodes and hyper edges into the forest. [sent-33, score-0.418]
</p><p>9 The tree sequence approach adds pseudo nodes and hyper edges to the forest, which makes the forest even denser and harder for navigation and search. [sent-35, score-0.648]
</p><p>10 As trees thrive in the search space, especially with the pseudo nodes and edges being added to the already dense forest, it is becoming harder to wade through the deep forest for the best derivation path out. [sent-36, score-0.72]
</p><p>11 For each source span in the given sentence, a subgraph, corresponding to an elementary tree (in Eqn. [sent-43, score-0.7]
</p><p>12 1), is proposed for PSCFG translation; we apply a few operators to transform the subgraph into some frequent subgraphs seen in the whole training data, and thus introduce alternative similar translational equivalences to explain the same source span with enriched statistics and features. [sent-44, score-0.666]
</p><p>13 For instance, if we regroup two adjacent nodes IV and NP-SBJ in the tree, we can obtain the correct reordering pattern for verb-subject order, which is not easily available otherwise. [sent-45, score-0.533]
</p><p>14 By finding a set of similar elementary trees derived from the original elementary trees, statistics can be shared for robust learning. [sent-46, score-0.936]
</p><p>15 This is to further disambiguate the transformed subgraphs so that informative neighboring nodes and edges can influence the reordering preferences for each of the transformed trees. [sent-48, score-0.84]
</p><p>16 For instance, at the beginning and end of a sentence, we do not expect dramatic long distance reordering to happen; or under SBAR context, the clause may prefer monotonic reordering for verb and subject. [sent-49, score-0.476]
</p><p>17 We integrate the neighboring context of the subgraph in our transformation preference predictions, and this improve translation qualities further. [sent-53, score-0.357]
</p><p>18 2  The Projectable Structures  A context-free style nonterminal in PSCFG rules means the source span governed by the nonterminal should be translated into a contiguous target chunk. [sent-55, score-0.383]
</p><p>19 We carried out a controlled study on the projectable structures using human annotated parse trees and word alignment for 5k Arabic-English sentence-pairs. [sent-57, score-0.576]
</p><p>20 In Table 1, the unlabeled F-measures with machine alignment and parse trees show that, for only 48. [sent-58, score-0.382]
</p><p>21 71% of the time, the boundaries introduced by the source parses 847  AlignmentParseLabelsAccuracy  the source nodes onto the target side via alignments and parse trees; unlabeled F-measures show the bracketing accuracies for translating a source span contiguously. [sent-59, score-0.76]
</p><p>22 are real translation boundaries that can be explained by a nonterminal in PSCFG rule. [sent-61, score-0.293]
</p><p>23 In Figure 1, several non-projectable nodes were illustrated: the deleted nonterminals PRON (+hA), the many-  to-many alignment for IV(ttAlf) PREP(mn), fork-style alignment for NOUN (Azmp). [sent-68, score-0.492]
</p><p>24 Intuitively, it would be good to glue the nodes NOUN(Al$rq) ADJ(AlAwsT) under the node ofNP, because it is more frequent for moving ADJ before NOUN in our training data. [sent-69, score-0.325]
</p><p>25 It should be easier to model the swapping of (NOUN ADJ) using the tree (NP NOUN, ADJ) instead of the original bigger tree of (NP-SBJ Azmp, NOUN, ADJ) with one lexicalized node. [sent-70, score-0.324]
</p><p>26 , 2009) tried to address the bracketing problem by using arbitrary pseudo nodes to weave a new “tree” back into the forest for further grammar extractions. [sent-72, score-0.498]
</p><p>27 Some of the interior nodes connecting the frontier nodes might be very informative for modeling reordering. [sent-74, score-0.889]
</p><p>28 The created pseudo node could easily block the informative neighbor nodes associated with the subgraph which could change the reordering nature. [sent-76, score-0.755]
</p><p>29 We propose to navigate through the forest, via simplifying trees by grouping the nodes, cutting the branches,  and attaching connected neighboring informative nodes to further disambiguate the derivation path. [sent-79, score-0.663]
</p><p>30 3  Elementary Trees to String Grammar  We propose to use variations of an elementary tree, which is a connected subgraph fitted in the original monolingual parse tree. [sent-81, score-0.622]
</p><p>31 vi, however, are not necessarily informative for the reordering decisions, like the unary nodes WHNP,VP, and PP-CLR in Figure 1; while the frontier nodes γ. [sent-86, score-0.9]
</p><p>32 We can selectively cut off the interior nodes, which have no or only weak causal relations to the reordering decisions. [sent-88, score-0.473]
</p><p>33 This will enable the frequency or derived probabilities for executing the reordering to be more focused. [sent-89, score-0.281]
</p><p>34 We specified a few operators for transforming an elementary tree γ, including flattening tree operators such as removing interior nodes in vi, or grouping the children via binarizations. [sent-91, score-1.86]
</p><p>35 vi = {WHNP VP S PP-CLR}; the frontier nodes are γ. [sent-94, score-0.358]
</p><p>36 The frontier nodes can be merged, lexicalized, or even deleted in the tree-to-string rule associated with γ0, as long as the alignment for the nonterminals are book-kept in the derivations. [sent-105, score-0.599]
</p><p>37 This is because it uses all similar set of elementary trees to generate the best target strings. [sent-110, score-0.603]
</p><p>38 In the next section, we’ll first define the operators conceptually, and then explain how we learn each of the models. [sent-111, score-0.273]
</p><p>39 For instance using th|e operator t¯ of cutting an unary interior node in γ. [sent-117, score-0.455]
</p><p>40 vi has more than one unary interior node, like the SBAR tree in Figure 1, having three unary interior node: WHNP, VP and PP-CLR, pb(γ0|t¯, γ, m¯ ) specifies which one should have more probabilities to be cut. [sent-119, score-0.75]
</p><p>41 It ranks the operators owuhric ohp are rva plirdo otos lb e m applied for the given source tree γ together with its neighborhood m¯ . [sent-123, score-0.487]
</p><p>42 The feature sets we use h|γe,re m¯ are ∝alm eoxpst λth ·e f same set we used to train our Arabic parser; the only difference is the future space here is operator categories, and we check bag-of-nodes for interior nodes and frontier nodes. [sent-125, score-0.733]
</p><p>43 4 t¯: Tree Transformation Function Obvious systematic linguistic divergences between language-pairs could be handled by some simple operators such as using binarization to re-group contiguously aligned children. [sent-132, score-0.488]
</p><p>44 In decoding time, we need to select trees from all possible binarizations, while in the training time, we restrict the choices allowed  with the alignment constraint, that every grouped children should be aligned contiguously on the target side. [sent-139, score-0.55]
</p><p>45 In this paper, we applied the four basic operators for binarizing a tree: left-most, right-most and additionally head-out left and head-out right for more than three children. [sent-141, score-0.273]
</p><p>46 However, if the verb and its relevant arguments for reordering are at different levels in the tree, the reordering is difficult to model as more interior nodes combinations will distract the distributions and make the model less focused. [sent-148, score-0.97]
</p><p>47 3 Removing interior nodes and edges For reordering patterns, keeping the deep tree structure might not be the best choice. [sent-152, score-0.949]
</p><p>48 So, we introduce the operators to remove the interior nodes γ. [sent-154, score-0.767]
</p><p>49 vi selectively; this way, we can flatten the tree, remove irrelevant nodes and edges, and can use more frequent observations of simplified structures to capture the reordering patterns. [sent-155, score-0.559]
</p><p>50 The second operator deletes all the interior nodes, labels and edges; thus reordering will become a Hiero-alike (Chiang, 2007) unlabeled rule, and additionally a special glue rule: X1X2 → X1X2. [sent-157, score-0.678]
</p><p>51 This operator is necessary, we need a schem→e t oX automatically back off to the meaningful glue or Hiero-alike rules, which may lead to a cheaper derivation path for constructing a partial hypothesis, at the decoding time. [sent-158, score-0.317]
</p><p>52 The NP tree in Figure 2 happens to be an “inside-out” style alignment, and context free grammar such as ITG (Wu, 1997) can not explain this structure well without necessary lexicalization. [sent-163, score-0.3]
</p><p>53 nOtuira operators ra all otewrn us to back off to such Hiero-style rules to construct derivations, which share the immediate common parent NP, as 850 defined for the elementary tree, for the given source span. [sent-166, score-0.789]
</p><p>54 5 m¯: Neighboring Function For a given elementary tree, we use function m¯ to check the context beyond the subgraph. [sent-168, score-0.369]
</p><p>55 This includes looking  the nodes and edges connected to the subgraph. [sent-169, score-0.348]
</p><p>56 1 Sentence boundaries When the tree γ frontier sets contain the left-most token, right-most token, or both sides, we will add to the neighboring nodes the corresponding decoration tags L (left), R (right), and B (both), respectively. [sent-174, score-0.699]
</p><p>57 These decorations are important especially when the reordering patterns for the same trees are depending on the context. [sent-175, score-0.419]
</p><p>58 These labels indicate a partial sentence or clause, and the reordering patterns may get different distributions due to the position relative to these nodes. [sent-180, score-0.26]
</p><p>59 For instance, the PV and SBJ nodes under SBAR tends to have more monotone preference for word reordering (Carpuat et al. [sent-181, score-0.511]
</p><p>60 We mark the boundaries with position markers such as L-PP, to indicate having a left sibling PP, R-IP for having a right sibling IP, and C-SBAR to indicate the elementary tree is a child of SBAR. [sent-183, score-0.632]
</p><p>61 3 Translation boundaries In the Figure 2, there are two special nodes under NP: NP* and PP*. [sent-190, score-0.343]
</p><p>62 These two nodes are aligned in a “insideout” fashion, and none of them can be generalized into a nonterminal to be rewritten in a PSCFG rule. [sent-191, score-0.347]
</p><p>63 The translation boundaries over elementary trees have much richer representation power. [sent-194, score-0.798]
</p><p>64 (2010), defined translation boundaries on phrase-decoder style derivation trees due to the nature of their shift-reduce algorithm, which is a special case in our model. [sent-196, score-0.519]
</p><p>65 4  Decoding  Decoding using the proposed elementary tree to string grammar naturally resembles bottom up chart parsing algorithms. [sent-197, score-0.649]
</p><p>66 Given a grammar G, and the input source parse tree π from a monolingual parser, we first construct the elementary tree for a source span, and then retrieve all the relevant subgraphs seen in the given grammar through the proposed operators. [sent-199, score-1.113]
</p><p>67 This step is called populating, using the proposed operators to find all relevant elementary trees γ which may have contributed to explain the source span, and put them in the corresponding cells in the chart. [sent-200, score-0.892]
</p><p>68 There would have been exponential number of relevant elementary trees to search if we do not have any restrictions in the populating step; we restrict the maximum number of interior nodes |γ. [sent-201, score-1.103]
</p><p>69 tvha|n t o6 ;b ead 3d,i atinodna thl pruning rfoonr ileerss n frequent elementary str teheasn i s6 ;ca ardrdieitdio onuatl. [sent-204, score-0.369]
</p><p>70 After populating the elementary trees, we construct the partial hypotheses bottom up, by rewriting the frontier nodes of each elementary tree with the probabilities(costs) for γ → α∗ as in Eqn. [sent-205, score-1.339]
</p><p>71 It generalizes over the dotted-product operator in Earley style parser, to allow us to leverage many operators t¯ ∈ T as above-mentioned, such as binarizations, at  diffetre ∈nt Tlev aesls a bfoorv constructing partial hypothesis. [sent-208, score-0.489]
</p><p>72 From the parallel data, we extract phrase pairs(blocks) and elementary trees to string grammar in various configurations: basic tree-to-string rules (Tr2str), elementary tree-to-string rules with boundaries ¯t(elm2str+ m¯), and with both t¯ and m¯ (elm2str+t¯ m¯ ). [sent-218, score-1.155]
</p><p>73 There are 16 thousand human parse trees with human alignment; additional 1 thousand human parse and aligned sent-pairs are used as unseen test set to verify our MaxEnt models and parsers. [sent-223, score-0.383]
</p><p>74 Our baseline used basic elementary tree to string grammar without any manipulations and boundary markers in the model, 1DEV10 are unseen testsets used in our GALE project. [sent-240, score-0.754]
</p><p>75 1 Predicting Projectable Structures The projectable structure is important for our proposed elementary tree to string grammar (elm2str). [sent-251, score-0.818]
</p><p>76 7% Table 8: Accuracies of predicting projectable structures We zoom in the translation boundaries for MT08-NW, in which we studied a few important frequent labels including VP and NP-SBJ as in Table 9. [sent-268, score-0.462]
</p><p>77 According to our MaxEnt model, 20% of times we should discourage a VP tree to be translated contiguously; such VP trees have an average span length of 16. [sent-269, score-0.477]
</p><p>78 8929  Table 9: The predicted projectable structures in MT08-NW  Using the predicted projectable structures for elm2str grammar, together with the probability defined in Eqn. [sent-283, score-0.462]
</p><p>79 We start from transforming the trees via simple operator t¯(γ), and then expand the function with more tree context to include the neighboring functions: t¯(γ, m¯ ). [sent-298, score-0.642]
</p><p>80 Experiments in Table 10 focus on testing operators especially binarizations for transforming the trees. [sent-302, score-0.453]
</p><p>81 7 5 074  Table 12: BLEU scores on various test sets; comparing elementary tree-to-string grammar (tr2str), transformation using the neighboring function for boundaries ( elm2str+ m¯), and combination of all together (  (elm2str+t¯),  MT08-NW  of the trees  elm2str+t¯(γ, m¯ )). [sent-307, score-0.866]
</p><p>82 Because during our decoding time, we do not frequently see large number of children (maximum at 6), and for smaller trees (with three or four children), these operators will largely generate same transformed trees, and that explains the differences from these individual binarization are small. [sent-314, score-0.741]
</p><p>83 Upon close examinations, we found it is usually beneficial to group verb (PV or IV) with its neighboring nodes for expressing phrases like “have to do” and “will not only”. [sent-317, score-0.354]
</p><p>84 Deleting the interior nodes helps on shrinking the trees, so that we can translate it with more statistics and confidences. [sent-318, score-0.494]
</p><p>85 Table 11 extends Table 10 with neighboring function to further disambiguate the reordering rule using the tree context. [sent-320, score-0.566]
</p><p>86 Besides the translation boundary, the reordering decisions should be different with regard to the positions of the elementary tree relative to the sentence. [sent-321, score-0.882]
</p><p>87 Also, the verb subject order under SBAR seems to be more like monotone with a leading pronoun, rather than the general strong reordering of moving verb after subject. [sent-330, score-0.337]
</p><p>88 Shown in Table 12, we apply separately the operators for t¯ and m¯ first, 853 then combine them as the final results. [sent-337, score-0.273]
</p><p>89 Regarding to the individual operators proposed in this paper, we observed consistent improvements of applying them across all the datasets. [sent-346, score-0.273]
</p><p>90 3 leverages the operators further by selecting the best transformed  tree form for executing the reorderings. [sent-348, score-0.55]
</p><p>91 3 A Translation Example To illustrate the advantages of the proposed grammar, we use a testing case with long distance word reordering and the source side parse trees. [sent-350, score-0.344]
</p><p>92 The highlighted parts in Figure 3 show that, the rules on partial trees are effectively selected and applied for capturing long-distance word reordering, which is otherwise rather difficult to get correct in a phrasal system even with a MaxEnt reordering model. [sent-353, score-0.496]
</p><p>93 6  Discussions and Conclusions  We proposed a framework to learn models to predict how to transform an elementary tree into its simplified forms for better executing the word reorderings. [sent-354, score-0.694]
</p><p>94 Two types of operators were explored, including (a) transforming the trees via binarizations, grouping or deleting interior nodes to change the structures; and (b) neighboring boundary context to further disambiguate the reordering decisions. [sent-355, score-1.489]
</p><p>95 The left panel is source parse tree for the Arabic sentence — the input to our decoder; the right panel is the English translation together with the simplified derivation tree and alignment from our decoder output. [sent-362, score-0.774]
</p><p>96 Each “X” is a nonterminal in the grammar rule; a “Block” means a phrase pair is applied to rewrite a nonterminal; “Glue” and “Hiero” means the unlabeled rules were chosen to explain the span as explained in § 3. [sent-363, score-0.3]
</p><p>97 3P ;V “ aTnrde eN”P m-SeaBnJ;s for the span [18,23], a rule is backed off to an unlabeled rule (Hiero-alike); for the span [21,22], it is another partial tree of NPs. [sent-368, score-0.606]
</p><p>98 Within the proposed framework, we also presented several special cases including the translation boundaries for nonterminals in SCFG for translation. [sent-369, score-0.329]
</p><p>99 Future works aim at transforming such non-projectable trees into projectable form (Eisner, 2003), driven by translation  rules from aligned data(Burkett et al. [sent-372, score-0.621]
</p><p>100 Relabeling syntax trees to improve syntax-based machine translation quality. [sent-438, score-0.328]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('elementary', 0.369), ('operators', 0.273), ('interior', 0.252), ('nodes', 0.242), ('reordering', 0.221), ('trees', 0.198), ('projectable', 0.169), ('tree', 0.162), ('translation', 0.13), ('operator', 0.123), ('sbar', 0.12), ('span', 0.117), ('frontier', 0.116), ('subgraph', 0.113), ('pseudo', 0.104), ('boundaries', 0.101), ('pscfg', 0.099), ('binarizations', 0.099), ('nonterminals', 0.098), ('bleu', 0.098), ('binarization', 0.095), ('maxent', 0.086), ('grammar', 0.084), ('transforming', 0.081), ('neighboring', 0.078), ('contiguously', 0.077), ('alignment', 0.076), ('np', 0.076), ('decoding', 0.074), ('edges', 0.072), ('vp', 0.072), ('parse', 0.071), ('boundary', 0.07), ('azmp', 0.07), ('regroup', 0.07), ('ttalf', 0.07), ('transform', 0.069), ('forest', 0.068), ('rule', 0.067), ('arabic', 0.065), ('pc', 0.064), ('nonterminal', 0.062), ('structures', 0.062), ('executing', 0.06), ('hiero', 0.058), ('pb', 0.057), ('immediate', 0.055), ('transformed', 0.055), ('style', 0.054), ('source', 0.052), ('alanfjar', 0.052), ('dfe', 0.052), ('regrouping', 0.052), ('zhao', 0.051), ('iv', 0.051), ('decoder', 0.051), ('adj', 0.049), ('monotone', 0.048), ('children', 0.046), ('bing', 0.046), ('markups', 0.046), ('aly', 0.046), ('glue', 0.045), ('pv', 0.045), ('aligned', 0.043), ('cased', 0.042), ('populating', 0.042), ('subgraphs', 0.042), ('unary', 0.042), ('huang', 0.041), ('marton', 0.041), ('vf', 0.04), ('ittycheriah', 0.04), ('parent', 0.04), ('mn', 0.039), ('partial', 0.039), ('phrasal', 0.038), ('disambiguate', 0.038), ('node', 0.038), ('informative', 0.037), ('unlabeled', 0.037), ('derivation', 0.036), ('transformation', 0.036), ('target', 0.036), ('deleting', 0.036), ('monolingual', 0.035), ('alawst', 0.035), ('alty', 0.035), ('ignite', 0.035), ('modelm', 0.035), ('testsets', 0.035), ('expose', 0.035), ('sbj', 0.035), ('noun', 0.035), ('string', 0.034), ('connected', 0.034), ('simplified', 0.034), ('verb', 0.034), ('prep', 0.034), ('ff', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="206-tfidf-1" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>2 0.27639836 <a title="206-tfidf-2" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>Author: Hao Zhang ; Licheng Fang ; Peng Xu ; Xiaoyun Wu</p><p>Abstract: Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.</p><p>3 0.26343763 <a title="206-tfidf-3" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>4 0.26173717 <a title="206-tfidf-4" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>5 0.26087427 <a title="206-tfidf-5" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>Author: Xiaoqiang Luo ; Bing Zhao</p><p>Abstract: In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.</p><p>6 0.26005137 <a title="206-tfidf-6" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>7 0.19799025 <a title="206-tfidf-7" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>8 0.19074725 <a title="206-tfidf-8" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>9 0.18778372 <a title="206-tfidf-9" href="./acl-2011-Terminal-Aware_Synchronous_Binarization.html">296 acl-2011-Terminal-Aware Synchronous Binarization</a></p>
<p>10 0.1851844 <a title="206-tfidf-10" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>11 0.1767787 <a title="206-tfidf-11" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>12 0.16773945 <a title="206-tfidf-12" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>13 0.16057667 <a title="206-tfidf-13" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>14 0.15682447 <a title="206-tfidf-14" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>15 0.1496304 <a title="206-tfidf-15" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>16 0.14379384 <a title="206-tfidf-16" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>17 0.1421897 <a title="206-tfidf-17" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>18 0.13963225 <a title="206-tfidf-18" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>19 0.13207579 <a title="206-tfidf-19" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>20 0.12924331 <a title="206-tfidf-20" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.3), (1, -0.281), (2, 0.146), (3, -0.066), (4, 0.048), (5, 0.018), (6, -0.238), (7, -0.074), (8, -0.06), (9, -0.047), (10, -0.06), (11, -0.052), (12, -0.046), (13, 0.026), (14, 0.058), (15, -0.087), (16, 0.008), (17, 0.009), (18, -0.058), (19, 0.006), (20, -0.058), (21, -0.018), (22, -0.047), (23, 0.072), (24, 0.024), (25, 0.005), (26, 0.026), (27, 0.031), (28, 0.051), (29, -0.007), (30, 0.033), (31, -0.042), (32, 0.044), (33, 0.004), (34, 0.079), (35, -0.103), (36, -0.069), (37, 0.005), (38, -0.014), (39, -0.027), (40, 0.086), (41, -0.072), (42, 0.015), (43, 0.022), (44, 0.038), (45, 0.006), (46, -0.028), (47, 0.102), (48, -0.02), (49, -0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95795691 <a title="206-lsi-1" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>2 0.83858466 <a title="206-lsi-2" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>3 0.83249336 <a title="206-lsi-3" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>Author: Hao Zhang ; Licheng Fang ; Peng Xu ; Xiaoyun Wu</p><p>Abstract: Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.</p><p>4 0.80071282 <a title="206-lsi-4" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>5 0.76756018 <a title="206-lsi-5" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>6 0.74055409 <a title="206-lsi-6" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>7 0.73919928 <a title="206-lsi-7" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>8 0.73218709 <a title="206-lsi-8" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>9 0.72573054 <a title="206-lsi-9" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>10 0.72073257 <a title="206-lsi-10" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>11 0.69106424 <a title="206-lsi-11" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>12 0.66235662 <a title="206-lsi-12" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>13 0.64798743 <a title="206-lsi-13" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>14 0.64292765 <a title="206-lsi-14" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>15 0.63951713 <a title="206-lsi-15" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>16 0.62000102 <a title="206-lsi-16" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>17 0.60750353 <a title="206-lsi-17" href="./acl-2011-Terminal-Aware_Synchronous_Binarization.html">296 acl-2011-Terminal-Aware Synchronous Binarization</a></p>
<p>18 0.59943169 <a title="206-lsi-18" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>19 0.59409517 <a title="206-lsi-19" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>20 0.58676124 <a title="206-lsi-20" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.033), (17, 0.072), (26, 0.023), (37, 0.117), (39, 0.075), (41, 0.052), (53, 0.014), (55, 0.028), (59, 0.032), (64, 0.198), (72, 0.039), (91, 0.049), (96, 0.17), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96986818 <a title="206-lda-1" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>Author: Alexander M. Rush and Michael Collins</p><p>Abstract: unkown-abstract</p><p>same-paper 2 0.84977949 <a title="206-lda-2" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>3 0.79485589 <a title="206-lda-3" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.</p><p>4 0.7855975 <a title="206-lda-4" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.</p><p>5 0.78135443 <a title="206-lda-5" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>6 0.77560985 <a title="206-lda-6" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>7 0.77494514 <a title="206-lda-7" href="./acl-2011-Ordering_Prenominal_Modifiers_with_a_Reranking_Approach.html">237 acl-2011-Ordering Prenominal Modifiers with a Reranking Approach</a></p>
<p>8 0.76804137 <a title="206-lda-8" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>9 0.76361555 <a title="206-lda-9" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>10 0.76215434 <a title="206-lda-10" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>11 0.75916994 <a title="206-lda-11" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>12 0.75868535 <a title="206-lda-12" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>13 0.75845301 <a title="206-lda-13" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>14 0.75783324 <a title="206-lda-14" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>15 0.7572186 <a title="206-lda-15" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>16 0.75663209 <a title="206-lda-16" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>17 0.75640893 <a title="206-lda-17" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>18 0.75624645 <a title="206-lda-18" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>19 0.75585693 <a title="206-lda-19" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>20 0.75576317 <a title="206-lda-20" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
