<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-338" href="#">acl2011-338</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</h1>
<br/><p>Source: <a title="acl-2011-338-pdf" href="http://aclweb.org/anthology//P/P11/P11-4013.pdf">pdf</a></p><p>Author: Daniel Bar ; Nicolai Erbs ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: We present Wikulu1, a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis.</p><p>Reference: <a title="acl-2011-338-reference" href="../acl2011_reference/acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We present Wikulu1, a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. [sent-4, score-0.654]
</p><p>2 Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. [sent-5, score-0.132]
</p><p>3 It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. [sent-6, score-0.756]
</p><p>4 Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis. [sent-7, score-0.172]
</p><p>5 However, as wikis do not enforce their users to structure pages or add complementary metadata, wikis often end up as a mass of unmanageable pages with meaningless page titles and no usable link structure (Buffa, 2006). [sent-16, score-0.62]
</p><p>6 To solve this issue, we present the Wikulu system which uses natural language processing to support wiki users with their typical tasks of adding, 1Portmanteau ofthe Hawaiian terms wiki (“fast”) and kukulu (“to organize”) 2http : / /www . [sent-17, score-1.137]
</p><p>7 For example, Wikulu supports users with reading longer texts by highlighting keyphrases using keyphrase extraction methods such as TextRank (Mihalcea and Tarau, 2004). [sent-23, score-0.301]
</p><p>8 Generally, Wikulu allows to integrate any NLP component which conforms to the standards of Apache UIMA (Ferrucci and Lally, 2004). [sent-25, score-0.051]
</p><p>9 Our system is implemented as an HTTP proxy server which intercepts the communication between the web browser and the underlying wiki engine. [sent-27, score-0.853]
</p><p>10 No further modifications to the original wiki installation are necessary. [sent-28, score-0.548]
</p><p>11 Currently, our system prototype contains adaptors for two widely used wiki engines: MediaWiki5 and TWiki6. [sent-29, score-0.584]
</p><p>12 Adaptors for other wiki engines can be added with minimal effort. [sent-30, score-0.546]
</p><p>13 Generally, Wikulu could also be applied to any webbased system other than wikis with only slight modifications to its architecture. [sent-31, score-0.228]
</p><p>14 7 The additional user interface components are integrated into the default toolbar (highlighted by a red box in the screenshot). [sent-33, score-0.162]
</p><p>15 In this example, the user has requested keyphrase highlighting in order to quickly get an idea about the main  content of the wiki article. [sent-34, score-0.728]
</p><p>16 The augmented toolbar (red box) and the results of a keyphrase extraction algorithm (yellow text spans) are highlighted. [sent-45, score-0.154]
</p><p>17 corresponding NLP component, and highlights the returned keyphrases in the article. [sent-46, score-0.074]
</p><p>18 Detecting Duplicates Whenever users add new content to a wiki there is the danger of duplicating already contained information. [sent-49, score-0.628]
</p><p>19 In order to avoid duplication, users would need comprehensive knowledge of what content is already present in the wiki, which is almost impossible for large wikis like Wikipedia. [sent-50, score-0.308]
</p><p>20 Wikulu helps to detect potential duplicates by computing the text similarity between newly added content and each existing wiki page. [sent-51, score-0.572]
</p><p>21 If a potential duplicate is detected, the user is notified and may decide to augment the duplicate page instead of adding a new one. [sent-52, score-0.127]
</p><p>22 Wikulu integrates text similarity measures such as Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007) and Latent  Semantic Analysis (Landauer et al. [sent-53, score-0.044]
</p><p>23 Suggesting Links While many wiki users readily add textual contents to wikis, they often restrain from also adding links to related pages. [sent-55, score-0.652]
</p><p>24 However, links in wikis are crucial as they allow users to quickly navigate from one page to another, or browse through the wiki. [sent-56, score-0.386]
</p><p>25 Therefore, it may be reasonable to augment a page about the topic sentiment 75  Figure 2: Automatic discovery of links to other wiki articles. [sent-57, score-0.643]
</p><p>26 Suitable text phrases to place a link on are highlighted in green. [sent-58, score-0.075]
</p><p>27 analysis by a link to a page providing related information such as evaluation datasets. [sent-59, score-0.123]
</p><p>28 Wikulu supports users in this tedious task by automatically suggesting links. [sent-60, score-0.081]
</p><p>29 Link suggestion thereby is a two-step process: (a) first, suitable text phrases are extracted which might be worth to place a link on (see Figure 2), and (b) for each phrase, related pages are ranked by comparing their relevance to the current page, and then presented to the user. [sent-61, score-0.066]
</p><p>30 The user may thus decide whether she wants to use a detected phrase as a link or not, and if so, which other wiki page to link this phrase to. [sent-62, score-0.737]
</p><p>31 Wikulu currently integrates link suggestion algorithms by Geva (2007) and Itakura and Clarke (2007). [sent-63, score-0.133]
</p><p>32 However, a page  might exist which is semantically related to the keyword, and should thus yield a match. [sent-68, score-0.073]
</p><p>33 As the search engine is typically a core part of the wiki system, it is rather difficult to modify its behavior. [sent-69, score-0.569]
</p><p>34 However, by leveraging Wikulu’s architecture, we can replace the default search mechanisms by algorithms which allow for semantic search to alleviate the vocabulary mismatch problem (Gurevych et al. [sent-70, score-0.063]
</p><p>35 Figure 3: Analysis of a wiki article with respect to topical coherence. [sent-73, score-0.528]
</p><p>36 Wikulu therefore supports users by analyzing long pages through employing text segmentation algorithms which detect topically coherent segments of text. [sent-76, score-0.13]
</p><p>37 It then suggests segment boundaries which the user may or may not accept for inserting a subheading which makes pages easier to read and better to navigate. [sent-77, score-0.055]
</p><p>38 As shown in Figure 3, users are also encouraged to set a title for each segment. [sent-78, score-0.081]
</p><p>39 Summarizing Pages Similarly to segmenting pages, Wikulu makes long wiki pages more accessible by generating an extractive summary. [sent-81, score-0.575]
</p><p>40 While generative summaries generate a summary in own words, extractive summaries analyze the original wiki text sentence-by-sentence, rank each sentence, and return a list of the most important ones (see Figure 4). [sent-82, score-0.557]
</p><p>41 Wikulu integrates extractive text summarization methods such as LexRank (Erkan and Radev, 2004). [sent-83, score-0.073]
</p><p>42 Highlighting Keyphrases Another approach to assist users in better grasping the idea of a wiki page at a glance is to highlight important keyphrases (see Figure 1). [sent-84, score-0.791]
</p><p>43 76  Figure 4: Extractive summary of the original wiki page shown in Figure 3 shown, highlighting important phrases assists users with reading longer texts and yields faster understanding. [sent-86, score-0.739]
</p><p>44 Wikulu thus improves readability by employing automatic keyphrase extraction algorithms. [sent-87, score-0.089]
</p><p>45 Additionally, Wikulu allows to dynamically adjust the number of keyphrases shown by presenting a slider to the user. [sent-88, score-0.09]
</p><p>46 We integrated keyphrase extraction methods such as TextRank (Mihalcea and Tarau,  2004) and KEA (Witten et al. [sent-89, score-0.089]
</p><p>47 3  Further Use Cases  Further use cases for supporting wiki users include (i) visually analyzing the results of NLP algorithms, (ii) educational purposes, and (iii) enabling semantic wikis. [sent-91, score-0.81]
</p><p>48 Visually Analyzing the Results of NLP Algorithms Wikulu facilitates analyzing the results of NLP algorithms by using wiki pages as input documents and visualizing the results directly on that page. [sent-92, score-0.577]
</p><p>49 This procedure suffers from two major drawbacks: (a) it is inconvenient to copy existing data into a custom input format which can be fed into the NLP system, and (b) the textual output does not allow presenting the results in a visually rich manner. [sent-95, score-0.097]
</p><p>50 Wikulu tackles both challenges by using wiki pages as input/output documents. [sent-96, score-0.528]
</p><p>51 For instance,  by running the sentiment analysis component right from within the wiki, its output can be written back to the originating wiki page, resulting in visually rich, possibly interactive presentations. [sent-97, score-0.657]
</p><p>52 Educational Purposes Wikulu is a handy tool for educational purposes as it allows to (a) rapidly create test data in a collaborative manner (see Section 2), and (b) visualize the results of NLP algorithms, as described above. [sent-98, score-0.061]
</p><p>53 Students can gather hands-on experience by experimenting with NLP components in an easy-to-use wiki system. [sent-99, score-0.55]
</p><p>54 Enabling Semantic Wikis Semantic wikis such as the Semantic MediaWiki (Kr o¨tzsch et al. [sent-104, score-0.208]
</p><p>55 , 2006) augment standard wikis with machine-readable semantic annotations of pages and links. [sent-105, score-0.25]
</p><p>56 As those annotations have to be entered manually, this step is often skipped by users which severely limits the usefulness of semantic wikis. [sent-106, score-0.105]
</p><p>57 by automatically suggesting the type of a link by means of relation detection or the type of a page by means of text categorization. [sent-109, score-0.123]
</p><p>58 4  System Architecture  In this section, we detail our system architecture and describe what is necessary to make NLP algorithms available through our system. [sent-111, score-0.065]
</p><p>59 It acts as an HTTP proxy server which intercepts the communication between the web browser and the target wiki engine, while it allows to run any Apache UIMA-compliant NLP component using an extensible plugin mechanism. [sent-115, score-0.995]
</p><p>60 Proxy Server Wikulu is designed to work with any underlying wiki engine such as MediaWiki or TWiki. [sent-117, score-0.597]
</p><p>61 Consequently, we implemented it as an HTTP proxy server which allows it to be enabled at any time by changing the proxy settings of a user’s web browser. [sent-118, score-0.268]
</p><p>62 9 The proxy server intercepts all requests between the user who interacts with her web browser, and the underlying wiki engine. [sent-119, score-0.862]
</p><p>63 For example, Wikulu passes certain requests to its language processing components, or augments the default wiki toolbar by additional commands. [sent-120, score-0.643]
</p><p>64 JavaScript Injection Wikulu modifies the requests between web browser and target wiki by injecting custom client-side JavaScript code. [sent-122, score-0.691]
</p><p>65 Wikulu is thus capable of altering the default behavior of the wiki engine, e. [sent-123, score-0.544]
</p><p>66 Section 2), adding novel behavior such as additional toolbar buttons or advanced input fields, or augmenting the originating web page after a certain request has been processed, e. [sent-126, score-0.233]
</p><p>67 It relies on Apache UIMAcompliant NLP components which use wiki pages (or parts thereof) as input texts. [sent-130, score-0.55]
</p><p>68 Wikulu offers a sophisticated plugin manager which takes care of dynamically loading those NLP components. [sent-131, score-0.19]
</p><p>69 The plugin loader is designed to run plugins either every time a wiki page loads, or manually by picking them from the augmented wiki toolbar. [sent-132, score-1.246]
</p><p>70 Via direct web remoting10, those components are made accessible through a JavaScript proxy object. [sent-134, score-0.131]
</p><p>71 Wikulu offers a generic language processing plugin which takes the current page contents 9The process of enabling a custom proxy server can be simplified by using web browser extensions such as Multiproxy Switch (https : / / addons . [sent-135, score-0.567]
</p><p>72 org  User  Figure 5: Wikulu acts as a proxy server which intercepts the communication between the web browser and the underlying wiki engine. [sent-139, score-0.874]
</p><p>73 Its plugin manager allows to integrate any Apache UIMA-compliant NLP component. [sent-140, score-0.173]
</p><p>74 To run a custom Apache UIMA-compliant NLP component with Wikulu, one just needs to plug that particular NLP component into the generic plugin. [sent-142, score-0.125]
</p><p>75 No further adaptations to  the generic plugin are necessary. [sent-143, score-0.133]
</p><p>76 However, more advanced users may create fully customized plugins. [sent-144, score-0.081]
</p><p>77 Wiki Abstraction Layer Wikulu communicates with the underlying wiki engine via an abstraction layer. [sent-145, score-0.624]
</p><p>78 That layer provides a generic interface for accessing and manipulating the underlying wiki engine. [sent-146, score-0.615]
</p><p>79 Thereby, Wikulu can both be tightly coupled to a certain wiki instance such as MediaWiki or TWiki, while being flexible at the same time to adapt to a changing environment. [sent-147, score-0.544]
</p><p>80 New adaptors for other target wiki engines such as Confluence11 can be added with minimal effort. [sent-148, score-0.58]
</p><p>81 2 Walk-Through Example Let’s assume that a user encounters a wiki page which is rather lengthy. [sent-150, score-0.637]
</p><p>82 She realizes that Wikulu’s keyphrase extraction component might help her to better grasp the idea of this page at a glance, so she activates Wikulu by setting her web browser to pass all requests through the proxy server. [sent-151, score-0.399]
</p><p>83 Wiki highlight  Figure 6: Illustration of Wikulu’s information flow when a user has requested to highlight keyphrases on the current page as described in Section 4. [sent-157, score-0.236]
</p><p>84 2 applying the settings, the JavaScript injection module adds additional links to the wiki’s toolbar on the originating wiki page. [sent-158, score-0.698]
</p><p>85 Having decided to apply keyphrase extraction, she then invokes that NLP component by clicking the corresponding link (see  Figure 6). [sent-159, score-0.198]
</p><p>86 Before the request is passed to that component, Wikulu extracts the wiki page contents using the high-level wiki abstraction layer. [sent-160, score-1.203]
</p><p>87 Thereafter, the request is passed via direct web remoting to the NLP component which has been loaded by Wikulu’s plugin mechanism. [sent-161, score-0.203]
</p><p>88 After processing the request, the extracted keyphrases are returned to Wikulu’s custom JavaScript handlers and finally highlighted in the originating wiki page. [sent-162, score-0.712]
</p><p>89 5  Related Work  Supporting wiki users with NLP techniques has not attracted a lot of research attention yet. [sent-163, score-0.609]
</p><p>90 They propose an architecture to connect wikis to services providing NLP functionality which are based on the General Architecture for Text Engineering (Cunningham et al. [sent-165, score-0.25]
</p><p>91 Contrary to Wikulu, though, their system does not integrate transparently with an underlying wiki engine, but rather uses a separate application to apply NLP techniques. [sent-167, score-0.6]
</p><p>92 Thereby, wiki users can leverage the power of NLP algorithms, but need to interrupt their cur-  rent workflow to switch to a different application. [sent-168, score-0.626]
</p><p>93 Moreover, their system is only loosely coupled with the underlying wiki engine. [sent-169, score-0.556]
</p><p>94 While it allows to read and write existing pages, it does not allow further modifications such as adding user interface controls. [sent-170, score-0.079]
</p><p>95 A lot ofwork in the wiki community is done in the context of Wikipedia. [sent-171, score-0.528]
</p><p>96 However, unlike Wikulu, FastestFox is tailored towards Wikipedia and cannot be used with any other wiki platform. [sent-173, score-0.528]
</p><p>97 6  Summary  We presented Wikulu, an extensible system which integrates natural language processing techniques with wikis. [sent-174, score-0.064]
</p><p>98 Wikulu addresses the major challenge of supporting wiki users with their everyday tasks. [sent-175, score-0.654]
</p><p>99 Besides that, we demonstrated how Wikulu serves as a flexible environment for (a) visually analyzing the results of NLP algorithms, (b) educational purposes, and (c) enabling semantic wikis. [sent-176, score-0.188]
</p><p>100 By its modular and flexible architecture, we envision that Wikulu can support wiki users both in small focused environments as well as in large-scale communities such as  Wikipedia. [sent-177, score-0.625]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wikulu', 0.716), ('wiki', 0.528), ('wikis', 0.208), ('plugin', 0.117), ('keyphrase', 0.089), ('proxy', 0.084), ('users', 0.081), ('server', 0.075), ('keyphrases', 0.074), ('page', 0.073), ('nlp', 0.069), ('toolbar', 0.065), ('javascript', 0.063), ('browser', 0.061), ('apache', 0.057), ('visually', 0.054), ('intercepts', 0.052), ('link', 0.05), ('integrates', 0.044), ('custom', 0.043), ('originating', 0.042), ('architecture', 0.042), ('engine', 0.041), ('highlighting', 0.039), ('injection', 0.039), ('manager', 0.038), ('user', 0.036), ('enabling', 0.035), ('adaptors', 0.034), ('mediawiki', 0.034), ('requests', 0.034), ('component', 0.033), ('educational', 0.033), ('cunningham', 0.032), ('gurevych', 0.03), ('textrank', 0.03), ('supporting', 0.029), ('extractive', 0.029), ('purposes', 0.028), ('request', 0.028), ('underlying', 0.028), ('abstraction', 0.027), ('invokes', 0.026), ('itakura', 0.026), ('kea', 0.026), ('leuf', 0.026), ('preproceedings', 0.026), ('transparently', 0.026), ('tucker', 0.026), ('witte', 0.026), ('analyzing', 0.026), ('highlighted', 0.025), ('duplicates', 0.025), ('web', 0.025), ('semantic', 0.024), ('links', 0.024), ('interface', 0.023), ('yellow', 0.023), ('inex', 0.023), ('darmstadt', 0.023), ('iryna', 0.023), ('torsten', 0.023), ('algorithms', 0.023), ('prototype', 0.022), ('components', 0.022), ('uima', 0.021), ('texttiling', 0.021), ('tarau', 0.021), ('org', 0.021), ('layer', 0.02), ('extensible', 0.02), ('modifications', 0.02), ('segment', 0.019), ('contents', 0.019), ('offers', 0.019), ('content', 0.019), ('ferrucci', 0.019), ('gabrilovich', 0.019), ('augment', 0.018), ('mihalcea', 0.018), ('engines', 0.018), ('highlight', 0.018), ('corporate', 0.018), ('kr', 0.018), ('reading', 0.018), ('segmenting', 0.018), ('integrate', 0.018), ('erkan', 0.017), ('switch', 0.017), ('lexrank', 0.017), ('requested', 0.017), ('glance', 0.017), ('generic', 0.016), ('witten', 0.016), ('everyday', 0.016), ('dynamically', 0.016), ('default', 0.016), ('suggestion', 0.016), ('flexible', 0.016), ('landauer', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="338-tfidf-1" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>Author: Daniel Bar ; Nicolai Erbs ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: We present Wikulu1, a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis.</p><p>2 0.12266383 <a title="338-tfidf-2" href="./acl-2011-Question_Detection_in_Spoken_Conversations_Using_Textual_Conversations.html">257 acl-2011-Question Detection in Spoken Conversations Using Textual Conversations</a></p>
<p>Author: Anna Margolis ; Mari Ostendorf</p><p>Abstract: We investigate the use of textual Internet conversations for detecting questions in spoken conversations. We compare the text-trained model with models trained on manuallylabeled, domain-matched spoken utterances with and without prosodic features. Overall, the text-trained model achieves over 90% of the performance (measured in Area Under the Curve) of the domain-matched model including prosodic features, but does especially poorly on declarative questions. We describe efforts to utilize unlabeled spoken utterances and prosodic features via domain adaptation.</p><p>3 0.10514564 <a title="338-tfidf-3" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>Author: Xin Zhao ; Jing Jiang ; Jing He ; Yang Song ; Palakorn Achanauparp ; Ee-Peng Lim ; Xiaoming Li</p><p>Abstract: Summarizing and analyzing Twitter content is an important and challenging task. In this paper, we propose to extract topical keyphrases as one way to summarize Twitter. We propose a context-sensitive topical PageRank method for keyword ranking and a probabilistic scoring function that considers both relevance and interestingness of keyphrases for keyphrase ranking. We evaluate our proposed methods on a large Twitter data set. Experiments show that these methods are very effective for topical keyphrase extraction.</p><p>4 0.052826326 <a title="338-tfidf-4" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.</p><p>5 0.045418963 <a title="338-tfidf-5" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<p>Author: Bo Pang ; Ravi Kumar</p><p>Abstract: Web search is an information-seeking activity. Often times, this amounts to a user seeking answers to a question. However, queries, which encode user’s information need, are typically not expressed as full-length natural language sentences in particular, as questions. Rather, they consist of one or more text fragments. As humans become more searchengine-savvy, do natural-language questions still have a role to play in web search? Through a systematic, large-scale study, we find to our surprise that as time goes by, web users are more likely to use questions to express their search intent. —</p><p>6 0.044045836 <a title="338-tfidf-6" href="./acl-2011-Wikipedia_Revision_Toolkit%3A_Efficiently_Accessing_Wikipedias_Edit_History.html">337 acl-2011-Wikipedia Revision Toolkit: Efficiently Accessing Wikipedias Edit History</a></p>
<p>7 0.041822866 <a title="338-tfidf-7" href="./acl-2011-An_Interface_for_Rapid_Natural_Language_Processing_Development_in_UIMA.html">42 acl-2011-An Interface for Rapid Natural Language Processing Development in UIMA</a></p>
<p>8 0.040294625 <a title="338-tfidf-8" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>9 0.033491328 <a title="338-tfidf-9" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>10 0.030735996 <a title="338-tfidf-10" href="./acl-2011-An_Interactive_Machine_Translation_System_with_Online_Learning.html">41 acl-2011-An Interactive Machine Translation System with Online Learning</a></p>
<p>11 0.030714869 <a title="338-tfidf-11" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>12 0.030493774 <a title="338-tfidf-12" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>13 0.028727552 <a title="338-tfidf-13" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>14 0.028599309 <a title="338-tfidf-14" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>15 0.028063158 <a title="338-tfidf-15" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<p>16 0.027592106 <a title="338-tfidf-16" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>17 0.026869537 <a title="338-tfidf-17" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>18 0.02668272 <a title="338-tfidf-18" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>19 0.024757423 <a title="338-tfidf-19" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>20 0.024258165 <a title="338-tfidf-20" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.067), (1, 0.036), (2, -0.01), (3, 0.026), (4, -0.044), (5, -0.001), (6, -0.009), (7, -0.024), (8, -0.005), (9, -0.002), (10, -0.021), (11, 0.01), (12, 0.009), (13, -0.035), (14, -0.032), (15, 0.007), (16, 0.044), (17, -0.013), (18, 0.019), (19, -0.024), (20, 0.022), (21, -0.038), (22, -0.059), (23, 0.006), (24, 0.023), (25, 0.005), (26, 0.003), (27, 0.017), (28, -0.027), (29, -0.028), (30, -0.019), (31, 0.055), (32, 0.04), (33, -0.042), (34, -0.022), (35, -0.032), (36, 0.008), (37, -0.052), (38, -0.015), (39, -0.003), (40, 0.033), (41, 0.026), (42, 0.001), (43, 0.059), (44, 0.072), (45, -0.013), (46, -0.016), (47, -0.006), (48, 0.019), (49, -0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88394785 <a title="338-lsi-1" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>Author: Daniel Bar ; Nicolai Erbs ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: We present Wikulu1, a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis.</p><p>2 0.66444892 <a title="338-lsi-2" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>Author: Andrei Popescu-Belis ; Majid Yazdani ; Alexandre Nanchen ; Philip N. Garner</p><p>Abstract: The Automatic Content Linking Device is a just-in-time document retrieval system which monitors an ongoing conversation or a monologue and enriches it with potentially related documents, including multimedia ones, from local repositories or from the Internet. The documents are found using keyword-based search or using a semantic similarity measure between documents and the words obtained from automatic speech recognition. Results are displayed in real time to meeting participants, or to users watching a recorded lecture or conversation.</p><p>3 0.54282707 <a title="338-lsi-3" href="./acl-2011-Language_of_Vandalism%3A_Improving_Wikipedia_Vandalism_Detection_via_Stylometric_Analysis.html">195 acl-2011-Language of Vandalism: Improving Wikipedia Vandalism Detection via Stylometric Analysis</a></p>
<p>Author: Manoj Harpalani ; Michael Hart ; Sandesh Signh ; Rob Johnson ; Yejin Choi</p><p>Abstract: Community-based knowledge forums, such as Wikipedia, are susceptible to vandalism, i.e., ill-intentioned contributions that are detrimental to the quality of collective intelligence. Most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in Wikipedia. In this paper, we explore more linguistically motivated approaches to vandalism detection. In particular, we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior. Experimental results suggest that (1) statistical models give evidence to unique language styles in vandalism, and that (2) deep syntactic patterns based on probabilistic context free grammars (PCFG) discriminate vandalism more effectively than shallow lexicosyntactic patterns based on n-grams. ,</p><p>4 0.53001922 <a title="338-lsi-4" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>Author: Benjamin Wing ; Jason Baldridge</p><p>Abstract: We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document’s raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset.</p><p>5 0.52883995 <a title="338-lsi-5" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In this paper we present Clairlib, an opensource toolkit for Natural Language Processing, Information Retrieval, and Network Analysis. Clairlib provides an integrated framework intended to simplify a number of generic tasks within and across those three areas. It has a command-line interface, a graphical interface, and a documented API. Clairlib is compatible with all the common platforms and operating systems. In addition to its own functionality, it provides interfaces to external software and corpora. Clairlib comes with a comprehensive documentation and a rich set of tutorials and visual demos.</p><p>6 0.52603477 <a title="338-lsi-6" href="./acl-2011-Wikipedia_Revision_Toolkit%3A_Efficiently_Accessing_Wikipedias_Edit_History.html">337 acl-2011-Wikipedia Revision Toolkit: Efficiently Accessing Wikipedias Edit History</a></p>
<p>7 0.48371533 <a title="338-lsi-7" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>8 0.47586471 <a title="338-lsi-8" href="./acl-2011-The_ACL_Anthology_Searchbench.html">298 acl-2011-The ACL Anthology Searchbench</a></p>
<p>9 0.46805471 <a title="338-lsi-9" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>10 0.46177533 <a title="338-lsi-10" href="./acl-2011-Towards_Style_Transformation_from_Written-Style_to_Audio-Style.html">306 acl-2011-Towards Style Transformation from Written-Style to Audio-Style</a></p>
<p>11 0.45645356 <a title="338-lsi-11" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>12 0.43560335 <a title="338-lsi-12" href="./acl-2011-An_Interface_for_Rapid_Natural_Language_Processing_Development_in_UIMA.html">42 acl-2011-An Interface for Rapid Natural Language Processing Development in UIMA</a></p>
<p>13 0.42713383 <a title="338-lsi-13" href="./acl-2011-Local_and_Global_Algorithms_for_Disambiguation_to_Wikipedia.html">213 acl-2011-Local and Global Algorithms for Disambiguation to Wikipedia</a></p>
<p>14 0.40254673 <a title="338-lsi-14" href="./acl-2011-N-Best_Rescoring_Based_on_Pitch-accent_Patterns.html">228 acl-2011-N-Best Rescoring Based on Pitch-accent Patterns</a></p>
<p>15 0.40086254 <a title="338-lsi-15" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>16 0.39886868 <a title="338-lsi-16" href="./acl-2011-Turn-Taking_Cues_in_a_Human_Tutoring_Corpus.html">312 acl-2011-Turn-Taking Cues in a Human Tutoring Corpus</a></p>
<p>17 0.39735886 <a title="338-lsi-17" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>18 0.39233434 <a title="338-lsi-18" href="./acl-2011-Question_Detection_in_Spoken_Conversations_Using_Textual_Conversations.html">257 acl-2011-Question Detection in Spoken Conversations Using Textual Conversations</a></p>
<p>19 0.37849605 <a title="338-lsi-19" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>20 0.37641647 <a title="338-lsi-20" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.012), (5, 0.043), (13, 0.013), (17, 0.029), (26, 0.052), (27, 0.021), (31, 0.011), (37, 0.049), (39, 0.023), (41, 0.064), (55, 0.018), (57, 0.027), (59, 0.019), (61, 0.011), (62, 0.306), (72, 0.023), (88, 0.01), (91, 0.054), (96, 0.1), (97, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79593742 <a title="338-lda-1" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<p>Author: Luc Boruta</p><p>Abstract: Allophonic rules are responsible for the great variety in phoneme realizations. Infants can not reliably infer abstract word representations without knowledge of their native allophonic grammar. We explore the hypothesis that some properties of infants’ input, referred to as indicators, are correlated with allophony. First, we provide an extensive evaluation of individual indicators that rely on distributional or lexical information. Then, we present a first evaluation of the combination of indicators of different types, considering both logical and numerical combinations schemes. Though distributional and lexical indicators are not redundant, straightforward combinations do not outperform individual indicators.</p><p>same-paper 2 0.75926131 <a title="338-lda-2" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>Author: Daniel Bar ; Nicolai Erbs ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: We present Wikulu1, a system focusing on supporting wiki users with their everyday tasks by means of an intelligent interface. Wikulu is implemented as an extensible architecture which transparently integrates natural language processing (NLP) techniques with wikis. It is designed to be deployed with any wiki platform, and the current prototype integrates a wide range of NLP algorithms such as keyphrase extraction, link discovery, text segmentation, summarization, or text similarity. Additionally, we show how Wikulu can be applied for visually analyzing the results of NLP algorithms, educational purposes, and enabling semantic wikis.</p><p>3 0.62114578 <a title="338-lda-3" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>4 0.57434934 <a title="338-lda-4" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>5 0.56060642 <a title="338-lda-5" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Meng Wang ; Tat-Seng Chua</p><p>Abstract: In this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. The important aspects are identified according to two observations: (a) the important aspects of a product are usually commented by a large number of consumers; and (b) consumers’ opinions on the important aspects greatly influence their overall opinions on the product. In particular, given consumer reviews of a product, we first identify the product aspects by a shallow dependency parser and determine consumers’ opinions on these aspects via a sentiment classifier. We then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the influence of consumers’ opinions given to each aspect on their overall opinions. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. We further apply the aspect ranking results to the application ofdocumentlevel sentiment classification, and improve the performance significantly.</p><p>6 0.44082576 <a title="338-lda-6" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>7 0.43610418 <a title="338-lda-7" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>8 0.43433321 <a title="338-lda-8" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>9 0.43254432 <a title="338-lda-9" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>10 0.43213278 <a title="338-lda-10" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>11 0.42725033 <a title="338-lda-11" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>12 0.42685246 <a title="338-lda-12" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>13 0.42400548 <a title="338-lda-13" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>14 0.42377758 <a title="338-lda-14" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>15 0.42363036 <a title="338-lda-15" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>16 0.4233692 <a title="338-lda-16" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>17 0.42330998 <a title="338-lda-17" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>18 0.42316043 <a title="338-lda-18" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>19 0.42304134 <a title="338-lda-19" href="./acl-2011-Wikipedia_Revision_Toolkit%3A_Efficiently_Accessing_Wikipedias_Edit_History.html">337 acl-2011-Wikipedia Revision Toolkit: Efficiently Accessing Wikipedias Edit History</a></p>
<p>20 0.42220536 <a title="338-lda-20" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
