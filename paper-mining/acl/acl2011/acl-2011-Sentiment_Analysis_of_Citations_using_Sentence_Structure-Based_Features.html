<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-281" href="#">acl2011-281</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</h1>
<br/><p>Source: <a title="acl-2011-281-pdf" href="http://aclweb.org/anthology//P/P11/P11-3015.pdf">pdf</a></p><p>Author: Awais Athar</p><p>Abstract: Sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-specific lexical features, dependency relations, sentence splitting and negation features. Our results show that 3-grams and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features.</p><p>Reference: <a title="acl-2011-281-reference" href="../acl2011_reference/acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. [sent-7, score-0.558]
</p><p>2 In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. [sent-8, score-1.153]
</p><p>3 Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-specific lexical features, dependency relations, sentence splitting and negation features. [sent-9, score-1.566]
</p><p>4 Our results show that 3-grams and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features. [sent-10, score-0.445]
</p><p>5 1 Introduction  Sentiment analysis is the task of identifying positive and negative opinions, sentiments, emotions and attitudes expressed in text. [sent-11, score-0.086]
</p><p>6 Although there has been in the past few years a growing interest in this field for different text genres such as newspaper text, reviews and narrative text, relatively less emphasis has been placed on extraction of opinions from scientific literature, more specifically, citations. [sent-12, score-0.227]
</p><p>7 Analysis of citation sentiment would open up many exciting new applications in bibliographic search and in bibliometrics, i. [sent-13, score-0.976]
</p><p>8 Existing bibliometric measures like H-Index (Hirsch, 2005) and adapted graph ranking algo81 rithms like PageRank (Radev et al. [sent-16, score-0.119]
</p><p>9 However, Bonzi (1982) argued that if a cited work is criticised, it should consequently carry lower or even negative weight for bibliometric measures. [sent-18, score-0.298]
</p><p>10 Automatic citation sentiment detection is a prerequisite for such a treatment. [sent-19, score-0.974]
</p><p>11 Moreover, citation sentiment detection can also help researchers during search, by detecting prob-  lems with a particular approach. [sent-20, score-1.0]
</p><p>12 It can be used as a first step to scientific summarisation, enable users to recognise unaddressed issues and possible gaps in the current research, and thus help them set their research directions. [sent-21, score-0.207]
</p><p>13 State-of-the-art systems report around 85-90% accuracy for different genres of text (Nakagawa et al. [sent-23, score-0.072]
</p><p>14 Given such good results, one might think that a sentence-based sentiment detection system trained on a different genre could be used equally well to classify citations. [sent-26, score-0.552]
</p><p>15 We argue that this might not be the case; our citation sentiment recogniser uses specialised training data and tests the performance of specialised features against current state-of-the-art features. [sent-27, score-1.244]
</p><p>16 The reasons for this are based on the fol-  lowing observations: • Sentiment in citations is often hidden. [sent-28, score-0.218]
</p><p>17 Ziman (1968) states that many works are cited out of “politeness, policy or piety”. [sent-32, score-0.134]
</p><p>18 Negative polarity is often expressed in contrastive terms, e. [sent-40, score-0.181]
</p><p>19 d Although tivhee sentiment is indirect in these cases, its negativity is implied by the fact that the authors’ own work is clearly evaluated positively in comparison. [sent-43, score-0.471]
</p><p>20 There is also much variation between scientific tTehxetsr a insd a ostohe mr genres concerning tnhe s ileenxtiicfiacl items chosen to convey sentiment. [sent-48, score-0.256]
</p><p>21 Sentiment carrying science-specific terms exist and are relatively frequent, which motivates the use of a sentiment lexicon specialised to science. [sent-49, score-0.666]
</p><p>22 For this reason, using higher order n-grams might prove to be useful in sentiment detection. [sent-56, score-0.511]
</p><p>23 •  82 The scope of influence of citations varies widely fTrhoem s a single cinlfaluuseen (as oinf cthitea example below) ltoy several paragraphs: As reported in Table 3, small increases in METEOR (Banerjee and Lavie, 2005), BLEU (Papineni et al. [sent-57, score-0.273]
</p><p>24 (2008) showed that assuming larger citation scopes has a positive effect in retrieval. [sent-63, score-0.568]
</p><p>25 , we assume short scopes and use a parser to split sentences, so that the features associated with the clauses not directly connected to the citation are disregarded. [sent-66, score-0.572]
</p><p>26 We created a new sentiment-annotated corpus of scientific text in the form of a sentence-based collection of over 8700 citations. [sent-67, score-0.155]
</p><p>27 Our results show that the most successful feature combination includes dependency features and n-grams longer than for other genres (n = 3), but the assumption of a smaller scope (sentence splitting) decreased results. [sent-69, score-0.253]
</p><p>28 2  Training and Test Corpus  We manually annotated 8736 citations from 3 10 research papers taken from the ACL Anthology (Bird et al. [sent-70, score-0.275]
</p><p>29 The citation summary data from the ACL Anthology Network1 (Radev et al. [sent-72, score-0.462]
</p><p>30 We identified the actual text of the citations by regular expressions and replaced it with a special token   in order to remove any lexical bias associated with proper names of researchers. [sent-74, score-0.218]
</p><p>31 We labelled each sentence as positive, negative or objective, and separated 1472 citations for development and training. [sent-75, score-0.3]
</p><p>32 The rest were used as the test set containing 244 negative, 743 positive and 6277 objective citations. [sent-76, score-0.07]
</p><p>33 Thus our dataset is heavily skewed, with subjective citations accounting for only around 14% of the corpus. [sent-77, score-0.248]
</p><p>34 org 3  Features  We represent each citation as a feature set in a Support Vector Machine (SVM) (Cortes and Vapnik, 1995) framework which has been shown to produce good results for sentiment classification (Pang et al. [sent-80, score-0.976]
</p><p>35 (2002), we use unigrams and bigrams as features and also add 3-grams as new features to capture longer technical terms. [sent-86, score-0.118]
</p><p>36 This may help in distinguishing between homonyms with different POS tags and signalling the presence of adjectives (e. [sent-88, score-0.1]
</p><p>37 Name of the primary author of the cited paper is also used as a feature. [sent-91, score-0.134]
</p><p>38 A science-specific sentiment lexicon is also added to the feature set. [sent-92, score-0.553]
</p><p>39 This lexicon consists of 83 polar phrases which have been manually extracted from the development set of 736 citations. [sent-93, score-0.199]
</p><p>40 Some of the most frequently occurring polar phrases in this set consists of adjectives such as efficient, popular, successful, state-of-the-art and effective. [sent-94, score-0.162]
</p><p>41 2 Contextual Polarity Features Features previously found to be useful for detecting phrase-level contextual polarity (Wilson et al. [sent-96, score-0.244]
</p><p>42 Since the task at hand is sentence-based, we use only the sentence-based features from the literature e. [sent-98, score-0.087]
</p><p>43 To handle negation, we include the count of negation phrases found within the citation sentence. [sent-101, score-0.758]
</p><p>44 The polarity shifter and negation phrase lists have been taken from the OpinionFinder system (Wilson et al. [sent-103, score-0.451]
</p><p>45 3  Sentence Structure Based Features  We explore three different feature sets which focus on the lexical and grammatical structure of a sentence and have not been explored previously for the task of sentiment analysis of scientific text. [sent-109, score-0.699]
</p><p>46 1 Dependency Structures The first set of these features include typed dependency structures (de Marneffe and Manning, 2008) which describe the grammatical relationships between words. [sent-112, score-0.162]
</p><p>47 For instance in the sentence below, the relationship between results and competitive will be missed by trigrams but the dependency representation captures it in a single feature nsub j compet it ive re sult s . [sent-114, score-0.09]
</p><p>48 2 Sentence Splitting Removing irrelevant polar phrases around a citation might improve results. [sent-119, score-0.619]
</p><p>49 For this purpose, we split each sentence by trimming its parse tree. [sent-120, score-0.085]
</p><p>50 Walking from the citation node ( ) towards the root, we select the subtree rooted at the first sentence node (S) and ignore the rest. [sent-121, score-0.554]
</p><p>51 For example, in Figure 1, the cited paper is not included in the scope of the discarded polar phrase significant improvements. [sent-122, score-0.28]
</p><p>52 3 Negation Dependencies and parse trees attach negation nodes, such as not, to the clause subtree and this shows no interaction with other nodes with respect to valence shifting. [sent-125, score-0.376]
</p><p>53 All words inside a k-word window of any negation term are suffixed with a token neg to distinguish them from their non-polar versions. [sent-127, score-0.382]
</p><p>54 For example, a 2word negation window inverts the polarity of the positive phrase work well in the sentence below. [sent-128, score-0.559]
</p><p>55 Turney ’s method did not work neg well neg although they reported 80% accuracy in  . [sent-129, score-0.164]
</p><p>56 The negation term list has been taken from the OpinionFinder system. [sent-130, score-0.27]
</p><p>57 Khan (2007) has shown that this approach produces results comparable to grammatical relations based negation models. [sent-131, score-0.342]
</p><p>58 Table 1: Results using science lexicon (scilex), contextual polarity (cpol), dependencies (dep), negation (neg), sentence splitting (split) and word-level (wlev) features. [sent-134, score-0.785]
</p><p>59 The selection of the features is on the basis of improvements over a baseline of 1-3 grams i. [sent-135, score-0.085]
</p><p>60 84 The results show that contextual polarity features do not work well on citation text. [sent-140, score-0.751]
</p><p>61 We find that word level and contextual polarity features are surpassed by dependency features. [sent-143, score-0.342]
</p><p>62 Sentence splitting does not help, possibly due to longer citation scope. [sent-144, score-0.586]
</p><p>63 Adding a negation window (k=15) improves the performance but the improvement was not found to be statistically significant. [sent-145, score-0.3]
</p><p>64 This might be due to skewed class distribution and a larger dataset may prove to be useful. [sent-146, score-0.117]
</p><p>65 5  Related Work  While different schemes have been proposed for annotating citations according to their function (Spiegel-R o¨sing, 1977; Nanba and Okumura, 1999; Garzone and Mercer, 2000), there have been no attempts on citation sentiment detection in a large corpus. [sent-147, score-1.192]
</p><p>66 (2006) worked on a 2829 sentence ci-  tation corpus using a 12-class classification scheme. [sent-149, score-0.08]
</p><p>67 However, this corpus has been annotated for the task of determining the author’s reason for citing a given paper and is thus built on top of sentiment of citation. [sent-150, score-0.56]
</p><p>68 It considers usage, modification and similarity with a cited paper as positive even when there is no sentiment attributed to it. [sent-151, score-0.646]
</p><p>69 Moreover, contrast between two cited methods (CoCoXY) is categorized as objective in the annotation scheme even if the text indicates that one method performs better than the other. [sent-152, score-0.163]
</p><p>70 For example, the sentence below talks about a positive attribute but is marked as neutral in the scheme. [sent-153, score-0.078]
</p><p>71 Using this corpus is thus more likely to lead to inconsistent representation of sentiment in any system which relies on lexical features. [sent-157, score-0.471]
</p><p>72 (2006) group the 12 categories into 3 in an attempt to perform a rough approximation of sentiment analysis over the classifications and report a 0. [sent-159, score-0.471]
</p><p>73 Unfortunately, we have ac-  cess to only a subset3 of this citation function corpus. [sent-161, score-0.462]
</p><p>74 We have extracted 1-3 grams, dependencies and negation features from the reduced citation function dataset and used them in our system with 10-fold cross-validation. [sent-162, score-0.863]
</p><p>75 When this subset is used to test the system trained on our newly annotated corpus, a low macro-F score of 0. [sent-167, score-0.064]
</p><p>76 Therefore, we can infer that citation sentiment classification is different from citation function classification. [sent-170, score-1.438]
</p><p>77 Other approaches to citation annotation and classification include Wilbur et al. [sent-171, score-0.505]
</p><p>78 (2006) who annotated a small 101 sentence corpus on focus, polarity, certainty, evidence and directionality. [sent-172, score-0.064]
</p><p>79 (2007) proposed a system to attach sentiment information to the citation links between biomedical papers. [sent-174, score-1.001]
</p><p>80 Different dependency relations have been explored by Dave et al. [sent-175, score-0.089]
</p><p>81 (2010) report that using dependencies on conditional random fields with lexicon based po-  larity reversal results in improvements over n-grams for news and reviews corpora. [sent-180, score-0.138]
</p><p>82 A common approach is to use a sentiment labelled lexicon to score sentences (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Yu and Hatzivassiloglou, 2003). [sent-181, score-0.553]
</p><p>83 Research suggests that creating a general sentiment classifier is a difficult task and existing approaches are highly topic dependent (Engstr o¨m, 2004; Gamon and Aue, 2005; Blitzer et al. [sent-182, score-0.471]
</p><p>84 6  Conclusion  In this paper, we focus on automatic identification of sentiment polarity in citations. [sent-184, score-0.694]
</p><p>85 Using a newly constructed annotated citation sentiment corpus, we examine the effectiveness of existing and novel features, including n-grams, scientific lexicon, dependency relations and sentence splitting. [sent-185, score-1.278]
</p><p>86 Our results show that 3-grams and dependencies perform best in this task; they outperform the scientific lexicon and the sentence splitting features. [sent-186, score-0.426]
</p><p>87 Future direc3This subset contains 591 positive, 59 negative and 1259 objective citations. [sent-187, score-0.074]
</p><p>88 New techniques for detection of the negation scope such as the one proposed by Councill et al. [sent-189, score-0.366]
</p><p>89 Exploring longer citation scopes by including citation contexts might also improve citation sentiment detection. [sent-191, score-1.99]
</p><p>90 The acl anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. [sent-209, score-0.135]
</p><p>91 Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. [sent-218, score-0.471]
</p><p>92 Characteristics of a literature as predictors of relatedness between cited and citing works. [sent-223, score-0.238]
</p><p>93 What’s great and what’s not: learning to classify the scope of negation for improved sentiment analysis. [sent-250, score-0.796]
</p><p>94 Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms. [sent-289, score-1.017]
</p><p>95 An index to quantify an individual’s scientific research output. [sent-318, score-0.155]
</p><p>96 Dependency tree-based sentiment classification using CRFs with hidden variables. [sent-366, score-0.514]
</p><p>97 Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. [sent-380, score-0.085]
</p><p>98 Discovering fine-grained sentiment with latent variable structured prediction models. [sent-435, score-0.471]
</p><p>99 Recognizing Contextual Polarity: an exploration of features for  phrase-level sentiment analysis. [sent-489, score-0.516]
</p><p>100 Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. [sent-503, score-0.309]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sentiment', 0.471), ('citation', 0.462), ('negation', 0.27), ('citations', 0.218), ('polarity', 0.181), ('scientific', 0.155), ('cited', 0.134), ('bibliometric', 0.119), ('specialised', 0.113), ('splitting', 0.096), ('polar', 0.091), ('macroberts', 0.089), ('teufel', 0.086), ('neg', 0.082), ('lexicon', 0.082), ('wilson', 0.08), ('genres', 0.072), ('scopes', 0.065), ('nakagawa', 0.065), ('opinion', 0.064), ('contextual', 0.063), ('anthology', 0.062), ('citing', 0.062), ('radev', 0.061), ('engstr', 0.06), ('garzone', 0.06), ('honavar', 0.06), ('justeson', 0.06), ('opinionfinder', 0.06), ('piao', 0.06), ('polanyi', 0.06), ('ritchie', 0.06), ('scilex', 0.06), ('wilbur', 0.06), ('libsvm', 0.059), ('thompson', 0.057), ('dependencies', 0.056), ('scope', 0.055), ('dependency', 0.053), ('gibson', 0.052), ('weka', 0.051), ('hatzivassiloglou', 0.05), ('cortes', 0.048), ('trimming', 0.048), ('dave', 0.048), ('ackstr', 0.048), ('thumbs', 0.048), ('skewed', 0.047), ('nanba', 0.045), ('councill', 0.045), ('adjectives', 0.045), ('features', 0.045), ('negative', 0.045), ('blitzer', 0.043), ('classification', 0.043), ('bibliographic', 0.043), ('yessenalina', 0.043), ('identification', 0.042), ('literature', 0.042), ('positive', 0.041), ('detection', 0.041), ('might', 0.04), ('valence', 0.04), ('grams', 0.04), ('cambridge', 0.039), ('sentence', 0.037), ('pang', 0.037), ('wiebe', 0.037), ('attach', 0.037), ('newly', 0.037), ('grammatical', 0.036), ('relations', 0.036), ('bird', 0.034), ('sentiments', 0.034), ('association', 0.033), ('marneffe', 0.033), ('gamon', 0.032), ('biomedical', 0.031), ('joshi', 0.031), ('turney', 0.031), ('social', 0.031), ('papers', 0.03), ('window', 0.03), ('adverbs', 0.03), ('dataset', 0.03), ('subtree', 0.029), ('orientation', 0.029), ('concerning', 0.029), ('presence', 0.029), ('objective', 0.029), ('pos', 0.028), ('longer', 0.028), ('emnlp', 0.028), ('typed', 0.028), ('annotated', 0.027), ('phrases', 0.026), ('towards', 0.026), ('help', 0.026), ('hedging', 0.026), ('unaddressed', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="281-tfidf-1" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>Author: Awais Athar</p><p>Abstract: Sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-specific lexical features, dependency relations, sentence splitting and negation features. Our results show that 3-grams and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features.</p><p>2 0.41787544 <a title="281-tfidf-2" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>3 0.33630368 <a title="281-tfidf-3" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>Author: Andrew L. Maas ; Raymond E. Daly ; Peter T. Pham ; Dan Huang ; Andrew Y. Ng ; Christopher Potts</p><p>Abstract: Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semanticterm–documentinformation as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset , of movie reviews to serve as a more robust benchmark for work in this area.</p><p>4 0.33242303 <a title="281-tfidf-4" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>Author: Danushka Bollegala ; David Weir ; John Carroll</p><p>Abstract: We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automat- ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.</p><p>5 0.3008869 <a title="281-tfidf-5" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>Author: Clinton Burfoot ; Steven Bird ; Timothy Baldwin</p><p>Abstract: This paper explores approaches to sentiment classification of U.S. Congressional floordebate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.</p><p>6 0.28741983 <a title="281-tfidf-6" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>7 0.27768138 <a title="281-tfidf-7" href="./acl-2011-Semi-supervised_latent_variable_models_for_sentence-level_sentiment_analysis.html">279 acl-2011-Semi-supervised latent variable models for sentence-level sentiment analysis</a></p>
<p>8 0.27503645 <a title="281-tfidf-8" href="./acl-2011-PsychoSentiWordNet.html">253 acl-2011-PsychoSentiWordNet</a></p>
<p>9 0.26246464 <a title="281-tfidf-9" href="./acl-2011-Dr_Sentiment_Knows_Everything%21.html">105 acl-2011-Dr Sentiment Knows Everything!</a></p>
<p>10 0.24420771 <a title="281-tfidf-10" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>11 0.23451532 <a title="281-tfidf-11" href="./acl-2011-Semantic_Representation_of_Negation_Using_Focus_Detection.html">273 acl-2011-Semantic Representation of Negation Using Focus Detection</a></p>
<p>12 0.22016416 <a title="281-tfidf-12" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>13 0.22010523 <a title="281-tfidf-13" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>14 0.21671182 <a title="281-tfidf-14" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>15 0.20503928 <a title="281-tfidf-15" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<p>16 0.19976354 <a title="281-tfidf-16" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>17 0.16176839 <a title="281-tfidf-17" href="./acl-2011-Automatic_Extraction_of_Lexico-Syntactic_Patterns_for_Detection_of_Negation_and_Speculation_Scopes.html">50 acl-2011-Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation and Speculation Scopes</a></p>
<p>18 0.1606892 <a title="281-tfidf-18" href="./acl-2011-Subjectivity_and_Sentiment_Analysis_of_Modern_Standard_Arabic.html">289 acl-2011-Subjectivity and Sentiment Analysis of Modern Standard Arabic</a></p>
<p>19 0.15468337 <a title="281-tfidf-19" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>20 0.15165317 <a title="281-tfidf-20" href="./acl-2011-Liars_and_Saviors_in_a_Sentiment_Annotated_Corpus_of_Comments_to_Political_Debates.html">211 acl-2011-Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.247), (1, 0.363), (2, 0.318), (3, -0.132), (4, 0.075), (5, 0.039), (6, -0.028), (7, 0.048), (8, 0.005), (9, -0.114), (10, 0.047), (11, -0.085), (12, -0.193), (13, 0.046), (14, -0.126), (15, -0.004), (16, 0.005), (17, -0.034), (18, 0.084), (19, -0.078), (20, -0.086), (21, -0.081), (22, 0.07), (23, -0.095), (24, 0.12), (25, 0.13), (26, -0.096), (27, 0.245), (28, -0.237), (29, 0.037), (30, -0.088), (31, 0.039), (32, -0.004), (33, 0.059), (34, -0.01), (35, -0.033), (36, -0.009), (37, -0.023), (38, 0.049), (39, 0.033), (40, -0.037), (41, -0.013), (42, 0.018), (43, 0.03), (44, -0.016), (45, 0.028), (46, 0.013), (47, -0.046), (48, 0.011), (49, -0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95204294 <a title="281-lsi-1" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>Author: Awais Athar</p><p>Abstract: Sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-specific lexical features, dependency relations, sentence splitting and negation features. Our results show that 3-grams and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features.</p><p>2 0.69750977 <a title="281-lsi-2" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>Author: Clinton Burfoot ; Steven Bird ; Timothy Baldwin</p><p>Abstract: This paper explores approaches to sentiment classification of U.S. Congressional floordebate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.</p><p>3 0.65231228 <a title="281-lsi-3" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>4 0.61065942 <a title="281-lsi-4" href="./acl-2011-Semi-supervised_latent_variable_models_for_sentence-level_sentiment_analysis.html">279 acl-2011-Semi-supervised latent variable models for sentence-level sentiment analysis</a></p>
<p>Author: Oscar Tackstrom ; Ryan McDonald</p><p>Abstract: We derive two variants of a semi-supervised model for fine-grained sentiment analysis. Both models leverage abundant natural supervision in the form of review ratings, as well as a small amount of manually crafted sentence labels, to learn sentence-level sentiment classifiers. The proposed model is a fusion of a fully supervised structured conditional model and its partially supervised counterpart. This allows for highly efficient estimation and inference algorithms with rich feature definitions. We describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence-level sentiment analysis compared to all baselines. 1 Sentence-level sentiment analysis In this paper, we demonstrate how combining coarse-grained and fine-grained supervision benefits sentence-level sentiment analysis an important task in the field of opinion classification and retrieval (Pang and Lee, 2008). Typical supervised learning approaches to sentence-level sentiment analysis rely on sentence-level supervision. While such fine-grained supervision rarely exist naturally, and thus requires labor intensive manual annotation effort (Wiebe et al., 2005), coarse-grained supervision is naturally abundant in the form of online review ratings. This coarse-grained supervision is, of course, less informative compared to fine-grained supervision, however, by combining a small amount of sentence-level supervision with a large amount of document-level supervision, we are able to substantially improve on the sentence-level classification task. Our work combines two strands of research: models for sentiment analysis that take document structure into account; – 569 Ryan McDonald Google, Inc., New York ryanmcd@ google com . and models that use latent variables to learn unobserved phenomena from that which can be observed. Exploiting document structure for sentiment analysis has attracted research attention since the early work of Pang and Lee (2004), who performed minimal cuts in a sentence graph to select subjective sentences. McDonald et al. (2007) later showed that jointly learning fine-grained (sentence) and coarsegrained (document) sentiment improves predictions at both levels. More recently, Yessenalina et al. (2010) described how sentence-level latent variables can be used to improve document-level prediction and Nakagawa et al. (2010) used latent variables over syntactic dependency trees to improve sentence-level prediction, using only labeled sentences for training. In a similar vein, Sauper et al. (2010) integrated generative content structure models with discriminative models for multi-aspect sentiment summarization and ranking. These approaches all rely on the availability of fine-grained annotations, but Ta¨ckstro¨m and McDonald (201 1) showed that latent variables can be used to learn fine-grained sentiment using only coarse-grained supervision. While this model was shown to beat a set of natural baselines with quite a wide margin, it has its shortcomings. Most notably, due to the loose constraints provided by the coarse supervision, it tends to only predict the two dominant fine-grained sentiment categories well for each document sentiment category, so that almost all sentences in positive documents are deemed positive or neutral, and vice versa for negative documents. As a way of overcoming these shortcomings, we propose to fuse a coarsely supervised model with a fully supervised model. Below, we describe two ways of achieving such a combined model in the framework of structured conditional latent variable models. Contrary to (generative) topic models (Mei et al., 2007; Titov and Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o.c?i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 569–574, Figure 1: a) Factor graph of the fully observed graphical model. b) Factor graph of the corresponding latent variable model. During training, shaded nodes are observed, while non-shaded nodes are unobserved. The input sentences si are always observed. Note that there are no factors connecting the document node, yd, with the input nodes, s, so that the sentence-level variables, ys, in effect form a bottleneck between the document sentiment and the input sentences. McDonald, 2008; Lin and He, 2009), structured conditional models can handle rich and overlapping features and allow for exact inference and simple gradient based estimation. The former models are largely orthogonal to the one we propose in this work and combining their merits might be fruitful. As shown by Sauper et al. (2010), it is possible to fuse generative document structure models and task specific structured conditional models. While we do model document structure in terms of sentiment transitions, we do not model topical structure. An interesting avenue for future work would be to extend the model of Sauper et al. (2010) to take coarse-grained taskspecific supervision into account, while modeling fine-grained task-specific aspects with latent variables. Note also that the proposed approach is orthogonal to semi-supervised and unsupervised induction of context independent (prior polarity) lexicons (Turney, 2002; Kim and Hovy, 2004; Esuli and Sebastiani, 2009; Rao and Ravichandran, 2009; Velikovich et al., 2010). The output of such models could readily be incorporated as features in the proposed model. 1.1 Preliminaries Let d be a document consisting of n sentences, s = (si)in=1, with a document–sentence-sequence pair denoted d = (d, s). Let yd = (yd, ys) denote random variables1 the document level sentiment, yd, and the sequence of sentence level sentiment, = (ysi)in=1 . – ys 1We are abusing notation throughout by using the same symbols to refer to random variables and their particular assignments. 570 In what follows, we assume that we have access to two training sets: a small set of fully labeled instances, DF = {(dj, and a large set of ydj)}jm=f1, coarsely labeled instances DC = {(dj, yjd)}jm=fm+fm+c1. Furthermore, we assume that yd and all yis take values in {POS, NEG, NEU}. We focus on structured conditional models in the exponential family, with the standard parametrization pθ(yd,ys|s) = expnhφ(yd,ys,s),θi − Aθ(s)o</p><p>5 0.59925807 <a title="281-lsi-5" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>Author: Danushka Bollegala ; David Weir ; John Carroll</p><p>Abstract: We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automat- ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.</p><p>6 0.59832746 <a title="281-lsi-6" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<p>7 0.58783704 <a title="281-lsi-7" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>8 0.58018219 <a title="281-lsi-8" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>9 0.576666 <a title="281-lsi-9" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>10 0.55406356 <a title="281-lsi-10" href="./acl-2011-PsychoSentiWordNet.html">253 acl-2011-PsychoSentiWordNet</a></p>
<p>11 0.53503388 <a title="281-lsi-11" href="./acl-2011-Semantic_Representation_of_Negation_Using_Focus_Detection.html">273 acl-2011-Semantic Representation of Negation Using Focus Detection</a></p>
<p>12 0.5333119 <a title="281-lsi-12" href="./acl-2011-Dr_Sentiment_Knows_Everything%21.html">105 acl-2011-Dr Sentiment Knows Everything!</a></p>
<p>13 0.52619308 <a title="281-lsi-13" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>14 0.51825601 <a title="281-lsi-14" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>15 0.48430529 <a title="281-lsi-15" href="./acl-2011-Automatic_Extraction_of_Lexico-Syntactic_Patterns_for_Detection_of_Negation_and_Speculation_Scopes.html">50 acl-2011-Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation and Speculation Scopes</a></p>
<p>16 0.47926539 <a title="281-lsi-16" href="./acl-2011-Subjectivity_and_Sentiment_Analysis_of_Modern_Standard_Arabic.html">289 acl-2011-Subjectivity and Sentiment Analysis of Modern Standard Arabic</a></p>
<p>17 0.47867179 <a title="281-lsi-17" href="./acl-2011-Liars_and_Saviors_in_a_Sentiment_Annotated_Corpus_of_Comments_to_Political_Debates.html">211 acl-2011-Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates</a></p>
<p>18 0.46354735 <a title="281-lsi-18" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>19 0.44267878 <a title="281-lsi-19" href="./acl-2011-A_Corpus_of_Scope-disambiguated_English_Text.html">8 acl-2011-A Corpus of Scope-disambiguated English Text</a></p>
<p>20 0.43700844 <a title="281-lsi-20" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.216), (5, 0.045), (17, 0.036), (26, 0.035), (37, 0.135), (39, 0.044), (41, 0.052), (53, 0.012), (55, 0.028), (59, 0.051), (72, 0.035), (88, 0.014), (89, 0.013), (91, 0.036), (96, 0.168)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94146168 <a title="281-lda-1" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>Author: Kenneth Hild ; Umut Orhan ; Deniz Erdogmus ; Brian Roark ; Barry Oken ; Shalini Purwar ; Hooman Nezamfar ; Melanie Fried-Oken</p><p>Abstract: Event related potentials (ERP) corresponding to stimuli in electroencephalography (EEG) can be used to detect the intent of a person for brain computer interfaces (BCI). This paradigm is widely used to build letter-byletter text input systems using BCI. Nevertheless using a BCI-typewriter depending only on EEG responses will not be sufficiently accurate for single-trial operation in general, and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed. Hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed. In this demonstration we will present a BCI system for typing that integrates a stochastic language model with ERP classification to achieve speedups, via the rapid serial visual presentation (RSVP) paradigm.</p><p>2 0.92327374 <a title="281-lda-2" href="./acl-2011-A_Comprehensive_Dictionary_of_Multiword_Expressions.html">6 acl-2011-A Comprehensive Dictionary of Multiword Expressions</a></p>
<p>Author: Kosho Shudo ; Akira Kurahone ; Toshifumi Tanabe</p><p>Abstract: It has been widely recognized that one of the most difficult and intriguing problems in natural language processing (NLP) is how to cope with idiosyncratic multiword expressions. This paper presents an overview of the comprehensive dictionary (JDMWE) of Japanese multiword expressions. The JDMWE is characterized by a large notational, syntactic, and semantic diversity of contained expressions as well as a detailed description of their syntactic functions, structures, and flexibilities. The dictionary contains about 104,000 expressions, potentially 750,000 expressions. This paper shows that the JDMWE’s validity can be supported by comparing the dictionary with a large-scale Japanese N-gram frequency dataset, namely the LDC2009T08, generated by Google Inc. (Kudo et al. 2009). 1</p><p>3 0.84465754 <a title="281-lda-3" href="./acl-2011-Learning_Sub-Word_Units_for_Open_Vocabulary_Speech_Recognition.html">203 acl-2011-Learning Sub-Word Units for Open Vocabulary Speech Recognition</a></p>
<p>Author: Carolina Parada ; Mark Dredze ; Abhinav Sethy ; Ariya Rastrow</p><p>Abstract: Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of subword units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.</p><p>4 0.8392837 <a title="281-lda-4" href="./acl-2011-Identification_of_Domain-Specific_Senses_in_a_Machine-Readable_Dictionary.html">158 acl-2011-Identification of Domain-Specific Senses in a Machine-Readable Dictionary</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki</p><p>Abstract: This paper focuses on domain-specific senses and presents a method for assigning category/domain label to each sense of words in a dictionary. The method first identifies each sense of a word in the dictionary to its corresponding category. We used a text classification technique to select appropriate senses for each domain. Then, senses were scored by computing the rank scores. We used Markov Random Walk (MRW) model. The method was tested on English and Japanese resources, WordNet 3.0 and EDR Japanese dictionary. For evaluation of the method, we compared English results with the Subject Field Codes (SFC) resources. We also compared each English and Japanese results to the first sense heuristics in the WSD task. These results suggest that identification of domain-specific senses (IDSS) may actually be of benefit.</p><p>same-paper 5 0.83266556 <a title="281-lda-5" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>Author: Awais Athar</p><p>Abstract: Sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-specific lexical features, dependency relations, sentence splitting and negation features. Our results show that 3-grams and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features.</p><p>6 0.78891212 <a title="281-lda-6" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>7 0.73466921 <a title="281-lda-7" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>8 0.73216778 <a title="281-lda-8" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>9 0.73167109 <a title="281-lda-9" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>10 0.7305209 <a title="281-lda-10" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>11 0.73036331 <a title="281-lda-11" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>12 0.72952813 <a title="281-lda-12" href="./acl-2011-Subjectivity_and_Sentiment_Analysis_of_Modern_Standard_Arabic.html">289 acl-2011-Subjectivity and Sentiment Analysis of Modern Standard Arabic</a></p>
<p>13 0.72888994 <a title="281-lda-13" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>14 0.72856772 <a title="281-lda-14" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>15 0.72710443 <a title="281-lda-15" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>16 0.72696602 <a title="281-lda-16" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>17 0.7267518 <a title="281-lda-17" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>18 0.72552311 <a title="281-lda-18" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>19 0.72549748 <a title="281-lda-19" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>20 0.72538543 <a title="281-lda-20" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
