<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>297 acl-2011-That's What She Said: Double Entendre Identification</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-297" href="#">acl2011-297</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>297 acl-2011-That's What She Said: Double Entendre Identification</h1>
<br/><p>Source: <a title="acl-2011-297-pdf" href="http://aclweb.org/anthology//P/P11/P11-2016.pdf">pdf</a></p><p>Author: Chloe Kiddon ; Yuriy Brun</p><p>Abstract: Humor identification is a hard natural language understanding problem. We identify a subproblem — the “that’s what she said” problem with two distinguishing characteristics: (1) use of nouns that are euphemisms for sexually explicit nouns and (2) structure common in the erotic domain. We address this problem in a classification approach that includes features that model those two characteristics. Experiments on web data demonstrate that our approach improves precision by 12% over baseline techniques that use only word-based features. —</p><p>Reference: <a title="acl-2011-297-reference" href="../acl2011_reference/acl-2011-That%27s_What_She_Said%3A_Double_Entendre_Identification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 That’s What She Said: Double Entendre Identification Chlo e´ Kiddon and Yuriy Brun Computer Science & Engineering University of Washington Seattle WA 98195-2350  {chloe  ,brun}@ cs  Abstract Humor identification is a hard natural language understanding problem. [sent-1, score-0.042]
</p><p>2 We identify a subproblem — the “that’s what she said” problem with two distinguishing characteristics: (1) use of nouns that are euphemisms for sexually explicit nouns and (2) structure common in the erotic domain. [sent-2, score-0.855]
</p><p>3 Experiments on web data demonstrate that our approach improves precision by 12% over baseline techniques that use only word-based features. [sent-4, score-0.037]
</p><p>4 The jokes consist of saying “that’s what she said” after someone  else utters a statement in a non-sexual context that could also have been used in a sexual context. [sent-7, score-0.127]
</p><p>5 For example, if Aaron refers to his late-evening basketball practice, saying “I was trying all night, but I just could not get it in! [sent-8, score-0.045]
</p><p>6 While somewhat juvenile, this joke presents an interesting natural language understanding problem. [sent-10, score-0.049]
</p><p>7 A “that’s what she said” (TWSS) joke is a type of double entendre. [sent-11, score-0.15]
</p><p>8 A double entendre, or adianoeta, is an expression that can be understood in two different ways: an innocuous, straightforward way, given the context, and a risqu ´e way that indirectly alludes to a different, indecent context. [sent-12, score-0.101]
</p><p>9 edu related research has not studied the task of identifying double entendres in text or speech. [sent-15, score-0.134]
</p><p>10 The task is complex and would require both deep semantic and cultural understanding to recognize the vast array of double entendres. [sent-16, score-0.101]
</p><p>11 We focus on a subtask of double entendre identification: TWSS recognition. [sent-17, score-0.216]
</p><p>12 We frame the problem of TWSS recognition as a type of metaphor identification. [sent-19, score-0.1]
</p><p>13 A metaphor is a figure of speech that creates an analogical mapping between two conceptual domains so that the terminology of one (source) domain can be used to describe situations and objects in the other (target) domain. [sent-20, score-0.168]
</p><p>14 Usage of the source domain’s terminology in the source domain is literal and is nonliteral in the target domain. [sent-21, score-0.101]
</p><p>15 Metaphor identification systems seek to differentiate between literal and nonliteral expressions. [sent-22, score-0.117]
</p><p>16 Some computational approaches to metaphor identification learn selectional preferences of words in multiple domains to help identify nonliteral usage (Mason, 2004; Shutova, 2010). [sent-23, score-0.226]
</p><p>17 Other approaches train support vector machine (SVM) models on labeled training data to distinguish metaphoric language from literal language (Pasanek and Sculley, 2008). [sent-24, score-0.061]
</p><p>18 TWSSs also represent mappings between two domains: the innocuous source domain and an erotic target domain. [sent-25, score-0.321]
</p><p>19 Therefore, we can apply methods from metaphor identification to TWSS identification. [sent-26, score-0.142]
</p><p>20 In particular, we (1) compare the adjectival selectional preferences of sexually explicit nouns to those of other nouns to determine which nouns may  be euphemisms for sexually explicit nouns and (2) Proceedings ofP tohretl 4an9tdh, O Anrneguoanl, M Jueentein 19g- o2f4 t,h 2e0 A1s1s. [sent-27, score-0.965]
</p><p>21 cc ia2t0io1n1 f Aors Cocoimatpiounta ftoiorn Caolm Lipnugtuaitsiotincasl:s Lhionrgtpuaisptiecrs , pages 89–94, examine the relationship between structures in the erotic domain and nonerotic contexts. [sent-29, score-0.386]
</p><p>22 We present a novel approach Double Entendre via Noun Transfer (DEviaNT) that applies metaphor identification techniques to solving the double entendre problem and evaluate it on the TWSS problem. [sent-30, score-0.389]
</p><p>23 DEviaNT classifies individual sentences as either funny if followed by “that’s what she said” or not, which is a type of automatic humor recognition (Mihalcea and Strapparava, 2005; Mihalcea and Pulman, 2007). [sent-31, score-0.095]
</p><p>24 We argue that in the TWSS domain, high precision is important, while low recall may be tolerated. [sent-32, score-0.037]
</p><p>25 In experiments on nearly 21K sentences, we find that DEviaNT has 12% higher precision than that of baseline classifiers that use n-gram TWSS models. [sent-33, score-0.059]
</p><p>26 First, sentences with nouns that are euphemisms for sexually explicit nouns are more likely to be TWSSs. [sent-39, score-0.589]
</p><p>27 For example, containing the noun “banana” makes a sentence more likely to be a TWSS than containing the noun “door” . [sent-40, score-0.175]
</p><p>28 Second, TWSSs share common structure with sentences in the erotic domain. [sent-41, score-0.302]
</p><p>29 Thus, we hypothesize that machine learning with euphemismand structure-based features is a promising approach to solving the TWSS problem. [sent-43, score-0.051]
</p><p>30 Accordingly, apart from a few basic features that define a TWSS joke (e. [sent-44, score-0.09]
</p><p>31 , short sentence), all of our approach’s lexical features model a metaphorical mapping to objects and structures in the erotic domain. [sent-46, score-0.328]
</p><p>32 Part of TWSS identification is recognizing that the source context in which the potential TWSS is uttered is not in an erotic one. [sent-47, score-0.304]
</p><p>33 If it is, then the mapping to the erotic domain is the identity and the state-  ment is not a TWSS. [sent-48, score-0.307]
</p><p>34 In this paper, we assume all test instances are from nonerotic domains and leave the 90 classification of erotic and nonerotic contexts to future work. [sent-49, score-0.481]
</p><p>35 First, many domains in which a TWSS classifier could be applied value high precision significantly more than high recall. [sent-51, score-0.06]
</p><p>36 For example, in a social setting, the cost of saying “that’s what she said” inappropriately is high, whereas the cost of not saying it when it might have been appropriate is negligible. [sent-52, score-0.09]
</p><p>37 For another example, in automated public tagging of twitter and facebook data, false positives are considered spam and violate usage policies, whereas false negatives go unnoticed. [sent-53, score-0.175]
</p><p>38 Second, the overwhelming majority of everyday sentences are not TWSSs, making achieving high precision even more difficult. [sent-54, score-0.06]
</p><p>39 In this paper, we strive specifically to achieve high precision but are willing to sacrifice recall. [sent-55, score-0.037]
</p><p>40 3  The DEviaNT Approach  The TWSS problem has two identifying characteristics: (1) TWSSs are likely to contain nouns that are euphemisms for sexually explicit nouns and (2) TWSSs share common structure with sentences in the erotic domain. [sent-56, score-0.883]
</p><p>41 Our approach to solving the TWSS problem is centered around an SVM model that uses features designed to model those characteristics. [sent-57, score-0.051]
</p><p>42 We will use features that build on corpus statistics computed for known erotic words, and their lexical contexts, as described in the rest of this section. [sent-59, score-0.282]
</p><p>43 1 Data and word classes Let SN be an open set of sexually explicit nouns. [sent-61, score-0.166]
</p><p>44 We manually approximated SN with a set of 76 nouns that are predominantly used in sexual contexts. [sent-62, score-0.187]
</p><p>45 We clustered the nouns into 9 categories based on which sexual object, body part, or participant they identify. [sent-63, score-0.166]
</p><p>46 Let SN− ⊂ SN be the set of sexually explicit nouns that are likely targets sfeotr euphemism. [sent-64, score-0.302]
</p><p>47 p Wliec td nido not consider euphemisms for people since they rarely, if e? [sent-65, score-0.147]
</p><p>48 5M sentences from the erotica section of text files . [sent-81, score-0.121]
</p><p>49 We tagged the erotica corpus with the Stanford Parser (Toutanova and Manning, 2000; Toutanova et al. [sent-85, score-0.098]
</p><p>50 To make the corpora more generic, we replaced all numbers with the CD tag, all proper nouns with the NNP tag, all nouns ∈ SN with an SN tag, and all nouns ∈ B,P al lw nitohu tnhse ∈NN SN tag. [sent-87, score-0.351]
</p><p>51 2 Word- and phrase-level analysis We define three functions to measure how closely related a noun, an adjective, and a verb phrase are to the erotica domain. [sent-90, score-0.133]
</p><p>52 The noun sexiness function NS(n) is a realvalued measure of the maximum similarity a noun n ∈/ SN has to each of the nouns ∈ SN−. [sent-92, score-0.365]
</p><p>53 For each  noun, Nlet h tahse t adjective count vector b SeN the vector of the absolute frequencies of each adjective that modifies the noun in the union of the erotica and the Brown corpora. [sent-93, score-0.266]
</p><p>54 We define NS(n) to be the maximum cosine similarity, over each noun ∈ SN−, using tmerumm frequency-inverse dvoecru emacehnt n frequency (tf-idf) weights of the nouns’ adjective count vectors. [sent-94, score-0.123]
</p><p>55 For nouns that occurred fewer that 200 times, occurred fewer than 50 times with adjectives, or were associated with 3 times as many adjectives that never occurred with nouns in SN than adjectives that did, NS(n) = 10−7 (smaller than all recorded similarities). [sent-95, score-0.382]
</p><p>56 Example nouns with high NS are “rod” and “meat” . [sent-96, score-0.117]
</p><p>57 The adjective sexiness function AS(a) is a real-valued measure of how likely an adjective a is to modify a noun ∈ SN. [sent-98, score-0.252]
</p><p>58 i nW see ndteefinncees A Sin( t)h teo e breoti tchae corpus that contain at least one noun ∈ SN. [sent-100, score-0.093]
</p><p>59 The verb sexiness function VS(v) is a realvalued measure of how much more likely a verb phrase v is to appear in an erotic context than a nonerotic one. [sent-104, score-0.541]
</p><p>60 Let SE be the set of sentences in the  erotica corpus that contain nouns ∈ SN. [sent-105, score-0.253]
</p><p>61 G Siven a sentence s containing a verb v, the verb phrase v is the contiguous substring of the sentence that con91 tains v and is bordered on each side by the closest noun or one of the set of pronouns {I, you, it, me}. [sent-107, score-0.167]
</p><p>62 (If unnei othre orn a noun nor none orofn ntoheu pronouns occur on a side of the verb, v itself is an endpoint of v. [sent-108, score-0.097]
</p><p>63 ) To define VS(v), we approximate the probabilities of v appearing in an erotic and a nonerotic context with counts in SE and SB, respectively. [sent-109, score-0.36]
</p><p>64 Intuitively, the verb sexiness is a measure of how likely the action described in a sentence could be an action (via some metaphoric mapping) to an action in an erotic context. [sent-113, score-0.47]
</p><p>65 3 Features DEviaNT uses the following features to identify potential mappings of a sentence s into the erotic domain, organized into two categories: NOUN EUPHEMISMS and STRUCTURAL ELEMENTS. [sent-115, score-0.282]
</p><p>66 NOUN EUPHEMISMS: • (boolean) does s contain a noun ∈ SN? [sent-116, score-0.093]
</p><p>67 , • (boolean) ddooeess s ccoonnttaaiinn a noun ∈ BP? [sent-117, score-0.111]
</p><p>68 , • (boolean) ddooeess s ocontnatianin a a noun n s? [sent-118, score-0.111]
</p><p>69 u,ch that NS(n) = )1 d0o−e7s, • (real) average NS(n), for all nouns n ∈ s such t(hreaat n ∈/ SerNa g∪e BP, STRUCTURAL ELEMENTS: • (boolean) does s contain a verb that never occurs ilena SE? [sent-119, score-0.206]
</p><p>70 , • (boolean) does s contain a verb phrase that never occurs eins SE? [sent-120, score-0.089]
</p><p>71 , • (real) average VS(v) over all verb phrases v ∈ s, • (real) average AS(a) over aallll adjectives a ∈ s, • (boolean) adgoees A s ca)on otvaeinr an adjective a ∈su sc,h t(hboato a never occurs innta a s aennte andcjee s ∈ SE s∪u SB twhaitht a noun ∈ cScNu. [sent-121, score-0.228]
</p><p>72 (We approximate rth per subject )w tihteh t shueb fjiercstt noun or pronoun. [sent-125, score-0.095]
</p><p>73 To minimize false positives, while tolerating false negatives, DEviaNT employs the MetaCost metaclassifier (Domingos, 1999), which uses bagging to reclassify the training data to produce a single cost-sensitive classifier. [sent-131, score-0.139]
</p><p>74 DEviaNT sets the  cost of a false positive to be 100 times that of a false negative. [sent-132, score-0.106]
</p><p>75 DEviaNT explores a particular approach to solving the TWSS problem: recognizing euphemistic and structural relationships between the source domain and an erotic domain. [sent-134, score-0.338]
</p><p>76 Thus, the goal of our evaluation is not to outperform the baselines in all aspects, but rather to show that by using only euphemism-based and structure-based features, DEviaNT can compete with the baselines, particularly where it matters most, delivering high precision and few false positives. [sent-138, score-0.111]
</p><p>77 DE92 viaNT’s positive training data are 2001 quoted sentences from tws s stories . [sent-141, score-0.041]
</p><p>78 DEviaNT’s negative training data are 2001 sentences from three sources (667 each): text s fromlastnight . [sent-143, score-0.04]
</p><p>79 com/ int imacy (FML), a set of short (1– 2 sentence) user-submitted stories about their love lives; and wikiquote . [sent-145, score-0.044]
</p><p>80 For testing, we used 262 other TS and 20,700 other TFLN, FML, and WQ sentences (all the data from these sources that were available at the time ofthe experiments). [sent-148, score-0.04]
</p><p>81 , changing the tag of “i” from the foreign word tag FW to the correct pronoun tag PRP). [sent-152, score-0.093]
</p><p>82 2  Baselines  Our experiments compare DEviaNT to seven other classifiers: (1) a Na¨ ıve Bayes classifier on unigram features, (2) an SVM model trained on unigram features, (3) an SVM model trained on unigram and bigram features, (4–6) MetaCost (Domingos, 1999) (see Section 3. [sent-154, score-0.144]
</p><p>83 The state-of-the-practice approach to TWSS identification is a na¨ ıve Bayes model trained on a unigram model of instances of twitter tweets, some tagged with #twss (VandenBos, 2011). [sent-157, score-0.106]
</p><p>84 For completeness, we tested whether adding unigram features to DEviaNT improved its performance but found that it did not. [sent-161, score-0.068]
</p><p>85 Recall Figure 1: The precision-recall curves for DEviaNT and baseline classifiers on TS, TFLN, FML, and WQ. [sent-162, score-0.044]
</p><p>86 The best competitor Unigram SVM w/o MetaCost has the maximum precision of 59. [sent-166, score-0.037]
</p><p>87 Note that the addition of bigram features yields no improvement in (and can hurt) both precision and recall. [sent-170, score-0.057]
</p><p>88 To qualitatively evaluate DEviaNT, we compared those sentences that DEviaNT, Basic Structure, and Unigram SVM w/o MetaCost are most sure are TWSSs. [sent-171, score-0.046]
</p><p>89 DEviaNT returned 28 such sentences (all tied for most likely to be a TWSS), 20 of which are true positives. [sent-172, score-0.042]
</p><p>90 However, 2 of the 8 false positives are in fact TWSSs (despite coming from the negative testing data): “Yes give me all the cream and he’s gone. [sent-173, score-0.099]
</p><p>91 However, DEviaNT was also able to identify TWSSs that deal with noun euphemisms (e. [sent-177, score-0.225]
</p><p>92 Note that while DE93 viaNT has a much lower recall than Unigram SVM w/o MetaCost, it accomplishes our goal of delivering high-precision, while tolerating low recall. [sent-182, score-0.054]
</p><p>93 Note that the DEviaNT’s precision appears low in large because the testing data is predominantly negative. [sent-183, score-0.058]
</p><p>94 5  Contributions  We formally defined the TWSS problem, a subproblem of the double entendre problem. [sent-186, score-0.245]
</p><p>95 We then identified two characteristics of the TWSS problem (1) TWSSs are likely to contain nouns that are euphemisms for sexually explicit nouns and (2) TWSSs share common structure with sentences in the erotic domain that we used to construct DEviaNT, an approach for TWSS classification. [sent-187, score-0.942]
</p><p>96 —  —  DEviaNT identifies euphemism and erotic-domain structure without relying heavily on structural features specific to TWSSs. [sent-188, score-0.056]
</p><p>97 DEviaNT delivers significantly higher precision than classifiers that use n-gram TWSS models. [sent-189, score-0.059]
</p><p>98 Our experiments indicate that euphemism- and erotic-domain-structure features contribute to improving the precision of TWSS identification. [sent-190, score-0.057]
</p><p>99 While significant future work in improving DEviaNT remains, we have identified two characteristics important to the TWSS problem and demonstrated that an approach based on these characteristics has promise. [sent-191, score-0.066]
</p><p>100 The technique of metaphorical mapping may be generalized to identify other types of double entendres and other forms of humor. [sent-192, score-0.18]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('twss', 0.589), ('deviant', 0.531), ('erotic', 0.262), ('twsss', 0.196), ('euphemisms', 0.147), ('sexually', 0.131), ('nouns', 0.117), ('entendre', 0.115), ('metacost', 0.115), ('sn', 0.107), ('double', 0.101), ('metaphor', 0.1), ('erotica', 0.098), ('nonerotic', 0.098), ('noun', 0.078), ('said', 0.067), ('sexiness', 0.065), ('false', 0.053), ('boolean', 0.05), ('se', 0.049), ('fml', 0.049), ('joke', 0.049), ('sexual', 0.049), ('tfln', 0.049), ('viant', 0.049), ('svm', 0.048), ('unigram', 0.048), ('saying', 0.045), ('adjective', 0.045), ('toutanova', 0.044), ('ns', 0.044), ('sb', 0.044), ('humor', 0.043), ('nonliteral', 0.043), ('identification', 0.042), ('vs', 0.039), ('precision', 0.037), ('verb', 0.035), ('explicit', 0.035), ('characteristics', 0.033), ('daniels', 0.033), ('ddooeess', 0.033), ('entendres', 0.033), ('innocuous', 0.033), ('jokes', 0.033), ('pasanek', 0.033), ('tolerating', 0.033), ('literal', 0.032), ('adjectives', 0.031), ('solving', 0.031), ('mihalcea', 0.03), ('bp', 0.03), ('positives', 0.03), ('metaphoric', 0.029), ('funny', 0.029), ('wq', 0.029), ('subproblem', 0.029), ('metaphorical', 0.027), ('rigorously', 0.027), ('realvalued', 0.027), ('int', 0.026), ('domain', 0.026), ('tag', 0.024), ('sure', 0.023), ('negatives', 0.023), ('television', 0.023), ('meat', 0.023), ('domains', 0.023), ('sentences', 0.023), ('occurred', 0.022), ('curves', 0.022), ('classifiers', 0.022), ('delivering', 0.021), ('predominantly', 0.021), ('pronoun', 0.021), ('basic', 0.021), ('never', 0.02), ('ts', 0.02), ('features', 0.02), ('action', 0.02), ('mapping', 0.019), ('francis', 0.019), ('domingos', 0.019), ('likely', 0.019), ('pronouns', 0.019), ('occurs', 0.019), ('weka', 0.019), ('structural', 0.019), ('stories', 0.018), ('brown', 0.018), ('let', 0.018), ('selectional', 0.018), ('rada', 0.018), ('structure', 0.017), ('sources', 0.017), ('subject', 0.017), ('bayes', 0.017), ('twitter', 0.016), ('coming', 0.016), ('contain', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="297-tfidf-1" href="./acl-2011-That%27s_What_She_Said%3A_Double_Entendre_Identification.html">297 acl-2011-That's What She Said: Double Entendre Identification</a></p>
<p>Author: Chloe Kiddon ; Yuriy Brun</p><p>Abstract: Humor identification is a hard natural language understanding problem. We identify a subproblem — the “that’s what she said” problem with two distinguishing characteristics: (1) use of nouns that are euphemisms for sexually explicit nouns and (2) structure common in the erotic domain. We address this problem in a classification approach that includes features that model those two characteristics. Experiments on web data demonstrate that our approach improves precision by 12% over baseline techniques that use only word-based features. —</p><p>2 0.049067657 <a title="297-tfidf-2" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>Author: Lei Zhang ; Bing Liu</p><p>Abstract: Identifying domain-dependent opinion words is a key problem in opinion mining and has been studied by several researchers. However, existing work has been focused on adjectives and to some extent verbs. Limited work has been done on nouns and noun phrases. In our work, we used the feature-based opinion mining model, and we found that in some domains nouns and noun phrases that indicate product features may also imply opinions. In many such cases, these nouns are not subjective but objective. Their involved sentences are also objective sentences and imply positive or negative opinions. Identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1</p><p>3 0.037923329 <a title="297-tfidf-3" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>4 0.036476713 <a title="297-tfidf-4" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>Author: Tony Veale</p><p>Abstract: Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning. IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text’s potential relevance than its potential meaning. In contrast, FLP views language as a system of unstable signs that can be used to talk about the world in creative new ways. There is another key difference: IR is practical, scalable and robust, and in daily use by millions of casual users. FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab. This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1</p><p>5 0.03615978 <a title="297-tfidf-5" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>Author: Michael Subotin</p><p>Abstract: This paper presents an exponential model for translation into highly inflected languages which can be scaled to very large datasets. As in other recent proposals, it predicts targetside phrases and can be conditioned on sourceside context. However, crucially for the task of modeling morphological generalizations, it estimates feature parameters from the entire training set rather than as a collection of separate classifiers. We apply it to English-Czech translation, using a variety of features capturing potential predictors for case, number, and gender, and one of the largest publicly available parallel data sets. We also describe generation and modeling of inflected forms unobserved in training data and decoding procedures for a model with non-local target-side feature dependencies.</p><p>6 0.035255078 <a title="297-tfidf-6" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>7 0.032812625 <a title="297-tfidf-7" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>8 0.031491887 <a title="297-tfidf-8" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>9 0.030367522 <a title="297-tfidf-9" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>10 0.028739642 <a title="297-tfidf-10" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>11 0.028049747 <a title="297-tfidf-11" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>12 0.02804617 <a title="297-tfidf-12" href="./acl-2011-A_Corpus_for_Modeling_Morpho-Syntactic_Agreement_in_Arabic%3A_Gender%2C_Number_and_Rationality.html">7 acl-2011-A Corpus for Modeling Morpho-Syntactic Agreement in Arabic: Gender, Number and Rationality</a></p>
<p>13 0.027663736 <a title="297-tfidf-13" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>14 0.027195446 <a title="297-tfidf-14" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>15 0.026958965 <a title="297-tfidf-15" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>16 0.026684856 <a title="297-tfidf-16" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>17 0.026624793 <a title="297-tfidf-17" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>18 0.026037637 <a title="297-tfidf-18" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>19 0.025931373 <a title="297-tfidf-19" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>20 0.025889028 <a title="297-tfidf-20" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.081), (1, 0.023), (2, -0.011), (3, -0.016), (4, -0.014), (5, 0.002), (6, 0.032), (7, -0.004), (8, -0.005), (9, -0.006), (10, -0.027), (11, -0.02), (12, -0.002), (13, 0.0), (14, -0.004), (15, -0.009), (16, -0.013), (17, 0.01), (18, 0.01), (19, -0.014), (20, 0.004), (21, 0.004), (22, -0.01), (23, 0.009), (24, -0.007), (25, 0.029), (26, 0.047), (27, -0.016), (28, 0.029), (29, -0.006), (30, 0.07), (31, -0.005), (32, -0.016), (33, -0.02), (34, -0.022), (35, 0.03), (36, -0.038), (37, 0.034), (38, -0.007), (39, 0.002), (40, -0.045), (41, 0.034), (42, -0.009), (43, -0.01), (44, 0.035), (45, -0.063), (46, -0.026), (47, 0.043), (48, -0.031), (49, -0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85514873 <a title="297-lsi-1" href="./acl-2011-That%27s_What_She_Said%3A_Double_Entendre_Identification.html">297 acl-2011-That's What She Said: Double Entendre Identification</a></p>
<p>Author: Chloe Kiddon ; Yuriy Brun</p><p>Abstract: Humor identification is a hard natural language understanding problem. We identify a subproblem — the “that’s what she said” problem with two distinguishing characteristics: (1) use of nouns that are euphemisms for sexually explicit nouns and (2) structure common in the erotic domain. We address this problem in a classification approach that includes features that model those two characteristics. Experiments on web data demonstrate that our approach improves precision by 12% over baseline techniques that use only word-based features. —</p><p>2 0.55857831 <a title="297-lsi-2" href="./acl-2011-Finding_Deceptive_Opinion_Spam_by_Any_Stretch_of_the_Imagination.html">136 acl-2011-Finding Deceptive Opinion Spam by Any Stretch of the Imagination</a></p>
<p>Author: Myle Ott ; Yejin Choi ; Claire Cardie ; Jeffrey T. Hancock</p><p>Abstract: Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam—fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.</p><p>3 0.54271114 <a title="297-lsi-3" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>Author: Lei Zhang ; Bing Liu</p><p>Abstract: Identifying domain-dependent opinion words is a key problem in opinion mining and has been studied by several researchers. However, existing work has been focused on adjectives and to some extent verbs. Limited work has been done on nouns and noun phrases. In our work, we used the feature-based opinion mining model, and we found that in some domains nouns and noun phrases that indicate product features may also imply opinions. In many such cases, these nouns are not subjective but objective. Their involved sentences are also objective sentences and imply positive or negative opinions. Identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1</p><p>4 0.53145468 <a title="297-lsi-4" href="./acl-2011-Extracting_Opinion_Expressions_and_Their_Polarities_-_Exploration_of_Pipelines_and_Joint_Models.html">131 acl-2011-Extracting Opinion Expressions and Their Polarities - Exploration of Pipelines and Joint Models</a></p>
<p>Author: Richard Johansson ; Alessandro Moschitti</p><p>Abstract: We investigate systems that identify opinion expressions and assigns polarities to the extracted expressions. In particular, we demonstrate the benefit of integrating opinion extraction and polarity classification into a joint model using features reflecting the global polarity structure. The model is trained using large-margin structured prediction methods. The system is evaluated on the MPQA opinion corpus, where we compare it to the only previously published end-to-end system for opinion expression extraction and polarity classification. The results show an improvement of between 10 and 15 absolute points in F-measure.</p><p>5 0.52077919 <a title="297-lsi-5" href="./acl-2011-Extracting_Comparative_Entities_and_Predicates_from_Texts_Using_Comparative_Type_Classification.html">130 acl-2011-Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification</a></p>
<p>Author: Seon Yang ; Youngjoong Ko</p><p>Abstract: The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use. 1</p><p>6 0.51802182 <a title="297-lsi-6" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>7 0.50594038 <a title="297-lsi-7" href="./acl-2011-Contrasting_Opposing_Views_of_News_Articles_on_Contentious_Issues.html">84 acl-2011-Contrasting Opposing Views of News Articles on Contentious Issues</a></p>
<p>8 0.5015437 <a title="297-lsi-8" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>9 0.49962845 <a title="297-lsi-9" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>10 0.49233598 <a title="297-lsi-10" href="./acl-2011-Liars_and_Saviors_in_a_Sentiment_Annotated_Corpus_of_Comments_to_Political_Debates.html">211 acl-2011-Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates</a></p>
<p>11 0.48590118 <a title="297-lsi-11" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<p>12 0.4782804 <a title="297-lsi-12" href="./acl-2011-Towards_Style_Transformation_from_Written-Style_to_Audio-Style.html">306 acl-2011-Towards Style Transformation from Written-Style to Audio-Style</a></p>
<p>13 0.47673035 <a title="297-lsi-13" href="./acl-2011-Effective_Measures_of_Domain_Similarity_for_Parsing.html">109 acl-2011-Effective Measures of Domain Similarity for Parsing</a></p>
<p>14 0.47281143 <a title="297-lsi-14" href="./acl-2011-Is_Machine_Translation_Ripe_for_Cross-Lingual_Sentiment_Classification%3F.html">179 acl-2011-Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?</a></p>
<p>15 0.47085795 <a title="297-lsi-15" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>16 0.47067371 <a title="297-lsi-16" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>17 0.47058621 <a title="297-lsi-17" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>18 0.46749547 <a title="297-lsi-18" href="./acl-2011-Even_the_Abstract_have_Color%3A_Consensus_in_Word-Colour_Associations.html">120 acl-2011-Even the Abstract have Color: Consensus in Word-Colour Associations</a></p>
<p>19 0.45998234 <a title="297-lsi-19" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>20 0.45658508 <a title="297-lsi-20" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (5, 0.026), (17, 0.033), (26, 0.044), (31, 0.026), (37, 0.082), (39, 0.035), (41, 0.053), (46, 0.341), (55, 0.019), (59, 0.025), (72, 0.042), (91, 0.028), (96, 0.088), (97, 0.019), (98, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71940529 <a title="297-lda-1" href="./acl-2011-That%27s_What_She_Said%3A_Double_Entendre_Identification.html">297 acl-2011-That's What She Said: Double Entendre Identification</a></p>
<p>Author: Chloe Kiddon ; Yuriy Brun</p><p>Abstract: Humor identification is a hard natural language understanding problem. We identify a subproblem — the “that’s what she said” problem with two distinguishing characteristics: (1) use of nouns that are euphemisms for sexually explicit nouns and (2) structure common in the erotic domain. We address this problem in a classification approach that includes features that model those two characteristics. Experiments on web data demonstrate that our approach improves precision by 12% over baseline techniques that use only word-based features. —</p><p>2 0.61591375 <a title="297-lda-2" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>Author: Svetlana Kiritchenko ; Colin Cherry</p><p>Abstract: The automatic coding of clinical documents is an important task for today’s healthcare providers. Though it can be viewed as multi-label document classification, the coding problem has the interesting property that most code assignments can be supported by a single phrase found in the input document. We propose a Lexically-Triggered Hidden Markov Model (LT-HMM) that leverages these phrases to improve coding accuracy. The LT-HMM works in two stages: first, a lexical match is performed against a term dictionary to collect a set of candidate codes for a document. Next, a discriminative HMM selects the best subset of codes to assign to the document by tagging candidates as present or absent. By confirming codes proposed by a dictionary, the LT-HMM can share features across codes, enabling strong performance even on rare codes. In fact, we are able to recover codes that do not occur in the training set at all. Our approach achieves the best ever performance on the 2007 Medical NLP Challenge test set, with an F-measure of 89.84.</p><p>3 0.5534386 <a title="297-lda-3" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: We present a pointwise approach to Japanese morphological analysis (MA) that ignores structure information during learning and tagging. Despite the lack of structure, it is able to outperform the current state-of-the-art structured approach for Japanese MA, and achieves accuracy similar to that of structured predictors using the same feature set. We also find that the method is both robust to outof-domain data, and can be easily adapted through the use of a combination of partial annotation and active learning.</p><p>4 0.40083846 <a title="297-lda-4" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>Author: Stefan Rud ; Massimiliano Ciaramita ; Jens Muller ; Hinrich Schutze</p><p>Abstract: We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries.</p><p>5 0.39907539 <a title="297-lda-5" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>Author: Shane Bergsma ; David Yarowsky ; Kenneth Church</p><p>Abstract: Resolving coordination ambiguity is a classic hard problem. This paper looks at coordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don’t do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via achieves data and pervised tations. co-training. The co-trained classifier close to 96% accuracy on Treebank makes 20% fewer errors than a susystem trained with Treebank anno-</p><p>6 0.39889795 <a title="297-lda-6" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>7 0.39889395 <a title="297-lda-7" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>8 0.39774954 <a title="297-lda-8" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>9 0.39755732 <a title="297-lda-9" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>10 0.3960093 <a title="297-lda-10" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>11 0.39539626 <a title="297-lda-11" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>12 0.39537317 <a title="297-lda-12" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>13 0.394685 <a title="297-lda-13" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>14 0.39467198 <a title="297-lda-14" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>15 0.39368916 <a title="297-lda-15" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>16 0.39322233 <a title="297-lda-16" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>17 0.39239916 <a title="297-lda-17" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>18 0.39216256 <a title="297-lda-18" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>19 0.39180237 <a title="297-lda-19" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>20 0.39126369 <a title="297-lda-20" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
