<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>200 acl-2011-Learning Dependency-Based Compositional Semantics</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-200" href="#">acl2011-200</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>200 acl-2011-Learning Dependency-Based Compositional Semantics</h1>
<br/><p>Source: <a title="acl-2011-200-pdf" href="http://aclweb.org/anthology//P/P11/P11-1060.pdf">pdf</a></p><p>Author: Percy Liang ; Michael Jordan ; Dan Klein</p><p>Abstract: Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.</p><p>Reference: <a title="acl-2011-200-reference" href="../acl2011_reference/acl-2011-Learning_Dependency-Based_Compositional_Semantics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. [sent-8, score-0.999]
</p><p>2 In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. [sent-9, score-0.51]
</p><p>3 In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. [sent-10, score-0.495]
</p><p>4 On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms. [sent-11, score-0.49]
</p><p>5 Answering these types of complex questions compositionally involves first mapping the  questions into logical forms (semantic parsing). [sent-13, score-0.648]
</p><p>6 Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001 ; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al. [sent-14, score-0.09]
</p><p>7 , 2010) rely on manual annotation of logical forms, which is expensive. [sent-15, score-0.375]
</p><p>8 On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. [sent-16, score-0.133]
</p><p>9 (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. [sent-18, score-0.622]
</p><p>10 However, we still model the logical form (now as a latent variable) to capture the complexities of language. [sent-19, score-0.445]
</p><p>11 We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. [sent-21, score-0.551]
</p><p>12 We want to induce latent logical forms z (and parameters θ) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs. [sent-22, score-0.509]
</p><p>13 The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answer y. [sent-23, score-0.463]
</p><p>14 Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z. [sent-24, score-0.053]
</p><p>15 The dominant paradigm in compositional se-  mantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner. [sent-26, score-0.447]
</p><p>16 CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e. [sent-27, score-0.053]
</p><p>17 However, the logical forms there can become quite complex, and in the context of program induction, this would lead to an unwieldy search space. [sent-30, score-0.502]
</p><p>18 (2010), are simpler but lack the full expressive power of lambda calculus. [sent-35, score-0.137]
</p><p>19 The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2). [sent-36, score-0.346]
</p><p>20 The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient. [sent-37, score-0.56]
</p><p>21 Our system outperforms all existing systems despite using no annotated logical forms. [sent-39, score-0.375]
</p><p>22 1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics. [sent-41, score-0.369]
</p><p>23 2), which handles linguistic phenomena such as quantification, where syntactic and semantic scope diverge. [sent-43, score-0.096]
</p><p>24 We start with some definitions, using US geography as an example domain. [sent-44, score-0.032]
</p><p>25 Let V be the set of all values, awnh eixcah minpclelud deosm primitives (e. [sent-45, score-0.035]
</p><p>26 , 3, CA ∈ V) as well as sets and tuples formed from other v∈alu Ves) (e. [sent-47, score-0.045]
</p><p>27 , state, count ∈ P), Pw bheich a are just symbols. [sent-52, score-0.031]
</p><p>28 A world w is mapping from each predicate p ∈ P Ato a rsledt wof tuples; nfogr example, w(state) = {(CA) , (OR) , . [sent-53, score-0.205]
</p><p>29 h Cereon ceaepchtu predicate irlsd a sre ala rteiloan(possibly infinite). [sent-61, score-0.192]
</p><p>30 As another ePxample, w(average) = {(S, x¯) : x¯ = |S1|−1 Px∈S1 S(x)}, where a s =et o {f( pairs )S : ¯ixs t=rea |tSed| as Pa xs∈etS-vSal(uxe)d} ,fu wnhcetiroen a S(x) = {y : (x, y) ∈ S} swP aith s edto-vmalauine S1 = {x : (x, y) ∈ S}. [sent-66, score-0.038]
</p><p>31 T,yh)e logical tfhor dmoms iani nD SCS= are c :a l(lxed,y D) ∈CS S trees, where nodes are labeled with predicates, and edges are labeled with relations. [sent-67, score-0.586]
</p><p>32 Formally: Definition 1(DCS trees) Let Z be the set of DCS trees, itwiohnere 1 e( DacChS z ∈ Zs) c LoentsZi sts b of (i) a predicate 591 Relations R  j0 Σ Xi  (join) (aggregate) (execute)  E Q C  (extract) (quantify) (compare)  Table 1: Possible relations appearing on the edges of a DCS tree. [sent-68, score-0.236]
</p><p>33 r ∈ R (see Teaacbhle 1) aend e a cnhsiilsdt tree e. [sent-84, score-0.103]
</p><p>34 Figure 2(a) DshCoSws tr an example of a DCS tree. [sent-90, score-0.032]
</p><p>35 though a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words. [sent-92, score-0.605]
</p><p>36 It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction. [sent-93, score-0.43]
</p><p>37 1 Basic Version The basic version of DCS restricts R to join and aggregate ir cel vaetrisoniosn (see TCaSbl ree 1). [sent-95, score-0.268]
</p><p>38 Ltse tR us st joaritn by considering a DCS tree z with only join relations. [sent-96, score-0.216]
</p><p>39 Such a z defines a constraint satisfaction problem (CSP) with nodes as variables. [sent-97, score-0.091]
</p><p>40 The CSP has two types of constraints: (i) x ∈ w(p) for each node x labeled cwoinths predicate p ∈ P; apn)d f (ii) xj = yj0 (the j-th component aotfe x must equal t(hiie) jx 0-th component of y) for each edge (x, y) labeled ∈ R. [sent-98, score-0.407]
</p><p>41 We say a value v is consistent for a node x if there exists a solution that assigns v to x. [sent-101, score-0.07]
</p><p>42 The denotation JzKw (z esvoalultuiaotned th oant wass) i gs nths ev s toet xof. [sent-102, score-0.289]
</p><p>43 Tcohnesi dsetnenotta avtiaolune Jsz oKf the rsoolout tinoonde th (aset ea sFsiiggunsre v v2 t foo xr . [sent-103, score-0.038]
</p><p>44 withjj0  Computation  We can compute  the denotation  JzKw of a DCS tree z by exploiting dynamic proJgzraKmming on trees (Dechter, 2003). [sent-105, score-0.405]
</p><p>45 The recurrence iJsz aKs follows:  JDp;jj110:c1;··· ;jjm0m:cmEKw  (1)  \m  = w(p) ∩  \{v  :  Kvji  = tj0i,t ∈ JciKw}. [sent-106, score-0.031]
</p><p>46 i\= \1 At each node, we compute the set of ,ttup ∈le Jsc vK consistent with the predicate at that node (v ∈ w(p)), and Example: major city in California z = hcity; 11 : hmajori ; 11 : hloc; 12 : hCAiii  maj1orc1it1yl1o2c  C1A  λccl∃iomtcy(∃(‘c)‘)∃ ∧ ∧sCm. [sent-107, score-0.193]
</p><p>47 Aa(js)o∧r(m)∧ c1=(‘ m) ∧1∧( cs1)=∧ ‘1∧ ‘2= s1  (a) DCS tree (b) Lambda calculus formula (c) Denotation: JzKw = {SF, LA, . [sent-108, score-0.133]
</p><p>48 } Fig(uc)re D2:e n(oa)t aAtino enx:a JmzKple of a DCS tree (written in both the mathematical and graphical notation). [sent-111, score-0.068]
</p><p>49 Each node is labeled with a predicate, and each edge is labeled with a relation. [sent-112, score-0.212]
</p><p>50 (b) A DCS tree z with only join relations encodes a constraint satisfaction problem. [sent-113, score-0.307]
</p><p>51 for each child i, the ji-th component of v must equal the j0i-th component of some t in the child’s denotation (t ∈ JciKw). [sent-115, score-0.321]
</p><p>52 1 iNonow ( tth ∈e d JucalK importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing. [sent-117, score-0.282]
</p><p>53 In addition, trees enable efficient computation, thereby establishing a new connection between dependency syntax and efficient semantic evaluation. [sent-118, score-0.244]
</p><p>54 Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language. [sent-119, score-0.539]
</p><p>55 For example, consider the phrase number of major cities, and suppose that number corresponds to the count predicate. [sent-120, score-0.031]
</p><p>56 It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated Σ. [sent-121, score-0.198]
</p><p>57 Consider a tree hΣ : ci, wgrhegosaet ero reotla aitsi ocon,nn noetcatetedd dto Σ a c Choilnds c veira a aΣ t. [sent-122, score-0.134]
</p><p>58 r eIef hthΣe :dcei-, notation of c is a set of values s, the parent’s denotation is then a singleton set containing s. [sent-123, score-0.249]
</p><p>59 The deJhnΣot:actiiKon o=f t{hJec Kmiddle node is {s}, example. [sent-126, score-0.07]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dcs', 0.638), ('logical', 0.375), ('denotation', 0.213), ('csp', 0.17), ('compositional', 0.16), ('join', 0.148), ('jzkw', 0.128), ('trees', 0.124), ('predicate', 0.123), ('mooney', 0.117), ('forms', 0.095), ('uc', 0.089), ('lambda', 0.089), ('geo', 0.085), ('jcikw', 0.085), ('zettlemoyer', 0.085), ('semantics', 0.085), ('aggregate', 0.082), ('node', 0.07), ('tree', 0.068), ('calculus', 0.065), ('quantification', 0.065), ('questions', 0.064), ('kate', 0.062), ('benchmarks', 0.062), ('predicates', 0.06), ('jobs', 0.059), ('satisfaction', 0.057), ('answer', 0.056), ('semantic', 0.053), ('clarke', 0.052), ('mapping', 0.05), ('labeled', 0.049), ('expressive', 0.048), ('berkeley', 0.048), ('tuples', 0.045), ('edge', 0.044), ('phenomena', 0.043), ('edges', 0.041), ('collins', 0.04), ('ii', 0.04), ('latent', 0.039), ('zelle', 0.038), ('oant', 0.038), ('ala', 0.038), ('nths', 0.038), ('denotations', 0.038), ('zs', 0.038), ('mantics', 0.038), ('foo', 0.038), ('aith', 0.038), ('alu', 0.038), ('cel', 0.038), ('fig', 0.038), ('iani', 0.038), ('obviate', 0.038), ('sts', 0.038), ('parsers', 0.037), ('child', 0.036), ('component', 0.036), ('notation', 0.036), ('kwiatkowski', 0.035), ('ves', 0.035), ('aend', 0.035), ('jh', 0.035), ('ero', 0.035), ('rela', 0.035), ('enx', 0.035), ('primitives', 0.035), ('transparency', 0.035), ('relations', 0.034), ('dependency', 0.034), ('nodes', 0.034), ('syntax', 0.033), ('wor', 0.032), ('rfe', 0.032), ('apn', 0.032), ('geography', 0.032), ('rea', 0.032), ('wz', 0.032), ('tahe', 0.032), ('world', 0.032), ('answering', 0.032), ('induced', 0.032), ('tr', 0.032), ('ca', 0.032), ('program', 0.032), ('complexities', 0.031), ('dto', 0.031), ('ato', 0.031), ('execute', 0.031), ('notated', 0.031), ('tang', 0.031), ('sre', 0.031), ('recurrence', 0.031), ('count', 0.031), ('nal', 0.03), ('higherorder', 0.03), ('hp', 0.03), ('ccl', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="200-tfidf-1" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>Author: Percy Liang ; Michael Jordan ; Dan Klein</p><p>Abstract: Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.</p><p>2 0.1639501 <a title="200-tfidf-2" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>3 0.11689694 <a title="200-tfidf-3" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>4 0.083356321 <a title="200-tfidf-4" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>Author: Raphael Hoffmann ; Congle Zhang ; Xiao Ling ; Luke Zettlemoyer ; Daniel S. Weld</p><p>Abstract: Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded ( Jobs Apple ) and CEO-o f ( Jobs Apple ) . , , This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extrac- , tion model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</p><p>5 0.064055346 <a title="200-tfidf-5" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>6 0.063051529 <a title="200-tfidf-6" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>7 0.061942592 <a title="200-tfidf-7" href="./acl-2011-Global_Learning_of_Typed_Entailment_Rules.html">144 acl-2011-Global Learning of Typed Entailment Rules</a></p>
<p>8 0.05761442 <a title="200-tfidf-8" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>9 0.057505306 <a title="200-tfidf-9" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>10 0.052872609 <a title="200-tfidf-10" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>11 0.052183777 <a title="200-tfidf-11" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>12 0.052108794 <a title="200-tfidf-12" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>13 0.051091924 <a title="200-tfidf-13" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>14 0.050603725 <a title="200-tfidf-14" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>15 0.047107413 <a title="200-tfidf-15" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>16 0.047042422 <a title="200-tfidf-16" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>17 0.046581592 <a title="200-tfidf-17" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>18 0.046335969 <a title="200-tfidf-18" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<p>19 0.045174912 <a title="200-tfidf-19" href="./acl-2011-A_Corpus_of_Scope-disambiguated_English_Text.html">8 acl-2011-A Corpus of Scope-disambiguated English Text</a></p>
<p>20 0.043366965 <a title="200-tfidf-20" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, -0.014), (2, -0.047), (3, -0.074), (4, 0.011), (5, -0.016), (6, -0.02), (7, 0.018), (8, -0.035), (9, -0.053), (10, 0.037), (11, 0.002), (12, 0.027), (13, 0.034), (14, -0.034), (15, -0.064), (16, -0.052), (17, -0.116), (18, -0.031), (19, -0.007), (20, 0.013), (21, 0.047), (22, -0.052), (23, 0.006), (24, 0.015), (25, -0.058), (26, -0.053), (27, -0.027), (28, 0.008), (29, -0.002), (30, -0.047), (31, -0.023), (32, 0.05), (33, 0.022), (34, 0.034), (35, 0.007), (36, -0.066), (37, -0.088), (38, 0.068), (39, -0.089), (40, -0.066), (41, -0.05), (42, -0.013), (43, -0.045), (44, 0.009), (45, -0.062), (46, -0.051), (47, -0.048), (48, -0.047), (49, -0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94806987 <a title="200-lsi-1" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>Author: Percy Liang ; Michael Jordan ; Dan Klein</p><p>Abstract: Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.</p><p>2 0.64904004 <a title="200-lsi-2" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<p>Author: Eduardo Blanco ; Dan Moldovan</p><p>Abstract: This paper presents an unsupervised method for deriving inference axioms by composing semantic relations. The method is independent of any particular relation inventory. It relies on describing semantic relations using primitives and manipulating these primitives according to an algebra. The method was tested using a set of eight semantic relations yielding 78 inference axioms which were evaluated over PropBank.</p><p>3 0.62475777 <a title="200-lsi-3" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>4 0.60556656 <a title="200-lsi-4" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>5 0.59071428 <a title="200-lsi-5" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>Author: Anselmo Penas ; Alvaro Rodrigo</p><p>Abstract: There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.</p><p>6 0.53623301 <a title="200-lsi-6" href="./acl-2011-Temporal_Evaluation.html">294 acl-2011-Temporal Evaluation</a></p>
<p>7 0.52800679 <a title="200-lsi-7" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>8 0.50374168 <a title="200-lsi-8" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>9 0.48204166 <a title="200-lsi-9" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>10 0.47191975 <a title="200-lsi-10" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>11 0.44987264 <a title="200-lsi-11" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>12 0.44842756 <a title="200-lsi-12" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>13 0.44443455 <a title="200-lsi-13" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>14 0.43533581 <a title="200-lsi-14" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>15 0.42231056 <a title="200-lsi-15" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>16 0.42025682 <a title="200-lsi-16" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>17 0.41601002 <a title="200-lsi-17" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>18 0.41371134 <a title="200-lsi-18" href="./acl-2011-Classifying_arguments_by_scheme.html">68 acl-2011-Classifying arguments by scheme</a></p>
<p>19 0.40245712 <a title="200-lsi-19" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>20 0.3974705 <a title="200-lsi-20" href="./acl-2011-Temporal_Restricted_Boltzmann_Machines_for_Dependency_Parsing.html">295 acl-2011-Temporal Restricted Boltzmann Machines for Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.02), (17, 0.073), (26, 0.026), (37, 0.115), (39, 0.04), (41, 0.039), (55, 0.045), (59, 0.049), (67, 0.257), (72, 0.019), (91, 0.101), (96, 0.114)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81568533 <a title="200-lda-1" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>Author: Percy Liang ; Michael Jordan ; Dan Klein</p><p>Abstract: Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.</p><p>2 0.60395914 <a title="200-lda-2" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>3 0.60222745 <a title="200-lda-3" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>4 0.6021806 <a title="200-lda-4" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>Author: David Chiang ; Steve DeNeefe ; Michael Pust</p><p>Abstract: We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems.</p><p>5 0.59939259 <a title="200-lda-5" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>Author: Shane Bergsma ; David Yarowsky ; Kenneth Church</p><p>Abstract: Resolving coordination ambiguity is a classic hard problem. This paper looks at coordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don’t do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via achieves data and pervised tations. co-training. The co-trained classifier close to 96% accuracy on Treebank makes 20% fewer errors than a susystem trained with Treebank anno-</p><p>6 0.59849989 <a title="200-lda-6" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>7 0.59570521 <a title="200-lda-7" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>8 0.59409845 <a title="200-lda-8" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>9 0.59346581 <a title="200-lda-9" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>10 0.59327513 <a title="200-lda-10" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>11 0.59312654 <a title="200-lda-11" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>12 0.59181499 <a title="200-lda-12" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>13 0.59168893 <a title="200-lda-13" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>14 0.59045398 <a title="200-lda-14" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>15 0.59014547 <a title="200-lda-15" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>16 0.58964735 <a title="200-lda-16" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>17 0.5895806 <a title="200-lda-17" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>18 0.58925039 <a title="200-lda-18" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>19 0.58860755 <a title="200-lda-19" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>20 0.5881629 <a title="200-lda-20" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
