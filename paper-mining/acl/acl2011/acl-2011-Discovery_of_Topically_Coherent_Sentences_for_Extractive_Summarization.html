<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-98" href="#">acl2011-98</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</h1>
<br/><p>Source: <a title="acl-2011-98-pdf" href="http://aclweb.org/anthology//P/P11/P11-1050.pdf">pdf</a></p><p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.</p><p>Reference: <a title="acl-2011-98-reference" href="../acl2011_reference/acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. [sent-3, score-0.403]
</p><p>2 Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. [sent-4, score-0.278]
</p><p>3 An ideal generated summary text should contain the shared relevant content among set of documents only once, plus other unique information from individual documents that are directly related to the user’s query addressing different levels of detail. [sent-8, score-0.48]
</p><p>4 Recent approaches to the summarization task has somewhat focused on the redundancy and coherence issues. [sent-9, score-0.259]
</p><p>5 In this paper, we introduce a series of new generative models for multiple-documents, based on a discovery of hierarchical topics and their correlations to extract topically coherent sentences. [sent-10, score-0.642]
</p><p>6 , sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al. [sent-16, score-0.416]
</p><p>7 In particular (Haghighi and Vanderwende, 2009; Celikyilmaz and HakkaniTur, 2010) build hierarchical topic models to identify salient sentences that contain abstract concepts rather than specific concepts. [sent-21, score-0.647]
</p><p>8 In this paper, we present a novel, fully generative Bayesian model of document corpus, which can discover topically coherent sentences that contain key shared information with as little detail and redundancy as possible. [sent-25, score-0.533]
</p><p>9 Our model can discover hierarchical latent structure of multi-documents, in which some words are governed by low-level topics (T) and others by high-level topics (H). [sent-26, score-0.62]
</p><p>10 Human evaluations of gener-  ated summaries confirm that our model can generate non-redundant and topically coherent summaries. [sent-32, score-0.383]
</p><p>11 , 2006); topic signatures based on user queries (Lin and Hovy, 2002; Conroy et al. [sent-35, score-0.249]
</p><p>12 Recent research focusing on the extraction of latent concepts from document clusters are close in spirit to our work (Barzilay and Lee, 2004; Daum e´III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al. [sent-38, score-0.278]
</p><p>13 Some of these work (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010) focus on the discovery of hierarchical concepts from documents (from abstract to specific) using extensions of hierarchal topic models (Blei et al. [sent-41, score-0.61]
</p><p>14 Hierarchical concept learning models help to discover, for instance, that ”baseball” and ”football” are both contained in a general class ”sports”, so that the summaries reference terms related to more abstract concepts like ”sports”. [sent-43, score-0.35]
</p><p>15 We need a model that can identify salient sentences referring to general concepts of documents and there should be minimum correlation between them. [sent-45, score-0.402]
</p><p>16 We define a tiered-topic clustering in which the upper nodes in the DAG are higher-level topics H, rep-  resenting common co-occurence patterns (correlations) between lower-level topics T in documents. [sent-49, score-0.438]
</p><p>17 Mainly, our model can discover correlated topics to eliminate redundant sentences in summary text. [sent-51, score-0.581]
</p><p>18 , 2004), in which words are generated by first selecting an author uniformly from an observed author list and then selecting a topic from a distribution over topics that is specific to that author. [sent-56, score-0.562]
</p><p>19 In our model, words are generated from different topics of documents by first selecting a sentence containing the word and then topics that are specific to that sentence. [sent-57, score-0.684]
</p><p>20 This way we can directly extract from documents the summary related sentences that contain high-level topics. [sent-58, score-0.314]
</p><p>21 In addition in (Celikyilmaz and Hakkani-Tur, 2010), the sentences can only share topics if the sentences are represented on the same path of captured topic hierarchy, restricting topic sharing across sen-  tences on different paths. [sent-59, score-0.835]
</p><p>22 Our DAG identifies tiered topics distributed over document clusters that can be shared by each sentence. [sent-60, score-0.449]
</p><p>23 3  Topic Coherence for Summarization  In this section we discuss the main contribution, our two hierarchical mixture models, which improve summary generation performance through the use of tiered topic models. [sent-61, score-0.569]
</p><p>24 Our models can identify lowerlevel topics T (concepts) defined as distributions over words or higher-level topics H, which represent correlations between these lower level topics given sentences. [sent-62, score-0.857]
</p><p>25 We present our synthetic experiment for model development to evaluate extracted summaries on redundancy measure. [sent-63, score-0.308]
</p><p>26 For model development we use the DUC 2005 dataset1 , which consists of 45 document clusters, each of which include 1-4 set of human generated summaries (10-15 sentences each). [sent-66, score-0.417]
</p><p>27 Each document cluster consists ∼ 25 documents (25-30 sen-  tences/document) s riesttrsi ∼eve 2d5 b daosceudm on a user query. [sent-67, score-0.273]
</p><p>28 For the synthetic experiments, we include the provided human generated summaries of each corpus as additional documents. [sent-69, score-0.279]
</p><p>29 The sentences in human summaries include general concepts mentioned in the corpus, the salient sentences of documents. [sent-70, score-0.512]
</p><p>30 Contrary to usual qualitative evaluations of summarization tasks, our aim during development is to measure the percentage of sentences in a human summary that our model can identify as salient among all other document cluster sentences. [sent-71, score-0.688]
</p><p>31 Because human produced summaries generally contain non-redundant sentences, we use total number of top-ranked human summary sentences as a qualitative redundancy measure in our synthetic experiments. [sent-72, score-0.554]
</p><p>32 In each model, a document d is a vector of Nd words wd, where each wid is chosen from a vocabulary of size V , and a vector of sentences S, representing all sentences in a corpus of size SD. [sent-73, score-0.647]
</p><p>33 We identify sentences as meta-variables of document clusters, which the generative process models both sentences and documents using tiered topics. [sent-74, score-0.483]
</p><p>34 A sentence’s re-  latedness to summary text is tied to the document cluster’s user query. [sent-75, score-0.312]
</p><p>35 4  Two-Tiered Topic Model - TTM  Our base model, the two-tiered topic model (TTM), is inspired by the hierarchical topic model, PAM, proposed by Li and McCallum (2006). [sent-77, score-0.545]
</p><p>36 PAM structures documents to represent and learn arbitrary, nested, and possibly sparse topic correlations using 1www-nlpir. [sent-78, score-0.36]
</p><p>37 html 493 Documents  in a  Document Cluster  Figure 1: Graphical model depiction of two-tiered topic model (TTM) described in section §4. [sent-81, score-0.314]
</p><p>38 K1 ), representing topic correlations, are modeled as distributions over lowlevel-topics (Tk2=1. [sent-88, score-0.281]
</p><p>39 Our goals are not so dif-  ferent: we aim to discover concepts from documents that would attribute for the general topics related to a  user query, however, we want to relate this information to sentences. [sent-95, score-0.558]
</p><p>40 We represent sentences S by discovery of general (more general) to specific topics (Fig. [sent-96, score-0.399]
</p><p>41 Similarly, we represent summary unrelated (document specific) sentences as corpus specific distributions θ over background words wB, (functional words like prepositions, etc. [sent-98, score-0.38]
</p><p>42 Our two-tiered topic model for salient sentence discovery can be generated for each word in the document (Algorithm 1) as follows: For a word wid in document d, a random variable xid is drawn, which determines if wid is query related, i. [sent-100, score-1.652]
</p><p>43 , wid either exists in the query or is related to the query2. [sent-102, score-0.496]
</p><p>44 Then sentence si is chosen uniformly at random (ysi∼ Uniform(si)) from sentences in the document containing wid (deterministic if there is only one sentence containing wid). [sent-104, score-0.817]
</p><p>45 If a word is query/summary related sentence S, first a sentence then a high-level (H) and a low-level (T) topic is sampled. [sent-110, score-0.286]
</p><p>46 Every time an si is sampled afo vre a query Sre ∈late Zd wid, we increment its count, a degree of sentence saliency. [sent-115, score-0.367]
</p><p>47 Given that wid is related to a query, it is associated with two-tiered multinomial distributions: high-level H topics and low-level T topics. [sent-116, score-0.613]
</p><p>48 A highlevel topic Hki is chosen first from a distribution over low-level topics T specific to that si and one low-level topic Tkj is chosen from a distribution over words, and wid is generated from the sampled low-level topic. [sent-117, score-1.4]
</p><p>49 If wid is not query-related, it is generated as a background word wB. [sent-118, score-0.479]
</p><p>50 A sentence sampled from a query related word is associated with a distribution over K1 number of high-level topics Hki , each of which are also associated with K2 number of low-level topics Tkj , a multinomial over lexical words of a corpus. [sent-121, score-0.72]
</p><p>51 if wid exists or related to the the query then x = 1deterministic, otherwise it is stochastically assigned x Bin(Ψ). [sent-149, score-0.529]
</p><p>52 , the topic ”acquisition” is found to be more correlated with ”retail” than the ”network” topic given H1. [sent-155, score-0.461]
</p><p>53 For each word, xid is sampled from a sentence specific binomial ψ which in turn has a smoothing prior η to determine if the sampled word wid is (query) summary-related or document-specific. [sent-165, score-0.936]
</p><p>54 Depending on xid, we either sample a sentence along with a high/low-level topic pair or just sample background words wB. [sent-166, score-0.361]
</p><p>55 The probability distribution over sentence assignments, P(ysi = s|S) si ∈ S, is assumed to be uniform over the =elems |eSn)t ss of∈ S, a,n ids d ase-terministic if there is only one sentence in the document containing the corresponding word. [sent-167, score-0.298]
</p><p>56 For each word we sample a high-level Hki and a low-level Tkj topic if the word is query related (xid = 1). [sent-169, score-0.356]
</p><p>57 Note that the number of tiered topics in the model is fixed to K1 and K2, which is optimized with validation experiments. [sent-171, score-0.325]
</p><p>58 SD: scoreTTM(sj) ∝ # [wid ∈ sj, xid = 1] /nwj (1) where wid indicates a word in a document d that exists in sj and is sampled as summary related based on random indicator variable xid. [sent-182, score-0.99]
</p><p>59 We compare TTM results on synthetic experiments against PAM (Li and McCallum, 2006) a similar topic model that clusters topics in a hierarchical structure, where super-topics are distributions over sub-topics. [sent-187, score-0.733]
</p><p>60 We obtain sentence scores for PAM models by calculating the sub-topic significance (TS) based on super-topic correlations, and discover topic correlations over the entire document space (corpus wide). [sent-188, score-0.535]
</p><p>61 So, sentences including such topics will have higher saliency scores, which we quantify by imposing topic’s significance on vocabulary:  scorePAM(si) =K12XKk2wY∈sip(w|zskub) ∗ TS(zk) (3) Fig. [sent-196, score-0.332]
</p><p>62 The higher the human summary sentences are ranked, the better the model is in selecting the salient sentences. [sent-201, score-0.327]
</p><p>63 5  Enriched Two-Tiered Topic Model  Our model can discover words that are related to summary text using posteriors and  Pˆ(θH)  Pˆ(θT),  Documents in a Document Cluster  Figure 3:  Graphical model depiction of sentence level enriched two-tiered model (ETTM) described in section §5. [sent-206, score-0.451]
</p><p>64 Each Hk1 also represented as distributions over general words WH as well as indicates the degree of correlation between  low-level topics denoted by boldness of the arrows. [sent-212, score-0.321]
</p><p>65 TTM can discover topic correlations, but cannot differentiate if a word in a sentence is more general or specific given a query. [sent-215, score-0.398]
</p><p>66 Sentences with general words would be more suitable to include in summary text compared to sentences containing specific words. [sent-216, score-0.345]
</p><p>67 Sentence containing words that are sampled from high-level topics would be a better candidate for summary text. [sent-222, score-0.555]
</p><p>68 3), which samples words not only from low-level topics but also from high-level topics as well. [sent-224, score-0.438]
</p><p>69 ETTM discovers three separate distributions over words: (i) high-level topics H as distributions over corpus general words WH, (ii) low-level topics T as distributions over corpus specific words WL, and 496  FeoIrftcL-xwhsei=aζvdm k,e1∼lip G,l=se BeaH1nel,tke. [sent-225, score-0.722]
</p><p>70 Similar to TTM’s generative process, if wid is related to a given query, then x = 1 is deterministic, otherwise x ∈ {0, 1} is stochastically determistiince,d o tihf wid esh xoul ∈d { be0 sampled as a background word (wB) or through hierarchical path, i. [sent-236, score-1.116]
</p><p>71 We first sample a sentence si for wid uniformly at random from the sentences containing the word ysi∼Uniform(si)). [sent-239, score-0.68]
</p><p>72 At this stage we sample a level Lwid ∈ {1, 2} for wid to determine if it is a high-level word, e. [sent-240, score-0.46]
</p><p>73 Each path through the DAG, defined by a H-T pair (total of K1K2 pairs), has a binomial ζK1K2 over which  % of sentences added to the generated  summary  text. [sent-243, score-0.394]
</p><p>74 If the word is a specific type, x = 0, then it is sampled from the background word distribution θ, a document specific multinomial. [sent-248, score-0.384]
</p><p>75 If the word is related to the query x = 1, we sample a high and low-level topic pair H − T as well as an additional level L is sampled tro H Hde −ter Tm ainse w wwelhlic ahs alenve ald odfit topics tehvee wl Lor ids should be sampled from. [sent-252, score-0.889]
</p><p>76 s Iafn Ld sampled wfroormd the high-level topic, otherwise (L = 2) the word is corpus specific and sampled from a the low-level topic. [sent-255, score-0.336]
</p><p>77 2 Summary Generation with ETTM For ETTM models, we extend the TTM sentence score to be able to include the effect of the general words in sentences (as word sequences in language 497 models) using probabilities of K1 high-level topic  distributions, φHwk=1. [sent-259, score-0.355]
</p><p>78 K1 Qw∈sip(w|Tk) where p(w|Tk) is theP probabilQity of a word in si being generated from high-level topic Hk. [sent-263, score-0.353]
</p><p>79 , super topics and subtopics, where super-topics are distributions over abstract words. [sent-268, score-0.286]
</p><p>80 Thus; ETTM is capable of capturing focused sentences with general words related to the main concepts of the documents and much less redundant sentences containing concepts specific to user query. [sent-271, score-0.614]
</p><p>81 6  Final Experiments  In this section, we qualitatively compare our models against state-of-the art models and later apply an intrinsic evaluation of generated summaries on topical coherence and informativeness. [sent-272, score-0.396]
</p><p>82 ROUGE Evaluations: We train each document cluster as a separate corpus to find the optimum parameters of each model and evaluate on test document clusters. [sent-280, score-0.335]
</p><p>83 ROUGE is a commonly used measure, a standard DUC evaluation metric, which computes recall over various n-grams statistics from a model generated summary against a set ofhuman generated summaries. [sent-281, score-0.302]
</p><p>84 , 2007): Utilizes human generated summaries to train a sentence ranking system using a classifier model; (ii) HIERSUM  (Haghighi and Vanderwende, 2009): Based on hierarchical topic models. [sent-288, score-0.541]
</p><p>85 Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence of the generated summary concept distributions from document word-frequency distributions. [sent-289, score-0.632]
</p><p>86 , 2007): Two hierarchical topic models to discover high and lowlevel concepts from documents, baselines for synthetic experiments in §4 & §5. [sent-293, score-0.643]
</p><p>87 Because HybHSum uses the human generated summaries as supervision during model development and our systems do not, 498  our performance is quite promising considering the generation is completely unsupervised without seeing any human generated summaries during training. [sent-297, score-0.466]
</p><p>88 For topic models bigrams tend to degenerate due to generating inconsistent bag of bi-grams (Wallach, 2006). [sent-300, score-0.255]
</p><p>89 We compare our best model ETTM to the results of PAM, our benchmark model in synthetic experiments, as well as hybrid hierarchical summarization model, hLDA (Celikyilmaz and Hakkani-Tur, 2010). [sent-304, score-0.349]
</p><p>90 Human annotators are given two sets of summary text for each document set, generated from either one of the two approaches: best ETTM and PAM or best ETTM and HybHSum models. [sent-305, score-0.331]
</p><p>91 The annotators are asked to mark the better summary according to five criteria: non-redundancy (which summary is less redundant),  coherence (which summary is more coherent), focus and readability (content and no unnecessary details), responsiveness and overall performance. [sent-306, score-0.571]
</p><p>92 We asked 3 annotators to rate DUC2007 predicted summaries (45 summary pairs per annotator). [sent-307, score-0.314]
</p><p>93 The participants rated ETTM generated summaries more coherent and focused compared to PAM, where the results are statistically significant (based on t-test on 95% confidence level) indicating that ETTM summaries are rated significantly better. [sent-312, score-0.487]
</p><p>94 7 Conclusion We introduce two new models for extracting topically coherent sentences from documents, an important property in extractive multi-document summarization systems. [sent-315, score-0.438]
</p><p>95 Our models combine approaches from the hierarchical topic models. [sent-316, score-0.342]
</p><p>96 size capturing correlated semantic concepts in documents as well as characterizing general and specific words, in order to identify topically coherent sentences in documents. [sent-319, score-0.559]
</p><p>97 We showed empirically that a fully unsupervised model for extracting general sentences performs well at summarization task using datasets that were originally used in building automatic summarization system challenges. [sent-320, score-0.389]
</p><p>98 The success of our model can be traced to its capability of directly capturing coherent topics in documents, which makes it able to identify salient sentences. [sent-321, score-0.387]
</p><p>99 Hierarchical topic models and the nested chinese restaurant process. [sent-353, score-0.255]
</p><p>100 Query-focused summarization by combining topic model and affinity propagation. [sent-369, score-0.371]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wid', 0.394), ('ettm', 0.372), ('ttm', 0.312), ('pam', 0.223), ('topics', 0.219), ('topic', 0.214), ('summary', 0.164), ('summaries', 0.15), ('sampled', 0.144), ('summarization', 0.127), ('concepts', 0.124), ('document', 0.113), ('starbucks', 0.112), ('xid', 0.112), ('query', 0.102), ('celikyilmaz', 0.099), ('topically', 0.094), ('tkj', 0.093), ('vanderwende', 0.093), ('hierarchical', 0.087), ('coffee', 0.085), ('si', 0.085), ('documents', 0.08), ('coherence', 0.079), ('nenkova', 0.078), ('tiered', 0.076), ('synthetic', 0.075), ('coherent', 0.075), ('hki', 0.074), ('hybhsum', 0.074), ('zskub', 0.074), ('wb', 0.071), ('sentences', 0.07), ('distributions', 0.067), ('duc', 0.066), ('correlations', 0.066), ('discover', 0.065), ('salient', 0.063), ('sj', 0.063), ('ysi', 0.06), ('binomial', 0.058), ('dag', 0.057), ('rouge', 0.056), ('pttm', 0.056), ('ventures', 0.056), ('generated', 0.054), ('tang', 0.054), ('blei', 0.053), ('redundancy', 0.053), ('pachinko', 0.049), ('specific', 0.048), ('path', 0.048), ('barzilay', 0.045), ('cluster', 0.045), ('saliency', 0.043), ('qualitative', 0.042), ('clusters', 0.041), ('models', 0.041), ('depiction', 0.04), ('mimno', 0.04), ('sample', 0.04), ('haghighi', 0.04), ('li', 0.04), ('hierarchal', 0.037), ('lowlevel', 0.037), ('phthy', 0.037), ('zsd', 0.037), ('microsoft', 0.037), ('sentence', 0.036), ('general', 0.035), ('user', 0.035), ('evaluations', 0.034), ('optimum', 0.034), ('correlated', 0.033), ('generative', 0.033), ('eisenstein', 0.033), ('conroy', 0.033), ('sip', 0.033), ('stochastically', 0.033), ('mccallum', 0.031), ('extractive', 0.031), ('topical', 0.031), ('background', 0.031), ('ts', 0.031), ('harabagiu', 0.03), ('enriched', 0.03), ('model', 0.03), ('rated', 0.029), ('generation', 0.028), ('diversify', 0.028), ('highlevel', 0.028), ('containing', 0.028), ('discovery', 0.027), ('uniformly', 0.027), ('ds', 0.027), ('daum', 0.026), ('zk', 0.026), ('labs', 0.026), ('level', 0.026), ('acyclic', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="98-tfidf-1" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.</p><p>2 0.22003561 <a title="98-tfidf-2" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>Author: Risa Kitajima ; Ichiro Kobayashi</p><p>Abstract: Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic.</p><p>3 0.21363746 <a title="98-tfidf-3" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>Author: Ivan Vulic ; Wim De Smet ; Marie-Francine Moens</p><p>Abstract: A topic model outputs a set of multinomial distributions over words for each topic. In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources. Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.</p><p>4 0.19921656 <a title="98-tfidf-4" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>Author: Jey Han Lau ; Karl Grieser ; David Newman ; Timothy Baldwin</p><p>Abstract: We propose a method for automatically labelling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.</p><p>5 0.178811 <a title="98-tfidf-5" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>Author: Yves Petinot ; Kathleen McKeown ; Kapil Thadani</p><p>Abstract: We investigate the relevance of hierarchical topic models to represent the content of Web gists. We focus our attention on DMOZ, a popular Web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories. Our first approach, based on information-theoretic grounds, uses an algorithm similar to recursive feature selection. Our second approach is fully Bayesian and derived from the more general model, hierarchical LDA. We evaluate the performance of both models against a flat 1-gram baseline and show improvements in terms of perplexity over held-out data.</p><p>6 0.17386283 <a title="98-tfidf-6" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>7 0.17183052 <a title="98-tfidf-7" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>8 0.17157143 <a title="98-tfidf-8" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>9 0.1687566 <a title="98-tfidf-9" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>10 0.16484419 <a title="98-tfidf-10" href="./acl-2011-Structural_Topic_Model_for_Latent_Topical_Structure_Analysis.html">287 acl-2011-Structural Topic Model for Latent Topical Structure Analysis</a></p>
<p>11 0.15950547 <a title="98-tfidf-11" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>12 0.14741349 <a title="98-tfidf-12" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>13 0.14425167 <a title="98-tfidf-13" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>14 0.13617344 <a title="98-tfidf-14" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>15 0.13001595 <a title="98-tfidf-15" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>16 0.12822582 <a title="98-tfidf-16" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>17 0.11496936 <a title="98-tfidf-17" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>18 0.11201226 <a title="98-tfidf-18" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>19 0.10352671 <a title="98-tfidf-19" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>20 0.10148533 <a title="98-tfidf-20" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, 0.14), (2, -0.102), (3, 0.2), (4, -0.074), (5, -0.145), (6, -0.182), (7, 0.291), (8, 0.019), (9, 0.033), (10, -0.097), (11, 0.05), (12, -0.035), (13, -0.026), (14, 0.024), (15, -0.019), (16, -0.026), (17, 0.027), (18, -0.025), (19, 0.093), (20, -0.075), (21, 0.05), (22, 0.018), (23, 0.013), (24, -0.015), (25, 0.017), (26, 0.018), (27, -0.033), (28, -0.028), (29, -0.019), (30, -0.053), (31, -0.031), (32, 0.039), (33, -0.01), (34, -0.035), (35, -0.003), (36, 0.082), (37, -0.004), (38, -0.032), (39, 0.026), (40, 0.02), (41, 0.043), (42, 0.003), (43, -0.032), (44, -0.054), (45, 0.007), (46, -0.007), (47, 0.003), (48, 0.027), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96862453 <a title="98-lsi-1" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.</p><p>2 0.8765744 <a title="98-lsi-2" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>Author: William M. Darling ; Fei Song</p><p>Abstract: Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.</p><p>3 0.83903378 <a title="98-lsi-3" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>Author: Risa Kitajima ; Ichiro Kobayashi</p><p>Abstract: Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic.</p><p>4 0.82676858 <a title="98-lsi-4" href="./acl-2011-Structural_Topic_Model_for_Latent_Topical_Structure_Analysis.html">287 acl-2011-Structural Topic Model for Latent Topical Structure Analysis</a></p>
<p>Author: Hongning Wang ; Duo Zhang ; ChengXiang Zhai</p><p>Abstract: Topic models have been successfully applied to many document analysis tasks to discover topics embedded in text. However, existing topic models generally cannot capture the latent topical structures in documents. Since languages are intrinsically cohesive and coherent, modeling and discovering latent topical transition structures within documents would be beneficial for many text analysis tasks. In this work, we propose a new topic model, Structural Topic Model, which simultaneously discovers topics and reveals the latent topical structures in text through explicitly modeling topical transitions with a latent first-order Markov chain. Experiment results show that the proposed Structural Topic Model can effectively discover topical structures in text, and the identified structures significantly improve the performance of tasks such as sentence annotation and sentence ordering. ,</p><p>5 0.79030079 <a title="98-lsi-5" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>Author: Yves Petinot ; Kathleen McKeown ; Kapil Thadani</p><p>Abstract: We investigate the relevance of hierarchical topic models to represent the content of Web gists. We focus our attention on DMOZ, a popular Web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories. Our first approach, based on information-theoretic grounds, uses an algorithm similar to recursive feature selection. Our second approach is fully Bayesian and derived from the more general model, hierarchical LDA. We evaluate the performance of both models against a flat 1-gram baseline and show improvements in terms of perplexity over held-out data.</p><p>6 0.73251486 <a title="98-lsi-6" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>7 0.72943002 <a title="98-lsi-7" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>8 0.72476995 <a title="98-lsi-8" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>9 0.67776829 <a title="98-lsi-9" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>10 0.67773288 <a title="98-lsi-10" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>11 0.67297536 <a title="98-lsi-11" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>12 0.65725607 <a title="98-lsi-12" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>13 0.65407366 <a title="98-lsi-13" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>14 0.6490466 <a title="98-lsi-14" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>15 0.62414211 <a title="98-lsi-15" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>16 0.59202361 <a title="98-lsi-16" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>17 0.58592373 <a title="98-lsi-17" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>18 0.57047796 <a title="98-lsi-18" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>19 0.56721312 <a title="98-lsi-19" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>20 0.53134823 <a title="98-lsi-20" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.018), (17, 0.045), (26, 0.019), (37, 0.096), (39, 0.067), (41, 0.05), (45, 0.256), (55, 0.026), (59, 0.051), (72, 0.029), (91, 0.043), (96, 0.185), (97, 0.014), (98, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87234414 <a title="98-lda-1" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>Author: Roger Levy</p><p>Abstract: A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.</p><p>same-paper 2 0.79224491 <a title="98-lda-2" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.</p><p>3 0.77019078 <a title="98-lda-3" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>Author: Thomas Mueller ; Hinrich Schuetze</p><p>Abstract: We present a class-based language model that clusters rare words of similar morphology together. The model improves the prediction of words after histories containing outof-vocabulary words. The morphological features used are obtained without the use of labeled data. The perplexity improvement compared to a state of the art Kneser-Ney model is 4% overall and 81% on unknown histories.</p><p>4 0.75501192 <a title="98-lda-4" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>Author: Omar F. Zaidan ; Chris Callison-Burch</p><p>Abstract: Naively collecting translations by crowdsourcing the task to non-professional translators yields disfluent, low-quality results if no quality control is exercised. We demonstrate a variety of mechanisms that increase the translation quality to near professional levels. Specifically, we solicit redundant translations and edits to them, and automatically select the best output among them. We propose a set of features that model both the translations and the translators, such as country of residence, LM perplexity of the translation, edit rate from the other translations, and (optionally) calibration against professional translators. Using these features to score the collected translations, we are able to discriminate between acceptable and unacceptable translations. We recreate the NIST 2009 Urdu-toEnglish evaluation set with Mechanical Turk, and quantitatively show that our models are able to select translations within the range of quality that we expect from professional trans- lators. The total cost is more than an order of magnitude lower than professional translation.</p><p>5 0.67981195 <a title="98-lda-5" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>6 0.67835492 <a title="98-lda-6" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>7 0.67818809 <a title="98-lda-7" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>8 0.67612994 <a title="98-lda-8" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>9 0.67483354 <a title="98-lda-9" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>10 0.67472261 <a title="98-lda-10" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>11 0.67469382 <a title="98-lda-11" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>12 0.67318159 <a title="98-lda-12" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>13 0.67228055 <a title="98-lda-13" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>14 0.67212427 <a title="98-lda-14" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>15 0.67181265 <a title="98-lda-15" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>16 0.67170674 <a title="98-lda-16" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>17 0.67109662 <a title="98-lda-17" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>18 0.67091322 <a title="98-lda-18" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>19 0.67032945 <a title="98-lda-19" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>20 0.66970891 <a title="98-lda-20" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
