<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>263 acl-2011-Reordering Constraint Based on Document-Level Context</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-263" href="#">acl2011-263</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>263 acl-2011-Reordering Constraint Based on Document-Level Context</h1>
<br/><p>Source: <a title="acl-2011-263-pdf" href="http://aclweb.org/anthology//P/P11/P11-2076.pdf">pdf</a></p><p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in JapaneseEnglish translation and 1.41% BLEU points in English-Japanese translation.</p><p>Reference: <a title="acl-2011-263-reference" href="../acl2011_reference/acl-2011-Reordering_Constraint_Based_on_Document-Level_Context_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 j p i  ,  ,  Abstract One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. [sent-5, score-0.562]
</p><p>2 In this paper, we propose a method of imposing reordering constraints using document-level context. [sent-6, score-0.512]
</p><p>3 As the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. [sent-7, score-0.442]
</p><p>4 Given a source sentence, zones which cover the noun phrases are used as reordering constraints. [sent-8, score-1.016]
</p><p>5 Then, in decoding, reorderings which violate the zones are restricted. [sent-9, score-0.65]
</p><p>6 Experiment results for patent translation tasks show a significant improvement of 1. [sent-10, score-0.636]
</p><p>7 1 Introduction  Phrase-based statistical machine translation is useful for translating between languages with similar word orders. [sent-13, score-0.234]
</p><p>8 However, it has problems with longdistance reordering when translating between languages with different word orders, such as JapaneseEnglish. [sent-14, score-0.402]
</p><p>9 These problems are especially crucial when translating long sentences, such as patent sentences, because many combinations of word orders cause high computational costs and low translation quality. [sent-15, score-0.748]
</p><p>10 These include methods where source sentences are divided into syntactic chunks or clauses and the translations are merged later (Koehn and 434 Knight, 2003; Sudoh et al. [sent-17, score-0.06]
</p><p>11 , 2010), methods where syntactic constraints or penalties for reordering are added to a decoder (Yamamoto et al. [sent-18, score-0.448]
</p><p>12 However, these methods did not use document-level context to constrain reorderings. [sent-22, score-0.066]
</p><p>13 We think it is a promising clue to improving translation quality. [sent-24, score-0.192]
</p><p>14 In this paper, we propose a method where reordering constraints are added to a decoder using document-level context. [sent-25, score-0.478]
</p><p>15 As the document-level context, we use noun phrases which significantly occur in context documents containing source sentences. [sent-26, score-0.417]
</p><p>16 Given a source sentence, zones which cover the noun phrases are used as reordering constraints. [sent-27, score-1.016]
</p><p>17 Then, in decoding, reorderings which violate the zones are restricted. [sent-28, score-0.65]
</p><p>18 By using document-level context, contextually-appropriate reordering constraints are preferentially considered. [sent-29, score-0.397]
</p><p>19 As a result, the translation quality and speed can be improved. [sent-30, score-0.195]
</p><p>20 Experiment results for the NTCIR-8 patent translation tasks show a significant improvement of 1. [sent-31, score-0.636]
</p><p>21 2  Patent Translation  Patent translation is difficult because of the amount of new phrases and long sentences. [sent-34, score-0.361]
</p><p>22 Since a patent document explains a newly-invented apparatus or method, it contains many new phrases. [sent-35, score-0.524]
</p><p>23 Learning phrase translations for these new phrases from the Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o. [sent-36, score-0.262]
</p><p>24 Baseline outputan interlayer insulating film 12 is formed on the surface of a semiconductor substrate 10 , a pad electrode 11 via a first insulating film . [sent-39, score-1.187]
</p><p>25 。  Source + Zoneパッド 電極 １ １ は 、 第 １ の 絶縁 膜 で ある 層間 絶  縁 膜   １ ２   を 介 し て 半導体 基板 １ ０ の 表面 に 形成 さ れ て い い る Proposed outputpad electrode 11 is formed on the surface of the semiconductor substrate 10 through the interlayer insulating film 12 of the first insulating film . [sent-40, score-1.157]
</p><p>26 training corpora is difficult because these phrases occur only in that patent specification. [sent-42, score-0.62]
</p><p>27 Therefore, when translating such phrases, a decoder has to combine multiple smaller phrase translations. [sent-43, score-0.216]
</p><p>28 Moreover, sentences in patent documents tend to be long. [sent-44, score-0.479]
</p><p>29 This results in a large number of combinations of phrasal reorderings and a degradation of the translation quality and speed. [sent-45, score-0.336]
</p><p>30 Table 1 shows how a failure in phrasal reordering can spoil the whole translation. [sent-46, score-0.287]
</p><p>31 In the baseline  output, the translation of “第 １ の 絶縁 膜 で あ る 層間 絶縁 膜 １ ２ ” (an interlayer insulation film 12 that is a first insulation film) is divided into two blocks, “an interlayer insulating film 12” and “a first insulating film”. [sent-47, score-1.408]
</p><p>32 In this case, a reordering constraint to translate “第 １ の 絶縁 膜 で ある 層間 絶縁 膜 １ ２ ” as a single block can reduce incorrect reorderings and improve the translation quality. [sent-48, score-0.649]
</p><p>33 Therefore, how to specify ranges for reordering constraints is a very important problem. [sent-50, score-0.43]
</p><p>34 We propose a solution for this problem that uses the very nature of patent documents themselves. [sent-51, score-0.479]
</p><p>35 3  Proposed Method  In order to address the aforementioned problem, we propose a method for specifying phrases in a source sentence which are assumed to be translated as single blocks using document-level context. [sent-52, score-0.499]
</p><p>36 When translating a document, for example a patent specification, we first extract coherent phrase candidates from the document. [sent-54, score-1.053]
</p><p>37 Then, when translating each sentence in  the document, we set zones which cover the coher435 ent phrase candidates and restrict reorderings which violate the zones. [sent-55, score-1.012]
</p><p>38 1 Coherent phrases in patent documents As mentioned in the previous section, specifying coherent phrases is difficult when using only one source sentence. [sent-57, score-1.326]
</p><p>39 However, we have observed that document-level context can be a clue for specifying coherent phrases. [sent-58, score-0.543]
</p><p>40 In a patent specification, for example, noun phrases which indicate parts of the invention are very important noun phrases. [sent-59, score-0.869]
</p><p>41 Since this is not language dependent, in other words, this noun phrase is always a part of the invention in any other language, this noun phrase should be translated as a single block in every language. [sent-61, score-0.49]
</p><p>42 In this way, important phrases in patent documents are assumed to be coherent phrases. [sent-62, score-0.955]
</p><p>43 We therefore treat the problem of specifying coherent phrases as a problem of specifying important  絶  phrases, and we use these phrases as constraints on reorderings. [sent-63, score-1.037]
</p><p>44 The details of the proposed method are described below. [sent-64, score-0.068]
</p><p>45 2 Finding coherent phrases We propose the following method for finding coherent phrases in patent sentences. [sent-66, score-1.431]
</p><p>46 First, we extract coherent phrase candidates from a patent document. [sent-67, score-0.979]
</p><p>47 Next, the candidates are ranked by a criterion which reflects the document-level context. [sent-68, score-0.134]
</p><p>48 In this method, using document-level context is critically important because we cannot rank the candidates without it. [sent-70, score-0.2]
</p><p>49 1 Extracting coherent phrase candidates Coherent phrase candidates are extracted from a context document, a document that contains a source sentence. [sent-73, score-0.931]
</p><p>50 We extract all noun phrases as coherent phrase candidates since most noun phrases can be translated as single blocks in other languages (Koehn and Knight, 2003). [sent-74, score-1.123]
</p><p>51 2 Ranking with C-value The candidates which have been extracted are nested  and have different lengths. [sent-78, score-0.215]
</p><p>52 For example, ranking by frequency cannot pick up an important phrase which has a long length, yet, ranking by length may give a long but unimportant phrase a high rank. [sent-80, score-0.242]
</p><p>53 In order to select the appropriate coherent phrases, measurements which give high rank to phrases with high termhood are needed. [sent-81, score-0.507]
</p><p>54 C-value is a measurement of automatic term recognition and is suitable for extracting important phrases from nested candidates. [sent-83, score-0.303]
</p><p>55 Since phrases which have a large C-value frequently occur in a context document, these phrases are considered to be a significant unit, i. [sent-85, score-0.408]
</p><p>56 , a part of the invention, and to be coherent phrases. [sent-87, score-0.305]
</p><p>57 3 Specifying coherent phrases Given a source sentence, we find coherent phrase candidates in the sentence in order to set zones for reordering constraints. [sent-90, score-1.752]
</p><p>58 If a coherent phrase candidate is found in the source sentence, the phrase is regarded a coherent phrase and annotated with a zone tag, which will be mentioned in the next section. [sent-91, score-1.007]
</p><p>59 436 We check the coherent phrase candidates in the sentence in descending C-value order, and stop when the C-value goes below a certain threshold. [sent-92, score-0.557]
</p><p>60 Nested zones are allowed, unless their zones conflict with pre-existing zones. [sent-93, score-0.744]
</p><p>61 3 Decoding with reordering constraints In decoding, reorderings which violate zones, such as the baseline output in Table 1, are restricted and  we get a more appropriate translation, such as the proposed output in Table 1. [sent-96, score-0.77]
</p><p>62 , 2007; Koehn and Haddow, 2009), which can specify reordering constraints using   and   tags. [sent-98, score-0.43]
</p><p>63 Moses restricts reorderings which violate zones and translates zones as single blocks. [sent-99, score-1.022]
</p><p>64 4  Experiments  In order to evaluate the performance of the proposed method, we conducted Japanese-English (J-E) and English-Japanese (E-J) translation experiments using the NTCIR-8 patent translation task dataset (Fujii et al. [sent-100, score-0.807]
</p><p>65 This dataset contains a training set of 3 million sentence pairs, a development set of 2,000 sentence pairs, and a test set of 1,25 1(J-E) and 1,119 (E-J) sentence pairs. [sent-102, score-0.081]
</p><p>66 Moreover, this dataset contains the patent specifications from which sentence pairs are extracted. [sent-103, score-0.522]
</p><p>67 1 Baseline We used Moses as a baseline system, with all the set-  tings except distortion limit (dl) at the default. [sent-106, score-0.134]
</p><p>68 The distortion limit is a maximum distance of reordering. [sent-107, score-0.083]
</p><p>69 It is known that an appropriate distortion-limit can improve translation quality and decoding speed. [sent-108, score-0.262]
</p><p>70 In experiments, we compared dl = 6, 10, 20, 30, 40, and −1 (unlimited). [sent-110, score-0.068]
</p><p>71 2 Compared methods We compared two methods, the method of specifying reordering constraints with a context document w/o Contextin ( this case ) , ( the leading end ) 15f of ( the segment operating body ) ( ( 15 swings ) in ( a direction opposite ) ) to ( the a arrow direction ) . [sent-114, score-0.9]
</p><p>72 w/ Contextin ( this case ) , ( ( the leading end ) 15f ) of ( ( ( the segment ) operating body ) 15 ) swings in a direction opposite to ( the a arrow direction ) . [sent-115, score-0.217]
</p><p>73 (w/ Context) and the method of specifying reordering constraints without a context document (w/o Context). [sent-119, score-0.683]
</p><p>74 In both methods, the feature weights used in decoding are the same value as those for the baseline (dl = −1). [sent-120, score-0.097]
</p><p>75 1 Proposed method (w/ Context) In the proposed method, reordering constraints were defined with a context document. [sent-123, score-0.531]
</p><p>76 For J-E translation, we used the CaboCha parser (Kudo and Matsumoto, 2002) to analyze the context document. [sent-124, score-0.09]
</p><p>77 As coherent phrase candidates, we extracted all subtrees whose heads are noun. [sent-125, score-0.396]
</p><p>78 For E-J translation, we used the Charniak parser (Charniak, 2000) and ex-  tracted all noun phrases, labeled “NP”, as coherent phrase candidates. [sent-126, score-0.51]
</p><p>79 The parsers are used only when extracting coherent phrase candidates. [sent-127, score-0.421]
</p><p>80 When specifying zones for each source sentence, strings which match the coherent phrase candidates are defined to be zones. [sent-128, score-1.102]
</p><p>81 Therefore, the proposed method is robust against parsing errors. [sent-129, score-0.068]
</p><p>82 2 w/o Context In this method, reordering constraints were defined without a context document. [sent-133, score-0.463]
</p><p>83 For J-E translation, we converted the dependency trees of source sen437 tences processed by the CaboCha parser into bracketed trees and used these as reordering constraints. [sent-134, score-0.371]
</p><p>84 For E-J translation, we used all of the noun phrases detected by the Charniak parser as reordering constraints. [sent-135, score-0.572]
</p><p>85 In both directions, our proposed method yielded the highest BLEU scores. [sent-140, score-0.094]
</p><p>86 These results show that the proposed method using document-level context is effective in specifying reordering constraints. [sent-153, score-0.561]
</p><p>87 Moreover, as shown in Table 3, although zone setting without context is failed if source sentences have parsing errors, the proposed method can set zones appropriately using document-level context. [sent-154, score-0.63]
</p><p>88 The Charniak parser tends to make errors on noun phrases with ID numbers. [sent-155, score-0.285]
</p><p>89 This shows that document-level context can possibly improve parsing quality. [sent-156, score-0.066]
</p><p>90 As for the distortion limit, while an appropriate  distortion-limit, 30 for J-E and 40 for E-J, improved the translation quality, the gains from the proposed method were significantly better than the gains from the distortion limit. [sent-157, score-0.369]
</p><p>91 In general, imposing strong constraints causes fast decoding but low translation quality. [sent-158, score-0.426]
</p><p>92 However, the proposed method improves the translation quality and speed by imposing appropriate constraints. [sent-159, score-0.379]
</p><p>93 5  Conclusion  In this paper, we proposed a method for imposing reordering constraints using document-level context. [sent-160, score-0.55]
</p><p>94 In the proposed method, coherent phrase candidates are extracted from a context document in advance. [sent-161, score-0.684]
</p><p>95 Given a source sentence, zones which cover the coherent phrase candidates are defined. [sent-162, score-0.998]
</p><p>96 Then, in decoding, reorderings which violate the zones are restricted. [sent-163, score-0.65]
</p><p>97 Since reordering constraints reduce incorrect reorderings, the translation quality and speed can be improved. [sent-164, score-0.592]
</p><p>98 The experiment results for the NTCIR-8 patent translation tasks show a significant improvement of 1. [sent-165, score-0.636]
</p><p>99 We think that the proposed method is independent of language pair and domains. [sent-168, score-0.068]
</p><p>100 In the future,  we want to apply our proposed method to other language pairs and domains. [sent-169, score-0.068]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('patent', 0.449), ('zones', 0.372), ('coherent', 0.305), ('reordering', 0.287), ('film', 0.188), ('reorderings', 0.176), ('phrases', 0.171), ('insulating', 0.169), ('translation', 0.16), ('interlayer', 0.141), ('specifying', 0.14), ('candidates', 0.134), ('insulation', 0.113), ('constraints', 0.11), ('violate', 0.102), ('phrase', 0.091), ('noun', 0.09), ('bleu', 0.088), ('imposing', 0.085), ('semiconductor', 0.085), ('substrate', 0.085), ('nested', 0.081), ('electrode', 0.074), ('sudoh', 0.074), ('translating', 0.074), ('decoding', 0.071), ('invention', 0.069), ('dl', 0.068), ('context', 0.066), ('zone', 0.064), ('source', 0.06), ('koehn', 0.06), ('contextin', 0.056), ('frantzi', 0.056), ('swings', 0.056), ('yamamoto', 0.056), ('distortion', 0.055), ('decoder', 0.051), ('document', 0.05), ('katsuhito', 0.05), ('masao', 0.046), ('utiyama', 0.046), ('specifications', 0.046), ('moses', 0.045), ('charniak', 0.045), ('cabocha', 0.043), ('tsukada', 0.043), ('fujii', 0.041), ('arrow', 0.041), ('longdistance', 0.041), ('blocks', 0.038), ('proposed', 0.038), ('isozaki', 0.036), ('hajime', 0.036), ('cover', 0.036), ('points', 0.036), ('speed', 0.035), ('metricsmatr', 0.035), ('orders', 0.035), ('eiichiro', 0.034), ('philipp', 0.034), ('formed', 0.033), ('direction', 0.033), ('translated', 0.033), ('marton', 0.033), ('specify', 0.033), ('kudo', 0.032), ('clue', 0.032), ('shi', 0.032), ('appropriate', 0.031), ('specification', 0.031), ('xiong', 0.031), ('documents', 0.03), ('method', 0.03), ('long', 0.03), ('pad', 0.03), ('operating', 0.03), ('limit', 0.028), ('sentence', 0.027), ('improvement', 0.027), ('measurement', 0.026), ('block', 0.026), ('baseline', 0.026), ('yielded', 0.026), ('surface', 0.025), ('tsutomu', 0.025), ('tings', 0.025), ('documentlevel', 0.025), ('sov', 0.025), ('japaneseenglish', 0.025), ('hideo', 0.025), ('okuma', 0.025), ('keihanna', 0.025), ('apparatus', 0.025), ('utsuro', 0.025), ('extracting', 0.025), ('parser', 0.024), ('opposite', 0.024), ('kevin', 0.024), ('japanese', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="263-tfidf-1" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in JapaneseEnglish translation and 1.41% BLEU points in English-Japanese translation.</p><p>2 0.2407655 <a title="263-tfidf-2" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>Author: Zhanyi Liu ; Haifeng Wang ; Hua Wu ; Ting Liu ; Sheng Li</p><p>Abstract: This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1</p><p>3 0.18429323 <a title="263-tfidf-3" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>Author: Nadir Durrani ; Helmut Schmid ; Alexander Fraser</p><p>Abstract: We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the “N-gram” model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.</p><p>4 0.17412472 <a title="263-tfidf-4" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>5 0.15697135 <a title="263-tfidf-5" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>Author: Alexandra Birch ; Miles Osborne</p><p>Abstract: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.</p><p>6 0.14379384 <a title="263-tfidf-6" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>7 0.14160059 <a title="263-tfidf-7" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>8 0.11519224 <a title="263-tfidf-8" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>9 0.11354946 <a title="263-tfidf-9" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>10 0.11171462 <a title="263-tfidf-10" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>11 0.11096458 <a title="263-tfidf-11" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>12 0.10492428 <a title="263-tfidf-12" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>13 0.10485877 <a title="263-tfidf-13" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>14 0.10350408 <a title="263-tfidf-14" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>15 0.10130931 <a title="263-tfidf-15" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>16 0.099769257 <a title="263-tfidf-16" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>17 0.099220984 <a title="263-tfidf-17" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>18 0.089744404 <a title="263-tfidf-18" href="./acl-2011-Phrase-Based_Translation_Model_for_Question_Retrieval_in_Community_Question_Answer_Archives.html">245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</a></p>
<p>19 0.087983213 <a title="263-tfidf-19" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>20 0.086609662 <a title="263-tfidf-20" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.19), (1, -0.164), (2, 0.095), (3, 0.097), (4, 0.061), (5, 0.026), (6, -0.042), (7, 0.008), (8, 0.017), (9, -0.011), (10, -0.005), (11, -0.055), (12, -0.013), (13, -0.173), (14, -0.004), (15, 0.002), (16, -0.039), (17, 0.045), (18, -0.086), (19, -0.008), (20, -0.069), (21, 0.046), (22, 0.006), (23, -0.119), (24, -0.042), (25, 0.079), (26, 0.115), (27, -0.015), (28, -0.028), (29, 0.082), (30, 0.056), (31, 0.009), (32, 0.18), (33, -0.046), (34, 0.01), (35, -0.168), (36, -0.091), (37, -0.02), (38, 0.009), (39, 0.047), (40, 0.073), (41, -0.015), (42, -0.064), (43, 0.014), (44, 0.03), (45, -0.059), (46, -0.032), (47, 0.069), (48, -0.023), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94112456 <a title="263-lsi-1" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in JapaneseEnglish translation and 1.41% BLEU points in English-Japanese translation.</p><p>2 0.91901243 <a title="263-lsi-2" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>Author: Zhanyi Liu ; Haifeng Wang ; Hua Wu ; Ting Liu ; Sheng Li</p><p>Abstract: This paper proposes a novel reordering model for statistical machine translation (SMT) by means of modeling the translation orders of the source language collocations. The model is learned from a word-aligned bilingual corpus where the collocated words in source sentences are automatically detected. During decoding, the model is employed to softly constrain the translation orders of the source language collocations, so as to constrain the translation orders of those source phrases containing these collocated words. The experimental results show that the proposed method significantly improves the translation quality, achieving the absolute improvements of 1.1~1.4 BLEU score over the baseline methods. 1</p><p>3 0.8217904 <a title="263-lsi-3" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>Author: Nadir Durrani ; Helmut Schmid ; Alexander Fraser</p><p>Abstract: We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the “N-gram” model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.</p><p>4 0.76527578 <a title="263-lsi-4" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>Author: Susan Howlett ; Mark Dras</p><p>Abstract: There are a number of systems that use a syntax-based reordering step prior to phrasebased statistical MT. An early work proposing this idea showed improved translation performance, but subsequent work has had mixed results. Speculations as to cause have suggested the parser, the data, or other factors. We systematically investigate possible factors to give an initial answer to the question: Under what conditions does this use of syntax help PSMT?</p><p>5 0.75608689 <a title="263-lsi-5" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>Author: Alexandra Birch ; Miles Osborne</p><p>Abstract: One of the major challenges facing statistical machine translation is how to model differences in word order between languages. Although a great deal of research has focussed on this problem, progress is hampered by the lack of reliable metrics. Most current metrics are based on matching lexical items in the translation and the reference, and their ability to measure the quality of word order has not been demonstrated. This paper presents a novel metric, the LRscore, which explicitly measures the quality of word order by using permutation distance metrics. We show that the metric is more consistent with human judgements than other metrics, including the BLEU score. We also show that the LRscore can successfully be used as the objective function when training translation model parameters. Training with the LRscore leads to output which is preferred by humans. Moreover, the translations incur no penalty in terms of BLEU scores.</p><p>6 0.6734429 <a title="263-lsi-6" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>7 0.61152762 <a title="263-lsi-7" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>8 0.60657746 <a title="263-lsi-8" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>9 0.56931204 <a title="263-lsi-9" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>10 0.56882524 <a title="263-lsi-10" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>11 0.53699583 <a title="263-lsi-11" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>12 0.53262609 <a title="263-lsi-12" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>13 0.52712387 <a title="263-lsi-13" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>14 0.51939499 <a title="263-lsi-14" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>15 0.50489789 <a title="263-lsi-15" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>16 0.49515823 <a title="263-lsi-16" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>17 0.49187443 <a title="263-lsi-17" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>18 0.48051256 <a title="263-lsi-18" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>19 0.47915223 <a title="263-lsi-19" href="./acl-2011-Hindi_to_Punjabi_Machine_Translation_System.html">151 acl-2011-Hindi to Punjabi Machine Translation System</a></p>
<p>20 0.47599223 <a title="263-lsi-20" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.021), (12, 0.256), (17, 0.078), (26, 0.023), (31, 0.01), (37, 0.081), (39, 0.041), (41, 0.045), (55, 0.025), (59, 0.036), (72, 0.022), (91, 0.035), (96, 0.231)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87648094 <a title="263-lda-1" href="./acl-2011-Contrasting_Opposing_Views_of_News_Articles_on_Contentious_Issues.html">84 acl-2011-Contrasting Opposing Views of News Articles on Contentious Issues</a></p>
<p>Author: Souneil Park ; Kyung Soon Lee ; Junehwa Song</p><p>Abstract: We present disputant relation-based method for classifying news articles on contentious issues. We observe that the disputants of a contention are an important feature for understanding the discourse. It performs unsupervised classification on news articles based on disputant relations, and helps readers intuitively view the articles through the opponent-based frame. The readers can attain balanced understanding on the contention, free from a specific biased view. We applied a modified version of HITS algorithm and an SVM classifier trained with pseudo-relevant data for article analysis. 1</p><p>same-paper 2 0.84011352 <a title="263-lda-2" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in JapaneseEnglish translation and 1.41% BLEU points in English-Japanese translation.</p><p>3 0.7686038 <a title="263-lda-3" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>Author: William M. Darling ; Fei Song</p><p>Abstract: Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.</p><p>4 0.72300625 <a title="263-lda-4" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>5 0.72056007 <a title="263-lda-5" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>6 0.71818787 <a title="263-lda-6" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>7 0.71610707 <a title="263-lda-7" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>8 0.71514517 <a title="263-lda-8" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>9 0.71501625 <a title="263-lda-9" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>10 0.71465433 <a title="263-lda-10" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>11 0.71401823 <a title="263-lda-11" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>12 0.7137019 <a title="263-lda-12" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>13 0.71335268 <a title="263-lda-13" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>14 0.71254802 <a title="263-lda-14" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>15 0.71252847 <a title="263-lda-15" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>16 0.71243668 <a title="263-lda-16" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>17 0.71196038 <a title="263-lda-17" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>18 0.7119143 <a title="263-lda-18" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>19 0.71137965 <a title="263-lda-19" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>20 0.71128017 <a title="263-lda-20" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
