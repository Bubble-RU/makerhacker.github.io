<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-40" href="#">acl2011-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</h1>
<br/><p>Source: <a title="acl-2011-40-pdf" href="http://aclweb.org/anthology//P/P11/P11-3012.pdf">pdf</a></p><p>Author: Gregory Brown</p><p>Abstract: Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another (e.g. partof). This paper presents an analysis of a relation extraction system based on prior work but applied to the J.D. Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media. The results are examined on three different subsets of the JDPA Corpus, showing that the system performs much worse on documents from certain sources. The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs.</p><p>Reference: <a title="acl-2011-40-reference" href="../acl2011_reference/acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An Error Analysis of Relation Extraction in Social Media Documents Gregory Ichneumon Brown University of Colorado at Boulder Boulder, Colorado browngp @ co l orado . [sent-1, score-0.019]
</p><p>2 edu  Abstract Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another (e. [sent-2, score-0.263]
</p><p>3 This paper presents an analysis of a relation extraction system based on prior work but applied to the J. [sent-5, score-0.179]
</p><p>4 Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media. [sent-7, score-0.209]
</p><p>5 The results are examined on three different subsets of the JDPA Corpus, showing that the system performs much worse on documents from certain sources. [sent-8, score-0.177]
</p><p>6 The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs. [sent-9, score-0.166]
</p><p>7 1 Introduction To summarize accurately, determine the sentiment, or answer questions about a document it is often nec-  essary to be able to determine the relationships between entities being discussed in the document (such as part-of or member-of). [sent-10, score-0.166]
</p><p>8 Ilove  determining the sentiment the author is expressing about the car requires knowing that the engine is a part of the car so that the positive sentiment being expressed about the engine can also be attributed to the car. [sent-14, score-0.412]
</p><p>9 In this paper we examine our preliminary results from applying a relation extraction system to the 64 J. [sent-15, score-0.199]
</p><p>10 Our system uses lexical features from prior work to classify relations, and we examine how the system works on different subsets from the JDPA Sentiment Corpus, breaking the source documents down into professionally written reviews, blog reviews, and social networking re-  views. [sent-19, score-0.399]
</p><p>11 These three document types represent quite different writing styles, and we see significant difference in how the relation extraction system performs on the documents from different sources. [sent-20, score-0.392]
</p><p>12 , 2005) is one of the most common corpora for performing relation extraction. [sent-23, score-0.119]
</p><p>13 In addition to the co-reference annotations, the Corpus is annotated to indicate 23 different relations between real-world entities that are mentioned in the same sentence. [sent-24, score-0.095]
</p><p>14 The documents consist of broadcast news transcripts and newswire articles from a variety of news organizations. [sent-25, score-0.252]
</p><p>15 2 JDPA Sentiment Corpus The JDPA Corpus consists of 457 documents containing discussions about cars, and 180 documents discussing cameras (Kessler et al. [sent-27, score-0.31]
</p><p>16 In this work we only use the automotive documents. [sent-29, score-0.046]
</p><p>17 The documents are drawn from a variety of sources, and we particularly focus on the 24% of the doc-  uments from the JDPA Power Steering blog, 18% from Blogspot, and 18% from LiveJournal. [sent-30, score-0.152]
</p><p>18 c T2 2001111 A Sstsuodceinatti Soens fsoiorn C,o pmagpeusta 64ti–o6n8a,l Linguistics The annotated mentions in the Corpus are single or multi-word expressions which refer to a particular real world or abstract entity. [sent-33, score-0.1]
</p><p>19 The mentions are annotated to indicate sets of mentions which constitute co-reference groups referring to the same entity. [sent-34, score-0.241]
</p><p>20 Five relationships are annotated between these entities: PartOf, FeatureOf, Produces, InstanceOf, and MemberOf. [sent-35, score-0.029]
</p><p>21 One significant difference between these relation annotations and those in the ACE Corpus is that the former are relations between sets of mentions (the co-reference groups) rather than between individual mentions. [sent-36, score-0.292]
</p><p>22 This means that these relations are not limited to being between mentions in the same sentence. [sent-37, score-0.156]
</p><p>23 1, “engine” would be marked as a part of “car” in the JDPA Corpus annotations, but there would be no relation annotated in the ACE Corpus. [sent-39, score-0.119]
</p><p>24 For a more direct comparison to the ACE Corpus results, we restrict ourselves only to mentions within the same sentence (we discuss this decision further in section 5. [sent-40, score-0.1]
</p><p>25 1 Overview The system extracts all pairs of mentions in a sentence, and then classifies each pair of mentions as either having a relationship, having an inverse relationship, or having no relationship. [sent-43, score-0.217]
</p><p>26 So for the PartOf relation in the JDPA Sentiment Corpus we consider both the relation “X is part of Y” and “Y is part of X”. [sent-44, score-0.238]
</p><p>27 The classification of each mention pair is performed using a support vector machine implemented using libLinear (Fan et al. [sent-45, score-0.094]
</p><p>28 To generate the features for each of the mention pairs a proprietary JDPA Tokenizer is used for parsing the document and the Stanford Parser (Klein and Manning, 2003) is used to generate parse trees and part of speech tags for the sentences in the documents. [sent-47, score-0.165]
</p><p>29 , 2005) as the basis for the features of our system similar to what other researchers have done (Chan and  Roth, 2010). [sent-51, score-0.039]
</p><p>30 Additional work has extended these features (Jiang and Zhai, 2007) or incorporated other data sources (e. [sent-52, score-0.022]
</p><p>31 WordNet), but in this paper we focus solely on the initial step of applying these same 65 lexical features to the JDPA Corpus. [sent-54, score-0.022]
</p><p>32 The Mention Level, Overlap, Base Phrase Chunking, Dependency Tree, and Parse Tree features are the same as Zhou et al. [sent-55, score-0.022]
</p><p>33 Entity Types: Some of the entity types in the JEDnPtAity Corpus :i Sndoimcaete o fth teh type toyf ttyhpee srel inati tohne (e. [sent-59, score-0.072]
</p><p>34 CarFeature, CarPart) and so we replace those entity types with “Unknown”. [sent-61, score-0.035]
</p><p>35 Token Class: We added an additional feature (TC12+ET12) indicating tnhe a dTdoiktioenn Cl fleasast roef  the head words (e. [sent-62, score-0.02]
</p><p>36 • Semantic Information: These features are specific tco Itnhfeo rAmCaEt orenl:ations and so are not used. [sent-65, score-0.04]
</p><p>37 ’s work, this set of features increases the overall F-Measure by 1. [sent-67, score-0.022]
</p><p>38 1 ACE Corpus Results We ran our system on the ACE-2004 Corpus as a baseline to prove that the system worked properly and could approximately duplicate Zhou et al. [sent-70, score-0.034]
</p><p>39 Using 5-fold cross validation on the newswire and broadcast news documents in the dataset we achieved an average overall F-Measure of 50. [sent-72, score-0.223]
</p><p>40 Table 1 shows relation extraction results of the system on the test portion of the corpus. [sent-80, score-0.179]
</p><p>41 The results are further broken out by three different source types to highlight the differences caused  RelationPAll DocRumentFsPLiveJRournalFPBloRgspotFPJDRPAF  IPFOMNAREvSeOMTrDaAUBOlNRECF OSF345671. [sent-81, score-0.057]
</p><p>42 ,9%5267  Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source. [sent-99, score-0.114]
</p><p>43 LiveJournalBlogspotJDPAACE  Table 2: Selected document statistics for three JDPA Corpus document sources. [sent-100, score-0.098]
</p><p>44 by the writing styles from different types of media: LiveJournal (livejournal. [sent-101, score-0.058]
</p><p>45 com), a social media site where users comment and discuss stories with each other; Blogspot (blospot. [sent-102, score-0.076]
</p><p>46 com’s Power Steering blog), consisting of reviews of cars written by JDPA professional writers/analysts. [sent-104, score-0.147]
</p><p>47 These subsets were selected because they provide the extreme (JDPA and LiveJournal) and average (Blogspot) results for the overall dataset. [sent-105, score-0.028]
</p><p>48 5  Analysis  Overall the system is not performing as well as it does on the ACE-2004 dataset. [sent-106, score-0.017]
</p><p>49 However, there is a 25 point F-Measure difference between the LiveJournal and JDPA authored documents. [sent-107, score-0.117]
</p><p>50 This suggests that the informal style of the LiveJournal documents may be reducing the effectiveness of the features developed by Zhou et al. [sent-108, score-0.205]
</p><p>51 , which were developed on newswire and broadcast news transcript documents. [sent-109, score-0.091]
</p><p>52 In the remainder of this section we look at a statistical analysis of the training portion of the JDPA Corpus, separated by document source, and suggest 66 areas where improved features may be able to aid relation extraction on the JDPA Corpus. [sent-110, score-0.233]
</p><p>53 1 Document Statistic Effects on Classifier Table 2 summarizes some important statistical differences between the documents from different  sources. [sent-112, score-0.15]
</p><p>54 These differences suggest two reasons why the instances being used to train the classifier could be skewed disproportionately towards the JDPA authored documents. [sent-113, score-0.189]
</p><p>55 First, the JDPA written documents express a much larger number of relations between entities. [sent-114, score-0.223]
</p><p>56 When training the classifier, these differences will cause a large share of the instances that have a relation to be from a JDPA written document, skewing the classifier towards any language clues specific to these documents. [sent-115, score-0.222]
</p><p>57 Second, the number of mention pairs occurring within one sentence is significantly higher in the JDPA authored documents than the other documents. [sent-116, score-0.343]
</p><p>58 This disparity is even true on a per sentence or per document basis. [sent-117, score-0.083]
</p><p>59 This provides the classifier with significantly more negative examples written in a JDPA written style. [sent-118, score-0.1]
</p><p>60 PMLheriavntsei oJonurna%lPMheBranltsoiegonspot%PMheranJtsiDeonPA%  Table 3: Top 10 phrases in mention pairs whose relation was incorrectly classified, and the total percentage of errors from the top ten. [sent-119, score-0.326]
</p><p>61 2 Common Errors Table 3 shows the mention phrases that occur most commonly in the incorrectly classified mention pairs. [sent-121, score-0.276]
</p><p>62 For the LiveJournal and Blogspot data, many more of the errors are due to a few specific phrases being classified incorrectly such as “car”,  “Maybach”, and various forms of “it”. [sent-122, score-0.114]
</p><p>63 The top four phrases constitute 17% of the errors for LiveJournal and 14% for Blogspot. [sent-123, score-0.087]
</p><p>64 Whereas the JDPA documents have the errors spread more evenly across mention phrases, with the top 10 phrases constituting 13. [sent-124, score-0.293]
</p><p>65 Furthermore, the phrases causing many of the problems for the LiveJournal and Blogspot relation detection are generic nouns and pronouns such as “car” and “it”. [sent-126, score-0.233]
</p><p>66 This suggests that the classifier is having difficulty determining relationships when these less descriptive words are involved. [sent-127, score-0.115]
</p><p>67 67  Table 4: Frequency of some common words per token. [sent-130, score-0.017]
</p><p>68 We find that despite all the documents discussing  cars, the JDPA reviews use the word “car” much less often, and use proper nouns significantly more often. [sent-132, score-0.213]
</p><p>69 Although “car” also appears in the top ten errors on the JDPA documents, the total percentage of the errors is one fifth of the error rate on the LiveJournal documents. [sent-133, score-0.07]
</p><p>70 The JDPA authored documents also tend to have more multi-word mention phrases (Table 2) suggesting that the authors use more descriptive language when referring to an entity. [sent-134, score-0.461]
</p><p>71 3% of the mentions in LiveJournal documents use only a single word while 61. [sent-136, score-0.232]
</p><p>72 2% of mentions JDPA authored documents are a single word. [sent-137, score-0.349]
</p><p>73 Rather than descriptive noun phrases, the LiveJournal and Blogspot documents make more use of pronouns. [sent-138, score-0.188]
</p><p>74 LiveJournal especially uses pronouns often, to the point of averaging one per sentence, while JDPA uses only one every five sentences. [sent-139, score-0.044]
</p><p>75 4  Extra-Sentential Relations  Many relations in the JDPA Corpus occur between entities which are not mentioned in the same sentence. [sent-141, score-0.095]
</p><p>76 Our system only detects relations between mentions in the same sentence, causing about 29% of entity relations to never be detected (Table 2). [sent-142, score-0.291]
</p><p>77 The LiveJournal documents are more likely to contain relationships between entities that are not mentioned in the same sentence. [sent-143, score-0.2]
</p><p>78 Improvements in entity relation extraction could likely be made by extending Zhou et al. [sent-145, score-0.197]
</p><p>79 6  Conclusion  The above analysis shows that at least some of the reason for the system performing worse on the JDPA Corpus than on the ACE-2004 Corpus is that many of the documents in the JDPA Corpus have a different writing style from the news articles in the ACE Corpus. [sent-147, score-0.237]
</p><p>80 Both the ACE news documents, and the JDPA authored documents are written by professional writers with stronger editorial standards than the other JDPA Corpus documents, and the relation extraction system performs much better on professionally edited documents. [sent-148, score-0.616]
</p><p>81 The heavy use of pronouns and less descriptive mention phrases in the other documents seems to be one cause of the re-  duction in relation extraction performance. [sent-149, score-0.531]
</p><p>82 There is also some evidence that because of the greater number ofrelations in the JPDA authored documents that the classifier training data could be skewed more towards those documents. [sent-150, score-0.323]
</p><p>83 Future work needs to explore features that can address the difference in language usage that the different authors use. [sent-151, score-0.022]
</p><p>84 This work also does not address whether the relation extraction task is being negatively impacted by poor tokenization or parsing of the documents rather than the problems being caused by the relation classification itself. [sent-152, score-0.45]
</p><p>85 Further work is also needed to classify extra-sentential relations, as the current methods look only at relations occurring within a single sentence thus ignoring a large percentage of relations between entities. [sent-153, score-0.13]
</p><p>86 A systematic exploration of the feature space for relation extraction. [sent-185, score-0.119]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('jdpa', 0.853), ('livejournal', 0.225), ('documents', 0.132), ('blogspot', 0.122), ('relation', 0.119), ('authored', 0.117), ('ace', 0.101), ('mentions', 0.1), ('car', 0.097), ('mention', 0.094), ('zhou', 0.086), ('sentiment', 0.082), ('kessler', 0.081), ('partof', 0.069), ('descriptive', 0.056), ('relations', 0.056), ('document', 0.049), ('cars', 0.048), ('automotive', 0.046), ('steering', 0.046), ('extraction', 0.043), ('phrases', 0.041), ('social', 0.04), ('entities', 0.039), ('editorial', 0.037), ('nicolov', 0.037), ('broadcast', 0.037), ('power', 0.037), ('media', 0.036), ('reviews', 0.036), ('chai', 0.035), ('professionally', 0.035), ('blog', 0.035), ('entity', 0.035), ('written', 0.035), ('writing', 0.032), ('srl', 0.031), ('gerber', 0.031), ('classifier', 0.03), ('associates', 0.03), ('tokenizer', 0.03), ('relationships', 0.029), ('liblinear', 0.029), ('news', 0.029), ('subsets', 0.028), ('incorrectly', 0.028), ('professional', 0.028), ('corpus', 0.027), ('causing', 0.027), ('engine', 0.027), ('pronouns', 0.027), ('style', 0.027), ('discussing', 0.026), ('styles', 0.026), ('errors', 0.026), ('chan', 0.026), ('newswire', 0.025), ('standards', 0.024), ('skewed', 0.024), ('informal', 0.024), ('fan', 0.023), ('broken', 0.022), ('features', 0.022), ('zhai', 0.021), ('referring', 0.021), ('colorado', 0.02), ('blogging', 0.02), ('cameras', 0.02), ('honorific', 0.02), ('impacted', 0.02), ('nielsen', 0.02), ('ofrelations', 0.02), ('rodney', 0.02), ('roef', 0.02), ('skewing', 0.02), ('uments', 0.02), ('constitute', 0.02), ('boulder', 0.02), ('examine', 0.02), ('classified', 0.019), ('nouns', 0.019), ('ations', 0.019), ('duction', 0.019), ('headden', 0.019), ('orado', 0.019), ('toyf', 0.019), ('roth', 0.018), ('jiang', 0.018), ('differences', 0.018), ('percentage', 0.018), ('jim', 0.018), ('tohne', 0.018), ('networking', 0.018), ('ilove', 0.018), ('tco', 0.018), ('system', 0.017), ('per', 0.017), ('annotations', 0.017), ('caused', 0.017), ('mitchell', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="40-tfidf-1" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>Author: Gregory Brown</p><p>Abstract: Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another (e.g. partof). This paper presents an analysis of a relation extraction system based on prior work but applied to the J.D. Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media. The results are examined on three different subsets of the JDPA Corpus, showing that the system performs much worse on documents from certain sources. The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs.</p><p>2 0.12691125 <a title="40-tfidf-2" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>Author: Ang Sun ; Ralph Grishman ; Satoshi Sekine</p><p>Abstract: We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system. 1</p><p>3 0.10409205 <a title="40-tfidf-3" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>4 0.10260194 <a title="40-tfidf-4" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>5 0.092016958 <a title="40-tfidf-5" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>Author: Harr Chen ; Edward Benson ; Tahira Naseem ; Regina Barzilay</p><p>Abstract: We present a novel approach to discovering relations and their instantiations from a collection of documents in a single domain. Our approach learns relation types by exploiting meta-constraints that characterize the general qualities of a good relation in any domain. These constraints state that instances of a single relation should exhibit regularities at multiple levels of linguistic structure, including lexicography, syntax, and document-level context. We capture these regularities via the structure of our probabilistic model as well as a set of declaratively-specified constraints enforced during posterior inference. Across two domains our approach successfully recovers hidden relation structure, comparable to or outperforming previous state-of-the-art approaches. Furthermore, we find that a small , set of constraints is applicable across the domains, and that using domain-specific constraints can further improve performance. 1</p><p>6 0.086001009 <a title="40-tfidf-6" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>7 0.081988238 <a title="40-tfidf-7" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>8 0.075971924 <a title="40-tfidf-8" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>9 0.072139755 <a title="40-tfidf-9" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>10 0.067897953 <a title="40-tfidf-10" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>11 0.067754611 <a title="40-tfidf-11" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>12 0.065721333 <a title="40-tfidf-12" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>13 0.063381463 <a title="40-tfidf-13" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>14 0.060891535 <a title="40-tfidf-14" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>15 0.060680397 <a title="40-tfidf-15" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>16 0.058252051 <a title="40-tfidf-16" href="./acl-2011-Semi-supervised_latent_variable_models_for_sentence-level_sentiment_analysis.html">279 acl-2011-Semi-supervised latent variable models for sentence-level sentiment analysis</a></p>
<p>17 0.057933029 <a title="40-tfidf-17" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>18 0.057850808 <a title="40-tfidf-18" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>19 0.05718847 <a title="40-tfidf-19" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>20 0.050804619 <a title="40-tfidf-20" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, 0.1), (2, -0.043), (3, -0.001), (4, 0.086), (5, 0.031), (6, -0.004), (7, -0.035), (8, -0.082), (9, 0.001), (10, 0.025), (11, -0.012), (12, -0.042), (13, -0.045), (14, 0.017), (15, 0.02), (16, -0.012), (17, -0.073), (18, 0.015), (19, 0.025), (20, -0.046), (21, 0.01), (22, 0.022), (23, -0.017), (24, -0.035), (25, -0.015), (26, 0.073), (27, 0.018), (28, 0.073), (29, -0.035), (30, -0.067), (31, -0.004), (32, 0.018), (33, -0.024), (34, -0.02), (35, -0.012), (36, -0.075), (37, 0.005), (38, 0.009), (39, 0.1), (40, -0.082), (41, 0.071), (42, 0.057), (43, -0.014), (44, -0.013), (45, 0.003), (46, 0.018), (47, -0.024), (48, -0.01), (49, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93736577 <a title="40-lsi-1" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>Author: Gregory Brown</p><p>Abstract: Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another (e.g. partof). This paper presents an analysis of a relation extraction system based on prior work but applied to the J.D. Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media. The results are examined on three different subsets of the JDPA Corpus, showing that the system performs much worse on documents from certain sources. The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs.</p><p>2 0.76123166 <a title="40-lsi-2" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>Author: Ang Sun ; Ralph Grishman ; Satoshi Sekine</p><p>Abstract: We present a simple semi-supervised relation extraction system with large-scale word clustering. We focus on systematically exploring the effectiveness of different cluster-based features. We also propose several statistical methods for selecting clusters at an appropriate level of granularity. When training on different sizes of data, our semi-supervised approach consistently outperformed a state-of-the-art supervised baseline system. 1</p><p>3 0.71079218 <a title="40-lsi-3" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>4 0.68609405 <a title="40-lsi-4" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>5 0.67919821 <a title="40-lsi-5" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>Author: Truc Vien T. Nguyen ; Alessandro Moschitti</p><p>Abstract: In this paper, we extend distant supervision (DS) based on Wikipedia for Relation Extraction (RE) by considering (i) relations defined in external repositories, e.g. YAGO, and (ii) any subset of Wikipedia documents. We show that training data constituted by sentences containing pairs of named entities in target relations is enough to produce reliable supervision. Our experiments with state-of-the-art relation extraction models, trained on the above data, show a meaningful F1 of 74.29% on a manually annotated test set: this highly improves the state-of-art in RE using DS. Additionally, our end-to-end experiments demonstrated that our extractors can be applied to any general text document.</p><p>6 0.64333081 <a title="40-lsi-6" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>7 0.62586761 <a title="40-lsi-7" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>8 0.6195153 <a title="40-lsi-8" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>9 0.51728523 <a title="40-lsi-9" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<p>10 0.50893986 <a title="40-lsi-10" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>11 0.48155177 <a title="40-lsi-11" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>12 0.47820118 <a title="40-lsi-12" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>13 0.45657688 <a title="40-lsi-13" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>14 0.44516647 <a title="40-lsi-14" href="./acl-2011-Automatically_Evaluating_Text_Coherence_Using_Discourse_Relations.html">53 acl-2011-Automatically Evaluating Text Coherence Using Discourse Relations</a></p>
<p>15 0.43749043 <a title="40-lsi-15" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>16 0.42057586 <a title="40-lsi-16" href="./acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution.html">212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</a></p>
<p>17 0.41910291 <a title="40-lsi-17" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>18 0.41072863 <a title="40-lsi-18" href="./acl-2011-Semi-supervised_latent_variable_models_for_sentence-level_sentiment_analysis.html">279 acl-2011-Semi-supervised latent variable models for sentence-level sentiment analysis</a></p>
<p>19 0.40217996 <a title="40-lsi-19" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>20 0.39707294 <a title="40-lsi-20" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.03), (17, 0.042), (26, 0.026), (31, 0.013), (35, 0.239), (37, 0.09), (39, 0.043), (41, 0.109), (55, 0.02), (59, 0.059), (72, 0.048), (91, 0.038), (96, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78526986 <a title="40-lda-1" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>Author: Gregory Brown</p><p>Abstract: Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another (e.g. partof). This paper presents an analysis of a relation extraction system based on prior work but applied to the J.D. Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media. The results are examined on three different subsets of the JDPA Corpus, showing that the system performs much worse on documents from certain sources. The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs.</p><p>2 0.75676715 <a title="40-lda-2" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>Author: Cheng-Te Li ; Chien-Yuan Wang ; Chien-Lin Tseng ; Shou-De Lin</p><p>Abstract: Micro-blogging services provide platforms for users to share their feelings and ideas on the move. In this paper, we present a search-based demonstration system, called MemeTube, to summarize the sentiments of microblog messages in an audiovisual manner. MemeTube provides three main functions: (1) recognizing the sentiments of messages (2) generating music melody automatically based on detected sentiments, and (3) produce an animation of real-time piano playing for audiovisual display. Our MemeTube system can be accessed via: http://mslab.csie.ntu.edu.tw/memetube/ .</p><p>3 0.70329833 <a title="40-lda-3" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>Author: Charles Greenbacker</p><p>Abstract: We propose a framework for generating an abstractive summary from a semantic model of a multimodal document. We discuss the type of model required, the means by which it can be constructed, how the content of the model is rated and selected, and the method of realizing novel sentences for the summary. To this end, we introduce a metric called information density used for gauging the importance of content obtained from text and graphical sources.</p><p>4 0.6572063 <a title="40-lda-4" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>Author: Shasha Liao ; Ralph Grishman</p><p>Abstract: Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 1</p><p>5 0.64873272 <a title="40-lda-5" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>Author: Joel Lang ; Mirella Lapata</p><p>Abstract: In this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. We present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality. The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.</p><p>6 0.6474489 <a title="40-lda-6" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>7 0.64613557 <a title="40-lda-7" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>8 0.6412549 <a title="40-lda-8" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>9 0.64037365 <a title="40-lda-9" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>10 0.637429 <a title="40-lda-10" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>11 0.6350584 <a title="40-lda-11" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>12 0.6347872 <a title="40-lda-12" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>13 0.63444251 <a title="40-lda-13" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>14 0.63301671 <a title="40-lda-14" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>15 0.63106722 <a title="40-lda-15" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>16 0.63092136 <a title="40-lda-16" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>17 0.63061047 <a title="40-lda-17" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>18 0.63054574 <a title="40-lda-18" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>19 0.63032687 <a title="40-lda-19" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>20 0.63003093 <a title="40-lda-20" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
