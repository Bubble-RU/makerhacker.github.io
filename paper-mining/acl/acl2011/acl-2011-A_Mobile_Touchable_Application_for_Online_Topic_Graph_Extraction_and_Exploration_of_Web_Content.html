<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-19" href="#">acl2011-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</h1>
<br/><p>Source: <a title="acl-2011-19-pdf" href="http://aclweb.org/anthology//P/P11/P11-4004.pdf">pdf</a></p><p>Author: Gunter Neumann ; Sven Schmeier</p><p>Abstract: We present a mobile touchable application for online topic graph extraction and exploration of web content. The system has been implemented for operation on an iPad. The topic graph is constructed from N web snippets which are determined by a standard search engine. We consider the extraction of a topic graph as a specific empirical collocation extraction task where collocations are extracted between chunks. Our measure of association strength is based on the pointwise mutual information between chunk pairs which explicitly takes their distance into account. An initial user evaluation shows that this system is especially helpful for finding new interesting information on topics about which the user has only a vague idea or even no idea at all.</p><p>Reference: <a title="acl-2011-19-reference" href="../acl2011_reference/acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content G u¨nter Neumann and Sven Schmeier Language Technology Lab, DFKI GmbH Stuhlsatzenhausweg 3, D-66123 Saarbr¨ ucken {neumann |s chme ie r} @ dfki . [sent-1, score-0.062]
</p><p>2 de  Abstract We present a mobile touchable application for online topic graph extraction and exploration of web content. [sent-2, score-0.945]
</p><p>3 The topic graph is constructed from N web snippets which are determined by a standard search engine. [sent-4, score-0.995]
</p><p>4 We consider the extraction of a topic graph as a specific empirical collocation extraction task where collocations are extracted between chunks. [sent-5, score-0.585]
</p><p>5 Our measure of association strength is based on the pointwise mutual information between chunk pairs which explicitly takes their distance into account. [sent-6, score-0.415]
</p><p>6 An initial user evaluation shows that this system is especially helpful for finding new interesting information on topics about which the user has only a vague idea or even no idea at all. [sent-7, score-0.449]
</p><p>7 1 Introduction  Today’s Web search is still dominated by a document perspective: a user enters one or more keywords that represent the information of interest and receives a ranked list of documents. [sent-8, score-0.263]
</p><p>8 This technology has been shown to be very successful when used on an ordinary computer, because it very often delivers concrete documents or web pages that contain the information the user is interested in. [sent-9, score-0.328]
</p><p>9 2) The documents serve as answers to user queries. [sent-11, score-0.167]
</p><p>10 If the user only has a vague idea of the information in question or just wants to explore the infor20 mation space, the current search engine paradigm does not provide enough assistance for these kind of searches. [sent-13, score-0.357]
</p><p>11 The user has to read through the documents and then eventually reformulate the query in order to find new information. [sent-14, score-0.248]
</p><p>12 This can be a tedious task especially on mobile devices. [sent-15, score-0.117]
</p><p>13 Seen in this con-  text, current search engines seem to be best suited for “one-shot search” and do not support contentoriented interaction. [sent-16, score-0.06]
</p><p>14 In order to overcome this restricted document perspective, and to provide a mobile device searches to “find out about something”, we want to help users with the web content exploration process in two ways: 1. [sent-17, score-0.458]
</p><p>15 We consider a user query as a specification of a topic that the user wants to know and learn more about. [sent-18, score-0.669]
</p><p>16 Hence, the search result is basically a graphical structure of the topic and associated topics that are found. [sent-19, score-0.348]
</p><p>17 The user can interactively explore this topic graph using a simple and intuitive touchable user interface in order to either learn more about the content of a topic or to interactively expand a topic with newly computed related topics. [sent-21, score-1.54]
</p><p>18 In the first step, the topic graph is computed on the fly from the a set of web snippets that has been collected by a standard search engine using the ini-  tial user query. [sent-22, score-1.331]
</p><p>19 Rather than considering each snippet in isolation, all snippets are collected into one document from which the topic graph is computed. [sent-23, score-0.854]
</p><p>20 We consider each topic as an entity, and the edges PortlandP, Ororce geodnin,g UsS oAf, t 2h1e J AuCnLe-2H 0L1T1. [sent-24, score-0.271]
</p><p>21 The content of a topic are the set of snippets it has been extracted from, and the documents retrievable via the snippets’ web links. [sent-27, score-0.741]
</p><p>22 A topic graph is then displayed on a mobile device (in our case an iPad) as a touch-sensitive graph. [sent-28, score-0.575]
</p><p>23 By just touching on a node, the user can either inspect the content of a topic (i. [sent-29, score-0.513]
</p><p>24 e, the snippets or web pages) or activate the expansion ofthe graph through an on the fly computation of new related topics for the selected node. [sent-30, score-0.817]
</p><p>25 In a second step, we provide additional background knowledge on the topic which consists of explicit relationships that are generated from an online Encyclopedia (in our case Wikipedia). [sent-31, score-0.312]
</p><p>26 The relevant background relation graph is also represented as a touchable graph in the same way as a topic graph. [sent-32, score-0.826]
</p><p>27 In this way the user can explore in an uniform way both new information nuggets and validated background information nuggets interactively. [sent-34, score-0.33]
</p><p>28 2  Touchable User Interface: Examples  The following screenshots show some results for the search query “Justin Bieber” running on the cur21 rent iPad demo–app. [sent-38, score-0.141]
</p><p>29 At the bottom of the iPad screen, the user can select whether to perform text exploration from the Web (via button labeled “i– GNSSMM”) or via Wikipedia (touching button “i– MILREX”). [sent-39, score-0.412]
</p><p>30 , language selection (so far, English and German are supported) or selection of the maximum number of snippets to be retrieved for each query. [sent-45, score-0.359]
</p><p>31 The other parameters mainly affect the display structure of the topic graph. [sent-46, score-0.274]
</p><p>32 Figure 2: The topic graph computed from the snippets for the query “Justin Bieber”. [sent-47, score-0.91]
</p><p>33 The user can double touch on a node to display the associated snippets and web pages. [sent-48, score-1.004]
</p><p>34 Since a topic graph can be very large, not all nodes are displayed. [sent-49, score-0.472]
</p><p>35 A single touch on such a node expands it, as shown in Fig. [sent-51, score-0.229]
</p><p>36 A single touch on a node that cannot be expanded adds its label to the initial user query and triggers a new search with that expanded query. [sent-53, score-0.703]
</p><p>37 2 has been expanded by a single touch on the node labeled “selena gomez”. [sent-55, score-0.283]
</p><p>38 Double touching on that node triggers the display of associated web snippets (Fig. [sent-56, score-0.858]
</p><p>39 3  Topic Graph Extraction  We consider the extraction of a topic graph as a specific empirical collocation extraction task. [sent-59, score-0.551]
</p><p>40 How-  ever, instead of extracting collations between words, which is still the dominating approach in collocation extraction research, e. [sent-60, score-0.103]
</p><p>41 Furthermore, our measure of association strength takes into account the distance between chunks and combines it with the PMI (pointwise mutual information) approach (Turney, 2001). [sent-65, score-0.285]
</p><p>42 The core idea is to compute a set of chunk– pair–distance elements for the N first web snippets returned by a search engine for the topic Q, and to compute the topic graph from these elements. [sent-66, score-1.31]
</p><p>43 1 In general for two chunks, a single chunk– pair–distance element stores the distance between 1For the remainder of the paper N=1000. [sent-67, score-0.127]
</p><p>44 are  using Bing  22  Figure 4: The snippets that are associated with the node label “selena gomez” of the topic graph from Fig. [sent-71, score-0.876]
</p><p>45 In order to go back to the topic graph, the user simply touches the button labeled i-GNSSMM on the left upper corner of the iPad screen. [sent-73, score-0.524]
</p><p>46 the chunks by counting the number of chunks in– between them. [sent-74, score-0.318]
</p><p>47 We begin by creating a document S from the N-first web snippets so that each line of S contains a complete snippet. [sent-77, score-0.52]
</p><p>48 The En-  Figure 5: The web page associated with the first snippet of Fig. [sent-81, score-0.241]
</p><p>49 A single touch on that snippet triggers a call to the iPad browser in order to display the corresponding web page. [sent-83, score-0.479]
</p><p>50 The left upper corner button labeled “Snippets” has to be touched in order to go back to the snippets page. [sent-84, score-0.495]
</p><p>51 We finally apply a kind of “phrasal head test” on each identified chunk to guarantee that the right–most element only belongs to a proper noun or verb tag. [sent-86, score-0.407]
</p><p>52 For example, the chunk “a/DT british/NNP formula/NNP one/NN racing/VBG driver/NN from/IN scotland/NNP” would be accepted as proper NP  chunk, where “compelling/VBG power/NN of/IN” is not. [sent-87, score-0.289]
</p><p>53 Performing this sort of shallow chunking is based on the assumptions: 1) noun groups can represent the arguments of a relation, a verb group the relation itself, and 2) web snippet chunking needs highly robust NL technologies. [sent-88, score-0.402]
</p><p>54 23  Figure 6: If mode “i–MILREX” is chosen then text exploration is performed based on relations computed from the info–boxes extracted from Wikipedia. [sent-92, score-0.15]
</p><p>55 The outer nodes represent the arguments and the inner nodes the predicate of a info–box relation. [sent-94, score-0.114]
</p><p>56 The center of the graph corresponds to the search query. [sent-95, score-0.254]
</p><p>57 Web snippets are even harder to process because they are not necessary contiguous pieces of texts,  and usually are not syntactically well-formed paragraphs due to some intentionally introduced breaks (e. [sent-97, score-0.359]
</p><p>58 On the other hand, we want to benefit from PoS tagging during chunk recognition in order to be able to identify, on the fly, a shallow phrase structure in web snippets with minimal efforts. [sent-103, score-0.809]
</p><p>59 The chunk–pair–distance model is computed from the list of chunks. [sent-104, score-0.055]
</p><p>60 This is done by traversing the chunks from left to right. [sent-105, score-0.159]
</p><p>61 For each chunk ci, a set is computed by considering all remaining chunks and their distance to ci, i. [sent-106, score-0.583]
</p><p>62 We do this for each chunk list computed for each web snippet. [sent-109, score-0.505]
</p><p>63 The distance distij of two chunks ci and cj is computed directly from the chunk list, i. [sent-110, score-0.92]
</p><p>64 The motivation for using chunk–pair–distance statistics is the assumption that the strength of hidden relationships between chunks can be covered by means of their collocation degree and the frequency oftheir relative positions in sentences extracted from web snippets; cf. [sent-113, score-0.466]
</p><p>65 Finally, we compute the frequencies of each chunk, each chunk pair, and each chunk pair distance. [sent-115, score-0.614]
</p><p>66 It is used for constructing the topic graph in the final step. [sent-117, score-0.415]
</p><p>67 Formally, a topic graph TG = (V, E, A) consists of a set V of nodes, a set E of edges, and a set A of node actions. [sent-118, score-0.517]
</p><p>68 Each node v ∈ V represents a chunk and aisc ltaiobnesle. [sent-119, score-0.391]
</p><p>69 The nodes and edges are computed from the chunk–pair–distance elements. [sent-124, score-0.162]
</p><p>70 Since, the number of these elements is quite large (up to several thousands), the elements are ranked according to a weighting scheme which takes into account the frequency information of the chunks and their collocations. [sent-125, score-0.285]
</p><p>71 More precisely, the weight of a chunk–pair– distance element cpd = (ci, cj , Dij), with Di,j = {(freq1 , dist1) , (freq2, dist2) , . [sent-126, score-0.386]
</p><p>72 , (freqn, distn)}, {is( computed based on PMI as follows: PMI(cpd) = log2 ((p(ci, cj)/(p(ci)  ∗  p(cj)))  = log2(p(ci,  cj)) − log2(p(ci) ∗ p(cj))  where relative frequency is used for approximating the probabilities p(ci) and p(cj) . [sent-129, score-0.055]
</p><p>73 Note also that k is actually restricted by the number of chunks in a snippet. [sent-131, score-0.159]
</p><p>74 24 The visualized topic graph TG is then computed from a subset CPDM0 ⊂ CPDM using the m highest ranked cpd for fixe⊂d ci. [sent-132, score-0.607]
</p><p>75 4  Wikipedia’s Infoboxes  In order to provide query specific background knowledge we make use of Wikipedia’s infoboxes. [sent-134, score-0.142]
</p><p>76 These infoboxes contain facts and important relationships related to articles. [sent-135, score-0.25]
</p><p>77 We also tested DBpedia as a background source (Bizer et al. [sent-136, score-0.061]
</p><p>78 For example, the Wikipedia infobox for Justin Bieber contains eleven basic relations whereas DBpedia has fifty relations containing lots of redundancies. [sent-139, score-0.076]
</p><p>79 In our current prototype, we followed a straightforward approach for extracting infobox relations: We downloaded a snapshot of the whole English Wikipedia database (images excluded), extracted the infoboxes for all articles if available and built a Lucene Index running on our server. [sent-140, score-0.177]
</p><p>80 076 infoboxes representing more than 2 million different searchable titles. [sent-143, score-0.101]
</p><p>81 Currently, we only support exact matches between the user’s query and an infobox title in order to avoid ambiguities. [sent-146, score-0.157]
</p><p>82 We plan to extend our user interface so that the user may choose different options. [sent-147, score-0.376]
</p><p>83 After a brief introduction to our system (and the iPad), the testers were asked to perform three different searches (using Google, i–GNSSMM and i–MILREX) by choosing the queries from a set of ten themes. [sent-152, score-0.157]
</p><p>84 The queries covered definition questions like EEUU and NLF, questions about persons like Justin Bieber, David Beckham, Pete Best, Clark Kent, and Wendy Carlos , and general themes like Brisbane, Balancity, and Adidas. [sent-153, score-0.173]
</p><p>85 ” but also to acquire knowledge about background facts, news, rumors (gossip) and more interesting facts that come into mind during the search. [sent-160, score-0.209]
</p><p>86 Half of the testers were asked to first use Google and then our system in order to compare  the results and the usage on the mobile device. [sent-161, score-0.242]
</p><p>87 We hoped to get feedback concerning the usability of our approach compared to the well known internet search paradigm. [sent-162, score-0.06]
</p><p>88 Here our research focus was to get information on user satisfaction of the search results. [sent-164, score-0.227]
</p><p>89 After each task, both testers had to rate several statements on a Likert scale and a general questionnaire had to be filled out after completing the entire test. [sent-165, score-0.125]
</p><p>90 poor  results first sight55%40%15%query answered 71% 29% interesting facts 33% 33% 33% suprising facts 33% 66% overall feeling 33% 50% 17% 4% Table 2: i-GNSSMM #Question v. [sent-169, score-0.463]
</p><p>91 poor  results first sight43%38%20%query answered 65% 20% 15% -  interesting facts suprising facts overall feeling  62% 66% 54%  24% 15% 28%  10% 13% 14%  4% 6% 4%  The results show that people in general prefer the result representation and accuracy in the Google style. [sent-171, score-0.463]
</p><p>92 Especially for the general themes the presentation of web snippets is more convenient and more easy to understand. [sent-172, score-0.598]
</p><p>93 However when it comes to interesting and suprising facts users enjoyed exploring the results using the topic graph. [sent-173, score-0.505]
</p><p>94 The overall feeling was in favor of our system which might also be due to the fact that it is new and somewhat more playful. [sent-174, score-0.065]
</p><p>95 They liked the paradigm of the explorative search on the iPad and preferred touching the graph instead of reformulating their queries. [sent-180, score-0.379]
</p><p>96 The presentation of background facts in i–MILREX was highly appreciated. [sent-181, score-0.21]
</p><p>97 However some users complained that the topic graph became confusing after expanding more than three nodes. [sent-182, score-0.486]
</p><p>98 As a result, in future versions of our system, we will automatically collapse nodes with higher distances from the node in focus. [sent-183, score-0.159]
</p><p>99 Although all of our test persons make use of standard search engines, most of them can imagine to using our system at least in combination with a search engine even on their own personal computers. [sent-184, score-0.218]
</p><p>100 Mining the web for synonyms: PMIIR versus LSA on TOEFL. [sent-213, score-0.161]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('snippets', 0.359), ('chunk', 0.289), ('ipad', 0.228), ('topic', 0.221), ('graph', 0.194), ('ci', 0.179), ('user', 0.167), ('web', 0.161), ('chunks', 0.159), ('cj', 0.158), ('touchable', 0.156), ('touch', 0.127), ('milrex', 0.125), ('testers', 0.125), ('touching', 0.125), ('facts', 0.119), ('mobile', 0.117), ('dbpedia', 0.11), ('node', 0.102), ('bieber', 0.101), ('cpd', 0.101), ('infoboxes', 0.101), ('gnssmm', 0.094), ('suprising', 0.094), ('button', 0.091), ('query', 0.081), ('snippet', 0.08), ('distance', 0.08), ('infobox', 0.076), ('tg', 0.076), ('neumann', 0.071), ('collocation', 0.07), ('justin', 0.068), ('pmi', 0.066), ('feeling', 0.065), ('fly', 0.065), ('exploration', 0.063), ('disti', 0.063), ('figueroa', 0.063), ('giesbrecht', 0.063), ('gomez', 0.063), ('selena', 0.063), ('dfki', 0.062), ('wikipedia', 0.061), ('background', 0.061), ('search', 0.06), ('triggers', 0.058), ('nodes', 0.057), ('bizer', 0.055), ('computed', 0.055), ('expanded', 0.054), ('pos', 0.054), ('display', 0.053), ('baroni', 0.051), ('gim', 0.051), ('nuggets', 0.051), ('edges', 0.05), ('persons', 0.049), ('engine', 0.049), ('vague', 0.048), ('interactively', 0.048), ('themes', 0.048), ('element', 0.047), ('strength', 0.046), ('corner', 0.045), ('evert', 0.045), ('elements', 0.045), ('chunking', 0.045), ('device', 0.043), ('info', 0.043), ('nez', 0.043), ('users', 0.042), ('interface', 0.042), ('taylor', 0.042), ('questions', 0.038), ('topics', 0.038), ('noun', 0.037), ('answered', 0.037), ('pair', 0.036), ('ranked', 0.036), ('chain', 0.036), ('mary', 0.036), ('double', 0.035), ('collocations', 0.034), ('verb', 0.034), ('wants', 0.033), ('peter', 0.033), ('extraction', 0.033), ('christian', 0.033), ('google', 0.033), ('mode', 0.032), ('lab', 0.032), ('taggers', 0.032), ('searches', 0.032), ('chains', 0.031), ('relationships', 0.03), ('presentation', 0.03), ('interesting', 0.029), ('basically', 0.029), ('expanding', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="19-tfidf-1" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>Author: Gunter Neumann ; Sven Schmeier</p><p>Abstract: We present a mobile touchable application for online topic graph extraction and exploration of web content. The system has been implemented for operation on an iPad. The topic graph is constructed from N web snippets which are determined by a standard search engine. We consider the extraction of a topic graph as a specific empirical collocation extraction task where collocations are extracted between chunks. Our measure of association strength is based on the pointwise mutual information between chunk pairs which explicitly takes their distance into account. An initial user evaluation shows that this system is especially helpful for finding new interesting information on topics about which the user has only a vague idea or even no idea at all.</p><p>2 0.22754358 <a title="19-tfidf-2" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>Author: Christina Sauper ; Aria Haghighi ; Regina Barzilay</p><p>Abstract: We present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets. Our model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties. This approach directly enables discovery of highly rated or inconsistent properties of a product. Our model admits an efficient variational meanfield inference algorithm which can be parallelized and run on large snippet collections. We evaluate our model on a large corpus of snippets from Yelp reviews to assess property and attribute prediction. We demonstrate that it outperforms applicable baselines by a considerable margin.</p><p>3 0.16930312 <a title="19-tfidf-3" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>Author: Jey Han Lau ; Karl Grieser ; David Newman ; Timothy Baldwin</p><p>Abstract: We propose a method for automatically labelling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.</p><p>4 0.14404052 <a title="19-tfidf-4" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>Author: Kugatsu Sadamitsu ; Kuniko Saito ; Kenji Imamura ; Genichiro Kikui</p><p>Abstract: This paper proposes three modules based on latent topics of documents for alleviating “semantic drift” in bootstrapping entity set expansion. These new modules are added to a discriminative bootstrapping algorithm to realize topic feature generation, negative example selection and entity candidate pruning. In this study, we model latent topics with LDA (Latent Dirichlet Allocation) in an unsupervised way. Experiments show that the accuracy of the extracted entities is improved by 6.7 to 28.2% depending on the domain.</p><p>5 0.12263689 <a title="19-tfidf-5" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>Author: Yuening Hu ; Jordan Boyd-Graber ; Brianna Satinoff</p><p>Abstract: Topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data. However, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software. In this work, we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions.</p><p>6 0.11986626 <a title="19-tfidf-6" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>7 0.11792079 <a title="19-tfidf-7" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<p>8 0.1164491 <a title="19-tfidf-8" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>9 0.10753483 <a title="19-tfidf-9" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>10 0.1064963 <a title="19-tfidf-10" href="./acl-2011-Jigs_and_Lures%3A_Associating_Web_Queries_with_Structured_Entities.html">181 acl-2011-Jigs and Lures: Associating Web Queries with Structured Entities</a></p>
<p>11 0.098473467 <a title="19-tfidf-11" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>12 0.09353894 <a title="19-tfidf-12" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>13 0.092742443 <a title="19-tfidf-13" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>14 0.092169091 <a title="19-tfidf-14" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>15 0.090348646 <a title="19-tfidf-15" href="./acl-2011-Structural_Topic_Model_for_Latent_Topical_Structure_Analysis.html">287 acl-2011-Structural Topic Model for Latent Topical Structure Analysis</a></p>
<p>16 0.088096142 <a title="19-tfidf-16" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>17 0.087189436 <a title="19-tfidf-17" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>18 0.086491123 <a title="19-tfidf-18" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>19 0.0816705 <a title="19-tfidf-19" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>20 0.076974735 <a title="19-tfidf-20" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.193), (1, 0.102), (2, -0.086), (3, 0.085), (4, -0.08), (5, -0.128), (6, -0.105), (7, 0.012), (8, 0.001), (9, -0.015), (10, -0.04), (11, 0.057), (12, 0.053), (13, -0.04), (14, 0.097), (15, -0.024), (16, 0.013), (17, -0.097), (18, -0.064), (19, -0.027), (20, -0.004), (21, 0.103), (22, -0.022), (23, 0.015), (24, 0.039), (25, -0.055), (26, -0.008), (27, 0.1), (28, -0.076), (29, -0.036), (30, 0.109), (31, 0.051), (32, -0.016), (33, -0.04), (34, 0.001), (35, -0.078), (36, -0.018), (37, -0.04), (38, -0.073), (39, -0.0), (40, 0.031), (41, -0.09), (42, 0.033), (43, 0.079), (44, 0.203), (45, 0.049), (46, -0.016), (47, 0.048), (48, -0.039), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97030288 <a title="19-lsi-1" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>Author: Gunter Neumann ; Sven Schmeier</p><p>Abstract: We present a mobile touchable application for online topic graph extraction and exploration of web content. The system has been implemented for operation on an iPad. The topic graph is constructed from N web snippets which are determined by a standard search engine. We consider the extraction of a topic graph as a specific empirical collocation extraction task where collocations are extracted between chunks. Our measure of association strength is based on the pointwise mutual information between chunk pairs which explicitly takes their distance into account. An initial user evaluation shows that this system is especially helpful for finding new interesting information on topics about which the user has only a vague idea or even no idea at all.</p><p>2 0.69225371 <a title="19-lsi-2" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>Author: Andrei Popescu-Belis ; Majid Yazdani ; Alexandre Nanchen ; Philip N. Garner</p><p>Abstract: The Automatic Content Linking Device is a just-in-time document retrieval system which monitors an ongoing conversation or a monologue and enriches it with potentially related documents, including multimedia ones, from local repositories or from the Internet. The documents are found using keyword-based search or using a semantic similarity measure between documents and the words obtained from automatic speech recognition. Results are displayed in real time to meeting participants, or to users watching a recorded lecture or conversation.</p><p>3 0.63743138 <a title="19-lsi-3" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>Author: Jey Han Lau ; Karl Grieser ; David Newman ; Timothy Baldwin</p><p>Abstract: We propose a method for automatically labelling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.</p><p>4 0.63584584 <a title="19-lsi-4" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>Author: Yuening Hu ; Jordan Boyd-Graber ; Brianna Satinoff</p><p>Abstract: Topic models have been used extensively as a tool for corpus exploration, and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data. However, creating such extensions requires expertise in machine learning unavailable to potential end-users of topic modeling software. In this work, we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation (LDA) by adding constraints that enforce that sets of words must appear together in the same topic. We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions.</p><p>5 0.62166119 <a title="19-lsi-5" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>Author: Xin Zhao ; Jing Jiang ; Jing He ; Yang Song ; Palakorn Achanauparp ; Ee-Peng Lim ; Xiaoming Li</p><p>Abstract: Summarizing and analyzing Twitter content is an important and challenging task. In this paper, we propose to extract topical keyphrases as one way to summarize Twitter. We propose a context-sensitive topical PageRank method for keyword ranking and a probabilistic scoring function that considers both relevance and interestingness of keyphrases for keyphrase ranking. We evaluate our proposed methods on a large Twitter data set. Experiments show that these methods are very effective for topical keyphrase extraction.</p><p>6 0.58854347 <a title="19-lsi-6" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>7 0.57210487 <a title="19-lsi-7" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>8 0.53686267 <a title="19-lsi-8" href="./acl-2011-Structural_Topic_Model_for_Latent_Topical_Structure_Analysis.html">287 acl-2011-Structural Topic Model for Latent Topical Structure Analysis</a></p>
<p>9 0.53430998 <a title="19-lsi-9" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>10 0.53135127 <a title="19-lsi-10" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>11 0.52875262 <a title="19-lsi-11" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>12 0.52793759 <a title="19-lsi-12" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>13 0.51599991 <a title="19-lsi-13" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>14 0.51038635 <a title="19-lsi-14" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>15 0.50991035 <a title="19-lsi-15" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>16 0.50878799 <a title="19-lsi-16" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<p>17 0.498319 <a title="19-lsi-17" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<p>18 0.49024519 <a title="19-lsi-18" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>19 0.48265091 <a title="19-lsi-19" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>20 0.47354597 <a title="19-lsi-20" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.021), (11, 0.014), (17, 0.486), (26, 0.056), (37, 0.047), (39, 0.033), (41, 0.034), (53, 0.011), (55, 0.012), (59, 0.024), (72, 0.018), (91, 0.025), (96, 0.157)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95371681 <a title="19-lda-1" href="./acl-2011-Dynamic_Programming_Algorithms_for_Transition-Based_Dependency_Parsers.html">107 acl-2011-Dynamic Programming Algorithms for Transition-Based Dependency Parsers</a></p>
<p>Author: Marco Kuhlmann ; Carlos Gomez-Rodriguez ; Giorgio Satta</p><p>Abstract: We develop a general dynamic programming technique for the tabulation of transition-based dependency parsers, and apply it to obtain novel, polynomial-time algorithms for parsing with the arc-standard and arc-eager models. We also show how to reverse our technique to obtain new transition-based dependency parsers from existing tabular methods. Additionally, we provide a detailed discussion of the conditions under which the feature models commonly used in transition-based parsing can be integrated into our algorithms.</p><p>same-paper 2 0.92844272 <a title="19-lda-2" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>Author: Gunter Neumann ; Sven Schmeier</p><p>Abstract: We present a mobile touchable application for online topic graph extraction and exploration of web content. The system has been implemented for operation on an iPad. The topic graph is constructed from N web snippets which are determined by a standard search engine. We consider the extraction of a topic graph as a specific empirical collocation extraction task where collocations are extracted between chunks. Our measure of association strength is based on the pointwise mutual information between chunk pairs which explicitly takes their distance into account. An initial user evaluation shows that this system is especially helpful for finding new interesting information on topics about which the user has only a vague idea or even no idea at all.</p><p>3 0.916215 <a title="19-lda-3" href="./acl-2011-Effective_Measures_of_Domain_Similarity_for_Parsing.html">109 acl-2011-Effective Measures of Domain Similarity for Parsing</a></p>
<p>Author: Barbara Plank ; Gertjan van Noord</p><p>Abstract: It is well known that parsing accuracy suffers when a model is applied to out-of-domain data. It is also known that the most beneficial data to parse a given domain is data that matches the domain (Sekine, 1997; Gildea, 2001). Hence, an important task is to select appropriate domains. However, most previous work on domain adaptation relied on the implicit assumption that domains are somehow given. As more and more data becomes available, automatic ways to select data that is beneficial for a new (unknown) target domain are becoming attractive. This paper evaluates various ways to automatically acquire related training data for a given test set. The results show that an unsupervised technique based on topic models is effective – it outperforms random data selection on both languages exam- ined, English and Dutch. Moreover, the technique works better than manually assigned labels gathered from meta-data that is available for English. 1 Introduction and Motivation Previous research on domain adaptation has focused on the task of adapting a system trained on one domain, say newspaper text, to a particular new domain, say biomedical data. Usually, some amount of (labeled or unlabeled) data from the new domain was given which has been determined by a human. However, with the growth of the web, more and more data is becoming available, where each document “is potentially its own domain” (McClosky et al., 2010). It is not straightforward to determine – 1566 Gertjan van Noord University of Groningen The Netherlands G J M van Noord@ rug nl . . . . . which data or model (in case we have several source domain models) will perform best on a new (unknown) target domain. Therefore, an important issue that arises is how to measure domain similarity, i.e. whether we can find a simple yet effective method to determine which model or data is most beneficial for an arbitrary piece of new text. Moreover, if we had such a measure, a related question is whether it can tell us something more about what is actually meant by “domain”. So far, it was mostly arbitrarily used to refer to some kind of coherent unit (related to topic, style or genre), e.g.: newspaper text, biomedical abstracts, questions, fiction. Most previous work on domain adaptation, for instance Hara et al. (2005), McClosky et al. (2006), Blitzer et al. (2006), Daum e´ III (2007), sidestepped this problem of automatic domain selection and adaptation. For parsing, to our knowledge only one recent study has started to examine this issue (McClosky et al., 2010) we will discuss their approach in Section 2. Rather, an implicit assumption of all of these studies is that domains are given, i.e. that they are represented by the respective corpora. Thus, a corpus has been considered a homogeneous unit. As more data is becoming available, it is unlikely that – domains will be ‘given’ . Moreover, a given corpus might not always be as homogeneous as originally thought (Webber, 2009; Lippincott et al., 2010). For instance, recent work has shown that the well-known Penn Treebank (PT) Wall Street Journal (WSJ) actually contains a variety of genres, including letters, wit and short verse (Webber, 2009). In this study we take a different approach. Rather than viewing a given corpus as a monolithic entity, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s566–1576, we break it down to the article-level and disregard corpora boundaries. Given the resulting set of documents (articles), we evaluate various ways to automatically acquire related training data for a given test set, to find answers to the following questions: • Given a pool of data (a collection of articles fGriovmen nun ak pnooowln o domains) caonldle a test article, eiss there a way to automatically select data that is relevant for the new domain? If so: • Which similarity measure is good for parsing? • How does it compare to human-annotated data? • Is the measure also useful for other languages Iasnd th/oer mtaesakssu?r To this end, we evaluate measures of domain similarity and feature representations and their impact on dependency parsing accuracy. Given a collection of annotated articles, and a new article that we want to parse, we want to select the most similar articles to train the best parser for that new article. In the following, we will first compare automatic measures to human-annotated labels by examining parsing performance within subdomains of the Penn Treebank WSJ. Then, we extend the experiments to the domain adaptation scenario. Experiments were performed on two languages: English and Dutch. The empirical results show that a simple measure based on topic distributions is effective for both languages and works well also for Part-of-Speech tagging. As the approach is based on plain surfacelevel information (words) and it finds related data in a completely unsupervised fashion, it can be easily applied to other tasks or languages for which annotated (or automatically annotated) data is available. 2 Related Work The work most related to ours is McClosky et al. (2010). They try to find the best combination of source models to parse data from a new domain, which is related to Plank and Sima’an (2008). In the latter, unlabeled data was used to create several parsers by weighting trees in the WSJ according to their similarity to the subdomain. McClosky et al. (2010) coined the term multiple source domain adaptation. Inspired by work on parsing accuracy 1567 prediction (Ravi et al., 2008), they train a linear regression model to predict the best (linear interpolation) of source domain models. Similar to us, McClosky et al. (2010) regard a target domain as mixture of source domains, but they focus on phrasestructure parsing. Furthermore, our approach differs from theirs in two respects: we do not treat source corpora as one entity and try to mix models, but rather consider articles as base units and try to find subsets of related articles (the most similar articles); moreover, instead of creating a supervised model (in their case to predict parsing accuracy), our approach is ‘simplistic’ : we apply measures of domain simi- larity directly (in an unsupervised fashion), without the necessity to train a supervised model. Two other related studies are (Lippincott et al., 2010; Van Asch and Daelemans, 2010). Van Asch and Daelemans (2010) explore a measure of domain difference (Renyi divergence) between pairs of domains and its correlation to Part-of-Speech tagging accuracy. Their empirical results show a linear correlation between the measure and the performance loss. Their goal is different, but related: rather than finding related data for a new domain, they want to estimate the loss in accuracy of a PoS tagger when applied to a new domain. We will briefly discuss results obtained with the Renyi divergence in Section 5.1. Lippincott et al. (2010) examine subdomain variation in biomedicine corpora and propose awareness of NLP tools to such variation. However, they did not yet evaluate the effect on a practical task, thus our study is somewhat complementary to theirs. The issue of data selection has recently been examined for Language Modeling (Moore and Lewis, 2010). A subset of the available data is automatically selected as training data for a Language Model based on a scoring mechanism that compares cross- entropy scores. Their approach considerably outperformed random selection and two previous proposed approaches both based on perplexity scoring.1 3 Measures of Domain Similarity 3.1 Measuring Similarity Automatically Feature Representations A similarity function may be defined over any set of events that are con1We tested data selection by perplexity scoring, but found the Language Models too small to be useful in our setting. sidered to be relevant for the task at hand. For parsing, these might be words, characters, n-grams (of words or characters), Part-of-Speech (PoS) tags, bilexical dependencies, syntactic rules, etc. However, to obtain more abstract types such as PoS tags or dependency relations, one would first need to gather respective labels. The necessary tools for this are again trained on particular corpora, and will suffer from domain shifts, rendering labels noisy. Therefore, we want to gauge the effect of the simplest representation possible: plain surface characteristics (unlabeled text). This has the advantage that we do not need to rely on additional supervised tools; moreover, it is interesting to know how far we can get with this level of information only. We examine the following feature representations: relative frequencies of words, relative frequencies of character tetragrams, and topic models. Our motivation was as follows. Relative frequencies of words are a simple and effective representation used e.g. in text classification (Manning and Sch u¨tze, 1999), while character n-grams have proven successful in genre classification (Wu et al., 2010). Topic models (Blei et al., 2003; Steyvers and Griffiths, 2007) can be considered an advanced model over word distributions: every article is represented by a topic distribution, which in turn is a distribution over words. Similarity between documents can be measured by comparing topic distributions. Similarity Functions There are many possible similarity (or distance) functions. They fall broadly into two categories: probabilistically-motivated and geometrically-motivated functions. The similarity functions examined in this study will be described in the following. The Kullback-Leibler (KL) divergence D(q| |r) is a cTlahsesic Kaull measure oibfl ‘edri s(KtaLn)ce d’i2v ebregtweneceen D Dtw(oq probability distributions, and is defined as: D(q| |r) = Pyq(y)logrq((yy)). It is a non-negative, additive, aPsymmetric measure, and 0 iff the two distributions are identical. However, the KL-divergence is undefined if there exists an event y such that q(y) > 0 but r(y) = 0, which is a property that “makes it unsuitable for distributions derived via maximumlikelihood estimates” (Lee, 2001). 2It is not a proper distance metric since it is asymmetric. 1568 One option to overcome this limitation is to apply smoothing techniques to gather non-zero estimates for all y. The alternative, examined in this paper, is to consider an approximation to the KL divergence, such as the Jensen-Shannon (JS) divergence (Lin, 1991) and the skew divergence (Lee, 2001). The Jensen-Shannon divergence, which is symmetric, computes the KL-divergence between q, r, and the average between the two. We use the JS divergence as defined in Lee (2001): JS(q, r) = [D(q| |avg(q, r)) + D(r| |avg(q, r))] . The asymm[eDtr(icq |s|akvewg( divergence sα, proposed by Lee (2001), mixes one distribution with the other by a degree de- 21 fined by α ∈ [0, 1) : sα (q, r, α) = D(q| |αr + (1 α)q). Ays α α approaches 1, rt,hαe )sk =ew D divergence approximates the KL-divergence. An alternative way to measure similarity is to consider the distributions as vectors and apply geometrically-motivated distance functions. This family of similarity functions includes the cosine cos(q, r) = qq(y) · r(y)/ | |q(y) | | | |r(y) | |, euclidean − euc(q,r) = qPy(q(y) − r(y))2 and variational (also known asq LP1 or MPanhattan) distance function, defined as var(q, r) = Py |q(y) − r(y) |. 3.2 Human-annotatePd data In contrast to the automatic measures devised in the previous section, we might have access to human annotated data. That is, use label information such as topic or genre to define the set of similar articles. Genre For the Penn Treebank (PT) Wall Street Journal (WSJ) section, more specifically, the subset available in the Penn Discourse Treebank, there exists a partition of the data by genre (Webber, 2009). Every article is assigned one of the following genre labels: news, letters, highlights, essays, errata, wit and short verse, quarterly progress reports, notable and quotable. This classification has been made on the basis of meta-data (Webber, 2009). It is wellknown that there is no meta-data directly associated with the individual WSJ files in the Penn Treebank. However, meta-data can be obtained by looking at the articles in the ACL/DCI corpus (LDC99T42), and a mapping file that aligns document numbers of DCI (DOCNO) to WSJ keys (Webber, 2009). An example document is given in Figure 1. The metadata field HL contains headlines, SO source info, and the IN field includes topic markers.</p><p>4 0.89187813 <a title="19-lda-4" href="./acl-2011-Entrainment_in_Speech_Preceding_Backchannels..html">118 acl-2011-Entrainment in Speech Preceding Backchannels.</a></p>
<p>Author: Rivka Levitan ; Agustin Gravano ; Julia Hirschberg</p><p>Abstract: In conversation, when speech is followed by a backchannel, evidence of continued engagement by one’s dialogue partner, that speech displays a combination of cues that appear to signal to one’s interlocutor that a backchannel is appropriate. We term these cues backchannel-preceding cues (BPC)s, and examine the Columbia Games Corpus for evidence of entrainment on such cues. Entrainment, the phenomenon of dialogue partners becoming more similar to each other, is widely believed to be crucial to conversation quality and success. Our results show that speaking partners entrain on BPCs; that is, they tend to use similar sets of BPCs; this similarity increases over the course of a dialogue; and this similarity is associated with measures of dialogue coordination and task success. 1</p><p>5 0.88722414 <a title="19-lda-5" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>6 0.85102737 <a title="19-lda-6" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>7 0.68316501 <a title="19-lda-7" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>8 0.67850506 <a title="19-lda-8" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>9 0.64891934 <a title="19-lda-9" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>10 0.61272854 <a title="19-lda-10" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>11 0.60575932 <a title="19-lda-11" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>12 0.60061294 <a title="19-lda-12" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>13 0.59560877 <a title="19-lda-13" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>14 0.58768857 <a title="19-lda-14" href="./acl-2011-Terminal-Aware_Synchronous_Binarization.html">296 acl-2011-Terminal-Aware Synchronous Binarization</a></p>
<p>15 0.58084798 <a title="19-lda-15" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>16 0.56930315 <a title="19-lda-16" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>17 0.56492692 <a title="19-lda-17" href="./acl-2011-Extracting_Paraphrases_from_Definition_Sentences_on_the_Web.html">132 acl-2011-Extracting Paraphrases from Definition Sentences on the Web</a></p>
<p>18 0.56151044 <a title="19-lda-18" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>19 0.55858946 <a title="19-lda-19" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>20 0.55853337 <a title="19-lda-20" href="./acl-2011-Automatic_Extraction_of_Lexico-Syntactic_Patterns_for_Detection_of_Negation_and_Speculation_Scopes.html">50 acl-2011-Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation and Speculation Scopes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
