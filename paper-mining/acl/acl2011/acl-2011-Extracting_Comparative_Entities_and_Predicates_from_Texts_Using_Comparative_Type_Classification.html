<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 acl-2011-Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-130" href="#">acl2011-130</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>130 acl-2011-Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification</h1>
<br/><p>Source: <a title="acl-2011-130-pdf" href="http://aclweb.org/anthology//P/P11/P11-1164.pdf">pdf</a></p><p>Author: Seon Yang ; Youngjoong Ko</p><p>Abstract: The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use. 1</p><p>Reference: <a title="acl-2011-130-reference" href="../acl2011_reference/acl-2011-Extracting_Comparative_Entities_and_Predicates_from_Texts_Using_Comparative_Type_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com Abstract The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. [sent-3, score-0.789]
</p><p>2 In this paper, we study how to build a Korean comparison mining system. [sent-4, score-0.178]
</p><p>3 Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. [sent-5, score-1.686]
</p><p>4 As a result, we achieve outstanding performance enough for practical use. [sent-7, score-0.108]
</p><p>5 To make better decisions, they probably attempt to compare entities that they are interesting in. [sent-9, score-0.092]
</p><p>6 Therefore, a comparison mining 1636 Youngjoong Ko Department of Computer Engineering, Dong-A University, Busan, Korea yjko@dau. [sent-15, score-0.153]
</p><p>7 kr system, which can automatically provide a summary of comparisons between two (or more)  entities from a large quantity of web documents, would be very useful in many areas such as marketing. [sent-17, score-0.092]
</p><p>8 We divide our work into two tasks to effectively build a comparison mining system. [sent-18, score-0.178]
</p><p>9 The first task is related to a sentence classification problem and the second is related to an information extraction problem. [sent-19, score-0.084]
</p><p>10 Classifying comparative sentences into one non-comparative class and seven comparative classes (or types); 1) Equality, 2) Similarity, 3) Difference, 4) Greater or lesser, 5) Superlative, 6) Pseudo, and 7) Implicit comparisons. [sent-21, score-1.48]
</p><p>11 Mining comparative entities and predicates taking into account the characteristics of each type. [sent-24, score-0.817]
</p><p>12 ” belonging to “4) Greater or lesser” type, we extract “stockX” as a subject entity (SE), “stock-Y” as an object entity (OE), and “worth” as a comparative predicate (PR). [sent-26, score-0.717]
</p><p>13 Classifying comparative sentences (Task 1): For the first task, we extract comparative sentences from text documents and then classify the extracted comparative sentences into seven ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. [sent-28, score-2.342]
</p><p>14 c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s636–1644, comparative types. [sent-30, score-0.649]
</p><p>15 However, any method that depends on just these linguistic-based keywords has obvious limitations as follows: 1)  Кling is insufficient to cover all of the actual comparison expressions. [sent-35, score-0.116]
</p><p>16 2) There are many non-comparative sentences that contain some elements of Кling. [sent-36, score-0.104]
</p><p>17 3) There is no one-to-one relationship between keyword types and sentence types. [sent-37, score-0.11]
</p><p>18 Mining comparative entities and predicates (Task 2): Our basic idea for the second task is selecting candidates first and finding answers from the candidates later. [sent-38, score-0.911]
</p><p>19 We regard each of noun words as a candidate for SE/OE, and each of adjective (or verb) words as a candidate for PR. [sent-39, score-0.068]
</p><p>20 However, this candidate detection has serious problems as follows: 4) There are many actual SEs, OEs, and PRs that consist of multiple words. [sent-40, score-0.058]
</p><p>21 5) There are many sentences with no OE, especially among superlative sentences. [sent-41, score-0.335]
</p><p>22 It means that the ellipsis is frequently occurred in superlative sentences. [sent-42, score-0.262]
</p><p>23 The final experimental results in 5-fold cross validation show the overall accuracy of 88. [sent-45, score-0.067]
</p><p>24 59% for the first task and the overall accuracy of 86. [sent-46, score-0.071]
</p><p>25 Section 3 and Section 4 describe our first task and second task in detail, respectively. [sent-50, score-0.06]
</p><p>26 2  Related Work  Linguistic researchers focus on defining the syntax and semantics of comparative constructs. [sent-52, score-0.649]
</p><p>27 Ha (1999a; 1999b) classified the structures of Korean comparative sentences into several classes and arranged comparison-bearing words from a linguistic perspective. [sent-53, score-0.745]
</p><p>28 Since he summarized the modern Korean comparative studies, his research helps us have a linguistic point of view. [sent-54, score-0.671]
</p><p>29 Jeong classified adjective superlatives using certain measures, and Oh discussed the gradability of comparatives. [sent-56, score-0.055]
</p><p>30 Jindal and  Liu (2006a; 2006b) studied to mine comparative relations from English text documents. [sent-58, score-0.706]
</p><p>31 They used comparative and superlative POS tags, and some additional keywords. [sent-59, score-0.911]
</p><p>32 Yang and Ko (2009; 2011) studied to extract comparative sentences in Korean text documents. [sent-61, score-0.753]
</p><p>33 (2010) studied to mine comparable entities from English comparative questions that users posted online. [sent-63, score-0.798]
</p><p>34 They focused on finding a set of comparable entities given a user‟s input entity. [sent-64, score-0.092]
</p><p>35 Opinion mining is also related to our work because many comparative sentences also contain the speaker‟s opinion/sentiment. [sent-65, score-0.837]
</p><p>36 (2008) surveyed various techniques that have been developed for the key tasks of opinion mining. [sent-67, score-0.061]
</p><p>37 Riloff and Wiebe (2003) presented a bootstrapping process that learns linguistically rich extraction patterns for subjective expressions. [sent-69, score-0.049]
</p><p>38 Ramshaw and Marcus (1995) applied TBL for locating chunks in tagged texts. [sent-76, score-0.115]
</p><p>39 3  Classifying (Task 1)  Comparative  Sentences  We first classify the sentences into comparatives and non-comparatives by extracting only comparatives from text documents. [sent-78, score-0.274]
</p><p>40 1  Extracting comparative text documents  sentences  from  Our strategy is to first detect Comparative Sentence candidates (CS-candidates), and then eliminate non-comparative sentences from the candidates. [sent-81, score-0.861]
</p><p>41 As mentioned in the introduction section, we easily construct a linguistic-based keyword set, Кling. [sent-82, score-0.055]
</p><p>42 However, we observe that Кling  is not enough to capture all the actual comparison expressions. [sent-83, score-0.065]
</p><p>43 Hence, we build a comparison lexicon as follows: ▪  Comparison Lexicon = Кling U {Additional keywords that are frequently used for actual comparative expressions}  This lexicon is composed of three parts. [sent-84, score-0.88]
</p><p>44 , “   …  …,  …,  Table 1: Feature examples for mining comparative elements Table 1lists some examples. [sent-90, score-0.795]
</p><p>45 1 Experimental Settings The experiments are conducted on 7,384 sentences collected from the web by three trained human labelers. [sent-95, score-0.095]
</p><p>46 85 showed that it was safe to say that the two labelers agreed in their judgments. [sent-98, score-0.092]
</p><p>47 All three labelers discussed any conflict, and finally reached an agreement. [sent-100, score-0.052]
</p><p>48 2  Classifying comparative sentences  Our experimental results for Task 1 showed an f1score of 90. [sent-112, score-0.762]
</p><p>49 23% in extracting comparative  sentences from text documents and an accuracy of 81. [sent-113, score-0.833]
</p><p>50 67% in classifying the comparative sentences into seven comparative types. [sent-114, score-1.562]
</p><p>51 Non-comparative sentences were regarded as an eighth comparative type in this integrated result. [sent-117, score-0.813]
</p><p>52 It means that we classify entire sentences into eight types (seven comparative types and one non-comparative type). [sent-118, score-0.832]
</p><p>53 Before evaluating our proposed method for comparative sentence extraction, we conducted four experiments with all of the lexical unigrams and bigrams using MEM and SVM. [sent-122, score-0.722]
</p><p>54 Among these four cases, SVM with lexical unigrams showed the highest performance, an f1-score of 79. [sent-123, score-0.062]
</p><p>55 Next, we did experiments using all of the continuous lexical sequences and using all of the POS tags sequences within a radius of n words from each CK as features (n=1,2,3,4,5). [sent-126, score-0.162]
</p><p>56 Among these ten cases, “the POS tags sequences within a radius of 3” showed the best performance. [sent-127, score-0.142]
</p><p>57 Besides, as SVM showed the better performance than MEM 1641 in overall experiments, we employ SVM as our  proposed learning technique. [sent-128, score-0.04]
</p><p>58 23  (proposed) Table 3: Final results in comparative sentence extraction (%) As given above, we successfully detected CScandidates with considerably high recall by using the comparison lexicon. [sent-139, score-0.806]
</p><p>59 We also successfully filtered the candidates with high precision while still preserving high recall by applying machine learning technique. [sent-140, score-0.074]
</p><p>60 Finally, we could achieve an outstanding performance, an f1-score of 90. [sent-141, score-0.108]
</p><p>61 Like the previous comparative sentence extraction task, we also conducted experiments for type classification using the same features (continuous  POS tags sequences within a radius of 3 words from each CK) and the same learning technique (SVM). [sent-146, score-0.872]
</p><p>62 We observed that the performance of type classification can be influenced by very subtle differences in many cases. [sent-152, score-0.045]
</p><p>63 Hence, we think that an error-driven approach can perform well in comparative type classification. [sent-153, score-0.694]
</p><p>64 In the first step, we roughly annotated the type of a sentence using the type of the CK itself. [sent-155, score-0.119]
</p><p>65 Then, we generated error-driven transformation rules from the incorrectly annotated sentences. [sent-156, score-0.056]
</p><p>66 Numerous transformation rules were generated on the basis of the templates. [sent-158, score-0.056]
</p><p>67 For example, “Change the type of the current sentence from “Greater or lesser” to “Superlative” if this sentence holds the CK of “보다 ([bo-da]: than)”, and the second preceding word of the CK is tagged as mm” is a transformation rule generated by the third template. [sent-159, score-0.331]
</p><p>68 Change the type of the current sentence from x to y if this sentence holds the CK of k, and  …  …  …  1. [sent-160, score-0.103]
</p><p>69 the preceding word of k is tagged z, and the following word of k is tagged w. [sent-169, score-0.287]
</p><p>70 the preceding word of k is tagged z, and the second preceding word of k is tagged w. [sent-171, score-0.344]
</p><p>71 the following word of k is tagged z, and the second following word of k is tagged w. [sent-173, score-0.23]
</p><p>72 Table 4: Transformation templates For evaluation of threshold values, we performed experiments with three options as given in Table 5. [sent-174, score-0.065]
</p><p>73 04 Table 5: Evaluation of threshold option (%); Threshold n means that the learning iterations continues while Ci-Ei ≥ n+1  We achieved the best performance with the threshold option 1. [sent-179, score-0.126]
</p><p>74 Finally, we classified comparative sentences into seven types using TBL with an accuracy of 81. [sent-180, score-0.921]
</p><p>75 3 Integrated results of Task 1 We sum up our proposed method for Task 1 as two steps as follows; 1) The comparison lexicon detects CS-candidates in text documents, and then SVM eliminates the non-comparative sentences from the candidates. [sent-184, score-0.156]
</p><p>76 Thus, all of the sentences are divided into two classes: a comparative class and a non-comparative class. [sent-185, score-0.722]
</p><p>77 2) TBL then classifies the sentences placed in the comparative class in the previous step into seven comparative types. [sent-186, score-1.48]
</p><p>78 1642 The integrated results showed an overall accuracy of 88. [sent-187, score-0.127]
</p><p>79 59 Table 6: Integrated results for Task 1 (%)  As shown above, Task 1 was successfully divided into two steps. [sent-194, score-0.042]
</p><p>80 3  Mining comparative entities and predicates For the mining task of comparative entities and predicates, we used 460 comparative sentences (Greater or lesser: 300, Superlative: 160). [sent-196, score-2.425]
</p><p>81 Table 7 lists the portion of multiple-word comparative elements. [sent-198, score-0.686]
</p><p>82 Multi-word  Multi-word rate Greater or lesser  rate  SE 30. [sent-199, score-0.23]
</p><p>83 1  Table 7: Portion (%) of multiple-word comparative elements As given above, each multiple-word portion, especially in SEs and OEs, is quite high. [sent-206, score-0.68]
</p><p>84 This fact proves that it is absolutely necessary to allow multiple-word comparative elements. [sent-207, score-0.649]
</p><p>85 If sentences that do not have any OEs are excluded, the portion of multiple-words becomes 32. [sent-210, score-0.11]
</p><p>86 We calculated the error rates of CEcandidate detection before and after simplification processes. [sent-213, score-0.12]
</p><p>87 Simplification  Simplification processes Greater or Before lesser  SE  34. [sent-214, score-0.16]
</p><p>88 43 Table 8: Error rate (%) in CE-candidate detection Here, the first value of 34. [sent-224, score-0.066]
</p><p>89 7% means that the real SEs of 104 sentences (among total 300 Greater or lesser sentences) were not detected by CEcandidate detection before simplification processes. [sent-225, score-0.376]
</p><p>90 The significant differences between before and after indicate that we successfully detect CEcandidates through the simplification processes. [sent-228, score-0.131]
</p><p>91 If sentences that do not have any OEs are excluded, the error rate is only 6. [sent-231, score-0.108]
</p><p>92 Both MEM and SVM showed outstanding performance; there was no significant difference between the two machine learning methods (SVM and MEM). [sent-235, score-0.148]
</p><p>93 To identify such sentences, if SVM tagged every “N” in a sentence as “not OE”, we tagged the sentence as “no OE”. [sent-238, score-0.288]
</p><p>94 Final  Results  Final Results Greater or lesser Superlative Total  SE 86. [sent-239, score-0.16]
</p><p>95 74  Table 9: Final results of Task 2 (Accuracy, %) As shown above, we successfully extracted the comparative entities and predicates with outstanding performance, an overall accuracy of 86. [sent-248, score-1.008]
</p><p>96 6  Conclusions and Future Work  This paper has studied a Korean comparison mining system. [sent-250, score-0.184]
</p><p>97 Our proposed system achieved an 1643 accuracy of 88. [sent-251, score-0.041]
</p><p>98 59% for classifying comparative sentences into eight types (one non-comparative type and seven comparative types), and an accuracy of 86. [sent-252, score-1.699]
</p><p>99 Since the comparison mining is an area of increasing interest around the world, our study can contribute greatly to text mining research. [sent-255, score-0.268]
</p><p>100 Our first plan is to complete the mining process on all the types of sentences. [sent-257, score-0.141]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('comparative', 0.649), ('tbl', 0.271), ('superlative', 0.262), ('korean', 0.218), ('mem', 0.162), ('lesser', 0.16), ('oe', 0.157), ('tagged', 0.115), ('mining', 0.115), ('seven', 0.109), ('oes', 0.108), ('outstanding', 0.108), ('svm', 0.103), ('ck', 0.096), ('entities', 0.092), ('simplification', 0.089), ('ling', 0.084), ('classifying', 0.082), ('seon', 0.081), ('youngjoong', 0.081), ('predicates', 0.076), ('sentences', 0.073), ('equality', 0.07), ('greater', 0.069), ('comparatives', 0.066), ('jindal', 0.066), ('radius', 0.066), ('pr', 0.063), ('jeong', 0.062), ('korea', 0.062), ('preceding', 0.057), ('transformation', 0.056), ('ses', 0.056), ('keyword', 0.055), ('busan', 0.054), ('cecandidate', 0.054), ('gat', 0.054), ('labelers', 0.052), ('keywords', 0.051), ('se', 0.049), ('integrated', 0.046), ('lexicon', 0.045), ('type', 0.045), ('successfully', 0.042), ('ko', 0.041), ('accuracy', 0.041), ('threshold', 0.04), ('showed', 0.04), ('opinion', 0.039), ('comparison', 0.038), ('portion', 0.037), ('sequences', 0.036), ('pseudo', 0.036), ('ha', 0.036), ('regard', 0.036), ('extracting', 0.036), ('rate', 0.035), ('documents', 0.034), ('entity', 0.034), ('classify', 0.033), ('candidates', 0.032), ('adjective', 0.032), ('yang', 0.032), ('elements', 0.031), ('berger', 0.031), ('nitin', 0.031), ('studied', 0.031), ('detection', 0.031), ('task', 0.03), ('ramshaw', 0.03), ('sentence', 0.029), ('oh', 0.028), ('actual', 0.027), ('final', 0.026), ('pos', 0.026), ('mine', 0.026), ('types', 0.026), ('riloff', 0.025), ('extraction', 0.025), ('build', 0.025), ('eight', 0.025), ('engines', 0.025), ('templates', 0.025), ('omitted', 0.025), ('continuous', 0.024), ('subjective', 0.024), ('bing', 0.024), ('seoul', 0.024), ('black', 0.024), ('classified', 0.023), ('option', 0.023), ('detected', 0.023), ('conducted', 0.022), ('excluded', 0.022), ('modern', 0.022), ('unigrams', 0.022), ('vicent', 0.022), ('anlp', 0.022), ('surveyed', 0.022), ('anchored', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="130-tfidf-1" href="./acl-2011-Extracting_Comparative_Entities_and_Predicates_from_Texts_Using_Comparative_Type_Classification.html">130 acl-2011-Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification</a></p>
<p>Author: Seon Yang ; Youngjoong Ko</p><p>Abstract: The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use. 1</p><p>2 0.36816117 <a title="130-tfidf-2" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>Author: Xiaojiang Huang ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Comparative News Summarization aims to highlight the commonalities and differences between two comparable news topics. In this study, we propose a novel approach to generating comparative news summaries. We formulate the task as an optimization problem of selecting proper sentences to maximize the comparativeness within the summary and the representativeness to both news topics. We consider semantic-related cross-topic concept pairs as comparative evidences, and consider topic-related concepts as representative evidences. The optimization problem is addressed by using a linear programming model. The experimental results demonstrate the effectiveness of our proposed model.</p><p>3 0.092841364 <a title="130-tfidf-3" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>Author: Rafael E. Banchs ; Haizhou Li</p><p>Abstract: This work introduces AM-FM, a semantic framework for machine translation evaluation. Based upon this framework, a new evaluation metric, which is able to operate without the need for reference translations, is implemented and evaluated. The metric is based on the concepts of adequacy and fluency, which are independently assessed by using a cross-language latent semantic indexing approach and an n-gram based language model approach, respectively. Comparative analyses with conventional evaluation metrics are conducted on two different evaluation tasks (overall quality assessment and comparative ranking) over a large collection of human evaluations involving five European languages. Finally, the main pros and cons of the proposed framework are discussed along with future research directions. 1</p><p>4 0.089141294 <a title="130-tfidf-4" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>Author: Or Biran ; Samuel Brody ; Noemie Elhadad</p><p>Abstract: We present a method for lexical simplification. Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.</p><p>5 0.079277992 <a title="130-tfidf-5" href="./acl-2011-Simple_English_Wikipedia%3A_A_New_Text_Simplification_Task.html">283 acl-2011-Simple English Wikipedia: A New Text Simplification Task</a></p>
<p>Author: William Coster ; David Kauchak</p><p>Abstract: In this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based trans- lation approach for simplification.</p><p>6 0.069008671 <a title="130-tfidf-6" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>7 0.056235082 <a title="130-tfidf-7" href="./acl-2011-Liars_and_Saviors_in_a_Sentiment_Annotated_Corpus_of_Comments_to_Political_Debates.html">211 acl-2011-Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates</a></p>
<p>8 0.049128164 <a title="130-tfidf-8" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>9 0.047890041 <a title="130-tfidf-9" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>10 0.045703087 <a title="130-tfidf-10" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>11 0.043556634 <a title="130-tfidf-11" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>12 0.043207604 <a title="130-tfidf-12" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>13 0.041560136 <a title="130-tfidf-13" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>14 0.041529302 <a title="130-tfidf-14" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>15 0.040599681 <a title="130-tfidf-15" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>16 0.040335864 <a title="130-tfidf-16" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>17 0.040093482 <a title="130-tfidf-17" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>18 0.040046945 <a title="130-tfidf-18" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>19 0.039394509 <a title="130-tfidf-19" href="./acl-2011-Global_Learning_of_Typed_Entailment_Rules.html">144 acl-2011-Global Learning of Typed Entailment Rules</a></p>
<p>20 0.039379783 <a title="130-tfidf-20" href="./acl-2011-Contrasting_Opposing_Views_of_News_Articles_on_Contentious_Issues.html">84 acl-2011-Contrasting Opposing Views of News Articles on Contentious Issues</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, 0.056), (2, -0.034), (3, 0.048), (4, -0.005), (5, 0.008), (6, -0.002), (7, 0.026), (8, -0.017), (9, -0.048), (10, -0.064), (11, -0.014), (12, -0.082), (13, -0.023), (14, -0.031), (15, 0.001), (16, 0.071), (17, -0.011), (18, 0.058), (19, -0.018), (20, 0.038), (21, -0.026), (22, 0.0), (23, -0.019), (24, 0.028), (25, -0.042), (26, 0.08), (27, -0.077), (28, 0.02), (29, 0.042), (30, 0.067), (31, -0.038), (32, -0.033), (33, 0.021), (34, -0.07), (35, -0.051), (36, 0.042), (37, -0.014), (38, -0.019), (39, -0.031), (40, 0.064), (41, -0.049), (42, -0.077), (43, -0.123), (44, -0.076), (45, -0.189), (46, -0.038), (47, 0.158), (48, 0.048), (49, -0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95028406 <a title="130-lsi-1" href="./acl-2011-Extracting_Comparative_Entities_and_Predicates_from_Texts_Using_Comparative_Type_Classification.html">130 acl-2011-Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification</a></p>
<p>Author: Seon Yang ; Youngjoong Ko</p><p>Abstract: The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use. 1</p><p>2 0.73916048 <a title="130-lsi-2" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>Author: Xiaojiang Huang ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Comparative News Summarization aims to highlight the commonalities and differences between two comparable news topics. In this study, we propose a novel approach to generating comparative news summaries. We formulate the task as an optimization problem of selecting proper sentences to maximize the comparativeness within the summary and the representativeness to both news topics. We consider semantic-related cross-topic concept pairs as comparative evidences, and consider topic-related concepts as representative evidences. The optimization problem is addressed by using a linear programming model. The experimental results demonstrate the effectiveness of our proposed model.</p><p>3 0.5559029 <a title="130-lsi-3" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>Author: Or Biran ; Samuel Brody ; Noemie Elhadad</p><p>Abstract: We present a method for lexical simplification. Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.</p><p>4 0.50742555 <a title="130-lsi-4" href="./acl-2011-Hierarchical_Text_Classification_with_Latent_Concepts.html">150 acl-2011-Hierarchical Text Classification with Latent Concepts</a></p>
<p>Author: Xipeng Qiu ; Xuanjing Huang ; Zhao Liu ; Jinlong Zhou</p><p>Abstract: Recently, hierarchical text classification has become an active research topic. The essential idea is that the descendant classes can share the information of the ancestor classes in a predefined taxonomy. In this paper, we claim that each class has several latent concepts and its subclasses share information with these different concepts respectively. Then, we propose a variant Passive-Aggressive (PA) algorithm for hierarchical text classification with latent concepts. Experimental results show that the performance of our algorithm is competitive with the recently proposed hierarchical classification algorithms.</p><p>5 0.50493151 <a title="130-lsi-5" href="./acl-2011-Simple_English_Wikipedia%3A_A_New_Text_Simplification_Task.html">283 acl-2011-Simple English Wikipedia: A New Text Simplification Task</a></p>
<p>Author: William Coster ; David Kauchak</p><p>Abstract: In this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based trans- lation approach for simplification.</p><p>6 0.49407253 <a title="130-lsi-6" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>7 0.4924356 <a title="130-lsi-7" href="./acl-2011-That%27s_What_She_Said%3A_Double_Entendre_Identification.html">297 acl-2011-That's What She Said: Double Entendre Identification</a></p>
<p>8 0.46485448 <a title="130-lsi-8" href="./acl-2011-Even_the_Abstract_have_Color%3A_Consensus_in_Word-Colour_Associations.html">120 acl-2011-Even the Abstract have Color: Consensus in Word-Colour Associations</a></p>
<p>9 0.4334926 <a title="130-lsi-9" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>10 0.41916776 <a title="130-lsi-10" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>11 0.41626137 <a title="130-lsi-11" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>12 0.41355178 <a title="130-lsi-12" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>13 0.40696356 <a title="130-lsi-13" href="./acl-2011-Subjective_Natural_Language_Problems%3A_Motivations%2C_Applications%2C_Characterizations%2C_and_Implications.html">288 acl-2011-Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications</a></p>
<p>14 0.405269 <a title="130-lsi-14" href="./acl-2011-Discrete_vs._Continuous_Rating_Scales_for_Language_Evaluation_in_NLP.html">99 acl-2011-Discrete vs. Continuous Rating Scales for Language Evaluation in NLP</a></p>
<p>15 0.40321457 <a title="130-lsi-15" href="./acl-2011-Liars_and_Saviors_in_a_Sentiment_Annotated_Corpus_of_Comments_to_Political_Debates.html">211 acl-2011-Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates</a></p>
<p>16 0.39877912 <a title="130-lsi-16" href="./acl-2011-Finding_Deceptive_Opinion_Spam_by_Any_Stretch_of_the_Imagination.html">136 acl-2011-Finding Deceptive Opinion Spam by Any Stretch of the Imagination</a></p>
<p>17 0.39642727 <a title="130-lsi-17" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>18 0.39063495 <a title="130-lsi-18" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>19 0.38695577 <a title="130-lsi-19" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>20 0.38671821 <a title="130-lsi-20" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.02), (17, 0.035), (26, 0.028), (37, 0.063), (39, 0.036), (41, 0.063), (53, 0.016), (55, 0.018), (59, 0.033), (72, 0.453), (91, 0.027), (96, 0.132)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90570068 <a title="130-lda-1" href="./acl-2011-They_Can_Help%3A_Using_Crowdsourcing_to_Improve_the_Evaluation_of_Grammatical_Error_Detection_Systems.html">302 acl-2011-They Can Help: Using Crowdsourcing to Improve the Evaluation of Grammatical Error Detection Systems</a></p>
<p>Author: Nitin Madnani ; Martin Chodorow ; Joel Tetreault ; Alla Rozovskaya</p><p>Abstract: Despite the rising interest in developing grammatical error detection systems for non-native speakers of English, progress in the field has been hampered by a lack of informative metrics and an inability to directly compare the performance of systems developed by different researchers. In this paper we address these problems by presenting two evaluation methodologies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions One of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or for- eign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than simple rule violations such as number agreement. As a consequence, it is common for two native speakers 508 to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourc1There has been a recent proposal for a related shared task (Dale and Kilgarriff, 2010) that shows promise. Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o.c?i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 508–513, ing to both address the lack ofappropriate evaluation metrics and to make system comparison easier. Our solution is general enough for, in the simplest case, intrinsically evaluating a single system on a single dataset and, more realistically, comparing two different systems (from same or different groups). 2 A Case Study: Extraneous Prepositions We consider the problem of detecting an extraneous preposition error, i.e., incorrectly using a preposition where none is licensed. In the sentence “They came to outside”, the preposition to is an extraneous error whereas in the sentence “They arrived to the town” the preposition to is a confusion error (cf. arrived in the town). Most work on automated correction of preposition errors, with the exception of Gamon (2010), addresses preposition confusion errors e.g., (Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010b). One reason is that in addition to the standard context-based features used to detect confusion errors, identifying extraneous prepositions also requires actual knowledge of when a preposition can and cannot be used. Despite this lack of attention, extraneous prepositions account for a significant proportion—as much as 18% in essays by advanced English learners (Rozovskaya and Roth, 2010a)—of all preposition usage errors. 2.1 Data and Systems For the experiments in this paper, we chose a proprietary corpus of about 500,000 essays written by ESL students for Test of English as a Foreign Language (TOEFL?R). Despite being common ESL errors, preposition errors are still infrequent overall, with over 90% of prepositions being used correctly (Leacock et al., 2010; Rozovskaya and Roth, 2010a). Given this fact about error sparsity, we needed an efficient method to extract a good number of error instances (for statistical reliability) from the large essay corpus. We found all trigrams in our essays containing prepositions as the middle word (e.g., marry with her) and then looked up the counts of each tri- gram and the corresponding bigram with the preposition removed (marry her) in the Google Web1T 5-gram Corpus. If the trigram was unattested or had a count much lower than expected based on the bi509 gram count, then we manually inspected the trigram to see whether it was actually an error. If it was, we extracted a sentence from the large essay corpus containing this erroneous trigram. Once we had extracted 500 sentences containing extraneous preposition error instances, we added 500 sentences containing correct instances of preposition usage. This yielded a corpus of 1000 sentences with a 50% error rate. These sentences, with the target preposition highlighted, were presented to 3 expert annotators who are native English speakers. They were asked to annotate the preposition usage instance as one of the following: extraneous (Error), not extraneous (OK) or too hard to decide (Unknown); the last category was needed for cases where the context was too messy to make a decision about the highlighted preposition. On average, the three experts had an agreement of 0.87 and a kappa of 0.75. For subse- quent analysis, we only use the classes Error and OK since Unknown was used extremely rarely and never by all 3 experts for the same sentence. We used two different error detection systems to illustrate our evaluation methodology:2 • • 3 LM: A 4-gram language model trained on tLhMe Google Wme lba1nTg 5-gram Corpus dw oithn SRILM (Stolcke, 2002). PERC: An averaged Perceptron (Freund and Schapire, 1999) calgaessdif Pieerr—ce as implemented nind the Learning by Java toolkit (Rizzolo and Roth, 2007)—trained on 7 million examples and using the same features employed by Tetreault and Chodorow (2008). Crowdsourcing Recently,we showed that Amazon Mechanical Turk (AMT) is a cheap and effective alternative to expert raters for annotating preposition errors (Tetreault et al., 2010b). In other current work, we have extended this pilot study to show that CrowdFlower, a crowdsourcing service that allows for stronger quality con- × trol on untrained human raters (henceforth, Turkers), is more reliable than AMT on three different error detection tasks (article errors, confused prepositions 2Any conclusions drawn in this paper pertain only to these specific instantiations of the two systems. & extraneous prepositions). To impose such quality control, one has to provide “gold” instances, i.e., examples with known correct judgments that are then used to root out any Turkers with low performance on these instances. For all three tasks, we obtained 20 Turkers’ judgments via CrowdFlower for each instance and found that, on average, only 3 Turkers were required to match the experts. More specifically, for the extraneous preposition error task, we used 75 sentences as gold and obtained judgments for the remaining 923 non-gold sentences.3 We found that if we used 3 Turker judgments in a majority vote, the agreement with any one of the three expert raters is, on average, 0.87 with a kappa of 0.76. This is on par with the inter-expert agreement and kappa found earlier (0.87 and 0.75 respectively). The extraneous preposition annotation cost only $325 (923 judgments 20 Turkers) and was com- pleted 9in2 a single day. T 2h0e only rres)st arnicdtio wna on tmheTurkers was that they be physically located in the USA. For the analysis in subsequent sections, we use these 923 sentences and the respective 20 judgments obtained via CrowdFlower. The 3 expert judgments are not used any further in this analysis. 4 Revamping System Evaluation In this section, we provide details on how crowdsourcing can help revamp the evaluation of error detection systems: (a) by providing more informative measures for the intrinsic evaluation of a single system (§ 4. 1), and (b) by easily enabling system comparison (§ 4.2). 4.1 Crowd-informed Evaluation Measures When evaluating the performance of grammatical error detection systems against human judgments, the judgments for each instance are generally reduced to the single most frequent category: Error or OK. This reduction is not an accurate reflection of a complex phenomenon. It discards valuable information about the acceptability of usage because it treats all “bad” uses as equal (and all good ones as equal), when they are not. Arguably, it would be fairer to use a continuous scale, such as the proportion of raters who judge an instance as correct or 3We found 2 duplicate sentences and removed them. 510 incorrect. For example, if 90% of raters agree on a rating of Error for an instance of preposition usage, then that is stronger evidence that the usage is an error than if 56% of Turkers classified it as Error and 44% classified it as OK (the sentence “In addition classmates play with some game and enjoy” is an example). The regular measures of precision and recall would be fairer if they reflected this reality. Besides fairness, another reason to use a continuous scale is that of stability, particularly with a small number of instances in the evaluation set (quite common in the field). By relying on majority judgments, precision and recall measures tend to be unstable (see below). We modify the measures of precision and recall to incorporate distributions of correctness, obtained via crowdsourcing, in order to make them fairer and more stable indicators of system performance. Given an error detection system that classifies a sentence containing a specific preposition as Error (class 1) if the preposition is extraneous and OK (class 0) otherwise, we propose the following weighted versions of hits (Hw), misses (Mw) and false positives (FPw): XN Hw = X(csiys ∗ picrowd) (1) Xi XN Mw = X((1 − csiys) ∗ picrowd) (2) Xi XN FPw = X(csiys ∗ (1 − picrowd)) (3) Xi In the above equations, N is the total number of instances, csiys is the class (1 or 0) , and picrowd indicates the proportion of the crowd that classified instance i as Error. Note that if we were to revert to the majority crowd judgment as the sole judgment for each instance, instead of proportions, picrowd would always be either 1 or 0 and the above formulae would simply compute the normal hits, misses and false positives. Given these definitions, weighted precision can be defined as Precisionw = Hw/(Hw Hw/(Hw + FPw) and weighted + Mw). recall as Recallw = agreement Figure 1: Histogram of Turker agreements for all 923 instances on whether a preposition is extraneous. UWnwei gihg tede Pr0 e.c9 i5s0i70onR0 .e3 c78al14l Table 1: Comparing commonly used (unweighted) and proposed (weighted) precision/recall measures for LM. To illustrate the utility of these weighted measures, we evaluated the LM and PERC systems on the dataset containing 923 preposition instances, against all 20 Turker judgments. Figure 1 shows a histogram of the Turker agreement for the majority rating over the set. Table 1 shows both the unweighted (discrete majority judgment) and weighted (continuous Turker proportion) versions of precision and recall for this system. The numbers clearly show that in the unweighted case, the performance of the system is overestimated simply because the system is getting as much credit for each contentious case (low agreement) as for each clear one (high agreement). In the weighted measure we propose, the contentious cases are weighted lower and therefore their contribution to the overall performance is reduced. This is a fairer representation since the system should not be expected to perform as well on the less reliable instances as it does on the clear-cut instances. Essentially, if humans cannot consistently decide whether 511 [n=93] [n=1 14] Agreement Bin [n=71 6] Figure 2: Unweighted precision/recall by agreement bins for LM & PERC. a case is an error then a system’s output cannot be considered entirely right or entirely wrong.4 As an added advantage, the weighted measures are more stable. Consider a contentious instance in a small dataset where 7 out of 15 Turkers (a minority) classified it as Error. However, it might easily have happened that 8 Turkers (a majority) classified it as Error instead of 7. In that case, the change in unweighted precision would have been much larger than is warranted by such a small change in the data. However, weighted precision is guaranteed to be more stable. Note that the instability decreases as the size of the dataset increases but still remains a problem. 4.2 Enabling System Comparison In this section, we show how to easily compare different systems both on the same data (in the ideal case of a shared dataset being available) and, more realistically, on different datasets. Figure 2 shows (unweighted) precision and recall of LM and PERC (computed against the majority Turker judgment) for three agreement bins, where each bin is defined as containing only the instances with Turker agreement in a specific range. We chose the bins shown 4The difference between unweighted and weighted measures can vary depending on the distribution of agreement. since they are sufficiently large and represent a reasonable stratification of the agreement space. Note that we are not weighting the precision and recall in this case since we have already used the agreement proportions to create the bins. This curve enables us to compare the two systems easily on different levels of item contentiousness and, therefore, conveys much more information than what is usually reported (a single number for unweighted precision/recall over the whole corpus). For example, from this graph, PERC is seen to have similar performance as LM for the 75-90% agreement bin. In addition, even though LM precision is perfect (1.0) for the most contentious instances (the 50-75% bin), this turns out to be an artifact of the LM classifier’s decision process. When it must decide between what it views as two equally likely possibilities, it defaults to OK. Therefore, even though LM has higher unweighted precision (0.957) than PERC (0.813), it is only really better on the most clear-cut cases (the 90-100% bin). If one were to report unweighted precision and recall without using any bins—as is the norm—this important qualification would have been harder to discover. While this example uses the same dataset for evaluating two systems, the procedure is general enough to allow two systems to be compared on two different datasets by simply examining the two plots. However, two potential issues arise in that case. The first is that the bin sizes will likely vary across the two plots. However, this should not be a significant problem as long as the bins are sufficiently large. A second, more serious, issue is that the error rates (the proportion of instances that are actually erroneous) in each bin may be different across the two plots. To handle this, we recommend that a kappa-agreement plot be used instead of the precision-agreement plot shown here. 5 Conclusions Our goal is to propose best practices to address the two primary problems in evaluating grammatical error detection systems and we do so by leveraging crowdsourcing. For system development, we rec- ommend that rather than compressing multiple judgments down to the majority, it is better to use agreement proportions to weight precision and recall to 512 yield fairer and more stable indicators of performance. For system comparison, we argue that the best solution is to use a shared dataset and present the precision-agreement plot using a set of agreed-upon bins (possibly in conjunction with the weighted precision and recall measures) for a more informative comparison. However, we recognize that shared datasets are harder to create in this field (as most of the data is proprietary). Therefore, we also provide a way to compare multiple systems across different datasets by using kappa-agreement plots. As for agreement bins, we posit that the agreement values used to define them depend on the task and, therefore, should be determined by the community. Note that both of these practices can also be implemented by using 20 experts instead of 20 Turkers. However, we show that crowdsourcing yields judgments that are as good but without the cost. To facilitate the adoption of these practices, we make all our evaluation code and data available to the com- munity.5 Acknowledgments We would first like to thank our expert annotators Sarah Ohls and Waverely VanWinkle for their hours of hard work. We would also like to acknowledge Lei Chen, Keelan Evanini, Jennifer Foster, Derrick Higgins and the three anonymous reviewers for their helpful comments and feedback. References Cem Akkaya, Alexander Conrad, Janyce Wiebe, and Rada Mihalcea. 2010. Amazon Mechanical Turk for Subjectivity Word Sense Disambiguation. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon ’s Mechanical Turk, pages 195–203. Chris Callison-Burch. 2009. Fast, Cheap, and Creative: Evaluating Translation Quality Using Amazon’s Mechanical Turk. In Proceedings of EMNLP, pages 286– 295. Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz. 2009. A Demonstration of Human Computation Using the Phrase Detectives Annotation Game. In ACM SIGKDD Workshop on Human Computation, pages 23–24. 5http : / /bit . ly/ crowdgrammar Robert Dale and Adam Kilgarriff. 2010. Helping Our Own: Text Massaging for Computational Linguistics as a New Shared Task. In Proceedings of INLG. Keelan Evanini, Derrick Higgins, and Klaus Zechner. 2010. Using Amazon Mechanical Turk for Transcription of Non-Native Speech. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon ’s Mechanical Turk, pages 53–56. Rachele De Felice and Stephen Pulman. 2008. A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English. In Proceedings of COLING, pages 169–176. Tim Finin, William Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating Named Entities in Twitter Data with Crowdsourcing. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon ’s Mechanical Turk, pages 80–88. Yoav Freund and Robert E. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277–296. Michael Gamon, Jianfeng Gao, Chris Brockett, Alexander Klementiev, William Dolan, Dmitriy Belenko, and Lucy Vanderwende. 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error Correction. In Proceedings of IJCNLP. Michael Gamon. 2010. Using Mostly Native Data to Correct Errors in Learners’ Writing. In Proceedings of NAACL, pages 163–171 . Y. Guo and Gulbahar Beckett. 2007. The Hegemony of English as a Global Language: Reclaiming Local Knowledge and Culture in China. Convergence: International Journal of Adult Education, 1. Ann Irvine and Alexandre Klementiev. 2010. Using Mechanical Turk to Annotate Lexicons for Less Commonly Used Languages. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon ’s Mechanical Turk, pages 108–1 13. Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies. Morgan Claypool. Nitin Madnani. 2010. The Circle of Meaning: From Translation to Paraphrasing and Back. Ph.D. thesis, Department of Computer Science, University of Maryland College Park. Scott Novotney and Chris Callison-Burch. 2010. Cheap, Fast and Good Enough: Automatic Speech Recognition with Non-Expert Transcription. In Proceedings of NAACL, pages 207–215. Nicholas Rizzolo and Dan Roth. 2007. Modeling Discriminative Global Inference. In Proceedings of 513 the First IEEE International Conference on Semantic Computing (ICSC), pages 597–604, Irvine, California, September. Alla Rozovskaya and D. Roth. 2010a. Annotating ESL errors: Challenges and rewards. In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications. Alla Rozovskaya and D. Roth. 2010b. Generating Confusion Sets for Context-Sensitive Error Correction. In Proceedings of EMNLP. Andreas Stolcke. 2002. SRILM: An Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 257–286. Joel Tetreault and Martin Chodorow. 2008. The Ups and Downs of Preposition Error Detection in ESL Writing. In Proceedings of COLING, pages 865–872. Joel Tetreault, Jill Burstein, and Claudia Leacock, editors. 2010a. Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications. Joel Tetreault, Elena Filatova, and Martin Chodorow. 2010b. Rethinking Grammatical Error Annotation and Evaluation with the Amazon Mechanical Turk. In Proceedings of the NAACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48. Rui Wang and Chris Callison-Burch. 2010. Cheap Facts and Counter-Facts. In Proceedings of the NAACL Workshop on Creating Speech and Language Data with Amazon ’s Mechanical Turk, pages 163–167. Omar F. Zaidan and Chris Callison-Burch. 2010. Predicting Human-Targeted Translation Edit Rate via Untrained Human Annotators. In Proceedings of NAACL, pages 369–372.</p><p>2 0.87219608 <a title="130-lda-2" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>Author: Paul Piwek ; Svetlana Stoyanchev</p><p>Abstract: This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.</p><p>same-paper 3 0.86550379 <a title="130-lda-3" href="./acl-2011-Extracting_Comparative_Entities_and_Predicates_from_Texts_Using_Comparative_Type_Classification.html">130 acl-2011-Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification</a></p>
<p>Author: Seon Yang ; Youngjoong Ko</p><p>Abstract: The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use. 1</p><p>4 0.85140347 <a title="130-lda-4" href="./acl-2011-Generalized_Interpolation_in_Decision_Tree_LM.html">142 acl-2011-Generalized Interpolation in Decision Tree LM</a></p>
<p>Author: Denis Filimonov ; Mary Harper</p><p>Abstract: In the face of sparsity, statistical models are often interpolated with lower order (backoff) models, particularly in Language Modeling. In this paper, we argue that there is a relation between the higher order and the backoff model that must be satisfied in order for the interpolation to be effective. We show that in n-gram models, the relation is trivially held, but in models that allow arbitrary clustering of context (such as decision tree models), this relation is generally not satisfied. Based on this insight, we also propose a generalization of linear interpolation which significantly improves the performance of a decision tree language model.</p><p>5 0.8093313 <a title="130-lda-5" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>6 0.79351175 <a title="130-lda-6" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>7 0.79172546 <a title="130-lda-7" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>8 0.71375376 <a title="130-lda-8" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>9 0.63123477 <a title="130-lda-9" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>10 0.58076894 <a title="130-lda-10" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>11 0.56648374 <a title="130-lda-11" href="./acl-2011-Identifying_Sarcasm_in_Twitter%3A_A_Closer_Look.html">160 acl-2011-Identifying Sarcasm in Twitter: A Closer Look</a></p>
<p>12 0.56489706 <a title="130-lda-12" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>13 0.55527437 <a title="130-lda-13" href="./acl-2011-Grammatical_Error_Correction_with_Alternating_Structure_Optimization.html">147 acl-2011-Grammatical Error Correction with Alternating Structure Optimization</a></p>
<p>14 0.55279702 <a title="130-lda-14" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>15 0.54252398 <a title="130-lda-15" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>16 0.5384537 <a title="130-lda-16" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>17 0.53493977 <a title="130-lda-17" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>18 0.53212082 <a title="130-lda-18" href="./acl-2011-A_Corpus_of_Scope-disambiguated_English_Text.html">8 acl-2011-A Corpus of Scope-disambiguated English Text</a></p>
<p>19 0.53043014 <a title="130-lda-19" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>20 0.52878839 <a title="130-lda-20" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
