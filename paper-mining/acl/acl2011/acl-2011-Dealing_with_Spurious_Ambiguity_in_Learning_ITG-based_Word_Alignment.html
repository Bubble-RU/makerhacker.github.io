<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-93" href="#">acl2011-93</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</h1>
<br/><p>Source: <a title="acl-2011-93-pdf" href="http://aclweb.org/anthology//P/P11/P11-2066.pdf">pdf</a></p><p>Author: Shujian Huang ; Stephan Vogel ; Jiajun Chen</p><p>Abstract: Word alignment has an exponentially large search space, which often makes exact inference infeasible. Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.</p><p>Reference: <a title="acl-2011-93-reference" href="../acl2011_reference/acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn Abstract Word alignment has an exponentially large search space, which often makes exact inference infeasible. [sent-4, score-0.235]
</p><p>2 Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. [sent-5, score-0.376]
</p><p>3 However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. [sent-6, score-0.625]
</p><p>4 In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. [sent-7, score-0.668]
</p><p>5 We also propose a variant of the grammar which eliminates those ambiguities. [sent-8, score-0.122]
</p><p>6 Our grammar shows advantages over previous grammars in both synthetic and real-world experiments. [sent-9, score-0.231]
</p><p>7 1 Introduction In statistical machine translation, word alignment attempts to find word correspondences in parallel sentence pairs. [sent-10, score-0.184]
</p><p>8 The search space of word alignment will grow exponentially with the length of source and target sentences, which makes the inference for complex models infeasible (Brown et al. [sent-11, score-0.21]
</p><p>9 Recently, inversion transduction grammars (Wu, 1997), namely ITG, have been used to constrain the search space for word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007; Haghighi et al. [sent-13, score-0.426]
</p><p>10 ITG is a family of grammars in which the right hand side of the rule is either two nonterminals or a terminal sequence. [sent-16, score-0.215]
</p><p>11 The most general case of the ITG family is the bracketing transduction grammar 379 Carnegie Mellon University voge l cs . [sent-17, score-0.282]
</p><p>12 [AA] denotes a monotone concatenation and hAAi denotes an inverted concatenation. [sent-25, score-0.281]
</p><p>13 Synchronous parsing of ITG may generate a large number of different derivations for the same underlying word alignment. [sent-27, score-0.256]
</p><p>14 This is often referred to as the spurious ambiguity problem. [sent-28, score-0.514]
</p><p>15 Calculating and saving those derivations will slow down the parsing speed significantly. [sent-29, score-0.278]
</p><p>16 Furthermore, spurious derivations may fill up the n-best list and supersede potentially good results, making it harder to find the best alignment. [sent-30, score-0.522]
</p><p>17 Besides, over-counting those spurious derivations will also affect the likelihood estimation. [sent-31, score-0.522]
</p><p>18 In order to reduce spurious derivations, Wu (1997), Haghighi et al. [sent-32, score-0.336]
</p><p>19 These grammars have different behaviors in parsing efficiency and accuracy, but so far no detailed comparison between them has been done. [sent-35, score-0.16]
</p><p>20 In this paper, we formally analyze alignments under ITG constraints and the different causes of spurious ambiguity for those alignments. [sent-36, score-0.645]
</p><p>21 We do an empirical study of the influence of spurious ambiguity on parsing and discriminative learning by compar-  ing different grammars in both synthetic and realdata experiments. [sent-37, score-0.771]
</p><p>22 A new variant of the grammar is proposed, which efficiently removes all spurious ambiguities. [sent-39, score-0.481]
</p><p>23 Our grammar shows advantages over previous ones in both experiments. [sent-40, score-0.095]
</p><p>24 2  ITG Alignment Family  By lexical rules like A → e/f, each ITG derivation actually represents a unique alignment G bet dwereivenat tiohen two sequences. [sent-44, score-0.377]
</p><p>25 Thus the family of ITG derivations represents a family of word alignment. [sent-45, score-0.342]
</p><p>26 The ITG alignment family is a set of word alignments that has at least one BTG derivation. [sent-47, score-0.36]
</p><p>27 ITG alignment family is only a subset of word alignments because there are cases, known as insideoutside alignments (Wu, 1997), that could not be represented by any ITG derivation. [sent-48, score-0.458]
</p><p>28 On the other hand, an ITG alignment may have multiple derivations. [sent-49, score-0.184]
</p><p>29 For a given grammar G, spurious ambiguity in word alignment is the case where two or more derivations d1, d2, . [sent-51, score-0.979]
</p><p>30 dk of G have the same underlying word alignment A. [sent-54, score-0.184]
</p><p>31 A grammar G is nonspurious if for any given word alignment, there exist at most one derivation under G. [sent-55, score-0.197]
</p><p>32 In any given derivation, an ITG rule applies by either generating a bilingual word pair (lexical rules) or splitting the current alignment into two parts, which will recursively generate two sub-derivations (transition rules). [sent-56, score-0.255]
</p><p>33 Applying a monotone (or inverted) concatenation transition rule forms a monotone tsplit (or inverted t-split) of the original alignment (Figure 2). [sent-58, score-0.661]
</p><p>34 1 Branching Ambiguity As shown in Figure 2, left-branching and rightbranching will produce different derivations under 380 A → [AB] | [BB] | [CB] | [AC] | [BC] | [CC] BA → hAAi | hBAi | hCAi | hACi | hBCi | hCCi BCA → e/f | ? [sent-60, score-0.209]
</p><p>35 Branching ambiguity was identified and solved in Wu (1997), using the grammar in Figure 3, denoted as LG. [sent-64, score-0.273]
</p><p>36 LG uses two separate non-terminals for monotone and inverted concatenation, respectively. [sent-65, score-0.219]
</p><p>37 It only allows left branching of such non-terminals, by excluding rules like A → [BA] . [sent-66, score-0.125]
</p><p>38 For each ITG alignment A, in which all the words are aligned, LG will produce a unique derivation. [sent-68, score-0.244]
</p><p>39 Induction hypothesis: the theorem holds  for any A with length less than n. [sent-71, score-0.078]
</p><p>40 For A of length n, let s be the right most t-split which splits A into S1 and S2. [sent-72, score-0.024]
</p><p>41 Assume that there exists another t-split s0, splitting A into S11 and (S12S2). [sent-74, score-0.049]
</p><p>42 Because A is fixed and fully aligned, it is easy to see that if s is a monotone t-split, s0 could only be monotone, and S12 and S2 in the right sub-derivation oft-split s0 could only be combined by monotone concatenation as well. [sent-75, score-0.378]
</p><p>43 So s0 will have a right branching of monotone concatenation, which contradicts with the definition of LG because right branching of monotone concatenations is prohibited. [sent-76, score-0.594]
</p><p>44 A similar contradiction occurs if s is an inverted t-split. [sent-77, score-0.095]
</p><p>45 , S1 and S2 have a unique derivation, because their lengths are less than n. [sent-81, score-0.06]
</p><p>46 For any given sentence pair (e, f) and its alignment A, let (e0, f0) be the sentence pairs with all null-aligned words removed from (e, f). [sent-85, score-0.184]
</p><p>47 The alignment skeleton AS is the alignment between (e0, f0) that preserves all links in A. [sent-86, score-0.458]
</p><p>48 From Theorem 1 we know that every ITG align-  ment has a unique LG derivation for its alignment skeleton (Figure 4 (c)). [sent-87, score-0.411]
</p><p>49 0C1fe012 fe23fe 4 (a)  (b)  (c)  Figure 4: Null-word attachment for the same alignment. [sent-91, score-0.046]
</p><p>50 ((a) and (b) are spurious derivations under LG caused by null-aligned words attachment. [sent-92, score-0.522]
</p><p>51 The dotted lines have omitted some unary rules for simplicity. [sent-94, score-0.031]
</p><p>52 /f Figure 5: A Left heavy Grammar with Fixed Null-word attachment (LGFN). [sent-98, score-0.072]
</p><p>53 no explicit correspondence in the other language and tend to stay unaligned. [sent-99, score-0.058]
</p><p>54 These null-aligned words, also called singletons, should be attached to some other nodes in the derivation. [sent-100, score-0.045]
</p><p>55 It will produce different derivations if those null-aligned words are attached by different rules, or to different nodes. [sent-101, score-0.231]
</p><p>56 3 LGFN Grammar We propose here a new variant of ITG, denoted as LGFN (Figure 5). [sent-108, score-0.027]
</p><p>57 Our grammar takes similar transition rules as LG and efficiently constrains the attachment of null-aligned words. [sent-109, score-0.222]
</p><p>58 We will empirically compare those different grammars in the next section. [sent-110, score-0.09]
</p><p>59 LGFN has a unique mapping from the  derivation of any given ITG alignment A to the derivation of its alignment skeleton AS. [sent-112, score-0.697]
</p><p>60 , Ctk0 , together with the aligned word-pair C00 that directly follows, to the node C exactly in the way of Equation 1. [sent-119, score-0.066]
</p><p>61 ]  (1)  The mapping exists when every null-aligned sequence has an aligned word-pair after it. [sent-133, score-0.09]
</p><p>62 Note that our grammar attaches null-aligned words in a right-branching manner, which means it builds the span only when there is an aligned wordpair. [sent-135, score-0.161]
</p><p>63 After initialization, any newly-built span will contain at least one aligned word-pair. [sent-136, score-0.066]
</p><p>64 LGFN has a unique derivation for each ITG alignment, i. [sent-141, score-0.162]
</p><p>65 1 Synthetic Experiments We automatically generated 1000 fully aligned ITG alignments of length 20 by generating random permutations first and checking ITG constraints using a linear time algorithm (Zhang et al. [sent-146, score-0.164]
</p><p>66 Sparser alignments were generated by random removal of alignment links according to a given null-aligned word ratio. [sent-148, score-0.307]
</p><p>67 Four grammars were used to parse these alignments, namely LG (Wu, 1997), HaG (Haghighi et al. [sent-149, score-0.09]
</p><p>68 Table 1 shows the average number of derivations per alignment generated under LG and HaG. [sent-153, score-0.37]
</p><p>69 The number of derivations produced by LG increased dramatically because LG has no restrictions on nullaligned word attachment. [sent-154, score-0.259]
</p><p>70 HaG also produced a large number of spurious derivations as the number of null-aligned words increased. [sent-155, score-0.522]
</p><p>71 Both LiuG and LGFN  produced a unique derivation for each alignment, as expected. [sent-156, score-0.162]
</p><p>72 9+ Table 1: Average #derivations per alignment for LG and HaG v. [sent-163, score-0.184]
</p><p>73 )  g)Pns it(mesrai6541230 0 0 0HL aF5G 10L Giu1G 5205 PPercenttage off nullll-a lliignedd wordds  Figure 6: Total parsing time (in seconds) v. [sent-167, score-0.07]
</p><p>74 the 10-best alignments for sentence pairs that have 10% of words unaligned, the top 109 HaG derivations should be generated, while the top 10 LiuG or LGFN derivations are already enough. [sent-170, score-0.47]
</p><p>75 Figure 6 shows the total parsing time using each grammar. [sent-171, score-0.07]
</p><p>76 LG and HaG showed better performances when most of the words were aligned because their grammars are simpler and less constrained. [sent-172, score-0.156]
</p><p>77 However, when the number of null-aligned words increased, the parsing times for LG and HaG became much longer, caused by the calculation of the large number of spurious derivations. [sent-173, score-0.406]
</p><p>78 The parsing times of LGFN and LiuG also slowly increased, but parsing LGFN consistently took less time than LiuG. [sent-175, score-0.14]
</p><p>79 It should be noticed that the above results came from parsing according to some given alignment. [sent-176, score-0.07]
</p><p>80 When searching without knowing the correct alignment, it is possible for every word to stay unaligned, which makes spurious ambiguity a much more serious issue. [sent-177, score-0.572]
</p><p>81 2 Discriminative Learning Experiments To further study how spurious ambiguity affects the discriminative learning, we implemented a framework following Haghighi et al. [sent-179, score-0.565]
</p><p>82 probabilities (collected from FBIS data), relative distances, matchings of high frequency words, matchings of pos-tags, etc. [sent-186, score-0.082]
</p><p>83 For each sentence pair (e, f), we optimized with alignment results generated from the nbest parsing results. [sent-189, score-0.287]
</p><p>84 We ran MIRA training for 20 iterations and evaluated the alignments of the best-scored derivations on the test set using the  average weights. [sent-191, score-0.306]
</p><p>85 We used the manually aligned Chinese-English corpus in NIST MT02 evaluation. [sent-192, score-0.066]
</p><p>86 3% words stay null-aligned in each sentence, but if restricted to sure links the average ratio increases to 22. [sent-195, score-0.083]
</p><p>87 Training with HaG only obtained similar results with 1best trained LGFN, which demonstrated that spurious ambiguity highly affected the nbest list here, resulting in a less accurate training. [sent-198, score-0.547]
</p><p>88 Actually, the 20-best parsing using HaG only generated 4. [sent-199, score-0.07]
</p><p>89 We also trained a similar discriminative model but extended the lexical rule of LGFN to accept at maximum 3 consecutive words. [sent-204, score-0.074]
</p><p>90 The model was used to align FBIS data for machine translation experiments. [sent-205, score-0.024]
</p><p>91 Without initializing by phrases extracted  from existing alignments (Cherry and Lin, 2007) or using complicated block features (Haghighi et al. [sent-206, score-0.098]
</p><p>92 , 2006) score over 5 test sets for a typical phrase-based translation system, Moses (Koehn et al. [sent-213, score-0.024]
</p><p>93 5  Conclusion  Great efforts have been made in reducing spurious ambiguities in parsing combinatory categorial grammar (Karttunen, 1986; Eisner, 1996). [sent-215, score-0.57]
</p><p>94 However, to our knowledge, we give the first detailed analysis on spurious ambiguity of word alignment. [sent-216, score-0.514]
</p><p>95 Empirical comparisons between different grammars also validates our analysis. [sent-217, score-0.09]
</p><p>96 This paper makes its own contribution in demonstrating that spurious ambiguity has a negative impact on discriminative learning. [sent-218, score-0.565]
</p><p>97 We will continue working on this line of research and improve our discriminative learning model in the future, for example, by adding more phrase level features. [sent-219, score-0.051]
</p><p>98 It is worth noting that the definition of spurious ambiguity actually varies for different tasks. [sent-220, score-0.558]
</p><p>99 It will also be interesting to explore spurious ambiguity and its effects in those different tasks. [sent-224, score-0.514]
</p><p>100 Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. [sent-288, score-0.335]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lgfn', 0.426), ('itg', 0.371), ('spurious', 0.336), ('lg', 0.324), ('hag', 0.24), ('derivations', 0.186), ('alignment', 0.184), ('ambiguity', 0.178), ('monotone', 0.146), ('btg', 0.108), ('haai', 0.107), ('liug', 0.107), ('derivation', 0.102), ('alignments', 0.098), ('grammar', 0.095), ('branching', 0.094), ('grammars', 0.09), ('transduction', 0.086), ('haghighi', 0.08), ('theorem', 0.078), ('family', 0.078), ('aa', 0.078), ('inverted', 0.073), ('parsing', 0.07), ('aer', 0.066), ('aligned', 0.066), ('inversion', 0.066), ('skeleton', 0.065), ('concatenation', 0.062), ('unique', 0.06), ('stay', 0.058), ('haci', 0.053), ('hbai', 0.053), ('hbci', 0.053), ('hcai', 0.053), ('hcci', 0.053), ('discriminative', 0.051), ('stroudsburg', 0.051), ('nanjing', 0.047), ('synthetic', 0.046), ('attachment', 0.046), ('attached', 0.045), ('definition', 0.044), ('csk', 0.043), ('ba', 0.043), ('proof', 0.043), ('unaligned', 0.042), ('cherry', 0.042), ('wu', 0.041), ('synchronous', 0.041), ('matchings', 0.041), ('combinatory', 0.039), ('fbis', 0.039), ('bb', 0.036), ('nbest', 0.033), ('causes', 0.033), ('cb', 0.032), ('rules', 0.031), ('liu', 0.03), ('categorial', 0.03), ('hao', 0.03), ('bc', 0.029), ('pa', 0.028), ('transition', 0.027), ('variant', 0.027), ('exponentially', 0.026), ('dashed', 0.026), ('heavy', 0.026), ('increased', 0.026), ('snover', 0.025), ('lemma', 0.025), ('splitting', 0.025), ('cn', 0.025), ('links', 0.025), ('exists', 0.024), ('crammer', 0.024), ('right', 0.024), ('zhang', 0.024), ('translation', 0.024), ('restrictions', 0.024), ('radical', 0.023), ('voge', 0.023), ('karttunen', 0.023), ('nullaligned', 0.023), ('rightbranching', 0.023), ('shujie', 0.023), ('association', 0.023), ('efficiently', 0.023), ('ct', 0.023), ('ab', 0.023), ('rule', 0.023), ('bilingual', 0.023), ('och', 0.022), ('iterations', 0.022), ('contradiction', 0.022), ('contradicts', 0.022), ('jiajun', 0.022), ('saving', 0.022), ('gildea', 0.021), ('cc', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="93-tfidf-1" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>Author: Shujian Huang ; Stephan Vogel ; Jiajun Chen</p><p>Abstract: Word alignment has an exponentially large search space, which often makes exact inference infeasible. Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.</p><p>2 0.17771283 <a title="93-tfidf-2" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>3 0.15604578 <a title="93-tfidf-3" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Eiichiro Sumita ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: We present an unsupervised model for joint phrase alignment and extraction using nonparametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size.</p><p>4 0.12866496 <a title="93-tfidf-4" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>5 0.12106571 <a title="93-tfidf-5" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>Author: Ning Xi ; Guangchao Tang ; Boyuan Li ; Yinggong Zhao</p><p>Abstract: In this paper, we present a new word alignment combination approach on language pairs where one language has no explicit word boundaries. Instead of combining word alignments of different models (Xiang et al., 2010), we try to combine word alignments over multiple monolingually motivated word segmentation. Our approach is based on link confidence score defined over multiple segmentations, thus the combined alignment is more robust to inappropriate word segmentation. Our combination algorithm is simple, efficient, and easy to implement. In the Chinese-English experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations. 1</p><p>6 0.11600646 <a title="93-tfidf-6" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>7 0.11467126 <a title="93-tfidf-7" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>8 0.10774905 <a title="93-tfidf-8" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>9 0.097998105 <a title="93-tfidf-9" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>10 0.090822123 <a title="93-tfidf-10" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>11 0.090384349 <a title="93-tfidf-11" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>12 0.089887157 <a title="93-tfidf-12" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>13 0.089098848 <a title="93-tfidf-13" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>14 0.084427431 <a title="93-tfidf-14" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>15 0.083199956 <a title="93-tfidf-15" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>16 0.083067738 <a title="93-tfidf-16" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>17 0.08042907 <a title="93-tfidf-17" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>18 0.080325738 <a title="93-tfidf-18" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>19 0.079970099 <a title="93-tfidf-19" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>20 0.077322759 <a title="93-tfidf-20" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (1, -0.156), (2, 0.071), (3, -0.009), (4, 0.029), (5, 0.001), (6, -0.07), (7, 0.016), (8, -0.049), (9, 0.011), (10, 0.08), (11, 0.114), (12, 0.017), (13, 0.077), (14, -0.084), (15, 0.034), (16, 0.12), (17, -0.013), (18, -0.092), (19, 0.02), (20, -0.032), (21, -0.009), (22, -0.096), (23, 0.003), (24, -0.006), (25, 0.053), (26, 0.027), (27, 0.037), (28, -0.017), (29, -0.07), (30, 0.024), (31, 0.045), (32, -0.023), (33, 0.019), (34, 0.014), (35, -0.016), (36, 0.002), (37, -0.008), (38, 0.005), (39, 0.059), (40, -0.049), (41, 0.05), (42, -0.051), (43, 0.031), (44, -0.053), (45, 0.015), (46, 0.102), (47, 0.002), (48, 0.084), (49, 0.081)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95621598 <a title="93-lsi-1" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>Author: Shujian Huang ; Stephan Vogel ; Jiajun Chen</p><p>Abstract: Word alignment has an exponentially large search space, which often makes exact inference infeasible. Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.</p><p>2 0.76488316 <a title="93-lsi-2" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>3 0.76339746 <a title="93-lsi-3" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>Author: Mohit Bansal ; Chris Quirk ; Robert Moore</p><p>Abstract: We propose a principled and efficient phraseto-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semiMarkov model, word-to-phrase and phraseto-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include “gappy phrases” (such as French ne ? pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime.</p><p>4 0.71296167 <a title="93-lsi-4" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>Author: Wang Ling ; Tiago Luis ; Joao Graca ; Isabel Trancoso ; Luisa Coheur</p><p>Abstract: In most statistical machine translation systems, the phrase/rule extraction algorithm uses alignments in the 1-best form, which might contain spurious alignment points. The usage ofweighted alignment matrices that encode all possible alignments has been shown to generate better phrase tables for phrase-based systems. We propose two algorithms to generate the well known MSD reordering model using weighted alignment matrices. Experiments on the IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set.</p><p>5 0.71017128 <a title="93-lsi-5" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>6 0.70624655 <a title="93-lsi-6" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>7 0.69234729 <a title="93-lsi-7" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>8 0.67749172 <a title="93-lsi-8" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>9 0.6412462 <a title="93-lsi-9" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>10 0.61215633 <a title="93-lsi-10" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>11 0.54253358 <a title="93-lsi-11" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>12 0.52351516 <a title="93-lsi-12" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>13 0.52097017 <a title="93-lsi-13" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>14 0.51642287 <a title="93-lsi-14" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>15 0.50801736 <a title="93-lsi-15" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>16 0.50059795 <a title="93-lsi-16" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>17 0.49839076 <a title="93-lsi-17" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>18 0.48525023 <a title="93-lsi-18" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>19 0.48506162 <a title="93-lsi-19" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>20 0.48294732 <a title="93-lsi-20" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.028), (6, 0.274), (17, 0.064), (26, 0.022), (37, 0.081), (39, 0.04), (41, 0.08), (53, 0.016), (55, 0.034), (59, 0.039), (72, 0.053), (91, 0.037), (96, 0.136), (98, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81601191 <a title="93-lda-1" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>Author: Clifton McFate ; Kenneth Forbus</p><p>Abstract: Broad coverage lexicons for the English language have traditionally been handmade. This approach, while accurate, requires too much human labor. Furthermore, resources contain gaps in coverage, contain specific types of information, or are incompatible with other resources. We believe that the state of open-license technology is such that a comprehensive syntactic lexicon can be automatically compiled. This paper describes the creation of such a lexicon, NU-LEX, an open-license feature-based lexicon for general purpose parsing that combines WordNet, VerbNet, and Wiktionary and contains over 100,000 words. NU-LEX was integrated into a bottom up chart parser. We ran the parser through three sets of sentences, 50 sentences total, from the Simple English Wikipedia and compared its performance to the same parser using Comlex. Both parsers performed almost equally with NU-LEX finding all lex-items for 50% of the sentences and Comlex succeeding for 52%. Furthermore, NULEX’s shortcomings primarily fell into two categories, suggesting future research directions. 1</p><p>same-paper 2 0.74057984 <a title="93-lda-2" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>Author: Shujian Huang ; Stephan Vogel ; Jiajun Chen</p><p>Abstract: Word alignment has an exponentially large search space, which often makes exact inference infeasible. Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.</p><p>3 0.70913696 <a title="93-lda-3" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In this paper we present Clairlib, an opensource toolkit for Natural Language Processing, Information Retrieval, and Network Analysis. Clairlib provides an integrated framework intended to simplify a number of generic tasks within and across those three areas. It has a command-line interface, a graphical interface, and a documented API. Clairlib is compatible with all the common platforms and operating systems. In addition to its own functionality, it provides interfaces to external software and corpora. Clairlib comes with a comprehensive documentation and a rich set of tutorials and visual demos.</p><p>4 0.59323472 <a title="93-lda-4" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>Author: Shasha Liao ; Ralph Grishman</p><p>Abstract: Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 1</p><p>5 0.59210062 <a title="93-lda-5" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>6 0.59034479 <a title="93-lda-6" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>7 0.5893814 <a title="93-lda-7" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>8 0.58933049 <a title="93-lda-8" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>9 0.58906633 <a title="93-lda-9" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>10 0.58755147 <a title="93-lda-10" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>11 0.58607626 <a title="93-lda-11" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>12 0.58416355 <a title="93-lda-12" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>13 0.58413994 <a title="93-lda-13" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>14 0.5841378 <a title="93-lda-14" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>15 0.58374619 <a title="93-lda-15" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>16 0.58354998 <a title="93-lda-16" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>17 0.5822168 <a title="93-lda-17" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>18 0.58134252 <a title="93-lda-18" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>19 0.581285 <a title="93-lda-19" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>20 0.58121383 <a title="93-lda-20" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
