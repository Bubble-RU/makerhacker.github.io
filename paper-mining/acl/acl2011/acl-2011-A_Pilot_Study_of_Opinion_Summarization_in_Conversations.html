<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-21" href="#">acl2011-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</h1>
<br/><p>Source: <a title="acl-2011-21-pdf" href="http://aclweb.org/anthology//P/P11/P11-1034.pdf">pdf</a></p><p>Author: Dong Wang ; Yang Liu</p><p>Abstract: This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.</p><p>Reference: <a title="acl-2011-21-reference" href="../acl2011_reference/acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper presents a pilot study of opinion summarization on conversations. [sent-3, score-0.672]
</p><p>2 We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. [sent-4, score-1.298]
</p><p>3 The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. [sent-7, score-0.439]
</p><p>4 1 Introduction Both sentiment analysis (opinion recognition) and summarization have been well studied in recent years in the natural language processing (NLP) community. [sent-10, score-0.449]
</p><p>5 Summarization has been applied to different genres, such as news articles, scientific articles, and speech domains including broadcast news, meetings, conversations and lectures. [sent-12, score-0.37]
</p><p>6 This kind of questions can be treated as a topic-oriented opinion summarization task. [sent-16, score-0.62]
</p><p>7 Opinion summarization was run as a pilot task in Text Analysis Conference  (TAC) in 2008. [sent-17, score-0.393]
</p><p>8 The task was to produce summaries of opinions on specified targets from a set of blog documents. [sent-18, score-0.2]
</p><p>9 The problem is defined as, given a conversation and a topic, a summarization system needs to generate a summary of the speaker’s opinion towards the topic. [sent-20, score-0.852]
</p><p>10 This task is built upon opinion recognition and topic or query based summarization. [sent-21, score-0.471]
</p><p>11 , 2008); (c) In conversational speech, information density is low and there are often off topic discussions, therefore presenting a need to identify utterances that are relevant to the topic. [sent-24, score-0.334]
</p><p>12 In this paper we perform an exploratory study on opinion summarization in conversations. [sent-25, score-0.62]
</p><p>13 Ac s2s0o1ci1a Atiosnso fcoirat Cioonm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 331–339, widely used in extractive summarization: sentenceranking and graph-based methods. [sent-28, score-0.371]
</p><p>14 Our system attempts to incorporate more information about topic relevancy and sentiment scores. [sent-29, score-0.326]
</p><p>15 Furthermore, in the graph-based method, we propose to better incorporate the dialogue structure information in the graph in order to select salient summary utterances. [sent-30, score-0.355]
</p><p>16 We explain our opinion-oriented conversation summarization system in Section 4 and present experimental results and analysis in Section 5. [sent-36, score-0.46]
</p><p>17 2  Related Work  Research in document summarization has been well  established over the past decades. [sent-38, score-0.341]
</p><p>18 Recently there is an increasing research interest in speech summarization, such as conversational telephone speech (Zhu and Penn, 2006; Zechner, 2002), broadcast news (Maskey and Hirschberg, 2005; Lin et al. [sent-41, score-0.312]
</p><p>19 Prior research has also explored using speech specific information, including prosodic features, dialog structure, and speech recognition confidence. [sent-48, score-0.303]
</p><p>20 In order to provide a summary over opinions, we  need to find out which utterances in the conversation contain opinion. [sent-49, score-0.351]
</p><p>21 Most of prior work used classification methods such as naive Bayes or SVMs to perform the polarity classification or opinion detection. [sent-53, score-0.279]
</p><p>22 Only a handful studies have used conversational speech for opinion recognition (Murray and Carenini, 2009; Raaijmakers et al. [sent-54, score-0.431]
</p><p>23 Our work is also related to question answering (QA), especially opinion question answering. [sent-56, score-0.343]
</p><p>24 , 2010) answers some specific opinion questions like “Why do people criticize Richard Branson? [sent-60, score-0.279]
</p><p>25 Our work is different in that we are not going to answer specific opinion questions, instead, we provide a summary on the speaker’s opinion towards a given topic. [sent-62, score-0.713]
</p><p>26 , 2010) have explored opinion summarization in review domain, and (Paul et al. [sent-65, score-0.651]
</p><p>27 However, opinion summarization in spontaneous conversation is seldom studied. [sent-67, score-0.821]
</p><p>28 3  Corpus Creation  Though there are many annotated data sets for the research of speech summarization and sentiment analysis, there is no corpus available for opinion summarization on spontaneous speech. [sent-68, score-1.222]
</p><p>29 1 These are conversational telephone speech between two strangers that were assigned a topic to talk about for around 5 minutes. [sent-70, score-0.34]
</p><p>30 pus, we selected 88 conversations from 6 topics for this study. [sent-74, score-0.236]
</p><p>31 Table 1lists the number of conversations in each topic, their average length (measured in the unit of dialogue acts (DA)) and standard deviation of length. [sent-75, score-0.368]
</p><p>32 4erv0of  conversations in each topic, average length (number of dialog acts), and standard deviation. [sent-78, score-0.265]
</p><p>33 The rest of the conversations has only one annotation. [sent-81, score-0.2]
</p><p>34 The annotators have access to both conversation transcripts and audio files. [sent-82, score-0.224]
</p><p>35 For each conversation, the annotator writes an abstractive summary of up to 100 words for each speaker about his/her opinion or attitude on the given topic. [sent-83, score-0.871]
</p><p>36 Then the annotator selects up to 15 DAs (no minimum limit) in the transcripts for each speaker, from which their abstractive summary is derived. [sent-85, score-0.495]
</p><p>37 The selected DAs are used as the human generated extractive summary. [sent-86, score-0.365]
</p><p>38 In addition, the annotator is asked to select an overall opinion towards the topic for each speaker among five categories: strongly support, somewhat support, neutral, somewhat against, strongly against. [sent-87, score-0.617]
</p><p>39 Therefore for each conversation, we have an abstractive summary, an extractive summary, and an overall opinion for each speaker. [sent-88, score-0.912]
</p><p>40 The following shows an example of such annotation for speaker B in a dialogue about “capital punishment”: [Extractive Summary] I think I’ve seen some statistics that say that, uh, it’s more expensive to kill somebody than to keep them in prison for life. [sent-89, score-0.398]
</p><p>41 I ’t think he could ever redeem himself, don but if you look at who gets accused and who are the ones who actually get executed, it’s very racially related and ethnically related [Abstractive Summary] B is against capital punishment except under certain circumstances. [sent-91, score-0.24]
</p><p>42 B finds that crimes deserving of capital punishment are “crimes of the moment” and as a result feels that capital punishment is not an effective deterrent. [sent-92, score-0.353]
</p><p>43 [Overall Opinion] Somewhat against Table 2 shows the compression ratio of the extractive summaries and abstractive summaries as well as their standard deviation. [sent-94, score-1.175]
</p><p>44 06 Table 2: Compression ratio and standard deviation of extractive and abstractive summaries. [sent-100, score-0.699]
</p><p>45 We measured the inter-annotator agreement among the three annotators for the 18 conversations (each has two speakers, thus 36 “documents” in total). [sent-101, score-0.301]
</p><p>46 For the extractive or abstractive summaries, we use ROUGE scores (Lin, 2004), a metric used to evaluate automatic summarization performance, to measure the pairwise agreement of summaries from different annotators. [sent-103, score-1.181]
</p><p>47 We notice that the inter-annotator agreement for extractive summaries is comparable to other speech  Table3:Int r-xsaovncetriavlos upamignreoamisntRαf o-2=1Lre0x . [sent-106, score-0.607]
</p><p>48 The agreement on abstractive summaries is much lower than extractive summaries, which is as expected. [sent-109, score-0.84]
</p><p>49 Even for the same opinion or sentence, annotators use different words in the abstractive summaries. [sent-110, score-0.646]
</p><p>50 The agreement for the overall opinion annotation is similar to other opinion/emotion studies (Wilson, 2008b), but slightly lower than the level recommended by Krippendorff for reliable data (α = 0. [sent-111, score-0.317]
</p><p>51 8) (Hayes and Krippendorff, 2007), which shows it is even difficult for humans to determine what  opinion a person holds (support or against something). [sent-112, score-0.279]
</p><p>52 Therefore this also demonstrates that it is more appropriate to provide a summary rather than a simple opinion category to answer questions about a person’s opinion towards something. [sent-114, score-0.713]
</p><p>53 4  Opinion Summarization Methods  Automatic summarization can be divided into extractive summarization and abstractive summarization. [sent-115, score-1.315]
</p><p>54 Extractive summarization selects sentences from the original documents to form a summary; whereas abstractive summarization requires generation of new sentences that represent the most salient content in the original documents like humans do. [sent-116, score-1.03]
</p><p>55 Often extractive summarization is used as the first step to generate abstractive summary. [sent-117, score-0.974]
</p><p>56 As a pilot study for the problem of opinion summarization in conversations, we treat this problem as an extractive summarization task. [sent-118, score-1.342]
</p><p>57 This section describes two approaches we have explored in generating extractive summaries. [sent-119, score-0.36]
</p><p>58 In addition, they do not require a large labeled data set for modeling training, as needed in some classification or feature based summarization approaches. [sent-123, score-0.341]
</p><p>59 score(s)  =  λsimsim(s, D) + λrelREL(s, topic) +λsentsentiment(s) + λlenlength(s)  Xλi = 1 Xi  (1)  •  •  sim(s, D) is the cosine similarity between DA s amn(ds ,aDll )th ise uhtete croasnicnees s i mn tlharei dialogue nfr DomA the same speaker, D. [sent-126, score-0.246]
</p><p>60 It measures the relevancy of s to the entire dialogue from the target speaker. [sent-127, score-0.223]
</p><p>61 It has been shown to be an important indicator in summarization for various domains. [sent-129, score-0.341]
</p><p>62 REL(s, topic) measures the topic relevance of RDAE s. [sent-132, score-0.244]
</p><p>63 html where all the statistics are collected from the Switchboard corpus: p(w&topic;) denotes the probability that word w appears in a dialogue of topic t, and p(w) is the probability of w appearing in a dialogue of any topic. [sent-140, score-0.499]
</p><p>64 In this approach, a document  335 is modeled as an adjacency matrix, where each node represents a sentence, and the weight of the edge between each pair of sentences is their similarity (cosine similarity is typically used). [sent-153, score-0.213]
</p><p>65 The basic framework we use in this study is similar to the query-based graph summarization system in (Zhao et al. [sent-156, score-0.371]
</p><p>66 We also consider sentiment and topic relevance information, and propose to incorporate information obtained from dialog structure in this framework. [sent-158, score-0.417]
</p><p>67 •  •  If s and v are from the same speaker, and separated only by one DsaAm efr sopmea aern,ot ahnedr speaker with length less than 3 words (usually backchannel), there is an edge from s to v as well as an edge from v to s with weight 1 (ADJ(s, v) = ADJ(v, s) = 1). [sent-164, score-0.269]
</p><p>68 Since we are using a directed graph for the sentence connections to model dialog structure, the resulting adjacency matrix is asymmetric. [sent-174, score-0.257]
</p><p>69 Also note that in the first sentence ranking method or the basic graph methods, summarization is conducted for each speaker separately. [sent-176, score-0.54]
</p><p>70 Utterances from one speaker have no influence on the summary decision for the other speaker. [sent-177, score-0.252]
</p><p>71 1 Experimental Setup The 18 conversations annotated by all 3 annotators are used as test set, and the rest of 70 conversations are used as development set to tune the parameters (determining the best combination weights). [sent-180, score-0.463]
</p><p>72 We perform extractive summarization using different word compression ratios (ranging from 10% to 25%). [sent-182, score-0.808]
</p><p>73 The system-generated summaries are compared to human annotated extractive and abstractive summaries. [sent-184, score-0.802]
</p><p>74 We use ROUGE as the evaluation metrics for summarization performance. [sent-185, score-0.341]
</p><p>75 This has been shown to be a relatively strong baseline for speech summarization (Gillick et al. [sent-188, score-0.412]
</p><p>76 We treat each annotator’s extractive summary as a system summary, and compare to the other two annotators’ extractive and  abstractive summaries. [sent-191, score-1.075]
</p><p>77 2 Results From the development set, we used the grid search method to obtain the best combination weights for the two summarization methods. [sent-194, score-0.341]
</p><p>78 In addition, REL score is already able to catch the topic relevancy of the sentence. [sent-201, score-0.261]
</p><p>79 This is different from graph-based summarization systems for text domains. [sent-208, score-0.341]
</p><p>80 tween utterances does not perform well in conversation summarization. [sent-211, score-0.238]
</p><p>81 Figure 1 shows the ROUGE-1 F-scores comparing to human extractive and abstractive summaries for different compression ratios. [sent-212, score-0.94]
</p><p>82 When compared to extractive reference summaries, sentence-ranking is slightly better except for the compression ratio of 0. [sent-218, score-0.565]
</p><p>83 When compared to abstractive reference summaries, the graphbased method is slightly better. [sent-220, score-0.336]
</p><p>84 3 Analysis To analyze the effect of dialogue structure we introduce in the graph-based summarization method, we compare two configurations: λadj = 0 (only using REL score and sentiment score in ranking) and λadj = 0. [sent-223, score-0.703]
</p><p>85 We generate summaries using these two setups and compare with human selected sentences. [sent-225, score-0.205]
</p><p>86 This results in a large number of reference summary DAs (because of low human agreement), and thus the number of false negatives in the system output is very high. [sent-228, score-0.205]
</p><p>87 As expected, a smaller compression ratio (fewer selected DAs in the system output) yields a higher false negative rate and a lower false positive rate. [sent-229, score-0.36]
</p><p>88 We use the statistics from the Switchboard corpus to measure the relevance of each word to a given topic (PMI score), therefore only when people use the same word in different conversations of the topic, the PMI score of this word and the topic is high. [sent-242, score-0.65]
</p><p>89 We used DA segments as units for extractive summarization, which can be problematic. [sent-247, score-0.329]
</p><p>90 The two DAs from speaker B are not selected by our system but selected by human anno338 tators, causing false negative errors. [sent-251, score-0.271]
</p><p>91 –  –  6  Conclusion and Future Work  This paper investigates two unsupervised methods in opinion summarization on spontaneous conversations by incorporating topic score and sentiment score in existing summarization techniques. [sent-253, score-1.6]
</p><p>92 In the graph-based method, we use an adjacency matrix to model the dialogue structure and utilize it to find salient utterances in conversations. [sent-255, score-0.429]
</p><p>93 Our experiments show that both methods are able to improve the baseline approach, and we find that the cosine similarity between utterances or between an utterance and the whole docu-  ment is not as useful as in other document summarization tasks. [sent-256, score-0.579]
</p><p>94 Going beyond traditional QA systems: challenges and keys in opinion question answering. [sent-265, score-0.311]
</p><p>95 Auto-  matic summarization of voicemail messages using lexical and prosodic features. [sent-299, score-0.377]
</p><p>96 An analysis of human extractive summaries in meeting corpus. [sent-312, score-0.498]
</p><p>97 Opinion summarization with integer linear programming formulation for sentence extraction and ordering. [sent-338, score-0.371]
</p><p>98 A sentiment education: sentiment analysis using subjectivity summarization based on minimum cuts. [sent-342, score-0.624]
</p><p>99 Improving supervised learning for meeting summarization using sampling and regression. [sent-380, score-0.341]
</p><p>100 Automatic summarization of open-domain multiparty dialogues in dive rse genres. [sent-384, score-0.385]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('summarization', 0.341), ('extractive', 0.329), ('abstractive', 0.304), ('opinion', 0.279), ('conversations', 0.2), ('adj', 0.176), ('das', 0.174), ('summaries', 0.169), ('dialogue', 0.168), ('topic', 0.163), ('da', 0.161), ('speaker', 0.139), ('compression', 0.138), ('conversation', 0.119), ('utterances', 0.119), ('rel', 0.116), ('summary', 0.113), ('sentiment', 0.108), ('pmi', 0.088), ('switchboard', 0.087), ('punishment', 0.085), ('spontaneous', 0.082), ('relevance', 0.081), ('speech', 0.071), ('murray', 0.068), ('subjectivity', 0.067), ('ratio', 0.066), ('edge', 0.065), ('dialog', 0.065), ('rouge', 0.064), ('adjacency', 0.064), ('capital', 0.064), ('raaijmakers', 0.063), ('annotators', 0.063), ('false', 0.06), ('sim', 0.059), ('crimes', 0.055), ('relevancy', 0.055), ('telephone', 0.054), ('conversational', 0.052), ('pilot', 0.052), ('qa', 0.051), ('think', 0.049), ('theresa', 0.048), ('krippendorff', 0.046), ('uh', 0.046), ('salient', 0.044), ('multiparty', 0.044), ('score', 0.043), ('similarity', 0.042), ('furui', 0.042), ('koumpis', 0.042), ('maskey', 0.042), ('prison', 0.042), ('redeem', 0.042), ('relorig', 0.042), ('sentenceranking', 0.042), ('longest', 0.042), ('transcripts', 0.042), ('answer', 0.042), ('utterance', 0.041), ('opinionated', 0.039), ('agreement', 0.038), ('wilson', 0.038), ('hirschberg', 0.037), ('subjective', 0.037), ('gabriel', 0.037), ('korbinian', 0.037), ('renals', 0.037), ('annotator', 0.036), ('selected', 0.036), ('prosodic', 0.036), ('cosine', 0.036), ('domains', 0.035), ('connections', 0.034), ('balahur', 0.034), ('godfrey', 0.034), ('nishikawa', 0.034), ('stoyanov', 0.034), ('matrix', 0.034), ('broadcast', 0.034), ('salience', 0.033), ('julia', 0.032), ('reference', 0.032), ('question', 0.032), ('garg', 0.032), ('hayes', 0.032), ('moment', 0.032), ('xie', 0.032), ('connection', 0.031), ('explored', 0.031), ('opinions', 0.031), ('janyce', 0.031), ('news', 0.03), ('viewpoints', 0.03), ('sentence', 0.03), ('graph', 0.03), ('recognition', 0.029), ('benoit', 0.029), ('gillick', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="21-tfidf-1" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>Author: Dong Wang ; Yang Liu</p><p>Abstract: This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.</p><p>2 0.2928099 <a title="21-tfidf-2" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Gillick ; Dan Klein</p><p>Abstract: We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.</p><p>3 0.2535181 <a title="21-tfidf-3" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>Author: Lei Zhang ; Bing Liu</p><p>Abstract: Identifying domain-dependent opinion words is a key problem in opinion mining and has been studied by several researchers. However, existing work has been focused on adjectives and to some extent verbs. Limited work has been done on nouns and noun phrases. In our work, we used the feature-based opinion mining model, and we found that in some domains nouns and noun phrases that indicate product features may also imply opinions. In many such cases, these nouns are not subjective but objective. Their involved sentences are also objective sentences and imply positive or negative opinions. Identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1</p><p>4 0.22058949 <a title="21-tfidf-4" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>Author: Charles Greenbacker</p><p>Abstract: We propose a framework for generating an abstractive summary from a semantic model of a multimodal document. We discuss the type of model required, the means by which it can be constructed, how the content of the model is rated and selected, and the method of realizing novel sentences for the summary. To this end, we introduce a metric called information density used for gauging the importance of content obtained from text and graphical sources.</p><p>5 0.19527498 <a title="21-tfidf-5" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>6 0.18476075 <a title="21-tfidf-6" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>7 0.17869842 <a title="21-tfidf-7" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>8 0.17093836 <a title="21-tfidf-8" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>9 0.1687566 <a title="21-tfidf-9" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>10 0.16748151 <a title="21-tfidf-10" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>11 0.16663174 <a title="21-tfidf-11" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>12 0.1664646 <a title="21-tfidf-12" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>13 0.159141 <a title="21-tfidf-13" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>14 0.15174191 <a title="21-tfidf-14" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<p>15 0.14990728 <a title="21-tfidf-15" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>16 0.14955951 <a title="21-tfidf-16" href="./acl-2011-Detection_of_Agreement_and_Disagreement_in_Broadcast_Conversations.html">95 acl-2011-Detection of Agreement and Disagreement in Broadcast Conversations</a></p>
<p>17 0.14390448 <a title="21-tfidf-17" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>18 0.14130831 <a title="21-tfidf-18" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<p>19 0.14059241 <a title="21-tfidf-19" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>20 0.13777617 <a title="21-tfidf-20" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.233), (1, 0.254), (2, 0.03), (3, 0.121), (4, -0.246), (5, 0.144), (6, -0.18), (7, 0.242), (8, 0.019), (9, -0.035), (10, -0.035), (11, 0.016), (12, -0.157), (13, -0.053), (14, -0.11), (15, -0.025), (16, 0.071), (17, 0.021), (18, 0.025), (19, 0.029), (20, -0.094), (21, -0.018), (22, 0.058), (23, 0.092), (24, 0.014), (25, 0.016), (26, 0.125), (27, -0.127), (28, 0.033), (29, 0.125), (30, 0.17), (31, -0.057), (32, -0.009), (33, -0.037), (34, 0.05), (35, 0.025), (36, 0.038), (37, 0.017), (38, -0.002), (39, -0.088), (40, -0.059), (41, 0.05), (42, 0.023), (43, -0.028), (44, -0.028), (45, 0.021), (46, 0.005), (47, -0.081), (48, 0.008), (49, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96385121 <a title="21-lsi-1" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>Author: Dong Wang ; Yang Liu</p><p>Abstract: This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.</p><p>2 0.65855318 <a title="21-lsi-2" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>Author: Xiaojiang Huang ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Comparative News Summarization aims to highlight the commonalities and differences between two comparable news topics. In this study, we propose a novel approach to generating comparative news summaries. We formulate the task as an optimization problem of selecting proper sentences to maximize the comparativeness within the summary and the representativeness to both news topics. We consider semantic-related cross-topic concept pairs as comparative evidences, and consider topic-related concepts as representative evidences. The optimization problem is addressed by using a linear programming model. The experimental results demonstrate the effectiveness of our proposed model.</p><p>3 0.65177643 <a title="21-lsi-3" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Gillick ; Dan Klein</p><p>Abstract: We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.</p><p>4 0.63731319 <a title="21-lsi-4" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>Author: Charles Greenbacker</p><p>Abstract: We propose a framework for generating an abstractive summary from a semantic model of a multimodal document. We discuss the type of model required, the means by which it can be constructed, how the content of the model is rated and selected, and the method of realizing novel sentences for the summary. To this end, we introduce a metric called information density used for gauging the importance of content obtained from text and graphical sources.</p><p>5 0.60912138 <a title="21-lsi-5" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>6 0.60046798 <a title="21-lsi-6" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<p>7 0.59082609 <a title="21-lsi-7" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>8 0.58970034 <a title="21-lsi-8" href="./acl-2011-Liars_and_Saviors_in_a_Sentiment_Annotated_Corpus_of_Comments_to_Political_Debates.html">211 acl-2011-Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates</a></p>
<p>9 0.58711451 <a title="21-lsi-9" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>10 0.58650184 <a title="21-lsi-10" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>11 0.57380152 <a title="21-lsi-11" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>12 0.57159644 <a title="21-lsi-12" href="./acl-2011-Finding_Deceptive_Opinion_Spam_by_Any_Stretch_of_the_Imagination.html">136 acl-2011-Finding Deceptive Opinion Spam by Any Stretch of the Imagination</a></p>
<p>13 0.56634355 <a title="21-lsi-13" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>14 0.55970091 <a title="21-lsi-14" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>15 0.54762906 <a title="21-lsi-15" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>16 0.53214478 <a title="21-lsi-16" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>17 0.52711284 <a title="21-lsi-17" href="./acl-2011-Extracting_Opinion_Expressions_and_Their_Polarities_-_Exploration_of_Pipelines_and_Joint_Models.html">131 acl-2011-Extracting Opinion Expressions and Their Polarities - Exploration of Pipelines and Joint Models</a></p>
<p>18 0.50901133 <a title="21-lsi-18" href="./acl-2011-Contrasting_Opposing_Views_of_News_Articles_on_Contentious_Issues.html">84 acl-2011-Contrasting Opposing Views of News Articles on Contentious Issues</a></p>
<p>19 0.4976103 <a title="21-lsi-19" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>20 0.47957402 <a title="21-lsi-20" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.05), (15, 0.172), (17, 0.07), (26, 0.013), (31, 0.03), (37, 0.084), (39, 0.037), (41, 0.058), (53, 0.033), (55, 0.021), (59, 0.026), (72, 0.037), (91, 0.032), (96, 0.234)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9416641 <a title="21-lda-1" href="./acl-2011-French_TimeBank%3A_An_ISO-TimeML_Annotated_Reference_Corpus.html">138 acl-2011-French TimeBank: An ISO-TimeML Annotated Reference Corpus</a></p>
<p>Author: Andre Bittar ; Pascal Amsili ; Pascal Denis ; Laurence Danlos</p><p>Abstract: This article presents the main points in the creation of the French TimeBank (Bittar, 2010), a reference corpus annotated according to the ISO-TimeML standard for temporal annotation. A number of improvements were made to the markup language to deal with linguistic phenomena not yet covered by ISO-TimeML, including cross-language modifications and others specific to French. An automatic preannotation system was used to speed up the annotation process. A preliminary evaluation of the methodology adopted for this project yields positive results in terms of data quality and annotation time.</p><p>2 0.91340601 <a title="21-lda-2" href="./acl-2011-Structural_Topic_Model_for_Latent_Topical_Structure_Analysis.html">287 acl-2011-Structural Topic Model for Latent Topical Structure Analysis</a></p>
<p>Author: Hongning Wang ; Duo Zhang ; ChengXiang Zhai</p><p>Abstract: Topic models have been successfully applied to many document analysis tasks to discover topics embedded in text. However, existing topic models generally cannot capture the latent topical structures in documents. Since languages are intrinsically cohesive and coherent, modeling and discovering latent topical transition structures within documents would be beneficial for many text analysis tasks. In this work, we propose a new topic model, Structural Topic Model, which simultaneously discovers topics and reveals the latent topical structures in text through explicitly modeling topical transitions with a latent first-order Markov chain. Experiment results show that the proposed Structural Topic Model can effectively discover topical structures in text, and the identified structures significantly improve the performance of tasks such as sentence annotation and sentence ordering. ,</p><p>same-paper 3 0.8973043 <a title="21-lda-3" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>Author: Dong Wang ; Yang Liu</p><p>Abstract: This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.</p><p>4 0.87338442 <a title="21-lda-4" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>Author: Lonneke van der Plas ; Paola Merlo ; James Henderson</p><p>Abstract: Broad-coverage semantic annotations for training statistical learners are only available for a handful of languages. Previous approaches to cross-lingual transfer of semantic annotations have addressed this problem with encouraging results on a small scale. In this paper, we scale up previous efforts by using an automatic approach to semantic annotation that does not rely on a semantic ontology for the target language. Moreover, we improve the quality of the transferred semantic annotations by using a joint syntacticsemantic parser that learns the correlations between syntax and semantics of the target language and smooths out the errors from automatic transfer. We reach a labelled F-measure for predicates and arguments of only 4% and 9% points, respectively, lower than the upper bound from manual annotations.</p><p>5 0.83002985 <a title="21-lda-5" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>6 0.82928371 <a title="21-lda-6" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>7 0.82844448 <a title="21-lda-7" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>8 0.82795763 <a title="21-lda-8" href="./acl-2011-Monolingual_Alignment_by_Edit_Rate_Computation_on_Sentential_Paraphrase_Pairs.html">225 acl-2011-Monolingual Alignment by Edit Rate Computation on Sentential Paraphrase Pairs</a></p>
<p>9 0.82772291 <a title="21-lda-9" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>10 0.82756853 <a title="21-lda-10" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>11 0.8265909 <a title="21-lda-11" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>12 0.82594955 <a title="21-lda-12" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>13 0.82494551 <a title="21-lda-13" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>14 0.82453001 <a title="21-lda-14" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>15 0.82284415 <a title="21-lda-15" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>16 0.82254612 <a title="21-lda-16" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>17 0.82193226 <a title="21-lda-17" href="./acl-2011-Disentangling_Chat_with_Local_Coherence_Models.html">101 acl-2011-Disentangling Chat with Local Coherence Models</a></p>
<p>18 0.82186359 <a title="21-lda-18" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>19 0.82180953 <a title="21-lda-19" href="./acl-2011-Translating_from_Morphologically_Complex_Languages%3A_A_Paraphrase-Based_Approach.html">310 acl-2011-Translating from Morphologically Complex Languages: A Paraphrase-Based Approach</a></p>
<p>20 0.82162768 <a title="21-lda-20" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
