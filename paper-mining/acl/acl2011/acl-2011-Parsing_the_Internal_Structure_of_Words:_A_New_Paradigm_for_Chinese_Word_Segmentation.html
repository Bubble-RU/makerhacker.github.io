<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-241" href="#">acl2011-241</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</h1>
<br/><p>Source: <a title="acl-2011-241-pdf" href="http://aclweb.org/anthology//P/P11/P11-1141.pdf">pdf</a></p><p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>Reference: <a title="acl-2011-241-reference" href="../acl2011_reference/acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. [sent-3, score-0.733]
</p><p>2 Instead, we propose that word structures should be recovered in morphological analysis. [sent-5, score-0.401]
</p><p>3 Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. [sent-7, score-0.414]
</p><p>4 Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al. [sent-9, score-0.294]
</p><p>5 However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. [sent-11, score-0.405]
</p><p>6 The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al. [sent-13, score-0.46]
</p><p>7 So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more  1  1Here it is necessary to add a note on terminology used in this paper. [sent-24, score-0.342]
</p><p>8 For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. [sent-32, score-0.268]
</p><p>9 The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. [sent-36, score-0.294]
</p><p>10 Regarding it as one flat word loses this important information. [sent-38, score-0.324]
</p><p>11 1406 structures is related to head driven statistical parsers (Collins, 2003). [sent-54, score-0.358]
</p><p>12 Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. [sent-56, score-0.223]
</p><p>13 Had there been only a few words with internal structures, current Chinese word segmentation paradigm would be sufficient. [sent-62, score-0.505]
</p><p>14 With so many productive suf-  fixes and prefixes, analyzing word structures in postprocessing is difficult, because a character may or may not act as an affix depending on the context. [sent-67, score-0.453]
</p><p>15 The structures of these two words are shown in Figure 4. [sent-69, score-0.237]
</p><p>16 The character 吼 ‘people’ is part of a personal name in tree (a), but is a suffix in (b). [sent-71, score-0.295]
</p><p>17 A second reason why generally we cannot recover word structures in post-processing is that some words have very complex structures. [sent-72, score-0.304]
</p><p>18 Finally, it must be mentioned that we cannot store all word structures in a dictionary, as the word formation process is very dynamic and productive in nature. [sent-79, score-0.446]
</p><p>19 This is understandable since the character 䌬 ‘hall’ is so productive that it is impossible for a dictionary to list every word with this character as a suffix. [sent-82, score-0.29]
</p><p>20 1407 In this paper, we propose a new paradigm for Chinese word segmentation in which not only word boundaries are identified but the internal structures of words are recovered (Section 3). [sent-85, score-0.882]
</p><p>21 To achieve this, we design a joint morphological and syntactic parsing model of Chinese (Section 4). [sent-86, score-0.231]
</p><p>22 Our generative story describes the complete process from sentence and word structures to the surface string of characters in a top-down fashion. [sent-87, score-0.44]
</p><p>23 The output of our parser incorporates word structures naturally. [sent-89, score-0.424]
</p><p>24 Evaluation shows that the model can learn much of the regularity of word structures, and also achieves reasonable accuracy in parsing higher level constituent structures (Section 6). [sent-90, score-0.566]
</p><p>25 2  Related Work  The necessity of parsing word structures has been noticed by Zhao (2009), who presented a character-  level dependency scheme as an alternative to the linear representation of words. [sent-91, score-0.465]
</p><p>26 The first one is that part-of-speech tags and constituent labels are fundamental for our parsing model, while Zhao focused on unlabeled dependencies between characters in a word, and part-ofspeech information was not utilized. [sent-93, score-0.361]
</p><p>27 Secondly, we distinguish explicitly the generation of flat words such as 䑵喏䃮 ‘Washington’ and words with internal structures. [sent-94, score-0.404]
</p><p>28 Many researchers have also noticed the awkwardness and insufficiency ofcurrent boundary-only Chinese word segmentation paradigm, so they tried to customize the output to meet the requirements of various applications (Wu, 2003; Gao et al. [sent-97, score-0.404]
</p><p>29 (2009) presented a strategy to transfer annotated corpora between different segmentation standards in the hope of saving some expensive human labor. [sent-100, score-0.269]
</p><p>30 Then applications can make use ofthese structures according to their own convenience. [sent-102, score-0.237]
</p><p>31 Since the distinction between morphology and syntax in Chinese is somewhat blurred, our model for word structure parsing is integrated with constituent parsing. [sent-103, score-0.364]
</p><p>32 There has been many efforts to integrate Chinese word segmentation, part-of-speech tagging and parsing (Wu and Zixin, 1998; Zhou and Su, 2003; Luo, 2003; Fung et al. [sent-104, score-0.244]
</p><p>33 However, in these research all words were considered to be flat, and thus word structures were not parsed. [sent-106, score-0.304]
</p><p>34 Our parser output tree Figure 6(a), while Luo (2003) output tree (b), giving no hint to the structure of this word since the result is the same with a real flat word 䧢哫膝 ‘Los Angeles’(c). [sent-109, score-0.703]
</p><p>35 (a)  NN  (b)  NN  (c)  NR  ZZ NNf  NNf  NNf  NRf  碾碜  扨  碾碜扨  䧢哫膝  Figure 6: Difference between our output (a) of parsing the word 碾碜 扨 ‘olive oil’ and the output (b) of Luo  (2003). [sent-110, score-0.278]
</p><p>36 In (c) we have a true flat word, namely the location name 䧢哫膝 ‘Los Angeles’ . [sent-111, score-0.301]
</p><p>37 (2010) reported that a joint syntactic and semantic model improved the accuracy of both tasks, while Ng and Low (2004) showed it’s beneficial to integrate word segmentation and part-of-speech tagging into one model. [sent-114, score-0.386]
</p><p>38 Goldberg and Tsarfaty (2008) showed that a single model for morphological segmentation and syntactic parsing of Hebrew yielded an error reduction of 12% over the best pipelined models. [sent-118, score-0.414]
</p><p>39 Parsing of Chinese word structures can be reduced to the usual constituent parsing, for which there has been great progress in the past several years. [sent-120, score-0.437]
</p><p>40 Our generative model for unified word and phrase structure parsing is a direct adaptation of the model presented by Collins (2003). [sent-121, score-0.319]
</p><p>41 Many other approaches of constituent parsing also use this kind  1408 of head-driven generative models (Charniak, 1997; Bikel and Chiang, 2000) . [sent-122, score-0.311]
</p><p>42 3  The New Paradigm  敯  Given a raw Chinese sentence like 䤕 撓 䏓 喴 䋳 㢧 喓, a traditional word segmentation system would output some result like 䤕撓䏓 喴 䋳㢧 喓(‘Lin Zhihao’, ‘is’, ‘chief engineer’). [sent-123, score-0.371]
</p><p>43 In our new paradigm, the output should at least be a linear sequence of trees representing the structures of each word as in Figure 7. [sent-124, score-0.345]
</p><p>44 HHH JJ NN ZZ JJf  NNf  NNf  敯  䋳㢧  喓  Figure 7: Proposed output for the new Chinese word segmentation paradigm. [sent-128, score-0.335]
</p><p>45 the information of the proposed output for the new paradigm, our model’s output also includes higherlevel syntactic parsing results. [sent-140, score-0.246]
</p><p>46 1 Training Data We employ a statistical model to parse phrase and word structures as illustrated in Figure 8. [sent-142, score-0.377]
</p><p>47 Whether a word has structures or not is mostly context independent, so we only have to annotate each word once. [sent-153, score-0.409]
</p><p>48 Firstly, as we’ll see in Section 4, flat words and non-flat words will be modeled differently, thus it’s important to adapt the part-of-speech tags to facilitate this modeling strategy. [sent-155, score-0.257]
</p><p>49 So we change the POS tag for flat nouns to NNf, then during bottom up parsing, whenever a new constituent ending with ‘f’ is found, we can assign it a probability in a way different from a structured word or phrase. [sent-158, score-0.494]
</p><p>50 Secondly, we should record the head position of each word tree in accordance with the requirements of head driven parsing models. [sent-159, score-0.495]
</p><p>51 In passing, the readers should note the fact that in Figure 9, we have to add a parent labeled NN to the flat word 憞䠮䞎 ‘Iraq’ so as not to change the context-free rules contained inherently in the original treebank. [sent-162, score-0.324]
</p><p>52 With this model, the parsing problem is to search for the tree T∗ such that  T∗ = argTmaxPr(T,S)  (1)  The generation of S is defined in a top down fashion, which can be roughly summarized as follows. [sent-166, score-0.233]
</p><p>53 First, the lexicalized constituent structures are gen-  erated, then the lexicalized structure of each word is generated. [sent-167, score-0.6]
</p><p>54 1 Generation of Constituent Structures Each node in the constituent tree corresponds to a lexicalized context free rule P → Ln Ln−1  · · ·  L1HR1 R2  · · ·  Rm  (2)  where P, Li, Ri and H are lexicalized nonterminals and H is the head. [sent-171, score-0.319]
</p><p>55 By (3)–(5), the generation of word structures is exactly the same as that of ordinary phrase structures. [sent-179, score-0.423]
</p><p>56 Note that in our case, each word with structures is naturally lexicalized, since in the annotation process we have been careful to record the head position of each complex word. [sent-181, score-0.387]
</p><p>57 As an example, consider a word w = R(r) S(s) where R is the root part-of-speech headed by the word r, and S is the suffix part-of-speech headed by s. [sent-182, score-0.316]
</p><p>58 If the head of this word is its suffix, then we  can define the probability of w by Pr(w) = Pr(S, s) · Pr(R, r|S, s)  (6)  This is equivalent to saying that to generate w, we first generate its head S(s), then conditioned on this head, other components of this word are generated. [sent-183, score-0.387]
</p><p>59 In actual parsing, because a word always occurs in some contexts, the above probability should also be conditioned on these contexts, such as its parent and the parent’s head word. [sent-184, score-0.237]
</p><p>60 3 Generation of Flat Words We say a word is flat if it contains only one morpheme such as 憞䠮䞎 ‘Iraq’, or if it is a compound like 䝭䅵 ‘develop’ which does not have a productive component we are currently interested in. [sent-186, score-0.461]
</p><p>61 Depending on whether a flat word is known or not, their generative probabilities are computed also differently. [sent-187, score-0.373]
</p><p>62 Generation of flat words seen in training is 1410 trivial and deterministic since every phrase and word structure rules are lexicalized. [sent-188, score-0.398]
</p><p>63 However, the generation of unknown flat words is a different story. [sent-189, score-0.303]
</p><p>64 Note that the generation of w is only conditioned on its part-ofspeech p, ignoring the larger constituent or word in which w occurs. [sent-193, score-0.296]
</p><p>65 4 Summary of the Generative Story We make a brief summary of our generative story for the integrated morphological and syntactic parsing model. [sent-198, score-0.269]
</p><p>66 Firstly, because we are proposing a new paradigm for Chinese word segmentation, the input to the parser must be raw sentences by definition. [sent-202, score-0.292]
</p><p>67 Hence to use the bottom-up parser, we need a lexicon of all characters together with what roles they can play in a flat word. [sent-203, score-0.311]
</p><p>68 The role bNNf means the beginning of the flat label NNf, while eNNf stands for the end of the label NNf. [sent-206, score-0.257]
</p><p>69 Secondly, in the bottom-up parser for head driven models, whenever a new edge is found, we must assign it a probability and a head word. [sent-208, score-0.32]
</p><p>70 If the newly discovered constituent is a flat word (its label ends with ‘f’), then we set its head word to be the concatenation of all its child characters, i. [sent-209, score-0.607]
</p><p>71 On the other hand, if the new edge is a phrase or word with internal structures, the probability is set according to (2), while the head word is found with the appropriate head rules. [sent-213, score-0.477]
</p><p>72 This probability includes both word generation probabilities and constituent probabilities. [sent-215, score-0.283]
</p><p>73 First and foremost, we currently know of no other same effort in parsing the structures of Chinese words, and we have to annotate word structures by ourselves. [sent-217, score-0.708]
</p><p>74 Secondly, simply reporting the accuracy of labeled precision 1411 and recall is not very informative because our parser takes raw sentences as input, and its output includes a lot of easy cases like word segmentation and partof-speech tagging results. [sent-219, score-0.498]
</p><p>75 Despite these difficulties, we note that higherlevel constituent parsing results are still somewhat comparable with previous performance in parsing Penn Chinese Treebank, because constituent parsing does not involve word structures directly. [sent-220, score-0.992]
</p><p>76 Another observation is we can still evaluate Chinese word segmentation and partof-speech tagging accuracy, by reading off the corresponding result from parse trees. [sent-223, score-0.376]
</p><p>77 Again because we split the words with internal structures into their components, comparison with other systems should be viewed with that in mind. [sent-224, score-0.338]
</p><p>78 With this classification, we report our parser’s accuracy for phrase labels, which is approximately the accuracy of constituent parsing of Penn Chinese Treebank. [sent-229, score-0.301]
</p><p>79 We report our parser’s word segmentation accuracy based on the flat labels. [sent-230, score-0.551]
</p><p>80 This accuracy is in fact the joint accuracy of segmentation and part-of-speech tagging. [sent-231, score-0.271]
</p><p>81 Most importantly, we can report our parser’s accuracy in recovering word structures based on POS labels and flat labels, since word structures may contain only these two kinds of labels. [sent-232, score-0.91]
</p><p>82 The line labeled ‘Flat*’ is for unlabeled metrics of flat words, which is effectively the ordinary word segmentation accuracy. [sent-241, score-0.585]
</p><p>83 Besides, the result for flat labels compares favorably with the state of the art accuracy of about 93% F1 for joint word segmentation and part-of-speech tagging (Jiang et al. [sent-246, score-0.688]
</p><p>84 3%, though we should remem-  ber that the result concerns flat words only. [sent-252, score-0.257]
</p><p>85 Finally, we see the performance of word structure recovery is almost as good as the recognition of flat words. [sent-253, score-0.359]
</p><p>86 This means that parsing word structures accurately is possible with a generative model. [sent-254, score-0.482]
</p><p>87 7  Conclusion and Discussion  In this paper we proposed a new paradigm for Chinese word segmentation in which not only flat words were identified but words with structures were also parsed. [sent-266, score-0.898]
</p><p>88 With the progress in statistical parsing technology and the development of large scale treebanks, the time has now come for  this paradigm shift to happen. [sent-268, score-0.239]
</p><p>89 We believe such a new paradigm for word segmentation is linguistically justified and pragmatically beneficial to real world applications. [sent-269, score-0.404]
</p><p>90 We showed that word structures can be recovered with high precision, though there’s still much room for improvement, especially for higher level constituent parsing. [sent-270, score-0.476]
</p><p>91 , 1996) can be used in parsing word structures too. [sent-272, score-0.433]
</p><p>92 Finally, we must note that the use of flat labels such as “NNf” is less than ideal. [sent-277, score-0.302]
</p><p>93 The most important reason these labels are used is we want to compare the performance of our parser with previous results in constituent parsing, part-of-speech tagging and word segmentation, as we did in Section 6. [sent-278, score-0.372]
</p><p>94 The problem with this approach is that word structures  and phrase structures are then not treated in a truly unified way, and besides the 33 part-of-speech tags originally contained in Penn Chinese Treebank, another 33 tags ending with ‘f’ are introduced. [sent-279, score-0.58]
</p><p>95 Chinese word segmentation and named entity recognition: A pragmatic approach. [sent-352, score-0.294]
</p><p>96 A single generative model for joint morphological segmentation and syntactic parsing. [sent-356, score-0.378]
</p><p>97 A cascaded linear model for joint Chinese word  segmentation and part-of-speech tagging. [sent-361, score-0.338]
</p><p>98 Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging a case study. [sent-366, score-0.342]
</p><p>99 An error-driven word-character hybrid model for joint Chinese word segmentation and POS tagging. [sent-371, score-0.338]
</p><p>100 Joint word segmentation and POS tagging using a single perceptron. [sent-435, score-0.342]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nnf', 0.502), ('chinese', 0.29), ('flat', 0.257), ('structures', 0.237), ('segmentation', 0.227), ('jjf', 0.181), ('zz', 0.147), ('vvf', 0.141), ('constituent', 0.133), ('parsing', 0.129), ('nn', 0.127), ('hhh', 0.12), ('paradigm', 0.11), ('internal', 0.101), ('ci', 0.084), ('head', 0.083), ('andi', 0.08), ('parser', 0.079), ('suffix', 0.078), ('vv', 0.078), ('productive', 0.075), ('character', 0.074), ('pr', 0.07), ('president', 0.07), ('treebank', 0.068), ('word', 0.067), ('luo', 0.067), ('kruengkrai', 0.065), ('penn', 0.065), ('lexicalized', 0.064), ('prefix', 0.062), ('compound', 0.062), ('nff', 0.06), ('ngf', 0.06), ('morphological', 0.058), ('tree', 0.058), ('collins', 0.057), ('characters', 0.054), ('wwiitthh', 0.053), ('headed', 0.052), ('ctb', 0.05), ('conditioned', 0.05), ('generative', 0.049), ('sproat', 0.049), ('tagging', 0.048), ('jiang', 0.048), ('nr', 0.047), ('generation', 0.046), ('labels', 0.045), ('joint', 0.044), ('name', 0.044), ('sighan', 0.044), ('iraq', 0.044), ('prefixes', 0.042), ('standards', 0.042), ('vice', 0.042), ('xue', 0.041), ('personal', 0.041), ('output', 0.041), ('gao', 0.04), ('bnnf', 0.04), ('ennf', 0.04), ('shuren', 0.04), ('swim', 0.04), ('swimming', 0.04), ('telescopic', 0.04), ('wiithth', 0.04), ('zhongguo', 0.04), ('segmented', 0.039), ('phrase', 0.039), ('fung', 0.039), ('recovered', 0.039), ('annotate', 0.038), ('driven', 0.038), ('secondly', 0.038), ('li', 0.038), ('probability', 0.037), ('wu', 0.037), ('zhou', 0.037), ('requirements', 0.037), ('raw', 0.036), ('olive', 0.035), ('higherlevel', 0.035), ('structure', 0.035), ('jj', 0.034), ('boundaries', 0.034), ('ordinary', 0.034), ('parse', 0.034), ('story', 0.033), ('separable', 0.033), ('association', 0.032), ('yue', 0.032), ('people', 0.032), ('zhang', 0.032), ('noticed', 0.032), ('clark', 0.031), ('pos', 0.031), ('bikel', 0.031), ('tsinghua', 0.031), ('oil', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="241-tfidf-1" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>2 0.2160296 <a title="241-tfidf-2" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun</p><p>Abstract: The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efficient decoding very hard. As a result, effective high order features representing rich contexts are inconvenient to use. In this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. Our solution is a two step process. First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information. Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. The coarse-to-fine search scheme is effi- cient, while in the sub-word tagging step rich contextual features can be approximately derived. Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature.</p><p>3 0.20546448 <a title="241-tfidf-3" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>Author: Maoxi Li ; Chengqing Zong ; Hwee Tou Ng</p><p>Abstract: Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 1</p><p>4 0.15959799 <a title="241-tfidf-4" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>Author: Yoav Goldberg ; Michael Elhadad</p><p>Abstract: We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.</p><p>5 0.15231335 <a title="241-tfidf-5" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>6 0.14534263 <a title="241-tfidf-6" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>7 0.14143004 <a title="241-tfidf-7" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>8 0.13098803 <a title="241-tfidf-8" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>9 0.12503397 <a title="241-tfidf-9" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>10 0.12222336 <a title="241-tfidf-10" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>11 0.12143121 <a title="241-tfidf-11" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>12 0.11539107 <a title="241-tfidf-12" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>13 0.1107718 <a title="241-tfidf-13" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>14 0.10964978 <a title="241-tfidf-14" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>15 0.1085686 <a title="241-tfidf-15" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>16 0.10610307 <a title="241-tfidf-16" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>17 0.10394059 <a title="241-tfidf-17" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>18 0.10326721 <a title="241-tfidf-18" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>19 0.10219355 <a title="241-tfidf-19" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>20 0.10036406 <a title="241-tfidf-20" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.236), (1, -0.092), (2, -0.024), (3, -0.165), (4, -0.048), (5, -0.038), (6, 0.035), (7, 0.02), (8, 0.074), (9, 0.086), (10, -0.009), (11, 0.085), (12, -0.109), (13, -0.092), (14, 0.038), (15, 0.004), (16, 0.032), (17, -0.082), (18, 0.247), (19, 0.277), (20, 0.105), (21, 0.054), (22, -0.106), (23, 0.013), (24, 0.056), (25, -0.027), (26, -0.037), (27, 0.059), (28, 0.068), (29, 0.003), (30, 0.031), (31, 0.051), (32, 0.024), (33, 0.022), (34, -0.03), (35, -0.055), (36, 0.017), (37, -0.05), (38, -0.008), (39, 0.035), (40, 0.003), (41, 0.047), (42, 0.022), (43, 0.024), (44, -0.026), (45, 0.033), (46, -0.016), (47, 0.04), (48, -0.053), (49, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96393275 <a title="241-lsi-1" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>2 0.87468719 <a title="241-lsi-2" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>3 0.82687706 <a title="241-lsi-3" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Weiwei Sun</p><p>Abstract: The large combined search space of joint word segmentation and Part-of-Speech (POS) tagging makes efficient decoding very hard. As a result, effective high order features representing rich contexts are inconvenient to use. In this work, we propose a novel stacked subword model for this task, concerning both efficiency and effectiveness. Our solution is a two step process. First, one word-based segmenter, one character-based segmenter and one local character classifier are trained to produce coarse segmentation and POS information. Second, the outputs of the three predictors are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a fine-grained sub-word tagger. The coarse-to-fine search scheme is effi- cient, while in the sub-word tagging step rich contextual features can be approximately derived. Evaluation on the Penn Chinese Treebank shows that our model yields improvements over the best system reported in the literature.</p><p>4 0.74561465 <a title="241-lsi-4" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>Author: Yoav Goldberg ; Michael Elhadad</p><p>Abstract: We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.</p><p>5 0.74244535 <a title="241-lsi-5" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>Author: Daniel Hewlett ; Paul Cohen</p><p>Abstract: Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</p><p>6 0.69905329 <a title="241-lsi-6" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>7 0.68316126 <a title="241-lsi-7" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>8 0.65121722 <a title="241-lsi-8" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>9 0.61926478 <a title="241-lsi-9" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>10 0.61115187 <a title="241-lsi-10" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>11 0.5705505 <a title="241-lsi-11" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>12 0.53445613 <a title="241-lsi-12" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>13 0.53179264 <a title="241-lsi-13" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>14 0.51395929 <a title="241-lsi-14" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>15 0.50568795 <a title="241-lsi-15" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>16 0.47102612 <a title="241-lsi-16" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>17 0.46626425 <a title="241-lsi-17" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>18 0.45006037 <a title="241-lsi-18" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>19 0.42278224 <a title="241-lsi-19" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>20 0.42051819 <a title="241-lsi-20" href="./acl-2011-Partial_Parsing_from_Bitext_Projections.html">243 acl-2011-Partial Parsing from Bitext Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.019), (17, 0.072), (26, 0.029), (31, 0.018), (32, 0.012), (37, 0.101), (39, 0.127), (41, 0.056), (53, 0.017), (55, 0.026), (59, 0.036), (72, 0.029), (91, 0.187), (96, 0.161), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92777312 <a title="241-lda-1" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>Author: Daniel Hewlett ; Paul Cohen</p><p>Abstract: Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</p><p>2 0.92492115 <a title="241-lda-2" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>3 0.92232883 <a title="241-lda-3" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>Author: David Chiang ; Steve DeNeefe ; Michael Pust</p><p>Abstract: We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems.</p><p>4 0.91986209 <a title="241-lda-4" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>5 0.90449786 <a title="241-lda-5" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>Author: Chung-chi Huang ; Mei-hua Chen ; Shih-ting Huang ; Jason S. Chang</p><p>Abstract: We introduce a new method for learning to detect grammatical errors in learner’s writing and provide suggestions. The method involves parsing a reference corpus and inferring grammar patterns in the form of a sequence of content words, function words, and parts-of-speech (e.g., “play ~ role in Ving” and “look forward to Ving”). At runtime, the given passage submitted by the learner is matched using an extended Levenshtein algorithm against the set of pattern rules in order to detect errors and provide suggestions. We present a prototype implementation of the proposed method, EdIt, that can handle a broad range of errors. Promising results are illustrated with three common types of errors in nonnative writing. 1</p><p>6 0.90227616 <a title="241-lda-6" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>same-paper 7 0.88626826 <a title="241-lda-7" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>8 0.86590934 <a title="241-lda-8" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>9 0.8538307 <a title="241-lda-9" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>10 0.84711695 <a title="241-lda-10" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>11 0.84595674 <a title="241-lda-11" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>12 0.84204799 <a title="241-lda-12" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>13 0.84189922 <a title="241-lda-13" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>14 0.83957016 <a title="241-lda-14" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>15 0.83912742 <a title="241-lda-15" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>16 0.83888894 <a title="241-lda-16" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>17 0.83836651 <a title="241-lda-17" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>18 0.83505821 <a title="241-lda-18" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>19 0.83364516 <a title="241-lda-19" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>20 0.83222049 <a title="241-lda-20" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
