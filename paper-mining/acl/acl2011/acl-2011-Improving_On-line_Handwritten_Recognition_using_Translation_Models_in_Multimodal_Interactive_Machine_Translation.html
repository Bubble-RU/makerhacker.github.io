<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 acl-2011-Improving On-line Handwritten Recognition using Translation Models in Multimodal Interactive Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-168" href="#">acl2011-168</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>168 acl-2011-Improving On-line Handwritten Recognition using Translation Models in Multimodal Interactive Machine Translation</h1>
<br/><p>Source: <a title="acl-2011-168-pdf" href="http://aclweb.org/anthology//P/P11/P11-2068.pdf">pdf</a></p><p>Author: Vicent Alabau ; Alberto Sanchis ; Francisco Casacuberta</p><p>Abstract: In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work.</p><p>Reference: <a title="acl-2011-168-reference" href="../acl2011_reference/acl-2011-Improving_On-line_Handwritten_Recognition_using_Translation_Models_in_Multimodal_Interactive_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 e s  ,  ,  Abstract In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. [sent-3, score-0.259]
</p><p>2 The human expert interacts with the IMT system by partially correcting the errors of the system’s output. [sent-4, score-0.109]
</p><p>3 In this scenario, the interaction is typically performed using the keyboard and the mouse. [sent-7, score-0.125]
</p><p>4 In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. [sent-8, score-0.063]
</p><p>5 An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. [sent-9, score-0.246]
</p><p>6 Our HTR system improves previous approaches in two main aspects. [sent-10, score-0.031]
</p><p>7 First, HTR decoding is tightly coupled with the IMT system. [sent-11, score-0.16]
</p><p>8 Second, the language models proposed are context aware, in the sense that they take  into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. [sent-12, score-0.067]
</p><p>9 The proposed system achieves an important boost in performance with respect to previous work. [sent-13, score-0.058]
</p><p>10 1 Introduction Although current state-of-the-art machine translation (MT) systems have improved greatly in the last ten years, they are not able to provide the high quality results that are needed for industrial and business purposes. [sent-14, score-0.077]
</p><p>11 For that reason, a new interactive paradigm has emerged recently. [sent-15, score-0.105]
</p><p>12 , 2009; Koehn and Haddow, 2009) the 389 system goal is not to produce “perfect” translations in a completely automatic way, but to help the user build the translation with the least effort possible. [sent-18, score-0.222]
</p><p>13 First, the system outputs a translation hypothesis in the target language, which would correspond to the out-  es  put of fully automated MT system. [sent-22, score-0.232]
</p><p>14 Next, the user analyses the source sentence and the decoded hypothesis, and validates the longest error-free prefix ep finding the first error. [sent-23, score-0.522]
</p><p>15 The user, then, corrects the erroneous word by typing some keystrokes κ, and sends them along with ep to the system, as a new validated prefix ep, κ. [sent-24, score-0.421]
</p><p>16 With that information, the system is able to produce a new, hopefully improved, suffix that continues the previous validated prefix. [sent-25, score-0.141]
</p><p>17 This process is repeated until the user agrees with the quality of the resulting translation. [sent-26, score-0.114]
</p><p>18 es  fep,suys eterm es Figure 1: Diagram of a typical approach to IMT The usual way in which the user introduces the corrections κ is by means of the keyboard. [sent-27, score-0.278]
</p><p>19 How-  ever, other interaction modalities are also possible. [sent-28, score-0.057]
</p><p>20 For example, the use of speech interaction was studied in (Vidal et al. [sent-29, score-0.084]
</p><p>21 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 389–394, narios were proposed, where the user was expected to speak aloud parts of the current hypothesis and possibly one or more corrections. [sent-33, score-0.114]
</p><p>22 On-line HTR for interactive systems was first explored for interactive transcription of text images (Toselli et al. [sent-34, score-0.241]
</p><p>23 For both cases, the decoding of the on-line handwritten text is performed independently as a previous step of the suffix es decoding. [sent-38, score-0.431]
</p><p>24 , 2010) has been the first and sole approach to the use of on-line handwriting in IMT so far. [sent-40, score-0.072]
</p><p>25 The novelties of this paper with respect to previous work are summarised in the following items: • in previous formalisations of the problem, the iHnT pRre decoding amnadl itshaeti oIMnsT o decoding were performed in two steps. [sent-42, score-0.154]
</p><p>26 Here, a sound statistical formalisation is presented where both systems  are tightly coupled. [sent-43, score-0.032]
</p><p>27 • the use of specific language modelling for onltihnee HseT Rof decoding tnhgatu atageke m inotdoe alicncgofu onrt othneprevious validated prefix ep, κ, and the source sentence f. [sent-44, score-0.257]
</p><p>28 • additionally, a thorough study of the errors cadodmitmioitnteadll by ath teh HorToRug subsystem ifs presented. [sent-46, score-0.04]
</p><p>29 The remainder of this paper is organised as follows: The statistical framework for multimodal IMT and their alternatives will be studied in Sec. [sent-47, score-0.154]
</p><p>30 2  Multimodal IMT  In the traditional IMT scenario, the user interacts with the system through a series of corrections introduced with the keyboard. [sent-53, score-0.223]
</p><p>31 1, which indicates that, for a source sentence to be translated, several interactions between the user and the system  should be performed. [sent-55, score-0.145]
</p><p>32 In each interaction, the system produces the most probable suffix that completes the prefix formed by concatenating the longest correct prefix from the previous hypothesis ep and the  es  390 keyboard correction κ. [sent-56, score-0.753]
</p><p>33 In addition, the concatenation of them, (ep, κ, es), must be a translation of f. [sent-57, score-0.077]
</p><p>34 Statistically, this problem can be formulated as  es = argemsaxPr(es|ep,κ,f)  (1)  The multimodal IMT approach differs from Eq. [sent-58, score-0.252]
</p><p>35 1 in that the user introduces the correction using a touch-screen or an electronic pen, t. [sent-59, score-0.114]
</p><p>36 1 can be rewritten as  es = argemsaxPr(es|ep,t,f)  (2)  As t is a non-deterministic input (contrarily to κ), t needs to be decoded in a word d of the vocabulary. [sent-61, score-0.18]
</p><p>37 Thus, we must marginalise for every possible decoding:  es= argemsaxXdPr(es,d|ep,t,f)  (3)  Furthermore, by applying simple Bayes transformations and making reasonable assumptions,  es ≈ argemsaxmdaxPr(t|d) Pr(d|ep,f) Pr(es |ep, d, f)  (4)  The first term in Eq. [sent-62, score-0.124]
</p><p>38 4 is a morphological model and it can be approximated with hidden Markov models (HMM). [sent-63, score-0.062]
</p><p>39 Note tPhart( tdh|ee language model is conditioned to the longest correct prefix, just as a regular language model. [sent-67, score-0.04]
</p><p>40 Besides, it is also conditioned to the source sentence, since d should result of the translation of it. [sent-68, score-0.077]
</p><p>41 A typical session of the multimodal IMT is exemplified in Fig. [sent-69, score-0.128]
</p><p>42 First, the system starts with an empty prefix, so it proposes a full hypothesis. [sent-71, score-0.064]
</p><p>43 Then, the user corrects the first error, not, by writing on a touch-screen. [sent-73, score-0.178]
</p><p>44 Consequently, the user falls back to the keyboard and types i Next, s. [sent-75, score-0.182]
</p><p>45 the system proposes a new suffix, in which the first word, not, has been automatically corrected. [sent-76, score-0.064]
</p><p>46 The user amends at by writing the word which is correctly recognised by the HTR subsystem. [sent-77, score-0.196]
</p><p>47 Finally, as the new proposed suffix is correct, the process ends. [sent-78, score-0.064]
</p><p>48 SOURCE (f): TARGET (e):  Figure  2:  si alguna funci ´on no se encuentra disponible en su red if any feature is not available in your network  Example of a multimodal IMT session for translating  a Spanish sentence  f  from the Xerox corpus to an  English sentence e. [sent-79, score-0.128]
</p><p>49 If the decoding of the pen strokes dˆ is correct, it is displayed in boldface. [sent-80, score-0.139]
</p><p>50 In this case, the user amends the error with the keyboard κ (in typewriter). [sent-82, score-0.265]
</p><p>51 4, where the on-line HTR decoding was a separate problem from the IMT problem. [sent-86, score-0.077]
</p><p>52 First, is obtained,  dˆ  dˆ ≈ argdmaxPr(t|d) Pr(d|ep,f)  (5)  Then, the most likely suffix is obtained as in Eq 1, but taking as the corrected word instead of κ,  dˆ es= argemsaxPr(es|ep,dˆ,f)  (6)  Finally, in that work, the terms of Eq. [sent-89, score-0.064]
</p><p>53 4 can be tackled directly to perform a coupled decoding. [sent-93, score-0.081]
</p><p>54 The problem resides in how to model the constrained language model. [sent-94, score-0.037]
</p><p>55 A first approach is to drop either the ep or f terms from the probability. [sent-95, score-0.228]
</p><p>56 O(dn| ethe other hand, if ep is dropped, but the position of d in the target sentence i = |ep | + 1is kept, Pr(d| f,i) can breg mt soednetlelendce as a w |eord| +-ba 1se ids 391 translation model. [sent-97, score-0.355]
</p><p>57 Let us introduce a hidden variable j that accounts for a position of a word in f which is a candidate translation of d. [sent-98, score-0.077]
</p><p>58 Word dictionary probabilities can be directly estimated by IBM1 models. [sent-102, score-0.039]
</p><p>59 However, a more interesting set up than using language models or translation models alone is to com-  bine both models. [sent-106, score-0.131]
</p><p>60 The most formal under a probabilistic point of view is a linear interpolation of the models, Pr(d|ep, f) = αPr(d|ep) + (1 − α)Pr(d|f, i) (10) However, a common approach to combine models nowadays is log-linear interpolation (Berger et al. [sent-108, score-0.101]
</p><p>61 , 1998; Och and Ney, 2002), Pr(d|ep,f)  =  exp(PmλmZhm(d,f,ep))  (11)  λm being a scaling factor for model m, hm the logprobability of each model considered in the loglineal interpolation and Z a normalisation factor. [sent-110, score-0.081]
</p><p>62 Finally, to balance the absolute values of the morphological model, the constrained language model and the IMT model, these probabilities are combined in a log-linear manner regardless of the language modelling approach. [sent-111, score-0.122]
</p><p>63 Test perplexities for Spanish and English are 33 and 48, respectively. [sent-119, score-0.059]
</p><p>64 For on-line HTR, the on-line handwritten UNIPEN corpus (Guyon et al. [sent-120, score-0.166]
</p><p>65 The morphological models were represented by continuous density left-to-right character HMMs with Gaussian mixtures, as in speech recognition (Rabiner, 1989), but with variable number of states per character. [sent-122, score-0.138]
</p><p>66 Feature extraction consisted on speed and size normalisation of pen positions and velocities, resulting in a sequence of vectors of six features (Toselli et al. [sent-123, score-0.106]
</p><p>67 The simulation of user interaction was performed in the following way. [sent-125, score-0.201]
</p><p>68 First, the publicly available IMT decoder Thot (Ortiz-Mart ı´nez et al. [sent-126, score-0.03]
</p><p>69 , 2005) 1 was used to run an off-line simulation for keyboardbased IMT. [sent-127, score-0.03]
</p><p>70 As a result, a list of words the system 1http://sourceforge. [sent-128, score-0.031]
</p><p>71 (†) is an independent, contIenx tb unaware system yussteedm as † b)a issel ainne i. [sent-136, score-0.07]
</p><p>72 Supposedly, this is the list of words that the user would like to correct with handwriting. [sent-141, score-0.114]
</p><p>73 Then, from UNIPEN corpus, three users (separated from the training) were selected to simulate user interaction. [sent-142, score-0.114]
</p><p>74 For each user, the handwritten words were generated by concatenating random character instances from the user’s data to form a single stroke. [sent-143, score-0.196]
</p><p>75 Finally, the generated handwritten words of the three users were decoded using the corresponding constrained language model  with a state-of-the-art HMM decoder, iAtros (Luj a´nMares et al. [sent-144, score-0.259]
</p><p>76 the ratio between the errors committed by the on-line HTR decoder and the number ofhandwritten words introduced by the user. [sent-149, score-0.07]
</p><p>77 The log-linear and linear weights were obtained with the simplex algorithm (Nelder and Mead, 1965) to optimise the development set. [sent-152, score-0.038]
</p><p>78 On the one hand, (†) is a completely independent Oannd t ceon otnexet unaware system. [sent-155, score-0.039]
</p><p>79 That would be the equivalent to decode the handwritten text in a separate on-line HTR decoder. [sent-156, score-0.166]
</p><p>80 ) is the most similar model to the best system in (Alabau et al. [sent-159, score-0.031]
</p><p>81 This system is clearly outperformed by the proposed coupled ap-  proach. [sent-161, score-0.082]
</p><p>82 A summary of the alternatives to language mod-  Table 2: Summary of the CER results for various language modelling approaches. [sent-162, score-0.076]
</p><p>83 Hence, if the IMT has failed to predict the correct word because of poor language modelling that will affect on-line HTR decoding as well. [sent-170, score-0.153]
</p><p>84 In fact, although language perplexities for the test sets are quite low (33 for Spanish and 48 for English), perplexities accounting only erroneous words increase until 305 and 420, respectively. [sent-171, score-0.145]
</p><p>85 On the contrary, using IBM models provides a significant boost in performance. [sent-172, score-0.054]
</p><p>86 Although inverse dictionaries have a better vocabulary coverage (4. [sent-173, score-0.056]
</p><p>87 4% in Spanish), they tend to perform worse than their direct dictionary counterparts. [sent-177, score-0.039]
</p><p>88 Still, inverse IBM models perform better than the n-grams alone. [sent-178, score-0.083]
</p><p>89 Nevertheless, none of them outperformed the best system in Table 2. [sent-183, score-0.031]
</p><p>90 7% of the recognition errors were produced by punctuation and other symbols. [sent-187, score-0.089]
</p><p>91 To circumvent this problem, we proposed a contextual menu in (Alabau et al. [sent-188, score-0.039]
</p><p>92 With such menu, errors would have been reduced (best test result) to 4. [sent-190, score-0.04]
</p><p>93 3% of the errors, could be tackled using linguistic information from both source and target sentences. [sent-200, score-0.03]
</p><p>94 Finally, the rest of the errors were mostly due to one-to-three letter words, which is basically a problem of handwriting morphological modelling. [sent-201, score-0.147]
</p><p>95 4  Conclusions  In this paper we have described a specific on-line HTR system that can serve as an alternative interaction modality to IMT. [sent-202, score-0.123]
</p><p>96 We have shown that a tight in-  tegration of the HTR and IMT decoding process and the use of the available information can produce significant HTR error reductions. [sent-203, score-0.106]
</p><p>97 Finally, a study of the system’s errors has revealed the system weaknesses, and how they could be addressed in the future. [sent-204, score-0.071]
</p><p>98 Interactive assistance to human translators using statistical machine translation methods. [sent-276, score-0.077]
</p><p>99 Thot: a toolkit to train phrase-based statistical translation models. [sent-305, score-0.077]
</p><p>100 Maximum likelihood and discriminative training of direct translation models. [sent-315, score-0.077]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('imt', 0.559), ('htr', 0.331), ('alabau', 0.239), ('ep', 0.228), ('pr', 0.178), ('handwritten', 0.166), ('toselli', 0.163), ('multimodal', 0.128), ('es', 0.124), ('user', 0.114), ('spanish', 0.105), ('interactive', 0.105), ('barrachina', 0.096), ('vidal', 0.096), ('prefix', 0.084), ('luj', 0.081), ('schulmbergersema', 0.081), ('unipen', 0.081), ('translation', 0.077), ('decoding', 0.077), ('nez', 0.075), ('argemsaxpr', 0.072), ('guyon', 0.072), ('handwriting', 0.072), ('nelder', 0.072), ('pastor', 0.072), ('cer', 0.07), ('keyboard', 0.068), ('sanchis', 0.066), ('vicent', 0.066), ('suffix', 0.064), ('pen', 0.062), ('casacuberta', 0.062), ('xerox', 0.062), ('perplexities', 0.059), ('interaction', 0.057), ('decoded', 0.056), ('inverse', 0.056), ('amends', 0.054), ('iatros', 0.054), ('mois', 0.054), ('thot', 0.054), ('coupled', 0.051), ('mt', 0.05), ('modelling', 0.05), ('recognition', 0.049), ('tica', 0.048), ('berger', 0.047), ('ibm', 0.047), ('validated', 0.046), ('fj', 0.045), ('alejandro', 0.044), ('civera', 0.044), ('normalisation', 0.044), ('decoupled', 0.044), ('della', 0.042), ('longest', 0.04), ('corrections', 0.04), ('errors', 0.04), ('dictionary', 0.039), ('alberto', 0.039), ('isabelle', 0.039), ('unaware', 0.039), ('menu', 0.039), ('foster', 0.038), ('interacts', 0.038), ('val', 0.038), ('simplex', 0.038), ('constrained', 0.037), ('interpolation', 0.037), ('corrects', 0.036), ('morphological', 0.035), ('modality', 0.035), ('spain', 0.034), ('proposes', 0.033), ('oov', 0.032), ('tightly', 0.032), ('pietra', 0.031), ('system', 0.031), ('mart', 0.031), ('boldface', 0.031), ('transcription', 0.031), ('tackled', 0.03), ('concatenating', 0.03), ('simulation', 0.03), ('decoder', 0.03), ('papineni', 0.029), ('error', 0.029), ('dropped', 0.028), ('writing', 0.028), ('contrary', 0.028), ('models', 0.027), ('boost', 0.027), ('erroneous', 0.027), ('speech', 0.027), ('alternatives', 0.026), ('xj', 0.026), ('vs', 0.026), ('failed', 0.026), ('summit', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="168-tfidf-1" href="./acl-2011-Improving_On-line_Handwritten_Recognition_using_Translation_Models_in_Multimodal_Interactive_Machine_Translation.html">168 acl-2011-Improving On-line Handwritten Recognition using Translation Models in Multimodal Interactive Machine Translation</a></p>
<p>Author: Vicent Alabau ; Alberto Sanchis ; Francisco Casacuberta</p><p>Abstract: In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work.</p><p>2 0.6043191 <a title="168-tfidf-2" href="./acl-2011-An_Interactive_Machine_Translation_System_with_Online_Learning.html">41 acl-2011-An Interactive Machine Translation System with Online Learning</a></p>
<p>Author: Daniel Ortiz-Martinez ; Luis A. Leiva ; Vicent Alabau ; Ismael Garcia-Varea ; Francisco Casacuberta</p><p>Abstract: State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework, where the knowledge of a human translator is combined with the MT system. We present a statistical IMT system able to learn from user feedback by means of the application of online learning techniques. These techniques allow the MT system to update the parameters of the underlying models in real time. According to empirical results, our system outperforms the results of conventional IMT systems. To the best of our knowledge, this online learning capability has never been provided by previous IMT systems. Our IMT system is implemented in C++, JavaScript, and ActionScript; and is publicly available on the Web.</p><p>3 0.074563377 <a title="168-tfidf-3" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks.</p><p>4 0.066187061 <a title="168-tfidf-4" href="./acl-2011-Translating_from_Morphologically_Complex_Languages%3A_A_Paraphrase-Based_Approach.html">310 acl-2011-Translating from Morphologically Complex Languages: A Paraphrase-Based Approach</a></p>
<p>Author: Preslav Nakov ; Hwee Tou Ng</p><p>Abstract: We propose a novel approach to translating from a morphologically complex language. Unlike previous research, which has targeted word inflections and concatenations, we focus on the pairwise relationship between morphologically related words, which we treat as potential paraphrases and handle using paraphrasing techniques at the word, phrase, and sentence level. An important advantage of this framework is that it can cope with derivational morphology, which has so far remained largely beyond the capabilities of statistical machine translation systems. Our experiments translating from Malay, whose morphology is mostly derivational, into English show signif- icant improvements over rivaling approaches based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens).</p><p>5 0.065598808 <a title="168-tfidf-5" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao</p><p>Abstract: To address the parse error issue for tree-tostring translation, this paper proposes a similarity-based decoding generation (SDG) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. Our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models. 1</p><p>6 0.064243525 <a title="168-tfidf-6" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>7 0.061324392 <a title="168-tfidf-7" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>8 0.061143976 <a title="168-tfidf-8" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>9 0.05966489 <a title="168-tfidf-9" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>10 0.05827583 <a title="168-tfidf-10" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>11 0.057858508 <a title="168-tfidf-11" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>12 0.056826629 <a title="168-tfidf-12" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>13 0.056427605 <a title="168-tfidf-13" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>14 0.055569742 <a title="168-tfidf-14" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>15 0.055262364 <a title="168-tfidf-15" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>16 0.054160897 <a title="168-tfidf-16" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>17 0.053686157 <a title="168-tfidf-17" href="./acl-2011-Using_Deep_Morphology_to_Improve_Automatic_Error_Detection_in_Arabic_Handwriting_Recognition.html">329 acl-2011-Using Deep Morphology to Improve Automatic Error Detection in Arabic Handwriting Recognition</a></p>
<p>18 0.053379867 <a title="168-tfidf-18" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>19 0.052313939 <a title="168-tfidf-19" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>20 0.051775027 <a title="168-tfidf-20" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.139), (1, -0.066), (2, 0.051), (3, 0.063), (4, -0.042), (5, 0.023), (6, 0.014), (7, -0.035), (8, 0.042), (9, 0.066), (10, -0.069), (11, -0.05), (12, -0.004), (13, -0.056), (14, -0.002), (15, 0.031), (16, -0.043), (17, -0.05), (18, -0.0), (19, -0.033), (20, 0.073), (21, 0.048), (22, 0.168), (23, 0.026), (24, -0.016), (25, -0.337), (26, 0.086), (27, -0.021), (28, -0.162), (29, -0.128), (30, -0.005), (31, 0.41), (32, -0.218), (33, -0.282), (34, -0.177), (35, -0.052), (36, 0.113), (37, 0.04), (38, 0.019), (39, -0.062), (40, -0.149), (41, 0.021), (42, -0.092), (43, -0.05), (44, -0.078), (45, 0.063), (46, 0.026), (47, 0.157), (48, -0.037), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92416883 <a title="168-lsi-1" href="./acl-2011-Improving_On-line_Handwritten_Recognition_using_Translation_Models_in_Multimodal_Interactive_Machine_Translation.html">168 acl-2011-Improving On-line Handwritten Recognition using Translation Models in Multimodal Interactive Machine Translation</a></p>
<p>Author: Vicent Alabau ; Alberto Sanchis ; Francisco Casacuberta</p><p>Abstract: In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work.</p><p>2 0.90811455 <a title="168-lsi-2" href="./acl-2011-An_Interactive_Machine_Translation_System_with_Online_Learning.html">41 acl-2011-An Interactive Machine Translation System with Online Learning</a></p>
<p>Author: Daniel Ortiz-Martinez ; Luis A. Leiva ; Vicent Alabau ; Ismael Garcia-Varea ; Francisco Casacuberta</p><p>Abstract: State-of-the-art Machine Translation (MT) systems are still far from being perfect. An alternative is the so-called Interactive Machine Translation (IMT) framework, where the knowledge of a human translator is combined with the MT system. We present a statistical IMT system able to learn from user feedback by means of the application of online learning techniques. These techniques allow the MT system to update the parameters of the underlying models in real time. According to empirical results, our system outperforms the results of conventional IMT systems. To the best of our knowledge, this online learning capability has never been provided by previous IMT systems. Our IMT system is implemented in C++, JavaScript, and ActionScript; and is publicly available on the Web.</p><p>3 0.40001971 <a title="168-lsi-3" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>Author: Kenneth Hild ; Umut Orhan ; Deniz Erdogmus ; Brian Roark ; Barry Oken ; Shalini Purwar ; Hooman Nezamfar ; Melanie Fried-Oken</p><p>Abstract: Event related potentials (ERP) corresponding to stimuli in electroencephalography (EEG) can be used to detect the intent of a person for brain computer interfaces (BCI). This paradigm is widely used to build letter-byletter text input systems using BCI. Nevertheless using a BCI-typewriter depending only on EEG responses will not be sufficiently accurate for single-trial operation in general, and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed. Hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed. In this demonstration we will present a BCI system for typing that integrates a stochastic language model with ERP classification to achieve speedups, via the rapid serial visual presentation (RSVP) paradigm.</p><p>4 0.32362968 <a title="168-lsi-4" href="./acl-2011-Blast%3A_A_Tool_for_Error_Analysis_of_Machine_Translation_Output.html">62 acl-2011-Blast: A Tool for Error Analysis of Machine Translation Output</a></p>
<p>Author: Sara Stymne</p><p>Abstract: We present BLAST, an open source tool for error analysis of machine translation (MT) output. We believe that error analysis, i.e., to identify and classify MT errors, should be an integral part ofMT development, since it gives a qualitative view, which is not obtained by standard evaluation methods. BLAST can aid MT researchers and users in this process, by providing an easy-to-use graphical user interface. It is designed to be flexible, and can be used with any MT system, language pair, and error typology. The annotation task can be aided by highlighting similarities with a reference translation.</p><p>5 0.28809845 <a title="168-lsi-5" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>Author: Matthew R. Scott ; Xiaohua Liu ; Ming Zhou ; Microsoft Engkoo Team</p><p>Abstract: This paper presents Engkoo 1, a system for exploring and learning language. It is built primarily by mining translation knowledge from billions of web pages - using the Internet to catch language in motion. Currently Engkoo is built for Chinese users who are learning English; however the technology itself is language independent and can be extended in the future. At a system level, Engkoo is an application platform that supports a multitude of NLP technologies such as cross language retrieval, alignment, sentence classification, and statistical machine translation. The data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web. Specifically, web pages that contain both Chinese and English are discovered and analyzed for parallelism, extracted and formulated into clear term definitions and sample sentences. This approach allows us to build perhaps the world’s largest lexicon linking both Chinese and English together - at the same time covering the most up-to-date terms as captured by the net.</p><p>6 0.26780227 <a title="168-lsi-6" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>7 0.2635729 <a title="168-lsi-7" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>8 0.25368679 <a title="168-lsi-8" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>9 0.25229546 <a title="168-lsi-9" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>10 0.25075829 <a title="168-lsi-10" href="./acl-2011-Translating_from_Morphologically_Complex_Languages%3A_A_Paraphrase-Based_Approach.html">310 acl-2011-Translating from Morphologically Complex Languages: A Paraphrase-Based Approach</a></p>
<p>11 0.24773185 <a title="168-lsi-11" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>12 0.2474882 <a title="168-lsi-12" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>13 0.24241205 <a title="168-lsi-13" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>14 0.23058385 <a title="168-lsi-14" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>15 0.22939514 <a title="168-lsi-15" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>16 0.2260986 <a title="168-lsi-16" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>17 0.22408383 <a title="168-lsi-17" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>18 0.22139385 <a title="168-lsi-18" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<p>19 0.22073042 <a title="168-lsi-19" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>20 0.21472806 <a title="168-lsi-20" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.016), (17, 0.022), (37, 0.049), (39, 0.013), (41, 0.041), (55, 0.023), (59, 0.029), (72, 0.023), (91, 0.016), (96, 0.68)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99921042 <a title="168-lda-1" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>Author: Anselmo Penas ; Alvaro Rodrigo</p><p>Abstract: There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.</p><p>2 0.99838352 <a title="168-lda-2" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>Author: Maoxi Li ; Chengqing Zong ; Hwee Tou Ng</p><p>Abstract: Word is usually adopted as the smallest unit in most tasks of Chinese language processing. However, for automatic evaluation of the quality of Chinese translation output when translating from other languages, either a word-level approach or a character-level approach is possible. So far, there has been no detailed study to compare the correlations of these two approaches with human assessment. In this paper, we compare word-level metrics with characterlevel metrics on the submitted output of English-to-Chinese translation systems in the IWSLT’08 CT-EC and NIST’08 EC tasks. Our experimental results reveal that character-level metrics correlate with human assessment better than word-level metrics. Our analysis suggests several key reasons behind this finding. 1</p><p>same-paper 3 0.99768007 <a title="168-lda-3" href="./acl-2011-Improving_On-line_Handwritten_Recognition_using_Translation_Models_in_Multimodal_Interactive_Machine_Translation.html">168 acl-2011-Improving On-line Handwritten Recognition using Translation Models in Multimodal Interactive Machine Translation</a></p>
<p>Author: Vicent Alabau ; Alberto Sanchis ; Francisco Casacuberta</p><p>Abstract: In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work.</p><p>4 0.99746656 <a title="168-lda-4" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>Author: Daniel Emilio Beck</p><p>Abstract: In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</p><p>5 0.99582171 <a title="168-lda-5" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>6 0.99537051 <a title="168-lda-6" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>7 0.99443477 <a title="168-lda-7" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>8 0.99114716 <a title="168-lda-8" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>9 0.98491865 <a title="168-lda-9" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>10 0.97400939 <a title="168-lda-10" href="./acl-2011-An_Interactive_Machine_Translation_System_with_Online_Learning.html">41 acl-2011-An Interactive Machine Translation System with Online Learning</a></p>
<p>11 0.96998972 <a title="168-lda-11" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>12 0.95988506 <a title="168-lda-12" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>13 0.95259815 <a title="168-lda-13" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>14 0.94620812 <a title="168-lda-14" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>15 0.94516408 <a title="168-lda-15" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>16 0.94511199 <a title="168-lda-16" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>17 0.9405272 <a title="168-lda-17" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>18 0.93963903 <a title="168-lda-18" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>19 0.93920612 <a title="168-lda-19" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>20 0.93690753 <a title="168-lda-20" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
