<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-57" href="#">acl2011-57</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2011-57-pdf" href="http://aclweb.org/anthology//P/P11/P11-2032.pdf">pdf</a></p><p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>Reference: <a title="acl-2011-57-reference" href="../acl2011_reference/acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bayesian Word Alignment for Statistical Machine Translation  Mermer1,2 1BILGEM  Cos ¸kun  TUBITAK Gebze 41470 Kocaeli, Turkey co s kun @ uekae tubit ak gov  . [sent-1, score-0.068]
</p><p>2 Abstract In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). [sent-3, score-0.499]
</p><p>3 We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. [sent-4, score-0.659]
</p><p>4 We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2. [sent-5, score-0.153]
</p><p>5 We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. [sent-7, score-0.06]
</p><p>6 t r  1 Introduction Word alignment is a crucial early step in the training of most statistical machine translation (SMT) systems, in which the estimated alignments are used for constraining the set ofcandidates in phrase/grammar extraction (Koehn et al. [sent-9, score-0.602]
</p><p>7 State-of-the-art word alignment models, such as IBM Models (Brown et al. [sent-12, score-0.256]
</p><p>8 , 1996), and the jointly-trained symmetric HMM (Liang et al. [sent-14, score-0.059]
</p><p>9 , word translation probabilities) that need to be estimated in addition to the desired hidden alignment variables. [sent-17, score-0.421]
</p><p>10 The most common method of inference in such models is expectation-maximization (EM) (Dempster et al. [sent-18, score-0.153]
</p><p>11 Bogazici University Bebek 34342 Istanbul, Turkey  murat . [sent-22, score-0.088]
</p><p>12 In essence, the alignment distribution obtained via EM takes into account only the most likely point in the parameter space, but does not consider contributions from other points. [sent-28, score-0.374]
</p><p>13 Zhao and Xing (2006) note that the parameter estimation (for which they use variational EM) suffers from data sparsity and use symmetric Dirichlet priors, but they find the MAP solution. [sent-30, score-0.204]
</p><p>14 , 2009) and learning phrase alignments directly (DeNero et al. [sent-33, score-0.22]
</p><p>15 Word alignment learning problem was addressed jointly with segmentation learning in Xu et al. [sent-35, score-0.256]
</p><p>16 The former two works place nonparametric priors (also known as cache models) on the parameters and utilize Gibbs sampling. [sent-38, score-0.118]
</p><p>17 However, alignment inference in neither of these works is exactly Bayesian since the alignments are updated by running GIZA++ (Xu et al. [sent-39, score-0.629]
</p><p>18 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 182–187, Chung and Gildea (2009) apply a sparse Dirichlet prior on the multinomial parameters to prevent overfitting. [sent-44, score-0.163]
</p><p>19 They use variational Bayes for inference, but they do not investigate the effect of Bayesian inference to word alignment in isolation. [sent-45, score-0.454]
</p><p>20 Even though they report substantial reductions in alignment error rate, the translation BLEU scores do not improve. [sent-47, score-0.382]
</p><p>21 Our approach in this paper is fully Bayesian in which the alignment probabilities are inferred by integrating over all possible parameter values assuming an intuitive, sparse prior. [sent-48, score-0.409]
</p><p>22 We evaluate the inferred alignments in terms of the end-toend translation performance, where we show the results with a variety of input data to illustrate the general applicability of the proposed technique. [sent-50, score-0.346]
</p><p>23 To our knowledge, this is the first work to directly investigate the effects of Bayesian alignment inference on translation performance. [sent-51, score-0.535]
</p><p>24 2  Bayesian Inference with IBM Model 1  ×  Given a sentence-aligned parallel corpus (E, F), let ei (fj) denote the i-th (j-th) source (target)1 word in e (f), which in turn consists of I words and (J) denotes the s-th sentence in E (F). [sent-52, score-0.115]
</p><p>25 2 Each source sentence is also hypothesized to have an additional imaginary “null” word e0. [sent-53, score-0.114]
</p><p>26 Also let VE (VF) denote the size of the observed source (target) vocabulary. [sent-54, score-0.077]
</p><p>27 , 1993), each target word 1We use the “source” and “target” labels following the generative process, in which E generates F (cf. [sent-56, score-0.055]
</p><p>28 2Dependence of the sentence-level variables e, f, I, J (and a and n, which are introduced later) on the sentence index s should be understood even though not explicitly indicated for notational simplicity. [sent-59, score-0.037]
</p><p>29 183  fj is associated with a hidden alignment variable aj whose value ranges over the word positions in the corresponding source sentence. [sent-60, score-0.577]
</p><p>30 The set of alignments for a sentence (corpus) is denoted by a (A). [sent-61, score-0.22]
</p><p>31 The model parameters consist of a VE VF table T of word translation probabilities ×suc Vh that te,f = P(f| e) . [sent-62, score-0.126]
</p><p>32 stribution of the Model-1 variables is given by the following generative model3 :  P(E,F,A;T) =  YP(e)P(a|e)P(f|a,e;T)  (1)  Ys  =Ys(IP +(e 1))JjY=J1teaj,fj  (2)  In the proposed Bayesian setting, we treat T as a random variable with a prior P(T). [sent-64, score-0.098]
</p><p>33 Since the distribution over {te,f} in (4) is iPn the exponential family, specifically being a nm (u4l)ti inso iPnm tihale distribution, we cihlyo,o spsee tchifei conjugate prior, in this case the Dirichlet distribution, for computational convenience. [sent-66, score-0.106]
</p><p>34 A sparse prior favors 3We omit P(J|e) since both J and e are observed and so this Wterme o dmoeits Pn(otJ |afef)ec sti ntchee i bnofethre Jnc aen odf ehiad dreen o bvsaeriravebdles a. [sent-69, score-0.163]
</p><p>35 n distributions that peak at a single target word and penalizes flatter translation distributions, even for rare words. [sent-70, score-0.278]
</p><p>36 This choice addresses the well-known problem in the IBM Models, and more severely in Model 1, in which rare words act as “garbage collectors” (Och and Ney, 2003) and get assigned excessively large number of word alignments. [sent-71, score-0.097]
</p><p>37 Then we obtain the joint distribution of all (observed + hidden) variables as:  P(E, F, A, T; Θ) = P(T; Θ) P(E, F, A|T) (5) where Θ = Θ1 · · · ΘVE . [sent-72, score-0.106]
</p><p>38 To infer the posterior distribution of the alignments, we use Gibbs sampling (Geman and Geman, 1984). [sent-73, score-0.179]
</p><p>39 One possible method is to derive the Gibbs sampler from P(E, F, A, T; Θ) obtained in (5) and sample the unknowns A and T in turn, resulting in an explicit Gibbs sampler. [sent-74, score-0.162]
</p><p>40 In this work, we marginalize out T by:  P(E,F,A;Θ) =ZTP(E,F,A,T;Θ)  (6)  and obtain a collapsed Gibbs sampler, which samples only the alignment variables. [sent-75, score-0.297]
</p><p>41 Using P(E, F, A; Θ) obtained in (6), the Gibbs sampling formula for the individual alignments is derived as:4 P(aj = i|E, F, A¬j; Θ)  =PfV=F1NNe¬ei¬,ij,jf j++ θPei,fVf=Fj1θei,f  (7)  where the superscPript ¬j denotePs the exclusion of twheh ecreurr tehent svuaplueers ocrfi aj. [sent-76, score-0.367]
</p><p>42 Once the Gibbs sampler is deemed to have converged after B burn-in iterations, we collect M samples of A with L iterations in-between5 to estimate P(A|E, F). [sent-82, score-0.203]
</p><p>43 , 2003), we select for each aj the most frequent value in the M collected samples. [sent-84, score-0.113]
</p><p>44 4The derivation is quite standard and similar to other Dirichlet-multinomial Gibbs sampler derivations, e. [sent-85, score-0.162]
</p><p>45 3  Experimental Setup  For Turkish↔English experiments, we used the 2F0orK- Tsuenrtkeinshce↔ tEranvgelli hdoem xapiner iBmTeEnCts, ,dwa taese uts (Kikui et al. [sent-90, score-0.037]
</p><p>46 For Czech↔English, we used the 95K-sentence news commentary parallel corpus hfreo m95 tKh-es eWntMenTc esh naerweds task8 for training, news2008 set for development, news2009 set for testing, and the 438M-word English and 81. [sent-92, score-0.135]
</p><p>47 7M-word Czech monolingual news corpora for additional language model (LM) training. [sent-93, score-0.061]
</p><p>48 All language models are 4-gram in the travel domain experiments and 5-gram in the news domain experiments. [sent-95, score-0.13]
</p><p>49 For each language pair, we trained standard phrase-based SMT systems in both directions (including alignment symmetrization and log-linear model tuning) using Moses (Koehn et al. [sent-96, score-0.293]
</p><p>50 To obtain word alignments, we used the accompanying Perl code for Bayesian inference and 6International Workshop on Spoken Language Translation. [sent-99, score-0.221]
</p><p>51 For each translation task, we report two EM estimates, obtained after 5 and 80 iterations (EM-5 and EM-80), respectively; and three Gibbs sampling estimates, two of which were initialized with those two EM Viterbi alignments (GS-5 and GS-80) and a third was initialized naively9 (GS-N). [sent-116, score-0.532]
</p><p>52 4  Results  Table 2 compares the BLEU scores of Bayesian inference and EM estimation. [sent-120, score-0.153]
</p><p>53 99 (in English-to-Turkish) BLEU points in travel domain and from 0. [sent-124, score-0.069]
</p><p>54 Compared to the state-of-the-art IBM Model 4, the Bayesian Model 1 is better in all travel domain tasks and is comparable or better in the news domain. [sent-127, score-0.13]
</p><p>55 Fertility of a source word is defined as the number of target words aligned to it. [sent-128, score-0.132]
</p><p>56 Table 3 shows the distribution of fertilities in alignments obtained from different methods. [sent-129, score-0.401]
</p><p>57 5282K 5 GEMMS--8480 2489289421893 109842490149 6 Table 3: Distribution ofinferred alignment fertilities. [sent-147, score-0.256]
</p><p>58 The four blocks of rows from top to bottom correspond to (in order) the total number of source tokens, source tokens with fertilities in the range 4–7, source tokens with fertilities higher than 7, and the maximum observed fertility. [sent-148, score-0.455]
</p><p>59 The first language listed is the source in alignment (Section 2). [sent-149, score-0.333]
</p><p>60 aTtehse “neuxmcebsesriv oef” d ailsitginnmct word-pairs yin ≥du 8c)ed by an alignment has been recently proposed as an objective function for word alignment (Bodrumlu et al. [sent-154, score-0.512]
</p><p>61 Table 4 shows that the proposed inference method substantially reduces the alignment dictionary size, in most cases by more than 50%. [sent-157, score-0.409]
</p><p>62 5  Conclusion  We developed a Gibbs sampling-based Bayesian inference method for IBM Model 1 word alignments and showed that it outperforms EM estimation in terms of translation BLEU scores across several language pairs, data sizes and domains. [sent-158, score-0.593]
</p><p>63 As a result of this increase, Bayesian Model 1 alignments perform close to or better than the state-of-the-art IBM  10The GIZA++ implementation of Model 4 artificially limits  fertility parameter values to at most nine. [sent-159, score-0.438]
</p><p>64 The proposed method learns a compact, sparse translation distribution, overcoming the wellknown “garbage collection” problem of rare words in EM-estimated current models. [sent-161, score-0.251]
</p><p>65 Scalable inference and training of context-rich syntactic translation models. [sent-208, score-0.279]
</p><p>66 Bayesian inference for PCFGs via Markov chain Monte Carlo. [sent-221, score-0.153]
</p><p>67 Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. [sent-276, score-0.242]
</p><p>68 A fast fertility hidden Markov model for word alignment using MCMC. [sent-280, score-0.464]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bayesian', 0.311), ('gibbs', 0.294), ('alignment', 0.256), ('em', 0.232), ('alignments', 0.22), ('ibm', 0.174), ('fertility', 0.169), ('sampler', 0.162), ('inference', 0.153), ('translation', 0.126), ('bleu', 0.119), ('aj', 0.113), ('fertilities', 0.112), ('sampling', 0.11), ('dirichlet', 0.109), ('geman', 0.097), ('fj', 0.092), ('murat', 0.088), ('bodrumlu', 0.084), ('egm', 0.084), ('source', 0.077), ('smt', 0.076), ('priors', 0.076), ('garbage', 0.074), ('zhao', 0.071), ('chung', 0.071), ('travel', 0.069), ('ys', 0.069), ('distribution', 0.069), ('kun', 0.068), ('accompanying', 0.068), ('turkey', 0.068), ('nguyen', 0.066), ('sparse', 0.065), ('afp', 0.064), ('kikui', 0.064), ('vf', 0.064), ('ip', 0.062), ('news', 0.061), ('prior', 0.061), ('rare', 0.06), ('czech', 0.059), ('symmetric', 0.059), ('vogel', 0.059), ('koehn', 0.056), ('iwslt', 0.056), ('target', 0.055), ('ml', 0.054), ('sujith', 0.054), ('turkish', 0.053), ('estimation', 0.051), ('sara', 0.051), ('gildea', 0.051), ('ve', 0.051), ('xu', 0.051), ('giza', 0.051), ('parameter', 0.049), ('hmm', 0.049), ('monte', 0.047), ('och', 0.047), ('variational', 0.045), ('dempster', 0.045), ('chiang', 0.044), ('goldwater', 0.043), ('blunsom', 0.043), ('graehl', 0.043), ('sizes', 0.043), ('sharon', 0.043), ('estimates', 0.042), ('nonparametric', 0.042), ('knight', 0.042), ('denero', 0.041), ('samples', 0.041), ('te', 0.041), ('hermann', 0.041), ('griffiths', 0.04), ('resnik', 0.04), ('fully', 0.039), ('hidden', 0.039), ('initialized', 0.038), ('ei', 0.038), ('ney', 0.038), ('variables', 0.037), ('multinomial', 0.037), ('wli', 0.037), ('shaojun', 0.037), ('symmetrization', 0.037), ('admixture', 0.037), ('clar', 0.037), ('esh', 0.037), ('excessively', 0.037), ('flatter', 0.037), ('hfreo', 0.037), ('imaginary', 0.037), ('inso', 0.037), ('ntchee', 0.037), ('seiichi', 0.037), ('twheh', 0.037), ('uts', 0.037), ('wme', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="57-tfidf-1" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>2 0.2635743 <a title="57-tfidf-2" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>3 0.23724531 <a title="57-tfidf-3" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>4 0.22559758 <a title="57-tfidf-4" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>Author: Graham Neubig ; Taro Watanabe ; Eiichiro Sumita ; Shinsuke Mori ; Tatsuya Kawahara</p><p>Abstract: We present an unsupervised model for joint phrase alignment and extraction using nonparametric Bayesian methods and inversion transduction grammars (ITGs). The key contribution is that phrases of many granularities are included directly in the model through the use of a novel formulation that memorizes phrases generated not only by terminal, but also non-terminal symbols. This allows for a completely probabilistic model that is able to create a phrase table that achieves competitive accuracy on phrase-based machine translation tasks directly from unaligned sentence pairs. Experiments on several language pairs demonstrate that the proposed model matches the accuracy of traditional two-step word alignment/phrase extraction approach while reducing the phrase table to a fraction of the original size.</p><p>5 0.2179828 <a title="57-tfidf-5" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>6 0.21590884 <a title="57-tfidf-6" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>7 0.21189313 <a title="57-tfidf-7" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>8 0.20683251 <a title="57-tfidf-8" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>9 0.19203082 <a title="57-tfidf-9" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>10 0.17468284 <a title="57-tfidf-10" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>11 0.16966942 <a title="57-tfidf-11" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>12 0.15044482 <a title="57-tfidf-12" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>13 0.14948952 <a title="57-tfidf-13" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>14 0.14677142 <a title="57-tfidf-14" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>15 0.14455903 <a title="57-tfidf-15" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>16 0.14312243 <a title="57-tfidf-16" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>17 0.14003621 <a title="57-tfidf-17" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>18 0.13560978 <a title="57-tfidf-18" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>19 0.13409676 <a title="57-tfidf-19" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>20 0.13331166 <a title="57-tfidf-20" href="./acl-2011-Nonparametric_Bayesian_Machine_Transliteration_with_Synchronous_Adaptor_Grammars.html">232 acl-2011-Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.306), (1, -0.212), (2, 0.135), (3, 0.173), (4, 0.073), (5, -0.002), (6, 0.034), (7, 0.078), (8, -0.035), (9, 0.179), (10, 0.167), (11, 0.129), (12, 0.075), (13, 0.176), (14, -0.102), (15, 0.067), (16, 0.048), (17, 0.03), (18, -0.084), (19, 0.034), (20, -0.015), (21, 0.091), (22, 0.028), (23, 0.034), (24, 0.118), (25, 0.067), (26, 0.003), (27, -0.002), (28, -0.017), (29, -0.014), (30, -0.043), (31, -0.005), (32, -0.062), (33, -0.005), (34, 0.035), (35, 0.003), (36, 0.058), (37, -0.058), (38, 0.041), (39, 0.02), (40, -0.025), (41, 0.009), (42, 0.056), (43, 0.002), (44, -0.117), (45, 0.045), (46, -0.042), (47, 0.05), (48, 0.007), (49, -0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96947551 <a title="57-lsi-1" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>2 0.86538249 <a title="57-lsi-2" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>3 0.83953714 <a title="57-lsi-3" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>Author: Mohit Bansal ; Chris Quirk ; Robert Moore</p><p>Abstract: We propose a principled and efficient phraseto-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semiMarkov model, word-to-phrase and phraseto-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include “gappy phrases” (such as French ne ? pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime.</p><p>4 0.82719052 <a title="57-lsi-4" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>5 0.81938791 <a title="57-lsi-5" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>Author: Kristina Toutanova ; Michel Galley</p><p>Abstract: Contrary to popular belief, we show that the optimal parameters for IBM Model 1 are not unique. We demonstrate that, for a large class of words, IBM Model 1 is indifferent among a continuum of ways to allocate probability mass to their translations. We study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate.</p><p>6 0.80819911 <a title="57-lsi-6" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>7 0.75233024 <a title="57-lsi-7" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>8 0.75180525 <a title="57-lsi-8" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>9 0.70453316 <a title="57-lsi-9" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>10 0.70013541 <a title="57-lsi-10" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>11 0.67824489 <a title="57-lsi-11" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>12 0.67757171 <a title="57-lsi-12" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>13 0.67095488 <a title="57-lsi-13" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>14 0.6597771 <a title="57-lsi-14" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>15 0.61534971 <a title="57-lsi-15" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>16 0.59986877 <a title="57-lsi-16" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>17 0.57511693 <a title="57-lsi-17" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>18 0.56735438 <a title="57-lsi-18" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>19 0.56689894 <a title="57-lsi-19" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>20 0.55620378 <a title="57-lsi-20" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.023), (17, 0.062), (26, 0.016), (37, 0.082), (39, 0.045), (41, 0.123), (53, 0.011), (55, 0.037), (59, 0.063), (60, 0.121), (72, 0.044), (91, 0.026), (96, 0.277), (97, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97091877 <a title="57-lda-1" href="./acl-2011-Even_the_Abstract_have_Color%3A_Consensus_in_Word-Colour_Associations.html">120 acl-2011-Even the Abstract have Color: Consensus in Word-Colour Associations</a></p>
<p>Author: Saif Mohammad</p><p>Abstract: Colour is a key component in the successful dissemination of information. Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complemented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures concept–colour associations. We present a method to create a large word–colour association lexicon by crowdsourcing. A wordchoice question was used to obtain sense-level annotations and to ensure data quality. We focus especially on abstract concepts and emotions to show that even they tend to have strong colour associations. Thus, using the right colours can not only improve semantic coherence, but also inspire the desired emotional response.</p><p>2 0.95650011 <a title="57-lda-2" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>Author: Xiaojun Wan</p><p>Abstract: Cross-language document summarization is defined as the task of producing a summary in a target language (e.g. Chinese) for a set of documents in a source language (e.g. English). Existing methods for addressing this task make use of either the information from the original documents in the source language or the information from the translated documents in the target language. In this study, we propose to use the bilingual information from both the source and translated documents for this task. Two summarization methods (SimFusion and CoRank) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction. Experimental results on the DUC2001 dataset with manually translated reference Chinese summaries show the effectiveness of the proposed methods. 1</p><p>same-paper 3 0.9506259 <a title="57-lda-3" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>4 0.92886245 <a title="57-lda-4" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>Author: Sujith Ravi ; Kevin Knight</p><p>Abstract: In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from nonparallel text.</p><p>5 0.92176849 <a title="57-lda-5" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>Author: Ziqi Wang ; Gu Xu ; Hang Li ; Ming Zhang</p><p>Abstract: This paper proposes a new method for approximate string search, specifically candidate generation in spelling error correction, which is a task as follows. Given a misspelled word, the system finds words in a dictionary, which are most “similar” to the misspelled word. The paper proposes a probabilistic approach to the task, which is both accurate and efficient. The approach includes the use of a log linear model, a method for training the model, and an algorithm for finding the top k candidates. The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word. The learning method employs the criterion in candidate generation as loss function. The retrieval algorithm is efficient and is guaranteed to find the optimal k candidates. Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings.</p><p>6 0.92109078 <a title="57-lda-6" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>7 0.91993219 <a title="57-lda-7" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>8 0.91801953 <a title="57-lda-8" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>9 0.91798192 <a title="57-lda-9" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>10 0.91687608 <a title="57-lda-10" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>11 0.9152931 <a title="57-lda-11" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>12 0.91421604 <a title="57-lda-12" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>13 0.91329026 <a title="57-lda-13" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>14 0.91257513 <a title="57-lda-14" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>15 0.91069257 <a title="57-lda-15" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>16 0.91025198 <a title="57-lda-16" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>17 0.91006386 <a title="57-lda-17" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>18 0.90994108 <a title="57-lda-18" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>19 0.90987492 <a title="57-lda-19" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>20 0.90954232 <a title="57-lda-20" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
