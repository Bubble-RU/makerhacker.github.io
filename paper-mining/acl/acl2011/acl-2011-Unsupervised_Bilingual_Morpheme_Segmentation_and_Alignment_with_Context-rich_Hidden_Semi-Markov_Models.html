<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-318" href="#">acl2011-318</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</h1>
<br/><p>Source: <a title="acl-2011-318-pdf" href="http://aclweb.org/anthology//P/P11/P11-1090.pdf">pdf</a></p><p>Author: Jason Naradowsky ; Kristina Toutanova</p><p>Abstract: This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</p><p>Reference: <a title="acl-2011-318-reference" href="../acl2011_reference/acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 narad@ c s umas s edu Abstract This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. [sent-3, score-1.427]
</p><p>2 It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. [sent-5, score-1.074]
</p><p>3 Our model outperforms a competitive word alignment system in alignment quality. [sent-6, score-0.517]
</p><p>4 Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models  on three Arabic and Hebrew datasets. [sent-7, score-0.524]
</p><p>5 The word alignment models of modern MT systems attempt to capture p(ei |fj), the probability that token ei is a translation o|ff fj. [sent-9, score-0.311]
</p><p>6 In comparison to a language which isn’t morphologically productive on adjectives, the alignment model must observe nine times as much data (assuming uniform distribution of the inflected forms)  to yield a comparable statistic. [sent-17, score-0.329]
</p><p>7 To address this issue we propose an alternative to word alignment: morpheme alignment, an alignment that operates over the smallest meaningful subsequences of words. [sent-20, score-0.838]
</p><p>8 h‫ه‬tes‫س‬yr~ّ‫در‬dwa‫ي‬ynt A‫ن‬o‫^أ‬tena‫ن‬wc‫و‬hd‫يد‬y‫ر‬rhi‫ي‬my PRN  VB  INF  VB  PRN  Here dark lines indicate the more stem-focused  alignment  strategy of a traditional word or phrasal alignment model, while thin lines indicate a more fine-grained alignment across morphemes. [sent-24, score-0.672]
</p><p>9 In the alignment between English and Bulgarian (a) the morpheme-specific alignment reduces sparsity in the adjective and noun (red flowers) by isolating the stems from their inflected forms. [sent-25, score-0.461]
</p><p>10 In the following sections we describe an unsupervised dynamic graphical model approach to monolingual morphological segmentation and bilingual morpheme alignment using a linguistically motivated statistical model. [sent-30, score-1.521]
</p><p>11 In a bilingual setting, the model relies on morpho-syntactic and lexical source-side information (part-of-speech, morpho-  logical segmentation, dependency analysis) while learning a morpheme segmentation over the target language. [sent-31, score-1.065]
</p><p>12 Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. [sent-34, score-0.37]
</p><p>13 Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER). [sent-35, score-0.575]
</p><p>14 2  Model  Our model defines the probability of a target language sequence of words (each consisting of a sequence of morphemes), and alignment from target to source morphemes, given a source language sequence of words (each consisting of a sequence of morphemes). [sent-37, score-0.561]
</p><p>15 An example morpheme segmentation and alignment of phrases in English-Arabic and EnglishBulgarian is shown in Figure 1. [sent-38, score-1.087]
</p><p>16 In our task setting, the words of the source and target language as well as the morpheme segmentation of the source (English) language are given. [sent-39, score-1.083]
</p><p>17 The morpheme segmentation of the target language and the alignments between source and target morphemes are hidden. [sent-40, score-1.436]
</p><p>18 Our model is derived from the hidden-markov model for word alignment (Vogel et al. [sent-44, score-0.362]
</p><p>19 (iv) we enrich the hidden state space of the model to encode morpheme types {prefix,suffix,stem}, in adcdoitdioen m mtoo morpheme alignment ,asnudf segmentation aidn-formation. [sent-49, score-1.854]
</p><p>20 Each possible morphological segmentation and alignment for a given sentence pair can be described by the following random variables: Let µ1µ2. [sent-51, score-0.618]
</p><p>21 µI denote I morphemes in the segmentation of the target sentence. [sent-54, score-0.678]
</p><p>22 bI denote Bernoulli variables indicating whether there is a word boundary after 897 morpheme µi. [sent-62, score-0.905]
</p><p>23 , wbT denote Bernoulli variables indicating whether there is a word boundary after the corresponding target character. [sent-70, score-0.384]
</p><p>24 sT denote Bernoulli segmentation variables indicating whether there is a morpheme boundary after the corresponding character. [sent-76, score-1.154]
</p><p>25 The values of the hidden segmentation variables s together with the values of the observed c and wb variables uniquely define the values of the morpheme variables µiand the word boundary variables bi. [sent-77, score-1.53]
</p><p>26 Naturally we enforce the constraint that a given word boundary wbt = 1 entails a segmentation boundary st = 1. [sent-78, score-0.628]
</p><p>27 We denote the observed source language morphemes by e1 . [sent-81, score-0.395]
</p><p>28 The last part of the hidden model state represents the alignment between target and source morphemes and the type of target morphemes. [sent-86, score-0.904]
</p><p>29 I indicate a factored state where ai represents one of the J source words (or NULL) and ti represents one of the three morpheme types {prefix,suffix,stem}. [sent-90, score-0.865]
</p><p>30 ai is the source morpheme aligned utof µi asntedm ti i. [sent-91, score-0.863]
</p><p>31 By simplifying the factors we can recover several previously used models for monolingual segmentation and bilingual joint segmentation and alignment. [sent-95, score-0.753]
</p><p>32 When the source sentence is assumed to be empty (and thus contains no morphemes to align to) our model turns into a monolingual morpheme segmentation model, which we show exceeds the performance of previous state-of-the-art models. [sent-97, score-1.394]
</p><p>33 1 Morpheme Translation Model In the model equation, PT denotes the morpheme translation probability. [sent-100, score-0.698]
</p><p>34 The standard dependence on the aligned source morpheme is represented as a de-  pendence on the state tai and the whole annotated source sentence e. [sent-101, score-1.03]
</p><p>35 When most context is used, there is a bigram dependency of target language morphemes as well as dependence on two previous boundary variables and dependence on the aligned source morpheme eai as well as its POS tag. [sent-103, score-1.473]
</p><p>36 As an example, suppose we estimate the morpheme translation probability as PT(µi |eai , ti). [sent-105, score-0.638]
</p><p>37 Note how our explicit treatment of word boundary variables bi allows us to use a higher order dependence on these variables. [sent-109, score-0.386]
</p><p>38 If word boundaries are treated as morphemes on their own, we would need to have a four-gram model on target morphemes to represent this dependency which we are now representing using only a bigram model on hidden morphemes. [sent-110, score-0.962]
</p><p>39 However, they can be powerful predictors of morpheme segments (by for example, indicating that common prefixes follow word boundaries, or that common suffixes precede them). [sent-114, score-0.867]
</p><p>40 , 2009) uses word boundaries as observed left and  right context features, and Morfessor (Creutz and Lagus, 2007) includes boundaries as special boundary symbols which can inform about the morpheme state of a morpheme (but not its identity). [sent-116, score-1.596]
</p><p>41 Our model includes a special generative process for boundaries which is conditioned not only on the previous morpheme state but also the previous two morphemes and other boundaries. [sent-117, score-1.113]
</p><p>42 It also allows the estimation of likelihood that particular morphemes are in the beginning/middle/end of words. [sent-120, score-0.3]
</p><p>43 Through the included factored state variable tai word boundaries can also inform about the likelihood of a morpheme aligned to a source word of a particular pos tag to end a word. [sent-121, score-1.086]
</p><p>44 1 Traditional distortion models represent P(aj |aj−1 , e), the probability of an alignment given the previous alignment, to bias the model away from placing large distances between the aligned tokens of consecutively sequenced tokens. [sent-126, score-0.525]
</p><p>45 In addition to modeling a larger state space to also predict morpheme types, we extend this model by using a special log-linear model form which allows the integration of rich morpho-syntactic context. [sent-127, score-0.797]
</p><p>46 The special log-linear form allows the inclusion of features targeted at learning the transitions among morpheme types and the transitions between corresponding source morphemes. [sent-133, score-0.783]
</p><p>47 The example is focussed on the features firing for the transition from the Bulgarian suffix te aligned to the first English morpheme µi−1 = te, ti−1=suffix, ai−1=1, to the Bulgarian root tsvet  aligned to the third English morpheme µi = tsvet, ti=root, ai=3. [sent-135, score-1.519]
</p><p>48 The first feature is the absolute difference between ai and ai−1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrasetranslation models (Koehn, 2004). [sent-136, score-0.347]
</p><p>49 The alignment positions ai are defined as indices of the aligned source morphemes. [sent-137, score-0.411]
</p><p>50 Looking at both kinds of distance is useful to capture the intuition that consecutive morphemes in the same target word should prefer to have a higher proximity of their aligned source words, as compared to consecutive morphemes which are not part of the same target word. [sent-140, score-0.922]
</p><p>51 The feature SAME TARGET WORD indicates whether the two consecu1To reduce complexity of exposition we have omitted the final transition to a special state beyond the source sentence end after the last target morpheme. [sent-142, score-0.288]
</p><p>52 This feature is not useful on its own because it does not distinguish between different alignment possibilities for tai, but is useful in conjunction with other features to differentiate the transition behaviors within and across target words. [sent-146, score-0.354]
</p><p>53 4 Length Penalty Following (Chung and Gildea, 2009) and (Liang and Klein, 2009) we use an exponential length penalty on morpheme lengths to bias the model away from the maximum likelihood under-segmentation solution. [sent-151, score-0.753]
</p><p>54 The form of the penalty is: LP(|µi|) = e|µ1i|lp Here lp is a hyper-parameter indicating the power that the morpheme length is raised to. [sent-152, score-0.765]
</p><p>55 In the M-step we re-estimate the model parameters (using LBFGS in the M-step for the distortion model and using count interpolation for the translation and word-boundary models). [sent-158, score-0.28]
</p><p>56 The computation of expectations in the E-step is of the same order as an order two semi-markov chain model using hidden state labels of cardinality (J 3 = number of source morphemes times num(bJer × ×of 3 target morpheme types). [sent-159, score-1.21]
</p><p>57 hTehmee running tuimmeof the forward and backward dynamic programming passes is T l2 (3J)2, where T is the length of tphaes target Tse ×nte lnc×e (i3nJ characters, J is the number of source morphemes, and l is the maximum morpheme length. [sent-160, score-0.773]
</p><p>58 To reduce the running time of the model we limit the space of considered morpheme boundaries as follows: Given the target side of the corpus, we derive a list of K most frequent prefixes and suffixes using a simple trie-based method proposed by (Schone and Jurafsky, 2000). [sent-163, score-1.028]
</p><p>59 2 After we determine a list of allowed prefixes and suffixes we restrict our model to allow only segmentations of the form : ((p*)r(s*))+ where p and s belong to the allowed prefixes and suffixes and r can match any substring. [sent-164, score-0.602]
</p><p>60 We determine the number of prefixes and suffixes to consider using the maximum recall achievable by limiting the segmentation points in this way. [sent-165, score-0.523]
</p><p>61 Restricting the allowable segmentations in this way not only improves the speed of inference but also leads to improvements in segmentation accuracy. [sent-166, score-0.354]
</p><p>62 We use an unmodified version of this corpus for the purpose of comparing morphological segmentation accuracy. [sent-172, score-0.403]
</p><p>63 For evaluating morpheme alignment accuracy, we have also augmented the English/Arabic subset of the corpus with a gold standard alignment between morphemes. [sent-173, score-1.026]
</p><p>64 Here morphological segmentations were obtained using the previously-annotated gold standard Arabic morphological segmentation, while the English was preprocessed with a morphological analyzer and then further hand annotated with corrections by two native speakers. [sent-174, score-0.433]
</p><p>65 Additionally, we evaluate monolingual segmentation models on the full Arabic Treebank (ATB), also used for unsupervised morpheme segmentation in (Poon et al. [sent-176, score-1.333]
</p><p>66 We focus first on a monolingual setting, where the source sentence aligned to each target sentence is empty. [sent-180, score-0.314]
</p><p>67 Unigram Model with Length Penalty The first model we study is the unigram monolingual segmentation model using an exponential length penalty as proposed by (Liang and Klein, 2009; Chung and Gildea, 2009), which has been shown to be quite accurate. [sent-181, score-0.635]
</p><p>68 µI) = (1−θ) θPT(µi)LP( |µi|) This model can be (almoQst) recovered as a special case of our full model, iQf we drop the transition and word boundary probabilities, do not model morpheme types, and use no conditioning for the morpheme translation model. [sent-186, score-1.72]
</p><p>69 The only parameter not present in our model is the probability θ of generating a morpheme as opposed to stopping to generate morphemes (with probability 1− θ). [sent-187, score-0.956]
</p><p>70 3 Morpheme Type Models The next model we consider is similar to the unigram model with penalty, but introduces the use of the hidden ta states which indicate only morpheme types in the monolingual setting. [sent-195, score-0.958]
</p><p>71 We use the ta states and test different configurations to derive the best set of features that can be used in the distortion model utilizing these states, and the morpheme translation model. [sent-196, score-0.875]
</p><p>72 This model is allowed to use word boundary information for conditioning (because word boundaries are observed), but does not include the PB predictive word boundary distribution. [sent-198, score-0.598]
</p><p>73 Full Model with Word Boundaries Finally we consider our full monolingual model which also includes the distribution predicting word boundary variables bi. [sent-199, score-0.469]
</p><p>74 We initialize this model with the morpheme trans3For the S&B; Arabic dataset, we selected to use seven prefixes and seven suffixes, which correspond to maximum achievable recall of 95. [sent-202, score-0.84]
</p><p>75 For our main results we use the automatically derived list of prefixes and suffixes to limit segmentation points. [sent-210, score-0.497]
</p><p>76 For comparison, we also report the results achieved by models that do not limit the segmentation points in this way. [sent-212, score-0.303]
</p><p>77 When the segmentation points are not limited, its performance is much worse. [sent-214, score-0.276]
</p><p>78 The introduction of hidden morpheme states in Dict-HMMP-basic gives substantial improvement on Arabic and does not change results much on the other datasets. [sent-215, score-0.696]
</p><p>79 Model-HMMP is also the first unconstrained model in our sequence to approach or surpass previous state-of-the-art segmentation performance. [sent-218, score-0.383]
</p><p>80 The best configuration of this model uses the same distortion model for all languages: using the morph state transition and boundary features. [sent-223, score-0.493]
</p><p>81 We find the dis4Note that the inclusion of states in HMMP-basic only serves to provide a different distribution over the number of morphemes in a word, so it is interesting it can have a positive impact. [sent-229, score-0.397]
</p><p>82 Results from models with a small, automatically-derived list  of possible prefixes and suffixes are labeled as ”Dict-” followed by the model name. [sent-259, score-0.308]
</p><p>83 However, in both the Arabic and Hebrew S&B; tasks we find that a tendency to over-segment certain characters off of their correct morphemes and on to other frequently occurring, yet incorrect, particles is actually the cause of many of these isolated errors. [sent-261, score-0.3]
</p><p>84 We use the morpheme-level annotation of the S&B; EnglishArabic dataset and project the morpheme alignments to word alignments. [sent-271, score-0.669]
</p><p>85 In addition to reporting alignment error rate for different segmentation models, we report their morphological segmentation F1. [sent-275, score-0.894]
</p><p>86 We can recover Model-1 similarly to Model-UP, except now every morpheme is conditioned on an aligned source morpheme. [sent-280, score-0.763]
</p><p>87 Our full bilingual model outperforms Model-1 in both AER and segmentation F1. [sent-281, score-0.422]
</p><p>88 phemes on source morphemes only, uses the boundary model with conditioning on number of morphemes in the word, aligned source part-of-speech, and type of target morpheme. [sent-295, score-1.161]
</p><p>89 The distortion model uses both morpheme and word-based absolute distortion, binned distortion, morpheme types of states, and aligned source-part-of-speech tags. [sent-296, score-1.472]
</p><p>90 Our best model for Arabic outperforms WDHMM in word alignment error rate. [sent-297, score-0.302]
</p><p>91 For Hebrew, the best model uses a similar boundary model configuration but a simpler uniform transition distortion distribution. [sent-298, score-0.472]
</p><p>92 Note that the bilingual models perform worse than the monolingual ones in segmentation F1. [sent-299, score-0.455]
</p><p>93 This finding is in line with previous work showing that the best segmentation for MT does not necessarily agree with a particular linguistic convention about what morphemes should contain (Chung and Gildea, 2009; Habash and Sadat, 2006), but contradicts other results (Snyder and Barzilay, 2008). [sent-300, score-0.576]
</p><p>94 5  Related Work  This work is most closely related to the unsupervised tokenization and alignment models of Chung and Gildea (2009), Xu et al. [sent-305, score-0.321]
</p><p>95 Snyder and Barzi903 lay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. [sent-309, score-0.289]
</p><p>96 Our work is partly inspired by that work and attempts to automate both the morpho-syntactic alignment and morphological analysis tasks. [sent-313, score-0.342]
</p><p>97 6  Conclusion  We have described an unsupervised model for morpheme segmentation and alignment based on Hidden Semi-Markov Models. [sent-314, score-1.183]
</p><p>98 On the task of monolingual morphological segmentation it produces a new state-of-the-art level on three datasets. [sent-316, score-0.497]
</p><p>99 The model shows quantitative im-  provements in both word segmentation and word alignment, but its true potential lies in its finergrained interpretation of word alignment, which will hopefully yield improvements in translation quality. [sent-317, score-0.459]
</p><p>100 Using word-dependent transition models in HMM based word alignment for statistical machine translation. [sent-347, score-0.333]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('morpheme', 0.596), ('morphemes', 0.3), ('segmentation', 0.276), ('alignment', 0.215), ('arabic', 0.195), ('boundary', 0.144), ('tai', 0.136), ('hebrew', 0.136), ('morphological', 0.127), ('distortion', 0.118), ('suffixes', 0.113), ('prefixes', 0.108), ('bulgarian', 0.1), ('penalty', 0.097), ('monolingual', 0.094), ('chung', 0.094), ('wdhmm', 0.093), ('bi', 0.089), ('variables', 0.088), ('aligned', 0.077), ('boundaries', 0.076), ('target', 0.075), ('wb', 0.071), ('ti', 0.071), ('conditioning', 0.069), ('source', 0.068), ('snyder', 0.068), ('gildea', 0.067), ('transition', 0.064), ('hidden', 0.064), ('model', 0.06), ('bilingual', 0.058), ('tsvet', 0.056), ('poon', 0.055), ('segmentations', 0.052), ('ai', 0.051), ('eai', 0.049), ('atb', 0.049), ('lp', 0.049), ('unigram', 0.048), ('state', 0.047), ('unconstrained', 0.047), ('alignments', 0.046), ('bernoulli', 0.045), ('tokenization', 0.043), ('creutz', 0.043), ('translation', 0.042), ('pd', 0.041), ('dependence', 0.038), ('wbt', 0.037), ('states', 0.036), ('unsupervised', 0.036), ('aer', 0.035), ('pt', 0.034), ('dynamic', 0.034), ('special', 0.034), ('inclusion', 0.033), ('habash', 0.033), ('lagus', 0.033), ('morfessor', 0.033), ('prn', 0.033), ('factored', 0.032), ('sparsity', 0.031), ('pb', 0.031), ('barzilay', 0.03), ('schone', 0.03), ('sadat', 0.03), ('suffix', 0.029), ('hmm', 0.029), ('full', 0.028), ('distribution', 0.028), ('distances', 0.028), ('models', 0.027), ('word', 0.027), ('depiction', 0.027), ('denote', 0.027), ('additionally', 0.027), ('transitions', 0.026), ('null', 0.026), ('vogel', 0.026), ('amherst', 0.026), ('lets', 0.026), ('allowable', 0.026), ('achievable', 0.026), ('uniform', 0.026), ('red', 0.025), ('kristina', 0.025), ('graphical', 0.025), ('character', 0.025), ('seven', 0.025), ('binned', 0.025), ('te', 0.024), ('listing', 0.024), ('allowed', 0.024), ('hierarchical', 0.023), ('configurations', 0.023), ('morphology', 0.023), ('indicating', 0.023), ('recover', 0.022), ('ney', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999857 <a title="318-tfidf-1" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>Author: Jason Naradowsky ; Kristina Toutanova</p><p>Abstract: This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</p><p>2 0.25800809 <a title="318-tfidf-2" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>Author: Ann Clifton ; Anoop Sarkar</p><p>Abstract: This paper extends the training and tuning regime for phrase-based statistical machine translation to obtain fluent translations into morphologically complex languages (we build an English to Finnish translation system) . Our methods use unsupervised morphology induction. Unlike previous work we focus on morphologically productive phrase pairs – our decoder can combine morphemes across phrase boundaries. Morphemes in the target language may not have a corresponding morpheme or word in the source language. Therefore, we propose a novel combination of post-processing morphology prediction with morpheme-based translation. We show, using both automatic evaluation scores and linguistically motivated analyses of the output, that our methods outperform previously proposed ones and pro- vide the best known results on the EnglishFinnish Europarl translation task. Our methods are mostly language independent, so they should improve translation into other target languages with complex morphology. 1 Translation and Morphology Languages with rich morphological systems present significant hurdles for statistical machine translation (SMT) , most notably data sparsity, source-target asymmetry, and problems with automatic evaluation. In this work, we propose to address the problem of morphological complexity in an Englishto-Finnish MT task within a phrase-based translation framework. We focus on unsupervised segmentation methods to derive the morphological information supplied to the MT model in order to provide coverage on very large datasets and for languages with few hand-annotated 32 resources. In fact, in our experiments, unsupervised morphology always outperforms the use of a hand-built morphological analyzer. Rather than focusing on a few linguistically motivated aspects of Finnish morphological behaviour, we develop techniques for handling morphological complexity in general. We chose Finnish as our target language for this work, because it exemplifies many of the problems morphologically complex languages present for SMT. Among all the languages in the Europarl data-set, Finnish is the most difficult language to translate from and into, as was demonstrated in the MT Summit shared task (Koehn, 2005) . Another reason is the current lack of knowledge about how to apply SMT successfully to agglutinative languages like Turkish or Finnish. Our main contributions are: 1) the introduction of the notion of segmented translation where we explicitly allow phrase pairs that can end with a dangling morpheme, which can connect with other morphemes as part of the translation process, and 2) the use of a fully segmented translation model in combination with a post-processing morpheme prediction system, using unsupervised morphology induction. Both of these approaches beat the state of the art on the English-Finnish translation task. Morphology can express both content and function categories, and our experiments show that it is important to use morphology both within the translation model (for morphology with content) and outside it (for morphology contributing to fluency) . Automatic evaluation measures for MT, BLEU (Papineni et al., 2002), WER (Word Error Rate) and PER (Position Independent Word Error Rate) use the word as the basic unit rather than morphemes. In a word comProce dPinogrstla ofn tdh,e O 4r9etghon A,n Jnu nael 1 M9-e 2t4i,n2g 0 o1f1 t.he ?c A2s0s1o1ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 32–42, prised of multiple morphemes, getting even a single morpheme wrong means the entire word is wrong. In addition to standard MT evaluation measures, we perform a detailed linguistic analysis of the output. Our proposed approaches are significantly better than the state of the art, achieving the highest reported BLEU scores on the English-Finnish Europarl version 3 data-set. Our linguistic analysis shows that our models have fewer morpho-syntactic errors compared to the word-based baseline. 2 2.1 Models Baseline Models We set up three baseline models for comparison in this work. The first is a basic wordbased model (called Baseline in the results) ; we trained this on the original unsegmented version of the text. Our second baseline is a factored translation model (Koehn and Hoang, 2007) (called Factored) , which used as factors the word, “stem” 1 and suffix. These are derived from the same unsupervised segmentation model used in other experiments. The results (Table 3) show that a factored model was unable to match the scores of a simple wordbased baseline. We hypothesize that this may be an inherently difficult representational form for a language with the degree of morphological complexity found in Finnish. Because the morphology generation must be precomputed, for languages with a high degree of morphological complexity, the combinatorial explosion makes it unmanageable to capture the full range of morphological productivity. In addition, because the morphological variants are generated on a per-word basis within a given phrase, it excludes productive morphological combination across phrase boundaries and makes it impossible for the model to take into account any longdistance dependencies between morphemes. We conclude from this result that it may be more useful for an agglutinative language to use morphology beyond the confines of the phrasal unit, and condition its generation on more than just the local target stem. In order to compare the 1see Section 2.2. 33 performance of unsupervised segmentation for translation, our third baseline is a segmented translation model based on a supervised segmentation model (called Sup) , using the hand-built Omorfi morphological analyzer (Pirinen and Listenmaa, 2007) , which provided slightly higher BLEU scores than the word-based baseline. 2.2 Segmented Translation For segmented translation models, it cannot be taken for granted that greater linguistic accuracy in segmentation yields improved translation (Chang et al. , 2008) . Rather, the goal in segmentation for translation is instead to maximize the amount of lexical content-carrying morphology, while generalizing over the information not helpful for improving the translation model. We therefore trained several different segmentation models, considering factors of granularity, coverage, and source-target symmetry. We performed unsupervised segmentation of the target data, using Morfessor (Creutz and Lagus, 2005) and Paramor (Monson, 2008) , two top systems from the Morpho Challenge 2008 (their combined output was the Morpho Challenge winner) . However, translation models based upon either Paramor alone or the combined systems output could not match the wordbased baseline, so we concentrated on Morfessor. Morfessor uses minimum description length criteria to train a HMM-based segmentation model. When tested against a human-annotated gold standard of linguistic morpheme segmentations for Finnish, this algorithm outperforms competing unsupervised methods, achieving an F-score of 67.0% on a 3 million sentence corpus (Creutz and Lagus, 2006) . Varying the perplexity threshold in Morfessor does not segment more word types, but rather over-segments the same word types. In order to get robust, common segmentations, we trained the segmenter on the 5000 most frequent words2 ; we then used this to segment the entire data set. In order to improve coverage, we then further segmented 2For the factored model baseline we also used the same setting perplexity = 30, 5,000 most frequent words, but with all but the last suffix collapsed and called the “stem” . TabHMleoat1nr:gplhiMngor phermphocTur631ae04in, 81c9ie03ns67gi,64n0S14e567theTp 2rsa51t, 29Se 3t168able and in translation. any word type that contained a match from the most frequent suffix set, looking for the longest matching suffix character string. We call this method Unsup L-match. After the segmentation, word-internal morpheme boundary markers were inserted into the segmented text to be used to reconstruct the surface forms in the MT output. We then trained the Moses phrase-based system (Koehn et al., 2007) on the segmented and marked text. After decoding, it was a simple matter to join together all adjacent morphemes with word-internal boundary markers to reconstruct the surface forms. Figure 1(a) gives the full model overview for all the variants of the segmented translation model (supervised/unsupervised; with and without the Unsup L-match procedure) . Table 1shows how morphemes are being used in the MT system. Of the phrases that included segmentations (‘Morph’ in Table 1) , roughly a third were ‘productive’, i.e. had a hanging morpheme (with a form such as stem+) that could be joined to a suffix (‘Hanging Morph’ in Table 1) . However, in phrases used while decoding the development and test data, roughly a quarter of the phrases that generated the translated output included segmentations, but of these, only a small fraction (6%) had a hanging morpheme; and while there are many possible reasons to account for this we were unable to find a single convincing cause. 2.3 Morphology Generation Morphology generation as a post-processing step allows major vocabulary reduction in the translation model, and allows the use of morphologically targeted features for modeling inflection. A possible disadvantage of this approach is that in this model there is no opportunity to con34 sider the morphology in translation since it is removed prior to training the translation model. Morphology generation models can use a variety of bilingual and contextual information to capture dependencies between morphemes, often more long-distance than what is possible using n-gram language models over morphemes in the segmented model. Similar to previous work (Minkov et al. , 2007; Toutanova et al. , 2008) , we model morphology generation as a sequence learning problem. Un- like previous work, we use unsupervised morphology induction and use automatically generated suffix classes as tags. The first phase of our morphology prediction model is to train a MT system that produces morphologically simplified word forms in the target language. The output word forms are complex stems (a stem and some suffixes) but still missing some important suffix morphemes. In the second phase, the output of the MT decoder is then tagged with a sequence of abstract suffix tags. In particular, the output of the MT decoder is a sequence of complex stems denoted by x and the output is a sequence of suffix class tags denoted by y. We use a list of parts from (x,y) and map to a d-dimensional feature vector Φ(x, y) , with each dimension being a real number. We infer the best sequence of tags using: F(x) = argymaxp(y | x,w) where F(x) returns the highest scoring output y∗ . A conditional random field (CRF) (Lafferty et al. , 2001) defines the conditional probability as a linear score for each candidate y and a global normalization term: logp(y | x, w) = Φ(x, y) · w − log Z where Z = Py0∈ exp(Φ(x, y0) · w) . We use stochastiPc gradient descent (using crfsgd3) to train the weight vector w. So far, this is all off-the-shelf sequence learning. However, the output y∗ from the CRF decoder is still only a sequence of abstract suffix tags. The third and final phase in our morphology prediction model GEN(x) 3 http://leon. bottou. org/projects/sgd English Training Data words Finnish Training Data words Morphological Pre-Processing stem+ +morph MT System Alignment: word word word stem+ +morph stem stem+ +morph Post-Process: Morph Re-Stitching Fully inflected surface form Evaluation against original reference (a) Segmented Translation Model English Training Data words Finnish Training Data Morphological Pre-Prowceosrdsisng 1 stem+ +morph1+ +morph2 Morphological Pre-Processing 2 stem+ +morph1+ MPosrpthe-mPRr+eo-+cSmetsio crhp1i:nhg+swteomrd+ MA+lTmigwnSomyrspdthen 1mt:+ wsotermd complex stem: stem+morph1+ MPo rpsht-oPlro gcyesGse2n:erCaRtioFnstem+morph1+ morph2sLuarnfagcueagfeorMmomdealp ing Fully inflected surface form Evaluation against original reference (b) Post-Processing Model Translation & Generation Figure 1: Training and testing pipelines for the SMT models. is to take the abstract suffix tag sequence y∗ and then map it into fully inflected word forms, and rank those outputs using a morphemic language model. The abstract suffix tags are extracted from the unsupervised morpheme learning process, and are carefully designed to enable CRF training and decoding. We call this model CRFLM for short. Figure 1(b) shows the full pipeline and Figure 2 shows a worked example of all the steps involved. We use the morphologically segmented training data (obtained using the segmented corpus described in Section 2.24) and remove selected suffixes to create a morphologically simplified version of the training data. The MT model is trained on the morphologically simplified training data. The output from the MT system is then used as input to the CRF model. The CRF model was trained on a ∼210,000 Finnish sentences, consisting noefd d∼ o1n.5 a am ∼il2li1o0n,0 tokens; tishhe 2,000 cseens,te cnoncse Europarl t.e5s tm isl eito nco tnoskiesntesd; hoef 41,434 stem tokens. The labels in the output sequence y were obtained by selecting the most productive 150 stems, and then collapsing certain vowels into equivalence classes corresponding to Finnish vowel harmony patterns. Thus 4Note that unlike Section 2.2 we do not use Unsup L-match because when evaluating the CRF model on the suffix prediction task it obtained 95.61% without using Unsup L-match and 82.99% when using Unsup L-match. 35 variants -k¨ o and -ko become vowel-generic enclitic particle -kO, and variants -ss ¨a and -ssa become the vowel-generic inessive case marker -ssA, etc. This is the only language-specific component of our translation model. However, we expect this approach to work for other agglutinative languages as well. For fusional languages like Spanish, another mapping from suffix to abstract tags might be needed. These suffix transformations to their equivalence classes prevent morphophonemic variants of the same morpheme from competing against each other in the prediction model. This resulted in 44 possible label outputs per stem which was a reasonable sized tag-set for CRF training. The CRF was trained on monolingual features of the segmented text for suffix prediction, where t is the current token: Word Stem st−n, .., st, .., st+n(n = 4) Morph Prediction yt−2 , yt−1 , yt With this simple feature set, we were able to use features over longer distances, resulting in a total of 1,110,075 model features. After CRF based recovery of the suffix tag sequence, we use a bigram language model trained on a full segmented version on the training data to recover the original vowels. We used bigrams only, because the suffix vowel harmony alternation depends only upon the preceding phonemes in the word from which it was segmented. original training koskevaa mietint o¨ ¨a data: k ¨asitell ¨a ¨an segmentation: koske+ +va+ +a mietint ¨o+ + a¨ k a¨si+ +te+ +ll a¨+ + a¨+ +n (train bigram language model with mapping A = { a , a }) map n fi bniaglr asmuff liaxn gtou agbest mraocdte tag-set: koske+ +va+ +A mietint ¨o+ +A k ¨asi+ +te+ +ll ¨a+ + ¨a+ +n (train CRF model to predict the final suffix) peeling of final suffix: koske+ +va+ mietint ¨o+ k a¨si+ +te+ +ll a¨+ + a¨+ (train SMT model on this transformation of training data) (a) Training decoder output: koske+ +va+ mietint o¨+ k a¨si+ +te+ +ll a¨+ + a¨+ decoder output stitched up: koskeva+ mietint o¨+ k ¨asitell ¨a ¨a+ CRF model prediction: x = ‘koskeva+ mietint ¨o+ k ¨asitell ¨a ¨a+’, y = ‘+A +A +n’ koskeva+ +A mietint ¨o+ +A k ¨asitell a¨ ¨a+ +n unstitch morphemes: koske+ +va+ +A mietint ¨o+ +A k ¨asi+ +te+ +ll ¨a+ + ¨a+ +n language model disambiguation: koske+ +va+ +a mietint ¨o+ + a¨ k a¨si+ +te+ +ll a¨+ + a¨+ +n final stitching: koskevaa mietint o¨ ¨a k ¨asitell ¨a ¨an (the output is then compared to the reference translation) (b) Decoding Figure 2: Worked example of all steps in the post-processing morphology prediction model. 3 Experimental Results used the Europarl version 3 corpus (Koehn, 2005) English-Finnish training data set, as well as the standard development and test data sets. Our parallel training data consists of ∼1 million senFor all of the models built in this paper, we tpeanrcaelsle lo tfr a4i0n nwgor ddast or less, sw ohfi ∼le 1t mhei development and test sets were each 2,000 sentences long. In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al. , 2007) , 2008 version. We trained all of the Moses systems herein using the standard features: language model, reordering model, translation model, and word penalty; in addition to these, the factored experiments called for additional translation and generation features for the added factors as noted above. We used in all experiments the following settings: a hypothesis stack size 100, distortion limit 6, phrase translations limit 20, and maximum phrase length 20. For the language models, we used SRILM 5-gram language models (Stolcke, 2002) for all factors. For our word-based Baseline system, we trained a word-based model using the same Moses system with identical settings. For evaluation against segmented translation systems in segmented forms before word reconstruction, we also segmented the baseline system’s word-based output. All the BLEU scores reported are for lowercase evaluation. We did an initial evaluation of the segmented output translation for each system using the no5http://www.statmt.org/moses/ 36 TabSlBUeuna2gps:meulSipengLmta-e nioatedchMo12dme804-.lB8S714cL±oEr0eUs.6 9 S8up19Nre.358ofe498rUs9ntoihe supervised segmentation baseline model. m-BLEU indicates that the segmented output was evaluated against a segmented version of the reference (this measure does not have the same correlation with human judgement as BLEU) . No Uni indicates the segmented BLEU score without unigrams. tion of m-BLEU score (Luong et al. , 2010) where the BLEU score is computed by comparing the segmented output with a segmented reference translation. Table 2 shows the m-BLEU scores for various systems. We also show the m-BLEU score without unigrams, since over-segmentation could lead to artificially high m-BLEU scores. In fact, if we compare the relative improvement of our m-BLEU scores for the Unsup L-match system we see a relative improvement of 39.75% over the baseline. Luong et. al. (2010) report an m-BLEU score of 55.64% but obtain a relative improvement of 0.6% over their baseline m-BLEU score. We find that when using a good segmentation model, segmentation of the morphologically complex target language improves model performance over an unsegmented baseline (the confidence scores come from bootstrap resampling) . Table 3 shows the evaluation scores for all the baselines and the methods introduced in this paper using standard wordbased lowercase BLEU, WER and PER. We do TSCMaFBU(LubanRolpcesdFotu3lne-ipLr:gMdeLT-tms.al,Stc2ho0r1es:)l 1wB54 Le.r682E90c 27a9Us∗eBL-7 W46E3. U659478R6,1WE-7 TR412E. 847Ra1528nd TER. The ∗ indicates a statistically significant improvement o∗f BndLiEcaUte score over tchalel yB saisgenli nfice mntod imel.The boldface scores are the best performing scores per evaluation measure. better than (Luong et al. , 2010) , the previous best score for this task. We also show a better relative improvement over our baseline when compared to (Luong et al., 2010) : a relative improvement of 4.86% for Unsup L-match compared to our baseline word-based model, compared to their 1.65% improvement over their baseline word-based model. Our best performing method used unsupervised morphology with L-match (see Section 2.2) and the improvement is significant: bootstrap resampling provides a confidence margin of ±0.77 and a t-test (Collins ceot nafli.d , 2005) sahrogwined o significance aw ti-thte p = 0o.0ll0in1s. 3.1 Morphological Fluency Analysis To see how well the models were doing at getting morphology right, we examined several patterns of morphological behavior. While we wish to explore minimally supervised morphological MT models, and use as little language specific information as possible, we do want to use linguistic analysis on the output of our system to see how well the models capture essential morphological information in the target language. So, we ran the word-based baseline system, the segmented model (Unsup L-match) , and the prediction model (CRF-LM) outputs, along with the reference translation through the supervised morphological analyzer Omorfi (Pirinen and Listenmaa, 2007) . Using this analysis, we looked at a variety of linguistic constructions that might reveal patterns in morphological behavior. These were: (a) explicitly marked 37 noun forms, (b) noun-adjective case agreement, (c) subject-verb person/number agreement, (d) transitive object case marking, (e) postpositions, and (f) possession. In each of these categories, we looked for construction matches on a per-sentence level between the models’ output and the reference translation. Table 4 shows the models’ performance on the constructions we examined. In all of the categories, the CRF-LM model achieves the best precision score, as we explain below, while the Unsup L-match model most frequently gets the highest recall score. A general pattern in the most prevalent of these constructions is that the baseline tends to prefer the least marked form for noun cases (corresponding to the nominative) more than the reference or the CRF-LM model. The baseline leaves nouns in the (unmarked) nominative far more than the reference, while the CRF-LM model comes much closer, so it seems to fare better at explicitly marking forms, rather than defaulting to the more frequent unmarked form. Finnish adjectives must be marked with the same case as their head noun, while verbs must agree in person and number with their subject. We saw that in both these categories, the CRFLM model outperforms for precision, while the segmented model gets the best recall. In addition, Finnish generally marks direct objects of verbs with the accusative or the partitive case; we observed more accusative/partitive-marked nouns following verbs in the CRF-LM output than in the baseline, as illustrated by example (1) in Fig. 3. While neither translation picks the same verb as in the reference for the input ‘clarify,’ the CRFLM-output paraphrases it by using a grammatical construction of the transitive verb followed by a noun phrase inflected with the accusative case, correctly capturing the transitive construction. The baseline translation instead follows ‘give’ with a direct object in the nominative case. To help clarify the constructions in question, we have used Google Translate6 to provide back6 http://translate.google. com/ of occurrences per sentence, recall and F-score. also averaged The constructions over the various translations. are listed in descending P, R and F stand for precision, order of their frequency in the texts. The highlighted value in each column is the most accurate with respect to the reference value. translations of our MT output into English; to contextualize these back-translations, we have provided Google’s back-translation of the reference. The use of postpositions shows another difference between the models. Finnish postpositions require the preceding noun to be in the genitive or sometimes partitive case, which occurs correctly more frequently in the CRF-LM than the baseline. In example (2) in Fig. 3, all three translations correspond to the English text, ‘with the basque nationalists. ’ However, the CRF-LM output is more grammatical than the baseline, because not only do the adjective and noun agree for case, but the noun ‘baskien’ to which the postposition ‘kanssa’ belongs is marked with the correct genitive case. However, this well-formedness is not rewarded by BLEU, because ‘baskien’ does not match the reference. In addition, while Finnish may express possession using case marking alone, it has another construction for possession; this can disambiguate an otherwise ambiguous clause. This alternate construction uses a pronoun in the genitive case followed by a possessive-marked noun; we see that the CRF-LM model correctly marks this construction more frequently than the baseline. As example (3) in Fig. 3 shows, while neither model correctly translates ‘matkan’ (‘trip’) , the baseline’s output attributes the inessive ‘yhteydess’ (‘connection’) as belonging to ‘tulokset’ (‘results’) , and misses marking the possession linking it to ‘Commissioner Fischler’. Our manual evaluation shows that the CRF38 LM model is producing output translations that are more morphologically fluent than the wordbased baseline and the segmented translation Unsup L-match system, even though the word choices lead to a lower BLEU score overall when compared to Unsup L-match. 4 Related Work The work on morphology in MT can be grouped into three categories, factored models, segmented translation, and morphology generation. Factored models (Koehn and Hoang, 2007) factor the phrase translation probabilities over additional information annotated to each word, allowing for text to be represented on multiple levels of analysis. We discussed the drawbacks of factored models for our task in Section 2. 1. While (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) obtain improvements using factored models for translation into English, German, Spanish, and Czech, these models may be less useful for capturing long-distance dependencies in languages with much more complex morphological systems such as Finnish. In our experiments factored models did worse than the baseline. Segmented translation performs morphological analysis on the morphologically complex text for use in the translation model (Brown et al. , 1993; Goldwater and McClosky, 2005; de Gispert and Mari n˜o, 2008) . This method unpacks complex forms into simpler, more frequently occurring components, and may also increase the symmetry of the lexically realized content be(1) Input: ‘the charter we are to approve today both strengthens and gives visible shape to the common fundamental rights and values our community is to be based upon. ’ a. Reference: perusoikeuskirja , jonka t ¨an ¨a ¨an aiomme hyv a¨ksy ¨a , sek ¨a vahvistaa ett ¨a selvent a¨ a¨ (selvent ¨a a¨/VERB/ACT/INF/SG/LAT-clarify) niit a¨ (ne/PRONOUN/PL/PAR-them) yhteisi ¨a perusoikeuksia ja arvoja , joiden on oltava yhteis¨ omme perusta. Back-translation: ‘Charter of Fundamental Rights, which today we are going to accept that clarify and strengthen the common fundamental rights and values, which must be community based. ’ b. Baseline: perusoikeuskirja me hyv ¨aksymme t¨ an ¨a a¨n molemmat vahvistaa ja antaa (antaa/VERB/INF/SG/LATgive) n a¨kyv a¨ (n¨ aky a¨/VERB/ACT/PCP/SG/NOM-visible) muokata yhteist ¨a perusoikeuksia ja arvoja on perustuttava. Back-translation: ‘Charter today, we accept both confirm and modify to make a visible and common values, fundamental rights must be based. ’ c. CRF-LM: perusoikeuskirja on hyv a¨ksytty t ¨an ¨a ¨an , sek ¨a vahvistaa ja antaa (antaa/VERB/ACT/INF/SG/LAT-give) konkreettisen (konkreettinen/ADJECTIVE/SG/GEN,ACC-concrete) muodon (muoto/NOUN/SG/GEN,ACCshape) yhteisi ¨a perusoikeuksia ja perusarvoja , yhteis¨ on on perustuttava. Back-translation: ‘Charter has been approved today, and to strengthen and give concrete shape to the common basic rights and fundamental values, the Community must be based. ’ (2) Input: ‘with the basque nationalists’ a. Reference: baskimaan kansallismielisten kanssa basque-SG/NOM+land-SG/GEN,ACC nationalists-PL/GEN with-POST b. Baseline: baskimaan kansallismieliset kanssa basque-SG/NOM-+land-SG/GEN,ACC kansallismielinen-PL/NOM,ACC-nationalists POST-with c. CRF-LM: kansallismielisten baskien kanssa nationalists-PL/GEN basques-PL/GEN with-POST (3) Input: ‘and in this respect we should value the latest measures from commissioner fischler , the results of his trip to morocco on the 26th of last month and the high level meetings that took place, including the one with the king himself’ a. Reference: ja t ¨ass¨ a mieless ¨a osaamme my¨ os arvostaa komission j¨ asen fischlerin viimeisimpi ¨a toimia , jotka ovat h a¨nen (h¨ anen/GEN-his) marokkoon 26 lokakuuta tekemns (tekem¨ ans ¨a/POSS-his) matkan (matkan/GENtour) ja korkean tason kokousten jopa itsens¨ a kuninkaan kanssa tulosta Back-translation: ‘and in this sense we can also appreciate the Commissioner Fischler’s latest actions, which are his to Morocco 26 October trip to high-level meetings and even the king himself with the result b. Baseline: ja t ¨ass¨ a yhteydess a¨ olisi arvoa viimeisin toimia komission j¨ asen fischler , tulokset monitulkintaisia marokon yhteydess a¨ (yhteydess/INE-connection) , ja viime kuussa pidettiin korkean tason kokouksissa , mukaan luettuna kuninkaan kanssa Back-translation: ‘and in this context would be the value of the last act, Commissioner Fischler, the results of the Moroccan context, ambiguous, and last month held high level meetings, including with the king’ c. CRF-LM: ja t ¨ass¨ a yhteydess a¨ meid ¨an olisi lis ¨aarvoa viimeist ¨a toimenpiteit a¨ kuin komission j¨ asen fischler , ett a¨ h a¨nen (h¨ anen/GEN-his) kokemuksensa (kokemuksensa/POSS-experience) marokolle (marokolle-Moroccan) viime kuun 26 ja korkean tason tapaamiset j¨ arjestettiin, kuninkaan kanssa Back-translation: ‘and in this context, we should value the last measures as the Commissioner Fischler, that his experience in Morocco has on the 26th and high-level meetings took place, including with the king. ’ Figure 3: Morphological fluency analysis (see Section 3. 1) . tween source and target. In a somewhat orthogonal approach to ours, (Ma et al. , 2007) use alignment of a parallel text to pack together adjacent segments in the alignment output, which are then fed back to the word aligner to bootstrap an improved alignment, which is then used in the translation model. We compared our results against (Luong et al. , 2010) in Table 3 since their results are directly comparable to ours. They use a segmented phrase table and language model along with the word-based versions in the decoder and in tuning a Finnish target. Their approach requires segmented phrases 39 to match word boundaries, eliminating morphologically productive phrases. In their work a segmented language model can score a translation, but cannot insert morphology that does not show source-side reflexes. In order to perform a similar experiment that still allowed for morphologically productive phrases, we tried training a segmented translation model, the output of which we stitched up in tuning so as to tune to a word-based reference. The goal of this experiment was to control the segmented model’s tendency to overfit by rewarding it for using correct whole-word forms. However, we found that this approach was less successful than using the segmented reference in tuning, and could not meet the baseline (13.97% BLEU best tuning score, versus 14.93% BLEU for the baseline best tuning score) . Previous work in segmented translation has often used linguistically motivated morphological analysis selectively applied based on a language-specific heuristic. A typical approach is to select a highly inflecting class of words and segment them for particular morphology (de Gispert and Mari n˜o, 2008; Ramanathan et al. , 2009) . Popovi¸ c and Ney (2004) perform segmentation to reduce morphological complexity of the source to translate into an isolating target, reducing the translation error rate for the English target. For Czech-to-English, Goldwater and McClosky (2005) lemmatized the source text and inserted a set of ‘pseudowords’ expected to have lexical reflexes in English. Minkov et. al. (2007) and Toutanova et. al. (2008) use a Maximum Entropy Markov Model for morphology generation. The main drawback to this approach is that it removes morphological information from the translation model (which only uses stems) ; this can be a problem for languages in which morphology ex- presses lexical content. de Gispert (2008) uses a language-specific targeted morphological classifier for Spanish verbs to avoid this issue. Talbot and Osborne (2006) use clustering to group morphological variants of words for word alignments and for smoothing phrase translation tables. Habash (2007) provides various methods to incorporate morphological variants of words in the phrase table in order to help recognize out of vocabulary words in the source language. 5 Conclusion and Future Work We found that using a segmented translation model based on unsupervised morphology induction and a model that combined morpheme segments in the translation model with a postprocessing morphology prediction model gave us better BLEU scores than a word-based baseline. Using our proposed approach we obtain better scores than the state of the art on the EnglishFinnish translation task (Luong et al. , 2010) : from 14.82% BLEU to 15.09%, while using a 40 simpler model. We show that using morphological segmentation in the translation model can improve output translation scores. We also demonstrate that for Finnish (and possibly other agglutinative languages) , phrase-based MT benefits from allowing the translation model access to morphological segmentation yielding productive morphological phrases. Taking advantage of linguistic analysis of the output we show that using a post-processing morphology generation model can improve translation fluency on a sub-word level, in a manner that is not captured by the BLEU word-based evaluation measure. In order to help with replication of the results in this paper, we have run the various morphological analysis steps and created the necessary training, tuning and test data files needed in order to train, tune and test any phrase-based machine translation system with our data. The files can be downloaded from natlang. cs.sfu. ca. In future work we hope to explore the utility of phrases with productive morpheme boundaries and explore why they are not used more pervasively in the decoder. Evaluation measures for morphologically complex languages and tun- ing to those measures are also important future work directions. Also, we would like to explore a non-pipelined approach to morphological preand post-processing so that a globally trained model could be used to remove the target side morphemes that would improve the translation model and then predict those morphemes in the target language. Acknowledgements This research was partially supported by NSERC, Canada (RGPIN: 264905) and a Google Faculty Award. We would like to thank Christian Monson, Franz Och, Fred Popowich, Howard Johnson, Majid Razmara, Baskaran Sankaran and the anonymous reviewers for their valuable comments on this work. We would particularly like to thank the developers of the open-source Moses machine translation toolkit and the Omorfi morphological analyzer for Finnish which we used for our experiments. References Eleftherios Avramidis and Philipp Koehn. 2008. Enriching morphologically poor languages for statistical machine translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, page 763?770, Columbus, Ohio, USA. Association for Computational Linguistics. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2) :263–31 1. Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, Columbus, Ohio, June. Association for Computational Linguistics. Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics (A CL05). Association for Computational Linguistics. Mathias Creutz and Krista Lagus. 2005. Inducing the morphological lexicon of a natural language from unannotated text. In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reason- ing (AKRR ’05), pages 106–113, Espoo, Finland. Mathias Creutz and Krista Lagus. 2006. Morfessor in the morpho challenge. In Proceedings of the PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes. Adri ´a de Gispert and Jos e´ Mari n˜o. 2008. On the impact of morphology in English to Spanish statistical MT. Speech Communication, 50(11-12) . Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 676–683, Vancouver, B.C. , Canada. Association for Computational Linguistics. Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 868–876, Prague, Czech Republic. Association for Computational Linguistics. 41 Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In A CL ‘07: Proceedings of the 45th Annual Meeting of the A CL on Interactive Poster and Demonstration Sessions, pages 177–108, Prague, Czech Republic. Association for Computational Linguistics. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X, pages 79–86, Phuket, Thailand. Association for Computational Linguistics. John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282–289, San Francisco, California, USA. Association for Computing Machinery. Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan. 2010. A hybrid morpheme-word representation for machine translation of morphologically rich languages. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 148–157, Cambridge, Massachusetts. Association for Computational Linguistics. Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304–311, Prague, Czech Republic. Association for Computational Linguistics. Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (A CL07), pages 128–135, Prague, Czech Republic. Association for Computational Linguistics. Christian Monson. 2008. Paramor and morpho challenge 2008. In Lecture Notes in Computer Science: Workshop of the Cross-Language Evaluation Forum (CLEF 2008), Revised Selected Papers. Habash Nizar. 2007. Four techniques for online handling of out-of-vocabulary words in arabic-english statistical machine translation. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics, Columbus, Ohio. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics A CL, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Tommi Pirinen and Inari Listenmaa. 2007. Omorfi morphological analzer. http://gna.org/projects/omorfi. Maja Popovi¸ c and Hermann Ney. 2004. Towards the use of word stems and suffixes for statistiWei jing cal machine translation. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC), pages 1585–1588, Lisbon, Portugal. European Language Resources Association (ELRA) . Ananthakrishnan Ramanathan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and morphology: Addressing the crux of the fluency problem in EnglishHindi SMT. In Proceedings of the Joint Conference of the 4 7th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, pages 800–808, Suntec, Singapore. Association for Computational Linguistics. Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. 7th International Conference on Spoken Language Processing, 3:901–904. David Talbot and Miles Osborne. 2006. Modelling lexical redundancy for machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 969–976, Sydney, Australia, July. Association for Computational Linguistics. Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying morphology generation models to machine translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 514–522, Columbus, Ohio, USA. Association for Computational Linguistics. Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proceedings of the European Chapter of the Association for Computational Linguistics, pages 41–48, Trento, Italy. Association for Computational Linguistics. 42</p><p>3 0.19004212 <a title="318-tfidf-3" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>Author: Daniel Hewlett ; Paul Cohen</p><p>Abstract: Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</p><p>4 0.18500799 <a title="318-tfidf-4" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>Author: Ning Xi ; Guangchao Tang ; Boyuan Li ; Yinggong Zhao</p><p>Abstract: In this paper, we present a new word alignment combination approach on language pairs where one language has no explicit word boundaries. Instead of combining word alignments of different models (Xiang et al., 2010), we try to combine word alignments over multiple monolingually motivated word segmentation. Our approach is based on link confidence score defined over multiple segmentations, thus the combined alignment is more robust to inappropriate word segmentation. Our combination algorithm is simple, efficient, and easy to implement. In the Chinese-English experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations. 1</p><p>5 0.17798842 <a title="318-tfidf-5" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>6 0.17468284 <a title="318-tfidf-6" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>7 0.15881243 <a title="318-tfidf-7" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>8 0.15272586 <a title="318-tfidf-8" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>9 0.15052271 <a title="318-tfidf-9" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>10 0.14946838 <a title="318-tfidf-10" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>11 0.14236689 <a title="318-tfidf-11" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>12 0.13488291 <a title="318-tfidf-12" href="./acl-2011-A_Corpus_for_Modeling_Morpho-Syntactic_Agreement_in_Arabic%3A_Gender%2C_Number_and_Rationality.html">7 acl-2011-A Corpus for Modeling Morpho-Syntactic Agreement in Arabic: Gender, Number and Rationality</a></p>
<p>13 0.13362224 <a title="318-tfidf-13" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>14 0.13236724 <a title="318-tfidf-14" href="./acl-2011-Using_Deep_Morphology_to_Improve_Automatic_Error_Detection_in_Arabic_Handwriting_Recognition.html">329 acl-2011-Using Deep Morphology to Improve Automatic Error Detection in Arabic Handwriting Recognition</a></p>
<p>15 0.12833628 <a title="318-tfidf-15" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>16 0.12809169 <a title="318-tfidf-16" href="./acl-2011-Exploiting_Morphology_in_Turkish_Named_Entity_Recognition_System.html">124 acl-2011-Exploiting Morphology in Turkish Named Entity Recognition System</a></p>
<p>17 0.12439615 <a title="318-tfidf-17" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>18 0.12222336 <a title="318-tfidf-18" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>19 0.11981872 <a title="318-tfidf-19" href="./acl-2011-Extracting_Paraphrases_from_Definition_Sentences_on_the_Web.html">132 acl-2011-Extracting Paraphrases from Definition Sentences on the Web</a></p>
<p>20 0.1190875 <a title="318-tfidf-20" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.245), (1, -0.137), (2, 0.071), (3, 0.029), (4, -0.02), (5, -0.001), (6, 0.187), (7, 0.041), (8, 0.04), (9, 0.237), (10, 0.089), (11, 0.192), (12, -0.196), (13, 0.125), (14, 0.07), (15, -0.096), (16, 0.077), (17, -0.052), (18, -0.059), (19, 0.165), (20, 0.045), (21, -0.032), (22, -0.051), (23, -0.02), (24, 0.02), (25, 0.037), (26, -0.037), (27, 0.082), (28, 0.023), (29, 0.009), (30, -0.027), (31, 0.003), (32, -0.016), (33, 0.027), (34, -0.019), (35, -0.026), (36, -0.007), (37, 0.05), (38, -0.037), (39, -0.002), (40, -0.062), (41, -0.061), (42, -0.009), (43, 0.029), (44, 0.003), (45, 0.012), (46, 0.022), (47, -0.035), (48, -0.016), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94193798 <a title="318-lsi-1" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>Author: Jason Naradowsky ; Kristina Toutanova</p><p>Abstract: This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</p><p>2 0.73343349 <a title="318-lsi-2" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>Author: Ning Xi ; Guangchao Tang ; Boyuan Li ; Yinggong Zhao</p><p>Abstract: In this paper, we present a new word alignment combination approach on language pairs where one language has no explicit word boundaries. Instead of combining word alignments of different models (Xiang et al., 2010), we try to combine word alignments over multiple monolingually motivated word segmentation. Our approach is based on link confidence score defined over multiple segmentations, thus the combined alignment is more robust to inappropriate word segmentation. Our combination algorithm is simple, efficient, and easy to implement. In the Chinese-English experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations. 1</p><p>3 0.65197128 <a title="318-lsi-3" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>Author: Ann Clifton ; Anoop Sarkar</p><p>Abstract: This paper extends the training and tuning regime for phrase-based statistical machine translation to obtain fluent translations into morphologically complex languages (we build an English to Finnish translation system) . Our methods use unsupervised morphology induction. Unlike previous work we focus on morphologically productive phrase pairs – our decoder can combine morphemes across phrase boundaries. Morphemes in the target language may not have a corresponding morpheme or word in the source language. Therefore, we propose a novel combination of post-processing morphology prediction with morpheme-based translation. We show, using both automatic evaluation scores and linguistically motivated analyses of the output, that our methods outperform previously proposed ones and pro- vide the best known results on the EnglishFinnish Europarl translation task. Our methods are mostly language independent, so they should improve translation into other target languages with complex morphology. 1 Translation and Morphology Languages with rich morphological systems present significant hurdles for statistical machine translation (SMT) , most notably data sparsity, source-target asymmetry, and problems with automatic evaluation. In this work, we propose to address the problem of morphological complexity in an Englishto-Finnish MT task within a phrase-based translation framework. We focus on unsupervised segmentation methods to derive the morphological information supplied to the MT model in order to provide coverage on very large datasets and for languages with few hand-annotated 32 resources. In fact, in our experiments, unsupervised morphology always outperforms the use of a hand-built morphological analyzer. Rather than focusing on a few linguistically motivated aspects of Finnish morphological behaviour, we develop techniques for handling morphological complexity in general. We chose Finnish as our target language for this work, because it exemplifies many of the problems morphologically complex languages present for SMT. Among all the languages in the Europarl data-set, Finnish is the most difficult language to translate from and into, as was demonstrated in the MT Summit shared task (Koehn, 2005) . Another reason is the current lack of knowledge about how to apply SMT successfully to agglutinative languages like Turkish or Finnish. Our main contributions are: 1) the introduction of the notion of segmented translation where we explicitly allow phrase pairs that can end with a dangling morpheme, which can connect with other morphemes as part of the translation process, and 2) the use of a fully segmented translation model in combination with a post-processing morpheme prediction system, using unsupervised morphology induction. Both of these approaches beat the state of the art on the English-Finnish translation task. Morphology can express both content and function categories, and our experiments show that it is important to use morphology both within the translation model (for morphology with content) and outside it (for morphology contributing to fluency) . Automatic evaluation measures for MT, BLEU (Papineni et al., 2002), WER (Word Error Rate) and PER (Position Independent Word Error Rate) use the word as the basic unit rather than morphemes. In a word comProce dPinogrstla ofn tdh,e O 4r9etghon A,n Jnu nael 1 M9-e 2t4i,n2g 0 o1f1 t.he ?c A2s0s1o1ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 32–42, prised of multiple morphemes, getting even a single morpheme wrong means the entire word is wrong. In addition to standard MT evaluation measures, we perform a detailed linguistic analysis of the output. Our proposed approaches are significantly better than the state of the art, achieving the highest reported BLEU scores on the English-Finnish Europarl version 3 data-set. Our linguistic analysis shows that our models have fewer morpho-syntactic errors compared to the word-based baseline. 2 2.1 Models Baseline Models We set up three baseline models for comparison in this work. The first is a basic wordbased model (called Baseline in the results) ; we trained this on the original unsegmented version of the text. Our second baseline is a factored translation model (Koehn and Hoang, 2007) (called Factored) , which used as factors the word, “stem” 1 and suffix. These are derived from the same unsupervised segmentation model used in other experiments. The results (Table 3) show that a factored model was unable to match the scores of a simple wordbased baseline. We hypothesize that this may be an inherently difficult representational form for a language with the degree of morphological complexity found in Finnish. Because the morphology generation must be precomputed, for languages with a high degree of morphological complexity, the combinatorial explosion makes it unmanageable to capture the full range of morphological productivity. In addition, because the morphological variants are generated on a per-word basis within a given phrase, it excludes productive morphological combination across phrase boundaries and makes it impossible for the model to take into account any longdistance dependencies between morphemes. We conclude from this result that it may be more useful for an agglutinative language to use morphology beyond the confines of the phrasal unit, and condition its generation on more than just the local target stem. In order to compare the 1see Section 2.2. 33 performance of unsupervised segmentation for translation, our third baseline is a segmented translation model based on a supervised segmentation model (called Sup) , using the hand-built Omorfi morphological analyzer (Pirinen and Listenmaa, 2007) , which provided slightly higher BLEU scores than the word-based baseline. 2.2 Segmented Translation For segmented translation models, it cannot be taken for granted that greater linguistic accuracy in segmentation yields improved translation (Chang et al. , 2008) . Rather, the goal in segmentation for translation is instead to maximize the amount of lexical content-carrying morphology, while generalizing over the information not helpful for improving the translation model. We therefore trained several different segmentation models, considering factors of granularity, coverage, and source-target symmetry. We performed unsupervised segmentation of the target data, using Morfessor (Creutz and Lagus, 2005) and Paramor (Monson, 2008) , two top systems from the Morpho Challenge 2008 (their combined output was the Morpho Challenge winner) . However, translation models based upon either Paramor alone or the combined systems output could not match the wordbased baseline, so we concentrated on Morfessor. Morfessor uses minimum description length criteria to train a HMM-based segmentation model. When tested against a human-annotated gold standard of linguistic morpheme segmentations for Finnish, this algorithm outperforms competing unsupervised methods, achieving an F-score of 67.0% on a 3 million sentence corpus (Creutz and Lagus, 2006) . Varying the perplexity threshold in Morfessor does not segment more word types, but rather over-segments the same word types. In order to get robust, common segmentations, we trained the segmenter on the 5000 most frequent words2 ; we then used this to segment the entire data set. In order to improve coverage, we then further segmented 2For the factored model baseline we also used the same setting perplexity = 30, 5,000 most frequent words, but with all but the last suffix collapsed and called the “stem” . TabHMleoat1nr:gplhiMngor phermphocTur631ae04in, 81c9ie03ns67gi,64n0S14e567theTp 2rsa51t, 29Se 3t168able and in translation. any word type that contained a match from the most frequent suffix set, looking for the longest matching suffix character string. We call this method Unsup L-match. After the segmentation, word-internal morpheme boundary markers were inserted into the segmented text to be used to reconstruct the surface forms in the MT output. We then trained the Moses phrase-based system (Koehn et al., 2007) on the segmented and marked text. After decoding, it was a simple matter to join together all adjacent morphemes with word-internal boundary markers to reconstruct the surface forms. Figure 1(a) gives the full model overview for all the variants of the segmented translation model (supervised/unsupervised; with and without the Unsup L-match procedure) . Table 1shows how morphemes are being used in the MT system. Of the phrases that included segmentations (‘Morph’ in Table 1) , roughly a third were ‘productive’, i.e. had a hanging morpheme (with a form such as stem+) that could be joined to a suffix (‘Hanging Morph’ in Table 1) . However, in phrases used while decoding the development and test data, roughly a quarter of the phrases that generated the translated output included segmentations, but of these, only a small fraction (6%) had a hanging morpheme; and while there are many possible reasons to account for this we were unable to find a single convincing cause. 2.3 Morphology Generation Morphology generation as a post-processing step allows major vocabulary reduction in the translation model, and allows the use of morphologically targeted features for modeling inflection. A possible disadvantage of this approach is that in this model there is no opportunity to con34 sider the morphology in translation since it is removed prior to training the translation model. Morphology generation models can use a variety of bilingual and contextual information to capture dependencies between morphemes, often more long-distance than what is possible using n-gram language models over morphemes in the segmented model. Similar to previous work (Minkov et al. , 2007; Toutanova et al. , 2008) , we model morphology generation as a sequence learning problem. Un- like previous work, we use unsupervised morphology induction and use automatically generated suffix classes as tags. The first phase of our morphology prediction model is to train a MT system that produces morphologically simplified word forms in the target language. The output word forms are complex stems (a stem and some suffixes) but still missing some important suffix morphemes. In the second phase, the output of the MT decoder is then tagged with a sequence of abstract suffix tags. In particular, the output of the MT decoder is a sequence of complex stems denoted by x and the output is a sequence of suffix class tags denoted by y. We use a list of parts from (x,y) and map to a d-dimensional feature vector Φ(x, y) , with each dimension being a real number. We infer the best sequence of tags using: F(x) = argymaxp(y | x,w) where F(x) returns the highest scoring output y∗ . A conditional random field (CRF) (Lafferty et al. , 2001) defines the conditional probability as a linear score for each candidate y and a global normalization term: logp(y | x, w) = Φ(x, y) · w − log Z where Z = Py0∈ exp(Φ(x, y0) · w) . We use stochastiPc gradient descent (using crfsgd3) to train the weight vector w. So far, this is all off-the-shelf sequence learning. However, the output y∗ from the CRF decoder is still only a sequence of abstract suffix tags. The third and final phase in our morphology prediction model GEN(x) 3 http://leon. bottou. org/projects/sgd English Training Data words Finnish Training Data words Morphological Pre-Processing stem+ +morph MT System Alignment: word word word stem+ +morph stem stem+ +morph Post-Process: Morph Re-Stitching Fully inflected surface form Evaluation against original reference (a) Segmented Translation Model English Training Data words Finnish Training Data Morphological Pre-Prowceosrdsisng 1 stem+ +morph1+ +morph2 Morphological Pre-Processing 2 stem+ +morph1+ MPosrpthe-mPRr+eo-+cSmetsio crhp1i:nhg+swteomrd+ MA+lTmigwnSomyrspdthen 1mt:+ wsotermd complex stem: stem+morph1+ MPo rpsht-oPlro gcyesGse2n:erCaRtioFnstem+morph1+ morph2sLuarnfagcueagfeorMmomdealp ing Fully inflected surface form Evaluation against original reference (b) Post-Processing Model Translation & Generation Figure 1: Training and testing pipelines for the SMT models. is to take the abstract suffix tag sequence y∗ and then map it into fully inflected word forms, and rank those outputs using a morphemic language model. The abstract suffix tags are extracted from the unsupervised morpheme learning process, and are carefully designed to enable CRF training and decoding. We call this model CRFLM for short. Figure 1(b) shows the full pipeline and Figure 2 shows a worked example of all the steps involved. We use the morphologically segmented training data (obtained using the segmented corpus described in Section 2.24) and remove selected suffixes to create a morphologically simplified version of the training data. The MT model is trained on the morphologically simplified training data. The output from the MT system is then used as input to the CRF model. The CRF model was trained on a ∼210,000 Finnish sentences, consisting noefd d∼ o1n.5 a am ∼il2li1o0n,0 tokens; tishhe 2,000 cseens,te cnoncse Europarl t.e5s tm isl eito nco tnoskiesntesd; hoef 41,434 stem tokens. The labels in the output sequence y were obtained by selecting the most productive 150 stems, and then collapsing certain vowels into equivalence classes corresponding to Finnish vowel harmony patterns. Thus 4Note that unlike Section 2.2 we do not use Unsup L-match because when evaluating the CRF model on the suffix prediction task it obtained 95.61% without using Unsup L-match and 82.99% when using Unsup L-match. 35 variants -k¨ o and -ko become vowel-generic enclitic particle -kO, and variants -ss ¨a and -ssa become the vowel-generic inessive case marker -ssA, etc. This is the only language-specific component of our translation model. However, we expect this approach to work for other agglutinative languages as well. For fusional languages like Spanish, another mapping from suffix to abstract tags might be needed. These suffix transformations to their equivalence classes prevent morphophonemic variants of the same morpheme from competing against each other in the prediction model. This resulted in 44 possible label outputs per stem which was a reasonable sized tag-set for CRF training. The CRF was trained on monolingual features of the segmented text for suffix prediction, where t is the current token: Word Stem st−n, .., st, .., st+n(n = 4) Morph Prediction yt−2 , yt−1 , yt With this simple feature set, we were able to use features over longer distances, resulting in a total of 1,110,075 model features. After CRF based recovery of the suffix tag sequence, we use a bigram language model trained on a full segmented version on the training data to recover the original vowels. We used bigrams only, because the suffix vowel harmony alternation depends only upon the preceding phonemes in the word from which it was segmented. original training koskevaa mietint o¨ ¨a data: k ¨asitell ¨a ¨an segmentation: koske+ +va+ +a mietint ¨o+ + a¨ k a¨si+ +te+ +ll a¨+ + a¨+ +n (train bigram language model with mapping A = { a , a }) map n fi bniaglr asmuff liaxn gtou agbest mraocdte tag-set: koske+ +va+ +A mietint ¨o+ +A k ¨asi+ +te+ +ll ¨a+ + ¨a+ +n (train CRF model to predict the final suffix) peeling of final suffix: koske+ +va+ mietint ¨o+ k a¨si+ +te+ +ll a¨+ + a¨+ (train SMT model on this transformation of training data) (a) Training decoder output: koske+ +va+ mietint o¨+ k a¨si+ +te+ +ll a¨+ + a¨+ decoder output stitched up: koskeva+ mietint o¨+ k ¨asitell ¨a ¨a+ CRF model prediction: x = ‘koskeva+ mietint ¨o+ k ¨asitell ¨a ¨a+’, y = ‘+A +A +n’ koskeva+ +A mietint ¨o+ +A k ¨asitell a¨ ¨a+ +n unstitch morphemes: koske+ +va+ +A mietint ¨o+ +A k ¨asi+ +te+ +ll ¨a+ + ¨a+ +n language model disambiguation: koske+ +va+ +a mietint ¨o+ + a¨ k a¨si+ +te+ +ll a¨+ + a¨+ +n final stitching: koskevaa mietint o¨ ¨a k ¨asitell ¨a ¨an (the output is then compared to the reference translation) (b) Decoding Figure 2: Worked example of all steps in the post-processing morphology prediction model. 3 Experimental Results used the Europarl version 3 corpus (Koehn, 2005) English-Finnish training data set, as well as the standard development and test data sets. Our parallel training data consists of ∼1 million senFor all of the models built in this paper, we tpeanrcaelsle lo tfr a4i0n nwgor ddast or less, sw ohfi ∼le 1t mhei development and test sets were each 2,000 sentences long. In all the experiments conducted in this paper, we used the Moses5 phrase-based translation system (Koehn et al. , 2007) , 2008 version. We trained all of the Moses systems herein using the standard features: language model, reordering model, translation model, and word penalty; in addition to these, the factored experiments called for additional translation and generation features for the added factors as noted above. We used in all experiments the following settings: a hypothesis stack size 100, distortion limit 6, phrase translations limit 20, and maximum phrase length 20. For the language models, we used SRILM 5-gram language models (Stolcke, 2002) for all factors. For our word-based Baseline system, we trained a word-based model using the same Moses system with identical settings. For evaluation against segmented translation systems in segmented forms before word reconstruction, we also segmented the baseline system’s word-based output. All the BLEU scores reported are for lowercase evaluation. We did an initial evaluation of the segmented output translation for each system using the no5http://www.statmt.org/moses/ 36 TabSlBUeuna2gps:meulSipengLmta-e nioatedchMo12dme804-.lB8S714cL±oEr0eUs.6 9 S8up19Nre.358ofe498rUs9ntoihe supervised segmentation baseline model. m-BLEU indicates that the segmented output was evaluated against a segmented version of the reference (this measure does not have the same correlation with human judgement as BLEU) . No Uni indicates the segmented BLEU score without unigrams. tion of m-BLEU score (Luong et al. , 2010) where the BLEU score is computed by comparing the segmented output with a segmented reference translation. Table 2 shows the m-BLEU scores for various systems. We also show the m-BLEU score without unigrams, since over-segmentation could lead to artificially high m-BLEU scores. In fact, if we compare the relative improvement of our m-BLEU scores for the Unsup L-match system we see a relative improvement of 39.75% over the baseline. Luong et. al. (2010) report an m-BLEU score of 55.64% but obtain a relative improvement of 0.6% over their baseline m-BLEU score. We find that when using a good segmentation model, segmentation of the morphologically complex target language improves model performance over an unsegmented baseline (the confidence scores come from bootstrap resampling) . Table 3 shows the evaluation scores for all the baselines and the methods introduced in this paper using standard wordbased lowercase BLEU, WER and PER. We do TSCMaFBU(LubanRolpcesdFotu3lne-ipLr:gMdeLT-tms.al,Stc2ho0r1es:)l 1wB54 Le.r682E90c 27a9Us∗eBL-7 W46E3. U659478R6,1WE-7 TR412E. 847Ra1528nd TER. The ∗ indicates a statistically significant improvement o∗f BndLiEcaUte score over tchalel yB saisgenli nfice mntod imel.The boldface scores are the best performing scores per evaluation measure. better than (Luong et al. , 2010) , the previous best score for this task. We also show a better relative improvement over our baseline when compared to (Luong et al., 2010) : a relative improvement of 4.86% for Unsup L-match compared to our baseline word-based model, compared to their 1.65% improvement over their baseline word-based model. Our best performing method used unsupervised morphology with L-match (see Section 2.2) and the improvement is significant: bootstrap resampling provides a confidence margin of ±0.77 and a t-test (Collins ceot nafli.d , 2005) sahrogwined o significance aw ti-thte p = 0o.0ll0in1s. 3.1 Morphological Fluency Analysis To see how well the models were doing at getting morphology right, we examined several patterns of morphological behavior. While we wish to explore minimally supervised morphological MT models, and use as little language specific information as possible, we do want to use linguistic analysis on the output of our system to see how well the models capture essential morphological information in the target language. So, we ran the word-based baseline system, the segmented model (Unsup L-match) , and the prediction model (CRF-LM) outputs, along with the reference translation through the supervised morphological analyzer Omorfi (Pirinen and Listenmaa, 2007) . Using this analysis, we looked at a variety of linguistic constructions that might reveal patterns in morphological behavior. These were: (a) explicitly marked 37 noun forms, (b) noun-adjective case agreement, (c) subject-verb person/number agreement, (d) transitive object case marking, (e) postpositions, and (f) possession. In each of these categories, we looked for construction matches on a per-sentence level between the models’ output and the reference translation. Table 4 shows the models’ performance on the constructions we examined. In all of the categories, the CRF-LM model achieves the best precision score, as we explain below, while the Unsup L-match model most frequently gets the highest recall score. A general pattern in the most prevalent of these constructions is that the baseline tends to prefer the least marked form for noun cases (corresponding to the nominative) more than the reference or the CRF-LM model. The baseline leaves nouns in the (unmarked) nominative far more than the reference, while the CRF-LM model comes much closer, so it seems to fare better at explicitly marking forms, rather than defaulting to the more frequent unmarked form. Finnish adjectives must be marked with the same case as their head noun, while verbs must agree in person and number with their subject. We saw that in both these categories, the CRFLM model outperforms for precision, while the segmented model gets the best recall. In addition, Finnish generally marks direct objects of verbs with the accusative or the partitive case; we observed more accusative/partitive-marked nouns following verbs in the CRF-LM output than in the baseline, as illustrated by example (1) in Fig. 3. While neither translation picks the same verb as in the reference for the input ‘clarify,’ the CRFLM-output paraphrases it by using a grammatical construction of the transitive verb followed by a noun phrase inflected with the accusative case, correctly capturing the transitive construction. The baseline translation instead follows ‘give’ with a direct object in the nominative case. To help clarify the constructions in question, we have used Google Translate6 to provide back6 http://translate.google. com/ of occurrences per sentence, recall and F-score. also averaged The constructions over the various translations. are listed in descending P, R and F stand for precision, order of their frequency in the texts. The highlighted value in each column is the most accurate with respect to the reference value. translations of our MT output into English; to contextualize these back-translations, we have provided Google’s back-translation of the reference. The use of postpositions shows another difference between the models. Finnish postpositions require the preceding noun to be in the genitive or sometimes partitive case, which occurs correctly more frequently in the CRF-LM than the baseline. In example (2) in Fig. 3, all three translations correspond to the English text, ‘with the basque nationalists. ’ However, the CRF-LM output is more grammatical than the baseline, because not only do the adjective and noun agree for case, but the noun ‘baskien’ to which the postposition ‘kanssa’ belongs is marked with the correct genitive case. However, this well-formedness is not rewarded by BLEU, because ‘baskien’ does not match the reference. In addition, while Finnish may express possession using case marking alone, it has another construction for possession; this can disambiguate an otherwise ambiguous clause. This alternate construction uses a pronoun in the genitive case followed by a possessive-marked noun; we see that the CRF-LM model correctly marks this construction more frequently than the baseline. As example (3) in Fig. 3 shows, while neither model correctly translates ‘matkan’ (‘trip’) , the baseline’s output attributes the inessive ‘yhteydess’ (‘connection’) as belonging to ‘tulokset’ (‘results’) , and misses marking the possession linking it to ‘Commissioner Fischler’. Our manual evaluation shows that the CRF38 LM model is producing output translations that are more morphologically fluent than the wordbased baseline and the segmented translation Unsup L-match system, even though the word choices lead to a lower BLEU score overall when compared to Unsup L-match. 4 Related Work The work on morphology in MT can be grouped into three categories, factored models, segmented translation, and morphology generation. Factored models (Koehn and Hoang, 2007) factor the phrase translation probabilities over additional information annotated to each word, allowing for text to be represented on multiple levels of analysis. We discussed the drawbacks of factored models for our task in Section 2. 1. While (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) obtain improvements using factored models for translation into English, German, Spanish, and Czech, these models may be less useful for capturing long-distance dependencies in languages with much more complex morphological systems such as Finnish. In our experiments factored models did worse than the baseline. Segmented translation performs morphological analysis on the morphologically complex text for use in the translation model (Brown et al. , 1993; Goldwater and McClosky, 2005; de Gispert and Mari n˜o, 2008) . This method unpacks complex forms into simpler, more frequently occurring components, and may also increase the symmetry of the lexically realized content be(1) Input: ‘the charter we are to approve today both strengthens and gives visible shape to the common fundamental rights and values our community is to be based upon. ’ a. Reference: perusoikeuskirja , jonka t ¨an ¨a ¨an aiomme hyv a¨ksy ¨a , sek ¨a vahvistaa ett ¨a selvent a¨ a¨ (selvent ¨a a¨/VERB/ACT/INF/SG/LAT-clarify) niit a¨ (ne/PRONOUN/PL/PAR-them) yhteisi ¨a perusoikeuksia ja arvoja , joiden on oltava yhteis¨ omme perusta. Back-translation: ‘Charter of Fundamental Rights, which today we are going to accept that clarify and strengthen the common fundamental rights and values, which must be community based. ’ b. Baseline: perusoikeuskirja me hyv ¨aksymme t¨ an ¨a a¨n molemmat vahvistaa ja antaa (antaa/VERB/INF/SG/LATgive) n a¨kyv a¨ (n¨ aky a¨/VERB/ACT/PCP/SG/NOM-visible) muokata yhteist ¨a perusoikeuksia ja arvoja on perustuttava. Back-translation: ‘Charter today, we accept both confirm and modify to make a visible and common values, fundamental rights must be based. ’ c. CRF-LM: perusoikeuskirja on hyv a¨ksytty t ¨an ¨a ¨an , sek ¨a vahvistaa ja antaa (antaa/VERB/ACT/INF/SG/LAT-give) konkreettisen (konkreettinen/ADJECTIVE/SG/GEN,ACC-concrete) muodon (muoto/NOUN/SG/GEN,ACCshape) yhteisi ¨a perusoikeuksia ja perusarvoja , yhteis¨ on on perustuttava. Back-translation: ‘Charter has been approved today, and to strengthen and give concrete shape to the common basic rights and fundamental values, the Community must be based. ’ (2) Input: ‘with the basque nationalists’ a. Reference: baskimaan kansallismielisten kanssa basque-SG/NOM+land-SG/GEN,ACC nationalists-PL/GEN with-POST b. Baseline: baskimaan kansallismieliset kanssa basque-SG/NOM-+land-SG/GEN,ACC kansallismielinen-PL/NOM,ACC-nationalists POST-with c. CRF-LM: kansallismielisten baskien kanssa nationalists-PL/GEN basques-PL/GEN with-POST (3) Input: ‘and in this respect we should value the latest measures from commissioner fischler , the results of his trip to morocco on the 26th of last month and the high level meetings that took place, including the one with the king himself’ a. Reference: ja t ¨ass¨ a mieless ¨a osaamme my¨ os arvostaa komission j¨ asen fischlerin viimeisimpi ¨a toimia , jotka ovat h a¨nen (h¨ anen/GEN-his) marokkoon 26 lokakuuta tekemns (tekem¨ ans ¨a/POSS-his) matkan (matkan/GENtour) ja korkean tason kokousten jopa itsens¨ a kuninkaan kanssa tulosta Back-translation: ‘and in this sense we can also appreciate the Commissioner Fischler’s latest actions, which are his to Morocco 26 October trip to high-level meetings and even the king himself with the result b. Baseline: ja t ¨ass¨ a yhteydess a¨ olisi arvoa viimeisin toimia komission j¨ asen fischler , tulokset monitulkintaisia marokon yhteydess a¨ (yhteydess/INE-connection) , ja viime kuussa pidettiin korkean tason kokouksissa , mukaan luettuna kuninkaan kanssa Back-translation: ‘and in this context would be the value of the last act, Commissioner Fischler, the results of the Moroccan context, ambiguous, and last month held high level meetings, including with the king’ c. CRF-LM: ja t ¨ass¨ a yhteydess a¨ meid ¨an olisi lis ¨aarvoa viimeist ¨a toimenpiteit a¨ kuin komission j¨ asen fischler , ett a¨ h a¨nen (h¨ anen/GEN-his) kokemuksensa (kokemuksensa/POSS-experience) marokolle (marokolle-Moroccan) viime kuun 26 ja korkean tason tapaamiset j¨ arjestettiin, kuninkaan kanssa Back-translation: ‘and in this context, we should value the last measures as the Commissioner Fischler, that his experience in Morocco has on the 26th and high-level meetings took place, including with the king. ’ Figure 3: Morphological fluency analysis (see Section 3. 1) . tween source and target. In a somewhat orthogonal approach to ours, (Ma et al. , 2007) use alignment of a parallel text to pack together adjacent segments in the alignment output, which are then fed back to the word aligner to bootstrap an improved alignment, which is then used in the translation model. We compared our results against (Luong et al. , 2010) in Table 3 since their results are directly comparable to ours. They use a segmented phrase table and language model along with the word-based versions in the decoder and in tuning a Finnish target. Their approach requires segmented phrases 39 to match word boundaries, eliminating morphologically productive phrases. In their work a segmented language model can score a translation, but cannot insert morphology that does not show source-side reflexes. In order to perform a similar experiment that still allowed for morphologically productive phrases, we tried training a segmented translation model, the output of which we stitched up in tuning so as to tune to a word-based reference. The goal of this experiment was to control the segmented model’s tendency to overfit by rewarding it for using correct whole-word forms. However, we found that this approach was less successful than using the segmented reference in tuning, and could not meet the baseline (13.97% BLEU best tuning score, versus 14.93% BLEU for the baseline best tuning score) . Previous work in segmented translation has often used linguistically motivated morphological analysis selectively applied based on a language-specific heuristic. A typical approach is to select a highly inflecting class of words and segment them for particular morphology (de Gispert and Mari n˜o, 2008; Ramanathan et al. , 2009) . Popovi¸ c and Ney (2004) perform segmentation to reduce morphological complexity of the source to translate into an isolating target, reducing the translation error rate for the English target. For Czech-to-English, Goldwater and McClosky (2005) lemmatized the source text and inserted a set of ‘pseudowords’ expected to have lexical reflexes in English. Minkov et. al. (2007) and Toutanova et. al. (2008) use a Maximum Entropy Markov Model for morphology generation. The main drawback to this approach is that it removes morphological information from the translation model (which only uses stems) ; this can be a problem for languages in which morphology ex- presses lexical content. de Gispert (2008) uses a language-specific targeted morphological classifier for Spanish verbs to avoid this issue. Talbot and Osborne (2006) use clustering to group morphological variants of words for word alignments and for smoothing phrase translation tables. Habash (2007) provides various methods to incorporate morphological variants of words in the phrase table in order to help recognize out of vocabulary words in the source language. 5 Conclusion and Future Work We found that using a segmented translation model based on unsupervised morphology induction and a model that combined morpheme segments in the translation model with a postprocessing morphology prediction model gave us better BLEU scores than a word-based baseline. Using our proposed approach we obtain better scores than the state of the art on the EnglishFinnish translation task (Luong et al. , 2010) : from 14.82% BLEU to 15.09%, while using a 40 simpler model. We show that using morphological segmentation in the translation model can improve output translation scores. We also demonstrate that for Finnish (and possibly other agglutinative languages) , phrase-based MT benefits from allowing the translation model access to morphological segmentation yielding productive morphological phrases. Taking advantage of linguistic analysis of the output we show that using a post-processing morphology generation model can improve translation fluency on a sub-word level, in a manner that is not captured by the BLEU word-based evaluation measure. In order to help with replication of the results in this paper, we have run the various morphological analysis steps and created the necessary training, tuning and test data files needed in order to train, tune and test any phrase-based machine translation system with our data. The files can be downloaded from natlang. cs.sfu. ca. In future work we hope to explore the utility of phrases with productive morpheme boundaries and explore why they are not used more pervasively in the decoder. Evaluation measures for morphologically complex languages and tun- ing to those measures are also important future work directions. Also, we would like to explore a non-pipelined approach to morphological preand post-processing so that a globally trained model could be used to remove the target side morphemes that would improve the translation model and then predict those morphemes in the target language. Acknowledgements This research was partially supported by NSERC, Canada (RGPIN: 264905) and a Google Faculty Award. We would like to thank Christian Monson, Franz Och, Fred Popowich, Howard Johnson, Majid Razmara, Baskaran Sankaran and the anonymous reviewers for their valuable comments on this work. We would particularly like to thank the developers of the open-source Moses machine translation toolkit and the Omorfi morphological analyzer for Finnish which we used for our experiments. References Eleftherios Avramidis and Philipp Koehn. 2008. Enriching morphologically poor languages for statistical machine translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, page 763?770, Columbus, Ohio, USA. Association for Computational Linguistics. Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2) :263–31 1. Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 224–232, Columbus, Ohio, June. Association for Computational Linguistics. Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation. In Proceedings of 43rd Annual Meeting of the Association for Computational Linguistics (A CL05). Association for Computational Linguistics. Mathias Creutz and Krista Lagus. 2005. Inducing the morphological lexicon of a natural language from unannotated text. In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reason- ing (AKRR ’05), pages 106–113, Espoo, Finland. Mathias Creutz and Krista Lagus. 2006. Morfessor in the morpho challenge. In Proceedings of the PASCAL Challenge Workshop on Unsupervised Segmentation of Words into Morphemes. Adri ´a de Gispert and Jos e´ Mari n˜o. 2008. On the impact of morphology in English to Spanish statistical MT. Speech Communication, 50(11-12) . Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 676–683, Vancouver, B.C. , Canada. Association for Computational Linguistics. Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 868–876, Prague, Czech Republic. Association for Computational Linguistics. 41 Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In A CL ‘07: Proceedings of the 45th Annual Meeting of the A CL on Interactive Poster and Demonstration Sessions, pages 177–108, Prague, Czech Republic. Association for Computational Linguistics. Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X, pages 79–86, Phuket, Thailand. Association for Computational Linguistics. John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282–289, San Francisco, California, USA. Association for Computing Machinery. Minh-Thang Luong, Preslav Nakov, and Min-Yen Kan. 2010. A hybrid morpheme-word representation for machine translation of morphologically rich languages. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 148–157, Cambridge, Massachusetts. Association for Computational Linguistics. Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304–311, Prague, Czech Republic. Association for Computational Linguistics. Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (A CL07), pages 128–135, Prague, Czech Republic. Association for Computational Linguistics. Christian Monson. 2008. Paramor and morpho challenge 2008. In Lecture Notes in Computer Science: Workshop of the Cross-Language Evaluation Forum (CLEF 2008), Revised Selected Papers. Habash Nizar. 2007. Four techniques for online handling of out-of-vocabulary words in arabic-english statistical machine translation. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics, Columbus, Ohio. Association for Computational Linguistics. Kishore Papineni, Salim Roukos, Todd Ward, and Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics A CL, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. Tommi Pirinen and Inari Listenmaa. 2007. Omorfi morphological analzer. http://gna.org/projects/omorfi. Maja Popovi¸ c and Hermann Ney. 2004. Towards the use of word stems and suffixes for statistiWei jing cal machine translation. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC), pages 1585–1588, Lisbon, Portugal. European Language Resources Association (ELRA) . Ananthakrishnan Ramanathan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and morphology: Addressing the crux of the fluency problem in EnglishHindi SMT. In Proceedings of the Joint Conference of the 4 7th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, pages 800–808, Suntec, Singapore. Association for Computational Linguistics. Andreas Stolcke. 2002. Srilm – an extensible language modeling toolkit. 7th International Conference on Spoken Language Processing, 3:901–904. David Talbot and Miles Osborne. 2006. Modelling lexical redundancy for machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 969–976, Sydney, Australia, July. Association for Computational Linguistics. Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying morphology generation models to machine translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 514–522, Columbus, Ohio, USA. Association for Computational Linguistics. Mei Yang and Katrin Kirchhoff. 2006. Phrase-based backoff models for machine translation of highly inflected languages. In Proceedings of the European Chapter of the Association for Computational Linguistics, pages 41–48, Trento, Italy. Association for Computational Linguistics. 42</p><p>4 0.65154541 <a title="318-lsi-4" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>Author: Daniel Hewlett ; Paul Cohen</p><p>Abstract: Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</p><p>5 0.64001822 <a title="318-lsi-5" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>Author: John Lee ; Jason Naradowsky ; David A. Smith</p><p>Abstract: Most previous studies of morphological disambiguation and dependency parsing have been pursued independently. Morphological taggers operate on n-grams and do not take into account syntactic relations; parsers use the “pipeline” approach, assuming that morphological information has been separately obtained. However, in morphologically-rich languages, there is often considerable interaction between morphology and syntax, such that neither can be disambiguated without the other. In this paper, we propose a discriminative model that jointly infers morphological properties and syntactic structures. In evaluations on various highly-inflected languages, this joint model outperforms both a baseline tagger in morphological disambiguation, and a pipeline parser in head selection.</p><p>6 0.62985897 <a title="318-lsi-6" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>7 0.60394269 <a title="318-lsi-7" href="./acl-2011-Translating_from_Morphologically_Complex_Languages%3A_A_Paraphrase-Based_Approach.html">310 acl-2011-Translating from Morphologically Complex Languages: A Paraphrase-Based Approach</a></p>
<p>8 0.59228235 <a title="318-lsi-8" href="./acl-2011-Exploiting_Morphology_in_Turkish_Named_Entity_Recognition_System.html">124 acl-2011-Exploiting Morphology in Turkish Named Entity Recognition System</a></p>
<p>9 0.58891702 <a title="318-lsi-9" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>10 0.58765233 <a title="318-lsi-10" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>11 0.58169436 <a title="318-lsi-11" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>12 0.57223636 <a title="318-lsi-12" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>13 0.57056338 <a title="318-lsi-13" href="./acl-2011-Using_Deep_Morphology_to_Improve_Automatic_Error_Detection_in_Arabic_Handwriting_Recognition.html">329 acl-2011-Using Deep Morphology to Improve Automatic Error Detection in Arabic Handwriting Recognition</a></p>
<p>14 0.56888509 <a title="318-lsi-14" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>15 0.55580986 <a title="318-lsi-15" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>16 0.55517435 <a title="318-lsi-16" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>17 0.53489065 <a title="318-lsi-17" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>18 0.53223282 <a title="318-lsi-18" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>19 0.53059214 <a title="318-lsi-19" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>20 0.52953303 <a title="318-lsi-20" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.02), (11, 0.073), (17, 0.051), (25, 0.01), (26, 0.027), (31, 0.018), (37, 0.091), (39, 0.08), (41, 0.073), (45, 0.012), (53, 0.022), (55, 0.068), (59, 0.053), (63, 0.029), (72, 0.06), (88, 0.011), (91, 0.045), (96, 0.155)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93347645 <a title="318-lda-1" href="./acl-2011-Finding_Deceptive_Opinion_Spam_by_Any_Stretch_of_the_Imagination.html">136 acl-2011-Finding Deceptive Opinion Spam by Any Stretch of the Imagination</a></p>
<p>Author: Myle Ott ; Yejin Choi ; Claire Cardie ; Jeffrey T. Hancock</p><p>Abstract: Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam—fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.</p><p>2 0.92312574 <a title="318-lda-2" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>Author: Alexander Volokh ; Gunter Neumann</p><p>Abstract: Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 1</p><p>same-paper 3 0.91503572 <a title="318-lda-3" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>Author: Jason Naradowsky ; Kristina Toutanova</p><p>Abstract: This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</p><p>4 0.90462881 <a title="318-lda-4" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>Author: Greg Durrett ; Dan Klein</p><p>Abstract: We investigate the empirical behavior of ngram discounts within and across domains. When a language model is trained and evaluated on two corpora from exactly the same domain, discounts are roughly constant, matching the assumptions of modified Kneser-Ney LMs. However, when training and test corpora diverge, the empirical discount grows essentially as a linear function of the n-gram count. We adapt a Kneser-Ney language model to incorporate such growing discounts, resulting in perplexity improvements over modified Kneser-Ney and Jelinek-Mercer baselines.</p><p>5 0.9031505 <a title="318-lda-5" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>Author: Ines Rehbein ; Josef Ruppenhofer</p><p>Abstract: Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when ap- plied to noisy data.</p><p>6 0.90129828 <a title="318-lda-6" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>7 0.89992261 <a title="318-lda-7" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>8 0.8970139 <a title="318-lda-8" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>9 0.89586246 <a title="318-lda-9" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>10 0.89217019 <a title="318-lda-10" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>11 0.89089859 <a title="318-lda-11" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>12 0.88954806 <a title="318-lda-12" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>13 0.88916463 <a title="318-lda-13" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>14 0.88827139 <a title="318-lda-14" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>15 0.88808322 <a title="318-lda-15" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>16 0.88793969 <a title="318-lda-16" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>17 0.88774097 <a title="318-lda-17" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>18 0.88737071 <a title="318-lda-18" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>19 0.88729572 <a title="318-lda-19" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>20 0.88666242 <a title="318-lda-20" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
