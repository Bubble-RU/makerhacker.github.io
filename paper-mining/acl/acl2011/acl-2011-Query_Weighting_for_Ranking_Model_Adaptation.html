<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>256 acl-2011-Query Weighting for Ranking Model Adaptation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-256" href="#">acl2011-256</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>256 acl-2011-Query Weighting for Ranking Model Adaptation</h1>
<br/><p>Source: <a title="acl-2011-256-pdf" href="http://aclweb.org/anthology//P/P11/P11-1012.pdf">pdf</a></p><p>Author: Peng Cai ; Wei Gao ; Aoying Zhou ; Kam-Fai Wong</p><p>Abstract: We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.</p><p>Reference: <a title="acl-2011-256-reference" href="../acl2011_reference/acl-2011-Query_Weighting_for_Ranking_Model_Adaptation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hinkistry of Education, China  ,  Abstract We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. [sent-13, score-1.424]
</p><p>2 Query weighting is a key step in ranking model adaptation. [sent-14, score-0.572]
</p><p>3 As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. [sent-15, score-1.946]
</p><p>4 The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. [sent-17, score-2.956]
</p><p>5 This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. [sent-18, score-1.323]
</p><p>6 The second measures the similarity between the source  query and each target query, and then combines these fine-grained similarity values for its importance estimation. [sent-19, score-0.952]
</p><p>7 0 data set demonstrate that query weighting significantly outperforms document instance weighting methods. [sent-21, score-1.619]
</p><p>8 This motivated the popular domain adaptation solution based on instance weighting, which assigns larger weights to those transferable instances so that the model trained on the source domain can adapt more effectively to the target domain (Jiang and Zhai, 2007). [sent-37, score-0.912]
</p><p>9 Existing instance weighting schemes mainly focus on the adaptation problem for classification (Zadrozny, 2004; Huang et al. [sent-38, score-0.661]
</p><p>10 Although instance weighting scheme may be applied to documents for ranking model adaptation, the difference between classification and learning to rank should be highlighted to take careful consideration. [sent-41, score-0.772]
</p><p>11 , 2007) to take the whole query as a learning object. [sent-45, score-0.565]
</p><p>12 To avoid losing this information, query weighting takes the query as a whole and directly measures  its importance. [sent-49, score-1.55]
</p><p>13 Inspired by the principle of listwise approach, we hypothesize that the importance weighting for ranking model adapta-  tion could be done better at query level rather than document level. [sent-51, score-1.448]
</p><p>14 Figure 1 demonstrates the difference between instance weighting and query weighting, where there are two queries qs1 and qs2 in the source domain and qt1 and qt2 in the target domain, respectively, and each query has three retrieved documents. [sent-52, score-2.102]
</p><p>15 It is worth noting that the information about which document instances belong to the same query is lost. [sent-54, score-0.771]
</p><p>16 To avoid this information loss, query weighting scheme shown as Figure 1(b) directly measures importance weight at query level. [sent-55, score-1.707]
</p><p>17 Instance weighting makes the importance estimation of document instances inaccurate when documents of the same source query are similar to the documents from different target queries. [sent-56, score-1.614]
</p><p>18 No matter what weighting schemes are used, it makes sense to assign high weights to source queries qs1 and qs2 because they are similar to target queries qt1 and qt2, respectively. [sent-58, score-1.001]
</p><p>19 it’s not quite similar to any of qt1 and qt2 at query level, meaning that the ranking knowledge from qs3 is different from that of qt1 and qt2 and thus less useful for the transfer to the target domain. [sent-60, score-0.858]
</p><p>20 Unfor-  tunately, the three source queries qs1, qs2 and qs3 would be weighted equally by document instance weighting scheme. [sent-61, score-0.88]
</p><p>21 The reason is that all of their documents are similar to the two document instances in target domain despite the fact that the documents of qs3 correspond to their counterparts from different target queries. [sent-62, score-0.647]
</p><p>22 Therefore, we should consider the source query as a whole and directly measure the query importance. [sent-63, score-1.263]
</p><p>23 However, it’s not trivial to directly estimate a query’s weight because a query is essentially provided as a matrix where each row represents a vector of document features. [sent-64, score-0.819]
</p><p>24 2  Instance Weighting Scheme Review  The basic idea of instance weighting is to put larger weights on source instances which are more similar to target domain. [sent-66, score-0.828]
</p><p>25 In this work, we also focus on weighting source queries only using unlabeled target queries. [sent-73, score-0.77]
</p><p>26 With the domain separator, the probability that a source instance is classified to target domain can be used as the importance weight. [sent-77, score-0.642]
</p><p>27 Other instance weighting methods were proposed for the sample selection bias or covariate shift in the more general setting of classifier learning (Shimodaira, 2000; Sugiyama et al. [sent-78, score-0.534]
</p><p>28 For using instance weighting in pairwise ranking algorithms, the weights of document instances should be transformed into those of document pairs (Gao et al. [sent-89, score-1.093]
</p><p>29 To consider query factor, query weight was furt∗hewr estimated as the average value of the weights over all the pairs, i. [sent-92, score-1.234]
</p><p>30 , wq = M1 ∑i,j wij, where M is the number of pairs in query ∑q. [sent-94, score-0.592]
</p><p>31 Additionally, to  take the advantage of both query and document information, a probabilistic weighting for ⟨xi, xj⟩ was mforomdealteiod by wq ∗ wij. [sent-95, score-1.149]
</p><p>32 Through tinheg transformation, instance weighting schemes for classification can be applied to ranking model adaptation. [sent-96, score-0.684]
</p><p>33 3  Query Weighting  In this section, we extend instance weighting to directly estimate query importance for more effective ranking model adaptation. [sent-97, score-1.354]
</p><p>34 We present two query weighting methods from different perspectives. [sent-98, score-0.985]
</p><p>35 Note that although our methods are based on domain separator scheme, other instance weighting schemes such as KLIEP (Sugiyama et al. [sent-99, score-0.777]
</p><p>36 1 Query Weighting by Document Feature Aggregation Our first query weighting method is inspired by the recent work on local learning for ranking (Geng et al. [sent-102, score-1.137]
</p><p>37 The query can be compressed into a query feature vector, where each feature value is obtained by the aggregate of its corresponding features of all documents in the query. [sent-105, score-1.254]
</p><p>38 B ia asnedd on dtehen ag-  within each query, we can  use a domain separator to directly weight the source queries with the set of queries from both domains. [sent-107, score-0.683]
</p><p>39 P(qsi ∈ Dt), which can be used as the importance of qsi rela∈tiv De to the target domain. [sent-110, score-0.499]
</p><p>40 From step 1to 9, Ds′ and Dt′ are constructed using query feature vectors from source and target domains. [sent-111, score-0.808]
</p><p>41 The distance of the query feature vector qsi from Hst are transformed to the probability P(qsi ∈ Dt) using a sigmoid function (Platt and Platt, 1999). [sent-113, score-0.922]
</p><p>42 2  Query Weighting by Comparing Queries across Domains Although the query feature vector in algorithm 1can approximate a query by aggregating its documents’ features, it potentially fails to capture important feature information due to the averaging effect during the aggregation. [sent-115, score-1.234]
</p><p>43 For example, the merit of features in some influential documents may be canceled out in the mean-variance calculation, resulting in many distorted feature values in the query feature vector that hurts the accuracy of query classification hyperplane. [sent-116, score-1.315]
</p><p>44 This urges us to propose another query 115 weighting method from a different perspective of  query similarity. [sent-117, score-1.55]
</p><p>45 Intuitively, the importance of a source query to the target domain is determined by its overall similarity to every target query. [sent-118, score-1.145]
</p><p>46 Based on this intuition, we leverage domain separator to measure the similarity between a source query and each one of the target queries, where an individual domain separator is created for each pair of queries. [sent-119, score-1.328]
</p><p>47 We estimate the weight of a source query using algorithm 2. [sent-120, score-0.752]
</p><p>48 Note that we assume document instances in the same query are conditionally independent and all queries are independent of each other. [sent-121, score-0.913]
</p><p>49 In step 3, Dq′si is constructed by all the document instances { x⃗k} in query qsi uwctitehd t bhye adlolm thaein do lcaubmele ys. [sent-122, score-1.059]
</p><p>50 Fsotarn ceaesch { target query qtj , we use the classification hyperplane Hij to estimate P(⃗ xk ∈ Dq′tj), i. [sent-123, score-0.933]
</p><p>51 the probability that each document xk of qsi is classified into the document set of qtj (step 8). [sent-125, score-0.73]
</p><p>52 Finally, the probability of qsi belonging to the target domain P(qsi ∈ Dt) is calculated at step 11. [sent-127, score-0.517]
</p><p>53 4  Ranking Model Adaptation via Query Weighting  To adapt the source ranking model to the target domain, we need to incorporate query weights into existing ranking algorithms. [sent-129, score-1.131]
</p><p>54 Note that query weights can be integrated with either pairwise or listwise algorithms. [sent-130, score-0.733]
</p><p>55 For pairwise algorithms, a straightforward way is to assign the query weight to all the document pairs associated with this query. [sent-131, score-0.799]
</p><p>56 However, document instance weighting cannot be appropriately utilized in listwise approach. [sent-132, score-0.701]
</p><p>57 In order to compare query  weighting with document instance weighting, we need to fairly apply them for the same approach of ranking. [sent-133, score-1.199]
</p><p>58 Therefore, we choose pairwise approach to incorporate query weighting. [sent-134, score-0.612]
</p><p>59 Let’s assume there are m queries in the data set of source domain, and for each query qi there are ℓ(qi) number of meaningful document pairs that can —  116 be constructed based on the ground truth rank labels. [sent-137, score-1.066]
</p><p>60 )+, Equation 1 can be turned to the following form:  minλ|| w⃗||2+∑m∑ℓ(∑jq=i)1(1 − zij∗ f( w⃗, x⃗qji(1)− x⃗ qji(2) )+ ∑i=1  (2))  Let IW(qi) represent the importance weight of source query qi. [sent-146, score-0.826]
</p><p>61 Equation 2 is extended for integrating the query weight into the loss function in a straightforward way: min λ|| w⃗||2+ ℓ∑(qi)  ∑m  ∑IW(qi) ∗ ∑i= ∑1  ∑ (1  − zij  ∗ f( w⃗, x⃗jq(i1) −⃗ x jq(i2)))+  j∑= ∑1  where IW(. [sent-147, score-0.708]
</p><p>62 5  Evaluation  We evaluated the proposed two query weighting methods on TREC-2003 and TREC-2004 web track datasets, which were released through LETOR3. [sent-149, score-0.985]
</p><p>63 Originally, different query tasks were defined on different parts of data in the collection, which can be considered as different domains for us. [sent-152, score-0.594]
</p><p>64 Our goal is to demonstrate that query weighting can be more effective than the state-of-the-art document instance weighting. [sent-154, score-1.199]
</p><p>65 1 Datasets and Setup Three query tasks were defined in TREC-2003 and TREC-2004 web track, which are home page finding (HP), named page finding (NP) and topic distillation (TD) (Voorhees, 2003; Voorhees, 2004). [sent-156, score-0.565]
</p><p>66 In this dataset, each document instance is represented by 64 features, including low-level features such as term frequency, inverse document frequency and document length, and high-level features such as BM25,  language-modeling, PageRank and HITS. [sent-157, score-0.488]
</p><p>67 The baseline ranking model is an RSVM directly trained on the source domain without using any weighting methods, denoted as no-weight. [sent-159, score-0.839]
</p><p>68 We implemented two weighting measures based on domain separator and Kullback-Leibler divergence, referred to DS and KL, respectively. [sent-160, score-0.665]
</p><p>69 In DS measure, three document instance weighting methods based on probability principle (Gao et al. [sent-161, score-0.634]
</p><p>70 Our proposed query weighting methods are denoted by query-aggr and querycomp, corresponding to document feature aggregation in query and query comparison across domains, respectively. [sent-164, score-2.396]
</p><p>71 All ranking models above were trained only on source domain training data and the labeled data of target domain was just used for testing. [sent-165, score-0.61]
</p><p>72 T-test on MAP indicates that the improvement of query-aggr over no-weight is statistically significant on two adaptation tasks while the improvement of document instance weighting over no-weight is statistically significant only on one task. [sent-178, score-0.763]
</p><p>73 This demonstrates the effectiveness of query ModelWeighting methodHP03 to TD03HP04 to TD04NP03 to TD03NP04 to TD04  doc-pair, doc-avg fa MndA dPoc f-ocro HmPb/N, respectively. [sent-180, score-0.565]
</p><p>74 e †n,c ‡e, ♯le avnedl i sb oseldtf aatc e95 i%nd  weighting compared to document instance weighting. [sent-182, score-0.694]
</p><p>75 By contrast, more accurate query weights can be achieved by the more fine-grained similarity measure between the source query and all target queries in algorithm 2. [sent-185, score-1.599]
</p><p>76 Specifically, after document feature aggregation, the number of query feature vectors in all adaptation tasks is no more than 150 in source and target domains. [sent-189, score-1.109]
</p><p>77 As each query contains 1000 documents, they seemed to provide query-comp enough samples for achieving reasonable estimation of the density functions in both domains. [sent-191, score-0.599]
</p><p>78 2 Adaptation from TD to HP/NP To further validate the effectiveness  of query  weighting, we also conducted adaptation from TD to HP and TD to NP . [sent-194, score-0.694]
</p><p>79 We can see that document instance weighting 118 schemes including doc-pair, doc-avg and doc-comb can not outperform no-weight based on MAP measure. [sent-196, score-0.669]
</p><p>80 The reason is that each query in TD has 1000 retrieved documents in which 10-15 documents are relevant whereas each query in HP or NP only consists 1-2 relevant documents. [sent-197, score-1.238]
</p><p>81 Since query weighting method directly estimates the query importance instead of document instance importance, both query-aggr and querycomp can avoid such kind of negative influence that is inevitable in the three document instance weighting methods. [sent-200, score-2.535]
</p><p>82 Query weighting assigns each source query with a weight value. [sent-204, score-1.139]
</p><p>83 Note that it’s not meaningful to directly compare absolute weight values between query-aggr and query-comp because source query weights from distinct weighting methods have different range and scale. [sent-205, score-1.193]
</p><p>84 However, it is feasible to compare the weights with the same weighting method. [sent-206, score-0.474]
</p><p>85 Intuitively, if the ranking model learned from a source query can work well in target domain, it should get high weight. [sent-207, score-0.925]
</p><p>86 from queries qs1 and qs2 respectively, and fq1s per-  forms better than fq2s , then the source query weight of qs1 should be higher than that of qs2. [sent-211, score-0.861]
</p><p>87 For further analysis, we compare the weight values between each source query pair, for which we trained RSVM on each source query and evaluated the learned model on test data from target domain. [sent-212, score-1.492]
</p><p>88 For comparison, we also ranked these queries according to randomly generated query weights, which is denoted as query-rand in addition to queryaggr and query-comp. [sent-217, score-0.775]
</p><p>89 The Kendall’s τ = is used to measure the correlation (Kendall, 1970), where P is the number of concordant query pairs and Q is the number of discordant pairs. [sent-218, score-0.594]
</p><p>90 4 Efficiency In the situation where there are large scale data in source and target domains, how to efficiently weight a source query is another interesting problem. [sent-227, score-0.954]
</p><p>91 Without the loss of generality, we reported the weighting time of doc-pair, query-aggr and query-comp from  adaptation from TD to HP using DS measure. [sent-228, score-0.575]
</p><p>92 As shown in table 6, query-aggr can efficiently weight query using query feature vector. [sent-230, score-1.242]
</p><p>93 The reason is two-fold: one is the operation of query document aggregation can be done very fast, and the other is there are 1000 documents in each query ofTD or HP, which means that the compression ratio is 1000: 1. [sent-231, score-1.392]
</p><p>94 And query-comp uses a divide-and-conquer method to measure the similarity of source query to each target query, and then efficiently combine these Weighting methodHP03 to TD03HP04 to TD04NP03 to TD03NP04 to TD04  q qu ue er ry y- -racoagngmdrp0 0. [sent-234, score-0.865]
</p><p>95 , 2008b), the parameters of ranking model trained on the source domain was adjusted with the small set of labeled data in the target domain. [sent-267, score-0.485]
</p><p>96 , 2010) studied instance weighting based on domain separator for learning to rank by only using training data from source domain. [sent-276, score-0.915]
</p><p>97 In this work, we propose to directly measure the query importance instead of document instance importance by considering information at both levels. [sent-277, score-1.022]
</p><p>98 7  Conclusion  We introduced two simple yet effective query weighting methods for ranking model adaptation. [sent-278, score-1.137]
</p><p>99 The first represents a set of document instances within the same query as a query feature vector,  and then directly measure the source query importance to the target domain. [sent-279, score-2.28]
</p><p>100 The second measures the similarity between a source query and each target query, and then combine the fine-grained similarity values to estimate its importance to target domain. [sent-280, score-1.089]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('query', 0.565), ('weighting', 0.42), ('qsi', 0.288), ('ranking', 0.152), ('queries', 0.142), ('document', 0.137), ('qtj', 0.136), ('adaptation', 0.129), ('domain', 0.125), ('td', 0.122), ('separator', 0.12), ('importance', 0.107), ('target', 0.104), ('source', 0.104), ('geng', 0.091), ('rsvm', 0.091), ('dt', 0.082), ('sugiyama', 0.08), ('instance', 0.077), ('rweight', 0.076), ('hp', 0.073), ('gao', 0.073), ('ds', 0.072), ('aggregation', 0.071), ('rank', 0.069), ('instances', 0.069), ('listwise', 0.067), ('dq', 0.067), ('zij', 0.067), ('hyperplane', 0.063), ('hst', 0.061), ('jq', 0.061), ('documents', 0.054), ('weights', 0.054), ('iws', 0.053), ('jn', 0.053), ('chen', 0.052), ('weight', 0.05), ('zadrozny', 0.049), ('qi', 0.049), ('pairwise', 0.047), ('kendall', 0.046), ('herbrich', 0.045), ('hij', 0.045), ('qis', 0.045), ('qji', 0.045), ('qjt', 0.045), ('rmap', 0.045), ('kl', 0.045), ('xjq', 0.04), ('im', 0.04), ('voorhees', 0.039), ('qin', 0.038), ('denoted', 0.038), ('transfer', 0.037), ('covariate', 0.037), ('similarity', 0.036), ('ij', 0.036), ('yan', 0.036), ('schemes', 0.035), ('feature', 0.035), ('density', 0.034), ('vector', 0.034), ('cu', 0.033), ('estimate', 0.033), ('xk', 0.032), ('wi', 0.031), ('aoying', 0.03), ('avnedl', 0.03), ('depin', 0.03), ('dpoc', 0.03), ('kliep', 0.03), ('mnda', 0.03), ('modelweighting', 0.03), ('oseldtf', 0.03), ('queryaggr', 0.03), ('querycomp', 0.03), ('hang', 0.03), ('tao', 0.03), ('jiang', 0.03), ('blitzer', 0.029), ('domains', 0.029), ('platt', 0.029), ('burges', 0.029), ('iw', 0.029), ('rc', 0.029), ('measure', 0.029), ('cao', 0.028), ('zhai', 0.028), ('map', 0.028), ('distorted', 0.027), ('ial', 0.027), ('wq', 0.027), ('efficiently', 0.027), ('loss', 0.026), ('yt', 0.026), ('ys', 0.025), ('toy', 0.025), ('shimodaira', 0.025), ('gang', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="256-tfidf-1" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>Author: Peng Cai ; Wei Gao ; Aoying Zhou ; Kam-Fai Wong</p><p>Abstract: We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.</p><p>2 0.36780092 <a title="256-tfidf-2" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>Author: Michael Bendersky ; W. Bruce Croft ; David A. Smith</p><p>Abstract: W. Bruce Croft Dept. of Computer Science University of Massachusetts Amherst, MA cro ft @ c s .uma s s .edu David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@ c s .umas s .edu articles or web pages). As previous research shows, these differences severely limit the applicability of Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an impor- tant part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.</p><p>3 0.31463423 <a title="256-tfidf-3" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>4 0.30573621 <a title="256-tfidf-4" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>Author: Marius Pasca</p><p>Abstract: The role of search queries, as available within query sessions or in isolation from one another, in examined in the context of ranking the class labels (e.g., brazilian cities, business centers, hilly sites) extracted from Web documents for various instances (e.g., rio de janeiro). The co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance. Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts.</p><p>5 0.24860001 <a title="256-tfidf-5" href="./acl-2011-Jigs_and_Lures%3A_Associating_Web_Queries_with_Structured_Entities.html">181 acl-2011-Jigs and Lures: Associating Web Queries with Structured Entities</a></p>
<p>Author: Patrick Pantel ; Ariel Fuxman</p><p>Abstract: We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query- product associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision.</p><p>6 0.24110298 <a title="256-tfidf-6" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<p>7 0.14766875 <a title="256-tfidf-7" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>8 0.14392661 <a title="256-tfidf-8" href="./acl-2011-Is_Machine_Translation_Ripe_for_Cross-Lingual_Sentiment_Classification%3F.html">179 acl-2011-Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?</a></p>
<p>9 0.13615853 <a title="256-tfidf-9" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>10 0.12790175 <a title="256-tfidf-10" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>11 0.12707053 <a title="256-tfidf-11" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>12 0.12441836 <a title="256-tfidf-12" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>13 0.11959875 <a title="256-tfidf-13" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>14 0.11669672 <a title="256-tfidf-14" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>15 0.11657692 <a title="256-tfidf-15" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>16 0.11026888 <a title="256-tfidf-16" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>17 0.10531395 <a title="256-tfidf-17" href="./acl-2011-Effective_Measures_of_Domain_Similarity_for_Parsing.html">109 acl-2011-Effective Measures of Domain Similarity for Parsing</a></p>
<p>18 0.10173581 <a title="256-tfidf-18" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>19 0.10071705 <a title="256-tfidf-19" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>20 0.098834127 <a title="256-tfidf-20" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.206), (1, 0.109), (2, -0.068), (3, 0.136), (4, -0.13), (5, -0.332), (6, -0.135), (7, -0.229), (8, 0.208), (9, 0.007), (10, 0.204), (11, -0.049), (12, -0.016), (13, -0.009), (14, 0.043), (15, -0.011), (16, -0.064), (17, 0.041), (18, -0.032), (19, -0.023), (20, -0.065), (21, -0.09), (22, 0.011), (23, 0.045), (24, -0.028), (25, 0.113), (26, -0.039), (27, -0.062), (28, 0.083), (29, -0.016), (30, -0.047), (31, -0.008), (32, -0.031), (33, -0.047), (34, -0.064), (35, 0.024), (36, -0.051), (37, 0.049), (38, 0.021), (39, -0.034), (40, 0.007), (41, -0.02), (42, -0.005), (43, -0.04), (44, -0.09), (45, 0.028), (46, 0.016), (47, 0.02), (48, -0.007), (49, -0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98286593 <a title="256-lsi-1" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>Author: Peng Cai ; Wei Gao ; Aoying Zhou ; Kam-Fai Wong</p><p>Abstract: We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.</p><p>2 0.86216915 <a title="256-lsi-2" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>Author: Marius Pasca</p><p>Abstract: The role of search queries, as available within query sessions or in isolation from one another, in examined in the context of ranking the class labels (e.g., brazilian cities, business centers, hilly sites) extracted from Web documents for various instances (e.g., rio de janeiro). The co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance. Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts.</p><p>3 0.84068722 <a title="256-lsi-3" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>4 0.83380234 <a title="256-lsi-4" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>Author: Michael Bendersky ; W. Bruce Croft ; David A. Smith</p><p>Abstract: W. Bruce Croft Dept. of Computer Science University of Massachusetts Amherst, MA cro ft @ c s .uma s s .edu David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@ c s .umas s .edu articles or web pages). As previous research shows, these differences severely limit the applicability of Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an impor- tant part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.</p><p>5 0.79815602 <a title="256-lsi-5" href="./acl-2011-Jigs_and_Lures%3A_Associating_Web_Queries_with_Structured_Entities.html">181 acl-2011-Jigs and Lures: Associating Web Queries with Structured Entities</a></p>
<p>Author: Patrick Pantel ; Ariel Fuxman</p><p>Abstract: We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query- product associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision.</p><p>6 0.7908113 <a title="256-lsi-6" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<p>7 0.68628979 <a title="256-lsi-7" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>8 0.57161492 <a title="256-lsi-8" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>9 0.56470507 <a title="256-lsi-9" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>10 0.54952723 <a title="256-lsi-10" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>11 0.50312972 <a title="256-lsi-11" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>12 0.44888884 <a title="256-lsi-12" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>13 0.44692022 <a title="256-lsi-13" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>14 0.44464862 <a title="256-lsi-14" href="./acl-2011-Is_Machine_Translation_Ripe_for_Cross-Lingual_Sentiment_Classification%3F.html">179 acl-2011-Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?</a></p>
<p>15 0.41443026 <a title="256-lsi-15" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>16 0.40264452 <a title="256-lsi-16" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>17 0.36770755 <a title="256-lsi-17" href="./acl-2011-Effective_Measures_of_Domain_Similarity_for_Parsing.html">109 acl-2011-Effective Measures of Domain Similarity for Parsing</a></p>
<p>18 0.35402483 <a title="256-lsi-18" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>19 0.35014936 <a title="256-lsi-19" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>20 0.33682498 <a title="256-lsi-20" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.012), (10, 0.23), (17, 0.043), (26, 0.079), (37, 0.178), (39, 0.043), (41, 0.062), (55, 0.037), (59, 0.026), (72, 0.044), (88, 0.018), (91, 0.032), (96, 0.115)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82094669 <a title="256-lda-1" href="./acl-2011-Latent_Class_Transliteration_based_on_Source_Language_Origin.html">197 acl-2011-Latent Class Transliteration based on Source Language Origin</a></p>
<p>Author: Masato Hagiwara ; Satoshi Sekine</p><p>Abstract: Transliteration, a rich source of proper noun spelling variations, is usually recognized by phonetic- or spelling-based models. However, a single model cannot deal with different words from different language origins, e.g., “get” in “piaget” and “target.” Li et al. (2007) propose a method which explicitly models and classifies the source language origins and switches transliteration models accordingly. This model, however, requires an explicitly tagged training set with language origins. We propose a novel method which models language origins as latent classes. The parameters are learned from a set of transliterated word pairs via the EM algorithm. The experimental results of the transliteration task of Western names to Japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes.</p><p>same-paper 2 0.80923814 <a title="256-lda-2" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>Author: Peng Cai ; Wei Gao ; Aoying Zhou ; Kam-Fai Wong</p><p>Abstract: We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.</p><p>3 0.70942277 <a title="256-lda-3" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.</p><p>4 0.69292498 <a title="256-lda-4" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>Author: Danushka Bollegala ; David Weir ; John Carroll</p><p>Abstract: We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automat- ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.</p><p>5 0.69266236 <a title="256-lda-5" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>6 0.69242638 <a title="256-lda-6" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>7 0.68875563 <a title="256-lda-7" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>8 0.68817854 <a title="256-lda-8" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>9 0.68812358 <a title="256-lda-9" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>10 0.68811393 <a title="256-lda-10" href="./acl-2011-Which_Noun_Phrases_Denote_Which_Concepts%3F.html">334 acl-2011-Which Noun Phrases Denote Which Concepts?</a></p>
<p>11 0.68644571 <a title="256-lda-11" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>12 0.68603843 <a title="256-lda-12" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>13 0.68582094 <a title="256-lda-13" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>14 0.68421167 <a title="256-lda-14" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>15 0.68174088 <a title="256-lda-15" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>16 0.68153781 <a title="256-lda-16" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>17 0.67597151 <a title="256-lda-17" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>18 0.67494291 <a title="256-lda-18" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>19 0.67399156 <a title="256-lda-19" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>20 0.66908574 <a title="256-lda-20" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
