<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-71" href="#">acl2011-71</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</h1>
<br/><p>Source: <a title="acl-2011-71-pdf" href="http://aclweb.org/anthology//P/P11/P11-1051.pdf">pdf</a></p><p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>Reference: <a title="acl-2011-71-reference" href="../acl2011_reference/acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 selecting a representative set of citation sentences that highlight the contribution of the target paper). [sent-5, score-0.943]
</p><p>2 For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. [sent-7, score-0.328]
</p><p>3 When a reference appears in a scientific paper, it is often accompanied by a span of text describing the work being cited. [sent-14, score-0.291]
</p><p>4 We name the sentence that contains an explicit reference to another paper citation sentence. [sent-15, score-0.995]
</p><p>5 Citation sentences usually highlight the most important aspects of the cited paper such as the research problem it addresses, the method it proposes, the good results it reports, and even its drawbacks and limitations. [sent-16, score-0.241]
</p><p>6 By aggregating all the citation sentences that cite a paper, we have a rich source of information about 500 Dragomir Radev EECS Department and  School of Information University of Michigan Ann Arbor, MI, USA radev@ umi ch . [sent-17, score-0.991]
</p><p>7 One way to make use of these sentences is creating a summary of the target paper. [sent-20, score-0.363]
</p><p>8 This summary is different from the abstract or a summary generated from the paper itself. [sent-21, score-0.266]
</p><p>9 While the abstract represents the author’s point of view, the citation summary is the summation of multiple scholars’ viewpoints. [sent-22, score-0.817]
</p><p>10 The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization. [sent-23, score-1.004]
</p><p>11 analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the  paper. [sent-30, score-0.954]
</p><p>12 The cohesion and the readability of the produced summaries have been mostly ignored. [sent-31, score-0.366]
</p><p>13 In this work, we focus on the coherence and readability aspects of the problem. [sent-33, score-0.252]
</p><p>14 Our experiments show that our approach produces better summaries than several baseline summarization systems. [sent-35, score-0.268]
</p><p>15 Ac s2s0o1ci1a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 50 –509, 2  Related Work  The idea of analyzing and utilizing citation information is far from new. [sent-43, score-0.684]
</p><p>16 Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. [sent-47, score-0.902]
</p><p>17 They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). [sent-48, score-0.684]
</p><p>18 Newman (2001) analyzed the structure of the citation networks. [sent-49, score-0.684]
</p><p>19 Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. [sent-52, score-0.786]
</p><p>20 (2008) performed a study on citation summaries and their importance. [sent-56, score-0.883]
</p><p>21 They concluded that citation summaries are more focused and contain more information than abstracts. [sent-57, score-0.883]
</p><p>22 (2009) suggested using citation information to generate surveys of scientific paradigms. [sent-59, score-0.786]
</p><p>23 (2010) proposed a citation-based summarization method that first extracts a number of important keyphrases from the set of citation sentences, and then finds the best subset of sentences that covers as many keyphrases as possible. [sent-63, score-1.035]
</p><p>24 501 3  Motivation  The coherence and readability of citation-based summaries are impeded by several factors. [sent-65, score-0.408]
</p><p>25 First, many citation sentences cite multiple papers besides the target. [sent-66, score-1.043]
</p><p>26 For example, the following is a citation  sentence that appeared in the NLP literature and talked about Resnik’s (1999) work. [sent-67, score-0.777]
</p><p>27 Including the irrelevant fragments in the summary causes several problems. [sent-71, score-0.253]
</p><p>28 These fragments take space in the summary while being irrelevant and less important. [sent-73, score-0.253]
</p><p>29 Second, including these fragments in the summary breaks the context and, hence, degrades the readability and confuses the reader. [sent-74, score-0.338]
</p><p>30 Third, the existence of irrelevant fragments in a sentence makes the ranking algorithm assign a low weight to it although the relevant fragment may cover an aspect of the paper that no other sentence  covers. [sent-75, score-0.465]
</p><p>31 For example, the following are two other citation sentences for Resnik (1999). [sent-77, score-0.85]
</p><p>32 If these two sentences are to be included in the summary, the reasonable ordering would be to put the second sentence first. [sent-80, score-0.288]
</p><p>33 Thirdly, in some instances of citation sentences, the reference is not a syntactic constituent in the sentence. [sent-81, score-0.873]
</p><p>34 For example, in sentence (2) above, the reference could be safely removed from the sentence without hurting its grammaticality. [sent-83, score-0.404]
</p><p>35 sentence (3) above), the reference is a syntactic constituent of the sentence  and removing it makes the sentence ungrammatical. [sent-86, score-0.468]
</p><p>36 However, in certain cases, the reference could be replaced with a suitable pronoun (i. [sent-87, score-0.306]
</p><p>37 Finally, a significant number of citation sentences are not suitable for summarization (Teufel et al. [sent-91, score-0.959]
</p><p>38 Teufel (2007) reported that a significant number of citation sentences (67% of the sentences in her dataset) were of this type. [sent-100, score-1.016]
</p><p>39 This sentence alone does not provide any valuable information about Eisner’s paper and should not be added to the summary unless its context is extracted and included in the summary as well. [sent-104, score-0.359]
</p><p>40 4  Approach  In this section we describe a system that takes a scientific paper and a set of citation sentences that cite it as input, and outputs a citation summary of the paper. [sent-106, score-1.875]
</p><p>41 In the first stage, the citation sentences are 502 preprocessed to rule out the unsuitable sentences and the irrelevant fragments of sentences. [sent-108, score-1.255]
</p><p>42 In the second stage, a number of citation sentences that cover the various aspects of the paper are selected. [sent-109, score-0.935]
</p><p>43 In the last stage, the selected sentences are post-processed  to enhance the readability of the summary. [sent-110, score-0.301]
</p><p>44 1 Preprocessing The aim of this stage is to determine which pieces of text (sentences or fragments of sentences) should be considered for selection in the next stage and which ones should be excluded. [sent-113, score-0.263]
</p><p>45 This stage involves three tasks: reference tagging, reference scope identification, and sentence filtering. [sent-114, score-0.668]
</p><p>46 1 Reference Tagging A citation sentence contains one or more references. [sent-117, score-0.777]
</p><p>47 The reference to the target is given a different tag than the references to other papers. [sent-121, score-0.253]
</p><p>48 The following example shows a citation sentence with all the references tagged and the target reference given a different tag. [sent-122, score-1.062]
</p><p>49 the fragment of the citation sentence that corresponds to the target paper. [sent-128, score-0.906]
</p><p>50 We define the scope of a reference as the shortest fragment of the citation sentence that contains the reference and could form a grammatical sentence if the rest of the sentence was removed. [sent-129, score-1.522]
</p><p>51 Since the parser is not trained on citation sentences, we replace the references with placeholders before passing the sentence to the parser. [sent-132, score-0.777]
</p><p>52 Figure 1: An example showing the scope of a target reference We extract the scope of the reference from the parse tree as follows. [sent-134, score-0.674]
</p><p>53 We find the smallest subtree rooted at an S node (sentence clause node) and contains the target reference node. [sent-135, score-0.353]
</p><p>54 For example, the parse tree shown in Figure 1 suggests that the scope of the reference is: Resnik (1999) describes a method for mining the web for bilingual texts. [sent-138, score-0.34]
</p><p>55 Formally, we classify the citation sentences into two classes: suitable and unsuitable sentences. [sent-147, score-1.041]
</p><p>56 2  Extraction  In the first stage, the sentences and sentence fragments that are not useful for our summarization task are ruled out. [sent-154, score-0.398]
</p><p>57 The input to this stage is a set of citation sentences that are believed to be suitable for the summary. [sent-155, score-0.971]
</p><p>58 The sentences are selected based on these three main properties: First, they should cover diverse aspects of the paper. [sent-157, score-0.251]
</p><p>59 Second, the sentences that cover the same aspect should not contain redundant information. [sent-158, score-0.26]
</p><p>60 For example, if two sentences talk about the drawbacks of the target paper, one sentence can mention the computation inefficiency, while the other criticize the assumptions the paper makes. [sent-159, score-0.355]
</p><p>61 Third, the sentences should cover as many important facts about the target paper as possible using minimal text. [sent-160, score-0.272]
</p><p>62 In this stage, the summary sentences are selected in three steps. [sent-161, score-0.299]
</p><p>63 In the second step, we cluster the sentences within each category into clusters of similar sentences. [sent-163, score-0.346]
</p><p>64 The summary sentences are selected based on the classification, the clustering, and the LexRank values. [sent-165, score-0.299]
</p><p>65 1 Functional Category Classification We classify the citation sentences into the five categories mentioned above using a machine learning technique. [sent-168, score-0.919]
</p><p>66 A classification model is trained on a number of features (Table 2) extracted from a labeled set of citation sentences. [sent-169, score-0.714]
</p><p>67 2 Sentence Clustering In the previous step we determined the category of each citation sentence. [sent-173, score-0.807]
</p><p>68 It is very likely that sentences from the same category contain similar or overlapping information. [sent-174, score-0.258]
</p><p>69 For example, Sentences (6), (7), and (8) below appear in the set of citation  sentences that cite Goldwater and Griffiths’ (2007). [sent-175, score-0.956]
</p><p>70 Clustering divides the sentences of each category into groups of similar sentences. [sent-181, score-0.258]
</p><p>71 Clusters within each category are ordered by the number of sentences in them whereas  the sentences of each cluster are ordered by their LexRank values. [sent-198, score-0.549]
</p><p>72 ) If the desired length of the summary is 3 sentences, the selected sentences will be in order S 1, S12, then S 18. [sent-202, score-0.299]
</p><p>73 Each citation sentence will have the target reference (the author’s names and the publication year) mentioned at least once. [sent-206, score-1.074]
</p><p>74 The reference could be either syntactically and semantically part of the sentence (e. [sent-207, score-0.282]
</p><p>75 In the following sentences, we either replace the reference  with a suitable personal pronoun or remove it. [sent-214, score-0.31]
</p><p>76 The reference is replaced with a pronoun if it is part of the sentence and this replacement does not make the sentence ungrammatical. [sent-215, score-0.491]
</p><p>77 To determine whether a reference is part of the sentence or not, we again use a machine learning approach. [sent-218, score-0.282]
</p><p>78 If a reference is to be replaced, and the paper has one author, we use ”he/she” (we do not know if the author is male or female). [sent-222, score-0.25]
</p><p>79 Then we evaluate the summaries that our system generate in terms of extraction quality. [sent-226, score-0.241]
</p><p>80 AAN provides all citation information from within the network including the citation network, the citation sentences, and the citation context for each paper. [sent-232, score-2.794]
</p><p>81 The papers have a variable number of citation sentences, ranging from 15 to 348. [sent-234, score-0.771]
</p><p>82 The total number of citation sentences in the dataset is 4,335. [sent-235, score-0.85]
</p><p>83 The agreement among the three annotators on distinguishing the unsuitable sentences from the other five categories is 0. [sent-243, score-0.322]
</p><p>84 The agreement on classifying the sentences into the five functional categories is 0. [sent-246, score-0.245]
</p><p>85 We asked humans with a good background in NLP (the papers topic) to generate a readable, coherent summary for each paper in the set using its citation sentences as the source text. [sent-250, score-1.177]
</p><p>86 We asked them to fix the length of the summaries to 5 sentences. [sent-251, score-0.234]
</p><p>87 2 Component Evaluation Reference Tagging and Reference Scope Identification Evaluation: We ran our reference tagging and scope identification components on the 2,284 sentences in dataset1. [sent-254, score-0.576]
</p><p>88 Then, we went through the tagged sentences and the extracted scopes, and counted the number of correctly/incorrectly tagged (extracted)/missed references (scopes). [sent-255, score-0.23]
</p><p>89 The reference to the target paper was tagged correctly in all the sentences. [sent-265, score-0.285]
</p><p>90 Our scope identification component extracted the scope of target references with good precision (86. [sent-266, score-0.371]
</p><p>91 In fact, extracting a useful scope for a reference requires more than just finding a grammatical substring. [sent-269, score-0.305]
</p><p>92 We use our system to generate summaries for each of the 30 papers in dataset2. [sent-285, score-0.286]
</p><p>93 We also generate summaries for the papers using a number of baseline systems (described in Section 5. [sent-286, score-0.286]
</p><p>94 In the first baseline, the sentences are selected randomly from the set of citation sentences and added to the summary. [sent-294, score-1.016]
</p><p>95 The third baseline is LexRank (Erkan and Radev, 2004) run on the entire set of citation sentences of the target paper. [sent-297, score-0.914]
</p><p>96 The forth baseline is Qazvinian and Radev (2008) citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are  ranked using LexRank. [sent-298, score-1.107]
</p><p>97 In another variation (FL-2), we remove the sentence classification component; so, all the sen507 tences are assumed to come from one category in the subsequent components. [sent-301, score-0.322]
</p><p>98 To make the comparison of the extraction quality to those baselines fair, we remove the author name replacement component from our system and all its variations. [sent-303, score-0.282]
</p><p>99 2 Results Table 6 shows the average ROUGE-L scores (with 95% confidence interval) for the summaries of the 30 papers in dataset2 generated using our system and the different baselines. [sent-306, score-0.286]
</p><p>100 4  Coherence and Readability Evaluation  We asked human judges (not including the authors) to rate the coherence and readability of a number of summaries for each of dataset2 papers. [sent-317, score-0.443]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('citation', 0.684), ('summaries', 0.199), ('reference', 0.189), ('sentences', 0.166), ('radev', 0.161), ('resnik', 0.135), ('readability', 0.135), ('summary', 0.133), ('unsuitable', 0.119), ('lexrank', 0.117), ('scope', 0.116), ('qazvinian', 0.108), ('cite', 0.106), ('scientific', 0.102), ('teufel', 0.094), ('sentence', 0.093), ('category', 0.092), ('papers', 0.087), ('stage', 0.081), ('eisner', 0.076), ('coherence', 0.074), ('fragments', 0.07), ('aan', 0.069), ('summarization', 0.069), ('goldwater', 0.067), ('fragment', 0.065), ('target', 0.064), ('griffiths', 0.061), ('author', 0.061), ('nanba', 0.06), ('network', 0.058), ('cluster', 0.055), ('citationbased', 0.052), ('elkiss', 0.052), ('erkan', 0.052), ('citations', 0.052), ('aspect', 0.052), ('irrelevant', 0.05), ('pronoun', 0.045), ('publication', 0.044), ('background', 0.044), ('aspects', 0.043), ('functional', 0.042), ('clauset', 0.042), ('keyphrases', 0.042), ('variation', 0.042), ('extraction', 0.042), ('cover', 0.042), ('mohammad', 0.04), ('suitable', 0.04), ('filtering', 0.039), ('subtree', 0.039), ('replacement', 0.039), ('component', 0.038), ('year', 0.038), ('kappa', 0.038), ('scopes', 0.038), ('tagging', 0.037), ('identification', 0.037), ('baselines', 0.037), ('categories', 0.037), ('summarizer', 0.036), ('remove', 0.036), ('describes', 0.035), ('okumura', 0.035), ('eecs', 0.035), ('umi', 0.035), ('asked', 0.035), ('ordered', 0.035), ('clustering', 0.034), ('confusing', 0.034), ('repeating', 0.034), ('rooted', 0.034), ('fl', 0.034), ('readable', 0.034), ('stages', 0.033), ('clusters', 0.033), ('drawbacks', 0.032), ('cohesion', 0.032), ('replaced', 0.032), ('covers', 0.032), ('tagged', 0.032), ('classify', 0.032), ('step', 0.031), ('components', 0.031), ('aim', 0.031), ('svm', 0.031), ('classification', 0.03), ('representative', 0.029), ('ordering', 0.029), ('removed', 0.029), ('name', 0.029), ('michigan', 0.029), ('tences', 0.029), ('humans', 0.028), ('kernel', 0.028), ('henceforth', 0.027), ('nguyen', 0.027), ('motivation', 0.027), ('smallest', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="71-tfidf-1" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>2 0.41787544 <a title="71-tfidf-2" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>Author: Awais Athar</p><p>Abstract: Sentiment analysis of citations in scientific papers and articles is a new and interesting problem due to the many linguistic differences between scientific texts and other genres. In this paper, we focus on the problem of automatic identification of positive and negative sentiment polarity in citations to scientific papers. Using a newly constructed annotated citation sentiment corpus, we explore the effectiveness of existing and novel features, including n-grams, specialised science-specific lexical features, dependency relations, sentence splitting and negation features. Our results show that 3-grams and dependencies perform best in this task; they outperform the sentence splitting, science lexicon and negation based features.</p><p>3 0.36261809 <a title="71-tfidf-3" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>Author: Clinton Burfoot ; Steven Bird ; Timothy Baldwin</p><p>Abstract: This paper explores approaches to sentiment classification of U.S. Congressional floordebate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.</p><p>4 0.25450569 <a title="71-tfidf-4" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>5 0.20315997 <a title="71-tfidf-5" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>Author: Vahed Qazvinian ; Dragomir R. Radev</p><p>Abstract: We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a net- work of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.</p><p>6 0.14948085 <a title="71-tfidf-6" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>7 0.12056258 <a title="71-tfidf-7" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>8 0.11824746 <a title="71-tfidf-8" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>9 0.11496936 <a title="71-tfidf-9" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>10 0.11029626 <a title="71-tfidf-10" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>11 0.10896607 <a title="71-tfidf-11" href="./acl-2011-The_ACL_Anthology_Searchbench.html">298 acl-2011-The ACL Anthology Searchbench</a></p>
<p>12 0.10614394 <a title="71-tfidf-12" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>13 0.10201069 <a title="71-tfidf-13" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>14 0.096836127 <a title="71-tfidf-14" href="./acl-2011-Automatic_Extraction_of_Lexico-Syntactic_Patterns_for_Detection_of_Negation_and_Speculation_Scopes.html">50 acl-2011-Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation and Speculation Scopes</a></p>
<p>15 0.085359298 <a title="71-tfidf-15" href="./acl-2011-A_Corpus_of_Scope-disambiguated_English_Text.html">8 acl-2011-A Corpus of Scope-disambiguated English Text</a></p>
<p>16 0.079110831 <a title="71-tfidf-16" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>17 0.076795429 <a title="71-tfidf-17" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>18 0.075790502 <a title="71-tfidf-18" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>19 0.073104478 <a title="71-tfidf-19" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>20 0.066831157 <a title="71-tfidf-20" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.2), (1, 0.104), (2, -0.012), (3, 0.034), (4, -0.036), (5, 0.008), (6, -0.064), (7, 0.164), (8, -0.011), (9, -0.102), (10, -0.108), (11, -0.085), (12, -0.237), (13, 0.015), (14, -0.277), (15, -0.089), (16, -0.005), (17, 0.01), (18, 0.132), (19, -0.077), (20, -0.128), (21, -0.133), (22, 0.16), (23, -0.087), (24, 0.076), (25, 0.142), (26, -0.132), (27, 0.271), (28, -0.186), (29, 0.005), (30, -0.177), (31, 0.029), (32, -0.018), (33, 0.046), (34, 0.058), (35, -0.065), (36, 0.056), (37, -0.018), (38, -0.047), (39, 0.076), (40, -0.033), (41, 0.004), (42, -0.0), (43, 0.05), (44, -0.027), (45, -0.07), (46, 0.023), (47, -0.041), (48, -0.01), (49, -0.131)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95040995 <a title="71-lsi-1" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>2 0.7957859 <a title="71-lsi-2" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>Author: Clinton Burfoot ; Steven Bird ; Timothy Baldwin</p><p>Abstract: This paper explores approaches to sentiment classification of U.S. Congressional floordebate transcripts. Collective classification techniques are used to take advantage of the informal citation structure present in the debates. We use a range of methods based on local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique.</p><p>3 0.72525996 <a title="71-lsi-3" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>Author: Vahed Qazvinian ; Dragomir R. Radev</p><p>Abstract: We analyze collective discourse, a collective human behavior in content generation, and show that it exhibits diversity, a property of general collective systems. Using extensive analysis, we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization. We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries. We show how different summaries use various phrasal information units (i.e., nuggets) to express the same atomic semantic units, called factoids. Finally, we present a ranker that employs distributional similarities to build a net- work of words, and captures the diversity of perspectives by detecting communities in this network. Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity.</p><p>4 0.68103731 <a title="71-lsi-4" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>5 0.61038697 <a title="71-lsi-5" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In this paper we present Clairlib, an opensource toolkit for Natural Language Processing, Information Retrieval, and Network Analysis. Clairlib provides an integrated framework intended to simplify a number of generic tasks within and across those three areas. It has a command-line interface, a graphical interface, and a documented API. Clairlib is compatible with all the common platforms and operating systems. In addition to its own functionality, it provides interfaces to external software and corpora. Clairlib comes with a comprehensive documentation and a rich set of tutorials and visual demos.</p><p>6 0.60071206 <a title="71-lsi-6" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>7 0.50212908 <a title="71-lsi-7" href="./acl-2011-The_ACL_Anthology_Searchbench.html">298 acl-2011-The ACL Anthology Searchbench</a></p>
<p>8 0.43950999 <a title="71-lsi-8" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>9 0.43071947 <a title="71-lsi-9" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>10 0.41347453 <a title="71-lsi-10" href="./acl-2011-A_Corpus_of_Scope-disambiguated_English_Text.html">8 acl-2011-A Corpus of Scope-disambiguated English Text</a></p>
<p>11 0.40440175 <a title="71-lsi-11" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>12 0.39367589 <a title="71-lsi-12" href="./acl-2011-Automatic_Extraction_of_Lexico-Syntactic_Patterns_for_Detection_of_Negation_and_Speculation_Scopes.html">50 acl-2011-Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation and Speculation Scopes</a></p>
<p>13 0.39072573 <a title="71-lsi-13" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>14 0.38379118 <a title="71-lsi-14" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>15 0.35631528 <a title="71-lsi-15" href="./acl-2011-Contrasting_Opposing_Views_of_News_Articles_on_Contentious_Issues.html">84 acl-2011-Contrasting Opposing Views of News Articles on Contentious Issues</a></p>
<p>16 0.33457169 <a title="71-lsi-16" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>17 0.33334762 <a title="71-lsi-17" href="./acl-2011-Semantic_Representation_of_Negation_Using_Focus_Detection.html">273 acl-2011-Semantic Representation of Negation Using Focus Detection</a></p>
<p>18 0.33117181 <a title="71-lsi-18" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>19 0.32769984 <a title="71-lsi-19" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>20 0.32572648 <a title="71-lsi-20" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.052), (11, 0.166), (17, 0.044), (18, 0.011), (26, 0.021), (37, 0.098), (39, 0.04), (41, 0.049), (55, 0.024), (59, 0.034), (72, 0.052), (91, 0.06), (96, 0.24), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95911682 <a title="71-lda-1" href="./acl-2011-Finding_Deceptive_Opinion_Spam_by_Any_Stretch_of_the_Imagination.html">136 acl-2011-Finding Deceptive Opinion Spam by Any Stretch of the Imagination</a></p>
<p>Author: Myle Ott ; Yejin Choi ; Claire Cardie ; Jeffrey T. Hancock</p><p>Abstract: Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam—fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.</p><p>2 0.92804664 <a title="71-lda-2" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>Author: Alexander Volokh ; Gunter Neumann</p><p>Abstract: Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 1</p><p>same-paper 3 0.9077782 <a title="71-lda-3" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>4 0.88602185 <a title="71-lda-4" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>Author: Jason Naradowsky ; Kristina Toutanova</p><p>Abstract: This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</p><p>5 0.85182303 <a title="71-lda-5" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>Author: Seth Kulick ; Ann Bies ; Justin Mott</p><p>Abstract: This work introduces a new approach to checking treebank consistency. Derivation trees based on a variant of Tree Adjoining Grammar are used to compare the annotation of word sequences based on their structural similarity. This overcomes the problems of earlier approaches based on using strings of words rather than tree structure to identify the appropriate contexts for comparison. We report on the result of applying this approach to the Penn Arabic Treebank and how this approach leads to high precision of error detection.</p><p>6 0.84917462 <a title="71-lda-6" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>7 0.84665579 <a title="71-lda-7" href="./acl-2011-A_New_Dataset_and_Method_for_Automatically_Grading_ESOL_Texts.html">20 acl-2011-A New Dataset and Method for Automatically Grading ESOL Texts</a></p>
<p>8 0.84596014 <a title="71-lda-8" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>9 0.84483165 <a title="71-lda-9" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>10 0.84457254 <a title="71-lda-10" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>11 0.84337997 <a title="71-lda-11" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>12 0.84287339 <a title="71-lda-12" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>13 0.84282905 <a title="71-lda-13" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>14 0.84281635 <a title="71-lda-14" href="./acl-2011-Automatically_Evaluating_Text_Coherence_Using_Discourse_Relations.html">53 acl-2011-Automatically Evaluating Text Coherence Using Discourse Relations</a></p>
<p>15 0.8427062 <a title="71-lda-15" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>16 0.84249949 <a title="71-lda-16" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>17 0.84242284 <a title="71-lda-17" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>18 0.8422513 <a title="71-lda-18" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>19 0.8420862 <a title="71-lda-19" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>20 0.84163082 <a title="71-lda-20" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
