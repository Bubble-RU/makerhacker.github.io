<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 acl-2011-A Class of Submodular Functions for Document Summarization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-4" href="#">acl2011-4</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 acl-2011-A Class of Submodular Functions for Document Summarization</h1>
<br/><p>Source: <a title="acl-2011-4-pdf" href="http://aclweb.org/anthology//P/P11/P11-1052.pdf">pdf</a></p><p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</p><p>Reference: <a title="acl-2011-4-reference" href="../acl2011_reference/acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 hl in@ ee washingt on edu Abstract We design a class of submodular functions meant for document summarization tasks. [sent-4, score-1.251]
</p><p>2 These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. [sent-5, score-0.21]
</p><p>3 Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. [sent-6, score-0.418]
</p><p>4 When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. [sent-7, score-0.118]
</p><p>5 Lastly, we show that several well-established methods for document summarization correspond, in fact,  to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization. [sent-8, score-2.19]
</p><p>6 1 Introduction In this paper, we address the problem of generic and query-based extractive summarization from collections of related documents, a task commonly known as multi-document summarization. [sent-9, score-0.379]
</p><p>7 We treat this task as monotone submodular function maximization (to be defined in Section 2). [sent-10, score-1.007]
</p><p>8 On the one hand, there exists a simple greedy algorithm for monotone submodular function maximization where the summary solution obtained (say is guaranteed to be almost as good as the best possible solution (say Sopt) according to an objective F. [sent-12, score-1.216]
</p><p>9 Of course, none of this is useful if the objective function F is inappropriate for the summarization task. [sent-21, score-0.371]
</p><p>10 In thFis paper, we argue that monotone nondecreasing submodular functions F are an ideal class of functions to investigate for docuFment summarization. [sent-22, score-1.24]
</p><p>11 We show, in fact, that many well-established methods for summarization (Carbonell and Goldstein, 1998; Filatova and Hatzivassiloglou, 2004; Takamura and Okumura, 2009; Riedhammer et al. [sent-23, score-0.254]
</p><p>12 , 2010; Shen and Li, 2010) correspond to submodular function optimization, a property not explicitly mentioned in these publications. [sent-24, score-0.892]
</p><p>13 We take this fact, however, as testament to the value of submodular functions for summarization: if summarization algorithms are repeatedly developed that, by chance, happen to be an instance of a −  submodular function optimization, this suggests that submodular functions are a natural fit. [sent-25, score-2.988]
</p><p>14 On the other hand, other authors have started realizing explicitly the value of submodular functions for summarization (Lin and Bilmes, 2010; Qazvinian et al. [sent-26, score-1.198]
</p><p>15 Submodular functions share many properties in common with convex functions, one of which is that they are closed under a number of common combination operations (summation, certain compositions, restrictions, and so on). [sent-28, score-0.172]
</p><p>16 These operations give us the tools necessary to design a powerful submodular objective for submodular document summarization that extends beyond any previous work. [sent-29, score-2.008]
</p><p>17 We demonstrate this by carefully crafting a class of submodular funcProce dinPgosrt olafn thde, 4 O9rtehg Aon ,n Ju anle M 1e9e-2tin4g, 2 o0f1 t1h. [sent-30, score-0.801]
</p><p>18 Ac s2s0o1ci1a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 510–520, tions we feel are ideal for extractive summarization tasks, both generic and query-focused. [sent-32, score-0.379]
</p><p>19 In doing so, we demonstrate better than existing state-of-the-art performance on a number of standard summarization evaluation tasks, namely DUC-04 through to DUC07. [sent-33, score-0.254]
</p><p>20 We believe our work, moreover, might act as a springboard for researchers in summarization to consider the problem of “how to design a submodular  function” for the summarization task. [sent-34, score-1.309]
</p><p>21 In Section 2, we provide a briefbackground on submodular functions and their optimization. [sent-35, score-0.924]
</p><p>22 Section 3 describes how the task of extractive summarization can be viewed as a problem of submodular function maximization. [sent-36, score-1.2]
</p><p>23 We also in this section show that many standard methods for summarization are, in fact, already performing submodular function optimization. [sent-37, score-1.12]
</p><p>24 In Section 4, we present our own submodular functions. [sent-38, score-0.801]
</p><p>25 Section 5 presents results on both generic and query-focused summarization tasks, showing as far as we know the best known ROUGE results for DUC04 through DUC-06, and the best known precision results for DUC-07, and the best recall DUC-07 results among those that do not use a web search engine. [sent-39, score-0.321]
</p><p>26 , vn} and a function F : 2V → R that retVurn =s a v real value }for any subset S ⊆ :V 2. [sent-44, score-0.085]
</p><p>27 For example, F might correspond to the value or coverage of a seFt of sensor locations in an environment, and the goal is to find the best locations for a fixed number of sensors (Krause et al. [sent-49, score-0.098]
</p><p>28 If the function F is monotone submodular then the maximization is sFtill NP complete, but it was shown in (Nemhauser et al. [sent-51, score-1.007]
</p><p>29 Submodular functions are those that satisfy the property ofdiminishing returns: for any A ⊆ B ⊆ V \v, a submodular function F must satisfy F(A+v) −F(A) ≥ 511 F(B + v) F(B). [sent-56, score-1.015]
</p><p>30 A set func|Vtio |n F is mono-  e−e1  −  f~a  f~of  tone nondecreasing if ∀A ⊆ B, F(A) ≤ F(B). [sent-60, score-0.095]
</p><p>31 As shorthand, in this paper, monotone nondecreasing submodular functions will simply be referred to as monotone submodular. [sent-61, score-1.215]
</p><p>32 Historically, submodular functions have their roots in economics, game theory, combinatorial optimization, and operations research. [sent-62, score-0.951]
</p><p>33 More recently, submodular functions have started receiving attention in the machine learning and computer vision community (Kempe et al. [sent-63, score-0.944]
</p><p>34 , 2008; Kolmogorov and Zabin, 2004) and have recently been introduced to natural language processing for the tasks of document summarization (Lin and Bilmes, 2010) and word alignment (Lin and Bilmes, 2011). [sent-65, score-0.327]
</p><p>35 For exam-  ple, if a collection of functions {Fi}Pi is submodular, then so is their weighted sum F{F = Pi αiFi where αi are nonnegative weights. [sent-67, score-0.123]
</p><p>36 ItF Fis = noPt hardF to show that submodular functions also haveP the following composition property with concave functions: Theorem 1. [sent-68, score-1.061]
</p><p>37 Given functions F : 2V → R and f : R → R, the composition F0F = f ◦ F : →2V R → R (i. [sent-69, score-0.147]
</p><p>38 , F0(S) = f(F(S))) iFs nondecreasing →sub Rmodular, if f =is non-decreasing concave and F is nondecreasing submodular. [sent-71, score-0.277]
</p><p>39 This property will be quite useful when defining submodular functions for document summarization. [sent-72, score-1.023]
</p><p>40 1 Summarization with knapsack constraint Let the ground set V represents all the sentences (or other linguistic units) in a document (or document collection, in the multi-document summarization case). [sent-74, score-0.476]
</p><p>41 The task of extractive document summarization is to select a subset S ⊆ V to represent the entirety (ground set V). [sent-75, score-0.407]
</p><p>42 In other words, adding k3 achieves a greater reward as it increases the diversity of the summary (by choosing from a different cluster). [sent-78, score-0.306]
</p><p>43 It is easy to show that R(S) is submodular by using the composition ruleR f(roSm) Theorem 1. [sent-80, score-0.825]
</p><p>44 Inside each square root lies a modular function with non-negative weights (and thus is monotone). [sent-82, score-0.151]
</p><p>45 Applying the square root to such a monotone submodular function yields a submodular function, and summing them all together retains submodularity, as mentioned in Section 2. [sent-83, score-1.824]
</p><p>46 , 2009)), recently shown to be related to submodularity (Bach, 2010; Jegelka and Bilmes, 2011). [sent-89, score-0.095]
</p><p>47 5 can be replaced with any other non-decreasing concave functions (e. [sent-93, score-0.21]
</p><p>48 , f(x) = log(1 + x)) while preserving the desired property of R(S), and the curvature of the concave function theRn dSe)termines the rate that the reward diminishes. [sent-95, score-0.339]
</p><p>49 5  Experiments  The document understanding conference (DUC) (http : / / duc . [sent-104, score-0.21]
</p><p>50 The tasks in DUC evolved from single-document summarization to multi-document summarization, and from generic summarization (2001–2004) to query-focused summarization (2005–2007). [sent-107, score-0.807]
</p><p>51 In all experiments, the modified greedy algorithm (Lin and Bilmes, 2010) was used for summary generation. [sent-110, score-0.109]
</p><p>52 1 Generic summarization Summarization tasks in DUC-03 and DUC-04 are multi-document summarization on English news  articles. [sent-112, score-0.508]
</p><p>53 For each document cluster, the system generated summary may not be longer than 665 bytes including spaces and punctuation. [sent-114, score-0.144]
</p><p>54 39 4 We first tested our coverage and diversity reward objectives sePparately. [sent-134, score-0.321]
</p><p>55 (6) When α = 1, L1(S) reduces to Pi∈V,j∈S wi,j, which measures Lthe overall similaritPy oi∈fV summary  set S to ground set V. [sent-138, score-0.095]
</p><p>56 1, using such similarity measurement could possibly over-concentrate on a small portion of the document and result in a poor coverage of the whole document. [sent-140, score-0.113]
</p><p>57 As shown in Table 1, optimizing this objective function gives a ROUGE-1 F-measure score 32. [sent-141, score-0.117]
</p><p>58 65% 516  Figure 1: ROUGE-1 F-measure scores on DUC-03 when α and K vary in objective function L1(S) + λR1 (S), where λ = 6 and α = Na . [sent-144, score-0.117]
</p><p>59 As for the diversity reward obPjective, we define the singleton reward as ri = N1 Pj wi,j, which is the average similarity of sentence Pi to the rest of the document. [sent-146, score-0.492]
</p><p>60 It basically states that tPhe more similar to the whole document a sentence is, the more reward there will be by adding this sentence to an empty summary set. [sent-147, score-0.331]
</p><p>61 By using this singleton reward, we have the following diversity reward function:  R1(S) =kX=K1sj∈XS∩PkN1iX∈Vwi,j. [sent-148, score-0.308]
</p><p>62 And as we can see in Table 1, optimizing the diversity reward function alone achieves comparable performance to the DUC-04 best system. [sent-153, score-0.32]
</p><p>63 Combining L1(S) and R1(S), our system outperforms the bestL system in DRUC-04 significantly, and it also outperforms several recent systems, including a concept-based summarization approach (Takamura and Okumura, 2009), a sentence topic model based system (Wang et al. [sent-154, score-0.383]
</p><p>64 , 2009), and our MMR-styled submodular system (Lin and Bilmes, 2010). [sent-155, score-0.821]
</p><p>65 2 Query-focused summarization We evaluated our approach on the task of queryfocused summarization using DUC 05-07 data. [sent-172, score-0.54]
</p><p>66 In DUC-05 and DUC-06, participants were given 50 document clusters, where each cluster contains 25 news articles related to the same topic. [sent-173, score-0.106]
</p><p>67 We used both the title and the narrative as query, where stop words, including some function words (e. [sent-181, score-0.094]
</p><p>68 Note that there are several ways to incorporate query-focused information into both the coverage and diversity reward objectives. [sent-185, score-0.295]
</p><p>69 Also, the coefficient α could be query and sentence dependent, where it takes larger value when a sentence is more relevant to query (i. [sent-187, score-0.19]
</p><p>70 Similarly, sentence clustering and singleton rewards in the diversity function can also 3ROUGE version 1. [sent-190, score-0.304]
</p><p>71 In this experiment, we explore an objective with a query-independent coverage function (R1(S)), indicating prior importance, combined with a query-dependent diversity reward function, where the latter is defined as:  RQ(S) =kX=K1uuvtj∈XS∩Pk NβiX∈Vwi,j+ (1 − β)rj,Q! [sent-210, score-0.412]
</p><p>72 , ≤  ≤  where 0 β 1, and rj,Q represents the relevance 0 be ≤twe βen ≤ sentence j to query Q. [sent-211, score-0.106]
</p><p>73 This query-dependent reward function is derived by using a singleton reward that is expressed as a conPvex combination of the query-independent score (N1 Pi∈V wi,j) and the query-dependent score (rj,Q) of aP sie∈Vntence. [sent-212, score-0.44]
</p><p>74 To better estimate of the relevance between query and sentences, we further expanded sentences with synonyms and hypernyms of its constituent words. [sent-217, score-0.161]
</p><p>75 In particular, part-of-speech tags were obtained for each sentence using the maximum entropy part-of-speech tagger (Ratnaparkhi, 1996), and all nouns were then expanded with their synonyms and hypernyms using WordNet (Fellbaum, 1998). [sent-218, score-0.101]
</p><p>76 Note that these expanded documents were only used in the estimation rj,Q, and we plan to further explore whether there is benefit to use the expanded documents either in sentence similarity estimation or in sentence clustering in our future work. [sent-219, score-0.187]
</p><p>77 We also tried to expand the query with synonyms and observed a performancedecrease, presumably due to noisy information in a query expression. [sent-220, score-0.147]
</p><p>78 , 2007) to learn the coefficients in our objective function, we trained all coefficients to maximize ROUGE-2 F-measure score using the Nelder-Mead (derivative-free) method. [sent-222, score-0.096]
</p><p>79 Using L1(S) λRQ (S) as the objective and with the same sentence clustering algorithm as in the generic summarization experiment (K = 0. [sent-223, score-0.407]
</p><p>80 2N), our system,  +  when both trained and tested on DUC-05 (results in Table 2), outperforms the Bayesian query-focused summarization approach and the search-based structured prediction approach, which were also trained and tested on DUC-05 (Daume´ et al. [sent-224, score-0.329]
</p><p>81 24% in ROUGE-2 recall) is a so called “vine-growth” system, which can be seen as an abstractive approach, whereas our system is purely an extractive system. [sent-228, score-0.1]
</p><p>82 Comparing to the extractive system in (Daume´ et al. [sent-229, score-0.1]
</p><p>83 , 2007), a state-of-the-art supervised summarization system, as well as two recent systems including a generative summarization system based  on topic models (Haghighi and Vanderwende, 2009), and a hybrid hierarchical summarization system (Celikyilmaz and Hakkani-tu¨r, 2010). [sent-241, score-0.802]
</p><p>84 To further improve the performance of our system, we used both DUC-05 and DUC-06 as a training set, and introduced three diversity reward terms into the objective where three different sentence clusterings with different resolutions were produced (with sizes 0. [sent-248, score-0.33]
</p><p>85 Denoting a diversity reward corresponding to clustering κ as RQ,κ(SP), we model the summary quality as L1(S) + λκRQ,κ(S). [sent-252, score-0.339]
</p><p>86 As shown in Table 5, using thisP Pobjective function with multi-resolution diversity Pr ewards improves our results further, and outperforms the best system in DUC-07 in terms of ROUGE-2 F-measure score. [sent-253, score-0.202]
</p><p>87 Pκ3=1  6  Conclusion and discussion  In this paper, we show that submodularity naturally arises in document summarization. [sent-254, score-0.168]
</p><p>88 Not only do many existing automatic summarization methods correspond to submodular function optimization, but also the widely used ROUGE evaluation is closely related to submodular functions. [sent-255, score-1.921]
</p><p>89 As the corresponding submodular optimization problem can be solved efficiently and effectively, the remaining question is then how to design a submodular objective that best models the task. [sent-256, score-1.698]
</p><p>90 To address this problem, we introduce a powerful class of monotone submodular functions that are well suited to document summarization by modeling two important properties of a summary, fidelity and diversity. [sent-257, score-1.349]
</p><p>91 While more advanced NLP techniques could be easily incorporated into our functions (e. [sent-258, score-0.123]
</p><p>92 , language models could define a better Ci (S), more advanced relevance estimations for the singleton rewards ri, and better and/or overlapping clustering algorithms for our diversity reward), we already show top results on standard benchmark evaluations using fairly basic NLP methods (e. [sent-260, score-0.237]
</p><p>93 , term weighting and WordNet expansion), all, we believe, thanks to the power and generality of submodular functions. [sent-262, score-0.822]
</p><p>94 Submodularity beyond submodular energies: coupling edges in graph cuts. [sent-326, score-0.801]
</p><p>95 What energy functions can be minimized via graph cuts? [sent-346, score-0.123]
</p><p>96 An analysis of approximations for maximizing submodular set functions I. [sent-432, score-0.958]
</p><p>97 A note on maximizing a submodular set function subject to a knapsack constraint. [sent-471, score-0.932]
</p><p>98 Text summarization model based on maximum coverage problem and its variant. [sent-477, score-0.294]
</p><p>99 The PYTHY summarization system: Microsoft research at DUC 2007. [sent-488, score-0.254]
</p><p>100 An analysis of the greedy algorithm for the submodular set covering problem. [sent-503, score-0.859]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('submodular', 0.801), ('summarization', 0.254), ('reward', 0.161), ('duc', 0.137), ('functions', 0.123), ('bilmes', 0.115), ('monotone', 0.098), ('nondecreasing', 0.095), ('submodularity', 0.095), ('diversity', 0.094), ('concave', 0.087), ('extractive', 0.08), ('krause', 0.079), ('rq', 0.078), ('document', 0.073), ('daume', 0.068), ('function', 0.065), ('narasimhan', 0.063), ('query', 0.062), ('greedy', 0.058), ('sopt', 0.054), ('singleton', 0.053), ('objective', 0.052), ('summary', 0.051), ('celikyilmaz', 0.048), ('generic', 0.045), ('optimization', 0.044), ('ground', 0.044), ('rouge', 0.043), ('maximization', 0.043), ('pi', 0.042), ('electrical', 0.041), ('pingali', 0.041), ('coverage', 0.04), ('square', 0.039), ('lin', 0.039), ('kx', 0.039), ('lova', 0.036), ('nemhauser', 0.036), ('jagarlamudi', 0.036), ('rewards', 0.036), ('takamura', 0.035), ('maximizing', 0.034), ('cluster', 0.033), ('clustering', 0.033), ('budgeted', 0.032), ('jegelka', 0.032), ('knapsack', 0.032), ('pythy', 0.032), ('queryfocused', 0.032), ('expanded', 0.031), ('narrative', 0.029), ('kolmogorov', 0.029), ('riedhammer', 0.029), ('truncation', 0.029), ('uc', 0.028), ('sd', 0.028), ('guestrin', 0.027), ('kempe', 0.027), ('keyphrase', 0.027), ('modular', 0.027), ('operations', 0.027), ('filatova', 0.026), ('porter', 0.026), ('tested', 0.026), ('property', 0.026), ('ifs', 0.025), ('monotonicity', 0.025), ('qazvinian', 0.025), ('solution', 0.024), ('composition', 0.024), ('toutanova', 0.024), ('hypernyms', 0.024), ('okumura', 0.024), ('synonyms', 0.023), ('pk', 0.023), ('sentence', 0.023), ('documents', 0.023), ('outperforms', 0.023), ('convex', 0.022), ('sin', 0.022), ('recall', 0.022), ('coefficients', 0.022), ('pj', 0.022), ('carbonell', 0.021), ('generality', 0.021), ('hyderabad', 0.021), ('norms', 0.021), ('theorem', 0.021), ('relevance', 0.021), ('tp', 0.021), ('ci', 0.021), ('root', 0.02), ('ix', 0.02), ('sj', 0.02), ('system', 0.02), ('value', 0.02), ('vision', 0.02), ('xs', 0.019), ('locations', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="4-tfidf-1" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</p><p>2 0.49651638 <a title="4-tfidf-2" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We cast the word alignment problem as maximizing a submodular function under matroid constraints. Our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. We show that submodularity naturally arises when modeling word fertility. Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.</p><p>3 0.1744159 <a title="4-tfidf-3" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>Author: Hajime Morita ; Tetsuya Sakai ; Manabu Okumura</p><p>Abstract: We propose a new method for query-oriented extractive multi-document summarization. To enrich the information need representation of a given query, we build a co-occurrence graph to obtain words that augment the original query terms. We then formulate the summarization problem as a Maximum Coverage Problem with Knapsack Constraints based on word pairs rather than single words. Our experiments with the NTCIR ACLIA question answering test collections show that our method achieves a pyramid F3-score of up to 0.3 13, a 36% improvement over a baseline using Maximal Marginal Relevance. 1</p><p>4 0.14990728 <a title="4-tfidf-4" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>Author: Dong Wang ; Yang Liu</p><p>Abstract: This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.</p><p>5 0.12798893 <a title="4-tfidf-5" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>Author: William M. Darling ; Fei Song</p><p>Abstract: Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.</p><p>6 0.12173167 <a title="4-tfidf-6" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>7 0.11210857 <a title="4-tfidf-7" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>8 0.10511693 <a title="4-tfidf-8" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>9 0.10148533 <a title="4-tfidf-9" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>10 0.095433734 <a title="4-tfidf-10" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>11 0.094610006 <a title="4-tfidf-11" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>12 0.088385694 <a title="4-tfidf-12" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>13 0.07457754 <a title="4-tfidf-13" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>14 0.073704109 <a title="4-tfidf-14" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>15 0.065126732 <a title="4-tfidf-15" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>16 0.063266955 <a title="4-tfidf-16" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<p>17 0.062388349 <a title="4-tfidf-17" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>18 0.059291672 <a title="4-tfidf-18" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>19 0.057402484 <a title="4-tfidf-19" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>20 0.05225873 <a title="4-tfidf-20" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.133), (1, 0.052), (2, -0.045), (3, 0.108), (4, -0.073), (5, -0.074), (6, -0.103), (7, 0.138), (8, 0.042), (9, 0.003), (10, 0.013), (11, 0.055), (12, -0.123), (13, 0.05), (14, -0.231), (15, -0.032), (16, 0.057), (17, 0.021), (18, -0.026), (19, 0.057), (20, -0.076), (21, -0.09), (22, 0.074), (23, 0.022), (24, -0.051), (25, -0.059), (26, 0.069), (27, -0.171), (28, 0.119), (29, -0.195), (30, 0.042), (31, -0.073), (32, 0.148), (33, -0.115), (34, 0.083), (35, 0.254), (36, -0.127), (37, 0.0), (38, 0.043), (39, -0.05), (40, -0.066), (41, -0.084), (42, 0.033), (43, -0.04), (44, 0.151), (45, 0.304), (46, -0.067), (47, 0.044), (48, -0.123), (49, -0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93595803 <a title="4-lsi-1" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</p><p>2 0.73462987 <a title="4-lsi-2" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We cast the word alignment problem as maximizing a submodular function under matroid constraints. Our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. We show that submodularity naturally arises when modeling word fertility. Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.</p><p>3 0.55145401 <a title="4-lsi-3" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>Author: Hajime Morita ; Tetsuya Sakai ; Manabu Okumura</p><p>Abstract: We propose a new method for query-oriented extractive multi-document summarization. To enrich the information need representation of a given query, we build a co-occurrence graph to obtain words that augment the original query terms. We then formulate the summarization problem as a Maximum Coverage Problem with Knapsack Constraints based on word pairs rather than single words. Our experiments with the NTCIR ACLIA question answering test collections show that our method achieves a pyramid F3-score of up to 0.3 13, a 36% improvement over a baseline using Maximal Marginal Relevance. 1</p><p>4 0.48342305 <a title="4-lsi-4" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Gillick ; Dan Klein</p><p>Abstract: We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.</p><p>5 0.45899725 <a title="4-lsi-5" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>6 0.41783467 <a title="4-lsi-6" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>7 0.38850603 <a title="4-lsi-7" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>8 0.37882766 <a title="4-lsi-8" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>9 0.37197077 <a title="4-lsi-9" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>10 0.36560854 <a title="4-lsi-10" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>11 0.32267648 <a title="4-lsi-11" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>12 0.31392536 <a title="4-lsi-12" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>13 0.30844852 <a title="4-lsi-13" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>14 0.28043804 <a title="4-lsi-14" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>15 0.26614353 <a title="4-lsi-15" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>16 0.26547015 <a title="4-lsi-16" href="./acl-2011-Automatic_Headline_Generation_using_Character_Cross-Correlation.html">51 acl-2011-Automatic Headline Generation using Character Cross-Correlation</a></p>
<p>17 0.25287145 <a title="4-lsi-17" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>18 0.23716846 <a title="4-lsi-18" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>19 0.23288797 <a title="4-lsi-19" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>20 0.23020585 <a title="4-lsi-20" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.025), (17, 0.045), (25, 0.078), (26, 0.021), (37, 0.074), (39, 0.049), (41, 0.068), (50, 0.168), (55, 0.03), (59, 0.045), (72, 0.021), (76, 0.021), (91, 0.029), (96, 0.209), (97, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92723441 <a title="4-lda-1" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>Author: Tony Veale</p><p>Abstract: Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning. IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text’s potential relevance than its potential meaning. In contrast, FLP views language as a system of unstable signs that can be used to talk about the world in creative new ways. There is another key difference: IR is practical, scalable and robust, and in daily use by millions of casual users. FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab. This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1</p><p>2 0.87961841 <a title="4-lda-2" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>Author: Elias Ponvert ; Jason Baldridge ; Katrin Erk</p><p>Abstract: We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing—the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL.</p><p>3 0.87913489 <a title="4-lda-3" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Dan Gillick ; Dan Klein</p><p>Abstract: We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set.</p><p>same-paper 4 0.87242281 <a title="4-lda-4" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We design a class of submodular functions meant for document summarization tasks. These functions each combine two terms, one which encourages the summary to be representative of the corpus, and the other which positively rewards diversity. Critically, our functions are monotone nondecreasing and submodular, which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality. When evaluated on DUC 2004-2007 corpora, we obtain better than existing state-of-art results in both generic and query-focused document summarization. Lastly, we show that several well-established methods for document summarization correspond, in fact, to submodular function optimization, adding further evidence that submodular functions are a natural fit for document summarization.</p><p>5 0.83939445 <a title="4-lda-5" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>Author: Hui Lin ; Jeff Bilmes</p><p>Abstract: We cast the word alignment problem as maximizing a submodular function under matroid constraints. Our framework is able to express complex interactions between alignment components while remaining computationally efficient, thanks to the power and generality of submodular functions. We show that submodularity naturally arises when modeling word fertility. Experiments on the English-French Hansards alignment task show that our approach achieves lower alignment error rates compared to conventional matching based approaches.</p><p>6 0.80135745 <a title="4-lda-6" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>7 0.791587 <a title="4-lda-7" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>8 0.7913661 <a title="4-lda-8" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>9 0.79111713 <a title="4-lda-9" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>10 0.79096204 <a title="4-lda-10" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>11 0.79007411 <a title="4-lda-11" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>12 0.78791946 <a title="4-lda-12" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>13 0.78751659 <a title="4-lda-13" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>14 0.78709412 <a title="4-lda-14" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>15 0.78698361 <a title="4-lda-15" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>16 0.7867654 <a title="4-lda-16" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>17 0.78614312 <a title="4-lda-17" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>18 0.78552645 <a title="4-lda-18" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>19 0.7852385 <a title="4-lda-19" href="./acl-2011-Automatically_Evaluating_Text_Coherence_Using_Discourse_Relations.html">53 acl-2011-Automatically Evaluating Text Coherence Using Discourse Relations</a></p>
<p>20 0.78520012 <a title="4-lda-20" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
