<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-149" href="#">acl2011-149</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</h1>
<br/><p>Source: <a title="acl-2011-149-pdf" href="http://aclweb.org/anthology//P/P11/P11-2115.pdf">pdf</a></p><p>Author: Nina Dethlefs ; Heriberto Cuayahuitl</p><p>Abstract: Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines.</p><p>Reference: <a title="acl-2011-149-reference" href="../acl2011_reference/acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. [sent-2, score-0.892]
</p><p>2 We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). [sent-3, score-0.898]
</p><p>3 To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. [sent-4, score-0.628]
</p><p>4 It is based on a generation space in the form of a Hidden Markov Model (HMM). [sent-5, score-0.179]
</p><p>5 Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines. [sent-6, score-0.191]
</p><p>6 1 Introduction Surface realisation decisions in a Natural Language Generation (NLG) system are often made accord-  ing to a language model of the domain (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Oh and Rudnicky, 2000; White, 2004; Belz, 2008). [sent-7, score-0.514]
</p><p>7 Also, in dialogue the most likely surface form may not always be appropriate, because it does not correspond to the user’s information need, the user is confused, or the most likely sequence is infelicitous with respect to the dialogue history. [sent-9, score-0.596]
</p><p>8 In such cases, it is important to optimise surface realisation in a unified fashion with content selection. [sent-10, score-0.863]
</p><p>9 Reinforcement Learning (RL) is an attractive framework for optimising a sequence of decisions given incomplete knowledge of the environment or best strategy to follow (Rieser et al. [sent-14, score-0.363]
</p><p>10 Since an HRL agent will ultimately learn the behaviour it is rewarded for, the reward function is arguably the agent’s most crucial component. [sent-17, score-0.573]
</p><p>11 Previous work has therefore suggested to learn a reward function from human data as in the PARADISE framework (Walker et al. [sent-18, score-0.326]
</p><p>12 Since PARADISE-based reward functions typically rely on objective metrics, they are not ideally suited for surface realisation, which is more dependend on linguistic phenomena, e. [sent-20, score-0.548]
</p><p>13 The contribution of this paper is therefore to induce a reward function from human data, specifically suited for surface generation. [sent-24, score-0.592]
</p><p>14 To this end, we train HMMs (Rabiner, 1989) on a corpus of grammatical word sequences and use them to inform the agent’s learning process. [sent-25, score-0.053]
</p><p>15 In addition, we suggest to optimise surface realisation and content selection decisions in a joint, rather than isolated, fashion. [sent-26, score-1.01]
</p><p>16 Results show that our combined approach generates more successful and human-like  utterances than a greedy or random baseline. [sent-27, score-0.122]
</p><p>17 2  Generation Spaces  We are concerned with the generation of navigation instructions in a virtual 3D world as in the GIVE scenario (Koller et al. [sent-38, score-0.451]
</p><p>18 In this task, two people engage in a ‘treasure hunt’, where one participant navigates the other through the world, pressing a sequence of buttons and completing the task by obtaining a trophy. [sent-40, score-0.063]
</p><p>19 We use HRL to optimise decisions of content selection (‘what to say’) and HMMs for decisions of surface realisation (‘how to say it’). [sent-46, score-1.122]
</p><p>20 Content selection involves whether to use a low-, or high-level navigation strategy. [sent-47, score-0.193]
</p><p>21 The former consists of a sequence of primitive instructions (‘go straight’, ‘turn left’), the latter represents contractions of se-  quences of low-level instructions (‘head to the next room’). [sent-48, score-0.337]
</p><p>22 Content selection also involves choosing a level of detail for the instruction corresponding to the user’s information need. [sent-49, score-0.154]
</p><p>23 We evaluate the learnt content selection decisions in terms of task success. [sent-50, score-0.408]
</p><p>24 For surface realisation, we use HMMs to inform the HRL agent’s learning process. [sent-51, score-0.287]
</p><p>25 655 the one-to-many relationship arising between a semantic form (from the content selection stage) and its possible realisations. [sent-53, score-0.19]
</p><p>26 Semantic forms of instructions have an average of 650 surface realisations, including syntactic and lexical variation, and decisions of granularity. [sent-54, score-0.457]
</p><p>27 We refer to the set of alternative realisations of a semantic form as its ‘generation space’. [sent-55, score-0.039]
</p><p>28 We evaluate the learnt surface realisation  decisions in terms of similarity with human data. [sent-57, score-0.886]
</p><p>29 Note that while we treat content selection and surface realisation as separate NLG tasks, their optimisation is achieved jointly. [sent-58, score-0.898]
</p><p>30 This is due to a tradeoff arising between the two tasks. [sent-59, score-0.032]
</p><p>31 In such situations, it is important to treat content selection and surface realisation as a unified whole. [sent-61, score-0.83]
</p><p>32 2 that their joint optimisation leads to better results than an isolated optimisation as in, for example, a two-stage model. [sent-63, score-0.208]
</p><p>33 Such states describe the system’s knowledge about the generation task (e. [sent-66, score-0.226]
</p><p>34 ‘use high level navigation strategy ’, ‘use imperative mood’, etc. [sent-71, score-0.167]
</p><p>35 The reward function assigns a numeric value for each action taken. [sent-73, score-0.423]
</p><p>36 In this way, language gen-  Figure 2: Hierarchy of learning agents (left), where shaded agents use an HMM-based reward function. [sent-74, score-0.498]
</p><p>37 The top three layers are responsible for content selection (CS) decisions and use HRL. [sent-75, score-0.27]
</p><p>38 The shaded agents in the bottom use HRL with an HMM-based reward function and joint optimisation of content selection and surface realisation (SR). [sent-76, score-1.332]
</p><p>39 example HMM, representing the generation space of ‘destination’  An  instructions, is shown on the right. [sent-79, score-0.179]
</p><p>40 eration can be seen as a finite sequence of states, actions and rewards {s0, a0, r1, s1, a1, . [sent-80, score-0.294]
</p><p>41 , rt−1 , st}, wachtieorne sth aen goal aisr to {fisnd an optimal strategy automatically. [sent-83, score-0.045]
</p><p>42 To do this we use RL with a divide-andconquer approach to optimise a hierarchy of generation policies rather than a single policy. [sent-84, score-0.419]
</p><p>43 The hierarchy of RL agents consists of L levels and N models  per level, denoted as Mji, where j ∈ {0, . [sent-85, score-0.159]
</p><p>44 Sji is a set of states, Aji is a set of actions, Tji is a transition function that determines the next state s′ from the current state s and the performed action a, and Rji is a reward function that specifies the reward that an agent receives for taking an action a in state s lasting τ time steps. [sent-98, score-1.172]
</p><p>45 The random variable τ represents the number of time steps the agent takes to complete a subtask. [sent-99, score-0.18]
</p><p>46 The goal of each SMDP is to find an optimal policy that maximises the reward for each visited state, according to π∗ij(s) = argmaxa∈Aji Q∗ij(s,a), where Qj∗i(s,a) specifies the expected cumulative reward for executing action a in state s and then following policy π∗ij. [sent-102, score-1.046]
</p><p>47 We use HSMQ-Learning (Dietterich, 1999) to learn a hierarchy of generation policies. [sent-103, score-0.23]
</p><p>48 2  Hidden Markov Models for NLG  The idea of representing the generation space of a surface realiser as an HMM can be roughly defined as the converse of POS tagging, where an input string of words is mapped onto a hidden sequence of POS tags. [sent-105, score-0.509]
</p><p>49 , ‘actor’, ‘process’, ‘destination’),2 what is the most likely sequence of words corresponding to the symbols? [sent-108, score-0.063]
</p><p>50 We treat states as representing words, and sequences of states i0. [sent-110, score-0.094]
</p><p>51 on consists of a finite set of semantic symbols specific to the instruction type (i. [sent-117, score-0.083]
</p><p>52 Each symbol has an observation likelihood bi (ot), which gives the probability of observing o in state iat time t. [sent-120, score-0.037]
</p><p>53 The transition and emission probabilities are learnt during training using the Baum-Welch algorithm. [sent-121, score-0.138]
</p><p>54 Subsequently, we constructed the HMMs from the CFGs, one for each instruction type, and trained them on the annotated data. [sent-123, score-0.083]
</p><p>55 3 An HMM-based Reward Function Induced from Human Data Due to its unique function in an RL framework, we suggest to induce a reward function for surface realisation from human data. [sent-126, score-1.006]
</p><p>56 To this end, we create and train HMMs to represent the generation space of a particular surface realisation task. [sent-127, score-0.815]
</p><p>57 We then use the forward probability, derived from the Forward algorithm, of an observation sequence to inform the agent’s learning process. [sent-128, score-0.116]
</p><p>58 ow  Whenever the agent has generated a word sequence w0. [sent-132, score-0.243]
</p><p>59 wn, the HMM assigns a reward corresponding to the likelihood of observing the sequence in the data. [sent-135, score-0.345]
</p><p>60 In addition, the agent is rewarded for short interactions at maximal task success3 and optimal content selection (cf. [sent-136, score-0.373]
</p><p>61 4, the other rewards apply to all agents of the hierarchy. [sent-144, score-0.252]
</p><p>62 4  Experimental Setting  We test our approach using the (hand-crafted) hierarchy of generation subtasks in Figure 2. [sent-145, score-0.274]
</p><p>63 It consists of a root agent (M00), and subtasks for low-level (M02) and high-level (M12) navigation strategies (M11), and for instruction types ‘orientation’ (M03), ‘straight’ (M13), ‘direction’ (M23), ‘path’ (M33) and destination’ (M43). [sent-146, score-0.429]
</p><p>64 They will be trained using HRL with an HMM-based reward function induced from human data. [sent-151, score-0.362]
</p><p>65 Finally, subtask M01 can repair a previous system utterance. [sent-153, score-0.039]
</p><p>66 The states of the agent contain  all situational and linguistic information relevant to its decision making, e. [sent-154, score-0.227]
</p><p>67 , the spatial environment, 3Task success is addressed by that each be ‘accepted’ by the user (cf. [sent-156, score-0.16]
</p><p>68 utterance  needs  to  657 discourse history, and status of grounding. [sent-159, score-0.047]
</p><p>69 We distinguish primitive actions (corresponding to single generation decisions) and composite actions (corresponding to generation subtasks (Fig. [sent-162, score-0.628]
</p><p>70 1 The Simulated Environment The simulated environment contains two kinds of uncertainties: (1) uncertainty regarding the state of the environment, and (2) uncertainty concerning the user’s reaction to a system utterance. [sent-165, score-0.253]
</p><p>71 The first aspect is represented by a set of contextual variables describing the environment, 5 and user behaviour. [sent-166, score-0.095]
</p><p>72 The uncertainty regarding  the user’s reaction to an utterance is represented by a Naive Bayes classifier, which is passed a set of contextual features describing the situation, mapped with a set of semantic features describing the utterance. [sent-169, score-0.166]
</p><p>73 7 From these data, the classifier specifies the most likely user reaction (after each system act) of perform desired action, perform undesired action, wait and request help. [sent-170, score-0.288]
</p><p>74 2 Comparison of Generation Policies We trained three different generation policies. [sent-173, score-0.179]
</p><p>75 The learnt policy optimises content selection and surface realisation decisions in a unified fashion, and is informed by an HMM-based generation space reward function. [sent-174, score-1.679]
</p><p>76 The greedy policy is informed only by the HMM and always chooses the most 4An example for the state variables of model M11 are the annotation values in Fig. [sent-175, score-0.297]
</p><p>77 9 Figure 3, which plots the average rewards of all three policies (averaged over ten runs), shows that the ‘learnt’ policy performs best in terms of task success by reaching the highest overall rewards over time. [sent-182, score-0.544]
</p><p>78 An absolute comparison of the average rewards (rescaled from 0 to 1) of the last 1000 training episodes of each policy shows that greedy improves ‘any valid sequence’ by 71%, and learnt improves greedy by 29% (these differences are significant at p < 0. [sent-183, score-0.753]
</p><p>79 This is due to the learnt policy showing more adaptation to contextual features than the greedy or ‘valid sequence’ policies. [sent-185, score-0.398]
</p><p>80 word sequences) using Precision-Recall based on the FMeasure score, and dialogue similarity based on the Kulback-Leibler (KL) divergence (Cuay ´ahuitl et al. [sent-188, score-0.102]
</p><p>81 The former shows how the texts generated by each of our generation policies compare to humanauthored texts in terms of precision and recall. [sent-190, score-0.264]
</p><p>82 While the greedy policy receives higher F-Measure scores, the learnt policy is most similar to the human data. [sent-193, score-0.536]
</p><p>83 This is due to variation: in contrast to greedy behaviour, which always exploits the most likely variant, the learnt policy varies surface forms. [sent-194, score-0.632]
</p><p>84 6  Conclusion  We have presented a novel approach to optimising surface realisation using HRL. [sent-197, score-0.714]
</p><p>85 We suggested to inform an HRL agent’s learning process by an HMM-based reward function, which was induced 9For training, the step-size parameter SMDP) , which indicates the learning rate,  (one for each was initiated with α  1and then reduced over time by α = 1+1t, where t is the time step. [sent-198, score-0.371]
</p><p>86 The discount rate γ, which indicates the relevance of future rewards in relation to immediate rewards, was set to 0. [sent-199, score-0.144]
</p><p>87 99, and the probability of a random action ǫ was 0. [sent-200, score-0.097]
</p><p>88 2 Episodes  x  104  ‘any  Figure 3: Performance of ‘learnt’, ‘greedy’, and valid sequence’ generation behaviours (average rewards). [sent-207, score-0.274]
</p><p>89 06  Table 1: Evaluation of generation behaviours Precision-Recall and KL-divergence. [sent-217, score-0.227]
</p><p>90 with  from data and in which the HMM represents the generation space of a surface realiser. [sent-218, score-0.413]
</p><p>91 We also proposed to jointly optimise surface realisation and content selection to balance the tradeoffs of (a) frequency in terms of a language model, (b) alignment/consistency vs variation, (c) properties of the user and environment. [sent-219, score-0.993]
</p><p>92 Results showed that our hybrid approach outperforms two baselines in terms of task success and human-likeness: a greedy baseline acting independent of content selection, and a random ‘valid sequence’ baseline. [sent-220, score-0.242]
</p><p>93 Also, a detailed human evaluation study is needed to assess the effects of different surface form variants. [sent-222, score-0.234]
</p><p>94 Finally, other graphical models besides HMMs, such as Bayesian Networks, can be explored for informing the surface realisation process of a generation system. [sent-223, score-0.815]
</p><p>95 Automatic generation of weather forecast texts using comprehensiveprobabilistic generation-space models. [sent-244, score-0.179]
</p><p>96 The GIVE-2 corpus of giving instructions in virtual environments. [sent-272, score-0.15]
</p><p>97 Learning to adapt to unknown users: referring expression generation in spoken dialogue systems. [sent-282, score-0.281]
</p><p>98 The first challenge on generating instructions in virtual environments. [sent-287, score-0.15]
</p><p>99 Phrase-based statistical language generation using graphical models and active learning. [sent-302, score-0.179]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('realisation', 0.402), ('reward', 0.282), ('hrl', 0.24), ('surface', 0.234), ('agent', 0.18), ('generation', 0.179), ('cuay', 0.144), ('rewards', 0.144), ('policy', 0.138), ('learnt', 0.138), ('reinforcement', 0.133), ('greedy', 0.122), ('navigation', 0.122), ('dethlefs', 0.12), ('nlg', 0.117), ('decisions', 0.112), ('instructions', 0.111), ('agents', 0.108), ('hmms', 0.105), ('optimisation', 0.104), ('optimise', 0.104), ('dialogue', 0.102), ('action', 0.097), ('heriberto', 0.096), ('huitl', 0.096), ('user', 0.095), ('destination', 0.091), ('content', 0.087), ('actions', 0.087), ('reaction', 0.087), ('policies', 0.085), ('route', 0.085), ('instruction', 0.083), ('optimising', 0.078), ('belz', 0.073), ('aji', 0.072), ('smdp', 0.072), ('selection', 0.071), ('straight', 0.069), ('hmm', 0.065), ('environment', 0.065), ('sequence', 0.063), ('nina', 0.063), ('rl', 0.062), ('lemon', 0.058), ('pickering', 0.058), ('inform', 0.053), ('primitive', 0.052), ('halliday', 0.052), ('hierarchy', 0.051), ('oliver', 0.05), ('behaviours', 0.048), ('janarthanam', 0.048), ('levelt', 0.048), ('rji', 0.048), ('sji', 0.048), ('valid', 0.047), ('states', 0.047), ('utterance', 0.047), ('strategy', 0.045), ('function', 0.044), ('subtasks', 0.044), ('koller', 0.042), ('paradise', 0.042), ('angeli', 0.042), ('episodes', 0.042), ('gargett', 0.042), ('rieser', 0.042), ('hierarchical', 0.041), ('virtual', 0.039), ('undesired', 0.039), ('oberlander', 0.039), ('repair', 0.039), ('garrod', 0.039), ('mairesse', 0.039), ('realisations', 0.039), ('variation', 0.039), ('state', 0.037), ('maximises', 0.037), ('inlg', 0.037), ('anja', 0.037), ('hasan', 0.037), ('langkilde', 0.037), ('induced', 0.036), ('unified', 0.036), ('orientation', 0.036), ('specifies', 0.035), ('rewarded', 0.035), ('reiter', 0.035), ('foster', 0.034), ('success', 0.033), ('sutton', 0.033), ('hidden', 0.033), ('uncertainty', 0.032), ('suited', 0.032), ('dfki', 0.032), ('wait', 0.032), ('spatial', 0.032), ('behaviour', 0.032), ('arising', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="149-tfidf-1" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>Author: Nina Dethlefs ; Heriberto Cuayahuitl</p><p>Abstract: Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines.</p><p>2 0.3285346 <a title="149-tfidf-2" href="./acl-2011-Underspecifying_and_Predicting_Voice_for_Surface_Realisation_Ranking.html">317 acl-2011-Underspecifying and Predicting Voice for Surface Realisation Ranking</a></p>
<p>Author: Sina Zarriess ; Aoife Cahill ; Jonas Kuhn</p><p>Abstract: This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice. 1</p><p>3 0.17357001 <a title="149-tfidf-3" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>Author: Luciana Benotti ; Alexandre Denis</p><p>Abstract: Virtual instructors can be used in several applications, ranging from trainers in simulated worlds to non player characters for virtual games. In this paper we present a novel algorithm for rapidly prototyping virtual instructors from human-human corpora without manual annotation. Automatically prototyping full-fledged dialogue systems from corpora is far from being a reality nowadays. Our algorithm is restricted in that only the virtual instructor can perform speech acts while the user responses are limited to physical actions in the virtual world. We evaluate a virtual instructor, generated using this algorithm, with human users. We compare our results both with human instructors and rule-based virtual instructors hand-coded for the same task.</p><p>4 0.12210999 <a title="149-tfidf-4" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>5 0.11165957 <a title="149-tfidf-5" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>Author: Paul Piwek ; Svetlana Stoyanchev</p><p>Abstract: This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.</p><p>6 0.10844024 <a title="149-tfidf-6" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>7 0.090137616 <a title="149-tfidf-7" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>8 0.08322449 <a title="149-tfidf-8" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>9 0.07457754 <a title="149-tfidf-9" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>10 0.072931096 <a title="149-tfidf-10" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>11 0.069181532 <a title="149-tfidf-11" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>12 0.068314768 <a title="149-tfidf-12" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>13 0.067312427 <a title="149-tfidf-13" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>14 0.054252628 <a title="149-tfidf-14" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>15 0.054177124 <a title="149-tfidf-15" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>16 0.050291196 <a title="149-tfidf-16" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>17 0.048089668 <a title="149-tfidf-17" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>18 0.048062023 <a title="149-tfidf-18" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>19 0.04650424 <a title="149-tfidf-19" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>20 0.045983966 <a title="149-tfidf-20" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.144), (1, 0.02), (2, -0.045), (3, 0.01), (4, -0.149), (5, 0.137), (6, -0.026), (7, 0.005), (8, -0.038), (9, 0.009), (10, 0.031), (11, 0.0), (12, 0.017), (13, 0.005), (14, -0.031), (15, -0.001), (16, 0.012), (17, -0.004), (18, -0.038), (19, 0.015), (20, 0.034), (21, 0.065), (22, 0.01), (23, -0.113), (24, -0.048), (25, -0.044), (26, -0.029), (27, -0.005), (28, -0.015), (29, -0.14), (30, 0.063), (31, 0.105), (32, 0.017), (33, 0.004), (34, 0.098), (35, 0.19), (36, -0.177), (37, 0.007), (38, 0.075), (39, -0.024), (40, 0.047), (41, -0.099), (42, -0.256), (43, 0.102), (44, -0.074), (45, -0.036), (46, -0.164), (47, 0.074), (48, -0.019), (49, 0.124)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95286453 <a title="149-lsi-1" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>Author: Nina Dethlefs ; Heriberto Cuayahuitl</p><p>Abstract: Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines.</p><p>2 0.83661127 <a title="149-lsi-2" href="./acl-2011-Underspecifying_and_Predicting_Voice_for_Surface_Realisation_Ranking.html">317 acl-2011-Underspecifying and Predicting Voice for Surface Realisation Ranking</a></p>
<p>Author: Sina Zarriess ; Aoife Cahill ; Jonas Kuhn</p><p>Abstract: This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice. 1</p><p>3 0.69257379 <a title="149-lsi-3" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>Author: Luciana Benotti ; Alexandre Denis</p><p>Abstract: Virtual instructors can be used in several applications, ranging from trainers in simulated worlds to non player characters for virtual games. In this paper we present a novel algorithm for rapidly prototyping virtual instructors from human-human corpora without manual annotation. Automatically prototyping full-fledged dialogue systems from corpora is far from being a reality nowadays. Our algorithm is restricted in that only the virtual instructor can perform speech acts while the user responses are limited to physical actions in the virtual world. We evaluate a virtual instructor, generated using this algorithm, with human users. We compare our results both with human instructors and rule-based virtual instructors hand-coded for the same task.</p><p>4 0.52970439 <a title="149-lsi-4" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>Author: Mariet Theune ; Ruud Koolen ; Emiel Krahmer ; Sander Wubben</p><p>Abstract: In this paper we investigate how much data is required to train an algorithm for attribute selection, a subtask of Referring Expressions Generation (REG). To enable comparison between different-sized training sets, a systematic training method was developed. The results show that depending on the complexity of the domain, training on 10 to 20 items may already lead to a good performance.</p><p>5 0.47180244 <a title="149-lsi-5" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>Author: Daniel de Kok ; Barbara Plank ; Gertjan van Noord</p><p>Abstract: An attractive property of attribute-value grammars is their reversibility. Attribute-value grammars are usually coupled with separate statistical components for parse selection and fluency ranking. We propose reversible stochastic attribute-value grammars, in which a single statistical model is employed both for parse selection and fluency ranking.</p><p>6 0.45325208 <a title="149-lsi-6" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>7 0.43980294 <a title="149-lsi-7" href="./acl-2011-Tier-based_Strictly_Local_Constraints_for_Phonology.html">303 acl-2011-Tier-based Strictly Local Constraints for Phonology</a></p>
<p>8 0.42503506 <a title="149-lsi-8" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>9 0.4202151 <a title="149-lsi-9" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>10 0.41215241 <a title="149-lsi-10" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>11 0.40410808 <a title="149-lsi-11" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>12 0.40201014 <a title="149-lsi-12" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>13 0.38542083 <a title="149-lsi-13" href="./acl-2011-Classifying_arguments_by_scheme.html">68 acl-2011-Classifying arguments by scheme</a></p>
<p>14 0.37744996 <a title="149-lsi-14" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>15 0.37092245 <a title="149-lsi-15" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>16 0.35854921 <a title="149-lsi-16" href="./acl-2011-An_Interface_for_Rapid_Natural_Language_Processing_Development_in_UIMA.html">42 acl-2011-An Interface for Rapid Natural Language Processing Development in UIMA</a></p>
<p>17 0.35121986 <a title="149-lsi-17" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>18 0.34919611 <a title="149-lsi-18" href="./acl-2011-Subjective_Natural_Language_Problems%3A_Motivations%2C_Applications%2C_Characterizations%2C_and_Implications.html">288 acl-2011-Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications</a></p>
<p>19 0.34467947 <a title="149-lsi-19" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>20 0.34445953 <a title="149-lsi-20" href="./acl-2011-Discrete_vs._Continuous_Rating_Scales_for_Language_Evaluation_in_NLP.html">99 acl-2011-Discrete vs. Continuous Rating Scales for Language Evaluation in NLP</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.035), (17, 0.032), (26, 0.022), (31, 0.013), (37, 0.051), (39, 0.029), (41, 0.058), (54, 0.301), (55, 0.023), (59, 0.076), (72, 0.064), (91, 0.039), (96, 0.145), (97, 0.018), (98, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.75550193 <a title="149-lda-1" href="./acl-2011-Lexicographic_Semirings_for_Exact_Automata_Encoding_of_Sequence_Models.html">210 acl-2011-Lexicographic Semirings for Exact Automata Encoding of Sequence Models</a></p>
<p>Author: Brian Roark ; Richard Sproat ; Izhak Shafran</p><p>Abstract: In this paper we introduce a novel use of the lexicographic semiring and motivate its use for speech and language processing tasks. We prove that the semiring allows for exact encoding of backoff models with epsilon transitions. This allows for off-line optimization of exact models represented as large weighted finite-state transducers in contrast to implicit (on-line) failure transition representations. We present preliminary empirical results demonstrating that, even in simple intersection scenarios amenable to the use of failure transitions, the use of the more powerful lexicographic semiring is competitive in terms of time of intersection. 1 Introduction and Motivation Representing smoothed n-gram language models as weighted finite-state transducers (WFST) is most naturally done with a failure transition, which reflects the semantics of the “otherwise” formulation of smoothing (Allauzen et al., 2003). For example, the typical backoff formulation of the probability of a word w given a history h is as follows P(w | h) = ?αPh(Pw( |w h )| h0) oift hc(ehrwwis)e > 0 (1) where P is an empirical estimate of the probability that reserves small finite probability for unseen n-grams; αh is a backoff weight that ensures normalization; and h0 is a backoff history typically achieved by excising the earliest word in the history h. The principle benefit of encoding the WFST in this way is that it only requires explicitly storing n-gram transitions for observed n-grams, i.e., count greater than zero, as opposed to all possible n-grams of the given order which would be infeasible in for example large vocabulary speech recognition. This is a massive space savings, and such an approach is also used for non-probabilistic stochastic language 1 models, such as those trained with the perceptron algorithm (Roark et al., 2007), as the means to access all and exactly those features that should fire for a particular sequence in a deterministic automaton. Similar issues hold for other finite-state se- quence processing problems, e.g., tagging, bracketing or segmenting. Failure transitions, however, are an implicit method for representing a much larger explicit automaton in the case of n-gram models, all possible n-grams for that order. During composition with the model, the failure transition must be interpreted on the fly, keeping track of those symbols that have already been found leaving the original state, and only allowing failure transition traversal for symbols that have not been found (the semantics of “otherwise”). This compact implicit representation cannot generally be preserved when composing with other models, e.g., when combining a language model with a pronunciation lexicon as in widelyused FST approaches to speech recognition (Mohri et al., 2002). Moving from implicit to explicit representation when performing such a composition leads to an explosion in the size of the resulting transducer, frequently making the approach intractable. In practice, an off-line approximation to the model is made, typically by treating the failure transitions as epsilon transitions (Mohri et al., 2002; Allauzen et al., 2003), allowing large transducers to be composed and optimized off-line. These complex approximate transducers are then used during first-pass – decoding, and the resulting pruned search graphs (e.g., word lattices) can be rescored with exact language models encoded with failure transitions. Similar problems arise when building, say, POStaggers as WFST: not every pos-tag sequence will have been observed during training, hence failure transitions will achieve great savings in the size of models. Yet discriminative models may include complex features that combine both input stream (word) and output stream (tag) sequences in a single feature, yielding complicated transducer topologies for which effective use of failure transitions may not Proceedings Pofo trhtlea 4nd9,th O Arnegnouna,l J Muneeet 1in9g-2 o4f, t 2h0e1 A1s.s ?oc ci2a0t1io1n A fosrso Ccioamtiopnut faotrio Cnoaml Lpiuntgauti osntiacls: Lsihnogrutpisatipcesrs, pages 1–5, be possible. An exact encoding using other mechanisms is required in such cases to allow for off-line representation and optimization. In this paper, we introduce a novel use of a semiring the lexicographic semiring (Golan, 1999) which permits an exact encoding of these sorts of models with the same compact topology as with failure transitions, but using epsilon transitions. Unlike the standard epsilon approximation, this semiring allows for an exact representation, while also allowing (unlike failure transition approaches) for off-line – – composition with other transducers, with all the optimizations that such representations provide. In the next section, we introduce the semiring, followed by a proof that its use yields exact representations. We then conclude with a brief evaluation of the cost of intersection relative to failure transitions in comparable situations. 2 The Lexicographic Semiring Weighted automata are automata in which the transitions carry weight elements of a semiring (Kuich and Salomaa, 1986). A semiring is a ring that may lack negation, with two associative operations ⊕ and ⊗lac akn nde tghaetiiro nre,ws piethcti twveo i dasesnotictiya ievleem oepnertas t 0io annsd ⊕ ⊕ 1. a nAd ⊗com anmdo tnh esierm reirsipnegc tiivn es pideeenchti ayn edl elmanegnutasg 0e processing, and one that we will be using in this paper, is the tropical semiring (R ∪ {∞}, min, +, ∞, 0), i.e., tmhein t riosp tihcea l⊕ s omfi trhineg gs(e mRi∪rin{g∞ (w},imthi nid,e+nt,i∞ty ,∞0)), ia.end., m+ ins tihse t ⊗e o⊕f othfe t hseem seirminigri n(wg i(wth tidhe indteitnyt t0y). ∞Th)i asn ids a+pp isro thpreia ⊗te o ofof rth h pee srfeomrmiri nngg (Vwitietrhb iid seenatricthy u0s).in Tgh negative log probabilities – we add negative logs along a path and take the min between paths. A hW1 , W2 . . . Wni-lexicographic weight is a tupleA o hfW weights wherei- eeaxichco gorfa pthhiec w weeiigghhtt cisla ass teusW1, W2 . . . Wn, must observe the path property (Mohri, 2002). The path property of a semiring K is defined in terms of the natural order on K such that: a  <2 ws e3 w&o4;)r The term “lexicographic” is an apt term for this semiring since the comparison for ⊕ is like the lexicseomgriaripnhgic s icnocmep thareis coonm opfa srtisrionngs f,o rco ⊕m ipsa lrikineg t thhee l feixrist- elements, then the second, and so forth. 3 Language model encoding 3.1 Standard encoding For language model encoding, we will differentiate between two classes of transitions: backoff arcs (labeled with a φ for failure, or with ? using our new semiring); and n-gram arcs (everything else, labeled with the word whose probability is assigned). Each state in the automaton represents an n-gram history string h and each n-gram arc is weighted with the (negative log) conditional probability of the word w labeling the arc given the history h. For a given history h and n-gram arc labeled with a word w, the destination of the arc is the state associated with the longest suffix of the string hw that is a history in the model. This will depend on the Markov order of the n-gram model. For example, consider the trigram model schematic shown in Figure 1, in which only history sequences of length 2 are kept in the model. Thus, from history hi = wi−2wi−1, the word wi transitions to hi+1 = wi−1wi, w2hii−ch1 is the longest suffix of hiwi in the modie−l1. As detailed in the “otherwise” semantics of equation 1, backoff arcs transition from state h to a state h0, typically the suffix of h of length |h| − 1, with we,i tgyhpti c(a−lllyog th αeh s)u. Wixe o cfa hll othf ele ndgestthin |hat|io −n 1s,ta wtei ah bwaecikgohtff s−taltoe.g αThis recursive backoff topology terminates at the unigram state, i.e., h = ?, no history. Backoff states of order k may be traversed either via φ-arcs from the higher order n-gram of order k + 1or via an n-gram arc from a lower order n-gram of order k −1. This means that no n-gram arc can enter tohred ezre rko−eth1. .o Trhdiesr mstaeaten s(fi tnhaalt bnaoc nk-ogfrfa),m ma andrc f cualln-o enrdteerr states history strings of length n − 1 for a model sotfa toersde —r n h may ihnagvse o n-gram a nrc −s e 1nt feorri nag m forodeml other full-order states as well as from backoff states of history size n − 2. — s—to 3.2 Encoding with lexicographic semiring For an LM machine M on the tropical semiring with failure transitions, which is deterministic and has the wih-2 =i1wφ/-logPwα(hi-1|whiφ)/-logwPhiα(+-1w i=|-1wiφ)/-logPαw(hi+)1 Figure 1: Deterministic finite-state representation of n-gram models with negative log probabilities (tropical semiring). The symbol φ labels backoff transitions. Modified from Roark and Sproat (2007), Figure 6.1. path property, we can simulate φ-arcs in a standard LM topology by a topologically equivalent machine M0 on the lexicographic hT, Ti semiring, where φ has boenen th hreep l eaxciceod gwraitphh eicps hilTo,nT, ais sfeomlloirwinsg. ,F worh every n-gram arc with label w and weight c, source state si and destination state sj, construct an n-gram arc with label w, weight h0, ci, source state si0, and deswtiniathtio lanb estla wte, s0j. gThhte h e0x,citi c, soosut rocfe e satcahte s state is constructed as follows. If the state is non-final, h∞, ∞i . sOttruhectrewdis aes fifo litl ofiwnsa.l Iwf tihthe e sxtiatt ec iosst n co nit- fwinilall ,b he∞ ∞h0,,∞ ∞cii . hLeertw n sbee tfh iet length oithf th exei longest history string iin. the model. For every φ-arc with (backoff) weight c, source state si, and destination state sj representing a history of length k, construct an ?-arc with source state si0, destination state s0j, and weight hΦ⊗(n−k) , ci, where Φ > 0 and Φ⊗(n−k) takes Φ to the (n − k)th power with the ⊗ operation. In the tropical semiring, ⊗ ris w +, so Φe⊗ ⊗(n o−pke) = (n − k)Φ. tFroorp iecxaalm sepmlei,r i nng a, t⊗rigi sra +m, msoo Φdel, if we= =ar (en b −ac kki)nΦg. off from a bigram state h (history length = 1) to a unigram state, n − k = 2 − 0 = 2, so we set the buanicgkroafmf w steaigteh,t nto −h2 kΦ, = =− l2og − α 0h) = =for 2 ,s soome w Φe s>et 0 th. cInk ofrfd were gtoh tco tom hb2iΦn,e −thleo gmαodel with another automaton or transducer, we would need to also convert those models to the hT, Ti semiring. For these aveutrotm thaotsae, mwoed seilmsp toly t uese hT a, Tdeif saeumlt rtrinagn.sf Foromra thtieosen such that every transition with weight c is assigned weight h0, ci . For example, given a word lattice wL,e iwghe tco h0n,vceir.t the lattice to L0 in the lexicographic semiring using this default transformation, and then perform the intersection L0 ∩ M0. By removing epsilon transitions and determ∩in Mizing the result, the low cost path for any given string will be retained in the result, which will correspond to the path achieved with Finally we project the second dimension of the hT, Ti weights to produce a lattice dini mtheen strioonpi ocfal t seem hTir,iTngi, wweihgichhts i tso e pqruoidvuacleen at ltaot tichee 3 result of L ∩ M, i.e., φ-arcs. C2(det(eps-rem(L0 ∩ M0))) = L ∩ M where C2 denotes projecting the second-dimension wofh tehree ChT, Ti weights, det(·) denotes determinizatoifon t,h aen hdT e,pTsi-r wemei(g·h) sde,n doette(s· )?- dreenmootveasl. d 4 Proof We wish to prove that for any machine N, ShortestPath(M0 ∩ N0) passes through the equivalent states in M0 to∩ t Nhose passed through in M for ShortestPath(M ∩ N) . Therefore determinization Sofh othrtee rsetsPualttihn(gM Mint ∩er Nse)c.ti Tonh rafefteorr e?- dreemteromvianl yzaiteilodns the same topology as intersection with the equivalent φ machine. Intuitively, since the first dimension of the hT, Ti weights is 0 for n-gram arcs and > 0 foofr t h beac hkTo,ffT arcs, tghhet ss ihsor 0te fostr p na-tghr awmil la rtcrasv aenrdse > >the 0 fewest possible backoff arcs; further, since higherorder backoff arcs cost less in the first dimension of the hT, Ti weights in M0, the shortest path will intchleud heT n-gram iagrhcst sa ti nth Meir earliest possible point. We prove this by induction on the state-sequence of the path p/p0 up to a given state si/si0 in the respective machines M/M0. Base case: If p/p0 is of length 0, and therefore the states si/si0 are the initial states of the respective machines, the proposition clearly holds. Inductive step: Now suppose that p/p0 visits s0...si/s00...si0 and we have therefore reached si/si0 in the respective machines. Suppose the cumulated weights of p/p0 are W and hΨ, Wi, respectively. We wish to show thaarte w Whic anhedv heΨr sj isi n reexspt evcitsiivteedly o. nW p (i.e., the path becomes s0...sisj) the equivalent state s0 is visited on p0 (i.e., the path becomes s00...si0s0j). Let w be the next symbol to be matched leaving states si and si0. There are four cases to consider: (1) there is an n-gram arc leaving states si and si0 labeled with w, but no backoff arc leaving the state; (2) there is no n-gram arc labeled with w leaving the states, but there is a backoff arc; (3) there is no ngram arc labeled with w and no backoff arc leaving the states; and (4) there is both an n-gram arc labeled with w and a backoff arc leaving the states. In cases (1) and (2), there is only one possible transition to take in either M or M0, and based on the algorithm for construction of M0 given in Section 3.2, these transitions will point to sj and s0j respectively. Case (3) leads to failure of intersection with either machine. This leaves case (4) to consider. In M, since there is a transition leaving state si labeled with w, the backoff arc, which is a failure transition, cannot be traversed, hence the destination of the n-gram arc sj will be the next state in p. However, in M0, both the n-gram transition labeled with w and the backoff transition, now labeled with ?, can be traversed. What we will now prove is that the shortest path through M0 cannot include taking the backoff arc in this case. In order to emit w by taking the backoff arc out of state si0, one or more backoff (?) transitions must be taken, followed by an n-gram arc labeled with w. Let k be the order of the history represented by state si0, hence the cost of the first backoff arc is h(n − k)Φ, −log(αsi0 )i in our semiring. If we tirsa vhe(rns e− km) Φ b,a−ckloofgf( αarcs) ip irnior o tro eemmiitrtiinngg. the w, the first dimension of our accumulated cost will be m(n −k + m−21)Φ, based on our algorithm for consmtr(unct−ionk +of M0 given in Section 3.2. Let sl0 be the destination state after traversing m backoff arcs followed by an n-gram arc labeled with w. Note that, by definition, m ≤ k, and k − m + 1 is the orbdeyr oeffi nstitaitoen ,sl0 m. B≤ as ked, onnd t khe − c mons +tru 1ct iiosn t ealg oor-rithm, the state sl0 is also reachable by first emitting w from state si0 to reach state s0j followed by some number of backoff transitions. The order of state s0j is either k (if k is the highest order in the model) or k + 1 (by extending the history of state si0 by one word). If it is of order k, then it will require m −1 backoff arcs to reach state sl0, one fewer tqhuainre t mhe− −pa1t hb ctok osftfat aer ss0l oth raeta cbheg sitanste w sith a backoff arc, for a total cost of (m − 1) (n − k + m−21)Φ which is less than m(n − k + m−21)Φ. If state s0j icish o ifs o lerdsser hka n+ m1,( th −er ke +will be m backoff arcs to reach state sl0, but with a total cost of m(n − (k + 1) + m−21)Φ m(n − k + m−23)Φ = which is also less than m(n − km + m−21)Φ. Hence twheh cstha ties asl0ls coa lne asl twhaayns mbe( n re −ac khe +d from si0 with a lower cost through state s0j than by first taking the backoff arc from si0. Therefore the shortest path on M0 must follow s00...si0s0j. 2 This completes the proof. 5 Experimental Comparison of ?, φ and hT, Ti encoded language models For our experiments we used lattices derived from a very large vocabulary continuous speech recognition system, which was built for the 2007 GALE Arabic speech recognition task, and used in the work reported in Lehr and Shafran (201 1). The lexicographic semiring was evaluated on the development 4 set (2.6 hours of broadcast news and conversations; 18K words). The 888 word lattices for the development set were generated using a competitive baseline system with acoustic models trained on about 1000 hrs of Arabic broadcast data and a 4-gram language model. The language model consisting of 122M n-grams was estimated by interpolation of 14 components. The vocabulary is relatively large at 737K and the associated dictionary has only single pronunciations. The language model was converted to the automaton topology described earlier, and represented in three ways: first as an approximation of a failure machine using epsilons instead of failure arcs; second as a correct failure machine; and third using the lexicographic construction derived in this paper. The three versions of the LM were evaluated by intersecting them with the 888 lattices of the development set. The overall error rate for the systems was 24.8%—comparable to the state-of-theart on this task1 . For the shortest paths, the failure and lexicographic machines always produced identical lattices (as determined by FST equivalence); in contrast, 81% of the shortest paths from the epsilon approximation are different, at least in terms of weights, from the shortest paths using the failure LM. For full lattices, 42 (4.7%) of the lexicographic outputs differ from the failure LM outputs, due to small floating point rounding issues; 863 (97%) of the epsilon approximation outputs differ. In terms of size, the failure LM, with 5.7 million arcs requires 97 Mb. The equivalent hT, Tillieoxnico argcrasp rheqicu iLreMs r 9e7qu Mireb.s 1 T20h eM ebq,u idvuael eton tth heT ,dToui-bling of the size of the weights.2 To measure speed, we performed the intersections 1000 times for each of our 888 lattices on a 2993 MHz Intel?R Xeon?R CPU, and took the mean times for each of our methods. The 888 lattices were processed with a mean of 1.62 seconds in total (1.8 msec per lattice) using the failure LM; using the hT, Ti-lexicographic iLnMg t rheequ fairieldur 1e.8 L Msec;o unsdinsg g(2 t.h0e m hTse,cT per lxaitctiocger)a, pahnidc is thus about 11% slower. Epsilon approximation, where the failure arcs are approximated with epsilon arcs took 1.17 seconds (1.3 msec per lattice). The 1The error rate is a couple of points higher than in Lehr and Shafran (2011) since we discarded non-lexical words, which are absent in maximum likelihood estimated language model and are typically augmented to the unigram backoff state with an arbitrary cost, fine-tuned to optimize performance for a given task. 2If size became an issue, the first dimension of the hT, TiweigIhft scizane bbee c raemprees aennt iesdsu eby, tah esi fnigrlste bdyimtee. slightly slower speeds for the exact method using the failure LM, and hT, Ti can be related to the overhfaeialdur eof L cMom, apnudtin hgT ,tThei f caailnur bee f urenlcattieodn aot rhuen toivmeer,and determinization, respectively. 6 Conclusion In this paper we have introduced a novel application of the lexicographic semiring, proved that it can be used to provide an exact encoding of language model topologies with failure arcs, and provided experimental results that demonstrate its efficiency. Since the hT, Ti-lexicographic semiring is both lefSt-i nacned hr iegh htT-d,iTstrii-bleuxtiicvoe,g roatphheirc o spetmimiriiznagtions such as minimization are possible. The particular hT, Ti-lexicographic semiring we have used thiceruel airs h bTu,t Toni-el oxifc many h piocss siebmlei ilnexgic woegr haapvheic u esendcodings. We are currently exploring the use of a lexicographic semiring that involves different semirings in the various dimensions, for the integration of part-of-speech taggers into language models. An implementation of the lexicographic semiring by the second author is already available as part of the OpenFst package (Allauzen et al., 2007). The methods described here are part of the NGram language-model-training toolkit, soon to be released at opengrm .org. Acknowledgments This research was supported in part by NSF Grant #IIS-081 1745 and DARPA grant #HR001 1-09-10041. Any opinions, findings, conclusions or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF or DARPA. We thank Maider Lehr for help in preparing the test data. We also thank the ACL reviewers for valuable comments. References Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003. Generalized algorithms for constructing statistical language models. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 40–47. Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. OpenFst: A general and efficient weighted finite-state transducer library. In Proceedings of the Twelfth International Conference on Implementation and Application of Automata (CIAA 2007), Lecture Notes in Computer Sci5 ence, volume 4793, pages 11–23, Prague, Czech Republic. Springer. Jonathan Golan. 1999. Semirings and their Applications. Kluwer Academic Publishers, Dordrecht. Werner Kuich and Arto Salomaa. 1986. Semirings, Automata, Languages. Number 5 in EATCS Monographs on Theoretical Computer Science. SpringerVerlag, Berlin, Germany. Maider Lehr and Izhak Shafran. 2011. Learning a discriminative weighted finite-state transducer for speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, July. Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2002. Weighted finite-state transducers in speech recognition. Computer Speech and Language, 16(1):69–88. Mehryar Mohri. 2002. Semiring framework and algorithms for shortest-distance problems. Journal of Automata, Languages and Combinatorics, 7(3):321–350. Brian Roark and Richard Sproat. 2007. Computational Approaches to Morphology and Syntax. Oxford University Press, Oxford. Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech and Language, 21(2):373–392.</p><p>same-paper 2 0.75271887 <a title="149-lda-2" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>Author: Nina Dethlefs ; Heriberto Cuayahuitl</p><p>Abstract: Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines.</p><p>3 0.65590835 <a title="149-lda-3" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>Author: Moshe Koppel ; Noam Ordan</p><p>Abstract: While it is has often been observed that the product of translation is somehow different than non-translated text, scholars have emphasized two distinct bases for such differences. Some have noted interference from the source language spilling over into translation in a source-language-specific way, while others have noted general effects of the process of translation that are independent of source language. Using a series of text categorization experiments, we show that both these effects exist and that, moreover, there is a continuum between them. There are many effects of translation that are consistent among texts translated from a given source language, some of which are consistent even among texts translated from families of source languages. Significantly, we find that even for widely unrelated source languages and multiple genres, differences between translated texts and non-translated texts are sufficient for a learned classifier to accurately determine if a given text is translated or original.</p><p>4 0.56813979 <a title="149-lda-4" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>Author: Luciana Benotti ; Alexandre Denis</p><p>Abstract: Virtual instructors can be used in several applications, ranging from trainers in simulated worlds to non player characters for virtual games. In this paper we present a novel algorithm for rapidly prototyping virtual instructors from human-human corpora without manual annotation. Automatically prototyping full-fledged dialogue systems from corpora is far from being a reality nowadays. Our algorithm is restricted in that only the virtual instructor can perform speech acts while the user responses are limited to physical actions in the virtual world. We evaluate a virtual instructor, generated using this algorithm, with human users. We compare our results both with human instructors and rule-based virtual instructors hand-coded for the same task.</p><p>5 0.53930521 <a title="149-lda-5" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>6 0.53626698 <a title="149-lda-6" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>7 0.53519684 <a title="149-lda-7" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>8 0.53161234 <a title="149-lda-8" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>9 0.5315268 <a title="149-lda-9" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>10 0.53080553 <a title="149-lda-10" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>11 0.53066349 <a title="149-lda-11" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>12 0.53018212 <a title="149-lda-12" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>13 0.52815652 <a title="149-lda-13" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>14 0.52812082 <a title="149-lda-14" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>15 0.52776128 <a title="149-lda-15" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>16 0.52751565 <a title="149-lda-16" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>17 0.52724266 <a title="149-lda-17" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>18 0.52681476 <a title="149-lda-18" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>19 0.5265075 <a title="149-lda-19" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>20 0.52634484 <a title="149-lda-20" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
