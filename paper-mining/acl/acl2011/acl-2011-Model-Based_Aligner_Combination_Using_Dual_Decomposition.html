<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-221" href="#">acl2011-221</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</h1>
<br/><p>Source: <a title="acl-2011-221-pdf" href="http://aclweb.org/anthology//P/P11/P11-1043.pdf">pdf</a></p><p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>Reference: <a title="acl-2011-221-reference" href="../acl2011_reference/acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. [sent-2, score-0.323]
</p><p>2 A similar model generating e from f will make different alignment predictions. [sent-3, score-0.34]
</p><p>3 Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. [sent-4, score-0.661]
</p><p>4 This paper presents a graphical model that embeds two directional aligners into a single model. [sent-5, score-0.748]
</p><p>5 Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. [sent-6, score-0.881]
</p><p>6 Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. [sent-7, score-0.892]
</p><p>7 The standard approach to word alignment employs directional Markov models that align the words of a sentence f to those of its translation e, such as IBM Model 4 (Brown et al. [sent-10, score-0.905]
</p><p>8 , 1993) or the HMM-based alignment model (Vogel et al. [sent-11, score-0.34]
</p><p>9 Machine translation systems typically combine the predictions of two directional models, one which aligns f to e and the other e to f (Och et al. [sent-13, score-0.634]
</p><p>10 Combination can reduce errors and relax the one-to-many structural restriction of directional models. [sent-15, score-0.539]
</p><p>11 Common combination methods include the union or intersection of directional alignments, as 420 Klaus Macherey Google Research kmach@ google com  . [sent-16, score-0.664]
</p><p>12 Inference in a probabilistic model resolves the conflicting predictions of two directional models, while taking into account each model’s uncertainty over its output. [sent-20, score-0.636]
</p><p>13 This result is achieved by embedding two directional HMM-based alignment models into a larger bidirectional graphical model. [sent-21, score-1.173]
</p><p>14 The full model structure and potentials allow the two embedded directional models to disagree to some extent, but reward agreement. [sent-22, score-0.731]
</p><p>15 Moreover, the bidirectional model enforces a one-to-one phrase alignment structure, similar to the output of phrase alignment models (Marcu and Wong, 2002; DeNero et al. [sent-23, score-1.045]
</p><p>16 However, we can employ dual decomposition as an approximate inference technique (Rush et al. [sent-28, score-0.524]
</p><p>17 In this approach, we iteratively apply the same efficient sequence algorithms for the underlying directional models, and thereby optimize a dual  bound on the model objective. [sent-30, score-0.835]
</p><p>18 Our model-based approach to aligner combination yields improvements in alignment quality and phrase extraction quality in Chinese-English experiments, relative to typical heuristic combinations methods applied to the predictions of independent directional models. [sent-33, score-1.061]
</p><p>19 Ac s2s0o1ci1a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 420–429, 2  Model Definition  Our bidirectional model G = (V, D) is a globally normalized, iuonndairle mctoedde graphical m,oDd)el i so fa th gleo bwaolrlyd alignment for a fixed sentence pair (e, f). [sent-36, score-0.679]
</p><p>20 Each vertex in the vertex set V corresponds to a model varitaebxle i Vi, aen vde reteaxch s uentd Vir ceoctrereds edge sin to oth ae edge ls veta rDicorresponds etoa a pair doirfe evcatreidab eldesg (Vi, Vj). [sent-37, score-0.404]
</p><p>21 P(v) ∝ vYi∈V ωi(vi) ·Y µij(vi,vj) Y (vi,Yvj)∈D Our model contains two directional hidden Markov alignment models, which we review in Section 2. [sent-41, score-0.853]
</p><p>22 1 HMM-Based Alignment Model This section describes the classic hidden Markov model (HMM) based alignment model (Vogel et al. [sent-45, score-0.392]
</p><p>23 P(f|e) aisl dye ifnindeedx ithne etwe rmords so fo a ela bteyn it alignment vector a, where aj = i indicates that word position i of e aligns to word position j of f. [sent-49, score-0.708]
</p><p>24 2 The highest probability word alignment vector under the model for a given sentence pair (e, f) can be computed exactly using the standard Viterbi algorithm for HMMs in O(|e|2 · |f|) time. [sent-57, score-0.366]
</p><p>25 ed trivially into a set of word alignment links A:  Aa = {(i, j) : aj = i,i  = 0} . [sent-59, score-0.717]
</p><p>26 We have defined a directional model that generates f from e. [sent-61, score-0.565]
</p><p>27 P(e,b|f) =Y|e|Df→e(bi|bi−1)Mf→e(ei|fbi) Yj=1  The vector b can be interpreted as a set of alignment links that is one-to-many: each value iappears at most once in the set. [sent-66, score-0.342]
</p><p>28 2 A Bidirectional Alignment Model We can combine two HMM-based directional alignment models by embedding them in a larger model 2In experiments, we set po = 10−6. [sent-69, score-0.969]
</p><p>29 that includes all of the random variables of two directional models, along with additional structure that promotes agreecm11(ean)t and resolves dci1s2(car)epancies. [sent-85, score-0.577]
</p><p>30 The original directional models include observed word sequences e and f, along with the two latent alignment vectors a and b defined in Section 2. [sent-86, score-0.838]
</p><p>31 = Me→f(fj|aeir)e  Hωjo(aw)(i) ωi(b)(j)  = Mf→e(ei|fj) The edge potentials between a and b encode the transition model in Equation 1. [sent-89, score-0.315]
</p><p>32 This matrix encodes the alignment links proposed by the bidirectional model: Ac = {(i, j) : cij = 1} . [sent-91, score-0.723]
</p><p>33 422 Each model node for anc 1e(lbe)ment cij ∈12 b{)0, 1} is connected to aj and bi via coherence edges. [sent-92, score-0.701]
</p><p>34 Instead, they are fixed functions that promote consistency between the integer-vaalu1ed directioana2l alignment av3ectors a and b and the boolean-valued matrix c. [sent-97, score-0.343]
</p><p>35 Consider the assignment aj = i, where i = 0 indicates that word fj is null-aligned, and i ≥ 1indicates that fj aligns tois ei. [sent-98, score-0.597]
</p><p>36 Tll-haeli gcnoheedr,e anncde potential ensures the following relationship between the vari-  able assignment aj = iand the variables ci0j, for any i0 ∈ [1, |e|] . [sent-99, score-0.521]
</p><p>37 Collectively, the list of cases above enforce an intuitive correspondence: an alignment aj = iensures that cij must be 1, adjacent neighbors may be 1 but incur a cost, and all other elements are 0. [sent-104, score-0.789]
</p><p>38 These yeodgue potential functions takes an integer value ifor some variable aj and a binary value k for some ci0j. [sent-106, score-0.438]
</p><p>39 In this way, we relax the one-to-many constraints of the directional models. [sent-111, score-0.539]
</p><p>40 However, all of the information about how words align is expressed by the vertex and edge potentials on a and b. [sent-112, score-0.337]
</p><p>41 The coherence edges and the link matrix c only serve to resolve conflicts between the directional models and communicate information between them. [sent-113, score-0.639]
</p><p>42 For any assignment to (a, b, c) with non-zero  probability, c must encode a one-to-one phrase alignment with a maximum phrase length of 3. [sent-117, score-0.443]
</p><p>43 For every pair of indices (i, j) and (i0, j0), the following cycle exists in the graph: cij → ci0j0  bi  → cij0 → aj0 →  →  bi0  → ci0j → aj → cij  Additional cycles also exist in the graph through the edges between aj−1 and aj and between bi−1 and bi. [sent-122, score-1.086]
</p><p>44 The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). [sent-123, score-0.393]
</p><p>45 The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al. [sent-130, score-0.524]
</p><p>46 The technique of dual decomposition has recently been shown to yield state-of-the-art performance in dependency parsing (Koo et al. [sent-133, score-0.426]
</p><p>47 2 Dual Problem Formulation To describe a dual decomposition inference procedure for our model, we first restate the inference problem under our graphical model in terms of the two overlapping subgraphs that admit tractable inference. [sent-136, score-0.827]
</p><p>48 In this case, the dual problem decomposes into two terms that are each local to an acyclic subgraph. [sent-144, score-0.325]
</p><p>49 As in previous work, we solve for the dual variable u by repeatedly performing inference in the two decoupled maximization problems. [sent-147, score-0.405]
</p><p>50 In fact, we can make a stronger claim: we can reuse the Viterbi inference algorithm for linear chain graphical models that applies to the embedded directional HMM models. [sent-152, score-0.783]
</p><p>51 ac Tthoree add dinittioo ntahel evremrtsex o potentials of this linear chain model, because the optimal 424  c(a)  Figure 3: The tree-structured subgraph Ga can be mapped Ftoi an equivalent c-hstraiunc-tsutrreudct suurbedgr amphod Gel by optimizing over ci0j for aj = i. [sent-156, score-0.59]
</p><p>52 choice of each cij can be determined from aj and the model parameters. [sent-157, score-0.553]
</p><p>53 If aj = i, then cij = 1according to our edge potential defined in Equation 2. [sent-158, score-0.658]
</p><p>54 Hence, setting aj = irequires the inclusion of the corre-  vertex potential ωj(a)(i),  sponding as well as u(i,j). [sent-159, score-0.52]
</p><p>55 For i0 i, either ci0j = 0, which contributes nothing to Equation 5, or ci0j = 1, which contributes u(i0, j) −α, according to our edge potential between aj a,njd) ci0j. [sent-160, score-0.532]
</p><p>56 Defining this potential allows us to collapse the  source-side sub-graph inference problem defined by Equation 5, into a simple linear chain model that only includes potential functions M0j and . [sent-162, score-0.306]
</p><p>57 4 Dual Decomposition Algorithm Now that we have the means to efficiently evaluate Equation 4 for fixed u, we can define the full dual decomposition algorithm for our model, which searches for a u that optimizes Equation 4. [sent-171, score-0.517]
</p><p>58 The full dual decomposition optimization procedure appears in Algorithm 1. [sent-174, score-0.458]
</p><p>59 5 Convergence and Early Stopping Our dual decomposition algorithm provides an inference method that is exact upon convergence. [sent-179, score-0.55]
</p><p>60 Therefore, our approach does not require any additional communication overhead relative to the independent directional models in a distributed  aligner implementation. [sent-190, score-0.644]
</p><p>61 4  Related Work  Alignment combination normally involves selecting some A from the output of two directional models. [sent-195, score-0.557]
</p><p>62 sCoommem Aon f approaches iuntc loufd tew forming otnhea lu mnioodne or intersection of the directional sets. [sent-196, score-0.556]
</p><p>63 , 2003), produce alignment link sets that include all of A∩ and some subsmete notf A∪ b saestsed th on itnhcel relationship of multiple links (Och e At al. [sent-199, score-0.342]
</p><p>64 In addition, supervised word alignment models often use the output of directional unsupervised aligners as features or pruning signals. [sent-201, score-0.971]
</p><p>65 In the case that a supervised model is restricted to proposing alignment links that appear in the output of a directional aligner, these models can be interpreted as a combination technique (Deng and Zhou, 2009). [sent-202, score-1.017]
</p><p>66 Such a model-based approach differs from ours in that it requires a supervised dataset and treats the directional aligners’ output as fixed. [sent-203, score-0.542]
</p><p>67 This approach to jointly learning two directional alignment models yields state-of-the-art unsupervised performance. [sent-206, score-0.838]
</p><p>68 In fact, we employ agreement-based training to estimate the parameters of the directional aligners in our experi-  ments. [sent-208, score-0.617]
</p><p>69 A parallel idea that closely relates to our bidirectional model is posterior regularization, which has also been applied to the word alignment problem (Gra ¸ca et al. [sent-209, score-0.6]
</p><p>70 This approach also yields state-of-the-art unsupervised alignment performance on some datasets, along with improvements in end-to-end translation quality (Ganchev et al. [sent-212, score-0.323]
</p><p>71 More importantly, we have changed the output space of the model to be a one-to-one phrase alignment via the coherence edge potential functions. [sent-216, score-0.614]
</p><p>72 Another similar line of work applies belief propagation to factor graphs that enforce a one-to-one word alignment (Cromi` eres and Kurohashi, 2009). [sent-217, score-0.403]
</p><p>73 Although differing in both model and inference, our work and theirs both find improvements from defining graphical models for alignment that do not admit exact polynomial-time inference algorithms. [sent-220, score-0.596]
</p><p>74 35%% Table 1: The bidirectional model’s dual decomposition algorithm substantially increases the overlap between the predictions of the directional models, measured by the number of links in their intersection. [sent-223, score-1.29]
</p><p>75 In this way, we can show that the bidirectional model improves alignment quality and enables the  extraction of more correct phrase pairs. [sent-225, score-0.623]
</p><p>76 We trained the model on a portion of FBIS data that has been used previously for alignment model evaluation (Ayan and Dorr, 2006; Haghighi et al. [sent-228, score-0.392]
</p><p>77 We trained the parameters of the directional models using the agreement training variant ofthe expectation maximization algorithm (Liang et al. [sent-232, score-0.613]
</p><p>78 Agreement-trained IBM Model 1 was used to initialize the parameters of the HMM-based alignment models (Brown et al. [sent-234, score-0.325]
</p><p>79 Both IBM Model 1 and the HMM alignment models were trained for 5 iterations on a 6. [sent-236, score-0.357]
</p><p>80 2 Convergence Analysis With n = 250 maximum iterations, our dual decomposition inference algorithm only converges 6. [sent-241, score-0.55]
</p><p>81 2% of the time, perhaps largely due to the fact that the two directional models have different one-to-many structural constraints. [sent-242, score-0.55]
</p><p>82 1R0462  Table 2: Alignment error rate results for the bidirectional model versus the baseline directional models. [sent-246, score-0.795]
</p><p>83 We can measure the agreement between models as the fraction of alignment links in the union A∪ that also appear in the intersection A∩ oef u tnhieo two directional models. [sent-254, score-0.999]
</p><p>84 Table 1 shows a 47% relative increase in the fraction of links that both models agree on by running dual decomposition (bidirectional), relative to independent directional inference (baseline). [sent-255, score-1.182]
</p><p>85 3  Alignment Error Evaluation  To evaluate alignment error of the baseline directional aligners, we must apply a combination procedure such as union or intersection to Aa and Ab. [sent-258, score-0.952]
</p><p>86 Likewise, ihn a osr udnerio to rev ianltuearstee alignment error Afor our combined model in cases where the inference algorithm does not converge, we must apply combiIn cases where the algorithm  nation to c(a) and c(b). [sent-259, score-0.49]
</p><p>87 First, we measure alignment error rate (AER), which compares the pro427 posed alignment set A to the sure set S and possible speots ePd i anl itghnem annotation, ow thheere s uSre ⊆ Pt S. [sent-262, score-0.576]
</p><p>88 The bidirectional model improves both precision and recall relative to all heuristic combination techniques, including grow-diag-final (Koehn et al. [sent-264, score-0.381]
</p><p>89 Extraction-based evaluations of alignment better coincide with the role of word aligners in machine translation systems (Ayan and Dorr, 2006). [sent-268, score-0.427]
</p><p>90 Finally, we evaluated our bidirectional model in a large-scale end-to-end phrase-based machine translation system from Chinese to English, based on  the alignment template approach (Och and Ney, 2004). [sent-278, score-0.605]
</p><p>91 The translation model weights were tuned for both the baseline and bidirectional alignments using lattice-based minimum error rate training (Kumar et al. [sent-279, score-0.448]
</p><p>92 82% after training IBM Model 1 for 3 iterations and training the HMM-based alignment model for 3 iterations. [sent-287, score-0.372]
</p><p>93 As our model only provides small improvements in alignment precision and recall for the union combiner, the magnitude of the BLEU improvement is not surprising. [sent-289, score-0.404]
</p><p>94 6  Conclusion  We have presented a graphical model that combines two classical HMM-based alignment models. [sent-290, score-0.419]
</p><p>95 Our bidirectional model, which requires no additional learning and no supervised data, can be applied using dual decomposition with only a constant factor  additional computation relative to independent directional inference. [sent-291, score-1.225]
</p><p>96 The resulting predictions improve the precision and recall of both alignment links and extraced phrase pairs in Chinese-English experiments. [sent-292, score-0.436]
</p><p>97 Because our technique is defined declaratively in terms of a graphical model, it can be extended in a straightforward manner, for instance with additional potentials on c or improvements to the component directional models. [sent-294, score-0.721]
</p><p>98 An alignment algorithm using belief propagation and a structure-based distortion model. [sent-325, score-0.418]
</p><p>99 Using word-dependent transition models in HMM-based word alignment for statistical machine. [sent-361, score-0.365]
</p><p>100 On dual decomposition and linear programming relaxations for natural language processing. [sent-403, score-0.426]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('directional', 0.513), ('aj', 0.375), ('alignment', 0.288), ('dual', 0.27), ('bidirectional', 0.23), ('decomposition', 0.156), ('alignments', 0.131), ('potentials', 0.129), ('cij', 0.126), ('aligners', 0.104), ('equation', 0.102), ('inference', 0.098), ('denero', 0.095), ('edge', 0.094), ('bi', 0.084), ('vertex', 0.082), ('graphical', 0.079), ('aa', 0.076), ('ci', 0.074), ('aligner', 0.067), ('coherence', 0.064), ('union', 0.064), ('fj', 0.064), ('potential', 0.063), ('ga', 0.063), ('ayan', 0.059), ('itg', 0.059), ('jb', 0.058), ('subgraph', 0.056), ('links', 0.054), ('po', 0.053), ('vi', 0.053), ('phrase', 0.053), ('model', 0.052), ('viterbi', 0.05), ('assignment', 0.049), ('aer', 0.048), ('rush', 0.048), ('aligns', 0.045), ('enforces', 0.044), ('combination', 0.044), ('intersection', 0.043), ('ja', 0.042), ('admit', 0.042), ('predictions', 0.041), ('gra', 0.04), ('transition', 0.04), ('bmiadoserlincteoaliugcnr', 0.039), ('brunning', 0.039), ('cromi', 0.039), ('eres', 0.039), ('sition', 0.039), ('belief', 0.038), ('propagation', 0.038), ('maximization', 0.037), ('models', 0.037), ('yj', 0.035), ('optimizes', 0.035), ('translation', 0.035), ('variables', 0.034), ('ganchev', 0.034), ('stopping', 0.034), ('shindo', 0.034), ('converge', 0.033), ('ibm', 0.033), ('haghighi', 0.033), ('convergence', 0.033), ('och', 0.033), ('optimization', 0.032), ('iterations', 0.032), ('align', 0.032), ('subgraphs', 0.032), ('posterior', 0.03), ('fixed', 0.03), ('chain', 0.03), ('resolves', 0.03), ('certificate', 0.03), ('decomposes', 0.03), ('deng', 0.03), ('burkett', 0.03), ('supervised', 0.029), ('markov', 0.029), ('heuristic', 0.028), ('distortion', 0.028), ('fbis', 0.028), ('joao', 0.028), ('relative', 0.027), ('vogel', 0.027), ('xb', 0.027), ('likewise', 0.026), ('embedding', 0.026), ('relax', 0.026), ('naseem', 0.026), ('optimality', 0.026), ('algorithm', 0.026), ('multinomial', 0.026), ('josef', 0.025), ('matrix', 0.025), ('mf', 0.025), ('acyclic', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="221-tfidf-1" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>2 0.24185802 <a title="221-tfidf-2" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>Author: Mohit Bansal ; Chris Quirk ; Robert Moore</p><p>Abstract: We propose a principled and efficient phraseto-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semiMarkov model, word-to-phrase and phraseto-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include “gappy phrases” (such as French ne ? pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime.</p><p>3 0.24108081 <a title="221-tfidf-3" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>4 0.23724531 <a title="221-tfidf-4" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>5 0.21785416 <a title="221-tfidf-5" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>Author: Alexander M. Rush and Michael Collins</p><p>Abstract: unkown-abstract</p><p>6 0.17414029 <a title="221-tfidf-6" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>7 0.15909626 <a title="221-tfidf-7" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>8 0.14601485 <a title="221-tfidf-8" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>9 0.13362224 <a title="221-tfidf-9" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>10 0.13055243 <a title="221-tfidf-10" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>11 0.12866496 <a title="221-tfidf-11" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>12 0.12586944 <a title="221-tfidf-12" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>13 0.11627126 <a title="221-tfidf-13" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>14 0.1149434 <a title="221-tfidf-14" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>15 0.11147678 <a title="221-tfidf-15" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>16 0.10698476 <a title="221-tfidf-16" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>17 0.097744524 <a title="221-tfidf-17" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>18 0.093851149 <a title="221-tfidf-18" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>19 0.089879833 <a title="221-tfidf-19" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>20 0.089629151 <a title="221-tfidf-20" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.214), (1, -0.132), (2, 0.055), (3, 0.062), (4, 0.051), (5, 0.005), (6, 0.018), (7, 0.094), (8, -0.064), (9, 0.11), (10, 0.202), (11, 0.21), (12, 0.042), (13, 0.158), (14, -0.223), (15, 0.093), (16, 0.104), (17, -0.021), (18, -0.146), (19, 0.001), (20, 0.002), (21, -0.026), (22, -0.039), (23, 0.057), (24, -0.027), (25, 0.025), (26, -0.033), (27, 0.046), (28, -0.049), (29, -0.049), (30, -0.027), (31, 0.054), (32, -0.075), (33, 0.06), (34, -0.021), (35, 0.02), (36, 0.035), (37, 0.016), (38, -0.064), (39, -0.004), (40, -0.01), (41, -0.002), (42, 0.023), (43, 0.008), (44, 0.024), (45, -0.084), (46, 0.011), (47, 0.031), (48, 0.085), (49, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95695049 <a title="221-lsi-1" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>2 0.83436179 <a title="221-lsi-2" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>Author: Mohit Bansal ; Chris Quirk ; Robert Moore</p><p>Abstract: We propose a principled and efficient phraseto-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semiMarkov model, word-to-phrase and phraseto-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include “gappy phrases” (such as French ne ? pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime.</p><p>3 0.82612723 <a title="221-lsi-3" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>Author: Kapil Thadani ; Kathleen McKeown</p><p>Abstract: The task of aligning corresponding phrases across two related sentences is an important component of approaches for natural language problems such as textual inference, paraphrase detection and text-to-text generation. In this work, we examine a state-of-the-art structured prediction model for the alignment task which uses a phrase-based representation and is forced to decode alignments using an approximate search approach. We propose instead a straightforward exact decoding technique based on integer linear programming that yields order-of-magnitude improvements in decoding speed. This ILP-based decoding strategy permits us to consider syntacticallyinformed constraints on alignments which significantly increase the precision of the model.</p><p>4 0.7705757 <a title="221-lsi-4" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>Author: Shujian Huang ; Stephan Vogel ; Jiajun Chen</p><p>Abstract: Word alignment has an exponentially large search space, which often makes exact inference infeasible. Recent studies have shown that inversion transduction grammars are reasonable constraints for word alignment, and that the constrained space could be efficiently searched using synchronous parsing algorithms. However, spurious ambiguity may occur in synchronous parsing and cause problems in both search efficiency and accuracy. In this paper, we conduct a detailed study of the causes of spurious ambiguity and how it effects parsing and discriminative learning. We also propose a variant of the grammar which eliminates those ambiguities. Our grammar shows advantages over previous grammars in both synthetic and real-world experiments.</p><p>5 0.75633055 <a title="221-lsi-5" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>6 0.73598564 <a title="221-lsi-6" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>7 0.70339686 <a title="221-lsi-7" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>8 0.68279159 <a title="221-lsi-8" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>9 0.66981232 <a title="221-lsi-9" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>10 0.66108364 <a title="221-lsi-10" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>11 0.6569429 <a title="221-lsi-11" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>12 0.65093935 <a title="221-lsi-12" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>13 0.61712021 <a title="221-lsi-13" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>14 0.54096764 <a title="221-lsi-14" href="./acl-2011-full-for-print.html">342 acl-2011-full-for-print</a></p>
<p>15 0.5265848 <a title="221-lsi-15" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>16 0.52473921 <a title="221-lsi-16" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>17 0.48058996 <a title="221-lsi-17" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>18 0.47901541 <a title="221-lsi-18" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>19 0.43649179 <a title="221-lsi-19" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>20 0.42506951 <a title="221-lsi-20" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.019), (17, 0.065), (26, 0.017), (37, 0.113), (39, 0.038), (41, 0.072), (53, 0.028), (55, 0.035), (59, 0.034), (64, 0.048), (72, 0.047), (90, 0.16), (91, 0.039), (96, 0.173), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90942955 <a title="221-lda-1" href="./acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution.html">212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</a></p>
<p>Author: Hugo Jair Escalante ; Thamar Solorio ; Manuel Montes-y-Gomez</p><p>Abstract: This paper proposes the use of local histograms (LH) over character n-grams for authorship attribution (AA). LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word histograms. In this work we explore the suitability of LHs over n-grams at the character-level for AA. We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors. We report experimental results in AA data sets that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.</p><p>2 0.90506035 <a title="221-lda-2" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>Author: Oleksandr Kolomiyets ; Steven Bethard ; Marie-Francine Moens</p><p>Abstract: We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia. 1</p><p>same-paper 3 0.88684571 <a title="221-lda-3" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>4 0.87989867 <a title="221-lda-4" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>Author: Aditya Joshi ; Balamurali AR ; Pushpak Bhattacharyya ; Rajat Mohanty</p><p>Abstract: Social networking and micro-blogging sites are stores of opinion-bearing content created by human users. We describe C-Feel-It, a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus.</p><p>5 0.86238801 <a title="221-lda-5" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>Author: Marius Pasca</p><p>Abstract: The role of search queries, as available within query sessions or in isolation from one another, in examined in the context of ranking the class labels (e.g., brazilian cities, business centers, hilly sites) extracted from Web documents for various instances (e.g., rio de janeiro). The co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance. Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts.</p><p>6 0.81892288 <a title="221-lda-6" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>7 0.80825055 <a title="221-lda-7" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>8 0.8077296 <a title="221-lda-8" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>9 0.80683237 <a title="221-lda-9" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>10 0.80574393 <a title="221-lda-10" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>11 0.80522263 <a title="221-lda-11" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>12 0.80361825 <a title="221-lda-12" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>13 0.80354518 <a title="221-lda-13" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>14 0.80278206 <a title="221-lda-14" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>15 0.80258989 <a title="221-lda-15" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>16 0.80248833 <a title="221-lda-16" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>17 0.80061692 <a title="221-lda-17" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>18 0.80017793 <a title="221-lda-18" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>19 0.80017471 <a title="221-lda-19" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>20 0.80014461 <a title="221-lda-20" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
