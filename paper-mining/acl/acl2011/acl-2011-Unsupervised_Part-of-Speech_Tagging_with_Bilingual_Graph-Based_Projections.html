<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-323" href="#">acl2011-323</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</h1>
<br/><p>Source: <a title="acl-2011-323-pdf" href="http://aclweb.org/anthology//P/P11/P11-1061.pdf">pdf</a></p><p>Author: Dipanjan Das ; Slav Petrov</p><p>Abstract: We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</p><p>Reference: <a title="acl-2011-323-reference" href="../acl2011_reference/acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. [sent-3, score-0.281]
</p><p>2 We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al. [sent-5, score-0.445]
</p><p>3 Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76. [sent-18, score-0.187]
</p><p>4 To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages. [sent-21, score-0.444]
</p><p>5 1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language. [sent-22, score-0.571]
</p><p>6 (2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available. [sent-28, score-0.193]
</p><p>7 To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation utoa project syntactic ninf uoserm gartaiopnh lfrabomel English to the foreign language (§4). [sent-31, score-1.264]
</p><p>8 hTeor mthaanke u tshineg projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. [sent-38, score-0.259]
</p><p>9 We evaluate our approach on eight European languages (§6), and show that both our contributions provide §c6on),si asntednt s haonwd statistically significant iomn-s provements. [sent-44, score-0.149]
</p><p>10 2  Approach Overview  The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages. [sent-50, score-0.631]
</p><p>11 Central to our approach (see Algorithm 1) is a bilingual similarity graph built  from a sentence-aligned parallel corpus. [sent-51, score-0.382]
</p><p>12 As discussed in more detail in §3, we use two types of vceusrtsiecdes inn our graph: on t§h3e, foreign language ssid oef vertices correspond to trigram types, while the vertices on the English side are individual word types. [sent-52, score-1.388]
</p><p>13 The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function, designed to indicate how syntactically 2See Christodoulopoulos et al. [sent-54, score-0.601]
</p><p>14 601 Algorithm 1 Bilingual POS Induction  Require: Parallel English and foreign language  data De and Df, unlabeled foreign training data dΓaft;a English tagger. [sent-56, score-0.888]
</p><p>15 3 Sridn caeli we ehantve s no ltiacbse(l e§d3 foreign data, our goal is to project syntactic information from the English side to the foreign side. [sent-64, score-0.918]
</p><p>16 To initialize the graph we tag the English side of the parallel text using a supervised model. [sent-65, score-0.386]
</p><p>17 By aggregating the POS labels of the English tokens to types, we can generate label distributions for the English vertices. [sent-66, score-0.192]
</p><p>18 Label propagation can then be used to transfer the labels to the peripheral foreign vertices (i. [sent-67, score-1.114]
</p><p>19 the ones adjacent to the English vertices) first, and then among all of the foreign vertices (§4). [sent-69, score-0.841]
</p><p>20 The POS distributions over the foreign trigram types are uPsOedS as fteriabtuutrieosn to vleearrn th a  better unsupervised POS tagger (§5). [sent-70, score-0.752]
</p><p>21 3  Graph Construction  In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples, and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al. [sent-73, score-0.708]
</p><p>22 Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand, using individual words as the vertices throws away the context 3The word alignment methods do not use POS information. [sent-75, score-0.47]
</p><p>23 necessary for disambiguation; on the other hand, it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences. [sent-76, score-0.465]
</p><p>24 (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning. [sent-78, score-0.293]
</p><p>25 (2010) defined a graph over the cliques in an underlying structured prediction model. [sent-80, score-0.171]
</p><p>26 They  considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types, and edge weights based on distributional similarity, to improve a supervised conditional random field tagger. [sent-81, score-0.427]
</p><p>27 Because the information flow in our graph is asymmetric (from English to the foreign language), we use different types of vertices for each language. [sent-85, score-1.041]
</p><p>28 The foreign language vertices (denoted by Vf) correspond to foreign trigram types, exactly as in Subramanya et al. [sent-86, score-1.376]
</p><p>29 On the English side, however, the vertices (denoted by Ve) correspond to word types. [sent-88, score-0.397]
</p><p>30 Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams. [sent-89, score-0.397]
</p><p>31 Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices. [sent-90, score-0.841]
</p><p>32 4 The graph vertices are extracted from the different sides of a parallel corpus (De, Df) and an addenittio snidale sun olfa abe pleadra monolingual foreign corpus Γf, which will be used later for training. [sent-91, score-1.133]
</p><p>33 We use two different similarity functions to define the edge weights  among the foreign vertices and between vertices from different languages. [sent-92, score-1.341]
</p><p>34 2 Monolingual Similarity Function Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. [sent-94, score-0.686]
</p><p>35 We define a symmetric similarity function K(ui, uj) over two for4This is because we are primarily interested in learning foreign language taggers, rather than improving supervised English taggers. [sent-97, score-0.604]
</p><p>36 eign language vertices ui, uj ∈ Vf based on the co-occurrence statistics of the n∈ine V Vfeature concepts given in Table 1. [sent-101, score-0.469]
</p><p>37 For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5 , we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two. [sent-103, score-0.223]
</p><p>38 5 The  similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common. [sent-104, score-0.229]
</p><p>39 Given this similarity function, we define a nearest neighbor graph, where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices. [sent-107, score-0.603]
</p><p>40 3 Bilingual Similarity Function To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments. [sent-110, score-0.547]
</p><p>41 Since our graph is built from a parallel corpus, we can use standard word alignment techniques to align the English sentences De ment  techniqu  5Note that many combinations are impossible giving a PMI value of 0; e. [sent-111, score-0.244]
</p><p>42 6 Label propagation ienig tnhe l graph ewi tlrla provide coverage and high recall, and we therefore extract only intersected high-confidence (> 0. [sent-115, score-0.381]
</p><p>43 gBha-cseodn on ntchees (>e high-confidence alignments we can extract tuples of the form [u ↔ v], where u is a foreign trigram type, w fohromse umi ↔ddle v ,w worhde aligns to an English word type v. [sent-117, score-0.535]
</p><p>44 Our bilingual similarity function then sets the edge weights in proportion to these tuple counts. [sent-118, score-0.208]
</p><p>45 4 Graph Initialization So far the graph has been completely unlabeled. [sent-120, score-0.171]
</p><p>46 To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext. [sent-121, score-0.682]
</p><p>47 7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types. [sent-122, score-0.161]
</p><p>48 These tag distributions are used to initialize the label distributions over the English vertices in the graph. [sent-123, score-0.686]
</p><p>49 Note that since all English vertices were extracted from the parallel text, we will have an initial label distribution for all vertices in Ve. [sent-124, score-0.953]
</p><p>50 5  Graph Example  A very small excerpt from an Italian-English graph is shown in Figure 1. [sent-126, score-0.209]
</p><p>51 In this particular case, all English vertices are labeled as nouns by the supervised tagger. [sent-128, score-0.508]
</p><p>52 It is worth noting that the middle words of the Italian trigrams are nouns too, which exhibits the fact that the similarity metric connects types having the same syntactic category. [sent-130, score-0.19]
</p><p>53 In the label propagation stage, we propagate the automatic English tags to the aligned Italian trigram types, followed by further propagation solely among the Italian vertices. [sent-131, score-0.71]
</p><p>54 603  NOUN  [ enactment ]  NOUN  [ imprisonment ]  Figure 1: An excerpt from the graph for Italian. [sent-138, score-0.209]
</p><p>55 Three of the Italian vertices are connected to an automatically labeled English vertex. [sent-139, score-0.484]
</p><p>56 Label propagation is used to propagate these tags inwards and results in tag distributions for the middle word of each Italian trigram. [sent-140, score-0.491]
</p><p>57 4  POS Projection  Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language. [sent-141, score-1.013]
</p><p>58 We use label propagation in two stages to generate soft la-  bels on all the vertices in the graph. [sent-142, score-0.693]
</p><p>59 In the first stage, we run a single step of label propagation, which transfers the label distributions from the English vertices to the connected foreign language vertices (say, Vfl) at the periphery of the graph. [sent-143, score-1.517]
</p><p>60 Note that because we extracted only high-confidence alignments, many foreign vertices will not be connected to any English vertices. [sent-144, score-0.874]
</p><p>61 , |Vf |) are the label distributions over the( foreign language eve trhteic laebs ealn ddi iabnudν are hyperparameters that we discuss in §6. [sent-150, score-0.658]
</p><p>62 tic Wese that have different label distributions: kqi − qj k2 = Py(qi (y) −qj e(nyt)) l2ab, aenld d additionally regularize the lPabel d(isyt)ri−buqtions towards the uniform distribution UP over all possible labels Y. [sent-154, score-0.189]
</p><p>63 The first term in the objective function is the graph smoothness regularizer which encourages the distributions of similar vertices (large wij) to be similar. [sent-156, score-0.707]
</p><p>64 If an unlabeled vertex does not have a path to any labeled vertex, this term ensures that the converged marginal for this vertex will be uniform over all tags, allowing the middle word of such an unlabeled vertex to take on any of the possible tags. [sent-159, score-0.279]
</p><p>65 foreign vocabulary and will be used to provide features for the unsupervised foreign language POS tagger. [sent-168, score-0.96]
</p><p>66 This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx. [sent-184, score-0.205]
</p><p>67 The function λ : F → C maps from the language specific fine-grained tagset F to the coarser universal tagset C and is described in detail in §6. [sent-185, score-0.291]
</p><p>68 This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold τ on the posterior distribution of tags for a given word type (Eq. [sent-187, score-0.37]
</p><p>69 605 Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side. [sent-194, score-0.502]
</p><p>70 The availability of these resources guided our selection of foreign languages. [sent-199, score-0.444]
</p><p>71 Taking the intersection of languages in these resources, and selecting languages with large amounts of parallel data, yields  the following set of eight Indo-European languages: Danish, Dutch, German, Greek, Italian, Portuguese, Spanish and Swedish. [sent-203, score-0.305]
</p><p>72 Of course, we are primarily interested in applying our techniques to languages for which no labeled resources are available. [sent-204, score-0.137]
</p><p>73 2 Part-of-Speech Tagset and HMM States We use the universal POS tagset of Petrov et al. [sent-209, score-0.181]
</p><p>74 (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words). [sent-215, score-0.444]
</p><p>75 (201 1) provide a mapping λ from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags. [sent-218, score-0.669]
</p><p>76 The taggers were trained on datasets labeled with the universal tags. [sent-220, score-0.232]
</p><p>77 The number of latent HMM states for each language in our experiments was set to the number of fine tags in the language’s treebank. [sent-221, score-0.14]
</p><p>78 Therefore, the number of fine tags varied across languages for our experiments; however, one could as well have fixed the set of HMM states to be a constant across languages, and created one mapping to the universal POS tagset. [sent-223, score-0.329]
</p><p>79 Projection: Our third baseline incorporates bilingual inn:formation by projecting POS tags directly across alignments in the parallel data. [sent-233, score-0.219]
</p><p>80 We tried two versions of our graph-based approach: •  •  No LP: Our first version takes advantage of our bilingual graph, e brustio enxt traakcetss t ahdev acnontasgtreai notf feature after the first stage of label propagation (Eq. [sent-237, score-0.398]
</p><p>81 Because many foreign word types are not aligned to an English word (see Table 3), and we do not run label propagation on the foreign side, we expect the projected information to have less coverage. [sent-239, score-1.258]
</p><p>82 Furthermore we expect the label distributions on the foreign to be fairly noisy, because the graph constraints have not been taken into account yet. [sent-240, score-0.775]
</p><p>83 With LP: Our full model uses both stages oWf ltahbe Ll propagation (Eq. [sent-241, score-0.21]
</p><p>84 As a result, we are  able to extract the constraint feature for all foreign word types and furthermore expect the projected tag distributions to be smoother and more stable. [sent-243, score-0.697]
</p><p>85 Our oracles took advantage of the labeled treebanks: •  •  TB Dictionary: We extracted tagging dictionTaBrie Ds fcrtoimon tahrey :tre Webean exkstr aanctde adn tda gugsiendg gth deimcti as constraint features in the feature-based HMM. [sent-244, score-0.208]
</p><p>86 Supervised: We trained the supervised model Sofu Bperranvtiss (2000) on itnheed original etrreveibseadnk ms oadnedl mapped the language-specific tags to the universal tags for evaluation. [sent-246, score-0.315]
</p><p>87 For seven out of eight languages a threshold of 0. [sent-260, score-0.149]
</p><p>88 For graph propagation, the hyperparameter ν was set to 2 10−6 aangadt was n thoet t huynpeedr. [sent-263, score-0.171]
</p><p>89 p Tahraem graph was sco snetst trouc 2te ×d using 2 million trigrams; we chose these by truncating the parallel datasets up to the number of sentence pairs that contained 2 million trigrams. [sent-264, score-0.244]
</p><p>90 The “No LP” model does not outperform direct projection for German and Greek, but performs better for six out of eight languages. [sent-272, score-0.143]
</p><p>91 For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79. [sent-277, score-0.254]
</p><p>92 As indicated by bolding, for seven out of eight languages the improvements of the “With LP” setting are statistically significant with respect to the other models, including the “No LP” setting. [sent-282, score-0.149]
</p><p>93 Although the tag distributions of the foreign words (Eq. [sent-290, score-0.573]
</p><p>94 6) are noisy, the results confirm that label propagation within the foreign language part of the graph adds significant quality for every language. [sent-291, score-0.911]
</p><p>95 7  Conclusion  We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages. [sent-315, score-0.296]
</p><p>96 Because we are interested in applying our techniques to languages for which no labeled resources are available, we paid particular attention to minimize the number of free parame-  ters and used the same hyperparameters for all language pairs. [sent-316, score-0.191]
</p><p>97 Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language. [sent-317, score-0.155]
</p><p>98 Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models. [sent-318, score-0.274]
</p><p>99 We would also like to 608 thank Amarnag Subramanya for helping us with the implementation of label propagation and Shankar Kumar for access to the parallel data. [sent-320, score-0.369]
</p><p>100 Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. [sent-453, score-0.188]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('foreign', 0.444), ('vertices', 0.397), ('propagation', 0.21), ('adp', 0.205), ('lp', 0.183), ('graph', 0.171), ('pos', 0.157), ('ui', 0.155), ('italian', 0.13), ('det', 0.13), ('noun', 0.123), ('vf', 0.109), ('universal', 0.106), ('vfl', 0.103), ('subramanya', 0.1), ('trigram', 0.091), ('zi', 0.087), ('label', 0.086), ('languages', 0.083), ('hmm', 0.079), ('projection', 0.077), ('qi', 0.077), ('tags', 0.076), ('tx', 0.075), ('tagset', 0.075), ('distributions', 0.074), ('tagging', 0.073), ('parallel', 0.073), ('unsupervised', 0.072), ('taggers', 0.072), ('uj', 0.072), ('bilingual', 0.07), ('naseem', 0.068), ('similarity', 0.068), ('english', 0.067), ('eight', 0.066), ('vanilla', 0.063), ('vertex', 0.062), ('suo', 0.062), ('greek', 0.059), ('induction', 0.058), ('supervised', 0.057), ('tag', 0.055), ('hyperparameters', 0.054), ('trigrams', 0.054), ('labeled', 0.054), ('em', 0.054), ('num', 0.053), ('petrov', 0.051), ('christodoulopoulos', 0.05), ('constraint', 0.05), ('snyder', 0.05), ('monolingual', 0.048), ('pron', 0.047), ('conj', 0.047), ('xi', 0.046), ('projected', 0.045), ('treebank', 0.043), ('danish', 0.043), ('tagger', 0.042), ('bergkirkpatrick', 0.041), ('fidanzato', 0.041), ('kqi', 0.041), ('tahira', 0.041), ('instantiations', 0.041), ('portuguese', 0.04), ('middle', 0.039), ('multilingual', 0.039), ('excerpt', 0.038), ('propagate', 0.037), ('ganchev', 0.036), ('verb', 0.035), ('edge', 0.035), ('function', 0.035), ('adj', 0.034), ('states', 0.034), ('connected', 0.033), ('universals', 0.033), ('vy', 0.033), ('altun', 0.033), ('controversy', 0.033), ('pmi', 0.032), ('german', 0.032), ('labels', 0.032), ('stage', 0.032), ('alshawi', 0.031), ('ods', 0.031), ('peripheral', 0.031), ('marginalizing', 0.031), ('oracles', 0.031), ('ri', 0.031), ('fine', 0.03), ('un', 0.03), ('side', 0.03), ('yarowsky', 0.03), ('qj', 0.03), ('lenient', 0.03), ('regularizer', 0.03), ('types', 0.029), ('xn', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999875 <a title="323-tfidf-1" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>Author: Dipanjan Das ; Slav Petrov</p><p>Abstract: We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</p><p>2 0.19351518 <a title="323-tfidf-2" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>Author: Ahmed Hassan ; Amjad AbuJbara ; Rahul Jha ; Dragomir Radev</p><p>Abstract: We present a method for identifying the positive or negative semantic orientation of foreign words. Identifying the semantic orientation of words has numerous applications in the areas of text classification, analysis of product review, analysis of responses to surveys, and mining online discussions. Identifying the semantic orientation of English words has been extensively studied in literature. Most of this work assumes the existence of resources (e.g. Wordnet, seeds, etc) that do not exist in foreign languages. In this work, we describe a method based on constructing a multilingual network connecting English and foreign words. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results.</p><p>3 0.16128694 <a title="323-tfidf-3" href="./acl-2011-Semi-Supervised_Frame-Semantic_Parsing_for_Unknown_Predicates.html">274 acl-2011-Semi-Supervised Frame-Semantic Parsing for Unknown Predicates</a></p>
<p>Author: Dipanjan Das ; Noah A. Smith</p><p>Abstract: We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement in full frame-semantic parsing F1 score on a blind test set, over a state-of-the-art supervised baseline.</p><p>4 0.142078 <a title="323-tfidf-4" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>5 0.14112081 <a title="323-tfidf-5" href="./acl-2011-Optimal_Head-Driven_Parsing_Complexity_for_Linear_Context-Free_Rewriting_Systems.html">234 acl-2011-Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems</a></p>
<p>Author: Pierluigi Crescenzi ; Daniel Gildea ; Andrea Marino ; Gianluca Rossi ; Giorgio Satta</p><p>Abstract: We study the problem offinding the best headdriven parsing strategy for Linear ContextFree Rewriting System productions. A headdriven strategy must begin with a specified righthand-side nonterminal (the head) and add the remaining nonterminals one at a time in any order. We show that it is NP-hard to find the best head-driven strategy in terms of either the time or space complexity of parsing.</p><p>6 0.12572153 <a title="323-tfidf-6" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>7 0.12325773 <a title="323-tfidf-7" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>8 0.12255412 <a title="323-tfidf-8" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>9 0.11324304 <a title="323-tfidf-9" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>10 0.1121219 <a title="323-tfidf-10" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>11 0.10144502 <a title="323-tfidf-11" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>12 0.095345147 <a title="323-tfidf-12" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>13 0.089813806 <a title="323-tfidf-13" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>14 0.089441255 <a title="323-tfidf-14" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>15 0.08678177 <a title="323-tfidf-15" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>16 0.085194044 <a title="323-tfidf-16" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>17 0.085093558 <a title="323-tfidf-17" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>18 0.08469855 <a title="323-tfidf-18" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>19 0.084248535 <a title="323-tfidf-19" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>20 0.082856447 <a title="323-tfidf-20" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.229), (1, -0.022), (2, -0.011), (3, -0.044), (4, -0.017), (5, -0.055), (6, 0.112), (7, 0.065), (8, -0.01), (9, 0.051), (10, 0.059), (11, 0.04), (12, 0.05), (13, 0.09), (14, 0.003), (15, -0.041), (16, 0.007), (17, 0.01), (18, 0.042), (19, -0.066), (20, -0.009), (21, 0.004), (22, 0.106), (23, 0.078), (24, 0.015), (25, 0.035), (26, -0.16), (27, -0.008), (28, 0.068), (29, -0.031), (30, 0.185), (31, 0.023), (32, 0.007), (33, -0.063), (34, 0.096), (35, -0.047), (36, 0.113), (37, 0.077), (38, -0.157), (39, -0.06), (40, -0.047), (41, -0.018), (42, 0.014), (43, -0.03), (44, 0.162), (45, -0.165), (46, -0.095), (47, 0.085), (48, -0.01), (49, -0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95072597 <a title="323-lsi-1" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>Author: Dipanjan Das ; Slav Petrov</p><p>Abstract: We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</p><p>2 0.63658959 <a title="323-lsi-2" href="./acl-2011-Semi-Supervised_Frame-Semantic_Parsing_for_Unknown_Predicates.html">274 acl-2011-Semi-Supervised Frame-Semantic Parsing for Unknown Predicates</a></p>
<p>Author: Dipanjan Das ; Noah A. Smith</p><p>Abstract: We describe a new approach to disambiguating semantic frames evoked by lexical predicates previously unseen in a lexicon or annotated data. Our approach makes use of large amounts of unlabeled data in a graph-based semi-supervised learning framework. We construct a large graph where vertices correspond to potential predicates and use label propagation to learn possible semantic frames for new ones. The label-propagated graph is used within a frame-semantic parser and, for unknown predicates, results in over 15% absolute improvement in frame identification accuracy and over 13% absolute improvement in full frame-semantic parsing F1 score on a blind test set, over a state-of-the-art supervised baseline.</p><p>3 0.60878277 <a title="323-lsi-3" href="./acl-2011-Tier-based_Strictly_Local_Constraints_for_Phonology.html">303 acl-2011-Tier-based Strictly Local Constraints for Phonology</a></p>
<p>Author: Jeffrey Heinz ; Chetan Rawal ; Herbert G. Tanner</p><p>Abstract: Beginning with Goldsmith (1976), the phonological tier has a long history in phonological theory to describe non-local phenomena. This paper defines a class of formal languages, the Tier-based Strictly Local languages, which begin to describe such phenomena. Then this class is located within the Subregular Hierarchy (McNaughton and Papert, 1971). It is found that these languages contain the Strictly Local languages, are star-free, are incomparable with other known sub-star-free classes, and have other interesting properties.</p><p>4 0.58188826 <a title="323-lsi-4" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>Author: Ahmed Hassan ; Amjad AbuJbara ; Rahul Jha ; Dragomir Radev</p><p>Abstract: We present a method for identifying the positive or negative semantic orientation of foreign words. Identifying the semantic orientation of words has numerous applications in the areas of text classification, analysis of product review, analysis of responses to surveys, and mining online discussions. Identifying the semantic orientation of English words has been extensively studied in literature. Most of this work assumes the existence of resources (e.g. Wordnet, seeds, etc) that do not exist in foreign languages. In this work, we describe a method based on constructing a multilingual network connecting English and foreign words. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results.</p><p>5 0.55448419 <a title="323-lsi-5" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>Author: Delip Rao ; David Yarowsky</p><p>Abstract: This paper presents an original approach to semi-supervised learning of personal name ethnicity from typed graphs of morphophonemic features and first/last-name co-occurrence statistics. We frame this as a general solution to an inference problem over typed graphs where the edges represent labeled relations between features that are parameterized by the edge types. We propose a framework for parameter estimation on different constructions of typed graphs for this problem using a gradient-free optimization method based on grid search. Results on both in-domain and out-of-domain data show significant gains over 30% accuracy improvement using the techniques presented in the paper.</p><p>6 0.54922956 <a title="323-lsi-6" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>7 0.53851932 <a title="323-lsi-7" href="./acl-2011-Optimal_Head-Driven_Parsing_Complexity_for_Linear_Context-Free_Rewriting_Systems.html">234 acl-2011-Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems</a></p>
<p>8 0.53812367 <a title="323-lsi-8" href="./acl-2011-full-for-print.html">342 acl-2011-full-for-print</a></p>
<p>9 0.52729571 <a title="323-lsi-9" href="./acl-2011-Semi-supervised_condensed_nearest_neighbor_for_part-of-speech_tagging.html">278 acl-2011-Semi-supervised condensed nearest neighbor for part-of-speech tagging</a></p>
<p>10 0.52269417 <a title="323-lsi-10" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>11 0.51661718 <a title="323-lsi-11" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>12 0.49627426 <a title="323-lsi-12" href="./acl-2011-That%27s_What_She_Said%3A_Double_Entendre_Identification.html">297 acl-2011-That's What She Said: Double Entendre Identification</a></p>
<p>13 0.49298728 <a title="323-lsi-13" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>14 0.48383436 <a title="323-lsi-14" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>15 0.48206112 <a title="323-lsi-15" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>16 0.47574723 <a title="323-lsi-16" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>17 0.47040072 <a title="323-lsi-17" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>18 0.46041787 <a title="323-lsi-18" href="./acl-2011-Partial_Parsing_from_Bitext_Projections.html">243 acl-2011-Partial Parsing from Bitext Projections</a></p>
<p>19 0.45719907 <a title="323-lsi-19" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>20 0.45163083 <a title="323-lsi-20" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.014), (17, 0.036), (26, 0.016), (37, 0.097), (39, 0.077), (41, 0.081), (53, 0.287), (55, 0.033), (57, 0.011), (59, 0.041), (72, 0.028), (88, 0.012), (91, 0.031), (96, 0.145), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8503722 <a title="323-lda-1" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>Author: Lei Zhang ; Bing Liu</p><p>Abstract: Identifying domain-dependent opinion words is a key problem in opinion mining and has been studied by several researchers. However, existing work has been focused on adjectives and to some extent verbs. Limited work has been done on nouns and noun phrases. In our work, we used the feature-based opinion mining model, and we found that in some domains nouns and noun phrases that indicate product features may also imply opinions. In many such cases, these nouns are not subjective but objective. Their involved sentences are also objective sentences and imply positive or negative opinions. Identifying such nouns and noun phrases and their polarities is very challenging but critical for effective opinion mining in these domains. To the best of our knowledge, this problem has not been studied in the literature. This paper proposes a method to deal with the problem. Experimental results based on real-life datasets show promising results. 1</p><p>2 0.82242525 <a title="323-lda-2" href="./acl-2011-Extracting_Paraphrases_from_Definition_Sentences_on_the_Web.html">132 acl-2011-Extracting Paraphrases from Definition Sentences on the Web</a></p>
<p>Author: Chikara Hashimoto ; Kentaro Torisawa ; Stijn De Saeger ; Jun'ichi Kazama ; Sadao Kurohashi</p><p>Abstract: ¶ kuro@i . We propose an automatic method of extracting paraphrases from definition sentences, which are also automatically acquired from the Web. We observe that a huge number of concepts are defined in Web documents, and that the sentences that define the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. We show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 Web docu3m0e0n,t0s0 w0i ptha a precision oramte 6 6o ×f a 1b0out 94%. 108</p><p>same-paper 3 0.82066214 <a title="323-lda-3" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>Author: Dipanjan Das ; Slav Petrov</p><p>Abstract: We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</p><p>4 0.79009032 <a title="323-lda-4" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>5 0.71058923 <a title="323-lda-5" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>Author: Qin Gao ; Stephan Vogel</p><p>Abstract: We present an approach of expanding parallel corpora for machine translation. By utilizing Semantic role labeling (SRL) on one side of the language pair, we extract SRL substitution rules from existing parallel corpus. The rules are then used for generating new sentence pairs. An SVM classifier is built to filter the generated sentence pairs. The filtered corpus is used for training phrase-based translation models, which can be used directly in translation tasks or combined with baseline models. Experimental results on ChineseEnglish machine translation tasks show an average improvement of 0.45 BLEU and 1.22 TER points across 5 different NIST test sets.</p><p>6 0.69589043 <a title="323-lda-6" href="./acl-2011-Monolingual_Alignment_by_Edit_Rate_Computation_on_Sentential_Paraphrase_Pairs.html">225 acl-2011-Monolingual Alignment by Edit Rate Computation on Sentential Paraphrase Pairs</a></p>
<p>7 0.65336227 <a title="323-lda-7" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>8 0.6488018 <a title="323-lda-8" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>9 0.64548284 <a title="323-lda-9" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>10 0.64452261 <a title="323-lda-10" href="./acl-2011-Extracting_Opinion_Expressions_and_Their_Polarities_-_Exploration_of_Pipelines_and_Joint_Models.html">131 acl-2011-Extracting Opinion Expressions and Their Polarities - Exploration of Pipelines and Joint Models</a></p>
<p>11 0.62970614 <a title="323-lda-11" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>12 0.6290825 <a title="323-lda-12" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<p>13 0.62734801 <a title="323-lda-13" href="./acl-2011-Finding_Deceptive_Opinion_Spam_by_Any_Stretch_of_the_Imagination.html">136 acl-2011-Finding Deceptive Opinion Spam by Any Stretch of the Imagination</a></p>
<p>14 0.62577182 <a title="323-lda-14" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>15 0.62011683 <a title="323-lda-15" href="./acl-2011-Semi-Supervised_Frame-Semantic_Parsing_for_Unknown_Predicates.html">274 acl-2011-Semi-Supervised Frame-Semantic Parsing for Unknown Predicates</a></p>
<p>16 0.61318254 <a title="323-lda-16" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>17 0.60560584 <a title="323-lda-17" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>18 0.60477054 <a title="323-lda-18" href="./acl-2011-Optimal_Head-Driven_Parsing_Complexity_for_Linear_Context-Free_Rewriting_Systems.html">234 acl-2011-Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems</a></p>
<p>19 0.60091496 <a title="323-lda-19" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>20 0.59994978 <a title="323-lda-20" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
