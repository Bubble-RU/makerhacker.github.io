<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-15" href="#">acl2011-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</h1>
<br/><p>Source: <a title="acl-2011-15-pdf" href="http://aclweb.org/anthology//P/P11/P11-1087.pdf">pdf</a></p><p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>Reference: <a title="acl-2011-15-reference" href="../acl2011_reference/acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. [sent-5, score-0.228]
</p><p>2 We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. [sent-6, score-0.253]
</p><p>3 Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. [sent-7, score-0.248]
</p><p>4 1 Introduction  Unsupervised part-of-speech (PoS) induction has long been a central challenge in computational linguistics, with applications in human language learning and for developing portable language processing systems. [sent-9, score-0.144]
</p><p>5 Despite considerable research effort, progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al. [sent-10, score-0.185]
</p><p>6 One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar, however this limits portability and also comprimises any cognitive conclusions. [sent-13, score-0.181]
</p><p>7 In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing state-of-the-art across all our corpora in 10 different languages. [sent-14, score-0.185]
</p><p>8 Moreover, the performance of our unsupervised model approaches 865 Trevor Cohn Department of Computer Science University of Sheffield T . [sent-15, score-0.129]
</p><p>9 In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. [sent-20, score-0.179]
</p><p>10 HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al. [sent-21, score-0.185]
</p><p>11 Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. [sent-27, score-0.408]
</p><p>12 This allows the modelling of sub-word structure, thereby capturing  tag-specific morphological variation. [sent-28, score-0.162]
</p><p>13 , 1992), we develop a new typelevel inference procedure in the form of an MCMC sampler with an approximate method for incorporating the complex dependencies that arise between jointly sampled events. [sent-31, score-0.327]
</p><p>14 Our experimental evaluation demonstrates that our model, particularly when restricted to a single tag per type, produces ProceedinPgosrt olafn thde, 4 O9rtehg Aonn,n Juuanle M 1e9e-2tin4g, 2 o0f1 t1h. [sent-32, score-0.135]
</p><p>15 This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcl s (Och, 1999)1 has become a standard part of statistical machine translation systems. [sent-40, score-0.432]
</p><p>16 (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. [sent-43, score-0.245]
</p><p>17 Our work draws from these models, in that we develop a HMM with a one  class per tag restriction and include a character level language model. [sent-44, score-0.344]
</p><p>18 In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. [sent-45, score-0.422]
</p><p>19 A number ofresearchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). [sent-46, score-0.234]
</p><p>20 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. [sent-49, score-0.221]
</p><p>21 com/mkcl  s  Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distribu-  tions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). [sent-52, score-0.39]
</p><p>22 These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. [sent-53, score-0.196]
</p><p>23 However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. [sent-59, score-0.321]
</p><p>24 This work differs from previous Bayesian models in that we explicitly model a complex backoff path using a hierachical prior, such that our model jointly infers  distributions over tag trigrams, bigrams and unigrams and whole words and their character level representation. [sent-62, score-0.546]
</p><p>25 This smoothing is critical to ensure adequate generalisation from small data samples. [sent-63, score-0.146]
</p><p>26 , 2010) has shown that models employing Pitman-Yor priors can significantly outperform the more frequently used Dirichlet priors, especially where complex hierarchical relationships exist between latent variables. [sent-66, score-0.165]
</p><p>27 In this work we apply these advances to unsupervised PoS tagging, developing a HMM smoothed using a Pitman-Yor process prior. [sent-67, score-0.131]
</p><p>28 A key decision in formulating such a model is the smoothing of the tag trigram and emission distributions, which would otherwise be too difficult to estimate from small datasets. [sent-69, score-0.49]
</p><p>29 Here we build upon previous work by developing a PoS induction model smoothed with a sophisticated non-parametric prior. [sent-72, score-0.187]
</p><p>30 Our model uses a hierarchical Pitman-Yor process prior for both the transition and emission distributions, encoding a backoff path from complex distributions to successsively simpler ones. [sent-73, score-0.475]
</p><p>31 , over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise. [sent-76, score-0.241]
</p><p>32 The PYP has been shown to generate  distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al. [sent-77, score-0.229]
</p><p>33 , 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Goodman, 1996). [sent-78, score-0.269]
</p><p>34 At its centre is a standard trigram HMM, which generates a sequence of tags and words,  tl| tl−1, tl−2, T wl  | tl, E  ∼ ∼  Tt,t Etl . [sent-80, score-0.315]
</p><p>35 867  Figure 1: Plate diagram representation of the trigram HMM. [sent-81, score-0.157]
</p><p>36 This allows the modelling of trigram tag sequences, while smoothing these estimates with their corresponding bigram and unigram distributions. [sent-85, score-0.628]
</p><p>37 ∼  We consider two different settings for the base distribution Cj : 1) a simple uniform distribution over the vocabulary (denoted HMM for the experiments in section 4); and 2) a character-level language model (denoted HMM+LM). [sent-89, score-0.281]
</p><p>38 This model was inspired by Clark (2003)  Figure 2: The conditioning structure of the hierarchical PYP with an embedded character language models. [sent-93, score-0.281]
</p><p>39 who applied a character level distribution to the single class HMM (Brown et al. [sent-94, score-0.188]
</p><p>40 We expect that the HMM+LM model will outperform the uniform HMM as it can capture many consistent morphological affixes and thereby better distinguish between different parts-of-speech. [sent-97, score-0.14]
</p><p>41 The HMM+LM is shown in Figure 2, illustrating the decomposition of the tag sequence into n-grams and a word into its component character bigrams. [sent-98, score-0.257]
</p><p>42 1 Training In order to induce a tagging under this model we use Gibbs sampling, a Markov chain Monte Carlo (MCMC) technique for drawing samples from the posterior distribution over the tag sequences given observed word sequences. [sent-100, score-0.375]
</p><p>43 We present two different sampling strategies: First, a simple Gibbs sampler which randomly samples an update to a single tag given all other tags; and second, a type-level sampler which updates all tags for a given word under a  868 one-tag-per-word-type constraint. [sent-101, score-0.758]
</p><p>44 In order to extract a single tag sequence to test our model against the gold standard we find the tag at each site with maximum marginal probability in the sample set. [sent-102, score-0.313]
</p><p>45 Following standard practice, we perform inference using a collapsed sampler whereby the model parameters U, B, T, E and C are marginalised out. [sent-103, score-0.29]
</p><p>46 After marginalisation the posterior distribution under a PYP prior is described by a variant of the Chinese Restaurant Process (CRP). [sent-104, score-0.194]
</p><p>47 The CRP is based around the analogy of a restaurant with an infinite number of tables, with customers entering one at a time and seating themselves at a table. [sent-105, score-0.33]
</p><p>48 The arrangement of customers at tables defines a clustering which exhibits a power-law behavior controlled by the hyperparameters a and b. [sent-107, score-0.186]
</p><p>49 To complete the restaurant analogy, a dish is then served to each table which is shared by all the customers seated there. [sent-108, score-0.323]
</p><p>50 This corresponds to a draw from the base distribution, which in our case ranges over tags for the transition distribution, and words for the observation distribution. [sent-109, score-0.189]
</p><p>51 A hierarchy of PYPs can be formed by making the  base distribution of a PYP another PYP, following a semantics whereby whenever a customer sits at an empty table in a restaurant, a new customer is also said to enter the restaurant for its base distribution. [sent-118, score-0.477]
</p><p>52 That is, each table at one level is equivalent to a customer at the next deeper level, creating the invariants: Kh−i = nu−i andKu−i = ni−, where u = tl−1 indicates the unigram backoff context of h. [sent-119, score-0.2]
</p><p>53 The hierarchical setting allows for the modelling of elaborate backoff paths from rich and complex structure to successively simpler structures. [sent-121, score-0.253]
</p><p>54 The first local Gibbs sampler (PYP-HMM) updates a single tag assignment at a time, in a similar fashion to Goldwater and Griffiths (2007). [sent-123, score-0.334]
</p><p>55 Changing one tag affects three trigrams, with  posterior P(tl| z−l, t−l, w) ∝ P(tl±2, wl|z−l±2, t−l±2)  ,  where l±2 denotes the range l−2, l−1, l, l+ 1, l+2. [sent-124, score-0.173]
</p><p>56 Twhhee joint 2d disetnroibtuetsio thne over et lhe− t2h,rle−e trigrams contained in tl±2 can be calculated using the PYP formulation. [sent-125, score-0.137]
</p><p>57 Many HMMs used for inducing word classes for language modelling include the restriction that all occurrences of a word type always appear with the same class throughout the corpus (Brown et al. [sent-127, score-0.169]
</p><p>58 Our second sampler (PYP-1HMM) restricts inference to taggings which adhere to this one tag per type restriction. [sent-129, score-0.382]
</p><p>59 This restriction permits efficient inference techniques in which all tags of all occurrences of a word type are updated in parallel. [sent-130, score-0.193]
</p><p>60 The dependency on table counts in the conditional distributions complicates the process of drawing samples for both our models. [sent-133, score-0.229]
</p><p>61 In the non-hierarchical model (Goldwater and Griffiths, 2007) these dependencies can easily be accounted for by incrementing customer counts when such a dependence occurs. [sent-134, score-0.228]
</p><p>62 In our model we would need to sum over all possible table assignments that result in the same tagging, at all levels in the hierarchy: tag trigrams, bigrams and unigrams; and also words, character bigrams and character unigrams. [sent-135, score-0.512]
</p><p>63 To avoid this rather onerous marginalisation2 we instead use expected table counts to calculate the conditional distributions for sampling. [sent-136, score-0.182]
</p><p>64 This formulation defines a simple recPurrence starting with the first customer seated at a table, E1 [Ki] = 1, and as each subsequent customer arrives we fractionally assign them to a new table based on their conditional probability of sitting alone. [sent-138, score-0.291]
</p><p>65 These fractional counts are then carried forward for subsequent customers. [sent-139, score-0.167]
</p><p>66 This approximation is tight for small n, and therefore it should be effective in the case of the local Gibbs sampler where only three trigrams are being resampled. [sent-140, score-0.387]
</p><p>67 number of customers Figure 3: Simulation comparing the expected table count (solid lines) versus the approximation under Eq. [sent-145, score-0.156]
</p><p>68 To resample a sequence of trigrams we start by removing their counts from the current restaurant configuration (resulting in z−). [sent-150, score-0.31]
</p><p>69 For each tag we simulate adding back the trigrams one at a time, calculating their probability under the given z−plus the fractional table counts accumulated by Equation 3. [sent-151, score-0.482]
</p><p>70 We then calculate the expected table count contribution from this trigram and add it to the accumulated counts. [sent-152, score-0.2]
</p><p>71 The fractional table count from the trigram then results in a fractional customer entering the bigram restaurant, and so on down to unigrams. [sent-153, score-0.574]
</p><p>72 After performing this process for all trigrams under consideration and for all tags, we then normalise the resulting tag probabilities and sample an outcome. [sent-155, score-0.272]
</p><p>73 Once a tag has been sampled, we then add all the trigrams to the restaurants sampling their tables assignments explicitly (which are no longer fractional), recorded in z. [sent-156, score-0.397]
</p><p>74 Because we do not marginalise out the table  counts and our expectations are only approximate, this sampler will be biased. [sent-157, score-0.275]
</p><p>75 We place prior distributions on the PYP discount and concentration bx hyperparamters and sample their values using a slice sampler. [sent-162, score-0.29]
</p><p>76 For the discount parameters we employ a uniform Beta distribution (ax ∼ Beta(1, 1)), and for the concentration parameters we use a vague gamma prior (bx ∼ Gamma(10, 0. [sent-163, score-0.288]
</p><p>77 The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred. [sent-175, score-0.199]
</p><p>78 The 1HMM sampler converges more quickly so we use two hundred samples for these models. [sent-176, score-0.246]
</p><p>79 Our model was run with the local sampler (HMM), the type-level sampler (1HMM) and also with the character LM (1HMM-LM). [sent-211, score-0.563]
</p><p>80 As a baseline we report the performance of mkcl s (Och, 1999) on all test corpora. [sent-218, score-0.218]
</p><p>81 This model seems not to have been evaluated in prior work on unsupervised PoS tagging, which is surprising given its consistently good performance. [sent-219, score-0.255]
</p><p>82 We also see that incorporating a character language model  (1HMM-LM) leads to further gains in performance, improving over the best reported scores under both M-1 and VM. [sent-224, score-0.165]
</p><p>83 We have omitted the results for the HMM-LM as experimentation showed that the local Gibbs sampler became hopelessly stuck, failing to 871  Tags sorted by frequency  Figure 4: Sorted frequency of tags for WSJ. [sent-225, score-0.338]
</p><p>84 The gold standard distribution follows a steep exponential curve while the induced model distributions are more uniform. [sent-226, score-0.215]
</p><p>85 To evaluate the effectiveness of the PYP prior we include results using a Dirichlet Process prior (DP). [sent-228, score-0.18]
</p><p>86 Note that the bigram PYP-HMM outperforms the closely related BHMM (the main difference being that we smooth tag bigrams with unigrams). [sent-232, score-0.263]
</p><p>87 The former shows that both our models and mkcl s induce a more uniform  distribution over tags than specified by the treebank. [sent-238, score-0.441]
</p><p>88 The graph in Figure 5 shows that the type-based 1HMM sampler finds a good tagging extremely quickly and then sticks with it,  Number of samples  Figure 5: M-1 accuracy vs. [sent-240, score-0.292]
</p><p>89 and predicted (x-axis) tags, comparing mkcl s (top) and PYP-1HMM-LM (bottom). [sent-242, score-0.218]
</p><p>90 In Figure 6 we compare the distributions over WSJ tags for mkcl s and the PYP-1HMM-LM. [sent-247, score-0.423]
</p><p>91 872 In the first column for mkcl s and the third column for our model we can see similar classes with significant counts for DTs and PRPs, indicating a class that the models may be using to represent the start of sentences (informed by start transitions or capitalisation). [sent-250, score-0.337]
</p><p>92 The character language model provides large  gains in performance on a number of corpora, in particular those with rich morphology (Arabic +5%, Portuguese +5%, Spanish +4%). [sent-255, score-0.165]
</p><p>93 We again note the strong performance of the mkcl s model, significantly beating recently published state-of-theart results for both Dutch and Swedish. [sent-256, score-0.218]
</p><p>94 Overall our best model (PYP-1HMM-LM) outperforms both the state-of-the-art, where previous work exists, as well as mkcl s consistently across all languages. [sent-257, score-0.297]
</p><p>95 We have combined hierarchical Bayesian priors with a trigram HMM and character language model to produce a model with consistently state-of-the-art performance across corpora in ten languages. [sent-260, score-0.566]
</p><p>96 However our analysis indicates that there is still room for improvement, particularly in model formulation and developing effective inference algorithms. [sent-261, score-0.136]
</p><p>97 The continued successes of models combining hierarchical Pitman-Yor priors with expressive work’s  graphical models attests to this frame-  enduring  attraction,  we foresee continued  interest in applying this technique  to other NLP  tasks. [sent-263, score-0.165]
</p><p>98 Tokens  Tag types  Table 2: Many-to-1 accuracy across a range of languages, comparing our model with mkcl s and the best published result (? [sent-265, score-0.261]
</p><p>99 A comparison of bayesian estimators for unsupervised hidden markov model pos taggers. [sent-318, score-0.337]
</p><p>100 A hierarchical bayesian language model based on pitman-yor processes. [sent-389, score-0.21]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pyp', 0.413), ('tl', 0.235), ('mkcl', 0.218), ('hmm', 0.215), ('sampler', 0.199), ('goldwater', 0.187), ('trigram', 0.157), ('trigrams', 0.137), ('tag', 0.135), ('modelling', 0.123), ('character', 0.122), ('customer', 0.109), ('distributions', 0.106), ('customers', 0.105), ('griffiths', 0.103), ('tags', 0.099), ('induction', 0.099), ('restaurant', 0.097), ('smoothing', 0.091), ('fractional', 0.091), ('prior', 0.09), ('bayesian', 0.089), ('priors', 0.087), ('unsupervised', 0.086), ('gibbs', 0.084), ('bigram', 0.083), ('pos', 0.083), ('bj', 0.081), ('brown', 0.08), ('sampling', 0.079), ('christodoulopoulos', 0.079), ('hierarchical', 0.078), ('counts', 0.076), ('seated', 0.073), ('tij', 0.073), ('nh', 0.07), ('distribution', 0.066), ('ki', 0.065), ('emission', 0.064), ('dirichlet', 0.062), ('sharon', 0.061), ('bx', 0.059), ('kh', 0.059), ('vm', 0.059), ('wl', 0.059), ('morristown', 0.058), ('uniform', 0.058), ('bt', 0.057), ('clark', 0.056), ('generalisation', 0.055), ('nj', 0.052), ('backoff', 0.052), ('approximation', 0.051), ('bu', 0.05), ('bhmm', 0.048), ('dish', 0.048), ('seating', 0.048), ('wlk', 0.048), ('inference', 0.048), ('base', 0.048), ('samples', 0.047), ('tagging', 0.046), ('tables', 0.046), ('restriction', 0.046), ('developing', 0.045), ('bigrams', 0.045), ('model', 0.043), ('accumulated', 0.043), ('dj', 0.043), ('ganchev', 0.043), ('entering', 0.043), ('mkcls', 0.043), ('strands', 0.043), ('en', 0.043), ('transition', 0.042), ('develop', 0.041), ('ax', 0.04), ('hmms', 0.04), ('sorted', 0.04), ('sampled', 0.039), ('gamma', 0.039), ('zl', 0.039), ('morphological', 0.039), ('unigram', 0.039), ('conditioning', 0.038), ('posterior', 0.038), ('teh', 0.037), ('dp', 0.037), ('samplers', 0.037), ('analogy', 0.037), ('crp', 0.037), ('mcmc', 0.037), ('plate', 0.037), ('johnson', 0.036), ('consistently', 0.036), ('markov', 0.036), ('concentration', 0.035), ('arrangement', 0.035), ('beta', 0.035), ('focussed', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999881 <a title="15-tfidf-1" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>2 0.2179828 <a title="15-tfidf-2" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>3 0.142078 <a title="15-tfidf-3" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>Author: Dipanjan Das ; Slav Petrov</p><p>Abstract: We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language. Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages. We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (BergKirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</p><p>4 0.13109347 <a title="15-tfidf-4" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>5 0.11531541 <a title="15-tfidf-5" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>Author: Ming Tan ; Wenli Zhou ; Lei Zheng ; Shaojun Wang</p><p>Abstract: This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-the- art parsing-based machine translation system.</p><p>6 0.10498431 <a title="15-tfidf-6" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>7 0.10139764 <a title="15-tfidf-7" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>8 0.099672578 <a title="15-tfidf-8" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>9 0.093149632 <a title="15-tfidf-9" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>10 0.093012765 <a title="15-tfidf-10" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>11 0.089722872 <a title="15-tfidf-11" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>12 0.086383671 <a title="15-tfidf-12" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>13 0.083645545 <a title="15-tfidf-13" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>14 0.0834025 <a title="15-tfidf-14" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>15 0.082892679 <a title="15-tfidf-15" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>16 0.082645528 <a title="15-tfidf-16" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>17 0.079422332 <a title="15-tfidf-17" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>18 0.079386123 <a title="15-tfidf-18" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>19 0.078386553 <a title="15-tfidf-19" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>20 0.076442845 <a title="15-tfidf-20" href="./acl-2011-full-for-print.html">342 acl-2011-full-for-print</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.227), (1, -0.032), (2, -0.027), (3, -0.022), (4, -0.01), (5, -0.052), (6, 0.038), (7, 0.079), (8, -0.028), (9, 0.139), (10, 0.04), (11, 0.073), (12, 0.039), (13, 0.18), (14, 0.009), (15, 0.026), (16, -0.095), (17, 0.046), (18, 0.005), (19, 0.049), (20, 0.046), (21, 0.048), (22, 0.101), (23, -0.036), (24, 0.072), (25, 0.002), (26, -0.02), (27, -0.031), (28, -0.048), (29, 0.021), (30, 0.001), (31, -0.029), (32, -0.042), (33, -0.023), (34, 0.051), (35, -0.054), (36, 0.084), (37, -0.093), (38, -0.009), (39, -0.023), (40, 0.019), (41, -0.008), (42, 0.032), (43, -0.047), (44, -0.061), (45, -0.068), (46, -0.109), (47, 0.061), (48, -0.008), (49, -0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95475632 <a title="15-lsi-1" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>2 0.69415408 <a title="15-lsi-2" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>Author: Ming Tan ; Wenli Zhou ; Lei Zheng ; Shaojun Wang</p><p>Abstract: This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-the- art parsing-based machine translation system.</p><p>3 0.68667865 <a title="15-lsi-3" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>Author: Kristina Toutanova ; Michel Galley</p><p>Abstract: Contrary to popular belief, we show that the optimal parameters for IBM Model 1 are not unique. We demonstrate that, for a large class of words, IBM Model 1 is indifferent among a continuum of ways to allocate probability mass to their translations. We study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate.</p><p>4 0.65246511 <a title="15-lsi-4" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>Author: Elias Ponvert ; Jason Baldridge ; Katrin Erk</p><p>Abstract: We consider a new subproblem of unsupervised parsing from raw text, unsupervised partial parsing—the unsupervised version of text chunking. We show that addressing this task directly, using probabilistic finite-state methods, produces better results than relying on the local predictions of a current best unsupervised parser, Seginer’s (2007) CCL. These finite-state models are combined in a cascade to produce more general (full-sentence) constituent structures; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English, German and Chinese. Finally, we address the use of phrasal punctuation as a heuristic indicator of phrasal boundaries, both in our system and in CCL.</p><p>5 0.62710643 <a title="15-lsi-5" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>Author: Hinrich Schutze</p><p>Abstract: Building on earlier work that integrates different factors in language modeling, we view (i) backing off to a shorter history and (ii) class-based generalization as two complementary mechanisms of using a larger equivalence class for prediction when the default equivalence class is too small for reliable estimation. This view entails that the classes in a language model should be learned from rare events only and should be preferably applied to rare events. We construct such a model and show that both training on rare events and preferable application to rare events improve perplexity when compared to a simple direct interpolation of class-based with standard language models.</p><p>6 0.62354213 <a title="15-lsi-6" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>7 0.60554248 <a title="15-lsi-7" href="./acl-2011-Generalized_Interpolation_in_Decision_Tree_LM.html">142 acl-2011-Generalized Interpolation in Decision Tree LM</a></p>
<p>8 0.60377586 <a title="15-lsi-8" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>9 0.60028124 <a title="15-lsi-9" href="./acl-2011-A_Scalable_Probabilistic_Classifier_for_Language_Modeling.html">24 acl-2011-A Scalable Probabilistic Classifier for Language Modeling</a></p>
<p>10 0.59854066 <a title="15-lsi-10" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>11 0.58862478 <a title="15-lsi-11" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>12 0.58707118 <a title="15-lsi-12" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>13 0.57526284 <a title="15-lsi-13" href="./acl-2011-Hierarchical_Text_Classification_with_Latent_Concepts.html">150 acl-2011-Hierarchical Text Classification with Latent Concepts</a></p>
<p>14 0.56129682 <a title="15-lsi-14" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>15 0.55492073 <a title="15-lsi-15" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>16 0.54652846 <a title="15-lsi-16" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>17 0.54523939 <a title="15-lsi-17" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>18 0.531578 <a title="15-lsi-18" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>19 0.52508217 <a title="15-lsi-19" href="./acl-2011-Temporal_Restricted_Boltzmann_Machines_for_Dependency_Parsing.html">295 acl-2011-Temporal Restricted Boltzmann Machines for Dependency Parsing</a></p>
<p>20 0.52142763 <a title="15-lsi-20" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.027), (10, 0.17), (17, 0.071), (26, 0.024), (31, 0.01), (37, 0.094), (39, 0.069), (41, 0.09), (55, 0.054), (59, 0.062), (72, 0.029), (91, 0.042), (96, 0.158), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90836906 <a title="15-lda-1" href="./acl-2011-Latent_Class_Transliteration_based_on_Source_Language_Origin.html">197 acl-2011-Latent Class Transliteration based on Source Language Origin</a></p>
<p>Author: Masato Hagiwara ; Satoshi Sekine</p><p>Abstract: Transliteration, a rich source of proper noun spelling variations, is usually recognized by phonetic- or spelling-based models. However, a single model cannot deal with different words from different language origins, e.g., “get” in “piaget” and “target.” Li et al. (2007) propose a method which explicitly models and classifies the source language origins and switches transliteration models accordingly. This model, however, requires an explicitly tagged training set with language origins. We propose a novel method which models language origins as latent classes. The parameters are learned from a set of transliterated word pairs via the EM algorithm. The experimental results of the transliteration task of Western names to Japanese show that the proposed model can achieve higher accuracy compared to the conventional models without latent classes.</p><p>same-paper 2 0.86252964 <a title="15-lda-2" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>3 0.8250621 <a title="15-lda-3" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>Author: Sara Stymne</p><p>Abstract: In this thesis proposal Ipresent my thesis work, about pre- and postprocessing for statistical machine translation, mainly into Germanic languages. I focus my work on four areas: compounding, definite noun phrases, reordering, and error correction. Initial results are positive within all four areas, and there are promising possibilities for extending these approaches. In addition Ialso focus on methods for performing thorough error analysis of machine translation output, which can both motivate and evaluate the studies performed.</p><p>4 0.8142336 <a title="15-lda-4" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>Author: Peng Cai ; Wei Gao ; Aoying Zhou ; Kam-Fai Wong</p><p>Abstract: We propose to directly measure the importance of queries in the source domain to the target domain where no rank labels of documents are available, which is referred to as query weighting. Query weighting is a key step in ranking model adaptation. As the learning object of ranking algorithms is divided by query instances, we argue that it’s more reasonable to conduct importance weighting at query level than document level. We present two query weighting schemes. The first compresses the query into a query feature vector, which aggregates all document instances in the same query, and then conducts query weighting based on the query feature vector. This method can efficiently estimate query importance by compressing query data, but the potential risk is information loss resulted from the compression. The second measures the similarity between the source query and each target query, and then combines these fine-grained similarity values for its importance estimation. Adaptation experiments on LETOR3.0 data set demonstrate that query weighting significantly outperforms document instance weighting methods.</p><p>5 0.80231953 <a title="15-lda-5" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>Author: Joel Lang ; Mirella Lapata</p><p>Abstract: In this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. We present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality. The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.</p><p>6 0.79509902 <a title="15-lda-6" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>7 0.79130411 <a title="15-lda-7" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>8 0.79114264 <a title="15-lda-8" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>9 0.78951091 <a title="15-lda-9" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>10 0.78921688 <a title="15-lda-10" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>11 0.78832459 <a title="15-lda-11" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>12 0.78825551 <a title="15-lda-12" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>13 0.78692877 <a title="15-lda-13" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>14 0.78586942 <a title="15-lda-14" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>15 0.78563845 <a title="15-lda-15" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>16 0.78373665 <a title="15-lda-16" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>17 0.7827459 <a title="15-lda-17" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>18 0.78268826 <a title="15-lda-18" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>19 0.78187799 <a title="15-lda-19" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>20 0.77919698 <a title="15-lda-20" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
