<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 acl-2011-Clustering Comparable Corpora For Bilingual Lexicon Extraction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-70" href="#">acl2011-70</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 acl-2011-Clustering Comparable Corpora For Bilingual Lexicon Extraction</h1>
<br/><p>Source: <a title="acl-2011-70-pdf" href="http://aclweb.org/anthology//P/P11/P11-2083.pdf">pdf</a></p><p>Author: Bo Li ; Eric Gaussier ; Akiko Aizawa</p><p>Abstract: We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches.</p><p>Reference: <a title="acl-2011-70-reference" href="../acl2011_reference/acl-2011-Clustering_Comparable_Corpora_For_Bilingual_Lexicon_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. [sent-2, score-1.962]
</p><p>2 We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. [sent-3, score-0.814]
</p><p>3 Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches. [sent-4, score-1.179]
</p><p>4 fr  Bilingual lexicons are an important resource in multilingual natural language processing tasks such as statistical machine translation (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). [sent-6, score-0.294]
</p><p>5 Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. [sent-7, score-1.188]
</p><p>6 Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i. [sent-8, score-0.22]
</p><p>7 corpora consisting of documents in different languages covering overlapping information. [sent-10, score-0.302]
</p><p>8 Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D ´ejean et al. [sent-11, score-1.119]
</p><p>9 , 2009; 473 Akiko Aizawa National Institute of Informatics Tokyo, Japan ai z awa @ nii . [sent-16, score-0.056]
</p><p>10 The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. [sent-19, score-0.708]
</p><p>11 More recently, and departing from such traditional approaches, we have proposed in (Li and Gaussier, 2010) an approach based on improving the comparability of the corpus under consideration, prior to extracting bilingual lexicons. [sent-22, score-0.869]
</p><p>12 This approach is interesting since there is no point in trying to extract lexicons from a corpus with a low degree of comparability, as the probability of finding translations of any given word is low in such cases. [sent-23, score-0.401]
</p><p>13 We follow here the same general idea and aim, in a first step, at improving the comparability of a given corpus while preserving most of its vocabulary. [sent-24, score-0.521]
</p><p>14 However, unlike the previous work, we show here that it is possible to guarantee a cer-  tain degree of homogeneity for the improved corpus, and that this homogeneity translates into a significant improvement of both the quality of the resulting corpora and the bilingual lexicons extracted. [sent-25, score-1.231]
</p><p>15 2  Enhancing Comparable Corpora: A Clustering Approach  We first introduce in this section the comparability measure proposed in former work, prior to describing the clustering-based algorithm to improve the Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o. [sent-26, score-0.472]
</p><p>16 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 473–478, quality of a given comparable corpus. [sent-28, score-0.28]
</p><p>17 For convenience, the following discussion will be made in the context of the English-French comparable corpus. [sent-29, score-0.22]
</p><p>18 Let σ be a function indicating whether a translation from the translation set Tw of the word w is found in the vocabulary Pv oseft a corpus P, io. [sent-32, score-0.223]
</p><p>19 01 eiflfs Tew∩ Pv6= ∅  and let D be a bilingual dictionary with Dev denoting iatns English vocabulary lan ddic Dfv niatsr yFr wenithch D vocabulary. [sent-35, score-0.453]
</p><p>20 iTtsheE comparability measure M can be written as: M(Pe, Pf)  =  (1)  Pw∈Pe∩Dev σ(w,Pf) + Pw∈Pf∩Dfv σ(w,Pe) P#w(Pe∩ Dev) +P #w(Pf∩ Dfv)  where #w (P) denotes the number of different words present i nd Pno. [sent-36, score-0.472]
</p><p>21 t sO tnhee can fbinerd ofrfom di equatwioonr 1s t phraets Ment directly measures the proportion of source/target words translated in the target/source vocabulary of P. [sent-37, score-0.101]
</p><p>22 2  Clustering Documents for High Quality Comparable Corpora If a corpus covers a limited set of topics, it is more  likely to contain consistent information on the words used (Morin et al. [sent-39, score-0.088]
</p><p>23 , 2007), leading to improved bilingual lexicons extracted with existing algorithms relying on the distributional hypothesis. [sent-40, score-0.594]
</p><p>24 The term homogeneity directly refers to this fact, and we will say, in an informal manner, that a corpus is homogeneous if it covers a limited set of topics. [sent-41, score-0.417]
</p><p>25 The rationale for the algorithm we introduce here to enhance corpus comparability is precisely based on the concept of homogeneity. [sent-42, score-0.5]
</p><p>26 In order to find document sets which are similar with each other (i. [sent-43, score-0.04]
</p><p>27 homogeneous), it 474 is natural to resort to clustering techniques. [sent-45, score-0.084]
</p><p>28 Furthermore, since we need homogeneous corpora for bilingual lexicon extraction, it will be convenient to rely on techniques which allows one to easily prune less relevant clusters. [sent-46, score-0.87]
</p><p>29 To perform all this, we use in this work a standard hierarchical agglomerative clustering method. [sent-47, score-0.084]
</p><p>30 1 Bilingual Clustering Algorithm The overall process retained to build high quality, homogeneous comparable corpora relies on the fol-  lowing steps: 1. [sent-50, score-0.607]
</p><p>31 Using the bilingual similarity measure defined in Section 2. [sent-51, score-0.457]
</p><p>32 2, cluster English and French documents so as to get bilingual dendrograms from the original corpus P by grouping docufmroemnts t hweit ohr rigeilantaeld c content; 2. [sent-53, score-0.586]
</p><p>33 Pick high quality sub-clusters by thresholding the obtained dendrograms according to the node depth, which retains nodes far from the roots of the clustering trees; 3. [sent-54, score-0.284]
</p><p>34 Combine all these sub-clusters to form a new comparable corpus PH, which thus contains homogeneous, high-quality subparts; 4. [sent-55, score-0.269]
</p><p>35 Use again steps (1), (2) and (3) to enrich the remaining subpart of P (denoted as PL, PL = rPe \ PH) gw situhb pexatretr onfal P resources. [sent-56, score-0.064]
</p><p>36 The first three steps aim at extracting the most comparable and homogeneous subpart of P. [sent-57, score-0.498]
</p><p>37 Once this hpaarsa bbeleen a done, one nneeoeduss stou b rpeasorrtt o tfo P new corpora  if one wants to build an homogeneous corpus with a high degree of comparability from PL. [sent-58, score-0.904]
</p><p>38 The tFwreon high quality subparts o Ebntagilnisehd farrotm o ft hPese two new comparable corpora in step (4) are then combined with PH to constitute the final comparable corpus woift higher quality. [sent-60, score-0.861]
</p><p>39 2 Similarity Measure Let us assume that we have two document sets (i. [sent-63, score-0.04]
</p><p>40 In the task of bilingual lexicon extraction, two document sets are similar to each other and should be clustered if the combination of the two can complement the content of each single set, which relates to the notion of homogeneity. [sent-66, score-0.523]
</p><p>41 In other words, both the English part C1e of C1 and the French part of C1 should be comparable to their counterparts (respectively the same for the French part C2f of C2 and the English part C2e of C2). [sent-67, score-0.344]
</p><p>42 C2f)  C1f)  and (Ce2, should dominate the overall similarity sim(C1 , C2). [sent-69, score-0.047]
</p><p>43 Since the content relatedness in the comparable corpus is basically reflected by the relations between all the possible bilingual document pairs, we use here the number of document pairs to represent the scale of the comparable corpus. [sent-70, score-0.927]
</p><p>44 The weight β can thus be defined as the proportion of possible document pairs in the current comparable corpus (Ce1, C2f) to all the possible document pairs, wcohrpicuhs si (s:C  β =#d(Ce1) · ##dd((CC2f1e)) + · # #dd((CC2f2e)) · #d(C1f) where #d(C) stands for the number of documents in C. [sent-71, score-0.444]
</p><p>45 However, )t shtiasn measure ed noeusm nboetr integrate theent rse iln-  aCt. [sent-72, score-0.08]
</p><p>46 iv He length ,o tfh itsh em Ferasenucreh daoneds English parts, hweh rieclhactually impacts the performance of bilingual lexicon extraction. [sent-73, score-0.483]
</p><p>47 assuming that all clusters should contain the same number of English and French documents), having completely unbalanced corpora is also not desirable. [sent-76, score-0.255]
</p><p>48 In addition, two monolingual corpora Wiki-En and Wiki-Fr were built by respectively retrieving all the articles below the category Society and Soci e´t e´ from the Wikipedia dump files3. [sent-79, score-0.308]
</p><p>49 The bilingual dictionary used in the experiments is constructed from an online dictionary. [sent-80, score-0.395]
</p><p>50 It consists of 33k distinct English words and 28k distinct French words, constituting 76k translation pairs. [sent-81, score-0.058]
</p><p>51 In our experiments, we use the method described in this paper, as well as the one in (Li and Gaussier, 2010) which is the only alternative method to enhance corpus comparability. [sent-82, score-0.08]
</p><p>52 1 Improving Corpus Quality In this subsection, the clustering algorithm described in Section 2. [sent-84, score-0.084]
</p><p>53 1 is employed to improve the quality of the comparable corpus. [sent-86, score-0.28]
</p><p>54 The corpora GH95 and  SDA95 are used as the original corpus P0 (56k English d5o acruem uesendts asn tdh e4 o2kri gFinreanlc cho documents). [sent-87, score-0.299]
</p><p>55 We consider two external corpora: PT1 (109k English dcoocnsuimdeernt tsw aond e 8x7tekr nFarlen cochrp documents) consisting of the corpora LAT94, MON94 and SDA94; PT2 (368k English odroac LumATe9n4t,s ManOdN 39748 akn dF SreDncAh9 documents) consisting of Wiki-En and Wiki-Fr. [sent-88, score-0.347]
</p><p>56 org 3The Wikipedia dump files can be downloaded at http://download. [sent-93, score-0.093]
</p><p>57 In this paper, we use the English dump file on July 13, 2009 and the French dump file on July 7, 2009. [sent-96, score-0.248]
</p><p>58 0 8%  Table 1: Performance of the bilingual lexicon extraction from different corpora (best results in bold)  After the clustering process, we obtain the resulting corpora P1 (with the external corpus PT1) and Ping2 (with PT2). [sent-106, score-1.132]
</p><p>59 A(ws imthen thteion eexdte before, we a Plso used tPhe (mweitthho Pd described in (Li and Gaussier, 2010) on the same data, producing resulting corpora (with PT1) aen dda (with PT2) sfruolmtin gP c0. [sent-107, score-0.243]
</p><p>60 vocabulary eof v otchea original corpus has been preserved. [sent-114, score-0.142]
</p><p>61 Breot ohf corpora are more comparable fth Pan P0 of which the comparability rise 0c o. [sent-119, score-0.855]
</p><p>62 Furthermore, both P1 and P2 are more comparable t Fhuanrt (comparability 0. [sent-121, score-0.22]
</p><p>63 The intrinsic evaluation shows the efficiency of our approach which can improve the quality of the given corpus while preserving most of its vocabulary. [sent-124, score-0.161]
</p><p>64 2 Bilingual Lexicon Extraction Experiments To extract bilingual lexicons from comparable corpora, we directly use here the method proposed by Fung and Yee (1998) which has been referred to as the standard approach in more recent studies (D´ ejean et al. [sent-126, score-0.937]
</p><p>65 In this approach, each word w is represented as a context vector consisting of the words co-occurring with w in a certain window in the corpus. [sent-129, score-0.035]
</p><p>66 The context vectors in different languages are then bridged with an existing bilingual dictionary. [sent-130, score-0.386]
</p><p>67 Finally, a similarity score is given to any word pair based on the cosine of their respective context vec-  tors. [sent-131, score-0.047]
</p><p>68 English words not present in Pe or with no translation ignl Pf are desxc nloutd epdre fsreonmt tnhe P evaluation set. [sent-135, score-0.058]
</p><p>69 For each English word in the evaluation set, all the French words in Pf are then ranked according to their similarity iwni tPh the English word. [sent-136, score-0.047]
</p><p>70 Precision and recall are then computed on the first N translation candidate lists. [sent-137, score-0.058]
</p><p>71 The precision amounts in this case to the proportion of lists containing the correct translation (in case of multiple translations, a list is deemed to contain the correct translation as soon as one of the possible translations is present). [sent-138, score-0.258]
</p><p>72 The recall is the proportion of correct translations found in the lists to all the translations in the corpus. [sent-139, score-0.179]
</p><p>73 This evaluation  procedure has been used in previous studies and is now standard. [sent-140, score-0.038]
</p><p>74 As one can note, tahtiev eb e dsitf freerseunltcse (in bold) are o Pbtained from the corpora P2 built with the method we have described in tphoirsa paper. [sent-150, score-0.215]
</p><p>75 The lexicons extracted from the enhanced corpora are of much higher quality than the ones obtained from the original corpus . [sent-151, score-0.629]
</p><p>76 ff0e%re rnceela ti sv more n re Pmarkable pwairthed dP w2,i twhh Pich is obtained from a large external corpus PT2. [sent-159, score-0.117]
</p><p>77 Intuitively, one can expect to find, in larger corpora, more documents related to a given corpus,  oPa2c0)h  (P10  an intuition which seems to be confirmed by our results. [sent-160, score-0.052]
</p><p>78 One can also notice, by comparing P2 and as Owneell as nP a1l aon ndo a y rem coamrkpaabrlien improvePmenta ws wheenll considering our approach and the early methodology. [sent-161, score-0.033]
</p><p>79 In a second series of experiments, we let N vary from 1 to 300 and plot the results obtained with different evaluation measure in Figure 1. [sent-163, score-0.086]
</p><p>80 recall) scores for the lexicons extractedon each ofthe 5 corporaP0, Pco1n asnedx rPac2. [sent-166, score-0.236]
</p><p>81 As one can note, our method consistently outperforms the previous work and also the original corpus on all the values considered for N. [sent-168, score-0.084]
</p><p>82 sPu2l0t  Ptic1e0,,  P10, P20,  N  (a) Precision  N  (b) Recall Figure 1: Performance of bilingual lexicon extraction from different corpora with varied N values from 1 to 300. [sent-169, score-0.75]
</p><p>83 477 4  Discussion  As previous studies on bilingual lexicon extraction from comparable corpora radically differ on resources used and technical choices, it is very difficult to compare them in a unified framework (Laroche and Langlais, 2010). [sent-171, score-1.008]
</p><p>84 enhancing bilingual corpora prior to extracting bilingual lexicons from them). [sent-174, score-1.304]
</p><p>85 , 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. [sent-176, score-0.22]
</p><p>86 However, their approach only focuses on a very small part of the original corpus, whereas our work aims at preserving most of  the vocabulary of the original corpus. [sent-177, score-0.211]
</p><p>87 We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus prior to extracting information from it. [sent-178, score-0.466]
</p><p>88 However, despite this latter work, we have shown here a method which ensures homogeneity of the obtained corpus, and which finally leads to comparable corpora of higher quality. [sent-179, score-0.626]
</p><p>89 In turn such corpora yield better bilingual lexicons extracted. [sent-180, score-0.809]
</p><p>90 Phrasal translation and query expansion techniques for crosslanguage information retrieval. [sent-185, score-0.058]
</p><p>91 An approach based on multilingual thesauri and model combination for bilingual lexicon extraction. [sent-189, score-0.483]
</p><p>92 An IR approach for translating new words from nonparallel, comparable texts. [sent-197, score-0.22]
</p><p>93 Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences. [sent-201, score-0.398]
</p><p>94 A geometric view on bilingual lexicon extraction from comparable corpora. [sent-211, score-0.755]
</p><p>95 Revisiting context-based projection methods for term-translation spotting in comparable corpora. [sent-215, score-0.22]
</p><p>96 Improving corpus comparability for bilingual lexicon extraction from comparable corpora. [sent-219, score-1.224]
</p><p>97 Bilingual terminology mining using brain, not brawn comparable corpora. [sent-223, score-0.277]
</p><p>98 Improved machine translation performance via parallel sentence extraction from comparable corpora. [sent-231, score-0.33]
</p><p>99 Automatic identification of word  translations from unrelated English and German corpora. [sent-240, score-0.068]
</p><p>100 Extracting bilingual dictionary from comparable corpora with dependency heterogeneity. [sent-252, score-0.83]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('comparability', 0.42), ('bilingual', 0.358), ('gaussier', 0.288), ('lexicons', 0.236), ('comparable', 0.22), ('corpora', 0.215), ('homogeneous', 0.172), ('homogeneity', 0.157), ('french', 0.157), ('lexicon', 0.125), ('pf', 0.121), ('fung', 0.104), ('dfv', 0.097), ('subparts', 0.097), ('enhancing', 0.095), ('dump', 0.093), ('munteanu', 0.089), ('ejean', 0.085), ('morin', 0.085), ('clustering', 0.084), ('pl', 0.076), ('pe', 0.071), ('translations', 0.068), ('dendrograms', 0.064), ('robitaille', 0.064), ('shezaf', 0.064), ('siml', 0.064), ('subpart', 0.064), ('quality', 0.06), ('translation', 0.058), ('vocabulary', 0.058), ('laroche', 0.057), ('sda', 0.057), ('ph', 0.056), ('li', 0.055), ('sim', 0.054), ('english', 0.054), ('measure', 0.052), ('ballesteros', 0.052), ('garera', 0.052), ('nonparallel', 0.052), ('documents', 0.052), ('preserving', 0.052), ('extraction', 0.052), ('corpus', 0.049), ('degree', 0.048), ('similarity', 0.047), ('pd', 0.047), ('pascale', 0.047), ('dev', 0.045), ('dragos', 0.045), ('proportion', 0.043), ('extracting', 0.042), ('thresholding', 0.042), ('yu', 0.041), ('dd', 0.04), ('unbalanced', 0.04), ('document', 0.04), ('covers', 0.039), ('pthe', 0.039), ('july', 0.038), ('studies', 0.038), ('dictionary', 0.037), ('yee', 0.037), ('tsujii', 0.036), ('consisting', 0.035), ('original', 0.035), ('obtained', 0.034), ('external', 0.034), ('pw', 0.033), ('ws', 0.033), ('intuitively', 0.032), ('eric', 0.032), ('part', 0.031), ('file', 0.031), ('enhance', 0.031), ('precision', 0.031), ('stefan', 0.03), ('terminology', 0.029), ('fir', 0.028), ('hweit', 0.028), ('nii', 0.028), ('yasuhiro', 0.028), ('junichi', 0.028), ('dda', 0.028), ('theent', 0.028), ('akn', 0.028), ('apnd', 0.028), ('ater', 0.028), ('audrey', 0.028), ('awa', 0.028), ('brawn', 0.028), ('bridged', 0.028), ('compiling', 0.028), ('daille', 0.028), ('herald', 0.028), ('iiosn', 0.028), ('kyo', 0.028), ('matveeva', 0.028), ('nikesh', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="70-tfidf-1" href="./acl-2011-Clustering_Comparable_Corpora_For_Bilingual_Lexicon_Extraction.html">70 acl-2011-Clustering Comparable Corpora For Bilingual Lexicon Extraction</a></p>
<p>Author: Bo Li ; Eric Gaussier ; Akiko Aizawa</p><p>Abstract: We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches.</p><p>2 0.27613506 <a title="70-tfidf-2" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>Author: Emmanuel Prochasson ; Pascale Fung</p><p>Abstract: We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.</p><p>3 0.15239531 <a title="70-tfidf-3" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>Author: Ivan Vulic ; Wim De Smet ; Marie-Francine Moens</p><p>Abstract: A topic model outputs a set of multinomial distributions over words for each topic. In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources. Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.</p><p>4 0.13312505 <a title="70-tfidf-4" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Hal Daume III ; Raghavendra Udupa</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. Since the candidate alignments are noisy, we de- velop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain.</p><p>5 0.12492705 <a title="70-tfidf-5" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>Author: Matthew R. Scott ; Xiaohua Liu ; Ming Zhou ; Microsoft Engkoo Team</p><p>Abstract: This paper presents Engkoo 1, a system for exploring and learning language. It is built primarily by mining translation knowledge from billions of web pages - using the Internet to catch language in motion. Currently Engkoo is built for Chinese users who are learning English; however the technology itself is language independent and can be extended in the future. At a system level, Engkoo is an application platform that supports a multitude of NLP technologies such as cross language retrieval, alignment, sentence classification, and statistical machine translation. The data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web. Specifically, web pages that contain both Chinese and English are discovered and analyzed for parallelism, extracted and formulated into clear term definitions and sample sentences. This approach allows us to build perhaps the world’s largest lexicon linking both Chinese and English together - at the same time covering the most up-to-date terms as captured by the net.</p><p>6 0.10675991 <a title="70-tfidf-6" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>7 0.10246957 <a title="70-tfidf-7" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>8 0.10055555 <a title="70-tfidf-8" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>9 0.09689445 <a title="70-tfidf-9" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>10 0.092430048 <a title="70-tfidf-10" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>11 0.089896433 <a title="70-tfidf-11" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>12 0.085428067 <a title="70-tfidf-12" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>13 0.079036884 <a title="70-tfidf-13" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>14 0.078854539 <a title="70-tfidf-14" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>15 0.074777216 <a title="70-tfidf-15" href="./acl-2011-Monolingual_Alignment_by_Edit_Rate_Computation_on_Sentential_Paraphrase_Pairs.html">225 acl-2011-Monolingual Alignment by Edit Rate Computation on Sentential Paraphrase Pairs</a></p>
<p>16 0.069938272 <a title="70-tfidf-16" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>17 0.064165436 <a title="70-tfidf-17" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>18 0.05872504 <a title="70-tfidf-18" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>19 0.058625359 <a title="70-tfidf-19" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>20 0.058577135 <a title="70-tfidf-20" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.185), (1, -0.025), (2, 0.017), (3, 0.121), (4, 0.026), (5, -0.024), (6, 0.078), (7, 0.05), (8, 0.021), (9, -0.05), (10, 0.007), (11, -0.023), (12, 0.066), (13, -0.041), (14, 0.083), (15, -0.036), (16, 0.028), (17, -0.016), (18, 0.091), (19, -0.099), (20, 0.034), (21, -0.048), (22, 0.04), (23, -0.007), (24, -0.036), (25, 0.008), (26, -0.061), (27, 0.067), (28, 0.097), (29, -0.176), (30, 0.06), (31, -0.108), (32, 0.04), (33, -0.076), (34, 0.012), (35, 0.049), (36, 0.064), (37, 0.031), (38, 0.006), (39, -0.008), (40, -0.098), (41, 0.106), (42, 0.117), (43, 0.002), (44, 0.014), (45, -0.074), (46, 0.044), (47, -0.096), (48, 0.072), (49, 0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96415949 <a title="70-lsi-1" href="./acl-2011-Clustering_Comparable_Corpora_For_Bilingual_Lexicon_Extraction.html">70 acl-2011-Clustering Comparable Corpora For Bilingual Lexicon Extraction</a></p>
<p>Author: Bo Li ; Eric Gaussier ; Akiko Aizawa</p><p>Abstract: We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches.</p><p>2 0.84973973 <a title="70-lsi-2" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>Author: Emmanuel Prochasson ; Pascale Fung</p><p>Abstract: We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.</p><p>3 0.74791765 <a title="70-lsi-3" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Hal Daume III ; Raghavendra Udupa</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. Since the candidate alignments are noisy, we de- velop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain.</p><p>4 0.64286107 <a title="70-lsi-4" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>Author: Matthew R. Scott ; Xiaohua Liu ; Ming Zhou ; Microsoft Engkoo Team</p><p>Abstract: This paper presents Engkoo 1, a system for exploring and learning language. It is built primarily by mining translation knowledge from billions of web pages - using the Internet to catch language in motion. Currently Engkoo is built for Chinese users who are learning English; however the technology itself is language independent and can be extended in the future. At a system level, Engkoo is an application platform that supports a multitude of NLP technologies such as cross language retrieval, alignment, sentence classification, and statistical machine translation. The data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web. Specifically, web pages that contain both Chinese and English are discovered and analyzed for parallelism, extracted and formulated into clear term definitions and sample sentences. This approach allows us to build perhaps the world’s largest lexicon linking both Chinese and English together - at the same time covering the most up-to-date terms as captured by the net.</p><p>5 0.61188412 <a title="70-lsi-5" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>Author: Moshe Koppel ; Noam Ordan</p><p>Abstract: While it is has often been observed that the product of translation is somehow different than non-translated text, scholars have emphasized two distinct bases for such differences. Some have noted interference from the source language spilling over into translation in a source-language-specific way, while others have noted general effects of the process of translation that are independent of source language. Using a series of text categorization experiments, we show that both these effects exist and that, moreover, there is a continuum between them. There are many effects of translation that are consistent among texts translated from a given source language, some of which are consistent even among texts translated from families of source languages. Significantly, we find that even for widely unrelated source languages and multiple genres, differences between translated texts and non-translated texts are sufficient for a learned classifier to accurately determine if a given text is translated or original.</p><p>6 0.60726213 <a title="70-lsi-6" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>7 0.56123161 <a title="70-lsi-7" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>8 0.55604565 <a title="70-lsi-8" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>9 0.54584557 <a title="70-lsi-9" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>10 0.5449416 <a title="70-lsi-10" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>11 0.51017398 <a title="70-lsi-11" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>12 0.50745094 <a title="70-lsi-12" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>13 0.49707773 <a title="70-lsi-13" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>14 0.49482536 <a title="70-lsi-14" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>15 0.48289371 <a title="70-lsi-15" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>16 0.46947226 <a title="70-lsi-16" href="./acl-2011-Hindi_to_Punjabi_Machine_Translation_System.html">151 acl-2011-Hindi to Punjabi Machine Translation System</a></p>
<p>17 0.44733778 <a title="70-lsi-17" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>18 0.43141568 <a title="70-lsi-18" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>19 0.41738918 <a title="70-lsi-19" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>20 0.41319221 <a title="70-lsi-20" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.012), (17, 0.037), (20, 0.234), (26, 0.149), (37, 0.056), (39, 0.039), (41, 0.083), (53, 0.013), (55, 0.035), (59, 0.053), (72, 0.028), (88, 0.018), (91, 0.035), (96, 0.142)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78794396 <a title="70-lda-1" href="./acl-2011-Clustering_Comparable_Corpora_For_Bilingual_Lexicon_Extraction.html">70 acl-2011-Clustering Comparable Corpora For Bilingual Lexicon Extraction</a></p>
<p>Author: Bo Li ; Eric Gaussier ; Akiko Aizawa</p><p>Abstract: We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches.</p><p>2 0.70426971 <a title="70-lda-2" href="./acl-2011-How_do_you_pronounce_your_name%3F_Improving_G2P_with_transliterations.html">153 acl-2011-How do you pronounce your name? Improving G2P with transliterations</a></p>
<p>Author: Aditya Bhargava ; Grzegorz Kondrak</p><p>Abstract: Grapheme-to-phoneme conversion (G2P) of names is an important and challenging problem. The correct pronunciation of a name is often reflected in its transliterations, which are expressed within a different phonological inventory. We investigate the problem of using transliterations to correct errors produced by state-of-the-art G2P systems. We present a novel re-ranking approach that incorporates a variety of score and n-gram features, in order to leverage transliterations from multiple languages. Our experiments demonstrate significant accuracy improvements when re-ranking is applied to n-best lists generated by three different G2P programs.</p><p>3 0.70085055 <a title="70-lda-3" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>Author: Emmanuel Prochasson ; Pascale Fung</p><p>Abstract: We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.</p><p>4 0.70065743 <a title="70-lda-4" href="./acl-2011-Engkoo%3A_Mining_the_Web_for_Language_Learning.html">115 acl-2011-Engkoo: Mining the Web for Language Learning</a></p>
<p>Author: Matthew R. Scott ; Xiaohua Liu ; Ming Zhou ; Microsoft Engkoo Team</p><p>Abstract: This paper presents Engkoo 1, a system for exploring and learning language. It is built primarily by mining translation knowledge from billions of web pages - using the Internet to catch language in motion. Currently Engkoo is built for Chinese users who are learning English; however the technology itself is language independent and can be extended in the future. At a system level, Engkoo is an application platform that supports a multitude of NLP technologies such as cross language retrieval, alignment, sentence classification, and statistical machine translation. The data set that supports this system is primarily built from mining a massive set of bilingual terms and sentences from across the web. Specifically, web pages that contain both Chinese and English are discovered and analyzed for parallelism, extracted and formulated into clear term definitions and sample sentences. This approach allows us to build perhaps the world’s largest lexicon linking both Chinese and English together - at the same time covering the most up-to-date terms as captured by the net.</p><p>5 0.69581401 <a title="70-lda-5" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>Author: Apoorv Agarwal</p><p>Abstract: In my thesis, Ipropose to build a system that would enable extraction of social interactions from texts. To date Ihave defined a comprehensive set of social events and built a preliminary system that extracts social events from news articles. Iplan to improve the performance of my current system by incorporating semantic information. Using domain adaptation techniques, Ipropose to apply my system to a wide range of genres. By extracting linguistic constructs relevant to social interactions, I will be able to empirically analyze different kinds of linguistic constructs that people use to express social interactions. Lastly, I will attempt to make convolution kernels more scalable and interpretable.</p><p>6 0.68917882 <a title="70-lda-6" href="./acl-2011-PsychoSentiWordNet.html">253 acl-2011-PsychoSentiWordNet</a></p>
<p>7 0.68577921 <a title="70-lda-7" href="./acl-2011-Dr_Sentiment_Knows_Everything%21.html">105 acl-2011-Dr Sentiment Knows Everything!</a></p>
<p>8 0.68145299 <a title="70-lda-8" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>9 0.65455508 <a title="70-lda-9" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>10 0.63236284 <a title="70-lda-10" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>11 0.62968504 <a title="70-lda-11" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>12 0.6189695 <a title="70-lda-12" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>13 0.61854482 <a title="70-lda-13" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>14 0.61457336 <a title="70-lda-14" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<p>15 0.61191201 <a title="70-lda-15" href="./acl-2011-Jigs_and_Lures%3A_Associating_Web_Queries_with_Structured_Entities.html">181 acl-2011-Jigs and Lures: Associating Web Queries with Structured Entities</a></p>
<p>16 0.60972244 <a title="70-lda-16" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>17 0.60516226 <a title="70-lda-17" href="./acl-2011-Language-independent_compound_splitting_with_morphological_operations.html">193 acl-2011-Language-independent compound splitting with morphological operations</a></p>
<p>18 0.60288858 <a title="70-lda-18" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>19 0.6017375 <a title="70-lda-19" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>20 0.60166478 <a title="70-lda-20" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
