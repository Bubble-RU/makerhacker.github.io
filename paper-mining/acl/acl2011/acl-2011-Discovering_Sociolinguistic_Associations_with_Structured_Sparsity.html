<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-97" href="#">acl2011-97</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</h1>
<br/><p>Source: <a title="acl-2011-97-pdf" href="http://aclweb.org/anthology//P/P11/P11-1137.pdf">pdf</a></p><p>Author: Jacob Eisenstein ; Noah A. Smith ; Eric P. Xing</p><p>Abstract: We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. By imposing a composite ‘1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. Next, we conjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.</p><p>Reference: <a title="acl-2011-97-reference" href="../acl2011_reference/acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu s  Abstract We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. [sent-5, score-0.228]
</p><p>2 Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. [sent-6, score-1.068]
</p><p>3 By imposing a composite ‘1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. [sent-7, score-0.18]
</p><p>4 First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. [sent-9, score-0.838]
</p><p>5 Next, we conjoin demographic attributes into features, which we use to predict term frequencies. [sent-10, score-0.917]
</p><p>6 The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties. [sent-11, score-0.808]
</p><p>7 Quantitative sociolinguistics usually addresses this question through carefully crafted studies that correlate individual demographic attributes and linguistic variables—for example, the interaction between income and the “dropped r” feature of the New York accent (Labov, 1966). [sent-13, score-0.875]
</p><p>8 Using multi-output regression with structured sparsity, 1365 our method identifies a small subset of lexical items that are most influenced by demographics, and discovers conjunctions of demographic attributes that are especially salient for lexical variation. [sent-16, score-1.047]
</p><p>9 On the demo-  graphic side, the interaction between demographic attributes is often non-linear: for example, gender may negate or amplify class-based language differences (Zhang, 2005). [sent-19, score-0.805]
</p><p>10 Thus, additive models which assume that each demographic attribute makes a linear contribution are inadequate. [sent-20, score-0.691]
</p><p>11 In this paper, we explore the large space of potential sociolinguistic associations using structured sparsity. [sent-21, score-0.168]
</p><p>12 We treat the relationship between language and demographics as a set of multi-input, multioutput regression problems. [sent-22, score-0.41]
</p><p>13 The regression coefficients are arranged in a matrix, with rows indicating predictors and columns indicating outputs. [sent-23, score-0.341]
</p><p>14 We apply a composite regularizer that drives entire rows of the coefficient matrix to zero, yielding compact, interpretable models that reuse features across different outputs. [sent-24, score-0.313]
</p><p>15 If we treat the lexical frequencies as inputs and the author’s demographics as outputs, the induced sparsity pattern reveals the set of lexical items that is most closely tied to demographics. [sent-25, score-0.3]
</p><p>16 If we treat the demographic attributes as inputs and build a model to predict the text, we can incrementally construct a conjunctive feature space of demographic attributes, capturing key non-linear interac-  tions. [sent-26, score-1.546]
</p><p>17 c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s365–1374, The primary purpose of this research is exploratory data analysis to identify both the most linguistic-salient demographic features, and the most demographically-salient words. [sent-29, score-0.631]
</p><p>18 However, this model also enables predictions about demographic features by analyzing raw text, potentially supporting applications in targeted information extraction or advertising. [sent-30, score-0.721]
</p><p>19 On the task of predicting demographics from text, we find that our sparse model yields performance that is statistically indistinguishable from the full vocabulary, even with a reduction in the model complexity an order of magnitude. [sent-31, score-0.208]
</p><p>20 On the task of predicting text from author demographics, we find that our incrementally constructed feature set obtains significantly better perplexity than a linear model of demographic attributes. [sent-32, score-0.695]
</p><p>21 2 Data Our dataset is derived from prior work in which we gathered the text and geographical locations of 9,250 microbloggers on the website twitter . [sent-33, score-0.123]
</p><p>22 (2010) obtained aggregate demographic statistics for these data by mapping geolocations to publicly-available data from the U. [sent-43, score-0.669]
</p><p>23 The demographic attributes that we consider in this paper are shown in Table 1. [sent-47, score-0.805]
</p><p>24 The race and ethnicity attributes are not mutually exclusive—individuals can indicate any number of races or ethnicities. [sent-49, score-0.367]
</p><p>25 4 % Spanish speakers % other language speakers  14. [sent-63, score-0.156]
</p><p>26 4 18,100  Table 1: The demographic attributes used in this research. [sent-73, score-0.805]
</p><p>27 “Urban areas” refer to sets of census tracts or census blocks which contain at least 2,500 residents; our “% urban” attribute is the percentage of individ-  uals in each ZCTA who are listed as living in an urban area. [sent-75, score-0.395]
</p><p>28 While geographical aggregate statistics are frequently used to proxy for individual socioeconomic status in research areas such as public health (e. [sent-77, score-0.277]
</p><p>29 Polling research suggests that users of both Twitter (Smith and Rainie, 2010) and geolocation services (Zickuhr and Smith, 2010) are much more diverse with respect to age, gender, race and ethnicity than the general population of Internet users. [sent-81, score-0.193]
</p><p>30 Nonetheless, at present we can only use aggregate statistics to make inferences about the geographic communities in which our authors live, and not the authors themselves. [sent-82, score-0.172]
</p><p>31 3  Models  The selection of both words and demographic features can be framed in terms of multi-output regression with structured sparsity. [sent-86, score-0.913]
</p><p>32 To select the lexical indicators that best predict demographics, we construct a regression problem in which term frequencies are the predictors and demographic attributes are the outputs; to select the demographic features that predict word use, this arrangement is reversed. [sent-87, score-1.974]
</p><p>33 Through structured sparsity, we learn models in which entire sets of coefficients are driven to zero; this tells us which words and demographic features can safely be ignored. [sent-88, score-0.77]
</p><p>34 This section describes the model and implementation for output-regression with structured sparsity; in Section 4 and 5 we give the details of its application to select terms and demographic features. [sent-89, score-0.747]
</p><p>35 We would like to solve the unconstrained optimization problem, minimizeB  | |Y − XB| |2F + λR(B),  (1)  where | |A| |2F indicates the squared Frobenius norm Pi Pj ai2j, and the function R(B) defines a norm oPn Pthe regression coefficients B. [sent-94, score-0.335]
</p><p>36 Ridqge regres-  PtT=1 qPpPbp2t,  sion applies the ‘2 norm R(B) = and lasso regression applies the ‘1P normq RP(B) = |bpt|; in both cases, it is possible to dePcompoPse th|eb multi-output regression problem, otre daet--  PtT=1 PpP  ing eacPh output dimension separately. [sent-95, score-0.654]
</p><p>37 However, our working hypothesis is that there will be substantial 1367 correlations across both the vocabulary and the demographic features—for example, a demographic feature such as the percentage of Spanish speakers will predict a large set of words. [sent-96, score-1.457]
</p><p>38 Our goal is to select a small set of predictors yielding good performance across all output dimensions. [sent-97, score-0.122]
</p><p>39 Thus, we desire structured sparsity, in which entire rows of the coefficient matrix B are driven to zero. [sent-98, score-0.19]
</p><p>40 The lasso gives element-wise sparsity, in which many entries of B are driven to zero, but each predictor may have a non-zero value for some output dimension. [sent-100, score-0.305]
</p><p>41 This norm, wPhich corresponds to a multioutput lasso regression, has the desired property of driving entire rows of B to zero. [sent-104, score-0.413]
</p><p>42 If the Xnum −b Ner ¯x of predictors is too large, it may not be possible to store the dense matrix D in memory. [sent-123, score-0.144]
</p><p>43 At each λi, we solve the sparse multi-output regression; the solution Bi defines a sparse set of predictors for all tasks. [sent-136, score-0.191]
</p><p>44 We then use this limited set of predictors to construct a new input matrix which serves as the input in a standard ridge regression, thus refitting the model. [sent-137, score-0.271]
</p><p>45 The tuning set performance of this regression is the score for λi. [sent-138, score-0.161]
</p><p>46 Such post hoc refitting is often used in tandem with the lasso and related sparse methods; the effectiveness of this procedure has been demonstrated in both theory (Wasserman and Roeder, 2009) and practice (Wu et al. [sent-139, score-0.371]
</p><p>47 The regularization parameter of the ridge regression is determined by internal cross-validation. [sent-141, score-0.302]
</p><p>48 Xˆi,  4  Predicting Demographics from Text  Sparse multi-output regression can be used to select a subset of vocabulary items that are especially indicative of demographic and geographic differences. [sent-142, score-0.947]
</p><p>49 1368 Starting from the regression problem (1), the predictors X are set to the term frequencies, with one column for each word type and one row for each author in the dataset. [sent-144, score-0.314]
</p><p>50 The outputs Y are set to the ten demographic attributes described in Table 1 (we consider much larger demographic feature spaces in the next section) The ‘1,∞ regularizer will drive entire rows of the coefficient matrix B to zero, eliminating all demographic effects for many words. [sent-145, score-2.267]
</p><p>51 1 Quantitative Evaluation We evaluate the ability of lexical features to predict the demographic attributes of their authors (as proxied by the census data from the author’s geographical area). [sent-147, score-1.089]
</p><p>52 In addition, this evaluation establishes a baseline for performance on the demographic prediction task. [sent-149, score-0.631]
</p><p>53 We perform five-fold cross-validation, using the multi-output lasso to identify a sparse feature set  in the training data. [sent-150, score-0.323]
</p><p>54 We compare against several other dimensionality reduction techniques, matching the number of features obtained by the multioutput lasso at each fold. [sent-151, score-0.408]
</p><p>55 As before, we perform post hoc refitting on the training data using a standard ridge regression. [sent-155, score-0.127]
</p><p>56 The regularization constant for the ridge regression is identified using nested five-fold cross validation within the training set. [sent-156, score-0.338]
</p><p>57 Linguistic features are best at predicting race, ethnicity, language, and the proportion of renters; the other de-  mographic attributes are more difficult to predict. [sent-163, score-0.25]
</p><p>58 Among feature sets, the highest average correlation is obtained by the full vocabulary, but the multioutput lasso obtains nearly identical performance using a feature set that is an order of magnitude smaller. [sent-164, score-0.364]
</p><p>59 We find that the multi-output lasso and tthhae nf ±ull0 vocabulary regression are not significantly different on any of the attributes. [sent-170, score-0.462]
</p><p>60 Thus, the multioutput lasso achieves a 93% compression of the feature set without a significant decrease in predictive performance. [sent-171, score-0.364]
</p><p>61 The multi-output lasso yields higher correlations than the other dimensionality reduction techniques on all of the attributes; these differences are statistically significant in many—but not all— cases. [sent-172, score-0.318]
</p><p>62 1369  Recall that the regularization coefficient was chosen by nested cross-validation within the training set; the average number of features selected is 394. [sent-174, score-0.175]
</p><p>63 Computing the truncated SVD of a sparse matrix at very large truncation levels is computationally expensive, so we cannot draw the complete performance curve for this method. [sent-177, score-0.195]
</p><p>64 The multi-output lasso dominates the alternatives, obtaining a particularly strong advantage with very small feature sets. [sent-178, score-0.269]
</p><p>65 For each identified term, we apply a significance test on the relationship between the presence of each term and the demographic indicators shown in the columns of the table. [sent-182, score-0.696]
</p><p>66 The use of sparse multioutput regression for variable selection increases the power of post hoc significance testing, because the Bonferroni correction bases the threshold for statistical significance on the total number of comparisons. [sent-184, score-0.346]
</p><p>67 Table 3 shows the terms identified by our model which have a significant correlation with at least one of the demographic indicators. [sent-187, score-0.661]
</p><p>68 Standard English words tend to appear in areas with more  English speakers; predictably, Spanish words tend to appear in areas with Spanish speakers and Hispanics. [sent-192, score-0.22]
</p><p>69 , lmaoo) have a nearly uniform demographic profile, displaying negative correlations with whites and English speakers, and positive correlations with African Americans, Hispanics, renters, Spanish speakers, and areas classified as urban. [sent-196, score-0.832]
</p><p>70 , dats) appear in areas with high proportions of renters, African Americans, and non-English speakers, though a subset (haha, hahaha, and yep) display the opposite demographic pattern. [sent-199, score-0.702]
</p><p>71 5  Conjunctive Demographic Features  Next, we demonstrate how to select conjunctions of demographic features that predict text. [sent-203, score-0.75]
</p><p>72 Again, we apply multi-output regression, but now we reverse the direction of inference: the predictors are demographic features, and the outputs are term frequencies. [sent-204, score-0.751]
</p><p>73 The sparsity-inducing ‘1,∞ norm will select a subset of demographic features that explain the term frequencies. [sent-205, score-0.814]
</p><p>74 We create an initial feature set f(0) (X) by binning each demographic attribute, using five equalfrequency bins. [sent-206, score-0.631]
</p><p>75 We then constructive conjunctive features by applying a procedure inspired by related work in computational biology, called “Screen and Clean” (Wu et al. [sent-207, score-0.118]
</p><p>76 On iteration i: 1370 • Solve the sparse multi-output regression problSeomlv Ye t = ? [sent-209, score-0.215]
</p><p>77 In addition to the binned versions of the demographic attributes described in Table 1, we include geographical information. [sent-216, score-0.897]
</p><p>78 For efficiency, the outputs Y are not set to the raw term frequencies; instead we compute a truncated singular value decomposition of the term frequencies W ≈ UVDT, and use the basis U. [sent-219, score-0.196]
</p><p>79 1 Quantitative Evaluation The ability of the induced demographic features to predict text is evaluated using a traditional perplexity metric. [sent-222, score-0.778]
</p><p>80 We construct a  language model from the induced demographic features by training a multi-output ridge regression, which gives a matrix that maps from demographic features to term frequencies across the entire vocabulary. [sent-224, score-1.598]
</p><p>81 1 Table 5: language features, frequency  Word perplexity on test documents, using models estimated from induced demographic raw demographic attributes, and a relativebaseline. [sent-245, score-1.375]
</p><p>82 The language models induced from demographic data yield small but statistically significant improvements over the baseline (Wilcoxon signed-rank test, p < . [sent-250, score-0.667]
</p><p>83 Moreover, the model based on conjunctive features significantly outperforms the model constructed from raw attributes (p < . [sent-252, score-0.338]
</p><p>84 The geographical area of F2 is completely contained by F1; the associated terms are thus very similar, but by having both features, the model can distinguish terms which are used in northeastern areas outside New York City, as well as terms which are especially likely in New York. [sent-264, score-0.289]
</p><p>85 For example, F9 further refines the New York City area by focusing on communities that have relatively low numbers of Spanish speakers; F17 emphasizes New York neighborhoods that  have very high numbers of African Americans and few speakers of languages other than English and Spanish. [sent-266, score-0.192]
</p><p>86 The regression model can use these features in combination to make fine-grained distinctions about the differences between such neighborhoods. [sent-267, score-0.205]
</p><p>87 Many of these features conjoined the proportion of African Americans with geographical features, identifying local linguistic styles used predominantly in either African Amer-  ican or white communities. [sent-273, score-0.168]
</p><p>88 Conversely, F23 selects areas with very few African Americans and Spanish-speakers in the western part of the United States, and F36 selects for similar demographics in the area of Washington and Philadelphia. [sent-275, score-0.261]
</p><p>89 Other features differentiate between African Americans and Hispanics: F8 identifies regions with many Spanish speakers and Hispanics, but few African Americans; F20 identifies regions with both Spanish speakers and whites, but few African Americans. [sent-278, score-0.268]
</p><p>90 While race, geography, and language predominate, the socioeconomic attributes appear in far fewer features. [sent-280, score-0.222]
</p><p>91 The most prevalent attribute is the proportion of renters, which appears in F4 and F7, and in three other features not shown here. [sent-281, score-0.136]
</p><p>92 This attribute may be a better indicator of the urban/rural divide than the “% urban” attribute, which has a very low threshold for what counts as urban (see Table 1). [sent-282, score-0.171]
</p><p>93 Overall, the selected features tend to include attributes that are easy to predict from text (compare with Table 2). [sent-284, score-0.254]
</p><p>94 Logistic regression has been used to identify relationships between demographic features and linguistic variables since the 1970s (Cedergren and Sankoff, 1974). [sent-286, score-0.836]
</p><p>95 More recent developments include the use of mixed factor models to account for idiosyncrasies of individual speakers (Johnson, 2009), as well as clustering and multidimensional scaling (Nerbonne, 2009) to en-  able aggregate inference across multiple linguistic variables. [sent-287, score-0.116]
</p><p>96 However, all of these approaches assume that both the linguistic indicators and demographic attributes have already been identified by the researcher. [sent-288, score-0.833]
</p><p>97 (2010) applies a similar generative model to demographic data. [sent-301, score-0.631]
</p><p>98 The model presented here differs in two key ways: first, we use sparsity-inducing regu-  larization to perform variable selection; second, we eschew high-dimensional mixture models in favor of a bottom-up approach of building conjunctions of demographic and geographic attributes. [sent-302, score-0.783]
</p><p>99 In a mixture model, each component must define a distribution over all demographic variables, which may be difficult to estimate in a high-dimensional setting. [sent-303, score-0.663]
</p><p>100 7  Conclusion  This paper demonstrates how regression with structured sparsity can be applied to select words and conjunctive demographic features that reveal sociolinguistic associations. [sent-308, score-1.131]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('demographic', 0.631), ('lasso', 0.269), ('african', 0.217), ('americans', 0.206), ('attributes', 0.174), ('regression', 0.161), ('demographics', 0.154), ('census', 0.112), ('renters', 0.111), ('urban', 0.111), ('ethnicity', 0.098), ('hispanics', 0.095), ('multioutput', 0.095), ('race', 0.095), ('geographical', 0.092), ('geographic', 0.084), ('predictors', 0.083), ('hispanic', 0.079), ('ridge', 0.079), ('speakers', 0.078), ('sparsity', 0.075), ('conjunctive', 0.074), ('areas', 0.071), ('income', 0.07), ('spanish', 0.067), ('eisenstein', 0.065), ('norm', 0.063), ('regularization', 0.062), ('matrix', 0.061), ('associations', 0.061), ('sociolinguistic', 0.06), ('attribute', 0.06), ('regularizer', 0.057), ('sparse', 0.054), ('communities', 0.05), ('correlations', 0.049), ('rows', 0.049), ('coefficients', 0.048), ('connor', 0.048), ('bonferroni', 0.048), ('refitting', 0.048), ('socioeconomic', 0.048), ('turlach', 0.048), ('structured', 0.047), ('raw', 0.046), ('features', 0.044), ('kathryn', 0.042), ('wasserman', 0.042), ('glossary', 0.042), ('vernacular', 0.042), ('truncated', 0.041), ('select', 0.039), ('truncation', 0.039), ('conjoin', 0.039), ('aggregate', 0.038), ('term', 0.037), ('nested', 0.036), ('predictor', 0.036), ('geography', 0.036), ('area', 0.036), ('predict', 0.036), ('variable', 0.036), ('induced', 0.036), ('composite', 0.036), ('frequencies', 0.035), ('identifies', 0.034), ('coefficient', 0.033), ('author', 0.033), ('west', 0.033), ('interpretable', 0.033), ('vocabulary', 0.032), ('mixture', 0.032), ('york', 0.032), ('compact', 0.032), ('blockwise', 0.032), ('bpt', 0.032), ('cedergren', 0.032), ('coast', 0.032), ('dats', 0.032), ('ptt', 0.032), ('quattoni', 0.032), ('rushton', 0.032), ('whites', 0.032), ('zcta', 0.032), ('zip', 0.032), ('proportion', 0.032), ('twitter', 0.031), ('perplexity', 0.031), ('brendan', 0.031), ('quantitative', 0.03), ('terms', 0.03), ('indicators', 0.028), ('proxy', 0.028), ('zickuhr', 0.028), ('compass', 0.028), ('duchi', 0.028), ('emphasizes', 0.028), ('geotagged', 0.028), ('residents', 0.028), ('xtx', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="97-tfidf-1" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>Author: Jacob Eisenstein ; Noah A. Smith ; Eric P. Xing</p><p>Abstract: We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. By imposing a composite ‘1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. Next, we conjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.</p><p>2 0.067857496 <a title="97-tfidf-2" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>Author: Sara Rosenthal ; Kathleen McKeown</p><p>Abstract: We investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. Our hypothesis is that significant changes in writing style distinguish pre-social media bloggers from post-social media bloggers. Through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as AIM, SMS text messaging, MySpace and Facebook first became popular, enable accurate age prediction. We also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce significantly more accurate results. Our best results allow for 81.57% accuracy.</p><p>3 0.067302577 <a title="97-tfidf-3" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>Author: Benjamin Wing ; Jason Baldridge</p><p>Abstract: We investigate automatic geolocation (i.e. identification of the location, expressed as latitude/longitude coordinates) of documents. Geolocation can be an effective means of summarizing large document collections and it is an important component of geographic information retrieval. We describe several simple supervised methods for document geolocation using only the document’s raw text as evidence. All of our methods predict locations in the context of geodesic grids of varying degrees of resolution. We evaluate the methods on geotagged Wikipedia articles and Twitter feeds. For Wikipedia, our best method obtains a median prediction error of just 11.8 kilometers. Twitter geolocation is more challenging: we obtain a median error of 479 km, an improvement on previous results for the dataset.</p><p>4 0.060759865 <a title="97-tfidf-4" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>5 0.059433211 <a title="97-tfidf-5" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>Author: Christina Sauper ; Aria Haghighi ; Regina Barzilay</p><p>Abstract: We present a probabilistic topic model for jointly identifying properties and attributes of social media review snippets. Our model simultaneously learns a set of properties of a product and captures aggregate user sentiments towards these properties. This approach directly enables discovery of highly rated or inconsistent properties of a product. Our model admits an efficient variational meanfield inference algorithm which can be parallelized and run on large snippet collections. We evaluate our model on a large corpus of snippets from Yelp reviews to assess property and attribute prediction. We demonstrate that it outperforms applicable baselines by a considerable margin.</p><p>6 0.059098795 <a title="97-tfidf-6" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>7 0.059038304 <a title="97-tfidf-7" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>8 0.057852101 <a title="97-tfidf-8" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>9 0.050121885 <a title="97-tfidf-9" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>10 0.048546277 <a title="97-tfidf-10" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>11 0.045212016 <a title="97-tfidf-11" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>12 0.044994511 <a title="97-tfidf-12" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>13 0.04431827 <a title="97-tfidf-13" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>14 0.043204244 <a title="97-tfidf-14" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>15 0.042693432 <a title="97-tfidf-15" href="./acl-2011-A_New_Dataset_and_Method_for_Automatically_Grading_ESOL_Texts.html">20 acl-2011-A New Dataset and Method for Automatically Grading ESOL Texts</a></p>
<p>16 0.042505164 <a title="97-tfidf-16" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>17 0.042457338 <a title="97-tfidf-17" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>18 0.041828249 <a title="97-tfidf-18" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>19 0.041809268 <a title="97-tfidf-19" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>20 0.041555613 <a title="97-tfidf-20" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.124), (1, 0.031), (2, -0.014), (3, 0.006), (4, -0.027), (5, -0.002), (6, 0.027), (7, -0.011), (8, -0.012), (9, 0.042), (10, -0.044), (11, -0.007), (12, 0.018), (13, 0.035), (14, -0.027), (15, -0.017), (16, -0.029), (17, -0.001), (18, -0.013), (19, -0.057), (20, 0.062), (21, -0.018), (22, -0.013), (23, 0.024), (24, -0.035), (25, -0.023), (26, -0.007), (27, -0.011), (28, -0.033), (29, -0.001), (30, 0.027), (31, 0.008), (32, 0.004), (33, 0.08), (34, 0.05), (35, 0.001), (36, -0.023), (37, 0.013), (38, -0.01), (39, 0.052), (40, 0.108), (41, -0.0), (42, 0.111), (43, -0.067), (44, 0.056), (45, 0.005), (46, 0.0), (47, 0.022), (48, 0.023), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89489847 <a title="97-lsi-1" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>Author: Jacob Eisenstein ; Noah A. Smith ; Eric P. Xing</p><p>Abstract: We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. By imposing a composite ‘1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. Next, we conjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.</p><p>2 0.68148822 <a title="97-lsi-2" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>Author: Sara Rosenthal ; Kathleen McKeown</p><p>Abstract: We investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. Our hypothesis is that significant changes in writing style distinguish pre-social media bloggers from post-social media bloggers. Through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as AIM, SMS text messaging, MySpace and Facebook first became popular, enable accurate age prediction. We also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce significantly more accurate results. Our best results allow for 81.57% accuracy.</p><p>3 0.63451213 <a title="97-lsi-3" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>Author: Philip Bramsen ; Martha Escobar-Molano ; Ami Patel ; Rafael Alonso</p><p>Abstract: Sociolinguists have long argued that social context influences language use in all manner of ways, resulting in lects 1. This paper explores a text classification problem we will call lect modeling, an example of what has been termed computational sociolinguistics. In particular, we use machine learning techniques to identify social power relationships between members of a social network, based purely on the content of their interpersonal communication. We rely on statistical methods, as opposed to language-specific engineering, to extract features which represent vocabulary and grammar usage indicative of social power lect. We then apply support vector machines to model the social power lects representing superior-subordinate communication in the Enron email corpus. Our results validate the treatment of lect modeling as a text classification problem – albeit a hard one – and constitute a case for future research in computational sociolinguistics. 1</p><p>4 0.60494179 <a title="97-lsi-4" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<p>Author: Aaron Michelony</p><p>Abstract: We consider the problem of predicting which words a student will click in a vocabulary learning system. Often a language learner will find value in the ability to look up the meaning of an unknown word while reading an electronic document by clicking the word. Highlighting words likely to be unknown to a readeris attractive due to drawing his orher attention to it and indicating that information is available. However, this option is usually done manually in vocabulary systems and online encyclopedias such as Wikipedia. Furthurmore, it is never on a per-user basis. This paper presents an automated way of highlighting words likely to be unknown to the specific user. We present related work in search engine ranking, a description of the study used to collect click data, the experiment we performed using the random forest machine learning algorithm and finish with a discussion of future work.</p><p>5 0.58818507 <a title="97-lsi-5" href="./acl-2011-A_New_Dataset_and_Method_for_Automatically_Grading_ESOL_Texts.html">20 acl-2011-A New Dataset and Method for Automatically Grading ESOL Texts</a></p>
<p>Author: Helen Yannakoudakis ; Ted Briscoe ; Ben Medlock</p><p>Abstract: We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.</p><p>6 0.58062428 <a title="97-lsi-6" href="./acl-2011-Automatically_Predicting_Peer-Review_Helpfulness.html">55 acl-2011-Automatically Predicting Peer-Review Helpfulness</a></p>
<p>7 0.57981122 <a title="97-lsi-7" href="./acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution.html">212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</a></p>
<p>8 0.56925488 <a title="97-lsi-8" href="./acl-2011-A_Scalable_Probabilistic_Classifier_for_Language_Modeling.html">24 acl-2011-A Scalable Probabilistic Classifier for Language Modeling</a></p>
<p>9 0.56849778 <a title="97-lsi-9" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>10 0.56263465 <a title="97-lsi-10" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>11 0.54882741 <a title="97-lsi-11" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>12 0.54207355 <a title="97-lsi-12" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>13 0.53316402 <a title="97-lsi-13" href="./acl-2011-Towards_Style_Transformation_from_Written-Style_to_Audio-Style.html">306 acl-2011-Towards Style Transformation from Written-Style to Audio-Style</a></p>
<p>14 0.53159022 <a title="97-lsi-14" href="./acl-2011-Confidence-Weighted_Learning_of_Factored_Discriminative_Language_Models.html">78 acl-2011-Confidence-Weighted Learning of Factored Discriminative Language Models</a></p>
<p>15 0.53138846 <a title="97-lsi-15" href="./acl-2011-Lost_in_Translation%3A_Authorship_Attribution_using_Frame_Semantics.html">214 acl-2011-Lost in Translation: Authorship Attribution using Frame Semantics</a></p>
<p>16 0.53123254 <a title="97-lsi-16" href="./acl-2011-Modeling_Wisdom_of_Crowds_Using_Latent_Mixture_of_Discriminative_Experts.html">223 acl-2011-Modeling Wisdom of Crowds Using Latent Mixture of Discriminative Experts</a></p>
<p>17 0.52955794 <a title="97-lsi-17" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>18 0.52752572 <a title="97-lsi-18" href="./acl-2011-Subjective_Natural_Language_Problems%3A_Motivations%2C_Applications%2C_Characterizations%2C_and_Implications.html">288 acl-2011-Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications</a></p>
<p>19 0.51443815 <a title="97-lsi-19" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<p>20 0.51164341 <a title="97-lsi-20" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.031), (17, 0.04), (26, 0.018), (37, 0.065), (39, 0.428), (41, 0.064), (55, 0.022), (59, 0.036), (72, 0.023), (91, 0.038), (96, 0.117), (97, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94778097 <a title="97-lda-1" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>Author: Yoav Goldberg ; Michael Elhadad</p><p>Abstract: We experiment with extending a lattice parsing methodology for parsing Hebrew (Goldberg and Tsarfaty, 2008; Golderg et al., 2009) to make use of a stronger syntactic model: the PCFG-LA Berkeley Parser. We show that the methodology is very effective: using a small training set of about 5500 trees, we construct a parser which parses and segments unsegmented Hebrew text with an F-score of almost 80%, an error reduction of over 20% over the best previous result for this task. This result indicates that lattice parsing with the Berkeley parser is an effective methodology for parsing over uncertain inputs.</p><p>2 0.93955004 <a title="97-lda-2" href="./acl-2011-%2811-06-spirl%29.html">1 acl-2011-(11-06-spirl)</a></p>
<p>Author: (hal)</p><p>Abstract: unkown-abstract</p><p>3 0.92530382 <a title="97-lda-3" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>Author: Jey Han Lau ; Karl Grieser ; David Newman ; Timothy Baldwin</p><p>Abstract: We propose a method for automatically labelling topics learned via LDA topic models. We generate our label candidate set from the top-ranking topic terms, titles of Wikipedia articles containing the top-ranking topic terms, and sub-phrases extracted from the Wikipedia article titles. We rank the label candidates using a combination of association measures and lexical features, optionally fed into a supervised ranking model. Our method is shown to perform strongly over four independent sets of topics, significantly better than a benchmark method.</p><p>4 0.92181551 <a title="97-lda-4" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>Author: Muhua Zhu ; Jingbo Zhu ; Minghan Hu</p><p>Abstract: For the task of automatic treebank conversion, this paper presents a feature-based approach which encodes bracketing structures in a treebank into features to guide the conversion of this treebank to a different standard. Experiments on two Chinese treebanks show that our approach improves conversion accuracy by 1.31% over a strong baseline.</p><p>same-paper 5 0.87613988 <a title="97-lda-5" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>Author: Jacob Eisenstein ; Noah A. Smith ; Eric P. Xing</p><p>Abstract: We present a method to discover robust and interpretable sociolinguistic associations from raw geotagged text data. Using aggregate demographic statistics about the authors’ geographic communities, we solve a multi-output regression problem between demographics and lexical frequencies. By imposing a composite ‘1,∞ regularizer, we obtain structured sparsity, driving entire rows of coefficients to zero. We perform two regression studies. First, we use term frequencies to predict demographic attributes; our method identifies a compact set of words that are strongly associated with author demographics. Next, we conjoin demographic attributes into features, which we use to predict term frequencies. The composite regularizer identifies a small number of features, which correspond to communities of authors united by shared demographic and linguistic properties.</p><p>6 0.82374185 <a title="97-lda-6" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>7 0.80567938 <a title="97-lda-7" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>8 0.77300519 <a title="97-lda-8" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>9 0.68656611 <a title="97-lda-9" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>10 0.63408667 <a title="97-lda-10" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>11 0.61762232 <a title="97-lda-11" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>12 0.61756957 <a title="97-lda-12" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>13 0.61646736 <a title="97-lda-13" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>14 0.61471748 <a title="97-lda-14" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>15 0.61248577 <a title="97-lda-15" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>16 0.61156309 <a title="97-lda-16" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>17 0.60357636 <a title="97-lda-17" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>18 0.60176408 <a title="97-lda-18" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>19 0.59109229 <a title="97-lda-19" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>20 0.59014428 <a title="97-lda-20" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
