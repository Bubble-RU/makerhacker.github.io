<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-155" href="#">acl2011-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2011-155-pdf" href="http://aclweb.org/anthology//P/P11/P11-1126.pdf">pdf</a></p><p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks.</p><p>Reference: <a title="acl-2011-155-reference" href="../acl2011_reference/acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. [sent-2, score-1.801]
</p><p>2 Few assumptions are made by our approach about the underlying component  systems, enabling us to leverage SMT models based on arbitrary paradigms. [sent-4, score-0.16]
</p><p>3 We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks. [sent-5, score-0.137]
</p><p>4 , 2008; Chiang 2010), many researchers have concentrated on the approaches that improve translation quality using information between hypotheses from one or more SMT systems as well. [sent-8, score-0.378]
</p><p>5 System combination is built on top of the N-best outputs generated by multiple component systems (Rosti et al. [sent-9, score-0.396]
</p><p>6 , 2009b) which aligns multiple hypotheses to build confusion networks as new search spaces, and outputs 1258 Mu Li, and Ming Zhou Natural Language Computing Group Microsoft Research Asia Beijing, China { mul i mingzhou } @mi cros o ft . [sent-12, score-0.426]
</p><p>7 Because hypotheses generated by a single model are highly correlated, improvements obtained are usually small; recently, dedicated efforts have been made to extend it from single system to multiple systems (Li et al. [sent-18, score-0.414]
</p><p>8 Such methods select translations by optimizing consensus models over the combined hypotheses using all component systems’ posterior distributions. [sent-22, score-0.734]
</p><p>9 Although these two types of approaches have shown consistent improvements over the standard Maximum a Posteriori (MAP) decoding scheme, most of them are implemented as post-processing procedures over translations generated by MAP decoders. [sent-23, score-0.677]
</p><p>10 (2009a) is different in that both partial and full hypotheses are re-ranked during the decoding phase directly using consensus between translations from differ-  ent SMT systems. [sent-25, score-0.998]
</p><p>11 However, their method does not change component systems’ search spaces. [sent-26, score-0.247]
</p><p>12 This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple component systems. [sent-27, score-1.85]
</p><p>13 92, 1]  中中国国 的的 经经济济 发发展展 Figure 1: A decoding example of a phrase-based SMT system. [sent-37, score-0.5]
</p><p>14 Each hypothesis is annotated with a feature vector, which includes a logarithmic probabil-  ity feature and a word count feature. [sent-38, score-0.188]
</p><p>15 theses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of component model independent features are used to seek the final best translation from this new constructed search space. [sent-39, score-0.76]
</p><p>16 We evaluate by combining two SMT models with state-of-the-art performances on the NIST Chinese-to-English translation tasks. [sent-40, score-0.148]
</p><p>17 Experimental results show that our approach outperforms the best component SMT system by up to 2. [sent-41, score-0.189]
</p><p>18 Consistent improvements can be observed over several related decoding techniques as well, including word-level system combination, collaborative decoding and model combination. [sent-43, score-1.09]
</p><p>19 Motivated by the success of system combination research, the key contribution of this work is to make more effective use of the extended search spaces from different SMT models in decoding phase directly, rather than just post-processing their final outputs. [sent-46, score-0.797]
</p><p>20 1 However, hypotheses generated by different SMT systems cannot be combined directly to form new translations because of two major issues: The first one is the heterogeneous structures of  . [sent-49, score-0.418]
</p><p>21 For example, a string-totree system cannot use hypotheses generated by a phrase-based system in decoding procedure, as such hypotheses are based on flat structures, which cannot provide any additional information needed in the syntactic model. [sent-51, score-1.104]
</p><p>22 The second one is the incompatible feature spaces of different SMT models. [sent-52, score-0.143]
</p><p>23 To address these two issues discussed above, we propose HM decoding that performs translation reconstruction using hypotheses generated by multiple component systems. [sent-54, score-1.154]
</p><p>24 2 Our method involves two decoding stages depicted as follows: 1. [sent-55, score-0.52]
</p><p>25 Independent decoding stage, in which each component system decodes input sentences independently based on its own model and search algorithm, and the explored search spaces (translation forests) are kept for use in  the next stage. [sent-56, score-1.021]
</p><p>26 1 There are also features independent of translation derivations, such as the language model feature. [sent-57, score-0.159]
</p><p>27 As a result, any phrase-based SMT system can be used as a component in our HM decoding method. [sent-60, score-0.689]
</p><p>28 Hypotheses light-shaded come from a phrase-based system, and hypotheses darkshaded come from a syntax-based system. [sent-62, score-0.24]
</p><p>29 HM decoding can use lexicalized hypotheses of arbitrary SMT models to derive translation, and a set of component model independent features are used to compute translation confidence. [sent-65, score-1.079]
</p><p>30 We discuss mixture search space construction, details of model and feature designs as well as HM decoding algorithms in Section 2. [sent-66, score-0.816]
</p><p>31 2 Let  Mixture Search Space Construction  denote component MT systems, denote the span of a source sentence starting at position and ending at position . [sent-71, score-0.203]
</p><p>32 of  denoting the search space predicted by , and denoting the mixture search space constructed by the HM decoder, which is defined recursively as follows:  This rule adds all component systems’ search spaces into the mixture search space for use in HM decoding. [sent-75, score-1.067]
</p><p>33 Thus hypotheses produced by all component systems are still available to the HM decoder. [sent-76, score-0.427]
</p><p>34 of  1260    in which  and is a translation rule provided by HM decoder that composes a new hypothesis using smaller hypotheses in the search spaces These rules further extend with hypotheses generated by the HM decoder itself. [sent-77, score-1.17]
</p><p>35 Figure 2 shows an example of HM decoding, in which hypotheses generated by two SMT systems are used together to compose new translations. [sent-78, score-0.371]
</p><p>36 Since search space pruning is the indispensable procedure for all SMT systems, we will omit its  explicit expression in the following descriptions and algorithms for convenience. [sent-79, score-0.137]
</p><p>37 where is an HM decoding feature with its corresponding feature weight In this paper, the HM decoder does not assume the availability of any internal knowledge of the underlying component systems. [sent-83, score-0.844]
</p><p>38 The HM decoding features are independent of component models as well, which fall into two categories: The first category contains a set of consensusbased features, which are inspired by the success of consensus decoding approaches. [sent-84, score-1.341]
</p><p>39 These features are described in details as follows:  1)  : n-gram the  posterior feature of computed based on the component search space generated by  :  2)  , , ,  is  the posterior probability of an n-gram in is the number of times that occurs in equals to 1 when occurs in and 0 otherwise. [sent-85, score-0.749]
</p><p>40 the stemmed n-gram posterior feature of computed based on the stemmed component search space A word stem dictionary that includes 22,660 entries is used to convert and into their stem forms and by replacing each word into its stem form. [sent-86, score-0.745]
</p><p>41 3)  : n-gram posterior the  feature of computed based on the mixture search space generated by the HM decoder:  , . [sent-89, score-0.526]
</p><p>42 Consensus features based on component search spaces have already shown effectiveness (Kumar et al. [sent-91, score-0.39]
</p><p>43 We leverage consensus features based on the mixture search space newly generated in HM decoding as well. [sent-95, score-1.061]
</p><p>44 Although there are more features that can be incorporated into HM decoding besides the ones we list below, we only utilize the most representative ones for convenience:  1) 2) 3)  : word count feature. [sent-98, score-0.575]
</p><p>45 1261  : the dictionary-based feature that counts how many lexicon pairs can be found in a given translation pair 4) and : reordering features that penalize the uses of straight and inverted BTG rules during the derivation of in HM decoding. [sent-101, score-0.425]
</p><p>46 These two features are specific to BTG-based HM decoding (Section 2. [sent-102, score-0.548]
</p><p>47 1):  5)  and : reordering features that penalize the uses of hierarchical and glue rules during the derivation of in HM decoding. [sent-104, score-0.271]
</p><p>48 These two features are specific to SCFG-based HM decoding (Section 2. [sent-105, score-0.548]
</p><p>49 2):  ,  is the hierarchical rule set provided by the HM decoder itself, equals to 1 when is provided by and 0 otherwise. [sent-107, score-0.198]
</p><p>50 6) the feature that counts how many n-grams in are newly generated by the HM decoder, which cannot be found in all existing component search spaces:  :  ,  equals to 1 when does not exist in and 0 otherwise. [sent-108, score-0.437]
</p><p>51 The MERT algorithm (Och, 2003) is used to tune weights of HM decoding features. [sent-109, score-0.5]
</p><p>52 4 Decoding Algorithms Two CKY-style algorithms for HM decoding are presented in this subsection. [sent-111, score-0.5]
</p><p>53 1 BTG-based HM Decoding The first algorithm, BTG-HMD, is presented in Algorithm 1, where hypotheses of two consecutive source spans are composed using two BTG rules:     Straight rule . [sent-115, score-0.313]
</p><p>54 Inverted rule It combines translations of two consecutive blocks into a single larger block in an inverted order. [sent-118, score-0.163]
</p><p>55 We use  two reordering rule penalty features,  and  to penalize the uses of these two rules. [sent-120, score-0.198]
</p><p>56 Algorithm 1: BTG-based HM Decoding 1: 2: 3: 4: 5: 6: 7:  8: 9: 10:  11: 12:  13:  14:  15:  16:  17:  18: 19: 20: 21: 22:  for each component model output the search space end for for to do for all s. [sent-121, score-0.329]
</p><p>57 1262  as well, and add them to From line 17 to 20, we update current HM decoding scores for all hypotheses in using the n-gram and length posterior features computed  on  . [sent-127, score-0.974]
</p><p>58 Two reordering rule penalty features, and are used to adjust the preferences of using hierarchical rules and glue rules. [sent-134, score-0.267]
</p><p>59 Algorithm 2: SCFG-based HM Decoding 1: 2: 3: 4: 5:  6: 7:  8:  9: 10: 11:  12: 13:  14: 15:  16: 17: 18:  for each component model output the search space end for for to do for all s. [sent-135, score-0.329]
</p><p>60 , 2010) is an approach that selects translations from a conjoint search space using information from multiple SMT component models; Duan et al. [sent-140, score-0.408]
</p><p>61 (2010) presents a similar method, which utilizes a mixture model to combine distributions of hypotheses from different systems for Bayes-risk computation, and selects final translations from the combined search spaces using MBR decoding. [sent-141, score-0.685]
</p><p>62 In contrast, by reusing hypotheses generated by all component systems in HM decoding, translations beyond any existing search space can be generated. [sent-143, score-0.715]
</p><p>63 (2009a) proposes collaborative decoding, an approach that combines translation systems by re-ranking partial and full translations iteratively using n-gram features from the predictions of other member systems. [sent-146, score-0.382]
</p><p>64 However, in co-decoding, all member systems must work in a synchronous way, and hypotheses between different systems cannot be shared during decoding procedure; Liu et al. [sent-147, score-0.83]
</p><p>65 (2009) proposes joint-decoding, in which multiple SMT models are combined in either translation or derivation levels. [sent-148, score-0.159]
</p><p>66 HM decoding, on the other hand, can use hypotheses from component search spaces directly without any restriction. [sent-150, score-0.582]
</p><p>67 This method uses the  system combination technique in decoding directly to combine partial hypotheses from different SMT models. [sent-154, score-0.875]
</p><p>68 What’s more, partial hypotheses generated by confusion network decoding cannot be assigned exact feature values for future use in higher level decoding, and they only use feature values of 1-best hypothesis as an approximation. [sent-156, score-1.029]
</p><p>69 HM decoding, on the other hand, leverages a set of enriched features, which 1263 are computable for all the hypotheses generated by either component systems or the HM decoder. [sent-157, score-0.493]
</p><p>70 , 2002), which compute the brevity penalty using the shortest reference translation for each segment. [sent-167, score-0.173]
</p><p>71 2 Component Systems For convenience of comparing HM decoding with several related decoding techniques, we include two state-of-the-art SMT systems as component  systems only:   PB. [sent-171, score-1.24]
</p><p>72 Phrasal rules are extracted on all bilingual data, hierarchical rules used in DHPB and reordering rules used in SCFG-HMD are extracted from a selected data set3. [sent-178, score-0.218]
</p><p>73 3 Contrastive Techniques We compare HM decoding with three multiplesystem based decoding techniques:  Word-Level System Combination (SC). [sent-182, score-1.0]
</p><p>74 4 Comparison to Component Systems We compared HM decoding with two component SMT systems first (in Table 2). [sent-195, score-0.687]
</p><p>75 HMD or SCFG-HMD; 4 count features for newly generated n-grams in HM decoding for All n-gram posteriors are computed using the efficient algorithm proposed by Kumar et al. [sent-197, score-0.715]
</p><p>76 Ty6s5409t28e3*m  decoding (*: significantly better than each component system with < 0. [sent-204, score-0.689]
</p><p>77 01)  From table 2 we can see, both BTG-HMD and SCFG-HMD outperform decoding results of the best component system (DHPB) with significant improvements: +1. [sent-205, score-0.689]
</p><p>78 We think the potential reason is that more reordering rules are used in SCFG-HMD to handle phrase movements than BTG-HMD do; however, current HM decoding model lacks the ability to distinguish the qualities of different rules. [sent-213, score-0.614]
</p><p>79 8 n-gram posterior features based on 2 component search spaces plus 3 commonly used features (1 LM feature, 1 word count feature and 1 dictionary-based feature). [sent-216, score-0.629]
</p><p>80 8 stemmed n-gram posterior features based on 2 stemmed component search spaces. [sent-218, score-0.577]
</p><p>81 4 n-gram posterior features and 1 length posterior feature based on the mixture search space of the HM decoder. [sent-220, score-0.596]
</p><p>82 4 count features for unseen n-grams generated by HM decoder itself. [sent-224, score-0.229]
</p><p>83 Except for the dictionary-based feature, all the features contained in Set-1 are used by the latest multiple-system based consensus decoding techniques (DeNero et al. [sent-225, score-0.701]
</p><p>84 Each time, we add one more feature set and describe the changes of performances by drawing two curves for each HM decoding algorithm on MT08 in Figure 3. [sent-229, score-0.604]
</p><p>85 5 Comparison to System Combination Word-level system combination is state-of-the-art method to improve translation performance using outputs generated by multiple SMT systems. [sent-234, score-0.349]
</p><p>86 In this paper, we compare our HM decoding with the combination method proposed by Li et al. [sent-235, score-0.566]
</p><p>87 We think the potential reason for these improvements is that, system combination can only use a small portion of the component systems’ search spaces; HM decoding, on the other hand, can make full use of the entire translation spaces of all component systems. [sent-245, score-0.734]
</p><p>88 6 Comparison to Consensus Decoding Consensus decoding is another decoding technique that motivates our approach. [sent-247, score-1.0]
</p><p>89 We compare our HM decoding with two latest multiple-system based consensus decoding approaches, co-decoding and model combination. [sent-248, score-1.153]
</p><p>90 We list the comparison results in Table 4, in which CD-PB and CD-DHPB denote the translation results of two member systems in co-decoding respectively, CD-Comb denotes the results of further combination using outputs of CD-PB and CD-DHPB, MC denotes the results of model combination. [sent-249, score-0.291]
</p><p>91 nificantly better than the best result of consensus decoding methods with < 0. [sent-250, score-0.661]
</p><p>92 7  System Combination over BTG-HMD and SCFG-HMD Outputs As BTG-HMD and SCFG-HMD are based on two different decoding grammars, we could perform system combination over the outputs of these two settings (SCBTG+SCFG) for further improvements as well, just as Li et al. [sent-254, score-0.672]
</p><p>93 065ts928+of  BTG-HMD and SCFG-HMD (+: significantly better than the best HM decoding algorithm (SCFG-HMD) with < 0. [sent-261, score-0.5]
</p><p>94 05) After system combination, translation results are significantly better than all decoding approaches investigated in this paper: up to 2. [sent-262, score-0.64]
</p><p>95 11 BLEU points over the best component system (DHPB), up to 1. [sent-263, score-0.213]
</p><p>96 8  Evaluation of Oracle Translations  In the last part, we evaluate the quality of oracle translations on the n-best lists generated by HM decoding and all decoding approaches discussed in this paper. [sent-268, score-1.184]
</p><p>97 (2007), and each decoding approach  outputs its 1000-best hypotheses, which are used to extract oracle translations. [sent-270, score-0.584]
</p><p>98 significantly better than the best multiple-system based decoding method (CD-Comb) with < 0. [sent-271, score-0.5]
</p><p>99 5  Conclusion  In this paper, we have presented the hypothesis mixture decoding approach to combine multiple SMT models, in which hypotheses generated by multiple component systems are used to compose new translations. [sent-273, score-1.279]
</p><p>100 HM decoding method integrates 1266 the advantages of both system combination and consensus decoding techniques into a unified framework. [sent-274, score-1.228]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hm', 0.605), ('decoding', 0.5), ('hypotheses', 0.24), ('smt', 0.179), ('component', 0.16), ('consensus', 0.133), ('mixture', 0.131), ('posterior', 0.116), ('translation', 0.111), ('spaces', 0.095), ('decoder', 0.088), ('search', 0.087), ('translations', 0.085), ('stemmed', 0.083), ('reordering', 0.076), ('economic', 0.074), ('duan', 0.07), ('generated', 0.066), ('combination', 0.066), ('btg', 0.065), ('hypothesis', 0.065), ('dhpb', 0.064), ('kumar', 0.064), ('denero', 0.063), ('bleu', 0.061), ('china', 0.056), ('rule', 0.052), ('outputs', 0.051), ('space', 0.05), ('growth', 0.049), ('scbtg', 0.048), ('features', 0.048), ('li', 0.048), ('feature', 0.048), ('newly', 0.046), ('mc', 0.043), ('decodes', 0.043), ('penalty', 0.042), ('nist', 0.041), ('ming', 0.041), ('mu', 0.04), ('partial', 0.04), ('composing', 0.039), ('scfg', 0.039), ('compose', 0.038), ('rules', 0.038), ('performances', 0.037), ('member', 0.036), ('shankar', 0.036), ('collaborative', 0.035), ('dongdong', 0.035), ('oracle', 0.033), ('scfghmd', 0.032), ('economy', 0.032), ('reconstruction', 0.032), ('end', 0.032), ('glue', 0.031), ('equals', 0.03), ('stem', 0.03), ('system', 0.029), ('forests', 0.029), ('nificantly', 0.028), ('tianjin', 0.028), ('computed', 0.028), ('hierarchical', 0.028), ('penalize', 0.028), ('straight', 0.028), ('nan', 0.028), ('chiang', 0.028), ('count', 0.027), ('systems', 0.027), ('convenience', 0.026), ('inverted', 0.026), ('improvements', 0.026), ('tromble', 0.026), ('multiple', 0.026), ('mbr', 0.025), ('cui', 0.025), ('rosti', 0.025), ('points', 0.024), ('xiaodong', 0.023), ('update', 0.023), ('minimum', 0.023), ('franz', 0.023), ('derivation', 0.022), ('decoders', 0.022), ('span', 0.022), ('confusion', 0.022), ('mt', 0.021), ('paradigms', 0.021), ('seek', 0.021), ('source', 0.021), ('stages', 0.02), ('kept', 0.02), ('compute', 0.02), ('covered', 0.02), ('final', 0.02), ('latest', 0.02), ('performs', 0.019), ('add', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="155-tfidf-1" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks.</p><p>2 0.25321683 <a title="155-tfidf-2" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Alfons Juan ; Francisco Casacuberta</p><p>Abstract: We present minimum Bayes-risk system combination, a method that integrates consensus decoding and system combination into a unified multi-system minimum Bayes-risk (MBR) technique. Unlike other MBR methods that re-rank translations of a single SMT system, MBR system combination uses the MBR decision rule and a linear combination of the component systems’ probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary. We introduce expected BLEU, an approximation to the BLEU score that allows to efficiently apply MBR in these conditions. MBR system combination is a general method that is independent of specific SMT models, enabling us to combine systems with heterogeneous structure. Experiments show that our approach bring significant improvements to single-system-based MBR decoding and achieves comparable results to different state-of-the-art system combination methods.</p><p>3 0.23003598 <a title="155-tfidf-3" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao</p><p>Abstract: To address the parse error issue for tree-tostring translation, this paper proposes a similarity-based decoding generation (SDG) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. Our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models. 1</p><p>4 0.18881717 <a title="155-tfidf-4" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>5 0.18874837 <a title="155-tfidf-5" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>Author: Bing Xiang ; Abraham Ittycheriah</p><p>Abstract: In this paper we present a novel discriminative mixture model for statistical machine translation (SMT). We model the feature space with a log-linear combination ofmultiple mixture components. Each component contains a large set of features trained in a maximumentropy framework. All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance. This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT. It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task.</p><p>6 0.17606963 <a title="155-tfidf-6" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>7 0.16060266 <a title="155-tfidf-7" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>8 0.14009386 <a title="155-tfidf-8" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>9 0.13579561 <a title="155-tfidf-9" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>10 0.12925281 <a title="155-tfidf-10" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>11 0.12511486 <a title="155-tfidf-11" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>12 0.12161637 <a title="155-tfidf-12" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>13 0.11993725 <a title="155-tfidf-13" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>14 0.1171935 <a title="155-tfidf-14" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>15 0.11292429 <a title="155-tfidf-15" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>16 0.11261449 <a title="155-tfidf-16" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>17 0.11137236 <a title="155-tfidf-17" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>18 0.10992268 <a title="155-tfidf-18" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>19 0.10847783 <a title="155-tfidf-19" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>20 0.10795546 <a title="155-tfidf-20" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, -0.212), (2, 0.132), (3, 0.053), (4, 0.05), (5, 0.024), (6, -0.114), (7, -0.044), (8, -0.001), (9, 0.024), (10, -0.004), (11, -0.048), (12, 0.001), (13, -0.115), (14, -0.012), (15, -0.017), (16, -0.03), (17, -0.009), (18, -0.093), (19, -0.001), (20, -0.012), (21, -0.071), (22, 0.107), (23, 0.075), (24, -0.029), (25, -0.076), (26, 0.071), (27, 0.027), (28, -0.072), (29, 0.133), (30, -0.062), (31, 0.022), (32, -0.009), (33, -0.021), (34, -0.003), (35, 0.062), (36, -0.048), (37, -0.011), (38, -0.025), (39, -0.107), (40, 0.204), (41, 0.118), (42, -0.059), (43, -0.079), (44, 0.16), (45, -0.019), (46, 0.053), (47, -0.104), (48, -0.109), (49, 0.14)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96950996 <a title="155-lsi-1" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks.</p><p>2 0.89294201 <a title="155-lsi-2" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>Author: Jesus Gonzalez-Rubio ; Alfons Juan ; Francisco Casacuberta</p><p>Abstract: We present minimum Bayes-risk system combination, a method that integrates consensus decoding and system combination into a unified multi-system minimum Bayes-risk (MBR) technique. Unlike other MBR methods that re-rank translations of a single SMT system, MBR system combination uses the MBR decision rule and a linear combination of the component systems’ probability distributions to search for the minimum risk translation among all the finite-length strings over the output vocabulary. We introduce expected BLEU, an approximation to the BLEU score that allows to efficiently apply MBR in these conditions. MBR system combination is a general method that is independent of specific SMT models, enabling us to combine systems with heterogeneous structure. Experiments show that our approach bring significant improvements to single-system-based MBR decoding and achieves comparable results to different state-of-the-art system combination methods.</p><p>3 0.78408164 <a title="155-lsi-3" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>Author: Jingbo Zhu ; Tong Xiao</p><p>Abstract: To address the parse error issue for tree-tostring translation, this paper proposes a similarity-based decoding generation (SDG) solution by reconstructing similar source parse trees for decoding at the decoding time instead of taking multiple source parse trees as input for decoding. Experiments on Chinese-English translation demonstrated that our approach can achieve a significant improvement over the standard method, and has little impact on decoding speed in practice. Our approach is very easy to implement, and can be applied to other paradigms such as tree-to-tree models. 1</p><p>4 0.70710611 <a title="155-lsi-4" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>5 0.63033599 <a title="155-lsi-5" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>Author: Alexander M. Rush ; Michael Collins</p><p>Abstract: We describe an exact decoding algorithm for syntax-based statistical translation. The approach uses Lagrangian relaxation to decompose the decoding problem into tractable subproblems, thereby avoiding exhaustive dynamic programming. The method recovers exact solutions, with certificates of optimality, on over 97% of test examples; it has comparable speed to state-of-the-art decoders.</p><p>6 0.62087899 <a title="155-lsi-6" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>7 0.57609624 <a title="155-lsi-7" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>8 0.55275232 <a title="155-lsi-8" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>9 0.55087918 <a title="155-lsi-9" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>10 0.54237092 <a title="155-lsi-10" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>11 0.54219311 <a title="155-lsi-11" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>12 0.54095864 <a title="155-lsi-12" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>13 0.53191173 <a title="155-lsi-13" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>14 0.53182638 <a title="155-lsi-14" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>15 0.50276464 <a title="155-lsi-15" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>16 0.50136632 <a title="155-lsi-16" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>17 0.49691576 <a title="155-lsi-17" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>18 0.48960268 <a title="155-lsi-18" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>19 0.48895979 <a title="155-lsi-19" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>20 0.48696423 <a title="155-lsi-20" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.031), (17, 0.068), (26, 0.041), (37, 0.123), (39, 0.053), (41, 0.056), (55, 0.03), (59, 0.064), (62, 0.012), (72, 0.022), (88, 0.013), (91, 0.043), (96, 0.236), (97, 0.019), (99, 0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98172969 <a title="155-lda-1" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>Author: Ming Tan ; Wenli Zhou ; Lei Zheng ; Shaojun Wang</p><p>Abstract: This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-the- art parsing-based machine translation system.</p><p>2 0.96501529 <a title="155-lda-2" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>Author: S.R.K Branavan ; David Silver ; Regina Barzilay</p><p>Abstract: This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built- . in AI of Civilization II. 1</p><p>same-paper 3 0.92577571 <a title="155-lda-3" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks.</p><p>4 0.92191952 <a title="155-lda-4" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>5 0.92183661 <a title="155-lda-5" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>6 0.92021334 <a title="155-lda-6" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>7 0.91974366 <a title="155-lda-7" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>8 0.9194833 <a title="155-lda-8" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>9 0.91948247 <a title="155-lda-9" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>10 0.91860223 <a title="155-lda-10" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>11 0.91737819 <a title="155-lda-11" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>12 0.91658974 <a title="155-lda-12" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>13 0.91560054 <a title="155-lda-13" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>14 0.91549462 <a title="155-lda-14" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>15 0.91541982 <a title="155-lda-15" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>16 0.91535991 <a title="155-lda-16" href="./acl-2011-Language-independent_compound_splitting_with_morphological_operations.html">193 acl-2011-Language-independent compound splitting with morphological operations</a></p>
<p>17 0.91527653 <a title="155-lda-17" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>18 0.91413265 <a title="155-lda-18" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>19 0.91374075 <a title="155-lda-19" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>20 0.91343021 <a title="155-lda-20" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
