<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-111" href="#">acl2011-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</h1>
<br/><p>Source: <a title="acl-2011-111-pdf" href="http://aclweb.org/anthology//P/P11/P11-3013.pdf">pdf</a></p><p>Author: Nathan Green</p><p>Abstract: Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation. .</p><p>Reference: <a title="acl-2011-111-reference" href="../acl2011_reference/acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. [sent-2, score-1.051]
</p><p>2 Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. [sent-3, score-0.17]
</p><p>3 It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. [sent-4, score-1.713]
</p><p>4 This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2. [sent-5, score-0.91]
</p><p>5 23 Bleu score, improvement for English to Czech machine translation. [sent-7, score-0.038]
</p><p>6 1 Introduction Noun phrase structure in the Penn Treebank has up until recently been only considered, due to underspecification, a flat structure. [sent-9, score-0.366]
</p><p>7 Due to the annotation and work of Vadas and Curran (2007a; 2007b; 2008), we are now able to create Natural Language Processing (NLP) systems that take advantage of the internal structure of noun phrases in the Penn Treebank. [sent-10, score-0.657]
</p><p>8 This extra internal structure introduces additional complications in NLP applications such as parsing. [sent-11, score-0.285]
</p><p>9 Dependency parsing has been shown to improve NLP systems in certain languages and in many cases is considered the state of the art in the field. [sent-16, score-0.151]
</p><p>10 Dependency parsing made many improvements due to the CoNLL X shared task (Buchholz and Marsi, 2006). [sent-17, score-0.189]
</p><p>11 However, in most cases, these systems were trained with a flat noun phrase structure in the Penn Treebank. [sent-18, score-0.721]
</p><p>12 Vadas’ internal noun phrase structure has been used in previous work on constituent parsing using Collin’s  parser (Vadas and Curran, 2007c), but has yet to be analyzed for its effects on dependency parsing. [sent-19, score-1.267]
</p><p>13 Therefore, improvements in parsing output could have an improvement on other areas of NLP in many cases, such as Machine Translation. [sent-21, score-0.151]
</p><p>14 At the same time, any errors in parsing will tend to propagate down the NLP pipeline. [sent-22, score-0.24]
</p><p>15 One would expect parsing accuracy to be reduced when the complexity of the parse is increased, such as adding noun phrase structure. [sent-23, score-0.73]
</p><p>16 But, for a machine translation system that is reliant on parsing, the new noun phrase structure, even with reduced parser accuracy, may yield improvements due to a more detailed grammatical structure. [sent-24, score-0.8]
</p><p>17 This is particularly of interest for dependency relations, as it may aid in finding the correct head of a term in a complex noun phrase. [sent-25, score-0.491]
</p><p>18 This paper examines the results and errors in parsing and machine translation of dependency parsers, trained with annotated noun phrase structure, against those with a flat noun phrase structure. [sent-26, score-1.715]
</p><p>19 c 201 1 Association for Computational Linguistics  gold standard internal noun phrase structure annotation. [sent-29, score-0.875]
</p><p>20 Additionally, we analyze the effect of these improvements and errors in parsing down the NLP pipeline on the TectoMT machine translation system (Zˇabokrtsk y´ et al. [sent-30, score-0.437]
</p><p>21 1 Dependency Parsing Dependence parsing is an alternative view to the common phrase or constituent parsing techniques used with the Penn Treebank. [sent-37, score-0.526]
</p><p>22 Dependency relations can be used in many applications and have been shown to be quite useful in languages with a free word order. [sent-38, score-0.061]
</p><p>23 With the influx of many data-driven techniques, the need for annotated dependency relations is apparent. [sent-39, score-0.229]
</p><p>24 Since there are many data sets  with constituent relations annotated, this paper uses free conversion software provided from the CoNLL 2008 shared task to create dependency relations (Johansson and Nugues, 2007; Surdeanu et al. [sent-40, score-0.367]
</p><p>25 2 Dependency Parsers Dependency parsing comes in two main forms: Graph algorithms and Greedy algorithms. [sent-43, score-0.151]
</p><p>26 Each parser has its advantages and disadvantages, but the accuracy overall is approximately the same. [sent-46, score-0.148]
</p><p>27 The types of errors made by each parser, however, are very different. [sent-47, score-0.049]
</p><p>28 MSTParser is globally trained for an optimal solution and this has led it to get the best results on longer sentences. [sent-48, score-0.067]
</p><p>29 MaltParser on the other hand, is a greedy algorithm. [sent-49, score-0.031]
</p><p>30 This allows it to perform extremely well on shorter sentences, as the errors tend to propagate and causemore egregious errors inlonger sentences with longer dependencies (McDonald and Nivre, 2007). [sent-50, score-0.138]
</p><p>31 We expect each parser to have different errors handling internal noun phrase structure, but for this paper we will only be examining the globally trained 70  MSTParser. [sent-51, score-0.898]
</p><p>32 3  TectoMT  TectoMT is a machine translation framework based on Praguian tectogrammatics (Sgall, 1967) which represents four main layers: word layer, morphological layer, analytical layer, and tectogrammatical layer (Popel et al. [sent-53, score-0.469]
</p><p>33 This framework is primarily focused on the translation from English into Czech. [sent-55, score-0.133]
</p><p>34 Since much of dependency parsing work has been focused on Czech, this choice of machine translation framework logically follows as TectoMT makes direct use of the dependency relationships. [sent-56, score-0.683]
</p><p>35 The work in this paper primarily addresses the noun phrase structure in the analytical layer (SEnglishA in Figure 1). [sent-57, score-0.781]
</p><p>36 Figure 1: Translation Process in TectoMT in which the tectogrammatical layer is transfered from English to Czech. [sent-58, score-0.172]
</p><p>37 This allows great ease in adding the two different parsers into the framework since each experiment can be run as a separate “Scenario” comprised ofdifferent parsing “Blocks”. [sent-60, score-0.247]
</p><p>38 This allows a simple comparison of two machine translation system in which everything remains constant except the dependency parser. [sent-61, score-0.309]
</p><p>39 , 1993), consisting of annotated portions of the Wall Street Journal. [sent-64, score-0.03]
</p><p>40 Much of the annotation task is painstakingly done by annotators in great detail. [sent-65, score-0.033]
</p><p>41 Some structures are not dealt with in detail, such as noun phrase structure. [sent-66, score-0.514]
</p><p>42 Not having this information makes it difficult to tell the dependencies on phrases such as “crude oil prices” (Vadas and Curran, 2007c). [sent-67, score-0.194]
</p><p>43 With-  out internal annotation it is ambiguous whether the phrase is stating “crude prices” (crude (oil prices)) or “crude oil” ((crude oil) prices). [sent-68, score-0.411]
</p><p>44 crude  oil prices  crude  oil prices  Figure 2: Ambiguous dependency caused by internal noun phrase structure. [sent-69, score-1.874]
</p><p>45 Manual annotation of these phrases would be quite time consuming and as seen in the example above, sometimes ambiguous and therefore prone to poor inter-annotator agreement. [sent-70, score-0.147]
</p><p>46 Vadas and Curran have constructed a Gold standard version Penn treebank with these structures. [sent-71, score-0.078]
</p><p>47 The additional complexity of noun phrase structure has been shown to reduce parser accuracy in Collin’s parser but no similar evaluation has been conducted for dependency parsers. [sent-74, score-1.045]
</p><p>48 The internal noun phrase structure has been used in experiments prior but without  evaluation with respect to the noun phrases (Galley and Manning, 2009). [sent-75, score-1.138]
</p><p>49 The Baseline system is McDonald’s MSTParser trained on the Penn Treebank in English without any extra noun phrase bracketing. [sent-78, score-0.572]
</p><p>50 The Gold NP Parser is McDonald’s MSTParser trained on the Penn Treebank in English with gold standard noun phrase structure annotations (Vadas and Curran, 2007a). [sent-80, score-0.751]
</p><p>51 1 Data Sets To maintain a consistent dataset to compare to previous work we use the Wall Street Journal (WSJ) section of the Penn Treebank since it was used in the CoNLL X shared task on dependency parsing (Buchholz and Marsi, 2006). [sent-82, score-0.355]
</p><p>52 To test the effects of the noun phrase structure on machine translation, ACL 2008’s Workshop on Statistical Machine translation’s (WMT) data are used. [sent-84, score-0.685]
</p><p>53 The Penn Treebank with no internal noun phrase structure (PTB w/o NP structure). [sent-88, score-0.771]
</p><p>54 The Penn Treebank with gold standard noun phrase annotations provided by Vadas and Curran (PTB w/ gold standard NP structure). [sent-90, score-0.722]
</p><p>55 These parsers are trained using McDonald’s Maximum Spanning Tree Algorithm (MSTParser) (McDonald et al. [sent-92, score-0.098]
</p><p>56 Both of the parsers are then tested on a subset of the WSJ corpus, section 22, of the Penn Treebank and the UAS and LAS scores are generated. [sent-94, score-0.1]
</p><p>57 Errors generated by each of these systems are then compared to discover where the internal noun phrase structure affects the output. [sent-95, score-0.771]
</p><p>58 Parser accuracy is not necessarily the most important aspect of this work. [sent-96, score-0.034]
</p><p>59 The effect of this noun phrase structure down the  NLP pipeline is also crucial. [sent-97, score-0.711]
</p><p>60 For this, the parsers are inserted into the TectoMT system. [sent-98, score-0.068]
</p><p>61 3 Metrics Labeled Accuracy Score (LAS) and Unlabeled Accuracy Score (UAS) are the primary ways to evaluate dependency parsers. [sent-100, score-0.166]
</p><p>62 LAS is the percentage of words that are connected to their correct heads and have the correct dependency label. [sent-102, score-0.166]
</p><p>63 The Bleu (BiLingual Evaluation Understudy) score is an automatic scoring mechanism for machine translation that is quick and can be reused as a benchmark across machine translation tasks. [sent-104, score-0.315]
</p><p>64 Bleu is calculated as the geometric mean of n-grams comparing a machine translation and a reference text (Papineni et al. [sent-105, score-0.143]
</p><p>65 This experiment compares the two parsing systems against each other using the above metrics. [sent-107, score-0.151]
</p><p>66 In both cases the test set data is sampled 1,000 times without replacement to calculate statistical significance using a pairwise comparison. [sent-108, score-0.06]
</p><p>67 4  Results and Discussion  When applied, the gold standard annotations changed approximately 1. [sent-109, score-0.104]
</p><p>68 Once trained, both parsers were tested against section 22 of their respective annotated corpora. [sent-111, score-0.13]
</p><p>69 This was expected given the additional complexity of predicting the noun phrase structure and the previous work on noun phrase bracketing’s effect on Collin’s parser. [sent-113, score-1.159]
</p><p>70 Each is trained on Section 02-21 of the WSJ and tested on Section 22 While possibly more error prone, the 1. [sent-117, score-0.062]
</p><p>71 5% change in edges in the training data did appear to add more useful syntactic structure to the resulting parses as can be seen in Table 2. [sent-118, score-0.103]
</p><p>72 With the additional noun 72  phrase bracketing, the resulting Bleu score increased 0. [sent-119, score-0.514]
</p><p>73 The improvement is statistically significant with 95% confidence using pairwise bootstrapping of 1,000 test sets randomly sampled with replacement (Koehn, 2004; Zhang et al. [sent-122, score-0.06]
</p><p>74 The samples were sorted by the difference in bleu score. [sent-131, score-0.084]
</p><p>75 Visually, changes can be seen in the English side parse that affect the overall translation quality. [sent-132, score-0.136]
</p><p>76 Sentences that contained incorrect noun phrase structure such as “The second vice-president and Economy minister, Pedro Solbes” as seen in Figure 5 and Figure 6 were more correctly parsed in the Gold NP Parser. [sent-133, score-0.617]
</p><p>77 In Figure 5 “and” is incorrectly assigned to the bottom of a noun phrase and does not connect any segments together in the output of the Baseline Parser, while it connects two phrases in Figure 6  which is the output of the Gold NP Parser. [sent-134, score-0.632]
</p><p>78 This shift in bracketing also allows the proper noun, which is shaded, to be assigned to the correct head, the rightmost noun in the phrase. [sent-135, score-0.422]
</p><p>79 Figure 5: The parse created with the data with flat structures does not appear to handle noun phrases with more depth, in this case the ’and’ does not properly connect the two components. [sent-136, score-0.506]
</p><p>80 Figure 6: With the addition of noun phrase structure in parser, the complicated noun phrase appears to be better structured. [sent-137, score-1.131]
</p><p>81 The ’and’ connects two components instead of improperly being a leaf node. [sent-138, score-0.042]
</p><p>82 73 5  Conclusion  This paper has demonstrated the benefit of additional noun phrase bracketing in training data for use in dependency parsing and machine translation. [sent-139, score-0.966]
</p><p>83 Using the additional structure, the dependency parser’s accuracy was minimally reduced. [sent-140, score-0.2]
</p><p>84 Despite this re-  duction, machine translation, much further down the NLP pipeline, obtained a 2. [sent-141, score-0.038]
</p><p>85 Future work should examine similar experiments with MaltParser and other machine translation systems. [sent-143, score-0.143]
</p><p>86 Building a large annotated corpus of english: the penn treebank. [sent-169, score-0.186]
</p><p>87 Bleu: a method for automatic evaluation of machine translation. [sent-188, score-0.038]
</p><p>88 The conll-2008 shared task on joint parsing of syntactic and semantic dependencies. [sent-201, score-0.189]
</p><p>89 Tectomt: highly modular mt system with tectogrammatics used as transfer layer. [sent-227, score-0.144]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vadas', 0.379), ('tectomt', 0.333), ('noun', 0.325), ('crude', 0.206), ('phrase', 0.189), ('curran', 0.168), ('dependency', 0.166), ('prices', 0.162), ('penn', 0.156), ('internal', 0.154), ('oil', 0.152), ('parsing', 0.151), ('uas', 0.118), ('abokrtsk', 0.117), ('parser', 0.114), ('las', 0.112), ('layer', 0.105), ('translation', 0.105), ('gold', 0.104), ('mstparser', 0.104), ('structure', 0.103), ('mcdonald', 0.101), ('np', 0.101), ('zden', 0.1), ('bracketing', 0.097), ('buchholz', 0.092), ('bleu', 0.084), ('treebank', 0.078), ('flat', 0.074), ('marsi', 0.069), ('parsers', 0.068), ('nlp', 0.067), ('petr', 0.067), ('popel', 0.067), ('tectogrammatical', 0.067), ('tectogrammatics', 0.067), ('pipeline', 0.066), ('collin', 0.064), ('analytical', 0.059), ('ptb', 0.057), ('conll', 0.057), ('maltparser', 0.056), ('wsj', 0.053), ('johansson', 0.051), ('modular', 0.05), ('jan', 0.049), ('errors', 0.049), ('morristown', 0.046), ('joakim', 0.046), ('examines', 0.044), ('spanning', 0.044), ('melbourne', 0.043), ('nivre', 0.043), ('connects', 0.042), ('phrases', 0.042), ('nj', 0.041), ('propagate', 0.04), ('wall', 0.04), ('surdeanu', 0.038), ('shared', 0.038), ('czech', 0.038), ('machine', 0.038), ('prone', 0.037), ('globally', 0.037), ('street', 0.036), ('green', 0.035), ('james', 0.035), ('constituent', 0.035), ('ambiguous', 0.035), ('conversion', 0.034), ('connect', 0.034), ('accuracy', 0.034), ('relations', 0.033), ('annotation', 0.033), ('replacement', 0.033), ('tested', 0.032), ('association', 0.032), ('parse', 0.031), ('greedy', 0.031), ('effects', 0.03), ('annotated', 0.03), ('trained', 0.03), ('logically', 0.029), ('msm', 0.029), ('academia', 0.029), ('icetal', 0.029), ('nodalida', 0.029), ('reliant', 0.029), ('reused', 0.029), ('tartu', 0.029), ('underspecification', 0.029), ('prague', 0.029), ('framework', 0.028), ('free', 0.028), ('extra', 0.028), ('effect', 0.028), ('galley', 0.028), ('australia', 0.028), ('sampled', 0.027), ('transfer', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="111-tfidf-1" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>Author: Nathan Green</p><p>Abstract: Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation. .</p><p>2 0.19808505 <a title="111-tfidf-2" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>Author: Eneko Agirre ; Kepa Bengoetxea ; Koldo Gojenola ; Joakim Nivre</p><p>Abstract: This paper presents the introduction of WordNet semantic classes in a dependency parser, obtaining improvements on the full Penn Treebank for the first time. We tried different combinations of some basic semantic classes and word sense disambiguation algorithms. Our experiments show that selecting the adequate combination of semantic features on development data is key for success. Given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements. 1</p><p>3 0.19430453 <a title="111-tfidf-3" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>Author: Alexander Volokh ; Gunter Neumann</p><p>Abstract: Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 1</p><p>4 0.19418043 <a title="111-tfidf-4" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>Author: Yue Zhang ; Joakim Nivre</p><p>Abstract: Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.</p><p>5 0.18156883 <a title="111-tfidf-5" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>Author: Jinho D. Choi ; Martha Palmer</p><p>Abstract: This paper suggests two ways of improving transition-based, non-projective dependency parsing. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set.</p><p>6 0.18097512 <a title="111-tfidf-6" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>7 0.17410769 <a title="111-tfidf-7" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>8 0.16644499 <a title="111-tfidf-8" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>9 0.16406831 <a title="111-tfidf-9" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>10 0.154477 <a title="111-tfidf-10" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>11 0.14143004 <a title="111-tfidf-11" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>12 0.13751481 <a title="111-tfidf-12" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>13 0.11865391 <a title="111-tfidf-13" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>14 0.11656663 <a title="111-tfidf-14" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>15 0.1119624 <a title="111-tfidf-15" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>16 0.11000509 <a title="111-tfidf-16" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>17 0.10680102 <a title="111-tfidf-17" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>18 0.10586884 <a title="111-tfidf-18" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>19 0.099769257 <a title="111-tfidf-19" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>20 0.093637012 <a title="111-tfidf-20" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.241), (1, -0.124), (2, -0.036), (3, -0.249), (4, -0.003), (5, -0.039), (6, 0.091), (7, 0.061), (8, 0.097), (9, -0.044), (10, 0.032), (11, -0.008), (12, 0.028), (13, -0.209), (14, 0.003), (15, 0.034), (16, 0.036), (17, 0.03), (18, -0.029), (19, 0.004), (20, -0.115), (21, 0.047), (22, -0.008), (23, 0.014), (24, 0.067), (25, 0.008), (26, 0.066), (27, -0.032), (28, -0.043), (29, -0.005), (30, 0.043), (31, -0.024), (32, 0.031), (33, 0.01), (34, 0.001), (35, -0.012), (36, -0.001), (37, -0.021), (38, 0.052), (39, 0.077), (40, -0.063), (41, 0.088), (42, 0.009), (43, -0.046), (44, 0.011), (45, 0.004), (46, 0.036), (47, 0.084), (48, 0.058), (49, -0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97718668 <a title="111-lsi-1" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>Author: Nathan Green</p><p>Abstract: Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation. .</p><p>2 0.84407634 <a title="111-lsi-2" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>Author: Yue Zhang ; Joakim Nivre</p><p>Abstract: Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.</p><p>3 0.83843786 <a title="111-lsi-3" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao ; Kang Liu ; Li Cai</p><p>Abstract: In this paper, we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing. Conventional selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous work to wordto-word selectional preferences by using webscale data. Experiments show that web-scale data improves statistical dependency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance.</p><p>4 0.79184693 <a title="111-lsi-4" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.</p><p>5 0.78296316 <a title="111-lsi-5" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>Author: Alexander Volokh ; Gunter Neumann</p><p>Abstract: Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 1</p><p>6 0.78218293 <a title="111-lsi-6" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>7 0.77703458 <a title="111-lsi-7" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>8 0.7768206 <a title="111-lsi-8" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>9 0.76022947 <a title="111-lsi-9" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>10 0.74128217 <a title="111-lsi-10" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>11 0.70108575 <a title="111-lsi-11" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>12 0.69420761 <a title="111-lsi-12" href="./acl-2011-Partial_Parsing_from_Bitext_Projections.html">243 acl-2011-Partial Parsing from Bitext Projections</a></p>
<p>13 0.63041604 <a title="111-lsi-13" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>14 0.59348232 <a title="111-lsi-14" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>15 0.58170873 <a title="111-lsi-15" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>16 0.57383472 <a title="111-lsi-16" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>17 0.5699538 <a title="111-lsi-17" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>18 0.56958157 <a title="111-lsi-18" href="./acl-2011-Temporal_Restricted_Boltzmann_Machines_for_Dependency_Parsing.html">295 acl-2011-Temporal Restricted Boltzmann Machines for Dependency Parsing</a></p>
<p>19 0.56335896 <a title="111-lsi-19" href="./acl-2011-Dynamic_Programming_Algorithms_for_Transition-Based_Dependency_Parsers.html">107 acl-2011-Dynamic Programming Algorithms for Transition-Based Dependency Parsers</a></p>
<p>20 0.56171304 <a title="111-lsi-20" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.029), (17, 0.07), (26, 0.019), (28, 0.01), (37, 0.176), (39, 0.066), (41, 0.085), (55, 0.011), (59, 0.036), (66, 0.194), (72, 0.017), (91, 0.044), (96, 0.121), (97, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82861316 <a title="111-lda-1" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>Author: Nathan Green</p><p>Abstract: Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation. .</p><p>2 0.78707558 <a title="111-lda-2" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>Author: Alexis Nasr ; Frederic Bechet ; Jean-Francois Rey ; Benoit Favre ; Joseph Le Roux</p><p>Abstract: MACAON is a tool suite for standard NLP tasks developed for French. MACAON has been designed to process both human-produced text and highly ambiguous word-lattices produced by NLP tools. MACAON is made of several native modules for common tasks such as a tokenization, a part-of-speech tagging or syntactic parsing, all communicating with each other through XML files . In addition, exchange protocols with external tools are easily definable. MACAON is a fast, modular and open tool, distributed under GNU Public License.</p><p>3 0.75705379 <a title="111-lda-3" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>Author: Mark-Jan Nederhof ; Giorgio Satta</p><p>Abstract: We present a method for the computation of prefix probabilities for synchronous contextfree grammars. Our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms.</p><p>4 0.75485677 <a title="111-lda-4" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>Author: Yue Zhang ; Joakim Nivre</p><p>Abstract: Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.</p><p>5 0.75110221 <a title="111-lda-5" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>Author: David McClosky ; Mihai Surdeanu ; Christopher Manning</p><p>Abstract: Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a “crime” event can cause a “investigation” event, which can lead to an “arrest” event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP’09 shared task with a F1 score of 53.5% in development and 48.6% in testing.</p><p>6 0.74629021 <a title="111-lda-6" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>7 0.74575335 <a title="111-lda-7" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>8 0.74461186 <a title="111-lda-8" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>9 0.74394244 <a title="111-lda-9" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>10 0.74289453 <a title="111-lda-10" href="./acl-2011-Which_Noun_Phrases_Denote_Which_Concepts%3F.html">334 acl-2011-Which Noun Phrases Denote Which Concepts?</a></p>
<p>11 0.7417444 <a title="111-lda-11" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>12 0.73931855 <a title="111-lda-12" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>13 0.73585391 <a title="111-lda-13" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>14 0.73468041 <a title="111-lda-14" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>15 0.73460442 <a title="111-lda-15" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>16 0.73299259 <a title="111-lda-16" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>17 0.73222208 <a title="111-lda-17" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>18 0.73222131 <a title="111-lda-18" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>19 0.7308346 <a title="111-lda-19" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>20 0.7306664 <a title="111-lda-20" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
