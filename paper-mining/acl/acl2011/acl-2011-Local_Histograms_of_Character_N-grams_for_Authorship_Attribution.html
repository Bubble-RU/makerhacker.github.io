<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-212" href="#">acl2011-212</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</h1>
<br/><p>Source: <a title="acl-2011-212-pdf" href="http://aclweb.org/anthology//P/P11/P11-1030.pdf">pdf</a></p><p>Author: Hugo Jair Escalante ; Thamar Solorio ; Manuel Montes-y-Gomez</p><p>Abstract: This paper proposes the use of local histograms (LH) over character n-grams for authorship attribution (AA). LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word histograms. In this work we explore the suitability of LHs over n-grams at the character-level for AA. We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors. We report experimental results in AA data sets that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.</p><p>Reference: <a title="acl-2011-212-reference" href="../acl2011_reference/acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper proposes the use of local histograms (LH) over character n-grams for authorship attribution (AA). [sent-9, score-1.03]
</p><p>2 LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word histograms. [sent-10, score-0.401]
</p><p>3 We report experimental results in AA data sets that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art  approaches. [sent-13, score-0.194]
</p><p>4 Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection. [sent-15, score-0.271]
</p><p>5 1 Introduction Authorship attribution (AA) is the task of deciding whom, from a set of candidates, is the author of a given document (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Stamatatos, 2009b). [sent-16, score-0.23]
</p><p>6 However, unlike usual text categorization tasks, where the core problem is modeling the thematic content of documents (Sebastiani, 2002), the goal in AA is modeling authors’ writing style (Stamatatos, 2009b). [sent-21, score-0.171]
</p><p>7 Hence, document representations that reveal information about writing style are required to achieve good accuracy in AA. [sent-22, score-0.202]
</p><p>8 Word and character based representations have been used in AA with some success so far (Houvardas and Stamatatos, 2006; Luyckx and Daelemans, 2010; Plakias and Stamatatos, 2008b). [sent-23, score-0.198]
</p><p>9 Such representations can capture style information through word or character usage, but they lack sequential information, which can reveal further stylistic information. [sent-24, score-0.309]
</p><p>10 In this paper, we study the use of richer document representations for the AA task. [sent-25, score-0.134]
</p><p>11 In particular, we consider local histograms over n-grams at the character-level obtained via the locally-weighted bag of words (LOWBOW) framework (Lebanon et al. [sent-26, score-0.756]
</p><p>12 Under LOWBOW, a document is represented by a set of local histograms, computed across the whole document but smoothed by kernels centered on different document locations. [sent-28, score-0.373]
</p><p>13 Ac s2s0o1ci1a Atiosnso fcoirat Cioonm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 288–298, representations preserve both word/character usage and sequential information (i. [sent-31, score-0.174]
</p><p>14 Results confirm that local histograms of character n-grams are more helpful for AA than the usual global histograms of words or character n-grams (Luyckx and Daelemans, 2010); our results are superior to those reported in related works. [sent-35, score-1.549]
</p><p>15 We also show that local histograms over character n-grams are more helpful than local histograms over words, as originally proposed by (Lebanon et al. [sent-36, score-1.484]
</p><p>16 The contributions of this work are as follows: •  We show that the LOWBOW framework can be helpful wfor AhaAt, giving WevBidOenWce trahamt sequential i bne-  •  •  •  2  formation encoded in local histograms is useful for modeling the writing style of authors. [sent-42, score-0.857]
</p><p>17 We propose the use of local histograms over cWheara pcrtoepr-olesveel t n-grams ffo lro cAalA. [sent-43, score-0.668]
</p><p>18 h Wstoeg rsahmows othveatr character-level representations, which have proved to be very effective for AA (Luyckx and Daelemans, 2010), can be further improved by adopting a local histogram formulation. [sent-44, score-0.232]
</p><p>19 Also, we empirically show that local histograms at the character-level are more helpful than local histograms at the word-level for AA. [sent-45, score-1.359]
</p><p>20 We study several kernels for a support vector macWhein est uAdAy scelavsesriaflie kr eurnnedlesr ftoher alo scualp histograms fmoar-mulation. [sent-46, score-0.646]
</p><p>21 Our study confirms that the diffusion kernel (Lafferty and Lebanon, 2005) is the most effective among those we tried, although competitive performance can be obtained with simpler kernels. [sent-47, score-0.146]
</p><p>22 Unfortunately, because of computational limitations, the latter methods cannot discover enough sequential information from documents (e. [sent-65, score-0.138]
</p><p>23 With respect to character-based features, n-grams at the character level have been widely used in AA as well (Plakias and Stamatatos, 2008b; Peng et al. [sent-74, score-0.125]
</p><p>24 However, as with word-based features, character n-grams are unable to incorporate sequential information from documents in their original form (in terms of the positions in which the terms appear across a document). [sent-81, score-0.263]
</p><p>25 We believe that sequential clues can be helpful for AA because different authors are expected to use different character n-grams or words in different parts of the document. [sent-82, score-0.245]
</p><p>26 Hence, the proposed features preserve sequential information besides capturing character and word usage information. [sent-84, score-0.226]
</p><p>27 , 2007), where researchers have used in-  formation derived from local histograms for displaying a 2D representation of document’s content. [sent-88, score-0.691]
</p><p>28 The latter can be due to the fact that local histograms provide little gain over usual global histograms for thematic classification tasks. [sent-95, score-1.252]
</p><p>29 In this paper we show that LOWBOW representations provide important improvements over global histograms for AA; in particular, local histograms at the character-level achieve the highest performance in our experiments. [sent-96, score-1.303]
</p><p>30 3 Background This section describes preliminary information on document representations and pattern classification 290  with SVMs. [sent-97, score-0.134]
</p><p>31 1 Bag of words representations In the bag of words (BOW) representation, documents are represented by histograms over the vocabulary1 that was used to generate a collection of documents; that is, a document iis represented as: di = [xi,1, . [sent-99, score-0.795]
</p><p>32 2  Locally-weighted bag-of-words representation Instead of using the BOW framework directly, we adopted the LOWBOW framework for document representation (Lebanon et al. [sent-106, score-0.145]
</p><p>33 The underlying idea in LOWBOW is to compute several local histograms per document, where these histograms  µ  are smoothed by a kernel function, see Figure 1. [sent-108, score-1.312]
</p><p>34 The parameters of the kernel specify the position of the kernel in the document (i. [sent-109, score-0.185]
</p><p>35 , where the local histogram is centered) and its scale (i. [sent-111, score-0.232]
</p><p>36 In this way the sequential information in the document is preserved together with term usage statistics. [sent-114, score-0.158]
</p><p>37 Given a kernel smoothing function Kµs,σ : [0, 1] → R with location parameter and  PNj=i1  scale parameter  σ,  where  Pjk=1 Kµs,σ (tj)  = 1 and  1In the following we will rPefer to arbitrary vocabularies, which can be formed with terms from either words or character n-grams. [sent-125, score-0.187]
</p><p>38 Then, the term position weighting is combined with term frequency weighting for obtaining local histograms over the terms in the vocabulary (1, . [sent-134, score-0.704]
</p><p>39 The LOWBOW framework computes a µloc ∈al histogram f LorO eWacBh position µj ∈ {µ1, . [sent-139, score-0.145]
</p><p>40 Hence, a set of k local histograms are computed for each document i. [sent-152, score-0.729]
</p><p>41 Each histogram carries information about the distribution of terms at a certain position µj of the document, where σ determines how the nearby terms to µj influence the local histogram j. [sent-153, score-0.358]
</p><p>42 Thus, sequential information of the document is considered throughout these local histograms. [sent-154, score-0.246]
</p><p>43 Note that when σ is small, most of the sequential information is preserved, as local histograms are calculated at very local scales; whereas when σ ≥ 1, local histograms rceaslem scballee t;h we terraedaitsio wnhael nB σOW ≥ representation. [sent-155, score-1.521]
</p><p>44 , 2007): as a single histogram diL = const (hereafter LOWBOW histograms) or by Pthe set of local histograms itself We performed experiments with 291 both forms of representation and considered words  dlji  Pjk=1 dlji  dli{1,. [sent-157, score-0.879]
</p><p>45 4  Authorship Attribution with LOWBOW  Representations For AA we represent the training documents of each author using the framework described in Section 3. [sent-178, score-0.156]
</p><p>46 2, thus each document of each candidate author is either a LOWBOW histogram or a bag of local histograms (BOLH). [sent-179, score-0.973]
</p><p>47 Recall that LOWBOW histograms are an un-weighted sum of local histograms and hence can be considered a summary of term usage and sequential information; whereas the BOLH can be seen as term occurrence frequencies across different locations of the document. [sent-180, score-1.405]
</p><p>48 , 2001; Grauman, 2006):  K(P,Q) = exp¡−D(Pγ,Q)2¢  (5)  where D(P, Q) is the sum of the distances between the elements of the bag of local histograms associated to author P and the elements of the bag of histograms associated with author Q; γ is the scale parameter of K. [sent-203, score-1.466]
</p><p>49 t Lhee tel Pem =ent {sp of the bags nodf Qloca =l histograms f}or b iens tthaen eceles mPe natnsd o Q, respectively, Tcaa-l ble 1presents the distance measures we consider for AA using local histograms. [sent-211, score-0.699]
</p><p>50 Diffusion, Euclidean, and χ2 kernels compare local histograms one to one, which means that the local histograms calculated at the same locations are compared to each other. [sent-214, score-1.48]
</p><p>51 We believe that for AA this is advantageous as it is expected that an author uses similar terms at similar locations of the document. [sent-215, score-0.174]
</p><p>52 The Earth mover’s distance (EMD), on the other hand, is an estimate of the optimal cost in taking local histograms from Q to local histograms in P (Rubner et al. [sent-216, score-1.336]
</p><p>53 , 2001); that is, this measure computes the optimal matching distance between local histograms from different authors that are not necessarily computed at similar locations. [sent-217, score-0.686]
</p><p>54 There are 50 documents per author for training and 50 documents per author for testing. [sent-223, score-0.314]
</p><p>55 For our character n-gram experiments, we obtained LOWBOW representations for character 3-grams (only n-grams of size n = 3 were used) considering the 2, 500 most common n-grams. [sent-227, score-0.352]
</p><p>56 Again, this setting was adopted in agreement with previous work on AA with character n-grams (Houvardas and Stamatatos, 2006; Plakias and Stamatatos, 2008b; Plakias and Stamatatos, 2008a; Luyckx and Daelemans, 2010). [sent-228, score-0.143]
</p><p>57 We perform experiments using all of the training documents per author, that is, a balanced corpus (we call this setting BC). [sent-233, score-0.128]
</p><p>58 We tried balanced reduced data sets with: 1, 3, 5 and 10 documents per author (we call this configuration RBC). [sent-235, score-0.188]
</p><p>59 BC setting represents the AA problem under ideal conditions, whereas settings RBC and IRBC aim at emulating a more realistic scenario, where limited sample documents are available and the whole data set is highly imbalanced (Plakias and Stamatatos, 2008b). [sent-237, score-0.149]
</p><p>60 2 Experimental results in balanced data We first compare the performance of the LOWBOW  histogram representation to that of the traditional BOW representation. [sent-245, score-0.18]
</p><p>61 , percentage of documents in the test set that were associated to its correct author) for the BOW and LOWBOW histogram representations when using words and character n-grams information. [sent-248, score-0.383]
</p><p>62 0% Table 2: Authorship attribution representation and LOWBOW shows the parameters we used tograms; columns 3 and 4 show character n-grams, respectively. [sent-265, score-0.27]
</p><p>63 Column 2 for the LOWBOW hisresults using words and  From Table 2 we can see that the BOW representation is very effective, outperforming most of the LOWBOW histogram configurations. [sent-267, score-0.149]
</p><p>64 Despite a small difference in performance, BOW is advantageous over LOWBOW histograms because it is simpler to compute and it does not rely on parameter selection. [sent-268, score-0.598]
</p><p>65 Recall that the LOWBOW histogram representations are obtained by the combination of several local histograms calculated at different locations of the document, hence, it seems that the raw sum of local histograms results in a loss of useful information for representing documents. [sent-269, score-1.624]
</p><p>66 The worse performance was obtained when k = 2 local histograms are considered (see row 3 in Table 2). [sent-270, score-0.719]
</p><p>67 This result is somewhat expected since the larger the number of local histograms, the more LOWBOW histograms approach the BOW formulation (Lebanon et al. [sent-271, score-0.702]
</p><p>68 Most of the results from this table are superior to those reported in Table 2, showing that bags of local histograms are a better  way to exploit the LOWBOW framework for AA. [sent-274, score-0.742]
</p><p>69 However, the diffusion kernel outperformed most of the results obtained with other kernels; confirming the results obtained by other researchers (Lebanon et al. [sent-276, score-0.175]
</p><p>70 2% Table 3: Authorship attribution accuracy when using bags of local histograms and different kernels for word-based and character-based representations. [sent-303, score-0.874]
</p><p>71 On average, the worse kernel was that based on the earth mover’s distance (EMD), suggesting that the comparison of local histograms at different locations is not a fruitful approach (recall that this is the only kernel that compares local histograms at differ-  ent locations). [sent-306, score-1.544]
</p><p>72 The best performance across settings and kernels was obtained with the diffusion kernel (in bold, column 3, row 9) (86. [sent-308, score-0.252]
</p><p>73 Therefore, the considered local histogram representations over character n-grams have proved to be very effective for AA. [sent-312, score-0.43]
</p><p>74 We believe this can be attributed to the fact that character n-grams provide a representation for the document at a finer  granularity, which can be better exploited with local histogram representations. [sent-316, score-0.441]
</p><p>75 Hence, the local histograms are less sparse when using character-level information, which results in better AA performance. [sent-322, score-0.668]
</p><p>76 Columns show the true author for test documents and rows show the authors predicted by the SVM. [sent-326, score-0.155]
</p><p>77 The SVM with BOW representation of character ngrams achieved recognition rates of 40% and 50% for BL and JM respectively. [sent-336, score-0.148]
</p><p>78 Thus, we can state that sequential information was indeed helpful for modeling BL writing style (improvement of 28%), although it is an author that resulted very difficult to model. [sent-337, score-0.248]
</p><p>79 On the other hand, local histograms were not very useful for identifying documents written by JM (made it worse by −8%). [sent-338, score-0.727]
</p><p>80 The largest improvement ((3m8a%de) iotf w wloorcasel histograms over atrhgee sBtO imWp foovremmuelan-t tion was obtained for author TN (T. [sent-339, score-0.669]
</p><p>81 For these experiments we compare the performance of the BOW, LOWBOW histogram and BOLH representations; for the latter, we considered the best setting as reported in Table 3 (i. [sent-345, score-0.144]
</p><p>82 From Tables 5 and 6 we can see that BOW and LOWBOW histogram representations obtained similar performance to each other across the different training set sizes, which agree with results in Table 2 for the BC data sets. [sent-350, score-0.228]
</p><p>83 The improvements of local histograms over the BOW formulation vary across different settings and when using information at word-level and character-level. [sent-352, score-0.702]
</p><p>84 Thus, it is evident that local histograms are more beneficial when less documents are considered. [sent-359, score-0.727]
</p><p>85 Here, the lack of information is compensated by the availability of several histograms per author. [sent-360, score-0.582]
</p><p>86 These are very positive results; for example, we can obtain almost 71% of accuracy, using local histograms of character n-grams when a single document is available per author (recall that we have used all of the test samples for evaluating the performance of our methods). [sent-375, score-0.952]
</p><p>87 The same pattern as before can be observed in experimental results for these data sets as well: BOW and LOWBOW histograms obtained comparable performance to each other and the BOLH formulation performed the best. [sent-377, score-0.625]
</p><p>88 Again, better results were obtained when using character n-grams for the local histograms. [sent-379, score-0.26]
</p><p>89 Summarizing, the results obtained in RBC and IRBC data sets show that the use of local histograms is advantageous under challenging conditions. [sent-381, score-0.733]
</p><p>90 Our hypothesis for this behavior is that local histograms can be thought of as expanding training instances, because for each training instance in the BOW formulation we have k−training instances under BOLH. [sent-383, score-0.702]
</p><p>91 io Tuhse as etnhee number of available documents per author decreases. [sent-385, score-0.157]
</p><p>92 We report results for the BOW, LOWBOW histogram and BOLH representations. [sent-395, score-0.126]
</p><p>93 6  Conclusions  We have described the use of local histograms (LH) over character n-grams for AA. [sent-406, score-0.793]
</p><p>94 LHs are enriched histogram representations that preserve sequential information in documents (in terms of the positions of terms in documents); we explored the suitability of LHs over n-grams at the character-level for AA. [sent-407, score-0.377]
</p><p>95 We showed evidence supporting our hypothesis that LHs are very helpful for AA; we believe that this is due to the fact that LOWBOW representations can uncover, to some extent, the writing preferences of authors. [sent-408, score-0.132]
</p><p>96 The improvements were larger in reduced and imbalanced data sets, which is a very positive result as in real AA applications one often faces highly imbalanced and small sample issues. [sent-410, score-0.144]
</p><p>97 Measuring the usefulness of function words for authorship attribution. [sent-432, score-0.146]
</p><p>98 The effect of author set size and data size in authorship attribution. [sent-543, score-0.224]
</p><p>99 Language independent authorship attribution using character level language models. [sent-558, score-0.362]
</p><p>100 An algorithm for automated authorship attribution using neural networks. [sent-639, score-0.237]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('histograms', 0.562), ('lowbow', 0.413), ('stamatatos', 0.322), ('plakias', 0.258), ('aa', 0.236), ('authorship', 0.146), ('lebanon', 0.145), ('histogram', 0.126), ('character', 0.125), ('bow', 0.12), ('bolh', 0.114), ('houvardas', 0.114), ('local', 0.106), ('attribution', 0.091), ('kernels', 0.084), ('rbc', 0.082), ('sequential', 0.079), ('author', 0.078), ('lhs', 0.075), ('luyckx', 0.073), ('representations', 0.073), ('irbc', 0.072), ('imbalanced', 0.072), ('kernel', 0.062), ('document', 0.061), ('locations', 0.06), ('documents', 0.059), ('diffusion', 0.055), ('daelemans', 0.045), ('birmingham', 0.041), ('keselj', 0.041), ('mao', 0.041), ('bag', 0.04), ('plagiarism', 0.039), ('peng', 0.039), ('writing', 0.036), ('advantageous', 0.036), ('bc', 0.034), ('formulation', 0.034), ('svm', 0.033), ('style', 0.032), ('columns', 0.031), ('lncs', 0.031), ('balanced', 0.031), ('bags', 0.031), ('dlji', 0.031), ('lambers', 0.031), ('lowbowk', 0.031), ('mover', 0.031), ('rubner', 0.031), ('obtained', 0.029), ('amida', 0.027), ('cristianini', 0.027), ('emd', 0.027), ('xi', 0.026), ('earth', 0.024), ('jm', 0.024), ('superior', 0.024), ('helpful', 0.023), ('representation', 0.023), ('row', 0.022), ('preserve', 0.022), ('tj', 0.022), ('multiclass', 0.022), ('usual', 0.022), ('categorization', 0.022), ('alabama', 0.021), ('canu', 0.021), ('chasanis', 0.021), ('dli', 0.021), ('ecrime', 0.021), ('exico', 0.021), ('forensics', 0.021), ('itfw', 0.021), ('mez', 0.021), ('pillay', 0.021), ('rifkin', 0.021), ('rouen', 0.021), ('sdbrloe', 0.021), ('shuurmans', 0.021), ('solorio', 0.021), ('tearle', 0.021), ('tensor', 0.021), ('veenman', 0.021), ('vel', 0.021), ('bl', 0.021), ('per', 0.02), ('xj', 0.02), ('koppel', 0.02), ('framework', 0.019), ('authors', 0.018), ('visualization', 0.018), ('suitability', 0.018), ('cis', 0.018), ('uncovering', 0.018), ('chapters', 0.018), ('term', 0.018), ('formulations', 0.018), ('setting', 0.018), ('pjk', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="212-tfidf-1" href="./acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution.html">212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</a></p>
<p>Author: Hugo Jair Escalante ; Thamar Solorio ; Manuel Montes-y-Gomez</p><p>Abstract: This paper proposes the use of local histograms (LH) over character n-grams for authorship attribution (AA). LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word histograms. In this work we explore the suitability of LHs over n-grams at the character-level for AA. We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors. We report experimental results in AA data sets that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.</p><p>2 0.16376846 <a title="212-tfidf-2" href="./acl-2011-Lost_in_Translation%3A_Authorship_Attribution_using_Frame_Semantics.html">214 acl-2011-Lost in Translation: Authorship Attribution using Frame Semantics</a></p>
<p>Author: Steffen Hedegaard ; Jakob Grue Simonsen</p><p>Abstract: We investigate authorship attribution using classifiers based on frame semantics. The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts. Our results suggest (i) that frame-based classifiers are usable for author attribution of both translated and untranslated texts; (ii) that framebased classifiers generally perform worse than the baseline classifiers for untranslated texts, but (iii) perform as well as, or superior to the baseline classifiers on translated texts; (iv) that—contrary to current belief—naïve clas- sifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.</p><p>3 0.071057305 <a title="212-tfidf-3" href="./acl-2011-A_New_Dataset_and_Method_for_Automatically_Grading_ESOL_Texts.html">20 acl-2011-A New Dataset and Method for Automatically Grading ESOL Texts</a></p>
<p>Author: Helen Yannakoudakis ; Ted Briscoe ; Ben Medlock</p><p>Abstract: We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.</p><p>4 0.06829682 <a title="212-tfidf-4" href="./acl-2011-Towards_Style_Transformation_from_Written-Style_to_Audio-Style.html">306 acl-2011-Towards Style Transformation from Written-Style to Audio-Style</a></p>
<p>Author: Amjad Abu-Jbara ; Barbara Rosario ; Kent Lyons</p><p>Abstract: In this paper, we address the problem of optimizing the style of textual content to make it more suitable to being listened to by a user as opposed to being read. We study the differences between the written style and the audio style by consulting the linguistics andjour- nalism literatures. Guided by this study, we suggest a number of linguistic features to distinguish between the two styles. We show the correctness of our features and the impact of style transformation on the user experience through statistical analysis, a style classification task, and a user study.</p><p>5 0.052294832 <a title="212-tfidf-5" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>Author: Andrew L. Maas ; Raymond E. Daly ; Peter T. Pham ; Dan Huang ; Andrew Y. Ng ; Christopher Potts</p><p>Abstract: Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semanticterm–documentinformation as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset , of movie reviews to serve as a more robust benchmark for work in this area.</p><p>6 0.052026544 <a title="212-tfidf-6" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>7 0.051319182 <a title="212-tfidf-7" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>8 0.050620414 <a title="212-tfidf-8" href="./acl-2011-Unsupervised_Decomposition_of_a_Document_into_Authorial_Components.html">319 acl-2011-Unsupervised Decomposition of a Document into Authorial Components</a></p>
<p>9 0.048549149 <a title="212-tfidf-9" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>10 0.042565122 <a title="212-tfidf-10" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>11 0.041182183 <a title="212-tfidf-11" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>12 0.039426781 <a title="212-tfidf-12" href="./acl-2011-A_Stacked_Sub-Word_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">27 acl-2011-A Stacked Sub-Word Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>13 0.039258286 <a title="212-tfidf-13" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>14 0.039067231 <a title="212-tfidf-14" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>15 0.036858249 <a title="212-tfidf-15" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>16 0.034194723 <a title="212-tfidf-16" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>17 0.033987626 <a title="212-tfidf-17" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>18 0.032056287 <a title="212-tfidf-18" href="./acl-2011-Effective_Measures_of_Domain_Similarity_for_Parsing.html">109 acl-2011-Effective Measures of Domain Similarity for Parsing</a></p>
<p>19 0.031014044 <a title="212-tfidf-19" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>20 0.030856621 <a title="212-tfidf-20" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.091), (1, 0.029), (2, -0.026), (3, 0.023), (4, -0.015), (5, 0.0), (6, 0.005), (7, 0.007), (8, -0.007), (9, 0.017), (10, -0.013), (11, -0.009), (12, -0.01), (13, 0.036), (14, -0.012), (15, 0.014), (16, 0.005), (17, -0.003), (18, 0.02), (19, -0.034), (20, 0.087), (21, -0.019), (22, -0.02), (23, -0.001), (24, -0.021), (25, -0.053), (26, -0.015), (27, -0.017), (28, 0.009), (29, -0.016), (30, -0.05), (31, 0.057), (32, -0.028), (33, 0.098), (34, 0.052), (35, -0.007), (36, -0.069), (37, -0.021), (38, -0.008), (39, 0.137), (40, 0.017), (41, 0.111), (42, -0.017), (43, -0.002), (44, -0.0), (45, -0.02), (46, -0.045), (47, -0.015), (48, 0.001), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89149284 <a title="212-lsi-1" href="./acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution.html">212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</a></p>
<p>Author: Hugo Jair Escalante ; Thamar Solorio ; Manuel Montes-y-Gomez</p><p>Abstract: This paper proposes the use of local histograms (LH) over character n-grams for authorship attribution (AA). LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word histograms. In this work we explore the suitability of LHs over n-grams at the character-level for AA. We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors. We report experimental results in AA data sets that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.</p><p>2 0.74821955 <a title="212-lsi-2" href="./acl-2011-Lost_in_Translation%3A_Authorship_Attribution_using_Frame_Semantics.html">214 acl-2011-Lost in Translation: Authorship Attribution using Frame Semantics</a></p>
<p>Author: Steffen Hedegaard ; Jakob Grue Simonsen</p><p>Abstract: We investigate authorship attribution using classifiers based on frame semantics. The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts. Our results suggest (i) that frame-based classifiers are usable for author attribution of both translated and untranslated texts; (ii) that framebased classifiers generally perform worse than the baseline classifiers for untranslated texts, but (iii) perform as well as, or superior to the baseline classifiers on translated texts; (iv) that—contrary to current belief—naïve clas- sifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.</p><p>3 0.71423972 <a title="212-lsi-3" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>Author: Sara Rosenthal ; Kathleen McKeown</p><p>Abstract: We investigate whether wording, stylistic choices, and online behavior can be used to predict the age category of blog authors. Our hypothesis is that significant changes in writing style distinguish pre-social media bloggers from post-social media bloggers. Through experimentation with a range of years, we found that the birth dates of students in college at the time when social media such as AIM, SMS text messaging, MySpace and Facebook first became popular, enable accurate age prediction. We also show that internet writing characteristics are important features for age prediction, but that lexical content is also needed to produce significantly more accurate results. Our best results allow for 81.57% accuracy.</p><p>4 0.64747053 <a title="212-lsi-4" href="./acl-2011-Towards_Style_Transformation_from_Written-Style_to_Audio-Style.html">306 acl-2011-Towards Style Transformation from Written-Style to Audio-Style</a></p>
<p>Author: Amjad Abu-Jbara ; Barbara Rosario ; Kent Lyons</p><p>Abstract: In this paper, we address the problem of optimizing the style of textual content to make it more suitable to being listened to by a user as opposed to being read. We study the differences between the written style and the audio style by consulting the linguistics andjour- nalism literatures. Guided by this study, we suggest a number of linguistic features to distinguish between the two styles. We show the correctness of our features and the impact of style transformation on the user experience through statistical analysis, a style classification task, and a user study.</p><p>5 0.64164883 <a title="212-lsi-5" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>Author: Philip Bramsen ; Martha Escobar-Molano ; Ami Patel ; Rafael Alonso</p><p>Abstract: Sociolinguists have long argued that social context influences language use in all manner of ways, resulting in lects 1. This paper explores a text classification problem we will call lect modeling, an example of what has been termed computational sociolinguistics. In particular, we use machine learning techniques to identify social power relationships between members of a social network, based purely on the content of their interpersonal communication. We rely on statistical methods, as opposed to language-specific engineering, to extract features which represent vocabulary and grammar usage indicative of social power lect. We then apply support vector machines to model the social power lects representing superior-subordinate communication in the Enron email corpus. Our results validate the treatment of lect modeling as a text classification problem – albeit a hard one – and constitute a case for future research in computational sociolinguistics. 1</p><p>6 0.55765772 <a title="212-lsi-6" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>7 0.54150999 <a title="212-lsi-7" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>8 0.53792977 <a title="212-lsi-8" href="./acl-2011-Unsupervised_Decomposition_of_a_Document_into_Authorial_Components.html">319 acl-2011-Unsupervised Decomposition of a Document into Authorial Components</a></p>
<p>9 0.51747292 <a title="212-lsi-9" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>10 0.5126856 <a title="212-lsi-10" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<p>11 0.4848803 <a title="212-lsi-11" href="./acl-2011-A_New_Dataset_and_Method_for_Automatically_Grading_ESOL_Texts.html">20 acl-2011-A New Dataset and Method for Automatically Grading ESOL Texts</a></p>
<p>12 0.48029 <a title="212-lsi-12" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>13 0.47791228 <a title="212-lsi-13" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>14 0.47044381 <a title="212-lsi-14" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>15 0.45763245 <a title="212-lsi-15" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>16 0.44369507 <a title="212-lsi-16" href="./acl-2011-Hierarchical_Text_Classification_with_Latent_Concepts.html">150 acl-2011-Hierarchical Text Classification with Latent Concepts</a></p>
<p>17 0.43793443 <a title="212-lsi-17" href="./acl-2011-Subjective_Natural_Language_Problems%3A_Motivations%2C_Applications%2C_Characterizations%2C_and_Implications.html">288 acl-2011-Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications</a></p>
<p>18 0.43234855 <a title="212-lsi-18" href="./acl-2011-Automatically_Predicting_Peer-Review_Helpfulness.html">55 acl-2011-Automatically Predicting Peer-Review Helpfulness</a></p>
<p>19 0.42666316 <a title="212-lsi-19" href="./acl-2011-An_Interface_for_Rapid_Natural_Language_Processing_Development_in_UIMA.html">42 acl-2011-An Interface for Rapid Natural Language Processing Development in UIMA</a></p>
<p>20 0.42642647 <a title="212-lsi-20" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.04), (5, 0.018), (17, 0.048), (37, 0.084), (39, 0.033), (41, 0.051), (55, 0.02), (59, 0.031), (72, 0.038), (90, 0.339), (91, 0.036), (96, 0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68853253 <a title="212-lda-1" href="./acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution.html">212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</a></p>
<p>Author: Hugo Jair Escalante ; Thamar Solorio ; Manuel Montes-y-Gomez</p><p>Abstract: This paper proposes the use of local histograms (LH) over character n-grams for authorship attribution (AA). LHs are enriched histogram representations that preserve sequential information in documents; they have been successfully used for text categorization and document visualization using word histograms. In this work we explore the suitability of LHs over n-grams at the character-level for AA. We show that LHs are particularly helpful for AA, because they provide useful information for uncovering, to some extent, the writing style of authors. We report experimental results in AA data sets that confirm that LHs over character n-grams are more helpful for AA than the usual global histograms, yielding results far superior to state of the art approaches. We found that LHs are even more advantageous in challenging conditions, such as having imbalanced and small training sets. Our results motivate further research on the use of LHs for modeling the writing style of authors for related tasks, such as authorship verification and plagiarism detection.</p><p>2 0.64804757 <a title="212-lda-2" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>Author: Oleksandr Kolomiyets ; Steven Bethard ; Marie-Francine Moens</p><p>Abstract: We explore a semi-supervised approach for improving the portability of time expression recognition to non-newswire domains: we generate additional training examples by substituting temporal expression words with potential synonyms. We explore using synonyms both from WordNet and from the Latent Words Language Model (LWLM), which predicts synonyms in context using an unsupervised approach. We evaluate a state-of-the-art time expression recognition system trained both with and without the additional training examples using data from TempEval 2010, Reuters and Wikipedia. We find that the LWLM provides substantial improvements on the Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia. 1</p><p>3 0.60821569 <a title="212-lda-3" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>Author: Aditya Joshi ; Balamurali AR ; Pushpak Bhattacharyya ; Rajat Mohanty</p><p>Abstract: Social networking and micro-blogging sites are stores of opinion-bearing content created by human users. We describe C-Feel-It, a system which can tap opinion content in posts (called tweets) from the micro-blogging website, Twitter. This web-based system categorizes tweets pertaining to a search string as positive, negative or objective and gives an aggregate sentiment score that represents a sentiment snapshot for a search string. We present a qualitative evaluation of this system based on a human-annotated tweet corpus.</p><p>4 0.60752213 <a title="212-lda-4" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>Author: Marius Pasca</p><p>Abstract: The role of search queries, as available within query sessions or in isolation from one another, in examined in the context of ranking the class labels (e.g., brazilian cities, business centers, hilly sites) extracted from Web documents for various instances (e.g., rio de janeiro). The co-occurrence of a class label and an instance, in the same query or within the same query session, is used to reinforce the estimated relevance of the class label for the instance. Experiments over evaluation sets of instances associated with Web search queries illustrate the higher quality of the query-based, re-ranked class labels, relative to ranking baselines using documentbased counts.</p><p>5 0.58586097 <a title="212-lda-5" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>6 0.47990009 <a title="212-lda-6" href="./acl-2011-Predicting_Relative_Prominence_in_Noun-Noun_Compounds.html">249 acl-2011-Predicting Relative Prominence in Noun-Noun Compounds</a></p>
<p>7 0.47755247 <a title="212-lda-7" href="./acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging.html">112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</a></p>
<p>8 0.47341943 <a title="212-lda-8" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>9 0.47185543 <a title="212-lda-9" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>10 0.47160095 <a title="212-lda-10" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>11 0.47134456 <a title="212-lda-11" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>12 0.47037336 <a title="212-lda-12" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>13 0.46902081 <a title="212-lda-13" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>14 0.46884203 <a title="212-lda-14" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>15 0.46841449 <a title="212-lda-15" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>16 0.46798903 <a title="212-lda-16" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>17 0.46789429 <a title="212-lda-17" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>18 0.4674809 <a title="212-lda-18" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>19 0.46713421 <a title="212-lda-19" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>20 0.46695259 <a title="212-lda-20" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
