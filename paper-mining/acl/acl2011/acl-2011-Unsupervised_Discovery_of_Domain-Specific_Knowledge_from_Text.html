<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-320" href="#">acl2011-320</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</h1>
<br/><p>Source: <a title="acl-2011-320-pdf" href="http://aclweb.org/anthology//P/P11/P11-1147.pdf">pdf</a></p><p>Author: Dirk Hovy ; Chunliang Zhang ; Eduard Hovy ; Anselmo Penas</p><p>Abstract: Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicateargument structures like “quarterbacks throw passes to receivers”. Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.</p><p>Reference: <a title="acl-2011-320-reference" href="../acl2011_reference/acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. [sent-5, score-0.341]
</p><p>2 4m sentences, we learn about 250k simple propositions about American football in the form of predicateargument structures like “quarterbacks throw passes to receivers”. [sent-7, score-0.972]
</p><p>3 The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems. [sent-11, score-0.293]
</p><p>4 Our system automatically acquires domainspecific knowledge (classes and actions) from large amounts of unlabeled data, and trains a probabilistic model to determine and apply the most informative classes (quarterback, etc. [sent-21, score-0.329]
</p><p>5 , from sentences such as “Steve Young threw a pass to Michael Holt”, “Quarterback Steve Young finished strong”, and “Michael Holt, the receiver, left early” we can learn the classes quarterback and receiver, and the proposition “quarterbacks throw passes to receivers”. [sent-25, score-1.028]
</p><p>6 We will thus assume that the implicit knowledge comes in two forms: actions in the form of predicate-argument structures, and classes as part of the source data. [sent-26, score-0.272]
</p><p>7 Our approach produces simple propositions about the domain (see Figure 1 for examples of ac-  tual propositions learned by our system). [sent-29, score-1.315]
</p><p>8 American football was the first official evaluation domain in the DARPA-sponsored Machine Reading program, and provides the background for a number ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. [sent-30, score-0.151]
</p><p>9 by our system for the American football domain Our approach differs from verb-argument identification or Named Entity (NE) tagging in several respects. [sent-39, score-0.151]
</p><p>10 , 2010) uses fixed sets of classes, we cannot know a priori how many and which classes we will encounter. [sent-42, score-0.237]
</p><p>11 We therefore provide a way to derive the appropriate classes automatically and include a probability distribution for each of them. [sent-43, score-0.334]
</p><p>12 While a NE-  tagged corpus could produce a general proposition like “PERSON throws to PERSON”, our method enables us to distinguish the arguments and learn “quarterback throws to receiver” for American football and “outfielder throws to third base” for baseball. [sent-45, score-0.601]
</p><p>13 While in NE tagging each word has only one correct tag in a given context, we have hierarchical classes: an entity can be correctly labeled as a player or a quarterback (and possibly many more classes), depending on the context. [sent-46, score-0.304]
</p><p>14 By taking context into account, we are also able to label each sentence individually and account for unseen entities without using external resources. [sent-47, score-0.137]
</p><p>15 This is an instance of the underlying proposition “quarterbacks throw passes  to receivers”, which is not explicitly stated in the data. [sent-50, score-0.461]
</p><p>16 A proposition is thus a more general statement about the domain than the sentences it derives. [sent-51, score-0.336]
</p><p>17 It contains domain-specific classes (quarterback, receiver), as well as lexical items (“throw”, “pass”). [sent-52, score-0.237]
</p><p>18 To facilitate extraction, we focus on propositions with the following predicate-argument structures: NOUN-VERB-NOUN (e. [sent-55, score-0.642]
</p><p>19 Given a sentence, we want to find the most likely class for each word and thereby derive the most likely proposition. [sent-64, score-0.133]
</p><p>20 (2006), we assume the observed data was produced by a process that generates the proposition and then transforms  the classes into a sentence, possibly adding additional words. [sent-66, score-0.514]
</p><p>21 2 Deriving Classes To derive the classes used for entities, we do not restrict ourselves to a fixed set, but derive a domainspecific set directly from the data. [sent-85, score-0.415]
</p><p>22 While we find it straightforward to collect classes for entities in this way, we did not find similar patterns for verbs. [sent-89, score-0.341]
</p><p>23 ”), and copula verbs (“Steve Young is the quarterback of the 49ers”). [sent-97, score-0.243]
</p><p>24 We extract those cooccurrences and store the proper nouns as entities and the common nouns as their possible classes. [sent-98, score-0.156]
</p><p>25 The total number of distinct classes for entities is 63, 942. [sent-107, score-0.374]
</p><p>26 1 Instead of manually choosing a subset of the classes we extracted, we defer the task of finding the best set to the model. [sent-109, score-0.237]
</p><p>27 We note, however, that the distribution of classes  for each entity is highly skewed. [sent-110, score-0.329]
</p><p>28 Due to the unsupervised nature of the extraction process, many of the extracted classes are hapaxes and/or random noise. [sent-111, score-0.268]
</p><p>29 Most entities have only a small number of applicable classes (a football player usually has one main posi1NE taggers usually use a set of only a few dozen classes at most. [sent-112, score-0.698]
</p><p>30 We handle this by limiting the number of classes considered to 3 per entity. [sent-115, score-0.263]
</p><p>31 This constraint reduces the total number of distinct classes to 26, 165, and the average number of classes per entity to 2. [sent-116, score-0.625]
</p><p>32 , 2007), or WordNet++ (Ponzetto and Navigli, 2010) to select the most appropriate classes for each entity. [sent-121, score-0.263]
</p><p>33 This is likely to have a positive effect on the quality of the applicable classes and merits further research. [sent-122, score-0.237]
</p><p>34 The number of classes we consider for each entity  also influences the number of possible propositions: if we consider exactly one class per entity, there will be little overlap between sentences, and thus no generalization possible—the model will produce many distinct propositions. [sent-124, score-0.538]
</p><p>35 , sn) was generated assumes that a proposition p is generated as a se-  quence of classes p1, . [sent-131, score-0.514]
</p><p>36 Each class pi generates a word si with probability P(si |pi). [sent-135, score-0.167]
</p><p>37 We allow additional words x in the sentence w|hpich do not depend on any class in the proposition and are thus generated inde1469 pendently with P(x) (cf. [sent-136, score-0.339]
</p><p>38 Since we observe the co-occurrence counts of classes and entities in the data, we can fix the emission parameter P(s|p) in our HMM. [sent-138, score-0.341]
</p><p>39 (1) where si, pi denote the ith word of sentence s and proposition p, respectively. [sent-145, score-0.357]
</p><p>40 3  Evaluation  We want to evaluate how well our model predicts the data, and how sensible the resulting propositions are. [sent-146, score-0.775]
</p><p>41 First, since we derive the classes in a data-driven way, we have no gold standard data available for comparison. [sent-149, score-0.308]
</p><p>42 However, while a proposition such as “PERSON does THING”, has excellent generality, it possesses no discriminating power. [sent-156, score-0.277]
</p><p>43 We also need the propositions to partition the sentences into clusters of semantic similarity, to support effective inference. [sent-157, score-0.67]
</p><p>44 We need to find an appropriate level of generality within which the sentences are clustered into propositions for the best overall groupings to support inference. [sent-160, score-0.73]
</p><p>45 Further, to assess label accuracy, we use Amazon’s Mechanical Turk annotators to judge the sensibility of the propositions produced by each system (Section 3. [sent-167, score-0.948]
</p><p>46 We reason that if our system learned to infer the correct classes, then the resulting propositions should constitute true, general statements about that domain, and thus be judged as sensible. [sent-169, score-0.747]
</p><p>47 We create two baseline systems from the same corpus, one which uses the most frequent class (MFC) for each entity, and another one which uses a class picked at random from the applicable classes of each entity. [sent-176, score-0.389]
</p><p>48 Ultimately, we are interested in labeling unseen data from the same domain with the correct class, so we evaluate separately on the full corpus and the subset of sentences that contain unknown entities (i. [sent-177, score-0.295]
</p><p>49 , entities for which no class information was available in the corpus, cf. [sent-179, score-0.166]
</p><p>50 Here, we have to consider a much larger set of possible classes per entity (the 20 overall most frequent classes). [sent-183, score-0.355]
</p><p>51 The MFC baseline for these cases is the most frequent of the 20 classes for UNK tokens, while the random baseline chooses randomly from that set. [sent-184, score-0.32]
</p><p>52 2  Generalization  Generalization measures how widely applicable the produced propositions are. [sent-186, score-0.674]
</p><p>53 A completely lexical ap2Unfortunately, if judged insensible, we can not infer whether our model used the wrong class despite better options, or whether we simply have not learned the correct label. [sent-187, score-0.192]
</p><p>54 At the other extreme, a model that produces only one proposition would generalize ex-  tremely well (but would fail to explain the data in any meaningful way). [sent-196, score-0.302]
</p><p>55 We define generalization as  g = 1 −|p|sroenpotesintcieosns||  (2)  The results in Figure 4 show that our model is capable of abstracting away from the lexical form, achieving a generalization rate of 25% for the full data set. [sent-198, score-0.18]
</p><p>56 The random baseline chooses between 20 classes per entity instead of 3, and is thus even less general. [sent-207, score-0.41]
</p><p>57 The extreme case of only one proposition has 0 entropy: 1. [sent-213, score-0.304]
</p><p>58 Entropy is directly influenced by the number of propositions used by a system. [sent-221, score-0.642]
</p><p>59 The best entropy we can hope to achieve given the number of propositions and sentences is actually 0. [sent-225, score-0.723]
</p><p>60 This might be due to the fact that we considered more classes for UNK than for regular entities. [sent-231, score-0.237]
</p><p>61 4  Perplexity  Since we assume that propositions are valid sentences in our domain, good propositions should have a higher probability than bad propositions in a lan-  guage model. [sent-233, score-1.954]
</p><p>62 We can compute this using perplex3Note that how many classes we consider per entity influences how many propositions are produced (cf. [sent-234, score-0.997]
</p><p>63 s92rmMaoFndCeolm Figure 6: Perplexity of models on the data sets  ity:4 perplexity(data)  =  2− log Pn(data)  (4)  where P(data) is the product of the proposition probabilities, and n is the number of propositions. [sent-244, score-0.277]
</p><p>64 The results in Figure 6 indicate that the propositions found by the model are preferable to the ones found by the baselines. [sent-246, score-0.667]
</p><p>65 As would be expected, the sensibility judgements for MFC and model5 (Tables 1 and 2, Section 3. [sent-247, score-0.187]
</p><p>66 We evaluate label accuracy by presenting subjects with the propositions we obtained from the Viterbi decoding of the corpus, and ask them to rate their sensibility. [sent-252, score-0.674]
</p><p>67 We compare the different systems by computing sensibility as the percentage of propositions judged sensible for each system. [sent-253, score-1.017]
</p><p>68 Since the underlying probability distributions are quite different, we weight the sensibility judge-  ment for each proposition by the likelihood of that proposition. [sent-254, score-0.464]
</p><p>69 5We did not collect sensibility judgements for the random baseline. [sent-256, score-0.187]
</p><p>70 1367 Table 1: Percentage of propositions derived from labeling the full data set that were judged sensible  Data setSystem10a0g gmost freqmuaejntaggrandommajagcgombinemdaj  unknownbmasoedleinle6561. [sent-269, score-0.967]
</p><p>71 7666 Table 2: Percentage of propositions derived from labeling unknown entities that were judged sensible  sensibility (using the total number of individual answers), and majority sensibility, where each proposition is scored according to the majority of annotators’ decisions. [sent-281, score-1.468]
</p><p>72 The model and baseline propositions for the full data set are both judged highly sensible, achieving accuracies of 96. [sent-282, score-0.804]
</p><p>73 The propositions produced by the model from unknown entities are less sensible (67. [sent-287, score-0.919]
</p><p>74 8%), albeit still significantly above chance level, and the baseline  propositions for the same data set (p < 0. [sent-288, score-0.699]
</p><p>75 For each system, we sample the 100 most frequent propositions and 100 random propositions found for both the full data set and the unknown entities6 and have 10 annotators rate each proposition as sensible or insensible. [sent-297, score-1.83]
</p><p>76 The 200 propositions from each of the four sys6We omit the random baseline here due to size issues, and because it is not likely to produce any informative comparison. [sent-304, score-0.698]
</p><p>77 We  break these up into 70 batches (Amazon Turk annotation HIT pages) of ten propositions each. [sent-306, score-0.642]
</p><p>78 The annotators are asked to state whether each proposition represents a sensible statement about American Football or not. [sent-309, score-0.477]
</p><p>79 A proposition like “Quarterbacks can throw passes to receivers” should make sense, while “Coaches can intercept teams” does not. [sent-310, score-0.461]
</p><p>80 To ensure that annotators judge sensibility and not grammaticalitPy,a we 1e format each proposition the same way, namely pluralizing the nouns and adding “can” before the verb. [sent-311, score-0.609]
</p><p>81 In addition, annotators can state whether a proposition sounds odd, seems ungrammatical, is a valid sentence, but against the rules (e. [sent-312, score-0.369]
</p><p>82 To identify those outliers, we compare each annotator’s agreement to the others and exclude those whose agreement falls more than one standard deviation below the average overall agreement. [sent-323, score-0.156]
</p><p>83 Apart from the simple agreement measure, which records how often annotators choose the same value for an item, there are several statistics that qualify this measure by adjusting for other factors. [sent-335, score-0.17]
</p><p>84 , 2003), classes have been derived from FrameNet (Baker et al. [sent-355, score-0.237]
</p><p>85 For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al. [sent-357, score-0.237]
</p><p>86 Pre-tagging the data with NE classes before training comes at a cost. [sent-362, score-0.237]
</p><p>87 It lumps entities together which can have very different classes (i. [sent-363, score-0.341]
</p><p>88 (2005) resolve the  problem with a web-based approach that learns hierarchies of the NE classes in an unsupervised manner. [sent-367, score-0.268]
</p><p>89 We do not enforce a taxonomy, but include statistical knowledge about the distribution of possible classes over each entity by incorporating a prior distribution P(class, entity). [sent-368, score-0.329]
</p><p>90 In addition, we can distinguish several classes for each entity, depending on the context (e. [sent-370, score-0.237]
</p><p>91 (2010) also use an unsupervised model to derive selectional predicates from unlabeled text. [sent-375, score-0.158]
</p><p>92 They do not assign classes altogether, but group similar predicates and arguments into unlabeled clusters using LDA. [sent-376, score-0.268]
</p><p>93 Pe˜ nas and Hovy (2010) employ syntactic patterns to derive classes from unlabeled data in the context of LbR. [sent-378, score-0.368]
</p><p>94 5  Conclusion  We use an unsupervised model to infer domainspecific classes from a corpus of 1. [sent-380, score-0.354]
</p><p>95 4m unlabeled sentences, and applied them to learn 250k propositions about American football. [sent-381, score-0.673]
</p><p>96 Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive selection of appropriate classes. [sent-382, score-0.367]
</p><p>97 We evaluate both the model qualities and sensibility of the resulting propositions. [sent-383, score-0.212]
</p><p>98 Several measures show that the model has good explanatory power and generalizes well, significantly outperforming two baseline approaches, especially where the possible classes of an entity can only be inferred from the context. [sent-384, score-0.474]
</p><p>99 6% of the propositions for the full data set, and 67. [sent-386, score-0.671]
</p><p>100 The probabilistic model and the extracted propositions can be used to enrich texts and support postparsing inference for question answering. [sent-392, score-0.667]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('propositions', 0.642), ('proposition', 0.277), ('classes', 0.237), ('quarterback', 0.212), ('sensibility', 0.187), ('lbr', 0.135), ('throw', 0.126), ('football', 0.12), ('quarterbacks', 0.116), ('sensible', 0.108), ('entities', 0.104), ('mfc', 0.097), ('pardo', 0.097), ('annotators', 0.092), ('entity', 0.092), ('receivers', 0.085), ('pi', 0.08), ('judged', 0.08), ('agreement', 0.078), ('unk', 0.074), ('young', 0.072), ('derive', 0.071), ('throws', 0.068), ('generalization', 0.063), ('class', 0.062), ('steve', 0.059), ('passes', 0.058), ('perplexity', 0.056), ('receiver', 0.056), ('spammers', 0.056), ('reading', 0.053), ('entropy', 0.053), ('pe', 0.051), ('threw', 0.051), ('holt', 0.051), ('ne', 0.043), ('outliers', 0.04), ('unknown', 0.04), ('pass', 0.039), ('circuits', 0.039), ('coaches', 0.039), ('feinstein', 0.039), ('freqmuaejntaggrandommajagcgombinemdaj', 0.039), ('gmost', 0.039), ('holley', 0.039), ('spammer', 0.039), ('twhaet', 0.039), ('hovy', 0.038), ('turk', 0.037), ('ritter', 0.036), ('domainspecific', 0.036), ('actions', 0.035), ('eduard', 0.035), ('generality', 0.034), ('anselmo', 0.034), ('appositions', 0.034), ('sandhaus', 0.034), ('explanatory', 0.034), ('framenet', 0.033), ('unseen', 0.033), ('distinct', 0.033), ('measures', 0.032), ('subjects', 0.032), ('formalisms', 0.032), ('enrichment', 0.031), ('copula', 0.031), ('fleischman', 0.031), ('unlabeled', 0.031), ('unsupervised', 0.031), ('domain', 0.031), ('game', 0.03), ('labeling', 0.03), ('nas', 0.029), ('chance', 0.029), ('full', 0.029), ('fan', 0.029), ('raw', 0.028), ('sentences', 0.028), ('ponzetto', 0.028), ('brody', 0.028), ('baseline', 0.028), ('omit', 0.028), ('judge', 0.027), ('extreme', 0.027), ('chooses', 0.027), ('winner', 0.027), ('suchanek', 0.027), ('yago', 0.027), ('nouns', 0.026), ('methodology', 0.026), ('per', 0.026), ('appropriate', 0.026), ('structures', 0.026), ('strassel', 0.026), ('mechanical', 0.026), ('generalizes', 0.026), ('amazon', 0.026), ('model', 0.025), ('si', 0.025), ('infer', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="320-tfidf-1" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>Author: Dirk Hovy ; Chunliang Zhang ; Eduard Hovy ; Anselmo Penas</p><p>Abstract: Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicateargument structures like “quarterbacks throw passes to receivers”. Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.</p><p>2 0.10411684 <a title="320-tfidf-2" href="./acl-2011-Classifying_arguments_by_scheme.html">68 acl-2011-Classifying arguments by scheme</a></p>
<p>Author: Vanessa Wei Feng ; Graeme Hirst</p><p>Abstract: Argumentation schemes are structures or templates for various kinds of arguments. Given the text of an argument with premises and conclusion identified, we classify it as an instance ofone offive common schemes, using features specific to each scheme. We achieve accuracies of 63–91% in one-against-others classification and 80–94% in pairwise classification (baseline = 50% in both cases).</p><p>3 0.098326549 <a title="320-tfidf-3" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>4 0.090556614 <a title="320-tfidf-4" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>Author: Xianpei Han ; Le Sun</p><p>Abstract: Linking entities with knowledge base (entity linking) is a key issue in bridging the textual data with the structural knowledge base. Due to the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the heterogenous knowledge of entities. In this paper, we propose a generative probabilistic model, called entitymention model, which can leverage heterogenous entity knowledge (including popularity knowledge, name knowledge and context knowledge) for the entity linking task. In our model, each name mention to be linked is modeled as a sample generated through a three-step generative story, and the entity knowledge is encoded in the distribution of entities in document P(e), the distribution of possible names of a specific entity P(s|e), and the distribution of possible contexts of a specific entity P(c|e). To find the referent entity of a name mention, our method combines the evidences from all the three distributions P(e), P(s|e) and P(c|e). Experimental results show that our method can significantly outperform the traditional methods. 1</p><p>5 0.06951116 <a title="320-tfidf-5" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>Author: Eneko Agirre ; Kepa Bengoetxea ; Koldo Gojenola ; Joakim Nivre</p><p>Abstract: This paper presents the introduction of WordNet semantic classes in a dependency parser, obtaining improvements on the full Penn Treebank for the first time. We tried different combinations of some basic semantic classes and word sense disambiguation algorithms. Our experiments show that selecting the adequate combination of semantic features on development data is key for success. Given the basic nature of the semantic classes and word sense disambiguation algorithms used, we think there is ample room for future improvements. 1</p><p>6 0.068567805 <a title="320-tfidf-6" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>7 0.067657128 <a title="320-tfidf-7" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>8 0.066518277 <a title="320-tfidf-8" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>9 0.066037379 <a title="320-tfidf-9" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>10 0.065652683 <a title="320-tfidf-10" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>11 0.065543875 <a title="320-tfidf-11" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>12 0.064350791 <a title="320-tfidf-12" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>13 0.063891768 <a title="320-tfidf-13" href="./acl-2011-Types_of_Common-Sense_Knowledge_Needed_for_Recognizing_Textual_Entailment.html">315 acl-2011-Types of Common-Sense Knowledge Needed for Recognizing Textual Entailment</a></p>
<p>14 0.063502192 <a title="320-tfidf-14" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>15 0.063269995 <a title="320-tfidf-15" href="./acl-2011-Extending_the_Entity_Grid_with_Entity-Specific_Features.html">129 acl-2011-Extending the Entity Grid with Entity-Specific Features</a></p>
<p>16 0.062732466 <a title="320-tfidf-16" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>17 0.061990231 <a title="320-tfidf-17" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>18 0.059278559 <a title="320-tfidf-18" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>19 0.056385458 <a title="320-tfidf-19" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>20 0.054340575 <a title="320-tfidf-20" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, 0.041), (2, -0.083), (3, 0.001), (4, 0.014), (5, 0.003), (6, 0.03), (7, -0.012), (8, -0.048), (9, -0.008), (10, 0.016), (11, -0.014), (12, -0.027), (13, 0.034), (14, 0.013), (15, -0.02), (16, -0.047), (17, 0.03), (18, -0.018), (19, -0.015), (20, 0.08), (21, 0.025), (22, -0.036), (23, -0.059), (24, 0.021), (25, -0.027), (26, -0.01), (27, -0.029), (28, 0.006), (29, 0.038), (30, -0.038), (31, -0.062), (32, 0.024), (33, 0.009), (34, 0.102), (35, 0.081), (36, 0.075), (37, -0.061), (38, 0.028), (39, -0.044), (40, -0.002), (41, 0.068), (42, -0.085), (43, -0.011), (44, -0.081), (45, -0.073), (46, -0.05), (47, 0.031), (48, -0.063), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91244698 <a title="320-lsi-1" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>Author: Dirk Hovy ; Chunliang Zhang ; Eduard Hovy ; Anselmo Penas</p><p>Abstract: Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicateargument structures like “quarterbacks throw passes to receivers”. Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.</p><p>2 0.60855407 <a title="320-lsi-2" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>3 0.60854113 <a title="320-lsi-3" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>Author: S.R.K Branavan ; David Silver ; Regina Barzilay</p><p>Abstract: This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built- . in AI of Civilization II. 1</p><p>4 0.56715298 <a title="320-lsi-4" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>5 0.55642599 <a title="320-lsi-5" href="./acl-2011-Even_the_Abstract_have_Color%3A_Consensus_in_Word-Colour_Associations.html">120 acl-2011-Even the Abstract have Color: Consensus in Word-Colour Associations</a></p>
<p>Author: Saif Mohammad</p><p>Abstract: Colour is a key component in the successful dissemination of information. Since many real-world concepts are associated with colour, for example danger with red, linguistic information is often complemented with the use of appropriate colours in information visualization and product marketing. Yet, there is no comprehensive resource that captures concept–colour associations. We present a method to create a large word–colour association lexicon by crowdsourcing. A wordchoice question was used to obtain sense-level annotations and to ensure data quality. We focus especially on abstract concepts and emotions to show that even they tend to have strong colour associations. Thus, using the right colours can not only improve semantic coherence, but also inspire the desired emotional response.</p><p>6 0.54948288 <a title="320-lsi-6" href="./acl-2011-Classifying_arguments_by_scheme.html">68 acl-2011-Classifying arguments by scheme</a></p>
<p>7 0.54887897 <a title="320-lsi-7" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>8 0.54838794 <a title="320-lsi-8" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<p>9 0.52896821 <a title="320-lsi-9" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>10 0.52655989 <a title="320-lsi-10" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>11 0.52506024 <a title="320-lsi-11" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>12 0.52307069 <a title="320-lsi-12" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>13 0.51376581 <a title="320-lsi-13" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>14 0.50572079 <a title="320-lsi-14" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>15 0.50029081 <a title="320-lsi-15" href="./acl-2011-Semi-Supervised_Frame-Semantic_Parsing_for_Unknown_Predicates.html">274 acl-2011-Semi-Supervised Frame-Semantic Parsing for Unknown Predicates</a></p>
<p>16 0.49796417 <a title="320-lsi-16" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>17 0.49381483 <a title="320-lsi-17" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>18 0.49006644 <a title="320-lsi-18" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>19 0.48690802 <a title="320-lsi-19" href="./acl-2011-Contrasting_Opposing_Views_of_News_Articles_on_Contentious_Issues.html">84 acl-2011-Contrasting Opposing Views of News Articles on Contentious Issues</a></p>
<p>20 0.47929826 <a title="320-lsi-20" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.025), (16, 0.217), (17, 0.05), (26, 0.029), (37, 0.08), (39, 0.06), (41, 0.066), (55, 0.048), (59, 0.063), (72, 0.053), (91, 0.033), (96, 0.149), (97, 0.029), (98, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92235625 <a title="320-lda-1" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>Author: Yunyao Li ; Frederick Reiss ; Laura Chiticariu</p><p>Abstract: Frederick R. Reiss IBM Research - Almaden 650 Harry Road San Jose, CA 95120 frre i s @us . ibm . com s Laura Chiticariu IBM Research - Almaden 650 Harry Road San Jose, CA 95120 chit i us .ibm . com @ magnitude larger than classical IE corpora. An Emerging text-intensive enterprise applications such as social analytics and semantic search pose new challenges of scalability and usability to Information Extraction (IE) systems. This paper presents SystemT, a declarative IE system that addresses these challenges and has been deployed in a wide range of enterprise applications. SystemT facilitates the development of high quality complex annotators by providing a highly expressive language and an advanced development environment. It also includes a cost-based optimizer and a high-performance, flexible runtime with minimum memory footprint. We present SystemT as a useful resource that is freely available, and as an opportunity to promote research in building scalable and usable IE systems.</p><p>2 0.86739945 <a title="320-lda-2" href="./acl-2011-A_Cross-Lingual_ILP_Solution_to_Zero_Anaphora_Resolution.html">9 acl-2011-A Cross-Lingual ILP Solution to Zero Anaphora Resolution</a></p>
<p>Author: Ryu Iida ; Massimo Poesio</p><p>Abstract: We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. We show that this new model outperforms several baselines and competing models, as well as a direct translation of the Denis / Baldridge model, for both Italian and Japanese zero anaphora. We incorporate our model in complete anaphoric resolvers for both Italian and Japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classifiers are used for zeros and for ex- plicitly realized anaphors.</p><p>same-paper 3 0.81393439 <a title="320-lda-3" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>Author: Dirk Hovy ; Chunliang Zhang ; Eduard Hovy ; Anselmo Penas</p><p>Abstract: Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicateargument structures like “quarterbacks throw passes to receivers”. Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.</p><p>4 0.79881644 <a title="320-lda-4" href="./acl-2011-Simple_English_Wikipedia%3A_A_New_Text_Simplification_Task.html">283 acl-2011-Simple English Wikipedia: A New Text Simplification Task</a></p>
<p>Author: William Coster ; David Kauchak</p><p>Abstract: In this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based trans- lation approach for simplification.</p><p>5 0.76998276 <a title="320-lda-5" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>Author: Or Biran ; Samuel Brody ; Noemie Elhadad</p><p>Abstract: We present a method for lexical simplification. Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.</p><p>6 0.69453067 <a title="320-lda-6" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>7 0.69145995 <a title="320-lda-7" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>8 0.68909025 <a title="320-lda-8" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>9 0.68870795 <a title="320-lda-9" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>10 0.6868186 <a title="320-lda-10" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>11 0.68639427 <a title="320-lda-11" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>12 0.68612206 <a title="320-lda-12" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>13 0.68490469 <a title="320-lda-13" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>14 0.68434405 <a title="320-lda-14" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>15 0.68243456 <a title="320-lda-15" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>16 0.68158269 <a title="320-lda-16" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>17 0.68139756 <a title="320-lda-17" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>18 0.68138361 <a title="320-lda-18" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>19 0.68038404 <a title="320-lda-19" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>20 0.67973506 <a title="320-lda-20" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
