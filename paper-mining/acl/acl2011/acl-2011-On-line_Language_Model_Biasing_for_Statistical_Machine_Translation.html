<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-233" href="#">acl2011-233</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2011-233-pdf" href="http://aclweb.org/anthology//P/P11/P11-2078.pdf">pdf</a></p><p>Author: Sankaranarayanan Ananthakrishnan ; Rohit Prasad ; Prem Natarajan</p><p>Abstract: The language model (LM) is a critical component in most statistical machine translation (SMT) systems, serving to establish a probability distribution over the hypothesis space. Most SMT systems use a static LM, independent of the source language input. While previous work has shown that adapting LMs based on the input improves SMT performance, none of the techniques has thus far been shown to be feasible for on-line systems. In this paper, we develop a novel measure of cross-lingual similarity for biasing the LM based on the test input. We also illustrate an efficient on-line implementation that supports integration with on-line SMT systems by transferring much of the computational load off-line. Our approach yields significant reductions in target perplexity compared to the static LM, as well as consistent improvements in SMT performance across language pairs (English-Dari and English-Pashto).</p><p>Reference: <a title="acl-2011-233-reference" href="../acl2011_reference/acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract The language model (LM) is a critical component in most statistical machine translation (SMT) systems, serving to establish a probability distribution over the hypothesis space. [sent-5, score-0.1]
</p><p>2 Most SMT systems use a static LM, independent of the source language input. [sent-6, score-0.346]
</p><p>3 While previous work has shown that adapting LMs based on the input improves SMT performance, none of the techniques has thus far been shown to be feasible for on-line systems. [sent-7, score-0.031]
</p><p>4 In this paper, we develop a novel measure of cross-lingual similarity for biasing the LM based on the test input. [sent-8, score-0.432]
</p><p>5 Our approach yields significant reductions in target perplexity compared to the static LM, as well as consistent improvements in SMT performance across language pairs (English-Dari and English-Pashto). [sent-10, score-0.738]
</p><p>6 1 Introduction While much of the focus in developing a statistical machine translation (SMT) system revolves around the translation model (TM), most systems do not emphasize the role of the language model (LM). [sent-11, score-0.2]
</p><p>7 The latter generally follows a n-gram structure and is estimated from a large, monolingual corpus of target sentences. [sent-12, score-0.309]
</p><p>8 In most systems, the LM is independent of the test input, i. [sent-13, score-0.043]
</p><p>9 fixed n-gram probabilities determine the likelihood of all translation hypotheses, regardless of the source input. [sent-15, score-0.216]
</p><p>10 445 Some previous work exists in LM adaptation for SMT. [sent-19, score-0.035]
</p><p>11 (2008) used a cross-lingual information retrieval (CLIR) system to select a subset of target documents “comparable” to the source document; bias LMs estimated from these subsets were interpolated with a static background LM. [sent-21, score-0.698]
</p><p>12 (2004) converted initial SMT hypotheses to queries and retrieved similar sentences from a large monolingual collection. [sent-23, score-0.109]
</p><p>13 The latter were used to  build source-specific LMs that were then interpolated with a background model. [sent-24, score-0.081]
</p><p>14 While feasible in offline evaluations where the test set is relatively static, the above techniques are computationally expensive and therefore not suitable for low-latency, interactive applications of SMT. [sent-26, score-0.164]
</p><p>15 Examples include speechto-speech and web-based interactive translation systems, where test inputs are user-generated and preclude off-line LM adaptation. [sent-27, score-0.234]
</p><p>16 In this paper, we present a novel technique for weighting a LM corpus at the sentence level based on the source language input. [sent-28, score-0.238]
</p><p>17 The weighting scheme relies on a measure of cross-lingual similarity evaluated by projecting sparse vector representations of the target sentences into the space of source sentences using a transformation matrix computed from the bilingual parallel data. [sent-29, score-0.927]
</p><p>18 The LM estimated from this weighted corpus boosts the probability of relevant target n-grams, while attenuating unrelated target segments. [sent-30, score-0.47]
</p><p>19 Our formulation, based on simple ideas in linear algebra, alleviates run-time complexity by pre-computing the majority of intermediate products off-line. [sent-31, score-0.067]
</p><p>20 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 445–449, 2  Cross-Lingual Similarity  We propose a novel measure of cross-lingual similarity that evaluates the likeness between an arbitrary pair of source and target language sentences. [sent-34, score-0.512]
</p><p>21 The proposed approach represents the source and target sentences in sparse vector spaces defined by their corresponding vocabularies, and relies on a bilingual projection matrix to transform vectors in the target language space to the source language space. [sent-35, score-0.952]
</p><p>22 , tN} represent Sthe = source and target language vocabul}arrieeps-. [sent-42, score-0.315]
</p><p>23 Let u represent the candidate source sentence in a M-dimensional vector space, whose mth dimension um represents the count ofvocabulary item sm in the sentence. [sent-43, score-0.366]
</p><p>24 Similarly, v represents the candidate target sentence in a N-dimensional vector space. [sent-44, score-0.355]
</p><p>25 Traditionally, the cosine similarity measure is used to evaluate the likeness of two term-frequency representations. [sent-46, score-0.197]
</p><p>26 Thus, it is necessary to find a projection of  ×  v in the source vocabulary vector space before similarity can be evaluated. [sent-48, score-0.343]
</p><p>27 Assuming we are able to compute a M NdimAesnssuimoninalg bilingual awbolerd t co-occurrence Mma ×trix N NΣfrom the SMT parallel corpus, the matrix-vector product = Σv is a projection of the target sentence in the source vector space. [sent-49, score-0.596]
</p><p>28 Those source terms of the M-dimensional vector will be emphasized that most frequently co-occur with the target terms in v. [sent-50, score-0.384]
</p><p>29 In other words, can be interpreted as a “bagof-words” translation of v. [sent-51, score-0.129]
</p><p>30 The cross-lingual similarity between the candidate source and target sentences then reduces to the cosine similarity between the source term-frequency vector u and the projected target term-frequency vector as shown in Equation 2. [sent-52, score-1.06]
</p><p>31 1)  In the above equation, we ensure that both u and are normalized to unit L2-norm. [sent-54, score-0.039]
</p><p>32 This prevents  over- or under-estimation of cross-lingual similarity due to sentence length mismatch. [sent-55, score-0.126]
</p><p>33 446 We estimate the bilingual word co-occurrence matrix Σ from an unsupervised, automatic word alignment induced over the parallel training corpus P. [sent-56, score-0.197]
</p><p>34 , 1999) eto u eest tihmea GteI tAhe+ parameters -oOf nIBaizMan M etod ale. [sent-58, score-0.058]
</p><p>35 , 1993), and combine the forward and backward Viterbi alignments to obtain many-tomany word alignments as described in Koehn et al. [sent-60, score-0.16]
</p><p>36 The (m, n)th entry Σm,n of this matrix is the number of times source word sm aligns to target word tn in P. [sent-62, score-0.454]
</p><p>37 3  Language Model Biasing  In traditional LM training, n-gram counts are evaluated assuming unit weight for each sentence. [sent-63, score-0.123]
</p><p>38 Our approach to LM biasing involves re-distributing these weights to favor target sentences that are “similar” to the candidate source sentence according to the measure of cross-lingual similarity developed in Section 2. [sent-64, score-0.834]
</p><p>39 Thus, n-grams that appear in the trans-  lation hypothesis for the candidate input will be assigned high probability by the biased LM, and viceversa. [sent-65, score-0.356]
</p><p>40 Let u be the term-frequency representation of the candidate source sentence for which the LM must be biased. [sent-66, score-0.203]
</p><p>41 ,vK} similarly represent hthee Ket target tLorMs training sent}ensc iems. [sent-70, score-0.199]
</p><p>42 i aWrley compute the similarity of the source sentence u to each target sentence vj according to Equation 3. [sent-71, score-0.587]
</p><p>43 1)  The biased LM is estimated by weighting n-gram counts collected from the jth target sentence with the corresponding cross-lingual similarity ωj. [sent-73, score-0.796]
</p><p>44 In order to alleviate the run-time complexity of on-line LM biasing, we present an efficient method for obtaining biased counts of an arbitrary target n-gram t. [sent-75, score-0.579]
</p><p>45 Let ω = [ω1, be the vector representing crosslingual similarity between the candidate source sentence and each of the K target sentences. [sent-84, score-0.639]
</p><p>46 Then, the biased count of this n-gram, denoted by C∗ (t), is given by Equation i3s. [sent-85, score-0.335]
</p><p>47 2)  The vector bt can be interpreted as the projection oftarget n-gram t in the source space. [sent-90, score-0.397]
</p><p>48 Note that bt is  independent of the source input u, and can therefore be pre-computed off-line. [sent-91, score-0.204]
</p><p>49 At run-time, the biased count of any n-gram can be obtained via a simple dot product. [sent-92, score-0.335]
</p><p>50 This adds very little on-line time complexity because u is a sparse vector. [sent-93, score-0.097]
</p><p>51 Since bt is technically a dense vector, the space complexity of this approach may seem very high. [sent-94, score-0.155]
</p><p>52 In practice, the mass of bt is concentrated around a very small number of source words that frequently co-occur with target ngram t; thus, it can be “sparsified” with little or no loss of information by simply establishing a cutoff threshold on its elements. [sent-95, score-0.517]
</p><p>53 Biased counts and probabilities can be computed on demand for specific ngrams without re-estimating the entire LM. [sent-96, score-0.042]
</p><p>54 We conduct experiments on two resource-poor language pairs commissioned under the DARPA Transtac speech-to-speech translation initiative, viz. [sent-98, score-0.128]
</p><p>55 English-Dari (E2D) and  English-Pashto (E2P), on test sets with single as well as multiple references. [sent-99, score-0.043]
</p><p>56 447  LT DeM astvTea(lr41osa-epirtmneif) gnt-123E,7 89D1k0sp eanirtsenc2513E,6023184Pk5s3apm eaniprtsle ncs  Table 1: Data configuration for perplexity/SMT experiments. [sent-100, score-0.03]
</p><p>57 1 Data Configuration Parallel data were made available under the Transtac program for both language pairs evaluated in this paper. [sent-105, score-0.042]
</p><p>58 We divided these into training, held-out development, and test sets for building, tuning, and evaluating the SMT system, respectively. [sent-106, score-0.043]
</p><p>59 These development and test sets provide only one reference translation for each source sentence. [sent-107, score-0.259]
</p><p>60 For E2P, DARPA has made available to all program participants an  additional evaluation set with multiple (four) references for each test input. [sent-108, score-0.043]
</p><p>61 The Dari and Pashto monolingual corpora for LM training are a superset of target sentences from the parallel training corpus, consisting of additional untranslated sentences, as well as data derived from other sources, such as the web. [sent-109, score-0.358]
</p><p>62 2 Perplexity Analysis For both Dari and Pashto, we estimated a static trigram LM with unit sentence level weights that served as a baseline. [sent-112, score-0.434]
</p><p>63 We tuned this LM by varying the bigram and trigram frequency cutoff thresholds to minimize perplexity on the held-out target sentences. [sent-113, score-0.744]
</p><p>64 Finally, we evaluated test target perplexity with the optimized baseline LM. [sent-114, score-0.538]
</p><p>65 We then applied the proposed technique to estimate trigram LMs biased to source sentences in the held-out and test sets. [sent-115, score-0.627]
</p><p>66 We evaluated sourceconditional target perplexity by computing the total log-probability of all target sentences in a parallel test corpus against the LM biased by the corresponding source sentences. [sent-116, score-1.273]
</p><p>67 Again, bigram and trigram cutoff thresholds were tuned to minimize  source-conditional target perplexity on the held-out set. [sent-117, score-0.744]
</p><p>68 The tuned biased LMs were used to compute source-conditional target perplexity on the test set. [sent-118, score-0.84]
</p><p>69 53 c%t ion  Table 2: Reduction in perplexity using biased LMs. [sent-122, score-0.553]
</p><p>70 Table 2 summarizes the reduction in target perplexity using biased LMs; on the E2D and E2P single-reference test sets, we obtained perplexity reductions of 12. [sent-124, score-1.135]
</p><p>71 This indicates that the biased models are significantly better predictors of the corresponding target sentences than the static baseline LM. [sent-127, score-0.771]
</p><p>72 We used GIZA++ to induce automatic word alignments from the parallel training corpus. [sent-132, score-0.124]
</p><p>73 Phrase translation rules (up to a maximum source span of 5 words) were extracted from a combination of forward and backward word alignments (Koehn et al. [sent-133, score-0.33]
</p><p>74 The SMT decoder uses a log-linear model that combines numerous features, including but not limited to phrase translation probability, LM probability, and distortion penalty, to estimate the posterior probability of target hypotheses. [sent-135, score-0.328]
</p><p>75 Finally, we evaluated SMT performance on the test set in terms of BLEU and TER (Snover et al. [sent-138, score-0.085]
</p><p>76 The baseline SMT system used the static trigram LM with cutoff frequencies optimized for minimum  perplexity on the development set. [sent-140, score-0.661]
</p><p>77 Biased LMs (with n-gram cutoffs tuned as above) were estimated for all source sentences in the development and test 448  ETe2 PsDt-1s4er tef- ts S21t5a34. [sent-141, score-0.319]
</p><p>78 84e5d  Table 3: SMT performance with static and biased LMs. [sent-145, score-0.529]
</p><p>79 Table 3 summarizes the consistent improvement in BLEU/TER across multiple test sets and language pairs. [sent-147, score-0.074]
</p><p>80 5  Discussion and Future Work  Existing methods for target LM biasing for SMT rely on information retrieval to select a comparable subset from the training corpus. [sent-148, score-0.457]
</p><p>81 A foreground LM estimated from this subset is interpolated with the static background LM. [sent-149, score-0.416]
</p><p>82 However, given the large size  of a typical LM corpus, these methods are unsuitable for on-line, interactive SMT applications. [sent-150, score-0.086]
</p><p>83 In this paper, we proposed a novel LM biasing technique based on linear transformations of target sentences in a sparse vector space. [sent-151, score-0.661]
</p><p>84 We adopted a fine-grained approach, weighting individual target sentences based on the proposed measure of crosslingual similarity, and by using the entire, weighted corpus to estimate a biased LM. [sent-152, score-0.735]
</p><p>85 Finally, we showed that biased LMs yield significant reductions in target perplexity, and consistent improvements in SMT performance. [sent-155, score-0.553]
</p><p>86 While we used phrase-based SMT as a test-bed for evaluating translation performance, it should be noted that the proposed LM biasing approach is independent of SMT architecture. [sent-156, score-0.358]
</p><p>87 We plan to test its effectiveness in hierarchical and syntax-based SMT systems. [sent-157, score-0.043]
</p><p>88 We also plan to investigate the relative usefulness of LM biasing as we move from low-  languages to those for which significantly larger parallel corpora and LM training data are available. [sent-158, score-0.336]
</p><p>89 A study of translation edit rate with targeted human annotation. [sent-203, score-0.1]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lm', 0.456), ('smt', 0.308), ('biased', 0.299), ('biasing', 0.258), ('perplexity', 0.254), ('lms', 0.242), ('static', 0.23), ('target', 0.199), ('vj', 0.116), ('source', 0.116), ('cutoff', 0.114), ('dari', 0.112), ('pashto', 0.112), ('translation', 0.1), ('similarity', 0.096), ('bt', 0.088), ('parallel', 0.078), ('estimated', 0.072), ('crosslingual', 0.072), ('snover', 0.071), ('vector', 0.069), ('likeness', 0.066), ('trigram', 0.063), ('projection', 0.062), ('transtac', 0.061), ('sparse', 0.058), ('weighting', 0.058), ('interactive', 0.058), ('sm', 0.058), ('candidate', 0.057), ('reductions', 0.055), ('interpolated', 0.05), ('koehn', 0.05), ('matrix', 0.048), ('alignments', 0.046), ('tuned', 0.045), ('sentences', 0.043), ('test', 0.043), ('bilingual', 0.042), ('counts', 0.042), ('evaluated', 0.042), ('equation', 0.041), ('bbn', 0.041), ('complexity', 0.039), ('unit', 0.039), ('morristown', 0.038), ('thresholds', 0.038), ('monolingual', 0.038), ('backward', 0.038), ('count', 0.036), ('stroudsburg', 0.036), ('measure', 0.035), ('adaptation', 0.035), ('bleu', 0.035), ('nj', 0.034), ('technique', 0.034), ('tn', 0.033), ('preclude', 0.033), ('mma', 0.033), ('dem', 0.033), ('oftarget', 0.033), ('foreground', 0.033), ('ctt', 0.033), ('natarajan', 0.033), ('computationally', 0.032), ('background', 0.031), ('minimize', 0.031), ('summarizes', 0.031), ('feasible', 0.031), ('sentence', 0.03), ('configuration', 0.03), ('bonnie', 0.03), ('rohit', 0.03), ('ananthakrishnan', 0.03), ('raytheon', 0.03), ('etod', 0.03), ('clir', 0.03), ('sankaranarayanan', 0.03), ('sthe', 0.03), ('giza', 0.03), ('dorr', 0.03), ('forward', 0.03), ('alexandra', 0.03), ('moses', 0.03), ('estimate', 0.029), ('interpreted', 0.029), ('eto', 0.028), ('ara', 0.028), ('prem', 0.028), ('unsuitable', 0.028), ('approved', 0.028), ('proceeded', 0.028), ('alleviates', 0.028), ('technically', 0.028), ('tahe', 0.028), ('matthew', 0.028), ('conduct', 0.028), ('hypotheses', 0.028), ('let', 0.027), ('darpa', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="233-tfidf-1" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>Author: Sankaranarayanan Ananthakrishnan ; Rohit Prasad ; Prem Natarajan</p><p>Abstract: The language model (LM) is a critical component in most statistical machine translation (SMT) systems, serving to establish a probability distribution over the hypothesis space. Most SMT systems use a static LM, independent of the source language input. While previous work has shown that adapting LMs based on the input improves SMT performance, none of the techniques has thus far been shown to be feasible for on-line systems. In this paper, we develop a novel measure of cross-lingual similarity for biasing the LM based on the test input. We also illustrate an efficient on-line implementation that supports integration with on-line SMT systems by transferring much of the computational load off-line. Our approach yields significant reductions in target perplexity compared to the static LM, as well as consistent improvements in SMT performance across language pairs (English-Dari and English-Pashto).</p><p>2 0.16713713 <a title="233-tfidf-2" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>3 0.14003621 <a title="233-tfidf-3" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>4 0.13106127 <a title="233-tfidf-4" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>Author: David Chiang ; Steve DeNeefe ; Michael Pust</p><p>Abstract: We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems.</p><p>5 0.12948215 <a title="233-tfidf-5" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>Author: Sara Stymne</p><p>Abstract: In this thesis proposal Ipresent my thesis work, about pre- and postprocessing for statistical machine translation, mainly into Germanic languages. I focus my work on four areas: compounding, definite noun phrases, reordering, and error correction. Initial results are positive within all four areas, and there are promising possibilities for extending these approaches. In addition Ialso focus on methods for performing thorough error analysis of machine translation output, which can both motivate and evaluate the studies performed.</p><p>6 0.12765089 <a title="233-tfidf-6" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>7 0.12576586 <a title="233-tfidf-7" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>8 0.11972418 <a title="233-tfidf-8" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>9 0.11292429 <a title="233-tfidf-9" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>10 0.11053101 <a title="233-tfidf-10" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>11 0.11009674 <a title="233-tfidf-11" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>12 0.10407244 <a title="233-tfidf-12" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>13 0.099805564 <a title="233-tfidf-13" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>14 0.098351754 <a title="233-tfidf-14" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>15 0.097230807 <a title="233-tfidf-15" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>16 0.09606953 <a title="233-tfidf-16" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>17 0.095310494 <a title="233-tfidf-17" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>18 0.092377201 <a title="233-tfidf-18" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>19 0.090400845 <a title="233-tfidf-19" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>20 0.08840239 <a title="233-tfidf-20" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.221), (1, -0.12), (2, 0.108), (3, 0.13), (4, 0.028), (5, -0.012), (6, 0.042), (7, -0.013), (8, 0.045), (9, 0.061), (10, 0.018), (11, -0.069), (12, 0.058), (13, -0.032), (14, 0.03), (15, 0.035), (16, -0.059), (17, 0.026), (18, 0.001), (19, -0.059), (20, 0.037), (21, -0.099), (22, 0.117), (23, -0.053), (24, -0.008), (25, -0.003), (26, 0.042), (27, 0.042), (28, 0.003), (29, 0.029), (30, -0.027), (31, -0.051), (32, -0.009), (33, 0.001), (34, 0.07), (35, -0.069), (36, 0.013), (37, 0.004), (38, 0.033), (39, -0.05), (40, -0.033), (41, -0.027), (42, -0.02), (43, -0.058), (44, -0.068), (45, 0.12), (46, -0.0), (47, -0.03), (48, -0.08), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.954898 <a title="233-lsi-1" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>Author: Sankaranarayanan Ananthakrishnan ; Rohit Prasad ; Prem Natarajan</p><p>Abstract: The language model (LM) is a critical component in most statistical machine translation (SMT) systems, serving to establish a probability distribution over the hypothesis space. Most SMT systems use a static LM, independent of the source language input. While previous work has shown that adapting LMs based on the input improves SMT performance, none of the techniques has thus far been shown to be feasible for on-line systems. In this paper, we develop a novel measure of cross-lingual similarity for biasing the LM based on the test input. We also illustrate an efficient on-line implementation that supports integration with on-line SMT systems by transferring much of the computational load off-line. Our approach yields significant reductions in target perplexity compared to the static LM, as well as consistent improvements in SMT performance across language pairs (English-Dari and English-Pashto).</p><p>2 0.72138858 <a title="233-lsi-2" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>Author: David Chiang ; Steve DeNeefe ; Michael Pust</p><p>Abstract: We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems.</p><p>3 0.6990844 <a title="233-lsi-3" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>Author: Hal Daume III ; Jagadeesh Jagarlamudi</p><p>Abstract: We show that unseen words account for a large part of the translation error when moving to new domains. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to find translations for otherwise OOV terms. We show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs.</p><p>4 0.68998379 <a title="233-lsi-4" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>Author: Bing Xiang ; Abraham Ittycheriah</p><p>Abstract: In this paper we present a novel discriminative mixture model for statistical machine translation (SMT). We model the feature space with a log-linear combination ofmultiple mixture components. Each component contains a large set of features trained in a maximumentropy framework. All features within the same mixture component are tied and share the same mixture weights, where the mixture weights are trained discriminatively to maximize the translation performance. This approach aims at bridging the gap between the maximum-likelihood training and the discriminative training for SMT. It is shown that the feature space can be partitioned in a variety of ways, such as based on feature types, word alignments, or domains, for various applications. The proposed approach improves the translation performance significantly on a large-scale Arabic-to-English MT task.</p><p>5 0.68758476 <a title="233-lsi-5" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>Author: Nadir Durrani ; Helmut Schmid ; Alexander Fraser</p><p>Abstract: We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the “N-gram” model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.</p><p>6 0.67157257 <a title="233-lsi-6" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>7 0.66074896 <a title="233-lsi-7" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>8 0.65742731 <a title="233-lsi-8" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>9 0.652915 <a title="233-lsi-9" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>10 0.65220594 <a title="233-lsi-10" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>11 0.64429075 <a title="233-lsi-11" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>12 0.63721681 <a title="233-lsi-12" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>13 0.62951589 <a title="233-lsi-13" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>14 0.626302 <a title="233-lsi-14" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>15 0.62363958 <a title="233-lsi-15" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>16 0.60943997 <a title="233-lsi-16" href="./acl-2011-Translating_from_Morphologically_Complex_Languages%3A_A_Paraphrase-Based_Approach.html">310 acl-2011-Translating from Morphologically Complex Languages: A Paraphrase-Based Approach</a></p>
<p>17 0.59885979 <a title="233-lsi-17" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>18 0.59674168 <a title="233-lsi-18" href="./acl-2011-Combining_Morpheme-based_Machine_Translation_with_Post-processing_Morpheme_Prediction.html">75 acl-2011-Combining Morpheme-based Machine Translation with Post-processing Morpheme Prediction</a></p>
<p>19 0.59613532 <a title="233-lsi-19" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>20 0.59607643 <a title="233-lsi-20" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.029), (7, 0.197), (17, 0.062), (26, 0.033), (37, 0.108), (39, 0.03), (41, 0.071), (55, 0.014), (59, 0.042), (72, 0.064), (91, 0.025), (96, 0.221)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87661862 <a title="233-lda-1" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>Author: Sankaranarayanan Ananthakrishnan ; Rohit Prasad ; Prem Natarajan</p><p>Abstract: The language model (LM) is a critical component in most statistical machine translation (SMT) systems, serving to establish a probability distribution over the hypothesis space. Most SMT systems use a static LM, independent of the source language input. While previous work has shown that adapting LMs based on the input improves SMT performance, none of the techniques has thus far been shown to be feasible for on-line systems. In this paper, we develop a novel measure of cross-lingual similarity for biasing the LM based on the test input. We also illustrate an efficient on-line implementation that supports integration with on-line SMT systems by transferring much of the computational load off-line. Our approach yields significant reductions in target perplexity compared to the static LM, as well as consistent improvements in SMT performance across language pairs (English-Dari and English-Pashto).</p><p>2 0.80455291 <a title="233-lda-2" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>Author: Hal Daume III ; Jagadeesh Jagarlamudi</p><p>Abstract: We show that unseen words account for a large part of the translation error when moving to new domains. Using an extension of a recent approach to mining translations from comparable corpora (Haghighi et al., 2008), we are able to find translations for otherwise OOV terms. We show several approaches to integrating such translations into a phrasebased translation system, yielding consistent improvements in translations quality (between 0.5 and 1.5 Bleu points) on four domains and two language pairs.</p><p>3 0.8033421 <a title="233-lda-3" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>Author: Raphael Hoffmann ; Congle Zhang ; Xiao Ling ; Luke Zettlemoyer ; Daniel S. Weld</p><p>Abstract: Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded ( Jobs Apple ) and CEO-o f ( Jobs Apple ) . , , This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extrac- , tion model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</p><p>4 0.80289483 <a title="233-lda-4" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>Author: Jason Naradowsky ; Kristina Toutanova</p><p>Abstract: This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</p><p>5 0.80154574 <a title="233-lda-5" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>6 0.79970717 <a title="233-lda-6" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>7 0.79864752 <a title="233-lda-7" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>8 0.79785728 <a title="233-lda-8" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>9 0.79717302 <a title="233-lda-9" href="./acl-2011-A_Pronoun_Anaphora_Resolution_System_based_on_Factorial_Hidden_Markov_Models.html">23 acl-2011-A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models</a></p>
<p>10 0.79660261 <a title="233-lda-10" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>11 0.79633904 <a title="233-lda-11" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>12 0.79579389 <a title="233-lda-12" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>13 0.79566157 <a title="233-lda-13" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>14 0.79550064 <a title="233-lda-14" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>15 0.79531002 <a title="233-lda-15" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>16 0.79516923 <a title="233-lda-16" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>17 0.79513788 <a title="233-lda-17" href="./acl-2011-Disentangling_Chat_with_Local_Coherence_Models.html">101 acl-2011-Disentangling Chat with Local Coherence Models</a></p>
<p>18 0.79497415 <a title="233-lda-18" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>19 0.79495752 <a title="233-lda-19" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>20 0.79482758 <a title="233-lda-20" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
