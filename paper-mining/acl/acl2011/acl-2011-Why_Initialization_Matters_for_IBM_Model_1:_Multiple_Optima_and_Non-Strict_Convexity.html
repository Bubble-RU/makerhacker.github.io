<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-335" href="#">acl2011-335</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</h1>
<br/><p>Source: <a title="acl-2011-335-pdf" href="http://aclweb.org/anthology//P/P11/P11-2081.pdf">pdf</a></p><p>Author: Kristina Toutanova ; Michel Galley</p><p>Abstract: Contrary to popular belief, we show that the optimal parameters for IBM Model 1 are not unique. We demonstrate that, for a large class of words, IBM Model 1 is indifferent among a continuum of ways to allocate probability mass to their translations. We study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate.</p><p>Reference: <a title="acl-2011-335-reference" href="../acl2011_reference/acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Contrary to popular belief, we show that the optimal parameters for IBM Model 1 are not unique. [sent-2, score-0.221]
</p><p>2 We demonstrate that, for a large class of words, IBM Model 1 is indifferent among a continuum of ways to allocate probability mass to their translations. [sent-3, score-0.065]
</p><p>3 We study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate. [sent-4, score-0.679]
</p><p>4 1 Introduction Statistical alignment models have become widely used in machine translation, question answering,  textual entailment, and non-NLP application areas such as information retrieval (Berger and Lafferty, 1999) and object recognition (Duygulu et al. [sent-5, score-0.079]
</p><p>5 To reduce the impact of getting stuck in bad local optima the original IBM paper (Brown et al. [sent-10, score-0.096]
</p><p>6 IBM Model 1 was the first model in this sequence and was considered a reliable initializer due to its convexity. [sent-12, score-0.041]
</p><p>7 In this paper we show that although IBM Model 1 is convex, it is not strictly convex, and there is a large 461 Michel Galley Microsoft Research Redmond, WA 98005, USA mgal ley@mi cro s o ft com  . [sent-13, score-0.26]
</p><p>8 space of parameter values that achieve the same optimal value of the objective. [sent-14, score-0.174]
</p><p>9 We study the magnitude of this problem by formulating the space of optimal parameters as solutions to a set of linear equalities and seek maximally different parameter values that reach the same objective, using a linear programming approach. [sent-15, score-0.468]
</p><p>10 This lets us quantify the percentage of model parameters that are not uniquely defined, as well as the number of word types that have uncertain translation probabilities. [sent-16, score-0.267]
</p><p>11 We additionally study the achieved variance in parameters resulting from different random initialization in EM, and the impact of initialization on test set log-likelihood and alignment error rate. [sent-17, score-0.569]
</p><p>12 These experiments suggest that initialization does matter in practice, contrary to what is suggested in (Brown et al. [sent-18, score-0.145]
</p><p>13 1 2  Preliminaries  In Appendix A we define convexity and strict convexity of functions following (Boyd and Vandenberghe, 2004). [sent-21, score-0.42]
</p><p>14 In this section we detail the generative model for Model 1. [sent-22, score-0.041]
</p><p>15 am given a corresponding target translation e = e0 . [sent-31, score-0.107]
</p><p>16 The generative process is as follows: (i) pick a length m using a uniform distribution with mass function proportional to ? [sent-35, score-0.19]
</p><p>17 ; (ii) for each source word position j, pick an alignment 1When referring to Model 1, Brown et al. [sent-36, score-0.079]
</p><p>18 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 461–466, position in the target sentence aj ∈ 0, 1, . [sent-40, score-0.081]
</p><p>19 , lfrom a uniform distribution; and (iii) generate a source word using the translation probability distribution t(fj |eaj ). [sent-43, score-0.131]
</p><p>20 A special empty word (NULL) is assumed to b|ee part of the target vocabulary and to occupy the first position in each target language sentence (e0=NULL). [sent-44, score-0.122]
</p><p>21 The trainable parameters of Model 1 are the lexical translation probabilities t(f|e), where f and e range over tiohen source laitnide target )v,o wcahbeurelar fies a,n respectively. [sent-45, score-0.234]
</p><p>22 The log-probability of a single source sentence f given its corresponding target sentence e  and values for the translation parameters t(f|e) can aben dw vriatltueens as rfo tlhloew trsa (Brown e pta aral. [sent-46, score-0.314]
</p><p>23 jX= X1  Xi=0  The parameters of IBM Model 1 are usually derived via maximum likelihood estimation from a corpus, which is equivalent to negative log-likelihood minimization. [sent-48, score-0.216]
</p><p>24 We can define the optimization problem as the one of minimizing negative log-likelihood LD (T) subject to constraints ensuring that the parameters  are well-formed probabilities, i. [sent-50, score-0.297]
</p><p>25 It is well-known that the EM algorithm for this problem converges to a local optimum of the objective function (Dempster et al. [sent-53, score-0.291]
</p><p>26 3  Convexity analysis for IBM Model 1  In this section we show that, contrary to the claim in (Brown et al. [sent-55, score-0.128]
</p><p>27 , 1993), the optimization problem for IBM Model 1 is not strictly convex, which means that there could be multiple parameter settings that 462 achieve the same globally optimal value of the objective. [sent-56, score-0.45]
</p><p>28 2 The function −log(x) is strictly convex (Boyd andT Vandenberghe, 2004). [sent-57, score-0.832]
</p><p>29 sE satcrihc ttleyrm co innv ethxe negative log-likelihood is a negative logarithm of a sum of parameters. [sent-58, score-0.172]
</p><p>30 The negative logarithm of a sum is not strictly convex, as illustrated by the following simple counterexample. [sent-59, score-0.342]
</p><p>31 We will come up with two parameter settings x,y and a value θ that violate the defini-  tion of strict convexity. [sent-62, score-0.092]
</p><p>32 Sθtrliocgt convexity requires tlhogat( ythe form) =er expression S btrei strictly sxmitaylrl eerthan the latter, but we have equality. [sent-76, score-0.418]
</p><p>33 , 1993), because it is a composition of log and a linear function. [sent-79, score-0.117]
</p><p>34 We thus showed that every term in the negative log-likelihood objective is convex but not strictly convex and thus the overall objective is convex, but not strictly convex. [sent-80, score-1.767]
</p><p>35 Because the objective is convex, the inequality constraints are convex, and the equality constraints are affine, the IBM Model 1optimization problem is a convex optimization problem. [sent-81, score-0.776]
</p><p>36 But since the objective is not strictly convex, there might be multiple distinct parameter values achieving the same optimal value. [sent-83, score-0.48]
</p><p>37 In the next section we study the actual space of optima for small and realistically-sized parallel corpora. [sent-84, score-0.131]
</p><p>38 303) claim the following about the log-likelihood function (Eq. [sent-87, score-0.133]
</p><p>39 1 in ours): “The objective function (51) for this model is a strictly concave function ofthe parameters”, which is equivalent to claiming that the negative log-likelihood function is strictly convex. [sent-89, score-0.796]
</p><p>40 Furthermore, we will empirically show in Sections 4 and 5 that multiple distinct parameter values can achieve the global optimum of the objective function, which also disproves Brown et al. [sent-92, score-0.319]
</p><p>41 ’s claim about the strict convexity of the objective function. [sent-93, score-0.386]
</p><p>42 Indeed, if a function is strictly convex, it admits a unique globally optimum solution (Boyd and Vandenberghe, 2004, p. [sent-94, score-0.541]
</p><p>43 4  Solution Space  In this section, we characterize the set of parameters that achieve the maximum of the log-likelihood of IBM Model 1. [sent-97, score-0.156]
</p><p>44 For instance, setting t(phrase|sentence) = t(courte|short) = 1 yields the maximum| likelihood value w|ith (0 + 1) (1 + 0) = 1, but the most divergent set of parameters (t(courte|sentence) = t(phrase|sentence) = 1) also reaches |the same optimum: (1+ 0) (0 + 1) = 1. [sent-106, score-0.199]
</p><p>45 While this example may not seem representative given the small size of this data, the laxity of Model 1 that we observe in this example also surfaces in real and much larger training sets. [sent-107, score-0.034]
</p><p>46 Indeed, it suffices that a given pair of target words (e1,e2) systematically co-occurs in the data (as with e1 = short e2 = sentence) to cause Model 1 to fail to distinguish the two. [sent-108, score-0.07]
</p><p>47 3 To characterize the solution space, we use the definition of IBM Model 1log-likelihood from Eq. [sent-109, score-0.102]
</p><p>48 We ask whether distinct sets of parameters yield the same minimum negative log-likelihood value of Eq. [sent-112, score-0.247]
</p><p>49 tT(hfis|e is) tr auned dif t (a) the two distributions remain well-formed: Pj t(fj |ei) = 1for i∈ {1, 2}; (b) any adjustments to paramePters of fj )le =ave 1 1e afocrh i ie s∈tim {1a,te2 t(fj |e1) y+ a t(fj |e2) unchanged. [sent-116, score-0.241]
</p><p>50 463 above equation can be satisfied for optimal parameters only if the following holds for each f, e pair: Xl Xl  Xt(fj|ei) Xi=0  = Xt0(fj|ei),j = 1. [sent-117, score-0.221]
</p><p>51 Using these EM parameters (θ) in the right hand side of  the equation, we replace these right hand sides with EM’s estimate tθ(fj |e). [sent-123, score-0.127]
</p><p>52 This finally gives us the following linear program (LP), sw fihniaclhly ych giavreasct uesri tzhees ftohlesolution space of the maximum log-likelihood:4 Xl  Xt(fj|ei)  = tθ(fj|e),  Xi=0  Xt(f|e) Xf  j = 1. [sent-124, score-0.077]
</p><p>53 To measure the maximum divergence in optimal model parameters, we solve the LP of Eq. [sent-131, score-0.135]
</p><p>54 3-5 by minimizing the linear objective function xTxk, where xk is the column-vector representing ka−ll1 parameters of the model t(f|e) currently optimized, aranmd ewtehrsere o xk−1 miso a pre-existing seentt loyf ompatixmimizuedm, log-likelihood parameters. [sent-132, score-0.445]
</p><p>55 Starting with x0 defined using EM parameters, we are effectively searching for the vector x1 with lowest cosine similarity to x0. [sent-133, score-0.142]
</p><p>56 We repeat with k > 1 until xk doesn’t reduce the cosine similarity with any of the previous parameter  vectors x0 . [sent-134, score-0.258]
</p><p>57 5 4In general, an LP admits either (a) an infinity of solutions, when the system is underconstrained; (b) exactly one solution; (c) zero solutions, when it is ill-posed. [sent-138, score-0.048]
</p><p>58 The latter case never occurs in our case, since the system was explicitly constructed to allow at least one solution: the parameter set returned by EM. [sent-139, score-0.05]
</p><p>59 5Note that this greedy procedure is not guaranteed to find the two points of the feasible region (a convex polytope) with minimum cosine similarity. [sent-140, score-0.686]
</p><p>60 cosine similarity [c] Figure 1: Percentage of target words for which we found pairs of distributions t(f|e) and t0 (f|e) whose cosine psiamirisla orifty d disrtorpibsu u btieolonws ta( gfi|eve)n a atnhdre tsh(ofld|e c ( wxh-aoxsise) . [sent-144, score-0.29]
</p><p>61 c  5  Experiments  In this section, we show that the solution space defined by the LP of Eq. [sent-145, score-0.073]
</p><p>62 We demonstrate this with Bulgarian-English parallel data drawn from the JRC-AQUIS corpus (Steinberger et al. [sent-147, score-0.061]
</p><p>63 Our training data consists of up to 10,000 sentence pairs, which is representative of the amount of data used to train SMT systems for language pairs that are relatively resource-poor. [sent-149, score-0.04]
</p><p>64 Figure 1relies on two methods for determining to what extent the model t(f|e) can vary while remaining optimal. [sent-150, score-0.041]
</p><p>65 hTeh me oEdMel-L t(Pf-|Ne) mcaenth vaodry c wonhsiliset sre mofa applying the method described at the end of Section 4 with N training sentence pairs. [sent-151, score-0.04]
</p><p>66 For EM-rand-N, we instead run EM 100 times (also on N sentence pairs) until convergence using different random starting points, and then use cosine similarity to compare the resulting models. [sent-152, score-0.288]
</p><p>67 6 Figure 1 shows some surprising results: First, EM-LP-128 finds that, for about 68% of target token types, cosine similarity between contrastive models is equal to 0. [sent-153, score-0.183]
</p><p>68 A cosine of zero essentially means that we can turn 1’s into 0’s without affecting log-likelihood, as in the short sentence  example in Section 4. [sent-154, score-0.218]
</p><p>69 Second, with a much larger training set, EM-rand-10K finds a cosine similarity lower or equal to 0. [sent-155, score-0.142]
</p><p>70 6While the first method is better at finding divergent optimal model parameters, it needs to construct large linear programs that do not scale with large training sets (linear systems quickly reach millions of entries, even with 128 sentence pairs). [sent-157, score-0.264]
</p><p>71 We use EM-rand to assess the model space on larger training set, while we use EM-LP mainly to illustrate that divergence between optimal models can be much larger than suggested by EM-rand. [sent-158, score-0.165]
</p><p>72 Every row represents statistics for a given training set size (in number of sent. [sent-172, score-0.034]
</p><p>73 The percent of non-unique types are reported overall, as well as only among coupled words (c. [sent-175, score-0.167]
</p><p>74 The last two columns show the standard deviation in test set log-likelihood across different random trials, as well as the difference between the log-likelihood of the uniformly initialized model and the best model from the random trials. [sent-178, score-0.333]
</p><p>75 We can see that as the training set size increases, the percentage of words that have non-unique translation probabilities goes down but is still very large. [sent-179, score-0.1]
</p><p>76 The coupled words almost always end up having varying translation parameters at convergence (more than 99. [sent-180, score-0.321]
</p><p>77 7 We also  computed the percent of word types that are coupled for two more-realistically sized data-sets: we found that in a 1. [sent-183, score-0.167]
</p><p>78 6 million sent pair English-Bulgarian corpus 15% of Bulgarian word types were coupled and in a 1. [sent-184, score-0.107]
</p><p>79 The log-likelihood statistics show that although 7We did not perform such experiments for larger data-sets, since EM takes thousands of iterations to converge. [sent-187, score-0.051]
</p><p>80 Interestingly, the uniformly initialized model performs worse for a very small data size, but it catches up and surpasses the random models at data sizes greater than 100 sentence pairs. [sent-189, score-0.249]
</p><p>81 To further evaluate the impact of initialization for IBM Model 1, we report on a set of experiments looking at alignment error rate achieved by different models. [sent-190, score-0.208]
</p><p>82 We report the performance of Model 1, as well as the performance of the more competitive HMM alignment model (Vogel et al. [sent-191, score-0.12]
</p><p>83 We look at two different training set sizes, a small set consisting of 1000 sentence pairs, and a reasonably-sized dataset containing 100,000 sentence pairs. [sent-195, score-0.08]
</p><p>84 In each data size condition, we report on the performance achieved by IBM-1, and the performance achieved by HMM initialized from the IBM1 parameters. [sent-196, score-0.186]
</p><p>85 For IBM Model 1 training, we either perform only 5 EM iterations (the standard setting in GIZA++), or run it to convergence. [sent-197, score-0.051]
</p><p>86 For each of these two settings, we either start training from uniform t(f|e) parameters, or random parameters. [sent-198, score-0.117]
</p><p>87 The numbers in the table are alignment error rates, aTchheie nvuemd baetr tsh ien e thnde otafb Mleo adreel 1l training, rarnodr at 5 iterations of HMM. [sent-201, score-0.164]
</p><p>88 When random initialization is used, we run 20 random trials with different ini-  tialization, and report the min, max, and mean AER achieved in each setting. [sent-202, score-0.314]
</p><p>89 First, in agreement with current practice using only 5 iterations of Model 1 training results in better final performance of the HMM model (even though the performance of Model 1 is higher when ran to convergence). [sent-204, score-0.092]
</p><p>90 Second, the minimum AER achieved by randomly initialized models was always smaller 465  form and random initialization. [sent-205, score-0.201]
</p><p>91 In  some cases, even the mean of the random trials was better than the corresponding uniform model. [sent-208, score-0.198]
</p><p>92 Interestingly, the advantage of the randomly initialized models in AER does not seem to diminish with increased training data size like their advantage in test set perplexity. [sent-209, score-0.12]
</p><p>93 6  Conclusions  Through theoretical analysis and three sets of experiments, we showed that IBM Model 1 is not strictly convex and that there is large variance in the set of optimal parameter values. [sent-210, score-1.008]
</p><p>94 This variance impacts a significant fraction of word types and results in variance in predictive performance of trained models, as measured by test set log-likelihood and word-alignment error rate. [sent-211, score-0.205]
</p><p>95 In future work we would like to study the im-  pact of non-determinism on higher order models in the standard alignment model sequence and to gain more insight into the impact of finer-grained features in alignment. [sent-214, score-0.12]
</p><p>96 Maximum likelihood from incomplete data via the em algorithm. [sent-249, score-0.13]
</p><p>97 466 Appendix A: Convex functions and convex optimization problems We denote the domain of a function f by dom f. [sent-282, score-0.737]
</p><p>98 l Where the functions f0 to fk are convex and the equality constraints are affine. [sent-289, score-0.623]
</p><p>99 It can be shown that the that satisfy the constraints) optimum for the problem is strictly convex then any global optimum. [sent-290, score-0.94]
</p><p>100 feasible set (the set of points is convex and that any local is a global optimum. [sent-291, score-0.606]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('convex', 0.549), ('fj', 0.241), ('strictly', 0.229), ('ibm', 0.195), ('convexity', 0.189), ('courte', 0.157), ('xl', 0.142), ('optimum', 0.133), ('parameters', 0.127), ('ei', 0.113), ('cosine', 0.107), ('brown', 0.104), ('em', 0.1), ('boyd', 0.096), ('initialization', 0.096), ('duygulu', 0.094), ('polytope', 0.094), ('vandenberghe', 0.094), ('optimal', 0.094), ('dom', 0.091), ('variance', 0.086), ('initialized', 0.086), ('lp', 0.083), ('trials', 0.081), ('xm', 0.081), ('claim', 0.079), ('alignment', 0.079), ('aer', 0.078), ('objective', 0.076), ('coupled', 0.074), ('log', 0.07), ('xi', 0.07), ('optima', 0.068), ('xk', 0.066), ('translation', 0.066), ('uniform', 0.065), ('barnard', 0.063), ('kaibel', 0.063), ('steinberger', 0.063), ('xxlogxt', 0.063), ('percent', 0.06), ('negative', 0.059), ('jx', 0.056), ('xt', 0.056), ('logarithm', 0.054), ('convergence', 0.054), ('function', 0.054), ('random', 0.052), ('iterations', 0.051), ('parameter', 0.05), ('contrary', 0.049), ('admits', 0.048), ('linear', 0.047), ('vogel', 0.044), ('appendix', 0.044), ('solution', 0.043), ('optimization', 0.043), ('strict', 0.042), ('redmond', 0.042), ('divergent', 0.042), ('affecting', 0.042), ('model', 0.041), ('target', 0.041), ('equality', 0.04), ('sentence', 0.04), ('magnitude', 0.039), ('mass', 0.037), ('hmm', 0.036), ('berger', 0.036), ('similarity', 0.035), ('proportional', 0.034), ('globally', 0.034), ('tsh', 0.034), ('solutions', 0.034), ('constraints', 0.034), ('cooccurrence', 0.034), ('dempster', 0.034), ('minimizing', 0.034), ('ld', 0.034), ('size', 0.034), ('null', 0.033), ('achieved', 0.033), ('types', 0.033), ('parallel', 0.033), ('wa', 0.032), ('cro', 0.031), ('deviation', 0.031), ('distinct', 0.031), ('likelihood', 0.03), ('space', 0.03), ('uniformly', 0.03), ('minimum', 0.03), ('definition', 0.03), ('global', 0.029), ('short', 0.029), ('characterize', 0.029), ('demonstrate', 0.028), ('local', 0.028), ('rows', 0.028), ('interestingly', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="335-tfidf-1" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>Author: Kristina Toutanova ; Michel Galley</p><p>Abstract: Contrary to popular belief, we show that the optimal parameters for IBM Model 1 are not unique. We demonstrate that, for a large class of words, IBM Model 1 is indifferent among a continuum of ways to allocate probability mass to their translations. We study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate.</p><p>2 0.19203082 <a title="335-tfidf-2" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>3 0.10698476 <a title="335-tfidf-3" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>4 0.10025707 <a title="335-tfidf-4" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>5 0.094125643 <a title="335-tfidf-5" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>6 0.088930525 <a title="335-tfidf-6" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>7 0.08678177 <a title="335-tfidf-7" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>8 0.083813131 <a title="335-tfidf-8" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>9 0.081045553 <a title="335-tfidf-9" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>10 0.080209613 <a title="335-tfidf-10" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>11 0.078423813 <a title="335-tfidf-11" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>12 0.076754674 <a title="335-tfidf-12" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>13 0.074796043 <a title="335-tfidf-13" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>14 0.071954638 <a title="335-tfidf-14" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>15 0.071187355 <a title="335-tfidf-15" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>16 0.069945171 <a title="335-tfidf-16" href="./acl-2011-Better_Hypothesis_Testing_for_Statistical_Machine_Translation%3A_Controlling_for_Optimizer_Instability.html">60 acl-2011-Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability</a></p>
<p>17 0.068157777 <a title="335-tfidf-17" href="./acl-2011-Phrase-Based_Translation_Model_for_Question_Retrieval_in_Community_Question_Answer_Archives.html">245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</a></p>
<p>18 0.068119377 <a title="335-tfidf-18" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>19 0.066963173 <a title="335-tfidf-19" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>20 0.066776119 <a title="335-tfidf-20" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.193), (1, -0.07), (2, 0.043), (3, 0.068), (4, 0.02), (5, -0.019), (6, 0.018), (7, 0.036), (8, -0.028), (9, 0.079), (10, 0.097), (11, 0.07), (12, 0.027), (13, 0.107), (14, -0.077), (15, 0.037), (16, -0.003), (17, 0.011), (18, -0.03), (19, -0.019), (20, 0.025), (21, 0.008), (22, 0.04), (23, 0.019), (24, 0.0), (25, -0.027), (26, -0.026), (27, -0.015), (28, 0.049), (29, -0.028), (30, 0.044), (31, 0.025), (32, -0.007), (33, 0.044), (34, 0.06), (35, 0.015), (36, 0.048), (37, -0.066), (38, 0.035), (39, -0.016), (40, -0.025), (41, -0.065), (42, 0.031), (43, 0.003), (44, 0.011), (45, -0.009), (46, -0.04), (47, 0.027), (48, -0.008), (49, -0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92694479 <a title="335-lsi-1" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>Author: Kristina Toutanova ; Michel Galley</p><p>Abstract: Contrary to popular belief, we show that the optimal parameters for IBM Model 1 are not unique. We demonstrate that, for a large class of words, IBM Model 1 is indifferent among a continuum of ways to allocate probability mass to their translations. We study the magnitude of the variance in optimal model parameters using a linear programming approach as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate.</p><p>2 0.79917347 <a title="335-lsi-2" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>3 0.76286805 <a title="335-lsi-3" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>4 0.76212811 <a title="335-lsi-4" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>5 0.70184708 <a title="335-lsi-5" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>Author: Mohit Bansal ; Chris Quirk ; Robert Moore</p><p>Abstract: We propose a principled and efficient phraseto-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semiMarkov model, word-to-phrase and phraseto-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include “gappy phrases” (such as French ne ? pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime.</p><p>6 0.69633609 <a title="335-lsi-6" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>7 0.68099201 <a title="335-lsi-7" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>8 0.64671576 <a title="335-lsi-8" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>9 0.62194735 <a title="335-lsi-9" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>10 0.61365694 <a title="335-lsi-10" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>11 0.61072087 <a title="335-lsi-11" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>12 0.60826665 <a title="335-lsi-12" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>13 0.58102071 <a title="335-lsi-13" href="./acl-2011-Efficient_Online_Locality_Sensitive_Hashing_via_Reservoir_Counting.html">113 acl-2011-Efficient Online Locality Sensitive Hashing via Reservoir Counting</a></p>
<p>14 0.58046103 <a title="335-lsi-14" href="./acl-2011-full-for-print.html">342 acl-2011-full-for-print</a></p>
<p>15 0.57142031 <a title="335-lsi-15" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>16 0.56611848 <a title="335-lsi-16" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>17 0.561647 <a title="335-lsi-17" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>18 0.54936534 <a title="335-lsi-18" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>19 0.54895329 <a title="335-lsi-19" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>20 0.54850119 <a title="335-lsi-20" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.012), (17, 0.029), (26, 0.017), (37, 0.072), (39, 0.012), (41, 0.027), (55, 0.026), (59, 0.039), (72, 0.035), (91, 0.021), (96, 0.638)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99817038 <a title="335-lda-1" href="./acl-2011-Improving_On-line_Handwritten_Recognition_using_Translation_Models_in_Multimodal_Interactive_Machine_Translation.html">168 acl-2011-Improving On-line Handwritten Recognition using Translation Models in Multimodal Interactive Machine Translation</a></p>
<p>Author: Vicent Alabau ; Alberto Sanchis ; Francisco Casacuberta</p><p>Abstract: In interactive machine translation (IMT), a human expert is integrated into the core of a machine translation (MT) system. The human expert interacts with the IMT system by partially correcting the errors of the system’s output. Then, the system proposes a new solution. This process is repeated until the output meets the desired quality. In this scenario, the interaction is typically performed using the keyboard and the mouse. In this work, we present an alternative modality to interact within IMT systems by writing on a tactile display or using an electronic pen. An on-line handwritten text recognition (HTR) system has been specifically designed to operate with IMT systems. Our HTR system improves previous approaches in two main aspects. First, HTR decoding is tightly coupled with the IMT system. Second, the language models proposed are context aware, in the sense that they take into account the partial corrections and the source sentence by using a combination of ngrams and word-based IBM models. The proposed system achieves an important boost in performance with respect to previous work.</p><p>2 0.9980371 <a title="335-lda-2" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>Author: Anselmo Penas ; Alvaro Rodrigo</p><p>Abstract: There are several tasks where is preferable not responding than responding incorrectly. This idea is not new, but despite several previous attempts there isn’t a commonly accepted measure to assess non-response. We study here an extension of accuracy measure with this feature and a very easy to understand interpretation. The measure proposed (c@1) has a good balance of discrimination power, stability and sensitivity properties. We show also how this measure is able to reward systems that maintain the same number of correct answers and at the same time decrease the number of incorrect ones, by leaving some questions unanswered. This measure is well suited for tasks such as Reading Comprehension tests, where multiple choices per question are given, but only one is correct.</p><p>3 0.99737942 <a title="335-lda-3" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>Author: Daniel Emilio Beck</p><p>Abstract: In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</p><p>4 0.99717337 <a title="335-lda-4" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>Author: Nitin Agarwal ; Ravi Shankar Reddy ; Kiran GVR ; Carolyn Penstein Rose</p><p>Abstract: In this demo, we present SciSumm, an interactive multi-document summarization system for scientific articles. The document collection to be summarized is a list of papers cited together within the same source article, otherwise known as a co-citation. At the heart of the approach is a topic based clustering of fragments extracted from each article based on queries generated from the context surrounding the co-cited list of papers. This analysis enables the generation of an overview of common themes from the co-cited papers that relate to the context in which the co-citation was found. SciSumm is currently built over the 2008 ACL Anthology, however the gen- eralizable nature of the summarization techniques and the extensible architecture makes it possible to use the system with other corpora where a citation network is available. Evaluation results on the same corpus demonstrate that our system performs better than an existing widely used multi-document summarization system (MEAD).</p><p>5 0.99700737 <a title="335-lda-5" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>Author: Delip Rao ; David Yarowsky</p><p>Abstract: This paper presents an original approach to semi-supervised learning of personal name ethnicity from typed graphs of morphophonemic features and first/last-name co-occurrence statistics. We frame this as a general solution to an inference problem over typed graphs where the edges represent labeled relations between features that are parameterized by the edge types. We propose a framework for parameter estimation on different constructions of typed graphs for this problem using a gradient-free optimization method based on grid search. Results on both in-domain and out-of-domain data show significant gains over 30% accuracy improvement using the techniques presented in the paper.</p><p>6 0.99637282 <a title="335-lda-6" href="./acl-2011-Automatic_Evaluation_of_Chinese_Translation_Output%3A_Word-Level_or_Character-Level%3F.html">49 acl-2011-Automatic Evaluation of Chinese Translation Output: Word-Level or Character-Level?</a></p>
<p>same-paper 7 0.99635464 <a title="335-lda-7" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>8 0.99626505 <a title="335-lda-8" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>9 0.99065036 <a title="335-lda-9" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>10 0.98182982 <a title="335-lda-10" href="./acl-2011-An_Interactive_Machine_Translation_System_with_Online_Learning.html">41 acl-2011-An Interactive Machine Translation System with Online Learning</a></p>
<p>11 0.97945583 <a title="335-lda-11" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>12 0.96864903 <a title="335-lda-12" href="./acl-2011-Reordering_with_Source_Language_Collocations.html">266 acl-2011-Reordering with Source Language Collocations</a></p>
<p>13 0.96274102 <a title="335-lda-13" href="./acl-2011-Reordering_Metrics_for_MT.html">264 acl-2011-Reordering Metrics for MT</a></p>
<p>14 0.95506972 <a title="335-lda-14" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>15 0.95433396 <a title="335-lda-15" href="./acl-2011-Improving_Question_Recommendation_by_Exploiting_Information_Need.html">169 acl-2011-Improving Question Recommendation by Exploiting Information Need</a></p>
<p>16 0.95263857 <a title="335-lda-16" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>17 0.95030499 <a title="335-lda-17" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>18 0.94853741 <a title="335-lda-18" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>19 0.9472149 <a title="335-lda-19" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>20 0.94549477 <a title="335-lda-20" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
