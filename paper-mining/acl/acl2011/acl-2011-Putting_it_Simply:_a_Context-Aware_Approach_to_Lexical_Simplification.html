<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-254" href="#">acl2011-254</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</h1>
<br/><p>Source: <a title="acl-2011-254-pdf" href="http://aclweb.org/anthology//P/P11/P11-2087.pdf">pdf</a></p><p>Author: Or Biran ; Samuel Brody ; Noemie Elhadad</p><p>Abstract: We present a method for lexical simplification. Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.</p><p>Reference: <a title="acl-2011-254-reference" href="../acl2011_reference/acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. [sent-6, score-0.201]
</p><p>2 We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. [sent-9, score-0.99]
</p><p>3 Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality. [sent-10, score-0.954]
</p><p>4 1 Introduction The task of simplification consists of editing an input text into a version that is less complex linguisti-  cally or more readable. [sent-11, score-0.805]
</p><p>5 Automated sentence simplification has been investigated mostly as a preprocessing step with the goal of improving NLP tasks, such as parsing (Chandrasekar et al. [sent-12, score-0.821]
</p><p>6 Automated simplification can also be considered as a way to help end users access relevant information, which would be too complex to understand if left unedited. [sent-16, score-0.773]
</p><p>7 As such, it was proposed as a tool for adults with aphasia (Carroll et al. [sent-17, score-0.055]
</p><p>8 , 2004), readers with low-literacy skills (Williams and Reiter, 2005), individuals with intellectual disabilities (Huenerfauth et al. [sent-19, score-0.079]
</p><p>9 In this paper, we present a sentence simplification approach, which focuses on lexical simplification. [sent-23, score-0.821]
</p><p>10 1 The key contributions of our work are (i) an unsupervised method for learning pairs of complex and simpler synonyms; and (ii) a contextaware method for substituting one for the other. [sent-24, score-0.178]
</p><p>11 The word magnate is determined as a candidate for sim–  plification. [sent-26, score-0.138]
</p><p>12 Two learned rules are available to the simplification system (substitute magnate with king or with businessman). [sent-27, score-0.861]
</p><p>13 In the context of this sentence, the second rule is selected, resulting in the simpler output sentence. [sent-28, score-0.169]
</p><p>14 Our method contributes to research on lexical simplification (both learning of rules and actual sentence simplification), a topic little investigated thus far. [sent-29, score-0.92]
</p><p>15 From a technical perspective, the task of lexical simplification bears similarity with that of para1Our resulting system is available http://www. [sent-30, score-0.838]
</p><p>16 Napoles and Dredze (2010) examined Wikipedia Simple articles looking for features that characterize a simple text, with the hope of informing research  in automatic simplification methods. [sent-38, score-0.81]
</p><p>17 (2010) learn lexical simplification rules from the edit histories of Wikipedia Simple articles. [sent-40, score-0.869]
</p><p>18 Our method differs from theirs, as we rely on the two corpora as a whole, and do not require any aligned or designated simple/complex sentences when learning simplification rules. [sent-41, score-0.732]
</p><p>19 SEW is a Wikipedia project providing articles in Simple English, a version of English which uses fewer words and easier grammar, and which aims to be easier to read for children, people who are learning English and people with learning difficulties. [sent-43, score-0.11]
</p><p>20 Due to the labor involved in simplifying Wikipedia articles, only about 2% of the EW articles have been simplified. [sent-44, score-0.044]
</p><p>21 Rather, we leverage SEW only as an example of an in-domain simple corpus, in order to extract word frequency estimates. [sent-46, score-0.084]
</p><p>22 In practice, this means that our method is suitable for other cases where there exists a simplified corpus in the same domain. [sent-50, score-0.109]
</p><p>23 The articles were preprocessed as follows: all comments, HTML tags, and Wiki links were removed. [sent-53, score-0.044]
</p><p>24 org 4Aligning sentences in monolingual comparable corpora has been investigated (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006), but is not a focus for this work. [sent-58, score-0.109]
</p><p>25 Further preprocessing was carried out with the Stanford NLP Package5 to tokenize the text, transform all words to lower case, and identify sentence boundaries. [sent-60, score-0.057]
</p><p>26 3  Method  Our sentence simplification system consists of two main stages: rule extraction and simplification. [sent-61, score-0.862]
</p><p>27 In the first stage, simplification rules are extracted from  the corpora. [sent-62, score-0.799]
</p><p>28 Each rule consists of an ordered word pair {original → simplified} along with a score indicating trhieg similarity bpelitfiweede}n a tlohen gw woridths. [sent-63, score-0.175]
</p><p>29 a sIcn otrhee second stage, the system decides whether to apply a rule (i. [sent-64, score-0.073]
</p><p>30 , transform the original word into the simplified one), based on the contextual information. [sent-66, score-0.193]
</p><p>31 For each candidate word w, we constructed a context vector CVw, containing co-occurrence information within a 10-token window. [sent-71, score-0.119]
</p><p>32 Each dimension iin the vector corresponds to a single word wi in the vocabulary, and a single dimension was added to represent any number token. [sent-72, score-0.104]
</p><p>33 The value in each dimension CVw [i] of the vector was the number of occurrences ofthe corresponding word wi within a tentoken window surrounding an instance of the candidate word w. [sent-73, score-0.142]
</p><p>34 From all possible word pairs (the Cartesian product of all words in the corpus vocabulary), we first remove pairs of morphological variants. [sent-76, score-0.124]
</p><p>35 We also prune pairs where one word is a prefix of the other and the suffix is in {s, es, ed, ly, er, ing}. [sent-78, score-0.103]
</p><p>36 edu its first sense (as listed in WordNet)7 is a synonym or hypernym of the first. [sent-87, score-0.091]
</p><p>37 Finally, we compute the cosine similarity scores for the remaining pairs using their context vectors. [sent-88, score-0.207]
</p><p>38 2 Ensuring Simplification From among our remaining candidate word pairs, we want to identify those that represent a complex word which can be replaced by a simpler one. [sent-91, score-0.198]
</p><p>39 Our definition of the complexity of a word is based on two measures: the corpus complexity and the lexical complexity. [sent-92, score-0.192]
</p><p>40 Specifically, we define the corpus com-  plexity of a word as  ×  Cw=ffww,E,Sinmglipslhe where fw,c is the frequency of word w in corpus c, and the lexical complexity as Lw = |w|, the length of the word. [sent-93, score-0.21]
</p><p>41 The final complexity χw f|o,r t hthee l ewngorthd is given by the product of the two. [sent-94, score-0.066]
</p><p>42 χw  = Cw  Lw  After calculating the complexity of all words participating in the word pairs, we discard the pairs for which the first word’s complexity is lower than that of the second. [sent-95, score-0.208]
</p><p>43 The remaining pairs constitute the final list of substitution candidates. [sent-96, score-0.094]
</p><p>44 3 Ensuring Grammaticality To ensure that our simplification substitutions maintain the grammaticality ofthe original sentence, we generate grammatically consistent rules from the substitution candidate list. [sent-99, score-1.14]
</p><p>45 For each candidate pair (original, simplified), we generate all consistent forms (fi (original) , fi(substitute)) of the two words using MorphAdorner. [sent-100, score-0.048]
</p><p>46 For example, the word pair (stride, walk) will generate the form pairs (stride, walk), (striding, walking), (strode, walked) and (strides, walks). [sent-102, score-0.076]
</p><p>47 Rather than attempting explicit disambiguation and adding complexity to the model, we rely on the first sense heuristic, which is know to be very strong, along with contextual information, as described in Section 3. [sent-104, score-0.117]
</p><p>48 498 exactly the same list of form pairs, eliminating the original ungrammatical pair. [sent-106, score-0.056]
</p><p>49 Finally, each pair (fi(original), fi(substitute)) becomes a rule {fi(original) → fi(substitute)}, cwoimthe weight Similarity(original, substitute). [sent-107, score-0.073]
</p><p>50 2 Stage 2: Sentence Simplification Given an input sentence and the set of rules learned in the first stage, this stage determines which words in the sentence should be simplified, and applies the corresponding rules. [sent-109, score-0.271]
</p><p>51 For example, suppose we have a rule {Han → Chinese}. [sent-111, score-0.073]
</p><p>52 n W 1e3 w68o Huldan w raenbte tlos drove out the Mongols”, but to avoid applying it to a sentence like “The history of the Han ethnic group is closely tied to that of China ”. [sent-113, score-0.104]
</p><p>53 The existence of related words like ethnic and China are clues that the latter sentence is in a specific, rather than general, context and therefore a more general and simpler hypernym is unsuitable. [sent-114, score-0.241]
</p><p>54 To identify such cases, we calculate the similarity between the target word (the candidate for replacement) and the input sentence as a whole. [sent-115, score-0.275]
</p><p>55 If this similarity is too high, it might be better not to simplify the original word. [sent-116, score-0.201]
</p><p>56 We wish to detect and avoid cases where a word appears in the sentence with a different sense than the one originally considered when creating the simplification rule. [sent-118, score-0.817]
</p><p>57 For this purpose, we  examine the similarity between the rule as a whole (including both the original and the substitute words, and their associated context vectors) and the context of the input sentence. [sent-119, score-0.398]
</p><p>58 If the similarity is high, it is likely the original word in the sentence and the rule are about the same sense. [sent-120, score-0.288]
</p><p>59 1 Simplification Procedure Both factors described above require sufficient context in the input sentence. [sent-123, score-0.075]
</p><p>60 Therefore, our system does not attempt to simplify sentences with less than seven content words. [sent-124, score-0.071]
</p><p>61 58% Table 1: Average scores in three categories: grammaticality (Gram. [sent-136, score-0.163]
</p><p>62 For grammaticality, we show percent of examples judged as good, with ok percent in parentheses. [sent-140, score-0.12]
</p><p>63 For all other sentences, each content word is examined in order, ignoring words inside quotation marks or parentheses. [sent-141, score-0.062]
</p><p>64 For each word w, the set of relevant simplification rules {w → x} is retrieved. [sent-142, score-0.827]
</p><p>65 rFeolre eaancth s i rmuplel {w → x}, u {nwles →s th xe} replacement Fwoorrd e x already appears xin} ,th uen sentence, our system does the following: •  •  •  Build the vector of sentence context SCVs,w in a Bsimuiil dar th manner t oof t sheant ednesccer ciobendte xint SSeCcVtion 3. [sent-143, score-0.146]
</p><p>66 Create a common context vector CCVw,x for the rCurleea {w → x}. [sent-148, score-0.043]
</p><p>67 We calculate the cosine similarity of the common context vector and the sentence context vector: ContextSim = cosine(CCVw,x, SCVs,w) If the context similarity is larger than a threshold (0. [sent-152, score-0.412]
</p><p>68 If multiple rules apply for the same word, we use the one with the highest context similarity. [sent-154, score-0.11]
</p><p>69 4  Experimental Setup  Baseline We employ the method of Devlin and Unthank (2006) which replaces a word with its most frequent synonym (presumed to be the simplest) as our baseline. [sent-155, score-0.078]
</p><p>70 To provide a fairer comparison to our system, we add the restriction that the synonyms should not share a prefix of four or more letters (a baseline version of lemmatization) and use MorphAdorner to produce a form that agrees with that of the original word. [sent-156, score-0.118]
</p><p>71 %738165% Evaluation  Dataset  We sampled simplification  examples for manual evaluation with the following criteria. [sent-168, score-0.732]
</p><p>72 Among all sentences in English Wikipedia, we first extracted those where our system chose to simplify exactly one word, to provide a straightforward example for the human judges. [sent-169, score-0.071]
</p><p>73 Of these, we chose the sentences where the baseline could also be used to simplify the target word (i. [sent-170, score-0.134]
</p><p>74 , the word had a more frequent synonym), and the baseline replacement was different from the system choice. [sent-172, score-0.109]
</p><p>75 Each was simplified by our system and the baseline, resulting in 130 simplification examples (consisting of an original and a simplified sentence). [sent-175, score-1.006]
</p><p>76 Frequency Bands Although we included only a single example of each rule, some rules could be applied much more frequently than others, as the words and associated contexts were common in the dataset. [sent-176, score-0.067]
</p><p>77 Since this factor strongly influences the utility of the system, we examined the performance along different frequency bands. [sent-177, score-0.123]
</p><p>78 We split the evaluation dataset into three frequency bands of roughly equal size, resulting in 46 high, 44 med and 40 low. [sent-178, score-0.132]
</p><p>79 Judgment Guidelines We divided the simplification examples among three annotators and ensured that no annotator saw both the system and baseline examples for the same sentence. [sent-179, score-0.795]
</p><p>80 A small portion of the sentence pairs were duplicated among annotators to calculate pairwise interannotator agreement. [sent-181, score-0.169]
</p><p>81 Our method is quantitatively better than the baseline at both grammaticality and meaning preservation, although the difference is not statistically significant. [sent-193, score-0.256]
</p><p>82 001) outperforms the baseline, which represents the established simplifi-  cation strategy of substituting a word with its most frequent WordNet synonym. [sent-195, score-0.064]
</p><p>83 The results demonstrate the value of correctly representing and addressing content when attempting automatic simplification. [sent-196, score-0.051]
</p><p>84 Table 2 contains the results for each of the frequency bands. [sent-197, score-0.056]
</p><p>85 Grammaticality is not strongly influenced by frequency, and remains between 80-85% for both the baseline and our system (considering the ok judgment as positive). [sent-198, score-0.128]
</p><p>86 This is not surprising, since the method for ensuring grammaticality is largely independent of context, and relies mostly on a morphological engine. [sent-199, score-0.215]
</p><p>87 Simplification varies somewhat with frequency, with the best results for the medium frequency band. [sent-200, score-0.093]
</p><p>88 The most noticeable effect is for preservation of meaning. [sent-202, score-0.129]
</p><p>89 Here, the performance of the system (and the baseline) is the best for the medium frequency group. [sent-203, score-0.093]
</p><p>90 However, the performance drops significantly for the low frequency band. [sent-204, score-0.056]
</p><p>91 Since there are few examples from which to learn, the system is unable to effectively distinguish between different contexts and meanings ofthe word being simplified, and applies the simplification rule  incorrectly. [sent-206, score-0.833]
</p><p>92 These results indicate our system can be effectively used for simplification of words that occur frequently in the domain. [sent-207, score-0.732]
</p><p>93 In many scenarios, these are precisely the cases where simplification is most desirable. [sent-208, score-0.732]
</p><p>94 For rare words, it may be advisable to maintain the more complex form, to ensure that the meaning is preserved. [sent-209, score-0.127]
</p><p>95 Query expansion, lexical simplification, and sentence selection strategies for multi-document summarization. [sent-224, score-0.089]
</p><p>96 Practical simplication of english newspaper text to assist aphasic readers. [sent-229, score-0.086]
</p><p>97 Automatic sentence simplification for subtitling in Dutch and English. [sent-241, score-0.789]
</p><p>98 Extracting lay paraphrases of specialized expressions from monolingual comparable medical corpora. [sent-247, score-0.153]
</p><p>99 Comparing evaluation techniques for text readability software for adults with intellectual disabilities. [sent-269, score-0.134]
</p><p>100 Towards effective sentence simplification for automatic processing of biomedical text. [sent-275, score-0.822]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('simplification', 0.732), ('grammaticality', 0.163), ('sew', 0.155), ('preservation', 0.129), ('cvw', 0.124), ('simplified', 0.109), ('wikipedia', 0.101), ('devlin', 0.095), ('stride', 0.093), ('elhadad', 0.077), ('substitute', 0.077), ('bands', 0.076), ('noemie', 0.076), ('similarity', 0.074), ('rule', 0.073), ('fi', 0.072), ('simplify', 0.071), ('rules', 0.067), ('complexity', 0.066), ('blake', 0.062), ('emie', 0.062), ('huenerfauth', 0.062), ('magnate', 0.062), ('unthank', 0.062), ('ok', 0.06), ('ew', 0.058), ('meaning', 0.058), ('stage', 0.058), ('sentence', 0.057), ('original', 0.056), ('frequency', 0.056), ('adults', 0.055), ('aphasic', 0.055), ('chandrasekar', 0.055), ('jonnalagadda', 0.055), ('napoles', 0.055), ('siobhan', 0.055), ('vickrey', 0.055), ('simpler', 0.053), ('ensuring', 0.052), ('attempting', 0.051), ('walked', 0.05), ('androutsopoulos', 0.05), ('intellectual', 0.05), ('nelken', 0.05), ('synonym', 0.05), ('wordnet', 0.049), ('pairs', 0.048), ('candidate', 0.048), ('ethnic', 0.047), ('yatskar', 0.047), ('replacement', 0.046), ('substitution', 0.046), ('ger', 0.045), ('siddharthan', 0.045), ('articles', 0.044), ('context', 0.043), ('columbia', 0.043), ('williams', 0.043), ('lay', 0.043), ('cosine', 0.042), ('monolingual', 0.042), ('hypernym', 0.041), ('complex', 0.041), ('mccarthy', 0.04), ('daelemans', 0.039), ('dimension', 0.038), ('cw', 0.038), ('carroll', 0.038), ('histories', 0.038), ('lw', 0.038), ('medium', 0.037), ('calculate', 0.036), ('walk', 0.036), ('substituting', 0.036), ('comparable', 0.035), ('baseline', 0.035), ('han', 0.035), ('lemmatization', 0.034), ('examined', 0.034), ('judgment', 0.033), ('influences', 0.033), ('medical', 0.033), ('people', 0.033), ('biomedical', 0.033), ('investigated', 0.032), ('input', 0.032), ('lexical', 0.032), ('english', 0.031), ('percent', 0.03), ('readability', 0.029), ('readers', 0.029), ('del', 0.029), ('fellbaum', 0.028), ('word', 0.028), ('annotators', 0.028), ('ensure', 0.028), ('prefix', 0.027), ('vte', 0.027), ('prodromos', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="254-tfidf-1" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>Author: Or Biran ; Samuel Brody ; Noemie Elhadad</p><p>Abstract: We present a method for lexical simplification. Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.</p><p>2 0.627666 <a title="254-tfidf-2" href="./acl-2011-Simple_English_Wikipedia%3A_A_New_Text_Simplification_Task.html">283 acl-2011-Simple English Wikipedia: A New Text Simplification Task</a></p>
<p>Author: William Coster ; David Kauchak</p><p>Abstract: In this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based trans- lation approach for simplification.</p><p>3 0.089141294 <a title="254-tfidf-3" href="./acl-2011-Extracting_Comparative_Entities_and_Predicates_from_Texts_Using_Comparative_Type_Classification.html">130 acl-2011-Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification</a></p>
<p>Author: Seon Yang ; Youngjoong Ko</p><p>Abstract: The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use. 1</p><p>4 0.083500557 <a title="254-tfidf-4" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>Author: Emmanuel Prochasson ; Pascale Fung</p><p>Abstract: We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.</p><p>5 0.069992363 <a title="254-tfidf-5" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>Author: Matt Post</p><p>Abstract: In this paper, we show that local features computed from the derivations of tree substitution grammars such as the identify of particular fragments, and a count of large and small fragments are useful in binary grammatical classification tasks. Such features outperform n-gram features and various model scores by a wide margin. Although they fall short of the performance of the hand-crafted feature set of Charniak and Johnson (2005) developed for parse tree reranking, they do so with an order of magnitude fewer features. Furthermore, since the TSGs employed are learned in a Bayesian setting, the use of their derivations can be viewed as the automatic discovery of tree patterns useful for classification. On the BLLIP dataset, we achieve an accuracy of 89.9% in discriminating between grammatical text and samples from an n-gram language model. — —</p><p>6 0.068029776 <a title="254-tfidf-6" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>7 0.065601163 <a title="254-tfidf-7" href="./acl-2011-Wikipedia_Revision_Toolkit%3A_Efficiently_Accessing_Wikipedias_Edit_History.html">337 acl-2011-Wikipedia Revision Toolkit: Efficiently Accessing Wikipedias Edit History</a></p>
<p>8 0.062582873 <a title="254-tfidf-8" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>9 0.062074963 <a title="254-tfidf-9" href="./acl-2011-Extracting_Paraphrases_from_Definition_Sentences_on_the_Web.html">132 acl-2011-Extracting Paraphrases from Definition Sentences on the Web</a></p>
<p>10 0.061704993 <a title="254-tfidf-10" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>11 0.060874466 <a title="254-tfidf-11" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>12 0.06051816 <a title="254-tfidf-12" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>13 0.059925415 <a title="254-tfidf-13" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>14 0.059736855 <a title="254-tfidf-14" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>15 0.058738388 <a title="254-tfidf-15" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>16 0.05597005 <a title="254-tfidf-16" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>17 0.05265912 <a title="254-tfidf-17" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>18 0.051229432 <a title="254-tfidf-18" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>19 0.051175974 <a title="254-tfidf-19" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>20 0.05070908 <a title="254-tfidf-20" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (1, -0.009), (2, -0.023), (3, 0.074), (4, -0.013), (5, -0.006), (6, 0.039), (7, 0.017), (8, -0.089), (9, -0.09), (10, -0.086), (11, 0.007), (12, -0.023), (13, 0.018), (14, 0.005), (15, 0.009), (16, 0.257), (17, -0.023), (18, -0.007), (19, -0.16), (20, 0.096), (21, -0.191), (22, -0.156), (23, -0.222), (24, 0.278), (25, -0.035), (26, 0.05), (27, -0.093), (28, 0.15), (29, -0.05), (30, 0.014), (31, -0.112), (32, -0.094), (33, -0.03), (34, -0.285), (35, -0.152), (36, 0.139), (37, 0.015), (38, -0.062), (39, -0.05), (40, 0.123), (41, -0.214), (42, -0.062), (43, -0.136), (44, -0.002), (45, -0.085), (46, 0.089), (47, 0.028), (48, -0.147), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94793946 <a title="254-lsi-1" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>Author: Or Biran ; Samuel Brody ; Noemie Elhadad</p><p>Abstract: We present a method for lexical simplification. Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.</p><p>2 0.90590584 <a title="254-lsi-2" href="./acl-2011-Simple_English_Wikipedia%3A_A_New_Text_Simplification_Task.html">283 acl-2011-Simple English Wikipedia: A New Text Simplification Task</a></p>
<p>Author: William Coster ; David Kauchak</p><p>Abstract: In this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based trans- lation approach for simplification.</p><p>3 0.49547583 <a title="254-lsi-3" href="./acl-2011-Wikipedia_Revision_Toolkit%3A_Efficiently_Accessing_Wikipedias_Edit_History.html">337 acl-2011-Wikipedia Revision Toolkit: Efficiently Accessing Wikipedias Edit History</a></p>
<p>Author: Oliver Ferschke ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: We present an open-source toolkit which allows (i) to reconstruct past states of Wikipedia, and (ii) to efficiently access the edit history of Wikipedia articles. Reconstructing past states of Wikipedia is a prerequisite for reproducing previous experimental work based on Wikipedia. Beyond that, the edit history of Wikipedia articles has been shown to be a valuable knowledge source for NLP, but access is severely impeded by the lack of efficient tools for managing the huge amount of provided data. By using a dedicated storage format, our toolkit massively decreases the data volume to less than 2% of the original size, and at the same time provides an easy-to-use interface to access the revision data. The language-independent design allows to process any language represented in Wikipedia. We expect this work to consolidate NLP research using Wikipedia in general, and to foster research making use of the knowledge encoded in Wikipedia’s edit history.</p><p>4 0.4900102 <a title="254-lsi-4" href="./acl-2011-Extracting_Comparative_Entities_and_Predicates_from_Texts_Using_Comparative_Type_Classification.html">130 acl-2011-Extracting Comparative Entities and Predicates from Texts Using Comparative Type Classification</a></p>
<p>Author: Seon Yang ; Youngjoong Ko</p><p>Abstract: The automatic extraction of comparative information is an important text mining problem and an area of increasing interest. In this paper, we study how to build a Korean comparison mining system. Our work is composed of two consecutive tasks: 1) classifying comparative sentences into different types and 2) mining comparative entities and predicates. We perform various experiments to find relevant features and learning techniques. As a result, we achieve outstanding performance enough for practical use. 1</p><p>5 0.4089607 <a title="254-lsi-5" href="./acl-2011-Language_of_Vandalism%3A_Improving_Wikipedia_Vandalism_Detection_via_Stylometric_Analysis.html">195 acl-2011-Language of Vandalism: Improving Wikipedia Vandalism Detection via Stylometric Analysis</a></p>
<p>Author: Manoj Harpalani ; Michael Hart ; Sandesh Signh ; Rob Johnson ; Yejin Choi</p><p>Abstract: Community-based knowledge forums, such as Wikipedia, are susceptible to vandalism, i.e., ill-intentioned contributions that are detrimental to the quality of collective intelligence. Most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in Wikipedia. In this paper, we explore more linguistically motivated approaches to vandalism detection. In particular, we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior. Experimental results suggest that (1) statistical models give evidence to unique language styles in vandalism, and that (2) deep syntactic patterns based on probabilistic context free grammars (PCFG) discriminate vandalism more effectively than shallow lexicosyntactic patterns based on n-grams. ,</p><p>6 0.33741099 <a title="254-lsi-6" href="./acl-2011-Local_and_Global_Algorithms_for_Disambiguation_to_Wikipedia.html">213 acl-2011-Local and Global Algorithms for Disambiguation to Wikipedia</a></p>
<p>7 0.30132881 <a title="254-lsi-7" href="./acl-2011-Contrasting_Opposing_Views_of_News_Articles_on_Contentious_Issues.html">84 acl-2011-Contrasting Opposing Views of News Articles on Contentious Issues</a></p>
<p>8 0.2882615 <a title="254-lsi-8" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>9 0.28024521 <a title="254-lsi-9" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>10 0.27732119 <a title="254-lsi-10" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>11 0.2688328 <a title="254-lsi-11" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>12 0.26379576 <a title="254-lsi-12" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>13 0.26049253 <a title="254-lsi-13" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>14 0.25164077 <a title="254-lsi-14" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>15 0.2516138 <a title="254-lsi-15" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>16 0.25029486 <a title="254-lsi-16" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>17 0.25001419 <a title="254-lsi-17" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>18 0.24779923 <a title="254-lsi-18" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<p>19 0.23718877 <a title="254-lsi-19" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>20 0.23518929 <a title="254-lsi-20" href="./acl-2011-Extracting_Paraphrases_from_Definition_Sentences_on_the_Web.html">132 acl-2011-Extracting Paraphrases from Definition Sentences on the Web</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.027), (16, 0.111), (17, 0.062), (24, 0.172), (26, 0.033), (31, 0.01), (37, 0.071), (39, 0.048), (41, 0.049), (53, 0.013), (55, 0.028), (59, 0.054), (72, 0.038), (91, 0.051), (96, 0.129), (97, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80062628 <a title="254-lda-1" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>Author: Or Biran ; Samuel Brody ; Noemie Elhadad</p><p>Abstract: We present a method for lexical simplification. Simplification rules are learned from a comparable corpus, and the rules are applied in a context-aware fashion to input sentences. Our method is unsupervised. Furthermore, it does not require any alignment or correspondence among the complex and simple corpora. We evaluate the simplification according to three criteria: preservation of grammaticality, preservation of meaning, and degree of simplification. Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification, while maintaining a high level of grammaticality.</p><p>2 0.7731545 <a title="254-lda-2" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>Author: Yunyao Li ; Frederick Reiss ; Laura Chiticariu</p><p>Abstract: Frederick R. Reiss IBM Research - Almaden 650 Harry Road San Jose, CA 95120 frre i s @us . ibm . com s Laura Chiticariu IBM Research - Almaden 650 Harry Road San Jose, CA 95120 chit i us .ibm . com @ magnitude larger than classical IE corpora. An Emerging text-intensive enterprise applications such as social analytics and semantic search pose new challenges of scalability and usability to Information Extraction (IE) systems. This paper presents SystemT, a declarative IE system that addresses these challenges and has been deployed in a wide range of enterprise applications. SystemT facilitates the development of high quality complex annotators by providing a highly expressive language and an advanced development environment. It also includes a cost-based optimizer and a high-performance, flexible runtime with minimum memory footprint. We present SystemT as a useful resource that is freely available, and as an opportunity to promote research in building scalable and usable IE systems.</p><p>3 0.74036813 <a title="254-lda-3" href="./acl-2011-A_Cross-Lingual_ILP_Solution_to_Zero_Anaphora_Resolution.html">9 acl-2011-A Cross-Lingual ILP Solution to Zero Anaphora Resolution</a></p>
<p>Author: Ryu Iida ; Massimo Poesio</p><p>Abstract: We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. We show that this new model outperforms several baselines and competing models, as well as a direct translation of the Denis / Baldridge model, for both Italian and Japanese zero anaphora. We incorporate our model in complete anaphoric resolvers for both Italian and Japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classifiers are used for zeros and for ex- plicitly realized anaphors.</p><p>4 0.72650045 <a title="254-lda-4" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>Author: Dirk Hovy ; Chunliang Zhang ; Eduard Hovy ; Anselmo Penas</p><p>Abstract: Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicateargument structures like “quarterbacks throw passes to receivers”. Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.</p><p>5 0.70789242 <a title="254-lda-5" href="./acl-2011-Simple_English_Wikipedia%3A_A_New_Text_Simplification_Task.html">283 acl-2011-Simple English Wikipedia: A New Text Simplification Task</a></p>
<p>Author: William Coster ; David Kauchak</p><p>Abstract: In this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based trans- lation approach for simplification.</p><p>6 0.69391882 <a title="254-lda-6" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>7 0.66795623 <a title="254-lda-7" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>8 0.66615361 <a title="254-lda-8" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>9 0.66580075 <a title="254-lda-9" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>10 0.66563827 <a title="254-lda-10" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>11 0.66328955 <a title="254-lda-11" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>12 0.66204709 <a title="254-lda-12" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>13 0.66152424 <a title="254-lda-13" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>14 0.66006148 <a title="254-lda-14" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>15 0.66002667 <a title="254-lda-15" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>16 0.65911579 <a title="254-lda-16" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>17 0.6587953 <a title="254-lda-17" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>18 0.65828347 <a title="254-lda-18" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>19 0.65759146 <a title="254-lda-19" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>20 0.6561799 <a title="254-lda-20" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
