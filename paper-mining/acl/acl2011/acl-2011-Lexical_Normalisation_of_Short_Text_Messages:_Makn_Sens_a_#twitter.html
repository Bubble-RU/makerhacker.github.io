<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-208" href="#">acl2011-208</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</h1>
<br/><p>Source: <a title="acl-2011-208-pdf" href="http://aclweb.org/anthology//P/P11/P11-1038.pdf">pdf</a></p><p>Author: Bo Han ; Timothy Baldwin</p><p>Abstract: Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</p><p>Reference: <a title="acl-2011-208-reference" href="../acl2011_reference/acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. [sent-4, score-0.175]
</p><p>2 Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. [sent-5, score-0.153]
</p><p>3 Both word similarity and context are then exploited to select the most probable correction candidate for the word. [sent-6, score-0.171]
</p><p>4 The quality of messages varies significantly, however, ranging from high quality newswire-like text to meaningless strings. [sent-9, score-0.149]
</p><p>5 Typos, ad hoc abbreviations, phonetic substitutions, ungrammatical structures and emoticons abound in short text messages, causing grief for text processing tools (Sproat et al. [sent-10, score-0.109]
</p><p>6 For instance, presented with the input u must be talkin bout the paper but I was thinkin movies (“You must be talking about the paper but Iwas thinking movies”),1 the Stanford parser (Klein and 1Throughout the paper, we will provide a normalised version of examples as a gloss in double quotes. [sent-13, score-0.218]
</p><p>7 If there were some way of preprocessing the message to produce a more canonical lexical rendering, we would expect the quality of the parser to improve appreciably. [sent-18, score-0.133]
</p><p>8 Our aim in this paper is this task of lexical normalisation of noisy  English text, with a particular focus on Twitter and SMS messages. [sent-19, score-0.623]
</p><p>9 In this paper, we will collectively refer to individual instances of typos, ad hoc abbreviations, unconventional spellings, phonetic substitutions and other causes of lexical deviation as “illformed words”. [sent-20, score-0.149]
</p><p>10 It has similarities with spell checking (Peterson, 1980), but differs in that ill-formedness in text messages is often intentional, whether due to the desire to save characters/keystrokes, for social identity, or due to convention in this text sub-genre. [sent-22, score-0.225]
</p><p>11 We propose to go beyond spell checkers, in performing deabbreviation when appropriate, and recovering the canonical word form of commonplace shorthands like b4 “before”, which tend to be considered beyond the remit of spell checking (Aw et al. [sent-23, score-0.21]
</p><p>12 The free writing style of text messages makes the task even more complex, e. [sent-25, score-0.149]
</p><p>13 In addition, the detection of ill-formed words is difficult due to noisy context. [sent-28, score-0.149]
</p><p>14 Our objective is to restore ill-formed words to their canonical lexical forms in standard English. [sent-29, score-0.131]
</p><p>15 Additionally, many illformed words are ambiguous, and require context to disambiguate. [sent-34, score-0.192]
</p><p>16 Our approach first generates a list of candidate canonical lexical forms, based on morphological and phonetic variation. [sent-37, score-0.197]
</p><p>17 Then, all candidates are ranked according to a list of features generated from noisy context and similarity between ill-formed words and candidates. [sent-38, score-0.248]
</p><p>18 2  Related work  The noisy channel model (Shannon, 1948) has traditionally been the primary approach to tackling text normalisation. [sent-41, score-0.173]
</p><p>19 (2007) model  the word-level text generation process for SMS messages, by considering graphemic/phonetic abbreviations and unintentional typos as hidden Markov 369 model (HMM) state transitions and emissions, respectively (Rabiner, 1989). [sent-47, score-0.128]
</p><p>20 While the noisy channel model is appropriate for text normalisation, P(T|S), which encodes the underlying error production process, i se hcaodrde sto th approximate accurately. [sent-49, score-0.173]
</p><p>21 (2006) propose a phrase-level SMT SMS normalisation method with bootstrapped phrase alignments. [sent-53, score-0.512]
</p><p>22 Furthermore, it is hard to harness SMT for the lexical normalisation problem, as even if phrase-level re-ordering is suppressed by constraints on phrase segmentation, word-level re-orderings within a phrase are still prevalent. [sent-56, score-0.556]
</p><p>23 Some researchers have also formulated text normalisation as a speech recognition problem. [sent-57, score-0.543]
</p><p>24 (2008) firstly convert input text tokens into phonetic tokens and then restore them to words by phonetic dictionary lookup. [sent-59, score-0.345]
</p><p>25 (2010) use finite state methods to perform French SMS normalisation, combining the advantages of SMT and the noisy channel model. [sent-61, score-0.142]
</p><p>26 1 Task Definition of Lexical Normalisation We define the task of text normalisation to be a mapping from “ill-formed” OOV lexical items to their standard lexical forms, focusing exclusively on English for the purposes of this paper. [sent-67, score-0.631]
</p><p>27 Given this definition, our first step is to identify candidate tokens for lexical normalisation, where we examine all tokens that consist of alphanumeric characters, and categorise them into in-vocabulary (IV) and out-of-vocabulary (OOV) words, relative to a dictionary. [sent-69, score-0.216]
</p><p>28 However, it greatly simplifies the candidate identification task, at the cost of pushing complexity downstream to the word detection task, in that we need to explicitly distinguish between correct OOV words and illformed OOV words such as typos (e. [sent-71, score-0.368]
</p><p>29 We also consider that deabbreviation largely falls outside the scope of text normalisation, as abbreviations can be formed freely in standard English. [sent-81, score-0.139]
</p><p>30 Note that single-word abbreviations such as govt “government” are very much within the scope of lexical normalisation, as they are OOV and match to a single token in their standard lexical form. [sent-82, score-0.212]
</p><p>31 In tokenising the text, hyphenanted tokens and tokens containing apostrophes (e. [sent-86, score-0.104]
</p><p>32 Dictionary lookup of Internet slang is performed relative to a dictionary of 5021 items collected from the Internet. [sent-98, score-0.24]
</p><p>33 In particular,  we calculate the proportion of OOV tokens per message (or sentence, in the case of edited text), bin the messages according to the OOV token proportion, and plot the probability mass contained in each bin for a given text type. [sent-101, score-0.323]
</p><p>34 This suggests that Twitter is a richer/noisier data source, and that text normalisation for Twitter needs to be more nuanced than for SMS. [sent-114, score-0.543]
</p><p>35 To further analyse the ill-formed words in Twitter, we randomly selected 449 tweets and manually analysed the sources of lexical variation, to determine the phenomena that lexical normalisation needs to deal with. [sent-115, score-0.72]
</p><p>36 We identified 254 token instances of lexical normalisation, and broke them down into categories, as listed in Table 1. [sent-116, score-0.124]
</p><p>37 “Letter” refers to instances where letters are missing or there are extraneous letters, but the lexical correspondence to the target word form is trivially accessible (e. [sent-117, score-0.144]
</p><p>38 6 thousand SMS messages from How and Kan (2005) and Choudhury et al. [sent-128, score-0.118]
</p><p>39 24% Table 1: Ill-formed word distribution of Internet slang (e. [sent-138, score-0.12]
</p><p>40 lol “laugh out loud”), as found in a slang dictionary (see Section 3. [sent-140, score-0.161]
</p><p>41 From Table 1, it is clear that “Letter” accounts for the majority of ill-formed words in Twitter, and that most ill-formed words are based on morphophonemic variations. [sent-148, score-0.127]
</p><p>42 In confusion set generation, we generate a set of IV normalisation candidates for each OOV word type based on morphophonemic variation. [sent-151, score-0.801]
</p><p>43 We call this set the confusion set of that OOV word, and aim to include all feasible normalisation candidates for the word type in the confusion set. [sent-152, score-0.836]
</p><p>44 The confusion candidates are then filtered for each token occurrence of  a given OOV word, based on their local context fit with a language model. [sent-153, score-0.278]
</p><p>45 7% 9515 ≤  2  ∨  T≤  2  Table 2: Recall and average number of candidates for different confusion set generation strategies duced to coool). [sent-166, score-0.188]
</p><p>46 Second, IV words within a threshold Tc character edit distance of the given OOV word are calculated, as is widely used in spell check-  ers. [sent-167, score-0.188]
</p><p>47 In Table 2, we list the recall and average size of the confusion set generated by the final two strategies with different threshold settings, based on our evaluation dataset (see Section 5. [sent-169, score-0.168]
</p><p>48 The recall for lexical edit distance with Tc ≤ 2 is moderately high, but it is unable to detect the correct candidate for about one quarter of words. [sent-171, score-0.215]
</p><p>49 Based on these results, we use Tc ≤ 2 ∨ Tp ≤ 1as the basis for confusion set generation. [sent-176, score-0.11]
</p><p>50 In addition to generating the confusion set, we rank the candidates based on a trigram language model trained over 1. [sent-178, score-0.188]
</p><p>51 2 Ill-formed Word Detection The next step is to detect whether a given OOV word in context is actually an ill-formed word or not, relative to its confusion set. [sent-185, score-0.205]
</p><p>52 To the best of our knowledge, we are the first to target the task of ill-formed word detection in the context of short text messages, although related work exists for text with lower relative occurrences of OOV words (Izumi et al. [sent-186, score-0.213]
</p><p>53 Note that we don’t record the dependency type here, because we have no intention of dependency parsing  ,  ,  text messages, due to their noisiness and the volume of the data. [sent-202, score-0.124]
</p><p>54 The counts of dependency forms are combined together to derive a confidence score, and the scored dependencies are stored in a dependency bank. [sent-203, score-0.124]
</p><p>55 Each word is represented by its IV words within a context window of three words to either side of the target word, together with their relative positions in the form of ( word1 word2 po s it ion ) tuples, and their score in the dependency bank. [sent-208, score-0.181]
</p><p>56 Negative exemplars are automatically constructed by replacing target words with highly-ranked candidates from their confusion set. [sent-210, score-0.255]
</p><p>57 To predict whether a given OOV word is ill-formed, we form an exemplar for each of its confusion candidates, and extract ( word1 word2 po s it i ) features. [sent-212, score-0.161]
</p><p>58 on If  ,  ,  ,  ,  all its candidates are predicted to be negative by the model, we mark it as correct; otherwise, we treat it as ill-formed, and pass all candidates (not just positively-classified candidates) on to the candidate selection step. [sent-213, score-0.255]
</p><p>59 For example, given the message way yu lookin shuld be a sin and the OOV word lookin, we would generate context features for each candidate word such as (way looking, -2 ) , and classify each such candidate. [sent-214, score-0.276]
</p><p>60 The ( word1 word2 po s it ion ) features are sparse and sometimes lead to conservative results in illformed word detection. [sent-217, score-0.174]
</p><p>61 As the context for a target word often contains OOV words which don’t occur in the dependency  ,  ,  ,  bank, we expand the dependency features to include context tokens up to a phonemic edit distance of 1 from context tokens in the dependency bank. [sent-220, score-0.517]
</p><p>62 In this way, we generate dependency-based features for context words such as seee “see” in ( seee flm, +2 ) (based on the target word flm in the context of flm to seee). [sent-221, score-0.322]
</p><p>63 However, expanded dependency features may introduce noise, and we therefore introduce expanded dependency weights wd ∈ 373 {0. [sent-222, score-0.172]
</p><p>64 , by }th oen d theteec ntuiomnb ecrla ossfi pfioesri over trehed scetito nofs normalisation candidates for a given OOV token: the token is considered to be ill-formed iff td or more candidates are positively classified, i. [sent-234, score-0.777]
</p><p>65 3 Candidate Selection For OOV words which are predicted to be illformed, we select the most likely candidate from the  confusion set as the basis of normalisation. [sent-238, score-0.204]
</p><p>66 Lexical edit distance, phonemic edit distance, prefix substring, suffix substring, and the longest common subsequence (LCS) are exploited to capture morphophonemic similarity. [sent-241, score-0.218]
</p><p>67 Both lexical and phonemic edit distance (ED) are normalised by the reciprocal of exp(ED) . [sent-242, score-0.238]
</p><p>68 To consolidate the context modelling, we obtain dependencies from the dependency bank used in ill-formed word detection. [sent-251, score-0.181]
</p><p>69 Although text messages are of a different genre to edited newswire text, we assume they form similar dependencies based on the common goal of getting across the message effectively. [sent-252, score-0.278]
</p><p>70 For example, uz “use” in i did #tt uz me and yu, dependencies can capture relationships like aux ( use-4 do-2 ) , which is beyond the capabilities of the language model due to the hashtag being treated as a correct OOV word. [sent-254, score-0.108]
</p><p>71 We reimplemented the state-of-art noisy channel model of Cook and Stevenson (2009) and SMT approach of Aw et al. [sent-263, score-0.142]
</p><p>72 We separately compare dictionary lookup over our Internet slang dictionary, the contextual feature model, and the word similarity feature model, as well as combinations of these three. [sent-278, score-0.3]
</p><p>73 2 Evaluation metrics The evaluation of lexical normalisation consists of two stages (Hirst and Budanitsky, 2005): (1) illformed word detection, and (2) candidate selection. [sent-280, score-0.773]
</p><p>74 This step is crucial to further normalisation, because if correct  OOV words are identified as ill-formed, the candidate selection step can never be correct. [sent-282, score-0.125]
</p><p>75 Conversely, if an ill-formed word is predicted to be correct, the candidate selection will have no chance to normalise it. [sent-283, score-0.179]
</p><p>76 Previous work over the SMS corpus has assumed perfect ill-formed word detection and focused only on the candidate selection step, so we evaluate ill-formed word detection for the Twitter data only. [sent-285, score-0.263]
</p><p>77 Additionally, we evaluate using the BLEU score over the normalised form of each message, as the SMT method can lead to perturbations of the token stream, vexing standard precision, recall and F-score evaluation. [sent-287, score-0.149]
</p><p>78 3 Results and Analysis First, we test the impact of the wd and td values on ill-formed word detection effectiveness, based on dependencies from either the Spinn3r blog corpus (Blog: Burton et al. [sent-289, score-0.285]
</p><p>79 First, higher detection threshold values (td) give better precision but lower recall. [sent-293, score-0.111]
</p><p>80 Generally, as td is raised from 1 to 10, the precision improves slightly but recall drops dramatically, with the net effect that the F-score decreases monotonically. [sent-294, score-0.129]
</p><p>81 Thus, we use a  Figure 2: Ill-formed word detection precision, recall and F-score smaller threshold, i. [sent-295, score-0.117]
</p><p>82 3%, obtained over the Blog corpus with td = 1and wd = 0. [sent-308, score-0.11]
</p><p>83 We leave the improvement of ill-formed word detection for future work, and perform evaluation of candidate selection for Twitter assuming perfect ill-formed word detection, as for the SMS data. [sent-311, score-0.207]
</p><p>84 In our annotation, the annotators only normalised ill-formed word if they had high confidence 375 of how to normalise, as with talkin “talking”. [sent-314, score-0.124]
</p><p>85 First, the Cook and Stevenson (2009) method is type-  based, so all token instances of a given ill-formed word will be normalised identically. [sent-320, score-0.173]
</p><p>86 Second, the noisy channel method was developed specifically for SMS normalisation, in which clipping is the most prevalent form of lexical variation, while in the Twitter data, we commonly have instances of word lengthening for emphasis, such as moviiie “movie”. [sent-326, score-0.276]
</p><p>87 Having said this, our method is superior to the noisy channel method over both the SMS and Twitter data. [sent-327, score-0.142]
</p><p>88 The dictionary lookup method (“DL”) unsurprisingly achieves the best precision, but the recall on Twitter is not competitive. [sent-332, score-0.208]
</p><p>89 Consequently, the Twitter normalisation cannot be tackled with dictionary lookup alone, although it is an effective preprocessing strategy when combined with more ro-  TSDMwaitesrRFPEBL-evscaElioUusareotin0 . [sent-333, score-0.658]
</p><p>90 , 2006); DL  = noisy channel model (Cook and Stevenson, = word similarity; CS = context support)  on different datasets (NC  = dictionary  lookup; WS  bust techniques such as our proposed method, and effective at capturing common abbreviations such as gf “girlfriend”. [sent-341, score-0.332]
</p><p>91 Of the component methods proposed in this research, word similarity (“WS”) achieves higher precision and recall than context support (“CS”), signifying that many of the ill-formed words emanate from morphophonemic variations. [sent-342, score-0.271]
</p><p>92 The best F-score is achieved when combining dictionary lookup, word similarity and context support (“DL+WS+CS”), in which ill-formed words are first looked up in the slang dictionary, and only if no match is found do we apply our normalisation method. [sent-345, score-0.802]
</p><p>93 Some highly noisy tweets contain almost all misspellings and unique symbols, and thus no context features can be extracted. [sent-348, score-0.175]
</p><p>94 Moreover, the IV words may not occur in the dependency bank, further decreasing the effectiveness of context support features. [sent-353, score-0.104]
</p><p>95 376 6  Conclusion and Future Work  In this paper, we have proposed the task of lexical normalisation for short text messages, as found in Twitter and SMS data. [sent-355, score-0.587]
</p><p>96 We found that most illformed words are based on morphophonemic variation and proposed a cascaded method to detect and normalise ill-formed words. [sent-356, score-0.278]
</p><p>97 In normalisation, we compared our method with two benchmark methods from the literature, and achieved that highest F-score and BLEU score by integrating dictionary lookup, word similarity and context support modelling. [sent-358, score-0.17]
</p><p>98 First, we plan to improve our ill-formed word detection classifier by introducing an OOV word whitelist. [sent-360, score-0.108]
</p><p>99 Furthermore, we intend to allevi-  ate noisy contexts with a bootstrapping approach, in which ill-formed words with high confidence and no ambiguity will be replaced by their standard forms, and fed into the normalisation model as new training data. [sent-361, score-0.605]
</p><p>100 An improved error model for noisy channel spelling correction. [sent-378, score-0.173]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('normalisation', 0.512), ('oov', 0.408), ('twitter', 0.404), ('sms', 0.29), ('illformed', 0.123), ('messages', 0.118), ('confusion', 0.11), ('slang', 0.094), ('iv', 0.09), ('cook', 0.089), ('tp', 0.08), ('lookup', 0.079), ('candidates', 0.078), ('morphophonemic', 0.075), ('channel', 0.075), ('tc', 0.073), ('candidate', 0.068), ('dictionary', 0.067), ('normalised', 0.067), ('noisy', 0.067), ('tweets', 0.065), ('nyt', 0.064), ('stevenson', 0.062), ('td', 0.062), ('smt', 0.061), ('phonemic', 0.059), ('detection', 0.056), ('abbreviations', 0.054), ('normalise', 0.054), ('choudhury', 0.054), ('dependencies', 0.054), ('tokens', 0.052), ('message', 0.051), ('wd', 0.048), ('token', 0.047), ('phonetic', 0.047), ('flm', 0.046), ('seee', 0.046), ('spell', 0.045), ('lexical', 0.044), ('context', 0.043), ('typos', 0.043), ('edit', 0.042), ('letters', 0.041), ('exemplars', 0.041), ('clean', 0.04), ('ws', 0.039), ('blog', 0.039), ('aw', 0.038), ('canonical', 0.038), ('recall', 0.035), ('dependency', 0.035), ('similarity', 0.034), ('hw', 0.034), ('instances', 0.033), ('precision', 0.032), ('movies', 0.032), ('selection', 0.031), ('text', 0.031), ('spelling', 0.031), ('bout', 0.031), ('deabbreviation', 0.031), ('kalita', 0.031), ('lengthening', 0.031), ('lookin', 0.031), ('shuld', 0.031), ('talkin', 0.031), ('thinkin', 0.031), ('analyse', 0.029), ('dl', 0.028), ('letter', 0.028), ('burton', 0.027), ('uz', 0.027), ('unsurprisingly', 0.027), ('beaufort', 0.027), ('gon', 0.027), ('metaphone', 0.027), ('expanded', 0.027), ('baldwin', 0.027), ('distance', 0.026), ('word', 0.026), ('double', 0.026), ('pronunciation', 0.026), ('words', 0.026), ('po', 0.025), ('marneffe', 0.025), ('lcs', 0.025), ('nicta', 0.025), ('commonplace', 0.025), ('kobus', 0.025), ('substitutions', 0.025), ('cs', 0.024), ('kaufmann', 0.024), ('edited', 0.024), ('noisiness', 0.023), ('restore', 0.023), ('earthquake', 0.023), ('bank', 0.023), ('threshold', 0.023), ('scope', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="208-tfidf-1" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>Author: Bo Han ; Timothy Baldwin</p><p>Abstract: Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</p><p>2 0.29087633 <a title="208-tfidf-2" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>3 0.25369969 <a title="208-tfidf-3" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>4 0.25005651 <a title="208-tfidf-4" href="./acl-2011-Learning_Sub-Word_Units_for_Open_Vocabulary_Speech_Recognition.html">203 acl-2011-Learning Sub-Word Units for Open Vocabulary Speech Recognition</a></p>
<p>Author: Carolina Parada ; Mark Dredze ; Abhinav Sethy ; Ariya Rastrow</p><p>Abstract: Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of subword units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.</p><p>5 0.20934308 <a title="208-tfidf-5" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>Author: Thomas Mueller ; Hinrich Schuetze</p><p>Abstract: We present a class-based language model that clusters rare words of similar morphology together. The model improves the prediction of words after histories containing outof-vocabulary words. The morphological features used are obtained without the use of labeled data. The perplexity improvement compared to a state of the art Kneser-Ney model is 4% overall and 81% on unknown histories.</p><p>6 0.19560905 <a title="208-tfidf-6" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>7 0.14257155 <a title="208-tfidf-7" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>8 0.11862677 <a title="208-tfidf-8" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>9 0.1139081 <a title="208-tfidf-9" href="./acl-2011-Identifying_Sarcasm_in_Twitter%3A_A_Closer_Look.html">160 acl-2011-Identifying Sarcasm in Twitter: A Closer Look</a></p>
<p>10 0.11131322 <a title="208-tfidf-10" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>11 0.11065627 <a title="208-tfidf-11" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>12 0.097988024 <a title="208-tfidf-12" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>13 0.09047731 <a title="208-tfidf-13" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>14 0.069685087 <a title="208-tfidf-14" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>15 0.069431975 <a title="208-tfidf-15" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>16 0.069198713 <a title="208-tfidf-16" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>17 0.064040601 <a title="208-tfidf-17" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>18 0.062397398 <a title="208-tfidf-18" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>19 0.058982056 <a title="208-tfidf-19" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>20 0.058839772 <a title="208-tfidf-20" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.181), (1, 0.053), (2, 0.027), (3, 0.009), (4, -0.029), (5, -0.005), (6, 0.083), (7, -0.145), (8, -0.02), (9, 0.168), (10, -0.264), (11, 0.167), (12, 0.169), (13, -0.024), (14, -0.083), (15, -0.037), (16, -0.093), (17, 0.021), (18, 0.061), (19, -0.006), (20, -0.002), (21, -0.069), (22, 0.044), (23, -0.061), (24, 0.057), (25, -0.053), (26, 0.025), (27, -0.008), (28, -0.069), (29, 0.04), (30, -0.019), (31, -0.172), (32, 0.026), (33, 0.002), (34, -0.063), (35, -0.017), (36, -0.152), (37, 0.232), (38, 0.062), (39, -0.073), (40, -0.024), (41, 0.121), (42, -0.07), (43, 0.105), (44, 0.01), (45, 0.01), (46, 0.017), (47, 0.076), (48, 0.097), (49, -0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92397124 <a title="208-lsi-1" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>Author: Bo Han ; Timothy Baldwin</p><p>Abstract: Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</p><p>2 0.85166478 <a title="208-lsi-2" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>3 0.69708437 <a title="208-lsi-3" href="./acl-2011-Learning_Sub-Word_Units_for_Open_Vocabulary_Speech_Recognition.html">203 acl-2011-Learning Sub-Word Units for Open Vocabulary Speech Recognition</a></p>
<p>Author: Carolina Parada ; Mark Dredze ; Abhinav Sethy ; Ariya Rastrow</p><p>Abstract: Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of subword units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.</p><p>4 0.64101374 <a title="208-lsi-4" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>5 0.53496891 <a title="208-lsi-5" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.</p><p>6 0.51932031 <a title="208-lsi-6" href="./acl-2011-Identifying_Sarcasm_in_Twitter%3A_A_Closer_Look.html">160 acl-2011-Identifying Sarcasm in Twitter: A Closer Look</a></p>
<p>7 0.51694399 <a title="208-lsi-7" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>8 0.46784478 <a title="208-lsi-8" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>9 0.45463708 <a title="208-lsi-9" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>10 0.40366504 <a title="208-lsi-10" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<p>11 0.39028063 <a title="208-lsi-11" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>12 0.39026767 <a title="208-lsi-12" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>13 0.38720217 <a title="208-lsi-13" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>14 0.38658035 <a title="208-lsi-14" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>15 0.34167385 <a title="208-lsi-15" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>16 0.33887383 <a title="208-lsi-16" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>17 0.33431405 <a title="208-lsi-17" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<p>18 0.32726082 <a title="208-lsi-18" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>19 0.31926477 <a title="208-lsi-19" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>20 0.31801516 <a title="208-lsi-20" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.012), (5, 0.029), (17, 0.053), (26, 0.029), (29, 0.022), (31, 0.012), (37, 0.083), (39, 0.067), (41, 0.058), (55, 0.035), (59, 0.036), (72, 0.048), (91, 0.038), (92, 0.221), (96, 0.134), (97, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80615252 <a title="208-lda-1" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>Author: Bo Han ; Timothy Baldwin</p><p>Abstract: Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</p><p>2 0.79141724 <a title="208-lda-2" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>Author: Hakan Ceylan ; Rada Mihalcea</p><p>Abstract: We introduce a new publicly available tool that implements efficient indexing and retrieval of large N-gram datasets, such as the Web1T 5-gram corpus. Our tool indexes the entire Web1T dataset with an index size of only 100 MB and performs a retrieval of any N-gram with a single disk access. With an increased index size of 420 MB and duplicate data, it also allows users to issue wild card queries provided that the wild cards in the query are contiguous. Furthermore, we also implement some of the smoothing algorithms that are designed specifically for large datasets and are shown to yield better language models than the traditional ones on the Web1T 5gram corpus (Yuret, 2008). We demonstrate the effectiveness of our tool and the smoothing algorithms on the English Lexical Substi- tution task by a simple implementation that gives considerable improvement over a basic language model.</p><p>3 0.79035568 <a title="208-lda-3" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>4 0.70288962 <a title="208-lda-4" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>5 0.69828457 <a title="208-lda-5" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>6 0.66390896 <a title="208-lda-6" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>7 0.65539527 <a title="208-lda-7" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>8 0.65492761 <a title="208-lda-8" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>9 0.65380037 <a title="208-lda-9" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>10 0.65321082 <a title="208-lda-10" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>11 0.65315807 <a title="208-lda-11" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>12 0.65286636 <a title="208-lda-12" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>13 0.65253067 <a title="208-lda-13" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>14 0.65237337 <a title="208-lda-14" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>15 0.65187627 <a title="208-lda-15" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>16 0.65165734 <a title="208-lda-16" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>17 0.65000963 <a title="208-lda-17" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>18 0.64969319 <a title="208-lda-18" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>19 0.64912033 <a title="208-lda-19" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>20 0.64902914 <a title="208-lda-20" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
