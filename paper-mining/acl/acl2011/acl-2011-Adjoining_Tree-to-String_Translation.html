<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 acl-2011-Adjoining Tree-to-String Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-30" href="#">acl2011-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 acl-2011-Adjoining Tree-to-String Translation</h1>
<br/><p>Source: <a title="acl-2011-30-pdf" href="http://aclweb.org/anthology//P/P11/P11-1128.pdf">pdf</a></p><p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>Reference: <a title="acl-2011-30-reference" href="../acl2011_reference/acl-2011-Adjoining_Tree-to-String_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn ct  ,  ,  Abstract We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. [sent-5, score-1.298]
</p><p>2 As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. [sent-7, score-1.005]
</p><p>3 Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0. [sent-8, score-0.693]
</p><p>4 So far, most of them have been based on synchronous context-free grammars (CFG) (Chiang, 2007), tree substitution grammars (TSG) (Eisner, 2003; Galley et al. [sent-11, score-0.661]
</p><p>5 Synchronous tree adjoining grammars (TAG) (Shieber and Schabes, 1990) are a good candidate. [sent-20, score-0.836]
</p><p>6 Synchronous TAG generalizes TAG by allowing the construction of a pair of trees using the TAG operations of substitution and adjoining on tree pairs. [sent-24, score-1.043]
</p><p>7 As a restricted form of TAG, TIG still allows for adjoining of unbounded trees but only requires O(n3) time for monolingual  parsing. [sent-32, score-0.649]
</p><p>8 m eˇigu o´  X  zoˇ ngt oˇng  Figure 1: Initial and auxiliary tree pairs. [sent-49, score-0.487]
</p><p>9 By convention, substitution and foot nodes are marked with a down arrow (↓) and an asterisk (∗), respectively. [sent-52, score-0.386]
</p><p>10 , NP↓ and X↓ kine βd1 w) tahnd a adjoining wsite (s↓ (e. [sent-55, score-0.564]
</p><p>11 e, NP↓-X↓ node pair in the auxiliary tree pair β1 yields a derived tree pair β2, which can be adjoined at NN-X in α2 to generate  α3  . [sent-60, score-0.851]
</p><p>12 DeNeefe and Knight (2009) prove that adjoining can improve translation quality significantly over a state-of-the-art string-  to-tree system (Galley et al. [sent-62, score-0.693]
</p><p>13 , 2004) that directly induces a synchronous TAG from an aligned and parsed bilingual corpus without converting Treebank-style trees to TAG derivations explicitly (Section 3). [sent-68, score-0.402]
</p><p>14 As tree-tostring translation takes a source parse tree as input, the decoding can be cast as a tree parsing problem (Eisner, 2003): reconstructing TAG derivations from a derived tree using tree-to-string rules that allow for both substitution and adjoining. [sent-69, score-1.279]
</p><p>15 We describe how to convert TAG derivations to translation forest (Section 4). [sent-70, score-0.44]
</p><p>16 2  Model  A synchronous TAG consists of a set of linked elementary tree pairs: initial and auxiliary. [sent-74, score-0.623]
</p><p>17 An initial tree is a tree of which the interior nodes are all labeled with non-terminal symbols, and the nodes on the frontier are either words or non-terminal symbols marked with a down arrow (↓). [sent-75, score-0.769]
</p><p>18 An auxiliary tbroeles i sm darekfiended w as an dinowitianl tree, except thnat a exactly one of its frontier nodes must be marked as foot node (∗). [sent-76, score-0.608]
</p><p>19 Synchronous TAG defines two operations to build derived tree pairs from elementary tree pairs: substitution and adjoining. [sent-78, score-0.774]
</p><p>20 Nodes in initial and auxiliary tree pairs are linked to indicate the correspondence between substitution and adjoining sites. [sent-79, score-1.207]
</p><p>21 , α1, α2, and α3) and two auxiliary tree pairs (i. [sent-82, score-0.387]
</p><p>22 nodeminimal initial ruleminimal auxiliary rule  For example, NP0:1 in rule 6 indicates that the node is an adjoining site linked to a target node dominating the target  string spanning from position 0 to position 1 (i. [sent-168, score-1.506]
</p><p>23 The target tree is hidden because tree-to-string translation only considers the target surface string. [sent-171, score-0.419]
</p><p>24 1280  the NP↓-X↓ node pair in the auxiliary tree pair β1 yields a derived tree pair β2, which can be adjoined at NN-X in α2 to generate α3. [sent-172, score-0.851]
</p><p>25 For simplicity, we represent α2 as a tree-to-string rule: ( NP0:1 ( NR m eˇigu o´ ) ) → US where NP0:1 indicates that the node is an adjoining site linked to a target node dominating the target string spanning from position 0 to position 1 (i. [sent-173, score-1.007]
</p><p>26 The target tree is hidden because treeto-string translation only considers the target surface string. [sent-176, score-0.419]
</p><p>27 The parameters of a probabilistic synchronous TAG are  XPi(α) Xα  XPs(α|η)  =1 =1  (1) (2)  Xα  XPa(β|η) + Pa(NONE|η)  =1  (3)  Xβ  where α ranges over initial tree pairs, β over auxiliary tree pairs, and η over node pairs. [sent-178, score-1.019]
</p><p>28 Pi (α) is the probability of beginning a derivation with α; Ps (α|η) is the probability of substituting α at η; Pa (β|η) i ss tthhee probability o off adjoining β a αt η; ηfi-; nally, Pa (NONE|η) aisb ithliety probability nofg nothing ;a fdi-joining at( η. [sent-179, score-0.746]
</p><p>29 They first classify tree nodes into heads, arguments, and adjuncts using heuristics (Collins, 2003), then transform a Treebank-style tree into a TIG derivation, and finally extract minimallysized rules from the derivation tree and the string on the other side, constrained by the alignments. [sent-184, score-1.007]
</p><p>30 They suggest that one possible solution is to use derivation forest rather than a single derivation tree for rule extraction. [sent-189, score-0.847]
</p><p>31 , 2004) to directly extract tree-to-string rules that allow for both substitution and adjoining from aligned and parsed data. [sent-191, score-0.839]
</p><p>32 There is no need for transforming a parse tree into a TAG derivation ex-  plicitly before rule extraction and all derivations can be easily reconstructed using extracted rules. [sent-192, score-0.553]
</p><p>33 substitution rules, in which the source tree is an initial tree without adjoining sites. [sent-203, score-1.256]
</p><p>34 adjoining rules, in which the source tree is an initial tree with at least one adjoining site. [sent-205, score-1.667]
</p><p>35 auxiliary rules, in which the source tree is an auxiliary tree. [sent-207, score-0.587]
</p><p>36 For example, in Figure 1, α1 is a substitution rule, α2 is an adjoining rule, and β1 is an auxiliary rule. [sent-208, score-0.888]
</p><p>37 For example, in Table 2, rule 1 (for short r1) is a minimal substitution rule extracted from NR0,1. [sent-213, score-0.508]
</p><p>38 Minimal adjoining rules are defined as minimal substitution rules, except that each root node must be an adjoining site. [sent-214, score-1.713]
</p><p>39 In Table 2, r2 is a minimal substitution rule extracted from NP0,1. [sent-215, score-0.383]
</p><p>40 As NP0,1 is a descendant of NP0,2 with the same label, NP0,1 is a possible adjoining site. [sent-216, score-0.623]
</p><p>41 Therefore, r6 can be derived from r2 and licensed as a minimal adjoining  rule extracted from NP0,2. [sent-217, score-0.834]
</p><p>42 Similarly, four minimal adjoining rules are extracted from NP0,3 because it has four frontier descendants labeled with NP. [sent-218, score-0.861]
</p><p>43 Minimal auxiliary rules are derived from minimal substitution and adjoining rules. [sent-219, score-1.155]
</p><p>44 For example, in Table 2, r7 and r10 are derived from the minimal substitution rule r5 while r8 and r11 are derived from r15. [sent-220, score-0.463]
</p><p>45 Note that a minimal auxiliary rule can have adjoining sites (e. [sent-221, score-1.094]
</p><p>46 Table 1lists 17 minimal substitution rules, 7 minimal adjoining rules, and 7 minimal auxiliary rules extracted from Figure 2. [sent-224, score-1.325]
</p><p>47 2 Composition We can obtain composed rules that capture rich contexts by substituting and adjoining minimal initial and auxiliary rules. [sent-226, score-1.116]
</p><p>48 For example, the composition of r12, r17, r25, r26, r29, and r31 yields an initial rule with two adjoining sites: ( IP ( NP0:1 ( NR `aob ¯am aˇ ) ) ( VP2:3 ( VV y ˇuy ˇı ) ( NP ( NN qiˇ anz e´ ) ) ) ) → Obama has condemned 1282 Note that the source phrase “ `aob¯ am aˇ . [sent-227, score-0.94]
</p><p>49 Our model allows both the source and target phrases of an initial rule with adjoining  sites to be discontinuous, which goes beyond the expressive power of synchronous CFG and TSG. [sent-231, score-1.142]
</p><p>50 Similarly, the composition of two auxiliary rules r8 and r16 yields a new auxiliary rule: ( NP ( NP ( x1:NP∗ ) ( x2:NP↓ ) ) ( x3:NP↓ ) ) → x1x2x3 We first compose initial rules and then compose auxiliary rules, both in a bottom-up way. [sent-232, score-0.917]
</p><p>51 To maintain a reasonable grammar size, we follow Liu (2006) to restrict that the tree height of a rule is no greater than 3 and the source surface string is no longer than 7. [sent-233, score-0.427]
</p><p>52 4  Decoding  Given a synchronous TAG and a derived source tree π, a tree-to-string decoder finds the English yield of the best derivation of which the Chinese yield matches π:  ˆe = e? [sent-235, score-0.639]
</p><p>53 The decoder first converts the input tree into a translation forest using a translation rule set by pattern matching. [sent-241, score-0.847]
</p><p>54 Then, the decoder searches for the best derivation in the translation forest intersected with n-gram language models and outputs the target string. [sent-244, score-0.558]
</p><p>55 As translation forest only supports substitution, it is difficult to construct a translation forest for STAG derivations because of 2Mi et al. [sent-246, score-0.762]
</p><p>56 ↓ `aob¯ am aˇ  β2  β3 NP0,3  NP0↓,2  NP0,2  NP2,3  NP0,1  NP1,2  NR0↓,1 elementary tree  translation rule β1  NP0,3  ? [sent-254, score-0.594]
</p><p>57 Each node in a matched tree is annotated with a span as superscript to facilitate identification. [sent-263, score-0.457]
</p><p>58 Note that its left child NP2,3 is not its direct descendant in Figure 2, suggesting that adjoining is required at this site. [sent-265, score-0.679]
</p><p>59 In a derivation forest, a node in a derivation forest is a matched elementary tree. [sent-269, score-0.846]
</p><p>60 A hyperedge corresponds to operations on related trees: substitution (dashed) or adjoining (solid). [sent-270, score-0.777]
</p><p>61 As translation forest only supports substitution, we combine trees with adjoining sites to form an equivalent tree without adjoining sites. [sent-277, score-1.905]
</p><p>62 matching, matching STAG rules against the input tree to obtain a TAG derivation forest; 2. [sent-283, score-0.51]
</p><p>63 conversion, converting the TAG derivation forest into a translation forest; 3. [sent-284, score-0.527]
</p><p>64 Given a tree-to-string rule, rule matching is to find a subtree of the input tree that is identical to the source side of the rule. [sent-286, score-0.398]
</p><p>65 While matching STSG rules against a derived tree is straightforward, it is some-  what non-trivial for STAG rules that move beyond nodes of a local tree. [sent-287, score-0.598]
</p><p>66 This can be done by first enumerating all minimal initial and auxiliary trees and then combining them to obtain composed trees, assuming that every node in the input tree is frontier (see Section 3). [sent-290, score-0.921]
</p><p>67 Each node in a matched tree is annotated with a span as superscript to facilitate identification. [sent-293, score-0.457]
</p><p>68 Note that its left child NP2,3 is not its direct descendant in Figure 2, suggesting that adjoining is required at this site. [sent-295, score-0.679]
</p><p>69 A TAG derivation tree specifies uniquely how a derived tree is constructed using elementary trees (Joshi, 1985). [sent-296, score-0.825]
</p><p>70 A node in a derivation tree is an elementary tree and an edge corresponds to operations on related elementary trees: substitution or adjoining. [sent-297, score-1.16]
</p><p>71 We introduce TAG derivation forest, a compact representation of multiple TAG derivation trees, to encodes all matched TAG derivation trees of the input derived tree. [sent-298, score-0.615]
</p><p>72 The six matched elementary trees are nodes in the derivation forest. [sent-300, score-0.481]
</p><p>73 We use Gorn addresses as tree addresses: 0 is the address of the root node, p is the address of the pth child of the root node, and p · q is the address of the qth child othfe th roeo nt nodode ea,t a tnhde pad·dqr iess tsh p. [sent-302, score-0.422]
</p><p>74 dTdrhees dse orfiv thateio qn forest 1284 should be interpreted as follows: α2 is substituted in the tree α1 of address 1. [sent-303, score-0.434]
</p><p>75 , the first child of the first child o↓f the root node) and β1 is adjoined in the tree α1 at the node NP2,3 of address 1. [sent-306, score-0.583]
</p><p>76 To take advantage of existing decoding techniques, it is necessary to convert a derivation forest to a translation forest. [sent-307, score-0.582]
</p><p>77 A hyperedge in a translation forest corresponds to a translation rule. [sent-308, score-0.511]
</p><p>78 (2008) describe how to convert a derived tree to a translation forest using tree-to-string rules only allowing for substitution. [sent-310, score-0.75]
</p><p>79 Unfortunately, it is not straightforward to convert a derivation forest including adjoining to a translation forest. [sent-311, score-1.08]
</p><p>80 To alleviate this problem, we combine initial rules with adjoining sites and associated auxiliary rules to form equiv-  at the node NR2↓,3  alent initial rules without adjoining sites on the fly during decoding. [sent-312, score-2.283]
</p><p>81 Adjoining β2 in α1 at the node NP2,3 produces an equivalent initial tree with only substitution sites: ( IP0,8 ( NP0,3 ( NP0↓,2 ) ( NP2,3 ( NR2↓,3 ) ) ) ( VP3↓,8 ) ) The corresponding composed rule r1 + r4 has no adjoining sites and can be added to translation forest. [sent-315, score-1.59]
</p><p>82 , α1 and β2) form a composition tree in a derivation forest. [sent-318, score-0.442]
</p><p>83 A node in a composition tree is a matched elementary tree and an edge corresponds to adjoining operations. [sent-319, score-1.418]
</p><p>84 The root node must be an initial tree with at least one adjoining site. [sent-320, score-1.063]
</p><p>85 The descendants ofthe root node must all be auxiliary trees. [sent-321, score-0.376]
</p><p>86 The number of children of a node in a composition tree depends on the number of adjoining sites in the node. [sent-323, score-1.149]
</p><p>87 We use composition forest to encode all possible composition trees. [sent-324, score-0.382]
</p><p>88 Often, a node in a composition tree may have multiple matched rules. [sent-325, score-0.514]
</p><p>89 As a large amount of composition trees and composed rules can be identified and constructed on the fly during forest conversion, we used cube pruning (Chiang, 2007; Huang and Chiang, 2007) to achieve a balance between translation quality and decoding efficiency. [sent-326, score-0.822]
</p><p>90 5  Evaluation  We evaluated our adjoining tree-to-string translation  system on Chinese-English translation. [sent-328, score-0.693]
</p><p>91 (2006) to restrict that the height of a rule tree is no greater than 3 and the surface string’s length is no greater than 7. [sent-335, score-0.398]
</p><p>92 We find that VP (verb phrase) is most likely to be the label of a foot node in an auxiliary rule. [sent-345, score-0.468]
</p><p>93 A distance is the difference of levels between a foot node 1285  distance Figure 5: Average occurrences of foot node labels VP, NP, and IP over various distances. [sent-350, score-0.594]
</p><p>94 As most foot nodes are usually very close to the root nodes, we restrict that a foot node must be the direct descendant of the root node in our experiments. [sent-368, score-0.841]
</p><p>95 To produce the 1-best translations on the MT05 test set that contains 1,082 sentences, while the STSG system used 40,169 initial rules without adjoining sites, the STAG system used 28,046 initial rules without adjoining sites, 1,057 initial rules with adjoining sites, and 1,527 auxiliary rules. [sent-386, score-2.463]
</p><p>96 As we use cube pruning, although the translation forest of STAG is bigger than that of STSG, the intersection time barely increases. [sent-393, score-0.383]
</p><p>97 With translation rules learned from Treebank-style trees, the adjoining tree-to-string system outperforms the baseline system using STSG without significant loss in efficiency. [sent-398, score-0.815]
</p><p>98 Statistical parsing with an automatically extracted tree adjoining grammar. [sent-420, score-0.78]
</p><p>99 A meta-level grammar: Redefining synchronous tag for translation and paraphrase. [sent-446, score-0.432]
</p><p>100 Extracting tree adjoining grammars from  Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. [sent-570, score-0.836]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('adjoining', 0.564), ('stag', 0.22), ('forest', 0.218), ('tree', 0.216), ('synchronous', 0.18), ('np', 0.173), ('auxiliary', 0.171), ('node', 0.158), ('substitution', 0.153), ('stsg', 0.148), ('derivation', 0.144), ('foot', 0.139), ('sites', 0.129), ('translation', 0.129), ('rule', 0.125), ('elementary', 0.124), ('tag', 0.123), ('rules', 0.122), ('minimal', 0.105), ('ngt', 0.1), ('zo', 0.1), ('trees', 0.085), ('composition', 0.082), ('nr', 0.078), ('initial', 0.078), ('tig', 0.076), ('aob', 0.073), ('frontier', 0.07), ('nodes', 0.07), ('derivations', 0.068), ('decoding', 0.066), ('deneefe', 0.065), ('ip', 0.065), ('huang', 0.06), ('descendant', 0.059), ('matched', 0.058), ('liu', 0.057), ('vp', 0.056), ('child', 0.056), ('grammars', 0.056), ('adjoined', 0.05), ('igu', 0.05), ('nn', 0.05), ('knight', 0.049), ('galley', 0.049), ('root', 0.047), ('fly', 0.046), ('chiang', 0.046), ('nesson', 0.04), ('derived', 0.04), ('schabes', 0.039), ('mi', 0.039), ('substituting', 0.038), ('qun', 0.038), ('composed', 0.038), ('target', 0.037), ('nist', 0.037), ('cube', 0.036), ('converting', 0.036), ('aravind', 0.036), ('joshi', 0.035), ('shieber', 0.035), ('rescoring', 0.035), ('hyperedge', 0.035), ('cfg', 0.034), ('abeille', 0.033), ('condemned', 0.033), ('height', 0.033), ('bilingual', 0.033), ('dashed', 0.032), ('yves', 0.032), ('bleu', 0.031), ('decoder', 0.03), ('anz', 0.029), ('slower', 0.029), ('haitao', 0.029), ('source', 0.029), ('matching', 0.028), ('site', 0.028), ('uy', 0.027), ('qi', 0.027), ('obama', 0.026), ('liang', 0.026), ('conversion', 0.026), ('interior', 0.025), ('superscript', 0.025), ('tsg', 0.025), ('convert', 0.025), ('ng', 0.025), ('operations', 0.025), ('linked', 0.025), ('president', 0.025), ('stuart', 0.025), ('restrict', 0.024), ('arrow', 0.024), ('ghkm', 0.024), ('reconstructing', 0.024), ('chinese', 0.024), ('yajuan', 0.023), ('adjuncts', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="30-tfidf-1" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>2 0.33286569 <a title="30-tfidf-2" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>3 0.30874655 <a title="30-tfidf-3" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>4 0.28481936 <a title="30-tfidf-4" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>Author: Hao Zhang ; Licheng Fang ; Peng Xu ; Xiaoyun Wu</p><p>Abstract: Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.</p><p>5 0.26173717 <a title="30-tfidf-5" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>6 0.25483039 <a title="30-tfidf-6" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>7 0.20740314 <a title="30-tfidf-7" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>8 0.19550243 <a title="30-tfidf-8" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>9 0.19120491 <a title="30-tfidf-9" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>10 0.18241461 <a title="30-tfidf-10" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>11 0.17018911 <a title="30-tfidf-11" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>12 0.15741986 <a title="30-tfidf-12" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>13 0.14668843 <a title="30-tfidf-13" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>14 0.14607266 <a title="30-tfidf-14" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>15 0.14101852 <a title="30-tfidf-15" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>16 0.1397665 <a title="30-tfidf-16" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>17 0.13512219 <a title="30-tfidf-17" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>18 0.1173248 <a title="30-tfidf-18" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>19 0.10406983 <a title="30-tfidf-19" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>20 0.10048192 <a title="30-tfidf-20" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.234), (1, -0.265), (2, 0.128), (3, -0.102), (4, 0.037), (5, 0.02), (6, -0.308), (7, -0.086), (8, -0.099), (9, -0.095), (10, -0.104), (11, -0.043), (12, -0.016), (13, 0.075), (14, 0.11), (15, -0.088), (16, 0.02), (17, 0.033), (18, 0.023), (19, 0.024), (20, -0.048), (21, -0.003), (22, -0.01), (23, 0.142), (24, 0.103), (25, -0.069), (26, -0.035), (27, 0.027), (28, 0.058), (29, -0.02), (30, -0.021), (31, -0.118), (32, -0.027), (33, 0.008), (34, -0.068), (35, 0.027), (36, -0.092), (37, -0.149), (38, -0.134), (39, -0.061), (40, -0.099), (41, -0.003), (42, -0.032), (43, -0.005), (44, -0.017), (45, 0.029), (46, 0.032), (47, 0.057), (48, 0.161), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97411865 <a title="30-lsi-1" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>2 0.87486553 <a title="30-lsi-2" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>3 0.84589982 <a title="30-lsi-3" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>4 0.82246923 <a title="30-lsi-4" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>Author: Andreas Maletti</p><p>Abstract: The local multi bottom-up tree transducer is introduced and related to the (non-contiguous) synchronous tree sequence substitution grammar. It is then shown how to obtain a weighted local multi bottom-up tree transducer from a bilingual and biparsed corpus. Finally, the problem of non-preservation of regularity is addressed. Three properties that ensure preservation are introduced, and it is discussed how to adjust the rule extraction process such that they are automatically fulfilled.</p><p>5 0.81588739 <a title="30-lsi-5" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>Author: Hiroyuki Shindo ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG). Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG. The experimental parsing results show that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG.</p><p>6 0.74204636 <a title="30-lsi-6" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>7 0.73408037 <a title="30-lsi-7" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>8 0.70871425 <a title="30-lsi-8" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>9 0.70816654 <a title="30-lsi-9" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>10 0.68925005 <a title="30-lsi-10" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>11 0.67285991 <a title="30-lsi-11" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>12 0.59267533 <a title="30-lsi-12" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>13 0.58868951 <a title="30-lsi-13" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>14 0.53313488 <a title="30-lsi-14" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>15 0.52721626 <a title="30-lsi-15" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>16 0.5178858 <a title="30-lsi-16" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>17 0.49938759 <a title="30-lsi-17" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>18 0.44902456 <a title="30-lsi-18" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>19 0.44445446 <a title="30-lsi-19" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>20 0.42253259 <a title="30-lsi-20" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.021), (17, 0.135), (26, 0.031), (28, 0.02), (37, 0.076), (39, 0.064), (41, 0.063), (55, 0.023), (59, 0.029), (64, 0.014), (72, 0.02), (89, 0.218), (91, 0.034), (96, 0.168), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91847956 <a title="30-lda-1" href="./acl-2011-Automatic_Extraction_of_Lexico-Syntactic_Patterns_for_Detection_of_Negation_and_Speculation_Scopes.html">50 acl-2011-Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation and Speculation Scopes</a></p>
<p>Author: Emilia Apostolova ; Noriko Tomuro ; Dina Demner-Fushman</p><p>Abstract: Detecting the linguistic scope of negated and speculated information in text is an important Information Extraction task. This paper presents ScopeFinder, a linguistically motivated rule-based system for the detection of negation and speculation scopes. The system rule set consists of lexico-syntactic patterns automatically extracted from a corpus annotated with negation/speculation cues and their scopes (the BioScope corpus). The system performs on par with state-of-the-art machine learning systems. Additionally, the intuitive and linguistically motivated rules will allow for manual adaptation of the rule set to new domains and corpora. 1 Motivation Information Extraction (IE) systems often face the problem of distinguishing between affirmed, negated, and speculative information in text. For example, sentiment analysis systems need to detect negation for accurate polarity classification. Similarly, medical IE systems need to differentiate between affirmed, negated, and speculated (possible) medical conditions. The importance of the task of negation and speculation (a.k.a. hedge) detection is attested by a number of research initiatives. The creation of the BioScope corpus (Vincze et al., 2008) assisted in the development and evaluation of several negation/hedge scope detection systems. The corpus consists of medical and biological texts annotated for negation, speculation, and their linguistic scope. The 2010 283 Noriko Tomuro Dina Demner-Fushman DePaul University Chicago, IL USA t omuro @ c s . depaul . edu National Library of Medicine Bethesda, MD USA ddemne r@mai l nih . gov . i2b2 NLP Shared Task1 included a track for detection of the assertion status of medical problems (e.g. affirmed, negated, hypothesized, etc.). The CoNLL2010 Shared Task (Farkas et al., 2010) focused on detecting hedges and their scopes in Wikipedia articles and biomedical texts. In this paper, we present a linguistically motivated rule-based system for the detection of negation and speculation scopes that performs on par with state-of-the-art machine learning systems. The rules used by the ScopeFinder system are automatically extracted from the BioScope corpus and encode lexico-syntactic patterns in a user-friendly format. While the system was developed and tested using a biomedical corpus, the rule extraction mechanism is not domain-specific. In addition, the linguistically motivated rule encoding allows for manual adaptation to new domains and corpora. 2 Task Definition Negation/Speculation detection is typically broken down into two sub-tasks - discovering a negation/speculation cue and establishing its scope. The following example from the BioScope corpus shows the annotated hedging cue (in bold) together with its associated scope (surrounded by curly brackets): Finally, we explored the {possible role of 5hydroxyeicosatetraenoic acid as a regulator of arachidonic acid liberation}. Typically, systems first identify negation/speculation cues and subsequently try to identify their associated cue scope. However, the two tasks are interrelated and both require 1https://www.i2b2.org/NLP/Relations/ Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o.c?i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 283–287, syntactic understanding. Consider the following two sentences from the BioScope corpus: 1) By contrast, {D-mib appears to be uniformly expre1ss)e Bdy yin c oimnatrgaisnta,l { dDis-mcsi }b. 2) Differentiation assays using water soluble phorbol esters reveal that differentiation becomes irreversible soon after AP-1 appears. Both sentences contain the word form appears, however in the first sentence the word marks a hedg- ing cue, while in the second sentence the word does not suggest speculation. Unlike previous work, we do not attempt to identify negation/speculation cues independently of their scopes. Instead, we concentrate on scope detection, simultaneously detecting corresponding cues. 3 Dataset We used the BioScope corpus (Vincze et al., 2008) to develop our system and evaluate its performance. To our knowledge, the BioScope corpus is the only publicly available dataset annotated with negation/speculation cues and their scopes. It consists of biomedical papers, abstracts, and clinical reports (corpus statistics are shown in Tables 1 and 2). Corpus Type Sentences Documents Mean Document Size Clinical752019543.85 Full Papers Paper Abstracts 3352 14565 9 1273 372.44 11.44 Table 1: Statistics of the BioScope corpus. Document sizes represent number of sentences. Corpus Type Negation Cues Speculation Cues Negation Speculation Clinical87211376.6%13.4% Full Papers Paper Abstracts 378 1757 682 2694 13.76% 13.45% 22.29% 17.69% Table 2: Statistics of the BioScope corpus. The 2nd and 3d columns show the total number of cues within the datasets; the 4th and 5th columns show the percentage of negated and speculative sentences. 70% ofthe corpus documents (randomly selected) were used to develop the ScopeFinder system (i.e. extract lexico-syntactic rules) and the remaining 30% were used to evaluate system performance. While the corpus focuses on the biomedical domain, our rule extraction method is not domain specific and in future work we are planning to apply our method on different types of corpora. 4 Method Intuitively, rules for detecting both speculation and negation scopes could be concisely expressed as a 284 Figure 1: Parse tree of the sentence ‘T cells {lack active NFkappa B } bPuatr express Sp1 as expected’ generated by cthtiev eS NtanF-fkoaprdp parser. Speculation scope ewxporedcste are gsehnoewrant eind ellipsis. tTanhecue word is shown in grey. The nearest common ancestor of all cue and scope leaf nodes is shown in a box. combination of lexical and syntactic patterns. example, BioScope O¨zg u¨r For and Radev (2009) examined sample sentences and developed hedging scope rules such as: The scope of a modal verb cue (e.g. may, might, could) is the verb phrase to which it is attached; The scope of a verb cue (e.g. appears, seems) followed by an infinitival clause extends to the whole sentence. Similar lexico-syntactic rules have been also manually compiled and used in a number of hedge scope detection systems, e.g. (Kilicoglu and Bergler, 2008), (Rei and Briscoe, 2010), (Velldal et al., 2010), (Kilicoglu and Bergler, 2010), (Zhou et al., 2010). However, manually creating a comprehensive set of such lexico-syntactic scope rules is a laborious and time-consuming process. In addition, such an approach relies heavily on the availability of accurately parsed sentences, which could be problematic for domains such as biomedical texts (Clegg and Shepherd, 2007; McClosky and Charniak, 2008). Instead, we attempted to automatically extract lexico-syntactic scope rules from the BioScope corpus, relying only on consistent (but not necessarily accurate) parse tree representations. We first parsed each sentence in the training dataset which contained a negation or speculation cue using the Stanford parser (Klein and Manning, 2003; De Marneffe et al., 2006). Figure 1 shows the parse tree of a sample sentence containing a negation cue and its scope. Next, for each cue-scope instance within the sen- tence, we identified the nearest common ancestor Figure 2: Lexico-syntactic pattern extracted from the sentence from Figure 1. The rule is equivalent to the following string representation: (VP (VBP lack) (NP (JJ *scope*) (NN *scope*) (NN *scope*))). which encompassed the cue word(s) and all words in the scope (shown in a box on Figure 1). The subtree rooted by this ancestor is the basis for the resulting lexico-syntactic rule. The leaf nodes of the resulting subtree were converted to a generalized representation: scope words were converted to *scope*; noncue and non-scope words were converted to *; cue words were converted to lower case. Figure 2 shows the resulting rule. This rule generation approach resulted in a large number of very specific rule patterns - 1,681 nega- tion scope rules and 3,043 speculation scope rules were extracted from the training dataset. To identify a more general set of rules (and increase recall) we next performed a simple transformation of the derived rule set. If all children of a rule tree node are of type *scope* or * (i.e. noncue words), the node label is replaced by *scope* or * respectively, and the node’s children are pruned from the rule tree; neighboring identical siblings of type *scope* or * are replaced by a single node of the corresponding type. Figure 3 shows an example of this transformation. (a)ThechildrenofnodesJ /N /N are(b)Thechildren pruned and their labels are replaced by of node NP are *scope*. pruned and its label is replaced by *scope*. Figure 3: Transformation of the tree shown in Figure 2. The final rule is equivalent to the following string representation: (VP (VBP lack) *scope* ) 285 The rule tree pruning described above reduced the negation scope rule patterns to 439 and the speculation rule patterns to 1,000. In addition to generating a set of scope finding rules, we also implemented a module that parses string representations of the lexico-syntactic rules and performs subtree matching. The ScopeFinder module2 identifies negation and speculation scopes in sentence parse trees using string-encoded lexicosyntactic patterns. Candidate sentence parse subtrees are first identified by matching the path of cue leafnodes to the root ofthe rule subtree pattern. Ifan identical path exists in the sentence, the root of the candidate subtree is thus also identified. The candidate subtree is evaluated for a match by recursively comparing all node children (starting from the root of the subtree) to the rule pattern subtree. Nodes of type *scope* and * match any number of nodes, similar to the semantics of Regex Kleene star (*). 5 Results As an informed baseline, we used a previously de- veloped rule-based system for negation and speculation scope discovery (Apostolova and Tomuro, 2010). The system, inspired by the NegEx algorithm (Chapman et al., 2001), uses a list of phrases split into subsets (preceding vs. following their scope) to identify cues using string matching. The cue scopes extend from the cue to the beginning or end of the sentence, depending on the cue type. Table 3 shows the baseline results. PSFCNalpueingpleciarPutcAlai opbtneisor tacsP6597C348o.r12075e4ctly6859RP203475r. 81e26d037icteF569784C52. 04u913e84s5F2A81905l.2786P14redictCus Table 3: Baseline system performance. P (Precision), R (Recall), and F (F1-score) are computed based on the sentence tokens of correctly predicted cues. The last column shows the F1-score for sentence tokens of all predicted cues (including erroneous ones). We used only the scopes of predicted cues (correctly predicted cues vs. all predicted cues) to mea- 2The rule sets and source code are publicly available at http://scopefinder.sourceforge.net/. sure the baseline system performance. The baseline system heuristics did not contain all phrase cues present in the dataset. The scopes of cues that are missing from the baseline system were not included in the results. As the baseline system was not penalized for missing cue phrases, the results represent the upper bound of the system. Table 4 shows the results from applying the full extracted rule set (1,681 negation scope rules and 3,043 speculation scope rules) on the test data. As expected, this rule set consisting of very specific scope matching rules resulted in very high precision and very low recall. Negation P R F A Clinical99.4734.3051.0117.58 Full Papers Paper Abstracts 95.23 87.33 25.89 05.78 40.72 10.84 28.00 07.85 Speculation Clinical96.5020.1233.3022.90 Full Papers Paper Abstracts 88.72 77.50 15.89 11.89 26.95 20.62 10.13 10.00 Table 4: Results from applying the full extracted rule set on the test data. Precision (P), Recall (R), and F1-score (F) are com- puted based the number of correctly identified scope tokens in each sentence. Accuracy (A) is computed for correctly identified full scopes (exact match). Table 5 shows the results from applying the rule set consisting of pruned pattern trees (439 negation scope rules and 1,000 speculation scope rules) on the test data. As shown, overall results improved significantly, both over the baseline and over the unpruned set of rules. Comparable results are shown in bold in Tables 3, 4, and 5. Negation P R F A Clinical85.5992.1588.7585.56 Full Papers 49.17 94.82 64.76 71.26 Paper Abstracts 61.48 92.64 73.91 80.63 Speculation Clinical67.2586.2475.5771.35 Full Papers 65.96 98.43 78.99 52.63 Paper Abstracts 60.24 95.48 73.87 65.28 Table 5: Results from applying the pruned rule set on the test data. Precision (P), Recall (R), and F1-score (F) are computed based on the number of correctly identified scope tokens in each sentence. Accuracy (A) is computed for correctly identified full scopes (exact match). 6 Related Work Interest in the task of identifying negation and spec- ulation scopes has developed in recent years. Rele286 vant research was facilitated by the appearance of a publicly available annotated corpus. All systems described below were developed and evaluated against the BioScope corpus (Vincze et al., 2008). O¨zg u¨r and Radev (2009) have developed a supervised classifier for identifying speculation cues and a manually compiled list of lexico-syntactic rules for identifying their scopes. For the performance of the rule based system on identifying speculation scopes, they report 61. 13 and 79.89 accuracy for BioScope full papers and abstracts respectively. Similarly, Morante and Daelemans (2009b) developed a machine learning system for identifying hedging cues and their scopes. They modeled the scope finding problem as a classification task that determines if a sentence token is the first token in a scope sequence, the last one, or neither. Results of the scope finding system with predicted hedge signals were reported as F1-scores of 38. 16, 59.66, 78.54 and for clinical texts, full papers, and abstracts respectively3. Accuracy (computed for correctly identified scopes) was reported as 26.21, 35.92, and 65.55 for clinical texts, papers, and abstracts respectively. Morante and Daelemans have also developed a metalearner for identifying the scope of negation (2009a). Results of the negation scope finding system with predicted cues are reported as F1-scores (computed on scope tokens) of 84.20, 70.94, and 82.60 for clinical texts, papers, and abstracts respectively. Accuracy (the percent of correctly identified exact scopes) is reported as 70.75, 41.00, and 66.07 for clinical texts, papers, and abstracts respectively. The top three best performers on the CoNLL2010 shared task on hedge scope detection (Farkas et al., 2010) report an F1-score for correctly identified hedge cues and their scopes ranging from 55.3 to 57.3. The shared task evaluation metrics used stricter matching criteria based on exact match of both cues and their corresponding scopes4. CoNLL-2010 shared task participants applied a variety of rule-based and machine learning methods 3F1-scores are computed based on scope tokens. Unlike our evaluation metric, scope token matches are computed for each cue within a sentence, i.e. a token is evaluated multiple times if it belongs to more than one cue scope. 4Our system does not focus on individual cue-scope pair de- tection (we instead optimized scope detection) and as a result performance metrics are not directly comparable. on the task - Morante et al. (2010) used a memorybased classifier based on the k-nearest neighbor rule to determine if a token is the first token in a scope sequence, the last, or neither; Rei and Briscoe (2010) used a combination of manually compiled rules, a CRF classifier, and a sequence of post-processing steps on the same task; Velldal et al (2010) manually compiled a set of heuristics based on syntactic information taken from dependency structures. 7 Discussion We presented a method for automatic extraction of lexico-syntactic rules for negation/speculation scopes from an annotated corpus. The developed ScopeFinder system, based on the automatically extracted rule sets, was compared to a baseline rule-based system that does not use syntactic information. The ScopeFinder system outperformed the baseline system in all cases and exhibited results comparable to complex feature-based, machine-learning systems. In future work, we will explore the use of statistically based methods for the creation of an optimum set of lexico-syntactic tree patterns and will evaluate the system performance on texts from different domains. References E. Apostolova and N. Tomuro. 2010. Exploring surfacelevel heuristics for negation and speculation discovery in clinical texts. In Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, pages 81–82. Association for Computational Linguistics. W.W. Chapman, W. Bridewell, P. Hanbury, G.F. Cooper, and B.G. Buchanan. 2001. A simple algorithm for identifying negated findings and diseases in discharge summaries. Journal of biomedical informatics, 34(5):301–310. A.B. Clegg and A.J. Shepherd. 2007. Benchmarking natural-language parsers for biological applications using dependency graphs. BMC bioinformatics, 8(1):24. M.C. De Marneffe, B. MacCartney, and C.D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In LREC 2006. Citeseer. R. Farkas, V. Vincze, G. M o´ra, J. Csirik, and G. Szarvas. 2010. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on 287 Computational Natural Language Learning (CoNLL2010): Shared Task, pages 1–12. H. Kilicoglu and S. Bergler. 2008. Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC bioinformatics, 9(Suppl 11):S10. H. Kilicoglu and S. Bergler. 2010. A High-Precision Approach to Detecting Hedges and Their Scopes. CoNLL-2010: Shared Task, page 70. D. Klein and C.D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. Advances in neural information processing systems, pages 3–10. D. McClosky and E. Charniak. 2008. Self-training for biomedical parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 101–104. Association for Computational Linguistics. R. Morante and W. Daelemans. 2009a. A metalearning approach to processing the scope of negation. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning, pages 21–29. Association for Computational Linguistics. R. Morante and W. Daelemans. 2009b. Learning the scope of hedge cues in biomedical texts. In Proceed- ings of the Workshop on BioNLP, pages 28–36. Association for Computational Linguistics. R. Morante, V. Van Asch, and W. Daelemans. 2010. Memory-based resolution of in-sentence scopes of hedge cues. CoNLL-2010: Shared Task, page 40. A. O¨zg u¨r and D.R. Radev. 2009. Detecting speculations and their scopes in scientific text. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1398–1407. Association for Computational Linguistics. M. Rei and T. Briscoe. 2010. Combining manual rules and supervised learning for hedge cue and scope detection. In Proceedings of the 14th Conference on Natural Language Learning, pages 56–63. E. Velldal, L. Øvrelid, and S. Oepen. 2010. Resolving Speculation: MaxEnt Cue Classification and Dependency-Based Scope Rules. CoNLL-2010: Shared Task, page 48. V. Vincze, G. Szarvas, R. Farkas, G. M o´ra, and J. Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC bioinformatics, 9(Suppl 11):S9. H. Zhou, X. Li, D. Huang, Z. Li, and Y. Yang. 2010. Exploiting Multi-Features to Detect Hedges and Their Scope in Biomedical Texts. CoNLL-2010: Shared Task, page 106.</p><p>2 0.84262371 <a title="30-lda-2" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>Author: Hiroyuki Shindo ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We propose a model that incorporates an insertion operator in Bayesian tree substitution grammars (BTSG). Tree insertion is helpful for modeling syntax patterns accurately with fewer grammar rules than BTSG. The experimental parsing results show that our model outperforms a standard PCFG and BTSG for a small dataset. For a large dataset, our model obtains comparable results to BTSG, making the number of grammar rules much smaller than with BTSG.</p><p>same-paper 3 0.84234428 <a title="30-lda-3" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>4 0.7412625 <a title="30-lda-4" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>Author: Andrei Popescu-Belis ; Majid Yazdani ; Alexandre Nanchen ; Philip N. Garner</p><p>Abstract: The Automatic Content Linking Device is a just-in-time document retrieval system which monitors an ongoing conversation or a monologue and enriches it with potentially related documents, including multimedia ones, from local repositories or from the Internet. The documents are found using keyword-based search or using a semantic similarity measure between documents and the words obtained from automatic speech recognition. Results are displayed in real time to meeting participants, or to users watching a recorded lecture or conversation.</p><p>5 0.74054801 <a title="30-lda-5" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>6 0.73273301 <a title="30-lda-6" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>7 0.71915406 <a title="30-lda-7" href="./acl-2011-Entrainment_in_Speech_Preceding_Backchannels..html">118 acl-2011-Entrainment in Speech Preceding Backchannels.</a></p>
<p>8 0.71674573 <a title="30-lda-8" href="./acl-2011-Issues_Concerning_Decoding_with_Synchronous_Context-free_Grammar.html">180 acl-2011-Issues Concerning Decoding with Synchronous Context-free Grammar</a></p>
<p>9 0.71555614 <a title="30-lda-9" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>10 0.7145077 <a title="30-lda-10" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>11 0.712919 <a title="30-lda-11" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>12 0.70954382 <a title="30-lda-12" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>13 0.7072199 <a title="30-lda-13" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>14 0.70298016 <a title="30-lda-14" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>15 0.7013095 <a title="30-lda-15" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>16 0.69976789 <a title="30-lda-16" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>17 0.69966131 <a title="30-lda-17" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>18 0.69908047 <a title="30-lda-18" href="./acl-2011-How_to_train_your_multi_bottom-up_tree_transducer.html">154 acl-2011-How to train your multi bottom-up tree transducer</a></p>
<p>19 0.6982379 <a title="30-lda-19" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>20 0.69792646 <a title="30-lda-20" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
