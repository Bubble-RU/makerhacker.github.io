<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-185" href="#">acl2011-185</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</h1>
<br/><p>Source: <a title="acl-2011-185-pdf" href="http://aclweb.org/anthology//P/P11/P11-2017.pdf">pdf</a></p><p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>Reference: <a title="acl-2011-185-reference" href="../acl2011_reference/acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu ct  Abstract Individual utterances often serve multiple communicative purposes in dialogue. [sent-3, score-0.416]
</p><p>2 We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. [sent-4, score-2.202]
</p><p>3 1 Introduction Natural language understanding (NLU) at the level of speech acts for conversational dialogue systems can be performed with high accuracy in limited domains using data-driven techniques (Bender et al. [sent-6, score-1.033]
</p><p>4 , 2008, for example), provided that enough training material is available. [sent-9, score-0.029]
</p><p>5 For most systems that implement novel  conversational scenarios, however, enough examples of user utterances, which can be annotated as NLU training data, only become available once several users have interacted with the system. [sent-10, score-0.127]
</p><p>6 Users of the dialogue system, whose utterances are collected for further NLU improvement, tend to notice that portions of their longer utterances are ignored and that they  are better understood when they express themselves with simpler sentences. [sent-13, score-1.576]
</p><p>7 We address this chicken-and-egg problem with a data-driven NLU approach that segments and identifies multiple dialogue acts in single utterances, even when only short (single dialogue act) utterances are available for training. [sent-15, score-2.224]
</p><p>8 In contrast to previous approaches that assume the existence of enough training data for learning to segment utterances, e. [sent-16, score-0.079]
</p><p>9 , 2003), our framework requires a relatively small dataset, which may not contain any utterances with multiple dialogue acts. [sent-21, score-1.191]
</p><p>10 This makes it possible to create new conversational dialogue system scenarios that allow and encourage users to express themselves with fewer restrictions, without an increased burden in the collection and annotation of  NLU training data. [sent-22, score-0.898]
</p><p>11 2  Method  Given (1) a predefined set of possible dialogue acts for a specific dialogue system, (2) a set of utterances Proceedings ofP tohertl 4a9nthd, A Onrneugaoln ,M Jeuentien 1g9 o-f2 t4h,e 2 A01ss1o. [sent-23, score-2.109]
</p><p>12 While short utterances are likely to be covered entirely by a single dialogue act that spans all of its words, longer utterances may be composed of spans that correspond to different dialogue acts. [sent-26, score-2.71]
</p><p>13 bestDialogueActEndingAt(Text,pos) begin  Algorithm 1: The function classify(T) calls the single dialogue act classifier subsystem on the input text T and returns the highest scoring dialogue act label c with its confidence score p. [sent-27, score-2.355]
</p><p>14 The function words(T, i,j) returns the string formed by concatenating the words in T from the ith to the jth included. [sent-28, score-0.024]
</p><p>15 To obtain the best segmentation of a given text, one has to work its way back from the end of the text: start by calling hk, hc, pii = bestDialogueActEndingAt(Text, numWords), where numWords is the number of words in Text. [sent-29, score-0.065]
</p><p>16 If k > 0 recursively call  bestDialogueActEndingAt(Text, k − 1) to obtain tbhees optimal dialogue ancgtA ending ,atk kk − −− 1 1). [sent-30, score-0.797]
</p><p>17 Algorithm 1 shows our approach for using a single dialogue act classifier to extract the sequence of dialogue acts with the highest overall score from a given utterance. [sent-31, score-2.146]
</p><p>18 The framework is independent of the particular subsystem used to select the dialogue act label for a given segment of text. [sent-32, score-1.19]
</p><p>19 The constraint is that this subsystem should return, for a given sequence of words, at least one dialogue act label and its confidence level in a normalized range that can 96 be used for comparisons with subsequent runs. [sent-33, score-1.153]
</p><p>20 In the work reported in this paper, we use an existing data-driven NLU module (Sagae et al. [sent-34, score-0.014]
</p><p>21 , 2009), developed for the SASO virtual human dialogue system (Traum et al. [sent-35, score-0.782]
</p><p>22 This NLU module performs maximum entropy multiclass classification, using features derived from the words in the input utterance, and using dialogue act labels as classes. [sent-37, score-1.19]
</p><p>23 The basic idea is to find the best segmentation (that is, the one with the highest score) of the portion  of the input text up to the ith word. [sent-38, score-0.078]
</p><p>24 The base case Si would be for i= 1and it is the result of our classifier when the input is the single first word. [sent-39, score-0.09]
</p><p>25 Then we assign to the best segmentation ending at i, Si, the label Ck,i iff:  k = a1rg≤hm≤aix? [sent-41, score-0.101]
</p><p>26 (1)  Algorithm 1 calls the classifier O(n2) where n is the number of words in the input text. [sent-43, score-0.079]
</p><p>27 Note that, as in the maximum entropy NLU of Bender et al. [sent-44, score-0.045]
</p><p>28 Therefore, our scores are not true probabilities, although they serve as a good approximation in the search for the best overall segmentation. [sent-46, score-0.013]
</p><p>29 1 Data To evaluate our approach we used data collected from users of the TACQ (Traum et al. [sent-50, score-0.042]
</p><p>30 , 2008a) dialogue system, as described by Artstein et al. [sent-51, score-0.757]
</p><p>31 Of the utterances in that dataset, about 30% are annotated with multiple dialogue acts. [sent-53, score-1.198]
</p><p>32 The annotation also contains for each dialogue act the corresponding segment of the input utterance. [sent-54, score-1.17]
</p><p>33 Of these, 1,204 utterances contain only a single dialogue act, and 375 utterances contain multiple dialogue acts, according to manual dialogue act annotation. [sent-56, score-3.452]
</p><p>34 Within the set of utterances that contain multiple dialogue acts, the average number of dialogue  acts per utterance is 2. [sent-57, score-2.273]
</p><p>35 The dialogue act annotation scheme uses a total of 77 distinct labels, with each label corresponding to a domain-specific dialogue act, including some semantic information. [sent-59, score-1.884]
</p><p>36 Each of these 77 labels is composed at least of a core speech act type (e. [sent-60, score-0.366]
</p><p>37 For example, the dialogue act annotation for the utterance What is the strange man ’s name ? [sent-63, score-1.202]
</p><p>38 would be whq ( ob j : st rangeMan att r : name ) , reflecting that it is a wh-question, with a specific object and attribute. [sent-64, score-0.164]
</p><p>39 In the set of utterances with only one speech act, 70 of the possible 77 dialogue act labels are used. [sent-65, score-1.502]
</p><p>40 In the remaining utterances (which contain multiple speech acts per utterance), 59 unique dialogue act labels are used, including 7 that are not used in utterances with only a single dialogue act (these 7 labels are used in only 1% of those utterances). [sent-66, score-3.29]
</p><p>41 A total of 18 unique labels are used only in the set of utterances with one dialogue act (these labels are used in 5% of those utterances). [sent-67, score-1.518]
</p><p>42 Table 1 shows the frequency information for the five most common dialogue act labels in our dataset. [sent-68, score-1.108]
</p><p>43 ,  The average number of words in utterances with only a single dialogue act is 7. [sent-69, score-1.507]
</p><p>44 5 (with a maximum of 34, and minimum of 1), and the average length of utterances with multiple dialogue acts is 15. [sent-70, score-1.428]
</p><p>45 To give a better idea of the dataset used here, we list below two examples of utterances in the dataset, and their dialogue act annotation. [sent-72, score-1.495]
</p><p>46 We add word indices as subscripts in the utterances for illustration purposes only, to facilitate identification of the word spans for each dialogue act. [sent-73, score-1.177]
</p><p>47 The annotation consists of a word interval and a 97  YSWienhsg/-Nlqeuo eD-qsAtuioe Unst sti. [sent-74, score-0.02]
</p><p>48 [32%41]  TYOGabefrsel eart1isn: stTowghea grefnetquencyo79 ftheY Tde hias /lnNaokngosuw-eq aucretsciloans esm187o1st used in the TACQ dataset (Artstein et al. [sent-76, score-0.039]
</p><p>49 The left column reports the statistics for the set of utterances annotated with a single dialogue act the right those for the  utterances annotated with multiple dialogue acts. [sent-78, score-2.709]
</p><p>50 Each dialogue act class typically contains several more specific dialogue acts that include domain-specific semantics (for example, there are 29 subtypes of wh-questions that can be performed in the domain, each with a separate domainspecific dialogue act label). [sent-79, score-3.14]
</p><p>51 h 0 his 1 name, 2 any 3 other 4 informathion 5 about 6 him, 7 where 8 he 9 lives 10i is labeled with: [0 2] whq ( ob j : st rangeMan att r : name ) , [2 7] whq ( ob j : st rangeMan ) and [7 10] whq ( ob j : st rangeMan att r : locat ion ) . [sent-82, score-0.455]
</p><p>52 h  I can ’t 2 offer 3 you 4 money 5 but 6 I can 1 7 8 offer 9 you 10 protection 11i is labeled with: [0 5] re j ect, [5 11] o f feri ( s afety ) . [sent-84, score-0.072]
</p><p>53 2 Setup In our experiments, we performed 10-fold crossvalidation using the dataset described above. [sent-86, score-0.039]
</p><p>54 For each evaluation fold we run the procedure described in Section 2, using the classifier obtained from the corresponding training fold. [sent-88, score-0.05]
</p><p>55 The segments present in the manual annotation are then aligned with the segments identified by our system (the 1Although the dialogue act labels could be thought of as compositional, since they include separate parts, we treat them as atomic labels. [sent-89, score-1.237]
</p><p>56 alignment takes in consideration both the word span and the dialogue act label associated to each segment). [sent-90, score-1.107]
</p><p>57 The evaluation then considers as correct only the subset of dialogue acts identified automatically that were successfully aligned with the same dialogue act label in the gold-standard annotation. [sent-91, score-2.093]
</p><p>58 We compared the performance of our proposed approach to two baselines; both use the same maximum entropy classifier used internally by our proposed approach. [sent-92, score-0.082]
</p><p>59 The first baseline simply uses the single dia-  logue act label chosen by the maximum entropy classifier as the only dialogue act for each utterance. [sent-94, score-1.58]
</p><p>60 In other words, this baseline corresponds to the NLU developed for the SASO dialogue system (Traum et al. [sent-95, score-0.774]
</p><p>61 This baseline is expected to have lower recall for those utterances that contain multiple dialogue acts, but potentially higher precision overall, since most utterances in the dataset contain only one dialogue act label. [sent-98, score-2.721]
</p><p>62 For the second baseline, we treat multiple dialogue act detection as a set of binary classification tasks, one for each possible dialogue act label in the domain. [sent-100, score-2.221]
</p><p>63 We start from the same training data as above, and create N copies, where N is the number of unique dialogue acts labels in the training set. [sent-101, score-1.044]
</p><p>64 Each utterance-label pair in the original training set is now present in all N training sets. [sent-102, score-0.026]
</p><p>65 Ifin the original training set an utterance was labeled with the ith dialogue act label, now it will be labeled as a positive example in the ith training set and as a negative example in all other training sets. [sent-103, score-1.286]
</p><p>66 Binary classifiers for each N dialogue act labels are then trained. [sent-104, score-1.108]
</p><p>67 During run-time, each utterance is classified by all N models and the result is the subset of dialogue acts associated with the models that labeled the example as positive. [sent-105, score-1.078]
</p><p>68 This baseline is excepted to be much closer in performance to our approach, but it is incapable of determining what words in the utterance correspond to each dialogue act3. [sent-106, score-0.9]
</p><p>69 , only the baseline NLU, which consist only of a maximum entropy classifier. [sent-108, score-0.062]
</p><p>70 3This corresponds to the transformation of a multi-label 98  our proposed approach (denoted by “this”) and the two baseline methods. [sent-109, score-0.017]
</p><p>71 Single indicates the performance when tested only on utterances annotated with a single dialogue act. [sent-110, score-1.191]
</p><p>72 Multiple is for utterances annotated with more than one dialogue act, and Overall indicates the performance over the entire set. [sent-111, score-1.161]
</p><p>73 All measures show that the proposed approach has considerably improved performance for utterances that contain multiple dialogue acts, with only a small increase in the number of errors for the utterances containing only a single dialogue act. [sent-115, score-2.357]
</p><p>74 The differences in  F-score over the entire dataset (shown in the Overall portion of Table 2) are statistically significant (p < 0. [sent-117, score-0.039]
</p><p>75 As a drawback of our approach, it is on average 25 times slower than our first baseline, which is incapable of identifying multiple dialogue acts in a Our approach is still about 15% faster than our second baseline, which  utterance4. [sent-119, score-1.071]
</p><p>76 4In our dataset, our method takes on average about 102ms to process an utterance that was originally labeled with multiple dialogue acts, and 12ms to process one annotated with a single dialogue act. [sent-122, score-1.732]
</p><p>77 Number of words in input text  Figure 1: Execution time in milliseconds of the classifier with respect to the number of words in the input text. [sent-123, score-0.083]
</p><p>78 identifies multiple speech acts, but without segmentation, and with lower F-score. [sent-124, score-0.052]
</p><p>79 Figure 1 shows the execution time versus the length of the input text. [sent-125, score-0.046]
</p><p>80 It also shows a histogram of utterance lengths in the dataset, suggesting that our approach is suitable for most utterances in our dataset, but may be too slow for some of the longer utterances (with 30 words or more). [sent-126, score-0.903]
</p><p>81 Figure 2 shows the histogram of the average error (absolute value of word offset) in the start and end of the dialogue act segmentation. [sent-127, score-1.167]
</p><p>82 Each dialogue act identified by Algorithm 1 is associated with a starting and ending index that corresponds to the portion of the input text that has been classified with the given dialogue act. [sent-128, score-1.918]
</p><p>83 During the evaluation, we find the best alignment between the manual annotation and the segmentation we computed. [sent-129, score-0.051]
</p><p>84 extracted dialogue act and dialogue act present in the annotation) we compute the absolute error between the starting point of the extracted dialogue act and the starting point of the paired annotation. [sent-132, score-3.289]
</p><p>85 We do the same for the ending point and we average the two error figures. [sent-133, score-0.077]
</p><p>86 The  result is binned to form the histogram displayed in figure 2. [sent-134, score-0.039]
</p><p>87 The figure also shows the average error and the standard deviation. [sent-135, score-0.037]
</p><p>88 The largest average error happens with the data annotated with multiple dialogue acts. [sent-136, score-0.856]
</p><p>89 In that case, the extracted segments have a starting and ending point that in average are misplaced by about ±2 words. [sent-137, score-0.13]
</p><p>90 4  Conclusion  We described a method to segment a given utterance into non-overlapping portions, each associated 99  Average error in the starting and ending indexes of each speech act segment  Figure 2: Histogram of the average absolute error in the two extremes (i. [sent-138, score-0.637]
</p><p>91 start and end) of segments corresponding to the dialogue acts identified in the dataset. [sent-140, score-1.035]
</p><p>92 The method addresses the problem that, in development of new scenarios for conversational dialogue systems, there is typically not enough training data covering all or most configurations of how multiple dialogue acts appear in single utterances. [sent-142, score-1.892]
</p><p>93 Our approach requires only labeled utterances (or utterance segments) corresponding to a single dialogue act, which tends to be the easiest type of training data to author and to collect. [sent-143, score-1.284]
</p><p>94 We performed an evaluation using existing data annotated with multiple dialogue acts for each utterance. [sent-144, score-1.035]
</p><p>95 The main drawback of the proposed approach is the complexity ofthe segment optimization that requires calling the dialogue act classifier O(n2) times with n representing the length of the input utterance. [sent-146, score-1.223]
</p><p>96 The benefit, however, is that having the ability to identify multiple dialogue acts in utterances takes us one step closer towards giving users more freedom to express themselves naturally with dialogue systems. [sent-147, score-2.188]
</p><p>97 Viability of a simple dialogue act scheme for a tactical questioning dialogue system. [sent-156, score-1.908]
</p><p>98 Comparison of alignment templates and maximum entropy models for natural language understanding. [sent-160, score-0.045]
</p><p>99 From domain specification to virtual humans: An integrated approach to authoring tactical questioning characters. [sent-166, score-0.099]
</p><p>100 Towards natural language understanding of partial speech recognition results in dialogue systems. [sent-171, score-0.772]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dialogue', 0.757), ('utterances', 0.379), ('act', 0.32), ('acts', 0.216), ('nlu', 0.171), ('utterance', 0.088), ('artstein', 0.06), ('rangeman', 0.06), ('whq', 0.06), ('traum', 0.057), ('gandhe', 0.053), ('sagae', 0.052), ('segment', 0.05), ('segments', 0.048), ('bender', 0.046), ('bestdialogueactendingat', 0.045), ('devault', 0.045), ('conversational', 0.045), ('ending', 0.04), ('questioning', 0.04), ('dataset', 0.039), ('ob', 0.039), ('histogram', 0.039), ('multiple', 0.037), ('classifier', 0.037), ('sudeep', 0.037), ('tactical', 0.034), ('sh', 0.034), ('att', 0.033), ('subsystem', 0.033), ('labels', 0.031), ('segmentation', 0.031), ('label', 0.03), ('bilyana', 0.03), ('morbini', 0.03), ('tacq', 0.03), ('single', 0.03), ('users', 0.028), ('spans', 0.028), ('entropy', 0.027), ('saso', 0.027), ('gerten', 0.027), ('jillian', 0.027), ('numwords', 0.027), ('annotated', 0.025), ('virtual', 0.025), ('logue', 0.024), ('anton', 0.024), ('incapable', 0.024), ('roque', 0.024), ('ith', 0.024), ('execution', 0.023), ('ron', 0.023), ('leuski', 0.023), ('antonio', 0.023), ('input', 0.023), ('scenarios', 0.021), ('starting', 0.021), ('offer', 0.021), ('average', 0.021), ('calling', 0.02), ('annotation', 0.02), ('calls', 0.019), ('contain', 0.018), ('maximum', 0.018), ('longer', 0.018), ('labeled', 0.017), ('baseline', 0.017), ('name', 0.017), ('army', 0.016), ('enough', 0.016), ('score', 0.016), ('david', 0.016), ('error', 0.016), ('drawback', 0.016), ('st', 0.015), ('portions', 0.015), ('speech', 0.015), ('si', 0.015), ('start', 0.014), ('collected', 0.014), ('correspond', 0.014), ('express', 0.014), ('kenji', 0.014), ('module', 0.014), ('overall', 0.013), ('training', 0.013), ('calo', 0.013), ('locat', 0.013), ('ars', 0.013), ('protection', 0.013), ('afc', 0.013), ('diaholmia', 0.013), ('marsella', 0.013), ('negotiation', 0.013), ('vista', 0.013), ('confidence', 0.013), ('identification', 0.013), ('semantics', 0.013), ('aligned', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="185-tfidf-1" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>2 0.5359804 <a title="185-tfidf-2" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>3 0.50073946 <a title="185-tfidf-3" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>Author: Paul Piwek ; Svetlana Stoyanchev</p><p>Abstract: This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.</p><p>4 0.40452284 <a title="185-tfidf-4" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>Author: Wei-Bin Liang ; Chung-Hsien Wu ; Chia-Ping Chen</p><p>Abstract: In this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. First, partial sentence trees are proposed to represent a speech recognition output sentence. Semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. The constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. The proposed approach is implemented and evaluated in a Mandarin spoken dialogue system for tour-guiding service. Combined with scores derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84.3% detection accuracy, an absolute improvement of 34.7% over the baseline of the semantic slot-based method with 49.6% detection accuracy.</p><p>5 0.3342804 <a title="185-tfidf-5" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>Author: Staffan Larsson ; Alexander Berman ; Jessica Villing</p><p>Abstract: Alexander Berman Jessica Villing Talkamatic AB University of Gothenburg Sweden Sweden alex@ t alkamat i . se c jessi ca@ l ing .gu . s e 2 In-vehicle dialogue systems This paper describes Dico II+, an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogueand a “speech cursor” which enables menu navigation as well as browsing long list using haptic input and spoken output.</p><p>6 0.21821155 <a title="185-tfidf-6" href="./acl-2011-Question_Detection_in_Spoken_Conversations_Using_Textual_Conversations.html">257 acl-2011-Question Detection in Spoken Conversations Using Textual Conversations</a></p>
<p>7 0.20918524 <a title="185-tfidf-7" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>8 0.19527498 <a title="185-tfidf-8" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>9 0.16795012 <a title="185-tfidf-9" href="./acl-2011-Turn-Taking_Cues_in_a_Human_Tutoring_Corpus.html">312 acl-2011-Turn-Taking Cues in a Human Tutoring Corpus</a></p>
<p>10 0.15043724 <a title="185-tfidf-10" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>11 0.13734722 <a title="185-tfidf-11" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>12 0.13377713 <a title="185-tfidf-12" href="./acl-2011-Entrainment_in_Speech_Preceding_Backchannels..html">118 acl-2011-Entrainment in Speech Preceding Backchannels.</a></p>
<p>13 0.10844024 <a title="185-tfidf-13" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>14 0.083428815 <a title="185-tfidf-14" href="./acl-2011-Detection_of_Agreement_and_Disagreement_in_Broadcast_Conversations.html">95 acl-2011-Detection of Agreement and Disagreement in Broadcast Conversations</a></p>
<p>15 0.063816108 <a title="185-tfidf-15" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<p>16 0.058257028 <a title="185-tfidf-16" href="./acl-2011-Disentangling_Chat_with_Local_Coherence_Models.html">101 acl-2011-Disentangling Chat with Local Coherence Models</a></p>
<p>17 0.053915311 <a title="185-tfidf-17" href="./acl-2011-Identifying_Sarcasm_in_Twitter%3A_A_Closer_Look.html">160 acl-2011-Identifying Sarcasm in Twitter: A Closer Look</a></p>
<p>18 0.048634738 <a title="185-tfidf-18" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>19 0.047114372 <a title="185-tfidf-19" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>20 0.027879426 <a title="185-tfidf-20" href="./acl-2011-N-Best_Rescoring_Based_on_Pitch-accent_Patterns.html">228 acl-2011-N-Best Rescoring Based on Pitch-accent Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.105), (1, 0.063), (2, -0.043), (3, 0.029), (4, -0.473), (5, 0.504), (6, -0.122), (7, -0.051), (8, -0.001), (9, 0.036), (10, 0.206), (11, 0.046), (12, 0.14), (13, -0.056), (14, 0.085), (15, -0.007), (16, 0.039), (17, 0.009), (18, -0.014), (19, 0.07), (20, -0.124), (21, -0.011), (22, 0.101), (23, -0.137), (24, 0.013), (25, 0.018), (26, -0.105), (27, 0.039), (28, 0.074), (29, 0.034), (30, 0.041), (31, -0.04), (32, -0.059), (33, -0.019), (34, 0.004), (35, -0.014), (36, 0.015), (37, -0.017), (38, 0.01), (39, 0.005), (40, 0.028), (41, 0.023), (42, 0.017), (43, -0.009), (44, -0.001), (45, 0.024), (46, 0.011), (47, -0.014), (48, -0.017), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98881966 <a title="185-lsi-1" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>2 0.95876122 <a title="185-lsi-2" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>3 0.94147432 <a title="185-lsi-3" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>Author: Paul Piwek ; Svetlana Stoyanchev</p><p>Abstract: This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.</p><p>4 0.92280775 <a title="185-lsi-4" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>Author: Staffan Larsson ; Alexander Berman ; Jessica Villing</p><p>Abstract: Alexander Berman Jessica Villing Talkamatic AB University of Gothenburg Sweden Sweden alex@ t alkamat i . se c jessi ca@ l ing .gu . s e 2 In-vehicle dialogue systems This paper describes Dico II+, an in-vehicle dialogue system demonstrating a novel combination of flexible multimodal menu-based dialogueand a “speech cursor” which enables menu navigation as well as browsing long list using haptic input and spoken output.</p><p>5 0.81285447 <a title="185-lsi-5" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>Author: Wei-Bin Liang ; Chung-Hsien Wu ; Chia-Ping Chen</p><p>Abstract: In this study, a novel approach to robust dialogue act detection for error-prone speech recognition in a spoken dialogue system is proposed. First, partial sentence trees are proposed to represent a speech recognition output sentence. Semantic information and the derivation rules of the partial sentence trees are extracted and used to model the relationship between the dialogue acts and the derivation rules. The constructed model is then used to generate a semantic score for dialogue act detection given an input speech utterance. The proposed approach is implemented and evaluated in a Mandarin spoken dialogue system for tour-guiding service. Combined with scores derived from the ASR recognition probability and the dialogue history, the proposed approach achieves 84.3% detection accuracy, an absolute improvement of 34.7% over the baseline of the semantic slot-based method with 49.6% detection accuracy.</p><p>6 0.61796796 <a title="185-lsi-6" href="./acl-2011-Turn-Taking_Cues_in_a_Human_Tutoring_Corpus.html">312 acl-2011-Turn-Taking Cues in a Human Tutoring Corpus</a></p>
<p>7 0.612683 <a title="185-lsi-7" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>8 0.58942324 <a title="185-lsi-8" href="./acl-2011-Entrainment_in_Speech_Preceding_Backchannels..html">118 acl-2011-Entrainment in Speech Preceding Backchannels.</a></p>
<p>9 0.48372233 <a title="185-lsi-9" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>10 0.43861252 <a title="185-lsi-10" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>11 0.32939571 <a title="185-lsi-11" href="./acl-2011-Question_Detection_in_Spoken_Conversations_Using_Textual_Conversations.html">257 acl-2011-Question Detection in Spoken Conversations Using Textual Conversations</a></p>
<p>12 0.31318477 <a title="185-lsi-12" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>13 0.30030119 <a title="185-lsi-13" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>14 0.198788 <a title="185-lsi-14" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>15 0.19125953 <a title="185-lsi-15" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<p>16 0.17168038 <a title="185-lsi-16" href="./acl-2011-Detection_of_Agreement_and_Disagreement_in_Broadcast_Conversations.html">95 acl-2011-Detection of Agreement and Disagreement in Broadcast Conversations</a></p>
<p>17 0.12503774 <a title="185-lsi-17" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>18 0.12002478 <a title="185-lsi-18" href="./acl-2011-Wikulu%3A_An_Extensible_Architecture_for_Integrating_Natural_Language_Processing_Techniques_with_Wikis.html">338 acl-2011-Wikulu: An Extensible Architecture for Integrating Natural Language Processing Techniques with Wikis</a></p>
<p>19 0.11926485 <a title="185-lsi-19" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>20 0.1192579 <a title="185-lsi-20" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.023), (17, 0.035), (26, 0.018), (37, 0.057), (39, 0.059), (41, 0.469), (55, 0.012), (59, 0.031), (72, 0.033), (91, 0.035), (96, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96048468 <a title="185-lda-1" href="./acl-2011-Contrasting_Multi-Lingual_Prosodic_Cues_to_Predict_Verbal_Feedback_for_Rapport.html">83 acl-2011-Contrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback for Rapport</a></p>
<p>Author: Siwei Wang ; Gina-Anne Levow</p><p>Abstract: Verbal feedback is an important information source in establishing interactional rapport. However, predicting verbal feedback across languages is challenging due to languagespecific differences, inter-speaker variation, and the relative sparseness and optionality of verbal feedback. In this paper, we employ an approach combining classifier weighting and SMOTE algorithm oversampling to improve verbal feedback prediction in Arabic, English, and Spanish dyadic conversations. This approach improves the prediction of verbal feedback, up to 6-fold, while maintaining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration.</p><p>2 0.95550305 <a title="185-lda-2" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>Author: Yu Hong ; Jianfeng Zhang ; Bin Ma ; Jianmin Yao ; Guodong Zhou ; Qiaoming Zhu</p><p>Abstract: Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entitytype consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction. 1</p><p>3 0.93697381 <a title="185-lda-3" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>Author: Antske Fokkens</p><p>Abstract: When designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. Moreover, because analyses interact, the choices made by the engineer influence the possibilities available in further grammar development. The order in which phenomena are treated may therefore have a major impact on the resulting grammar. This paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. Iargue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. The idea is illustrated through a comparative study of auxiliary structures in HPSG-based grammars for German and Dutch. Auxiliaries form a central phenomenon of German and Dutch and are likely to influence many components of the grammar. This study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation.</p><p>4 0.92630649 <a title="185-lda-4" href="./acl-2011-K-means_Clustering_with_Feature_Hashing.html">189 acl-2011-K-means Clustering with Feature Hashing</a></p>
<p>Author: Hajime Senuma</p><p>Abstract: One of the major problems of K-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task.</p><p>5 0.92430526 <a title="185-lda-5" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Hal Daume III ; Raghavendra Udupa</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. Since the candidate alignments are noisy, we de- velop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain.</p><p>6 0.92275548 <a title="185-lda-6" href="./acl-2011-Nonparametric_Bayesian_Machine_Transliteration_with_Synchronous_Adaptor_Grammars.html">232 acl-2011-Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars</a></p>
<p>7 0.92119533 <a title="185-lda-7" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>8 0.91342115 <a title="185-lda-8" href="./acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers.html">56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</a></p>
<p>same-paper 9 0.85053337 <a title="185-lda-9" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>10 0.65939021 <a title="185-lda-10" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>11 0.65371817 <a title="185-lda-11" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>12 0.64343238 <a title="185-lda-12" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>13 0.64196658 <a title="185-lda-13" href="./acl-2011-Modeling_Wisdom_of_Crowds_Using_Latent_Mixture_of_Discriminative_Experts.html">223 acl-2011-Modeling Wisdom of Crowds Using Latent Mixture of Discriminative Experts</a></p>
<p>14 0.62860864 <a title="185-lda-14" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>15 0.61375117 <a title="185-lda-15" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>16 0.61180151 <a title="185-lda-16" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>17 0.60577381 <a title="185-lda-17" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>18 0.6028744 <a title="185-lda-18" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>19 0.60281569 <a title="185-lda-19" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>20 0.59830475 <a title="185-lda-20" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
