<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 acl-2011-A Statistical Tree Annotator and Its Applications</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-28" href="#">acl2011-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 acl-2011-A Statistical Tree Annotator and Its Applications</h1>
<br/><p>Source: <a title="acl-2011-28-pdf" href="http://aclweb.org/anthology//P/P11/P11-1123.pdf">pdf</a></p><p>Author: Xiaoqiang Luo ; Bing Zhao</p><p>Abstract: In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.</p><p>Reference: <a title="acl-2011-28-reference" href="../acl2011_reference/acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present a statistical tree annotator augmenting nodes with additional information. [sent-6, score-0.369]
</p><p>2 We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. [sent-8, score-1.438]
</p><p>3 Our function tag prediction system outperforms significantly published results. [sent-9, score-0.244]
</p><p>4 1A constituent in the source language is projectable if it can be aligned to a contiguous span in the target language. [sent-20, score-0.444]
</p><p>5 1230 Such problems can be abstracted as adding additional annotations to an existing tree structure. [sent-21, score-0.15]
</p><p>6 , 1993) contains function tags and many carry semantic information. [sent-23, score-0.274]
</p><p>7 To add semantic information to the basic syntactic trees, a logical step is to predict these function tags after syntactic parsing. [sent-24, score-0.373]
</p><p>8 For the problem of predicting projectable syntactic constituent, one can use a sentence alignment tool and syntactic trees on source sentences to create training data  by annotating a tree node as projectable or not. [sent-25, score-1.502]
</p><p>9 A generic tree annotator can also open the door of solving other natural language problems so long as the problem can be cast as annotating tree nodes. [sent-26, score-0.529]
</p><p>10 As one such example, we will present how to predict empty elements for the Chinese language. [sent-27, score-0.473]
</p><p>11 Some of the above-mentioned problems have been studied before: predicting function tags were studied in (Blaheta and Charniak, 2000; Blaheta, 2003; Lintean and Rus, 2007a), and results of predicting and recovering empty elements can be found in (Dienes et al. [sent-28, score-1.289]
</p><p>12 In this work, we will show that these seemingly unrelated problems can be treated uniformly as adding annotations to an existing tree structure, which is the first goal of this work. [sent-30, score-0.15]
</p><p>13 Second, the proposed generic tree annotator can also be used to solve new problems: we will show how it can be used to predict projectable syntactic constituents. [sent-31, score-0.739]
</p><p>14 g, we find some features are very  effective in predicting function tags and our system ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. [sent-33, score-0.52]
</p><p>15 Section 2 describes our tree annotator, which is a conditional log-linear model. [sent-37, score-0.15]
</p><p>16 Next, three applications of the proposed tree annotator are presented in Section 4: predicting English function tags, predicting Chinese empty elements and predicting Arabic projectable constituents. [sent-39, score-1.915]
</p><p>17 2  A MaxEnt Tree Annotator Model  The input to the tree annotator is a tree T. [sent-41, score-0.408]
</p><p>18 While T can be of any type, we concentrate on the syntactic parse tree in this paper. [sent-42, score-0.238]
</p><p>19 eA |Ts an example, Figure 1 shows a syntactic parse tree with the prefix order (i. [sent-44, score-0.238]
</p><p>20 , the number at the up-right corner of  each non-terminal node), where child nodes are visited recursively from left to right before the parent node is visited. [sent-46, score-0.502]
</p><p>21 Thus, the NP -SBJ node is visited first, followed by the NP spanning duo act ion, followed by the PP-CLR node etc. [sent-47, score-0.386]
</p><p>22 Xx∈L  Xk  1231  Figure 1: A sample tree: the number on the upright corner of each non-terminal node is the visit order. [sent-55, score-0.249]
</p><p>23 Once a model is trained, at testing time it is applied to input tree nodes by the same order. [sent-60, score-0.261]
</p><p>24 Figure 1 highlights the prediction of the function tag for node 3(i. [sent-61, score-0.413]
</p><p>25 , PP-CLR-node in the thickened box) after 2 shaded nodes (NP-SBJ node and NP node) are predicted. [sent-63, score-0.318]
</p><p>26 Numbers in the first column are the feature indices. [sent-66, score-0.153]
</p><p>27 The second column contains a brief description of each feature, and the third column contains the feature value when the feature at the same row is applied to the PP-node of Figure 1 for the task of predicting function tags. [sent-67, score-0.685]
</p><p>28 Feature 1through 8 are non-lexical features in that all of them are computed based on the labels or POS  tags of neighboring nodes (e. [sent-68, score-0.252]
</p><p>29 When predicting the function tag for the PP-node in Figure 1, there is no predicted value for its left-sibling and any of its child node. [sent-74, score-0.583]
</p><p>30 That’s why both feature values are NONE, a special symbol signifying that a node does not carry any function tag. [sent-75, score-0.408]
</p><p>31 If we were to predict the function tag for the VP-node, the value of Feature 9 would be SBJ, while Feature 10 will be instantiated twice with one value being CLR, another being TMP. [sent-76, score-0.264]
</p><p>32 in Figure 1 would yield a feature instance that captures the fact that the current node is a PP node and its head child’s POS tag is TO. [sent-77, score-0.53]
</p><p>33 4  Applications and Results  A wide variety of language problems can be treated as or cast into a tree annotating problem. [sent-78, score-0.243]
</p><p>34 In this section, we present three applications of the statistical tree annotator. [sent-79, score-0.15]
</p><p>35 The first application is to predict function tags ofan input syntactic parse tree; the sec-  descriptions of each feature, and the 3rd column the feature value when it is applied to the PP-node in Figure 1. [sent-80, score-0.53]
</p><p>36 Feature 17 tests if the current node is the head of its parent. [sent-86, score-0.207]
</p><p>37 4% )AFPENCTDuUXLOnTFVMcSCLtBPiOoGNRnJCSPFTVHMaODRLgCNIsPTL  Table 2: Four types of function tags and their relative frequency 4. [sent-90, score-0.239]
</p><p>38 We use all features in Table 1 and build four models, each of which pre-  dicting one type of function tags. [sent-94, score-0.166]
</p><p>39 84% non-null function tags in the test set, our system achieves a relative error reduction of 77. [sent-98, score-0.239]
</p><p>40 4r97s1586%  Table 3: Function tag prediction accuracies on gold parse trees: breakdown by types of function tags. [sent-117, score-0.29]
</p><p>41 The 2nd column is due to (Blaheta and Charniak, 2000) and 3rd column due to (Lintean and Rus, 2007a). [sent-118, score-0.162]
</p><p>42 Less than 2% of nodes with non-empty function tags were assigned multiple function tags. [sent-127, score-0.482]
</p><p>43 traint esgt#71- ,s61e4n80t6s1#,32-14n,o21,d17e74s7#-f2u86n0,c7,N7 58o5des Table 4: Statistics of OntoNotes: #-sents number of sentences; #-nodes – number of non-terminal nodes; #-funcNodes number of nodes containing non-empty function tags. [sent-129, score-0.243]
</p><p>44 The dummy baseline is predicting the most likely prior the empty function tag, which indicates that there are 78. [sent-131, score-0.693]
</p><p>45 So does the node external lexical features (Feature 13 and 14) which added an ad–  ditional 1. [sent-138, score-0.203]
</p><p>46 From these results, we can conclude that, unlike syntactic parsing (Bikel, 2004), lexical information is extremely important for predicting and recovering function tags. [sent-144, score-0.49]
</p><p>47 This is not surprising since many function tags carry semantic information, and more often than not, the ambiguity can only be resolved by lexical information. [sent-145, score-0.274]
</p><p>48 This and its lack of subordinate conjunction complementizers lead to the ubiquitous use of empty elements in the Chinese treebank (Xue et al. [sent-151, score-0.588]
</p><p>49 Predicting or recovering these empty elements is therefore important for the Chinese language pro-  +FpNenrhioa endrsat-uel(gr-pxwieurn otScdaser iltcn NaiolbOne NsxEioc)anly9A752861c. [sent-153, score-0.492]
</p><p>50 02375u4102% racy  Table 5: Effects of feature sets: the second row contains the baseline result when always predicting NONE; Row 3 through 8 contain results by incrementally adding feature sets. [sent-154, score-0.391]
</p><p>51 Recently, Chung and Gildea (2010) has found it useful to recover empty elements in machine translation. [sent-156, score-0.459]
</p><p>52 Since empty elements do not have any surface string representation, we tackle the problem by attaching a pseudo function tag to an empty element’s lowest non-empty parent and then removing the subtree spanning it. [sent-157, score-1.481]
</p><p>53 Figure 2 contains an example tree before and after removing the empty element *pro * and annotating the non-empty parent with a pseudo function tag NoneL. [sent-158, score-1.186]
</p><p>54 In particular, line 2 of Algorithm 1find the lowest parent of an empty element that spans at least one non-trace word. [sent-160, score-0.513]
</p><p>55 Since *pro * is the left-most child, line 4 of Algorithm 1adds the pseudo function tag NoneL to the top IP-node. [sent-162, score-0.555]
</p><p>56 Line 9 then removes its NP child node and all lower children (i. [sent-163, score-0.32]
</p><p>57 , shaded subtree in Figure 2(1)), resulting in the tree in Figure 2(2). [sent-165, score-0.221]
</p><p>58 Line 4 to 8 of Algorithm 1 indicate that there are 3 types of pseudo function tags: NoneL, NoneM, and NoneR, encoding a trace found in the left, middle or right position of its lowest non-empty parent. [sent-166, score-0.621]
</p><p>59 The problem could be solved either using heuristics to determine the position of a middle empty element, or encoding the positional information in the pseudo function tag. [sent-168, score-0.757]
</p><p>60 Since here we just want to show that predicting empty elements can be cast as a tree annotation problem, we leave this option to future research. [sent-169, score-0.871]
</p><p>61 Algorithm 1 Procedure to remove empty elements and add pseudo function tags. [sent-171, score-0.866]
</p><p>62 TSDuresavbitng01F5908il432e160I- D013985s243 105, pc025h4to0v9e,3c-n03ix54n0,15m4078,s-31n0 b6957c0,1p-3h0o854e97n065ix-  Table 6: Data partition for CTB6 and CTB 7’s broadcast conversation portion  We then apply Algorithm 1to transform trees and predict pseudo function tags. [sent-182, score-0.719]
</p><p>63 Out of 1,100,506 nonterminal nodes in the training data, 80,212 of them contain pseudo function tags. [sent-183, score-0.554]
</p><p>64 There are 94 nodes containing 2 pseudo function tags. [sent-184, score-0.554]
</p><p>65 To understand why the accuracies are so high, we look into the 5 most frequent labels carrying pseudo tags in the development set, and tabulate their performance in Table 7. [sent-190, score-0.418]
</p><p>66 The 2nd column contains the number ofnodes in the reference; the 3rd column the number of nodes of system output; the 4th column the number of nodes with correct prediction; and the 5th column F-measure for each label. [sent-191, score-0.546]
</p><p>67 F9 831926538  Table 7: 5 most frequent labels carrying pseudo tags and their performances complementizers for subordinate clauses. [sent-196, score-0.496]
</p><p>68 In other words, left-most empty elements under CP are almost unambiguous: if a CP node has an immediate IP child, it almost always has a left-most empty element; similarly, if an IP node has a VP node as the left-most child (i. [sent-197, score-1.366]
</p><p>69 , without a subject), it almost always should have a left empty element (e. [sent-199, score-0.373]
</p><p>70 Another way to interpret these results is as follows: when developing the Chinese treebank, there is really no point to annotate leftmost traces for CP and IP when tree structures are available. [sent-202, score-0.201]
</p><p>71 On the other hand, predicting the left-most empty elements for VP is a lot harder: the F-measure is only 86. [sent-203, score-0.67]
</p><p>72 Predicting the rightmost empty elements under VP and middle empty elements under IP is somewhat easier: VP-NoneR and IP -NoneM’s F-measures are 92. [sent-205, score-0.846]
</p><p>73 3 Predicting Projectable Constituents The third application is predicting projectable con-  stituents for machine translation. [sent-209, score-0.608]
</p><p>74 We start from LDC’s bilingual Arabic-English treebank with source human parse trees and alignments, and mark source constituents as either pro-  Becauseb#o fthesb Iraqi"ofAficltizaAlmAt’stAr}p"s u d enl#oAblmigsat&iownslA. [sent-216, score-0.313]
</p><p>75 Figure 3: An example to show how a source tree is annotated with its alignment with the target sentence. [sent-218, score-0.179]
</p><p>76 The binary annotations can again be treated as pseudo function tags and the proposed tree annotator can be readily applied to this problem. [sent-220, score-0.808]
</p><p>77 The PP # node is not projectable due to an inserted stop from outside; NP # 1is not projectable because it is involved in a 2-to-2 alignment with the token b# outside NP # 1 NP # 2 is aligned ; to a span the I raqi o f fic i al ’ s sudden obl igat i s . [sent-223, score-0.979]
</p><p>78 The LDC’s Arabic-English bilingual treebank does not mark if a source node is projectable or not, but the information can be computed from word alignment. [sent-226, score-0.617]
</p><p>79 The statistics of the training and test data can be found in Table 8, where the number of sentences, the number of nonterminal nodes and the number of non-projectable 1236 nodes are listed in Column 2 through 4, respectively. [sent-228, score-0.222]
</p><p>80 DTra TtieansSitnegt#1 S6,1e,1n52t15s #45n08,o63d76e45s#1N28o1,n6,2P701r oj Table 8: Statistics of the data for predicting projectable constituents We get a 94. [sent-229, score-0.7]
</p><p>81 6% accuracy for predicting projectable constituents on the gold trees, and an 84. [sent-230, score-0.7]
</p><p>82 5  Related Work  Blaheta and Charniak (2000) used a feature tree model to predict function tags. [sent-234, score-0.404]
</p><p>83 There are considerable overlap in terms of features used in (Blaheta and Charniak, 2000; Blaheta, 2003) and our system: for example, the label of current node, parent node and sibling nodes. [sent-236, score-0.271]
</p><p>84 Table 2 of (Blaheta and Charniak, 2000) contains the accuracies for 4 types of function tags, and our results in Table 3 compare favorably with those in (Blaheta and Charniak, 2000). [sent-240, score-0.176]
</p><p>85 Lintean and Rus (2007a; Lintean and Rus (2007b) also studied the function tagging problem and applied naive Bayes and decision tree to it. [sent-241, score-0.314]
</p><p>86 Campbell (2004) and Schmid (2006) studied the problem of predicting and recovering empty categories, but they used very different approaches: in (Campbell, 2004), a rule-based approach is used while (Schmid, 2006) used a non-lexical PCFG similar to (Klein and Manning, 2003). [sent-244, score-0.662]
</p><p>87 Chung and  Gildea (2010) studied the effects of empty categories on machine translation and they found that even with noisy machine predictions, empty categories still helped machine translation. [sent-245, score-0.821]
</p><p>88 In this paper, we showed that empty categories can be encoded as pseudo function tags and thus predicting and recovering empty categories can be cast as a tree annotating problem. [sent-246, score-1.845]
</p><p>89 Our results also shed light on some empty categories can almost be determined unambiguously, given a gold tree structure, which suggests that these empty elements do not need to be annotated. [sent-247, score-0.941]
</p><p>90 Since their results for predicting function tags are on system parses, they are not comparable with ours. [sent-250, score-0.486]
</p><p>91 , 2006) also contains a second stage employing multiple classifiers to recover empty categories and resolve coindexations between an empty element and its antecedent. [sent-252, score-0.777]
</p><p>92 As for predicting projectable constituent, it is related to the work described in (Xiong et al. [sent-253, score-0.608]
</p><p>93 , 2010) defines projectable spans on a left-branching deriva-  tion tree solely for their phrase decoder and models, while translation boundaries in our work are defined from source parse trees. [sent-256, score-0.639]
</p><p>94 1237 6  Conclusions and Future Work  We proposed a generic statistical tree annotator in the paper. [sent-260, score-0.286]
</p><p>95 We have shown that a variety of natural language problems can be tackled with the proposed tree annotator, from predicting function tags, predicting empty categories, to predicting projectable syntactic constituents for machine translation. [sent-261, score-1.832]
</p><p>96 Our results of predicting function tags compare favorably with published results on the same data set, possibly due to new features employed in the system. [sent-262, score-0.564]
</p><p>97 We showed that empty categories can be represented as pseudo function tags, and thus predicting empty categories can be solved with the proposed tree annotator. [sent-263, score-1.576]
</p><p>98 The same technique can be used to predict projectable syntactic constituents for machine translation. [sent-264, score-0.545]
</p><p>99 First, the results for predicting function tags and Chinese empty elements were obtained on human-annotated trees and it would be interesting to do it on parse trees generated by system. [sent-266, score-1.073]
</p><p>100 Second, predicting projectable constituents is for improving machine translation and we are integrating the component into a syntax-based machine translation system. [sent-267, score-0.806]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('projectable', 0.361), ('empty', 0.314), ('pseudo', 0.311), ('blaheta', 0.307), ('predicting', 0.247), ('lintean', 0.216), ('node', 0.169), ('tree', 0.15), ('rus', 0.145), ('trace', 0.136), ('function', 0.132), ('charniak', 0.123), ('child', 0.122), ('nodes', 0.111), ('elements', 0.109), ('annotator', 0.108), ('tags', 0.107), ('constituents', 0.092), ('nonel', 0.089), ('ontonotes', 0.089), ('tag', 0.082), ('column', 0.081), ('np', 0.073), ('xiong', 0.073), ('feature', 0.072), ('cp', 0.072), ('recovering', 0.069), ('parent', 0.068), ('noner', 0.067), ('tabulated', 0.067), ('ip', 0.065), ('chinese', 0.063), ('broadcast', 0.06), ('trees', 0.059), ('element', 0.059), ('treebank', 0.058), ('campbell', 0.054), ('dienes', 0.054), ('constituent', 0.054), ('categories', 0.054), ('translation', 0.053), ('li', 0.052), ('cast', 0.051), ('gabbard', 0.051), ('traces', 0.051), ('predict', 0.05), ('spanning', 0.048), ('visit', 0.048), ('conversation', 0.046), ('parse', 0.046), ('complementizers', 0.044), ('fic', 0.044), ('nonem', 0.044), ('raqi', 0.044), ('favorably', 0.044), ('luo', 0.044), ('shen', 0.044), ('annotating', 0.042), ('syntactic', 0.042), ('lowest', 0.042), ('marcus', 0.042), ('ctb', 0.042), ('pro', 0.04), ('penn', 0.039), ('berger', 0.038), ('shaded', 0.038), ('head', 0.038), ('chung', 0.037), ('computes', 0.036), ('schmid', 0.036), ('recover', 0.036), ('wsj', 0.035), ('transform', 0.035), ('carry', 0.035), ('yamada', 0.034), ('xue', 0.034), ('subordinate', 0.034), ('xiaoqiang', 0.034), ('features', 0.034), ('vp', 0.034), ('subtree', 0.033), ('corner', 0.032), ('locative', 0.032), ('ldc', 0.032), ('pp', 0.032), ('studied', 0.032), ('gk', 0.031), ('prediction', 0.03), ('line', 0.03), ('children', 0.029), ('bing', 0.029), ('conjunction', 0.029), ('source', 0.029), ('removing', 0.028), ('zhao', 0.028), ('generic', 0.028), ('ralph', 0.027), ('partition', 0.026), ('recovery', 0.026), ('jinxi', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="28-tfidf-1" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>Author: Xiaoqiang Luo ; Bing Zhao</p><p>Abstract: In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.</p><p>2 0.3636671 <a title="28-tfidf-2" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>Author: Shu Cai ; David Chiang ; Yoav Goldberg</p><p>Abstract: We present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. This method outperforms the best published method we are aware of on English and a recently published method on Chinese.</p><p>3 0.26087427 <a title="28-tfidf-3" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>4 0.1397665 <a title="28-tfidf-4" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>5 0.1118451 <a title="28-tfidf-5" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: In the present paper, we propose the effective usage of function words to generate generalized translation rules for forest-based translation. Given aligned forest-string pairs, we extract composed tree-to-string translation rules that account for multiple interpretations of both aligned and unaligned target function words. In order to constrain the exhaustive attachments of function words, we limit to bind them to the nearby syntactic chunks yielded by a target dependency parser. Therefore, the proposed approach can not only capture source-tree-to-target-chunk correspondences but can also use forest structures that compactly encode an exponential number of parse trees to properly generate target function words during decoding. Extensive experiments involving large-scale English-toJapanese translation revealed a significant im- provement of 1.8 points in BLEU score, as compared with a strong forest-to-string baseline system.</p><p>6 0.1107718 <a title="28-tfidf-6" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>7 0.10708784 <a title="28-tfidf-7" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>8 0.1068658 <a title="28-tfidf-8" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>9 0.10297977 <a title="28-tfidf-9" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>10 0.092884913 <a title="28-tfidf-10" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>11 0.090344667 <a title="28-tfidf-11" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>12 0.086676814 <a title="28-tfidf-12" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>13 0.086079486 <a title="28-tfidf-13" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>14 0.077861592 <a title="28-tfidf-14" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>15 0.076464087 <a title="28-tfidf-15" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>16 0.074035555 <a title="28-tfidf-16" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>17 0.072491944 <a title="28-tfidf-17" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>18 0.071104057 <a title="28-tfidf-18" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>19 0.070993863 <a title="28-tfidf-19" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>20 0.070963964 <a title="28-tfidf-20" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.214), (1, -0.105), (2, 0.012), (3, -0.139), (4, -0.021), (5, -0.007), (6, -0.08), (7, -0.015), (8, -0.016), (9, -0.006), (10, -0.044), (11, 0.015), (12, -0.058), (13, -0.021), (14, 0.045), (15, -0.046), (16, -0.015), (17, -0.028), (18, 0.07), (19, 0.065), (20, 0.056), (21, 0.02), (22, -0.083), (23, 0.155), (24, 0.044), (25, -0.019), (26, -0.042), (27, 0.032), (28, 0.057), (29, -0.066), (30, 0.047), (31, -0.082), (32, -0.003), (33, -0.033), (34, 0.09), (35, -0.122), (36, -0.083), (37, -0.189), (38, 0.027), (39, -0.082), (40, 0.064), (41, -0.144), (42, 0.141), (43, -0.061), (44, 0.004), (45, 0.061), (46, -0.061), (47, 0.167), (48, -0.0), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93959886 <a title="28-lsi-1" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>Author: Xiaoqiang Luo ; Bing Zhao</p><p>Abstract: In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.</p><p>2 0.87912089 <a title="28-lsi-2" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>Author: Shu Cai ; David Chiang ; Yoav Goldberg</p><p>Abstract: We present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. This method outperforms the best published method we are aware of on English and a recently published method on Chinese.</p><p>3 0.63773972 <a title="28-lsi-3" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>Author: Seth Kulick ; Ann Bies ; Justin Mott</p><p>Abstract: This work introduces a new approach to checking treebank consistency. Derivation trees based on a variant of Tree Adjoining Grammar are used to compare the annotation of word sequences based on their structural similarity. This overcomes the problems of earlier approaches based on using strings of words rather than tree structure to identify the appropriate contexts for comparison. We report on the result of applying this approach to the Penn Arabic Treebank and how this approach leads to high precision of error detection.</p><p>4 0.63563162 <a title="28-lsi-4" href="./acl-2011-Chinese_sentence_segmentation_as_comma_classification.html">66 acl-2011-Chinese sentence segmentation as comma classification</a></p>
<p>Author: Nianwen Xue ; Yaqin Yang</p><p>Abstract: We describe a method for disambiguating Chinese commas that is central to Chinese sentence segmentation. Chinese sentence segmentation is viewed as the detection of loosely coordinated clauses separated by commas. Trained and tested on data derived from the Chinese Treebank, our model achieves a classification accuracy of close to 90% overall, which translates to an F1 score of 70% for detecting commas that signal sentence boundaries.</p><p>5 0.61451507 <a title="28-lsi-5" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>6 0.58060712 <a title="28-lsi-6" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>7 0.55468512 <a title="28-lsi-7" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>8 0.55281049 <a title="28-lsi-8" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>9 0.54207784 <a title="28-lsi-9" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>10 0.50535131 <a title="28-lsi-10" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>11 0.47723037 <a title="28-lsi-11" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>12 0.47056574 <a title="28-lsi-12" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>13 0.46823734 <a title="28-lsi-13" href="./acl-2011-Integrating_surprisal_and_uncertain-input_models_in_online_sentence_comprehension%3A_formal_techniques_and_empirical_results.html">176 acl-2011-Integrating surprisal and uncertain-input models in online sentence comprehension: formal techniques and empirical results</a></p>
<p>14 0.45342445 <a title="28-lsi-14" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>15 0.45128772 <a title="28-lsi-15" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>16 0.44015861 <a title="28-lsi-16" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>17 0.43379191 <a title="28-lsi-17" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>18 0.42053896 <a title="28-lsi-18" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>19 0.41698241 <a title="28-lsi-19" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>20 0.41596141 <a title="28-lsi-20" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.099), (5, 0.02), (17, 0.073), (26, 0.014), (37, 0.118), (39, 0.094), (41, 0.077), (53, 0.022), (55, 0.024), (59, 0.041), (70, 0.057), (72, 0.033), (83, 0.042), (91, 0.049), (96, 0.132), (97, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88721973 <a title="28-lda-1" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.</p><p>same-paper 2 0.87539452 <a title="28-lda-2" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>Author: Xiaoqiang Luo ; Bing Zhao</p><p>Abstract: In many natural language applications, there is a need to enrich syntactical parse trees. We present a statistical tree annotator augmenting nodes with additional information. The annotator is generic and can be applied to a variety of applications. We report 3 such applications in this paper: predicting function tags; predicting null elements; and predicting whether a tree constituent is projectable in machine translation. Our function tag prediction system outperforms significantly published results.</p><p>3 0.84977651 <a title="28-lda-3" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>4 0.84908301 <a title="28-lda-4" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>Author: Shu Cai ; David Chiang ; Yoav Goldberg</p><p>Abstract: We present a simple, language-independent method for integrating recovery of empty elements into syntactic parsing. This method outperforms the best published method we are aware of on English and a recently published method on Chinese.</p><p>5 0.83906156 <a title="28-lda-5" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>Author: Nathan Bodenstab ; Aaron Dunlop ; Keith Hall ; Brian Roark</p><p>Abstract: Efficient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more NLP applications leverage syntactic analyses. We review prior methods for pruning and then present a new framework that unifies their strengths into a single approach. Using a log linear model, we learn the optimal beam-search pruning parameters for each CYK chart cell, effectively predicting the most promising areas of the model space to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions.</p><p>6 0.83730555 <a title="28-lda-6" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>7 0.83609152 <a title="28-lda-7" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>8 0.83408463 <a title="28-lda-8" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>9 0.83338338 <a title="28-lda-9" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>10 0.83262688 <a title="28-lda-10" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>11 0.83053529 <a title="28-lda-11" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>12 0.8287127 <a title="28-lda-12" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>13 0.82615757 <a title="28-lda-13" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>14 0.82575583 <a title="28-lda-14" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>15 0.82574141 <a title="28-lda-15" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>16 0.82564473 <a title="28-lda-16" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>17 0.82462329 <a title="28-lda-17" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>18 0.82298422 <a title="28-lda-18" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>19 0.82288539 <a title="28-lda-19" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>20 0.82270372 <a title="28-lda-20" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
