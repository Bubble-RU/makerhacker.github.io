<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-171" href="#">acl2011-171</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</h1>
<br/><p>Source: <a title="acl-2011-171-pdf" href="http://aclweb.org/anthology//P/P11/P11-1063.pdf">pdf</a></p><p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>Reference: <a title="acl-2011-171-reference" href="../acl2011_reference/acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. [sent-6, score-0.976]
</p><p>2 We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. [sent-10, score-0.535]
</p><p>3 We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity. [sent-11, score-0.337]
</p><p>4 Drawing on earlier successes in speech recognition, research in statistical machine translation has effectively used n-gram word sequence models as language models. [sent-21, score-0.386]
</p><p>5 Modern phrase-based translation using large scale n-gram language models generally performs well in terms of lexical choice, but still often produces ungrammatical output. [sent-22, score-0.298]
</p><p>6 Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothe-  sized translations incrementally, from left-to-right. [sent-24, score-0.224]
</p><p>7 1 As a workaround, parsers can rerank the translated output of translation systems (Och et al. [sent-25, score-0.364]
</p><p>8 On the other hand, incremental parsers (Roark, 2001 ; Henderson, 2004; Schuler et al. [sent-27, score-0.391]
</p><p>9 We observe that incremental parsers, used as structured language models, provide an appropriate algorithmic match to incremental phrase-based decoding. [sent-29, score-0.65]
</p><p>10 We directly integrate incremental syntactic parsing into phrase-based translation. [sent-30, score-0.551]
</p><p>11 , 2003) nor hierarchical phrase-based translation (Chiang, 2005) take explicit advantage of the syntactic structure of either source or target language. [sent-36, score-0.549]
</p><p>12 The translation models in these techniques define phrases as contiguous word sequences (with gaps allowed in the case of hierarchical phrases) which may or may not correspond to any linguistic constituent. [sent-37, score-0.337]
</p><p>13 Early work in statistical phrase-based translation considered whether restricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al. [sent-38, score-0.982]
</p><p>14 , 2003) but found such restrictions failed to improve translation quality. [sent-39, score-0.298]
</p><p>15 Significant research has examined the extent to which syntax can be usefully incorporated into statistical tree-based translation models: string-to-tree  (Yamada and Knight, 2001 ; Gildea, 2003; Imamura et al. [sent-40, score-0.339]
</p><p>16 , 2005) techniques use syntactic information to inform the translation model. [sent-55, score-0.451]
</p><p>17 Recent work has shown that parsing-based machine translation using syntax-augmented (Zollmann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demonstrate substantial gains over hierarchical grammars for certain language pairs (Baker et al. [sent-56, score-0.805]
</p><p>18 In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. [sent-58, score-0.596]
</p><p>19 Traditional approaches to language models in  621 speech recognition and statistical machine translation focus on the use of n-grams, which provide a simple finite-state model approximation of the target language. [sent-60, score-0.492]
</p><p>20 Syntactic language models have also been explored with tree-based translation models. [sent-67, score-0.298]
</p><p>21 (2003) use syntactic language models to rescore the output of a tree-based translation system. [sent-69, score-0.451]
</p><p>22 Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. [sent-71, score-0.473]
</p><p>23 Our  work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models. [sent-72, score-0.776]
</p><p>24 Our syntactic language model fits into the family of linear-time dynamic programming parsers described in (Huang and Sagae, 2010). [sent-73, score-0.266]
</p><p>25 Like (Galley and Manning, 2009) our work implements an incremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse. [sent-74, score-0.737]
</p><p>26 The syntactic cohesion features of Cherry (2008) encourages the use of syntactically well-formed translation phrases. [sent-76, score-0.451]
</p><p>27 These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in concert with our work. [sent-77, score-0.478]
</p><p>28 Figure 1: Partial decoding lattice for standard phrase-based decoding stack algorithm translating the German sentence Der Pr a¨sident trifft am Freitag den Vorstand. [sent-87, score-0.507]
</p><p>29 Each node h in decoding stack t represents the application of a translation option, and includes the source sentence coverage vector, target language ngram state, and syntactic language model state ˜τ th . [sent-88, score-0.897]
</p><p>30 We use the English translation The president meets the board on Friday as a running example throughout all Figures. [sent-90, score-0.495]
</p><p>31 Typically, tree ˆτ is taken to be:  τˆ  = argτmax  P(τ |e)  (1)  We define a syntactic language model on the total probability  based  mass over all possible trees  for string e. [sent-93, score-0.299]
</p><p>32 1 Incremental syntactic language model An incremental parser processes each token of input sequentially from the beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. [sent-96, score-0.603]
</p><p>33 After 622 processing the tth token in string e, an incremental parser has some internal representation of possible hypothesized (incomplete) trees, τt. [sent-97, score-0.467]
</p><p>34 The syntactic language model probability of a partial sentence e1. [sent-98, score-0.256]
</p><p>35 An incremental syntactic language model  can then be defined by a probability mass function (Equation 5) and a transition function δ (Equation 6). [sent-108, score-0.58]
</p><p>36 ˆe = argemaxexp(Xjλjhj(e,f))  (7)  Phrase-based translation constructs a set of translation options hypothesized translations for contiguous portions of the source sentence from a trained phrase table, then incrementally constructs a —  —  lattice of partial target translations (Koehn, 2010). [sent-122, score-1.027]
</p><p>37 To prune the search space, lattice nodes are organized into beam stacks (Jelinek, 1969) according to the number of source words translated. [sent-123, score-0.22]
</p><p>38 An n-gram language model history is also maintained at each node in the translation lattice. [sent-124, score-0.388]
</p><p>39 The search space is further trimmed with hypothesis recombination, which collapses lattice nodes that share a common coverage vector and n-gram state. [sent-125, score-0.274]
</p><p>40 3 Incorporating a Syntactic Language Model Phrase-based translation produces target language words in an incremental left-to-right fashion, generating words at the beginning of a translation first and words at the end of a translation last. [sent-127, score-1.278]
</p><p>41 Similarly, incremental parsers process sentences in an incremental fashion, analyzing words at the beginning of a sentence first and words at the end of a sentence last. [sent-128, score-0.716]
</p><p>42 As such, an incremental parser with transition function δ can be incorporated into the phrase-based decoding process in a straightforward manner. [sent-129, score-0.579]
</p><p>43 Each node in the translation lattice is augmented with a syntactic language model state τ˜ t. [sent-130, score-0.788]
</p><p>44 The hypothesis at the root of the translation lattice is initialized with ˜τ 0, representing the internal state  of the incremental parser before any input words are processed. [sent-131, score-1.054]
</p><p>45 The phrase-based translation decoding process adds nodes to the lattice; each new node contains one or more target language words. [sent-132, score-0.521]
</p><p>46 Given a new target language word et and ˜τ t−1, the incremental parser’s transition function calculates ˜τ t. [sent-134, score-0.439]
</p><p>47 Figure 1 illustrates  δ  623 S NP DT  VP NN  The president  VP VB meets  PP NP  IN  DT  NN  the  NP  on Friday  board  Figure 2: Sample binarized phrase structure tree. [sent-135, score-0.237]
</p><p>48 S S/NP S/PP  NP IN Friday  S/VP  VP  NP  VP/NN  NP/NN  NN  VP/NP  DT  president  VB  The  on NN  DT board the  meets  Figure 3: Sample binarized phrase structure tree after application of right-corner transform. [sent-136, score-0.252]
</p><p>49 a sample phrase-based decoding lattice where each translation lattice node is augmented with syntactic language model state ˜τ t. [sent-137, score-1.077]
</p><p>50 In phrase-based translation, many translation lattice nodes represent multi-word target language phrases. [sent-138, score-0.525]
</p><p>51 For such translation lattice nodes, δ will be called once for each newly hypothesized target language word in the node. [sent-139, score-0.589]
</p><p>52 Only the final syntactic language model state in such sequences need be stored in the translation lattice node. [sent-140, score-0.745]
</p><p>53 4  Incremental Bounded-Memory Parsing with a Time Series Model  Having defined the framework by which any in-  cremental parser may be incorporated into phrasebased translation, we now formally define a specific incremental parser for use in our experiments. [sent-141, score-0.481]
</p><p>54 The parser must process target language words incrementally as the phrase-based decoder adds hypotheses to the translation lattice. [sent-142, score-0.435]
</p><p>55 To facilitate this incremental processing, ordinary phrase-structure trees can be transformed into right-corner recur-  Figure 4: Graphical representation of the dependency structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. [sent-143, score-0.369]
</p><p>56 This model of incremental parsing is implemented as a Hierarchical Hidden Markov Model (HHMM) (Murphy and Paskin, 2001), and is equivalent to a probabilistic pushdown automaton with a bounded pushdown store. [sent-153, score-0.703]
</p><p>57 5) for a partial target language hypothesis, using a bounded store of incomplete constituents cη/cηι. [sent-158, score-0.472]
</p><p>58 1 Formal Parsing Model: Scoring Partial Translation Hypotheses This model is essentially an extension of an HHMM, which obtains a most likely sequence of hidden store states, s11. [sent-161, score-0.225]
</p><p>59 T, using HHMM state transition model θA and observation symbol model θB (Rabiner, 1990):  s11. [sent-169, score-0.228]
</p><p>60 D) (8) The HHMM parser is equivalent to a probabilistic pushdown automaton with a bounded push-  down store. [sent-179, score-0.256]
</p><p>61 The model generates each successive store (using store model θS) only after considering whether each nested sequence of incomplete constituents has completed and reduced (using reduction model θR): PθA(st1. [sent-180, score-0.723]
</p><p>62 sentence  The  The shaded path through the parse lattice illustrates the recognized  right-corner tree structure of Figure 3. [sent-186, score-0.263]
</p><p>63 i f f r dtdt + + 1 1= 01 : P JrθtdR,=d(r ⊥tdK|rtd+1std−1sdt−−11)  (13)  where r⊥ is a null state resulting from the failure of an incomplete constituent to complete, and constants are defined for the edge conditions of st0 and Figure 5 illustrates this model in action. [sent-188, score-0.326]
</p><p>64 These pushdown automaton operations are then refined for right-corner parsing (Schuler, 2009), distinguishing active transitions (model θS-T-A,d, in which an incomplete constituent is completed, but not reduced, and then immediately expanded to a  rtD+1. [sent-189, score-0.36]
</p><p>65 o 625 new incomplete constituent in the same store element) from awaited transitions (model θS-T-W,d, which involve no completion):  PθS-T,d(std | rdt+1rdtsdstd−1)d=ef  ? [sent-191, score-0.429]
</p><p>66 Figure 6: A hypothesis in the phrase-based decoding lattice from Figure 1is expanded using translation option the board of source phrase den Vorstand. [sent-221, score-0.769]
</p><p>67 Syntactic language model state ˜τ 31 contains random variables s13. [sent-222, score-0.233]
</p><p>68 Figure 1illustrates an excerpt from a standard phrase-based translation lattice. [sent-264, score-0.298]
</p><p>69 Within each decoder stack t, each hypothesis h is augmented with a syntactic language model state ˜τ th . [sent-265, score-0.482]
</p><p>70 Each syntactic language model state is a random variable store, containing a slice of random variables from the HHMM. [sent-266, score-0.433]
</p><p>71 By maintaining these syntactic random variable stores, each hypothesis has access to the current  language model probability for the partial translation ending at that hypothesis, as calculated by an incremental syntactic language model defined by the HHMM. [sent-270, score-1.232]
</p><p>72 Specifically, the random variable store at hypothesis h provides P(˜ τth) = P(e1h. [sent-271, score-0.331]
</p><p>73 t is the sequence of words in a partial hypothesis ending at h which contains t target words, and where there are D syntactic random variables in each random variable store (Eq. [sent-279, score-0.706]
</p><p>74 In the simplest case, a new hypothesis extends an existing hypothesis by exactly one target word. [sent-283, score-0.271]
</p><p>75 As the new hypothesis is constructed by extending an existing stack element, the store and reduction state random variables are processed, along with the newly hypothesized word. [sent-284, score-0.679]
</p><p>76 This results in a new store of syntactic random variables (Eq. [sent-285, score-0.438]
</p><p>77 When a new hypothesis extends an existing hypothesis by more than one word, this process is first carried out for the first new word in the hypothe-  sis. [sent-287, score-0.212]
</p><p>78 Once the final word in the hypothesis has been processed, the resulting random variable store is associated with that hypothesis. [sent-289, score-0.331]
</p><p>79 Figure 6 illustrates this process, showing how a syntactic language model state ˜τ 51 in a phrase-based decoding lattice is obtained from a previous syntactic language model state ˜τ 31 (from Figure 1) by parsing the target language words from a phrasebased translation option. [sent-291, score-1.317]
</p><p>80 The HHMM outperforms the n-gram model in terms of out-of-domain test set perplexity when trained on the same WSJ data; the best perplexity results for in-domain and out-of-domain test sets4 are found by interpolating  4In-domain is  WSJ Section 23. [sent-308, score-0.201]
</p><p>81 152813 521 50179 435 3621345 17864 8239  Figure 8: Mean per-sentence decoding time (in seconds) for dev set using Moses with and without syntactic language model. [sent-312, score-0.274]
</p><p>82 HHMM parser beam sizes are indicated for the syntactic LM. [sent-313, score-0.283]
</p><p>83 We trained a phrase-based translation model on the full NIST Open MT08 Urdu-English translation model using the full training data. [sent-317, score-0.69]
</p><p>84 During tuning, Moses was first configured to use just the n-gram LM, then configured to use both the n-gram LM and the syntactic HHMM LM. [sent-319, score-0.227]
</p><p>85 In our integration with Moses, incorporating a syntactic language model dramatically slows the decoding process. [sent-321, score-0.321]
</p><p>86 7  Discussion  This paper argues that incremental syntactic languages models are a straightforward and appro628  Moses LM(s)BLEU  nH-HgrMamM o +n nly-gram1198. [sent-326, score-0.478]
</p><p>87 priate algorithmic fit for incorporating syntax into phrase-based statistical machine translation, since both process sentences in an incremental left-toright fashion. [sent-329, score-0.413]
</p><p>88 This means incremental syntactic LM scores can be calculated during the decoding process, rather than waiting until a complete sentence is posited, which is typically necessary in top-down or bottom-up parsing. [sent-330, score-0.599]
</p><p>89 We provided a rigorous formal definition of incremental syntactic languages models, and detailed what steps are necessary to incorporate such LMs into phrase-based decoding. [sent-331, score-0.515]
</p><p>90 We integrated an incremental syntactic language model into Moses. [sent-332, score-0.525]
</p><p>91 The translation quality significantly improved on a constrained task, and the perplexity improvements suggest that interpolating between n-gram and syntactic LMs may hold promise on larger data sets. [sent-333, score-0.567]
</p><p>92 The use of very large n-gram language models is typically a key ingredient in the best-performing machine translation systems (Brants et al. [sent-334, score-0.345]
</p><p>93 Our future work seeks to incorporate largescale n-gram language models in conjunction with incremental syntactic language models. [sent-337, score-0.478]
</p><p>94 The added decoding time cost of our syntactic language model is very high. [sent-338, score-0.321]
</p><p>95 By increasing the beam size and distortion limit of the baseline system, future work may examine whether a baseline system with comparable runtimes can achieve comparable translation quality. [sent-339, score-0.35]
</p><p>96 A more efficient implementation of the HHMM parser would speed decoding and make more extensive and conclusive translation experiments possible. [sent-340, score-0.497]
</p><p>97 Scalable inference and training of context-rich syntactic translation models. [sent-444, score-0.451]
</p><p>98 Example-based machine translation based on syntactic transfer with statistical models. [sent-480, score-0.539]
</p><p>99 Positive results for parsing with a bounded stack using a model-based right-corner trans631 form. [sent-582, score-0.221]
</p><p>100 A new string-to-dependency machine translation algorithm with a target dependency language model. [sent-586, score-0.404]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hhmm', 0.385), ('incremental', 0.325), ('translation', 0.298), ('store', 0.178), ('lattice', 0.168), ('ef', 0.157), ('syntactic', 0.153), ('rtd', 0.136), ('decoding', 0.121), ('schuler', 0.115), ('rdt', 0.113), ('hypothesis', 0.106), ('stack', 0.097), ('lms', 0.093), ('awaited', 0.091), ('lm', 0.081), ('moses', 0.081), ('incomplete', 0.081), ('pushdown', 0.08), ('std', 0.08), ('constituent', 0.079), ('state', 0.079), ('parser', 0.078), ('perplexity', 0.077), ('dt', 0.076), ('meets', 0.076), ('board', 0.076), ('parsing', 0.073), ('frtd', 0.068), ('huang', 0.068), ('parsers', 0.066), ('hypothesized', 0.064), ('deneefe', 0.064), ('hc', 0.06), ('variables', 0.06), ('ppl', 0.06), ('friday', 0.06), ('jc', 0.06), ('knight', 0.06), ('target', 0.059), ('chelba', 0.058), ('kevin', 0.057), ('galley', 0.057), ('partial', 0.056), ('meeting', 0.056), ('murphy', 0.055), ('cocke', 0.055), ('adjoining', 0.055), ('tree', 0.055), ('transition', 0.055), ('wsj', 0.054), ('jelinek', 0.053), ('beam', 0.052), ('frederick', 0.052), ('kenji', 0.052), ('bounded', 0.051), ('completed', 0.05), ('reduction', 0.048), ('shieber', 0.048), ('automaton', 0.047), ('random', 0.047), ('constituents', 0.047), ('machine', 0.047), ('model', 0.047), ('association', 0.047), ('annual', 0.046), ('hierarchic', 0.045), ('paskin', 0.045), ('slowdown', 0.045), ('tdtd', 0.045), ('lane', 0.045), ('mi', 0.045), ('president', 0.045), ('trees', 0.044), ('synchronous', 0.044), ('node', 0.043), ('grammars', 0.042), ('translations', 0.042), ('qun', 0.041), ('liu', 0.041), ('statistical', 0.041), ('devtest', 0.04), ('jr', 0.04), ('ades', 0.04), ('illustrates', 0.04), ('hierarchical', 0.039), ('haitao', 0.039), ('post', 0.039), ('constrained', 0.039), ('och', 0.038), ('baker', 0.037), ('daniel', 0.037), ('configured', 0.037), ('nesson', 0.037), ('formal', 0.037), ('air', 0.036), ('gildea', 0.036), ('cc', 0.036), ('koehn', 0.036), ('liang', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="171-tfidf-1" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>2 0.23530769 <a title="171-tfidf-2" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>3 0.19120491 <a title="171-tfidf-3" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>Author: Yang Liu ; Qun Liu ; Yajuan Lu</p><p>Abstract: We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST ChineseEnglish test sets.</p><p>4 0.18965062 <a title="171-tfidf-4" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>Author: Hao Zhang ; Licheng Fang ; Peng Xu ; Xiaoyun Wu</p><p>Abstract: Tree-to-string translation is syntax-aware and efficient but sensitive to parsing errors. Forestto-string translation approaches mitigate the risk of propagating parser errors into translation errors by considering a forest of alternative trees, as generated by a source language parser. We propose an alternative approach to generating forests that is based on combining sub-trees within the first best parse through binarization. Provably, our binarization forest can cover any non-consitituent phrases in a sentence but maintains the desirable property that for each span there is at most one nonterminal so that the grammar constant for decoding is relatively small. For the purpose of reducing search errors, we apply the synchronous binarization technique to forest-tostring decoding. Combining the two techniques, we show that using a fast shift-reduce parser we can achieve significant quality gains in NIST 2008 English-to-Chinese track (1.3 BLEU points over a phrase-based system, 0.8 BLEU points over a hierarchical phrase-based system). Consistent and significant gains are also shown in WMT 2010 in the English to German, French, Spanish and Czech tracks.</p><p>5 0.18920942 <a title="171-tfidf-5" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>Author: Ashish Vaswani ; Haitao Mi ; Liang Huang ; David Chiang</p><p>Abstract: Most statistical machine translation systems rely on composed rules (rules that can be formed out of smaller rules in the grammar). Though this practice improves translation by weakening independence assumptions in the translation model, it nevertheless results in huge, redundant grammars, making both training and decoding inefficient. Here, we take the opposite approach, where we only use minimal rules (those that cannot be formed out of other rules), and instead rely on a rule Markov model of the derivation history to capture dependencies between minimal rules. Large-scale experiments on a state-of-the-art tree-to-string translation system show that our approach leads to a slimmer model, a faster decoder, yet the same translation quality (measured using B ) as composed rules.</p><p>6 0.17253357 <a title="171-tfidf-6" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>7 0.17210566 <a title="171-tfidf-7" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>8 0.16713713 <a title="171-tfidf-8" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>9 0.16060266 <a title="171-tfidf-9" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>10 0.16057667 <a title="171-tfidf-10" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>11 0.15214317 <a title="171-tfidf-11" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>12 0.15034592 <a title="171-tfidf-12" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>13 0.14762053 <a title="171-tfidf-13" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>14 0.14656278 <a title="171-tfidf-14" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>15 0.13657475 <a title="171-tfidf-15" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>16 0.13615757 <a title="171-tfidf-16" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>17 0.13560978 <a title="171-tfidf-17" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>18 0.1315054 <a title="171-tfidf-18" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>19 0.13036165 <a title="171-tfidf-19" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>20 0.12684309 <a title="171-tfidf-20" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.317), (1, -0.265), (2, 0.111), (3, -0.064), (4, 0.045), (5, 0.011), (6, -0.113), (7, -0.001), (8, 0.006), (9, 0.016), (10, 0.015), (11, -0.037), (12, 0.011), (13, -0.127), (14, 0.041), (15, -0.001), (16, -0.052), (17, -0.004), (18, 0.012), (19, 0.005), (20, 0.059), (21, 0.04), (22, 0.072), (23, 0.011), (24, 0.023), (25, -0.056), (26, 0.05), (27, -0.004), (28, -0.038), (29, 0.024), (30, -0.022), (31, -0.074), (32, -0.001), (33, -0.007), (34, -0.009), (35, -0.033), (36, 0.005), (37, -0.063), (38, 0.028), (39, -0.043), (40, -0.012), (41, -0.042), (42, 0.064), (43, -0.047), (44, -0.073), (45, 0.012), (46, -0.081), (47, -0.09), (48, -0.027), (49, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96881676 <a title="171-lsi-1" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>2 0.86998254 <a title="171-lsi-2" href="./acl-2011-Syntax-based_Statistical_Machine_Translation_using_Tree_Automata_and_Tree_Transducers.html">290 acl-2011-Syntax-based Statistical Machine Translation using Tree Automata and Tree Transducers</a></p>
<p>Author: Daniel Emilio Beck</p><p>Abstract: In this paper I present a Master’s thesis proposal in syntax-based Statistical Machine Translation. Ipropose to build discriminative SMT models using both tree-to-string and tree-to-tree approaches. Translation and language models will be represented mainly through the use of Tree Automata and Tree Transducers. These formalisms have important representational properties that makes them well-suited for syntax modeling. Ialso present an experiment plan to evaluate these models through the use of a parallel corpus written in English and Brazilian Portuguese.</p><p>3 0.79329556 <a title="171-lsi-3" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>Author: Markos Mylonakis ; Khalil Sima'an</p><p>Abstract: While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by select- ing and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to +1.92 BLEU for Chinese as target.</p><p>4 0.79202598 <a title="171-lsi-4" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>Author: Taro Watanabe ; Eiichiro Sumita</p><p>Abstract: The state-of-the-art system combination method for machine translation (MT) is based on confusion networks constructed by aligning hypotheses with regard to word similarities. We introduce a novel system combination framework in which hypotheses are encoded as a confusion forest, a packed forest representing alternative trees. The forest is generated using syntactic consensus among parsed hypotheses: First, MT outputs are parsed. Second, a context free grammar is learned by extracting a set of rules that constitute the parse trees. Third, a packed forest is generated starting from the root symbol of the extracted grammar through non-terminal rewriting. The new hypothesis is produced by searching the best derivation in the forest. Experimental results on the WMT10 system combination shared task yield comparable performance to the conventional confusion network based method with smaller space.</p><p>5 0.77595711 <a title="171-lsi-5" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>Author: Bing Zhao ; Young-Suk Lee ; Xiaoqiang Luo ; Liu Li</p><p>Abstract: We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax-based translation models using synchronous context-free grammars. We transform the source tree phrasal structure into a set of simpler structures, expose such decisions to the decoding process, and find the least expensive transformation operation to better model word reordering. In particular, we integrate synchronous binarizations, verb regrouping, removal of redundant parse nodes, and incorporate a few important features such as translation boundaries. We learn the structural preferences from the data in a generative framework. The syntax-based translation system integrating the proposed techniques outperforms the best Arabic-English unconstrained system in NIST08 evaluations by 1.3 absolute BLEU, which is statistically significant.</p><p>6 0.75883442 <a title="171-lsi-6" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>7 0.75709182 <a title="171-lsi-7" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>8 0.74053264 <a title="171-lsi-8" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>9 0.73691255 <a title="171-lsi-9" href="./acl-2011-Rule_Markov_Models_for_Fast_Tree-to-String_Translation.html">268 acl-2011-Rule Markov Models for Fast Tree-to-String Translation</a></p>
<p>10 0.73610628 <a title="171-lsi-10" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>11 0.72730935 <a title="171-lsi-11" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>12 0.7212562 <a title="171-lsi-12" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>13 0.71813041 <a title="171-lsi-13" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>14 0.71639615 <a title="171-lsi-14" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>15 0.68469143 <a title="171-lsi-15" href="./acl-2011-A_Joint_Sequence_Translation_Model_with_Integrated_Reordering.html">16 acl-2011-A Joint Sequence Translation Model with Integrated Reordering</a></p>
<p>16 0.67445469 <a title="171-lsi-16" href="./acl-2011-Minimum_Bayes-risk_System_Combination.html">220 acl-2011-Minimum Bayes-risk System Combination</a></p>
<p>17 0.6740613 <a title="171-lsi-17" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>18 0.67320645 <a title="171-lsi-18" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>19 0.65965074 <a title="171-lsi-19" href="./acl-2011-Pre-_and_Postprocessing_for_Statistical_Machine_Translation_into_Germanic_Languages.html">247 acl-2011-Pre- and Postprocessing for Statistical Machine Translation into Germanic Languages</a></p>
<p>20 0.65670723 <a title="171-lsi-20" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.027), (17, 0.103), (26, 0.016), (37, 0.087), (39, 0.074), (41, 0.061), (43, 0.146), (53, 0.014), (55, 0.042), (59, 0.056), (72, 0.042), (91, 0.054), (96, 0.19), (98, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87070072 <a title="171-lda-1" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>2 0.85077083 <a title="171-lda-2" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>Author: Yashar Mehdad ; Matteo Negri ; Marcello Federico</p><p>Abstract: This paper explores the use of bilingual parallel corpora as a source of lexical knowledge for cross-lingual textual entailment. We claim that, in spite of the inherent difficulties of the task, phrase tables extracted from parallel data allow to capture both lexical relations between single words, and contextual information useful for inference. We experiment with a phrasal matching method in order to: i) build a system portable across languages, and ii) evaluate the contribution of lexical knowledge in isolation, without interaction with other inference mechanisms. Results achieved on an English-Spanish corpus obtained from the RTE3 dataset support our claim, with an overall accuracy above average scores reported by RTE participants on monolingual data. Finally, we show that using parallel corpora to extract paraphrase tables reveals their potential also in the monolingual setting, improving the results achieved with other sources of lexical knowledge.</p><p>3 0.84792686 <a title="171-lda-3" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>Author: Raphael Hoffmann ; Congle Zhang ; Xiao Ling ; Luke Zettlemoyer ; Daniel S. Weld</p><p>Abstract: Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded ( Jobs Apple ) and CEO-o f ( Jobs Apple ) . , , This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extrac- , tion model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</p><p>4 0.8471843 <a title="171-lda-4" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>Author: Zhongguo Li</p><p>Abstract: Lots of Chinese characters are very productive in that they can form many structured words either as prefixes or as suffixes. Previous research in Chinese word segmentation mainly focused on identifying only the word boundaries without considering the rich internal structures of many words. In this paper we argue that this is unsatisfying in many ways, both practically and theoretically. Instead, we propose that word structures should be recovered in morphological analysis. An elegant approach for doing this is given and the result is shown to be promising enough for encouraging further effort in this direction. Our probability model is trained with the Penn Chinese Treebank and actually is able to parse both word and phrase structures in a unified way. 1 Why Parse Word Structures? Research in Chinese word segmentation has progressed tremendously in recent years, with state of the art performing at around 97% in precision and recall (Xue, 2003; Gao et al., 2005; Zhang and Clark, 2007; Li and Sun, 2009). However, virtually all these systems focus exclusively on recognizing the word boundaries, giving no consideration to the internal structures of many words. Though it has been the standard practice for many years, we argue that this paradigm is inadequate both in theory and in practice, for at least the following four reasons. The first reason is that if we confine our definition of word segmentation to the identification of word boundaries, then people tend to have divergent 1405 opinions as to whether a linguistic unit is a word or not (Sproat et al., 1996). This has led to many different annotation standards for Chinese word segmentation. Even worse, this could cause inconsistency in the same corpus. For instance, 䉂 擌 奒 ‘vice president’ is considered to be one word in the Penn Chinese Treebank (Xue et al., 2005), but is split into two words by the Peking University corpus in the SIGHAN Bakeoffs (Sproat and Emerson, 2003). Meanwhile, 䉂 䀓 惼 ‘vice director’ and 䉂 䚲䡮 ‘deputy are both segmented into two words in the same Penn Chinese Treebank. In fact, all these words are composed of the prefix 䉂 ‘vice’ and a root word. Thus the structure of 䉂擌奒 ‘vice president’ can be represented with the tree in Figure 1. Without a doubt, there is complete agree- manager’ NN ,,ll JJf NNf 䉂 擌奒 Figure 1: Example of a word with internal structure. ment on the correctness of this structure among native Chinese speakers. So if instead of annotating only word boundaries, we annotate the structures of every word, then the annotation tends to be more 1 1Here it is necessary to add a note on terminology used in this paper. Since there is no universally accepted definition of the “word” concept in linguistics and especially in Chinese, whenever we use the term “word” we might mean a linguistic unit such as 䉂 擌奒 ‘vice president’ whose structure is shown as the tree in Figure 1, or we might mean a smaller unit such as 擌奒 ‘president’ which is a substructure of that tree. Hopefully, ProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. A ?c s 2o0ci1a1ti Aonss foocria Ctioomnp fourta Ctioomnaplu Ltaintigouniaslti Lcisn,g puaigsetsic 1s405–1414, consistent and there could be less duplication of efforts in developing the expensive annotated corpus. The second reason is applications have different requirements for granularity of words. Take the personal name 撱 嗤吼 ‘Zhou Shuren’ as an example. It’s considered to be one word in the Penn Chinese Treebank, but is segmented into a surname and a given name in the Peking University corpus. For some applications such as information extraction, the former segmentation is adequate, while for others like machine translation, the later finer-grained output is more preferable. If the analyzer can produce a structure as shown in Figure 4(a), then every application can extract what it needs from this tree. A solution with tree output like this is more elegant than approaches which try to meet the needs of different applications in post-processing (Gao et al., 2004). The third reason is that traditional word segmentation has problems in handling many phenomena in Chinese. For example, the telescopic compound 㦌 撥 怂惆 ‘universities, middle schools and primary schools’ is in fact composed ofthree coordinating elements 㦌惆 ‘university’, 撥 惆 ‘middle school’ and 怂惆 ‘primary school’ . Regarding it as one flat word loses this important information. Another example is separable words like 扩 扙 ‘swim’ . With a linear segmentation, the meaning of ‘swimming’ as in 扩 堑 扙 ‘after swimming’ cannot be properly represented, since 扩扙 ‘swim’ will be segmented into discontinuous units. These language usages lie at the boundary between syntax and morphology, and are not uncommon in Chinese. They can be adequately represented with trees (Figure 2). (a) NN (b) ???HHH JJ NNf ???HHH JJf JJf JJf 㦌 撥 怂 惆 VV ???HHH VV NNf ZZ VVf VVf 扩 扙 堑 Figure 2: Example of telescopic compound (a) and separable word (b). The last reason why we should care about word the context will always make it clear what is being referred to with the term “word”. 1406 structures is related to head driven statistical parsers (Collins, 2003). To illustrate this, note that in the Penn Chinese Treebank, the word 戽 䊂䠽 吼 ‘English People’ does not occur at all. Hence constituents headed by such words could cause some difficulty for head driven models in which out-ofvocabulary words need to be treated specially both when they are generated and when they are conditioned upon. But this word is in turn headed by its suffix 吼 ‘people’, and there are 2,233 such words in Penn Chinese Treebank. If we annotate the structure of every compound containing this suffix (e.g. Figure 3), such data sparsity simply goes away.</p><p>5 0.84463418 <a title="171-lda-5" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>Author: Joel Lang ; Mirella Lapata</p><p>Abstract: In this paper we describe an unsupervised method for semantic role induction which holds promise for relieving the data acquisition bottleneck associated with supervised role labelers. We present an algorithm that iteratively splits and merges clusters representing semantic roles, thereby leading from an initial clustering to a final clustering of better quality. The method is simple, surprisingly effective, and allows to integrate linguistic knowledge transparently. By combining role induction with a rule-based component for argument identification we obtain an unsupervised end-to-end semantic role labeling system. Evaluation on the CoNLL 2008 benchmark dataset demonstrates that our method outperforms competitive unsupervised approaches by a wide margin.</p><p>6 0.843494 <a title="171-lda-6" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>7 0.84344304 <a title="171-lda-7" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>8 0.84016645 <a title="171-lda-8" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>9 0.83962476 <a title="171-lda-9" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>10 0.83879894 <a title="171-lda-10" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>11 0.83879685 <a title="171-lda-11" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>12 0.83867478 <a title="171-lda-12" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>13 0.83853173 <a title="171-lda-13" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>14 0.83828163 <a title="171-lda-14" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>15 0.83809483 <a title="171-lda-15" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>16 0.8379041 <a title="171-lda-16" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>17 0.83773577 <a title="171-lda-17" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>18 0.83735275 <a title="171-lda-18" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>19 0.83646643 <a title="171-lda-19" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>20 0.8344 <a title="171-lda-20" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
