<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-56" href="#">acl2011-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</h1>
<br/><p>Source: <a title="acl-2011-56-pdf" href="http://aclweb.org/anthology//P/P11/P11-1025.pdf">pdf</a></p><p>Author: Sujith Ravi ; Kevin Knight</p><p>Abstract: We introduce a novel Bayesian approach for deciphering complex substitution ciphers. Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. Bayesian inference is performed on our model using an efficient sampling technique. We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before.</p><p>Reference: <a title="acl-2011-56-reference" href="../acl2011_reference/acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. [sent-3, score-0.692]
</p><p>2 We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. [sent-5, score-1.384]
</p><p>3 The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before. [sent-6, score-1.057]
</p><p>4 These ciphers replace (English) plaintext letters with cipher symbols in order to generate the ciphertext sequence. [sent-8, score-1.503]
</p><p>5 There exist many published works on automatic decipherment methods for solving simple lettersubstitution ciphers. [sent-9, score-0.529]
</p><p>6 Many existing methods use dictionary-based attacks employing huge word dictionaries to find plaintext patterns within the ciphertext (Peleg and Rosenfeld, 1979; Ganesan and Sherman, 1993; Jakobsen, 1995; Olson, 2007). [sent-10, score-0.643]
</p><p>7 Ravi and Knight (2008) formulate decipherment as an integer programming problem and provide an exact method to solve simple substitution ciphers by using letter n-gram mod-  els along with deterministic key constraints. [sent-16, score-1.269]
</p><p>8 Corlett and Penn (2010) work with large ciphertexts containing thousands of characters and provide another exact decipherment method using an A* search algorithm. [sent-17, score-0.519]
</p><p>9 The famous Zodiac serial killer used one such cipher system for communication. [sent-21, score-0.599]
</p><p>10 In 1969, the killer sent a three-part cipher message to newspapers claiming credit for recent shootings and crimes committed near the San Francisco area. [sent-22, score-0.623]
</p><p>11 Oranchak (2008) presents a method for solving the Zodiac-408 cipher automatically with a dictionary-based attack using a genetic algorithm. [sent-24, score-0.591]
</p><p>12 However, his method relies on using plaintext words from the known solution to solve the cipher, which  departs from a strict decipherment scenario. [sent-25, score-0.945]
</p><p>13 Ac s2s0o1ci1a Atiosnso fcoirat Cioonm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 239–247, solving substitution ciphers using Bayesian learning. [sent-28, score-0.528]
</p><p>14 Our novel contributions are as follows: •  •  •  •  We present a new probabilistic decipherment approach using Bayesian ibnifliesrtiecnc dee cwipithhe sparse priors, which can be used to solve different types of substitution ciphers. [sent-29, score-0.711]
</p><p>15 Our new method combines information from Owourrd n edwict mioentharoieds along swi itnhf lremttaerti n-gram models, providing a robust decipherment model which offsets the disadvantages faced by previous approaches. [sent-30, score-0.515]
</p><p>16 We evaluate the Bayesian decipherment output on t hevreaelu daitfefe threen Bt types onf d seucbipsthieturtmioenn ciphers and show that unlike a previous approach, our new method solves all the ciphers completely. [sent-31, score-1.17]
</p><p>17 In a letter substitution cipher, every letter p in the natural language (plaintext) sequence is replaced by a cipher token c, according to some substitution key. [sent-34, score-1.26]
</p><p>18 The sender can encrypt the message using one of many different cipher systems. [sent-43, score-0.618]
</p><p>19 The particular type of cipher system  chosen determines the properties of the key. [sent-44, score-0.533]
</p><p>20 For example, the substitution key can be deterministic in 240 both the encipherment and decipherment directions as shown in the above example—i. [sent-45, score-0.775]
</p><p>21 , there is a 1-to1 correspondence between the plaintext letters and ciphertext symbols. [sent-47, score-0.643]
</p><p>22 1 Simple Substitution Ciphers The key used in a simple substitution cipher is deterministic in both the encipherment and decipherment directions, i. [sent-50, score-1.305]
</p><p>23 , there is a 1-to-1 mapping between plaintext letters and ciphertext symbols. [sent-52, score-0.643]
</p><p>24 The example shown earlier depicts how a simple substitution cipher works. [sent-53, score-0.711]
</p><p>25 We encrypt an original English plaintext message using a randomly generated simple substitution key to create the ciphertext. [sent-55, score-0.76]
</p><p>26 , plaintext character “ ” maps to ciphertext character “ ”. [sent-58, score-0.659]
</p><p>27 Figure 1 (top) shows a portion of the ciphertext along with the original plaintext used to create the cipher. [sent-59, score-0.621]
</p><p>28 2 Homophonic Ciphers A homophonic cipher uses a substitution key that maps a plaintext letter to more than one cipher symbol. [sent-61, score-2.039]
</p><p>29 These ciphers are more complex than simple substitution ciphers. [sent-79, score-0.515]
</p><p>30 The number of potential cipher symbol substitutes for a particular plaintext letter is often proportional to the frequency of that letter in the plaintext language— for example, the English letter “E” is assigned more cipher symbols than “Z”. [sent-81, score-2.457]
</p><p>31 The substitution key is, however, deterministic in the decipherment direction—each ciphertext symbol maps to a single plaintext letter. [sent-83, score-1.374]
</p><p>32 Data: For our decipherment experiments on homophonic ciphers, we use the same  414-letter English plaintext used in Section 2. [sent-85, score-1.092]
</p><p>33 We encrypt this message using a homophonic substitution key (available from http://www. [sent-87, score-0.492]
</p><p>34 Figure 1 (middle) displays a section of the homophonic cipher (with spaces) and the original plaintext message used in our experiments. [sent-92, score-1.182]
</p><p>35 3  Homophonic Ciphers without spaces (Zodiac-408 cipher) In the previous two cipher systems, the wordboundary information was preserved in the cipher. [sent-94, score-0.567]
</p><p>36 We now consider a more difficult homophonic cipher by removing space characters from the original plaintext. [sent-95, score-0.724]
</p><p>37 ” Without the word boundary information, typical dictionary-based decipherment attacks fail on such  241 ciphers. [sent-102, score-0.522]
</p><p>38 One of the most famous homophonic ciphers in history was used by the infamous Zodiac serial killer in the 1960’s. [sent-104, score-0.581]
</p><p>39 The Zodiac messages include two interesting ciphers: (1) a 408-symbol homophonic cipher without spaces (which was solved manually by hand), and (2) a similar looking 340-symbol cipher that has yet to be solved. [sent-108, score-1.263]
</p><p>40 Here is a sample of the Zodiac-408 cipher message:  . [sent-109, score-0.541]
</p><p>41 Besides the difficulty with missing word boundaries and non-determinism associated with the key, the Zodiac-408 cipher poses several additional challenges  which makes  standard  homophonic  it harder to solve than any cipher. [sent-115, score-0.716]
</p><p>42 Also, the last 18 characters of the plaintext message does not seem to make any sense (“EBEORIETEMETHHPITI”). [sent-117, score-0.49]
</p><p>43 Data: Figure 1 (bottom) displays the Zodiac-408 cipher (consisting of 408 tokens, 54 symbol types) along with the original plaintext message. [sent-118, score-0.978]
</p><p>44 We run the new decipherment method (described in Section 3. [sent-119, score-0.515]
</p><p>45 cn, the goal of decipherment is to uncover the hidden plaintext message p1. [sent-124, score-0.966]
</p><p>46 , number of possible key mappings) that we have to navigate during decipherment is huge—a simple substitution cipher has a keyspace size of 26! [sent-130, score-1.255]
</p><p>47 , whereas a homophonic cipher such as the Zodiac-408 cipher has 2654 possible key mappings. [sent-131, score-1.247]
</p><p>48 Next, we describe a new Bayesian decipherment approach for tackling substitution ciphers. [sent-132, score-0.685]
</p><p>49 (2010) proposed a Bayesian approach in an archaeological decipherment scenario. [sent-139, score-0.492]
</p><p>50 For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. [sent-142, score-1.57]
</p><p>51 Here, we propose a novel approach for deciphering substitution ciphers using Bayesian inference. [sent-145, score-0.537]
</p><p>52 for a simple substitution cipher), our Bayesian framework requires us to sample only a small number of keys during the decipherment process. [sent-147, score-0.731]
</p><p>53 242 Probabilistic Decipherment: Our decipherment method follows a noisy-channel approach. [sent-148, score-0.503]
</p><p>54 Substitute each plaintext letter pi with a ciphertext token ci, with probability P(ci |pi) in order to generate the ciphertext sequence c = c1. [sent-164, score-1.096]
</p><p>55 We build a statistical English language model (LM) for the plaintext source model P(p), which assigns a probability to any English letter sequence. [sent-168, score-0.611]
</p><p>56 In our decipherment framework, a Chinese Restaurant Process formulation is used to model both the source and channel. [sent-170, score-0.492]
</p><p>57 Substitute p1 with cipher token bility P0 (c1|p1) +  4. [sent-174, score-0.551]
</p><p>58 Generate English plaintext letter ability  pi,  with prob-  α ·  P0(pi|pi−1) + Ci1−1(pi−1,pi)  α + Ci1−1(pi−1)  BayesiCnPplhaoienurt oxnt:WrDi qnRE IgCc fTI PfTm npEH wNEnqRcIMs Nw nE NwoA Tf wN gCcI vS nEwTfN pHT nEkL oA wN oAaG zULk AtYoG vSa EIcnS v WOhr FuHpEnDi Rq Oh En gCTfUzHMspHEn EwN. [sent-176, score-0.599]
</p><p>59 5286731  decipherment (using word+3-gram LM) for three different ciphers: (a) Simple Substitution Cipher (top), (b) Homophonic Substitution Cipher with spaces (middle), and (c) Zodiac-408 Cipher (bottom). [sent-181, score-0.541]
</p><p>60 Substitute bility  pi  with cipher token  ci,  with proba-  β · P0(ci|pi) + C1i−1(pi, ci) β + Ci1−1(pi) 7. [sent-183, score-0.636]
</p><p>61 , any plaintext hypothesis corresponding to the given ciphertext sequence. [sent-187, score-0.62]
</p><p>62 , a plaintext letter can be substituted with any given cipher type with equal probability). [sent-191, score-1.143]
</p><p>63 C1i−1 represents the count of events occurring before plaintext letter pi in the derivation (we call this the “cache”). [sent-192, score-0.715]
</p><p>64 We could follow a point-wise sampling strategy, where we sample plaintext letter choices for every cipher token, one at a time. [sent-196, score-1.242]
</p><p>65 But we already know that the substitution ciphers described here exhibit determinism in the deciphering direction,1 i. [sent-197, score-0.55]
</p><p>66 , although we have no idea about the key mappings themselves, we do know that there exists only a single plaintext letter mapping for every cipher symbol type in the true key. [sent-199, score-1.191]
</p><p>67 So sampling plaintext choices for every cipher token separately is not an efficient strategy— our sampler may spend too much time exploring invalid keys (which map the same cipher symbol to different plaintext letters). [sent-200, score-2.073]
</p><p>68 Under 1This assumption does not strictly apply to the Zodiac-408 cipher where a few cipher symbols exhibit non-determinism in the decipherment direction as well. [sent-203, score-1.561]
</p><p>69 244 this scheme, we sample plaintext letter choices for each cipher symbol type. [sent-204, score-1.189]
</p><p>70 In every step, we sample a new plaintext letter for a cipher type and update the entire plaintext hypothesis (i. [sent-205, score-1.588]
</p><p>71 , plaintext letters at all corresponding positions) to reflect this change. [sent-207, score-0.456]
</p><p>72 For example, if we sample a new choice pnew for a cipher symbol which occurs at positions 4, 10, 18, then we update plaintext letters p4, p10 and p18 with the new choice pnew. [sent-208, score-1.023]
</p><p>73 Combining letter n-gram language models with word dictionaries: Many existing probabilistic ap-  proaches use statistical letter n-gram language models of English to assign P(p) probabilities to plaintext hypotheses during decipherment. [sent-211, score-0.776]
</p><p>74 Unlike previous approaches, our decipherment method combines information from both sources— letter n-grams and word dictionaries. [sent-213, score-0.692]
</p><p>75 Sampling for ciphers without spaces: For ciphers without spaces, dictionaries are hard to use because we do not know where words start and end. [sent-227, score-0.662]
</p><p>76 We introduce a new sampling operator which counters this problem and allows us to perform inference using the same decipherment model described earlier. [sent-228, score-0.594]
</p><p>77 In a first sampling pass, we sample from 26 plaintext letter choices (e. [sent-229, score-0.724]
</p><p>78 The new strategy allows us to apply Bayesian decipherment even to ciphers without spaces. [sent-251, score-0.814]
</p><p>79 As a result, we now have a new decipherment method that consistently works for a range of different types of substitution ciphers. [sent-252, score-0.684]
</p><p>80 ””  Decoding the ciphertext: After the sampling run has we choose the final sample as our English plaintext decipherment output. [sent-253, score-1.028]
</p><p>81 finished,4  4For letter substitution decipherment we want to keep the language model probabilities fixed during training, and hence we set the prior on that model to be high (α = 104). [sent-254, score-0.864]
</p><p>82 We instantiate a key which matches frequently occurring plaintext letters to frequent cipher symbols and use this to generate an initial sample for the given ciphertext and run the sampler for 5000 iterations. [sent-257, score-1.286]
</p><p>83 245 4  Experiments and Results  We run decipherment experiments on different types of letter substitution ciphers (described in Section 2). [sent-259, score-1.184]
</p><p>84 In particular, we work with the following three ciphers: (a) 414-letter Simple Substitution Cipher (b) 414-letter Homophonic Cipher (with spaces) (c) Zodiac-408 Cipher Methods: For each cipher, we run and compare the output from two different decipherment approaches: 1. [sent-260, score-0.504]
</p><p>85 They use the EM algorithm to estimate the channel parameters θ during decipherment training. [sent-263, score-0.515]
</p><p>86 The given ciphertext c is then decoded by using the Viterbi algorithm to choose the plaintext decoding p that maximizes P(p) · Pθ(c|p)3, stretching itnhge pch tahnatn eml probabilities. [sent-264, score-0.629]
</p><p>87 Evaluation: We evaluate the quality of a particular decipherment as the percentage of cipher tokens that are decoded correctly. [sent-268, score-1.03]
</p><p>88 Results: Figure 2 compares the decipherment performance for the EM method with Bayesian decipherment (using type sampling and sparse priors) on three different types of substitution ciphers. [sent-269, score-1.288]
</p><p>89 Even with a 3-gram letter LM, our method yields a +63% improvement in decipherment accuracy over EM on the homophonic cipher with spaces. [sent-271, score-1.376]
</p><p>90 Figure 1 shows samples from the Bayesian decipherment output for all three ciphers. [sent-273, score-0.492]
</p><p>91 For ciphers without spaces, our method automatically guesses the word boundaries for the plaintext hypothesis. [sent-274, score-0.755]
</p><p>92 For the Zodiac-408 cipher, we compare the performance achieved by Bayesian decipherment under different settings: •  •  •  Letter n-gram versus Word+n-gram LMs— Figure 2n sghraowms tvhearts using a word+3-gram Ls—M instead of a 3-gram LM results in +75% improvement in decipherment accuracy. [sent-281, score-1.002]
</p><p>93 0) helps for such problems and produces better decipherment results (97. [sent-284, score-0.492]
</p><p>94 Type versus Point-wise sampling—Unlike point-wise sampling, type sampling quickly converges to better decipherment solutions. [sent-287, score-0.604]
</p><p>95 After 5000 sampling passes over the entire  data, decipherment output from type sampling scores 97. [sent-288, score-0.665]
</p><p>96 5  Conclusion  In this work, we presented a novel Bayesian decipherment approach that can effectively solve a va5Both sampling runs were seeded with the same random initial sample. [sent-294, score-0.591]
</p><p>97 Unlike previous approaches, our method combines information from letter n-gram language models and word dictionaries and provides a robust decipherment model. [sent-296, score-0.71]
</p><p>98 Using Bayesian decipherment, we can successfully solve the Zodiac-408 cipher—the first time this is achieved by a fully automatic method in a strict decipherment scenario. [sent-298, score-0.523]
</p><p>99 For future work, there are other interesting decipherment tasks where our method can be applied. [sent-299, score-0.503]
</p><p>100 Evolutionary algorithm for decryption of monoalphabetic homophonic substitution  ciphers encoded as constraint satisfaction problems. [sent-362, score-0.7]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cipher', 0.518), ('decipherment', 0.492), ('plaintext', 0.422), ('ciphers', 0.322), ('ciphertext', 0.187), ('substitution', 0.181), ('homophonic', 0.178), ('letter', 0.177), ('pi', 0.085), ('bayesian', 0.083), ('sampling', 0.079), ('message', 0.052), ('spaces', 0.049), ('encipherment', 0.048), ('encrypt', 0.048), ('zodiac', 0.048), ('killer', 0.042), ('ravi', 0.038), ('lm', 0.035), ('letters', 0.034), ('deciphering', 0.034), ('key', 0.033), ('cryptanalysis', 0.029), ('cryptologia', 0.029), ('sampler', 0.026), ('symbol', 0.026), ('solving', 0.025), ('attack', 0.025), ('sujith', 0.025), ('famous', 0.025), ('channel', 0.023), ('keys', 0.023), ('sample', 0.023), ('choices', 0.023), ('em', 0.022), ('knight', 0.022), ('geman', 0.022), ('ci', 0.022), ('deterministic', 0.021), ('solve', 0.02), ('derivation', 0.02), ('symbols', 0.02), ('decoded', 0.02), ('character', 0.019), ('corlett', 0.019), ('decryption', 0.019), ('enciphering', 0.019), ('ganesan', 0.019), ('keyspace', 0.019), ('maxxpp', 0.019), ('peleg', 0.019), ('dictionaries', 0.018), ('english', 0.018), ('sparse', 0.018), ('versus', 0.018), ('priors', 0.017), ('lms', 0.017), ('bility', 0.017), ('enciphered', 0.017), ('gibbs', 0.017), ('characters', 0.016), ('kevin', 0.016), ('token', 0.016), ('attacking', 0.016), ('attacks', 0.016), ('evolutionary', 0.015), ('type', 0.015), ('boundary', 0.014), ('serial', 0.014), ('exchangeability', 0.014), ('prior', 0.014), ('exhibit', 0.013), ('tackling', 0.012), ('genetic', 0.012), ('solves', 0.012), ('simple', 0.012), ('probability', 0.012), ('maps', 0.012), ('inference', 0.012), ('run', 0.012), ('cache', 0.012), ('substitute', 0.012), ('combines', 0.012), ('donald', 0.012), ('snyder', 0.012), ('original', 0.012), ('newspapers', 0.011), ('operator', 0.011), ('unlike', 0.011), ('alphabet', 0.011), ('arg', 0.011), ('method', 0.011), ('monte', 0.011), ('substituted', 0.011), ('hypothesis', 0.011), ('occurring', 0.011), ('carlo', 0.01), ('preserve', 0.01), ('dempster', 0.01), ('sequence', 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="56-tfidf-1" href="./acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers.html">56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</a></p>
<p>Author: Sujith Ravi ; Kevin Knight</p><p>Abstract: We introduce a novel Bayesian approach for deciphering complex substitution ciphers. Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. Bayesian inference is performed on our model using an efficient sampling technique. We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before.</p><p>2 0.7366907 <a title="56-tfidf-2" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>Author: Sujith Ravi ; Kevin Knight</p><p>Abstract: In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from nonparallel text.</p><p>3 0.099235132 <a title="56-tfidf-3" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>4 0.076816849 <a title="56-tfidf-4" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>5 0.046827152 <a title="56-tfidf-5" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>6 0.039111108 <a title="56-tfidf-6" href="./acl-2011-Adjoining_Tree-to-String_Translation.html">30 acl-2011-Adjoining Tree-to-String Translation</a></p>
<p>7 0.035104055 <a title="56-tfidf-7" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>8 0.032691404 <a title="56-tfidf-8" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>9 0.032154657 <a title="56-tfidf-9" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>10 0.031757355 <a title="56-tfidf-10" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>11 0.031009471 <a title="56-tfidf-11" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<p>12 0.029909324 <a title="56-tfidf-12" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>13 0.028896905 <a title="56-tfidf-13" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>14 0.028332276 <a title="56-tfidf-14" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>15 0.028248595 <a title="56-tfidf-15" href="./acl-2011-Latent_Class_Transliteration_based_on_Source_Language_Origin.html">197 acl-2011-Latent Class Transliteration based on Source Language Origin</a></p>
<p>16 0.027793545 <a title="56-tfidf-16" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>17 0.027706729 <a title="56-tfidf-17" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>18 0.027679248 <a title="56-tfidf-18" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>19 0.026487533 <a title="56-tfidf-19" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>20 0.02431659 <a title="56-tfidf-20" href="./acl-2011-Nonparametric_Bayesian_Machine_Transliteration_with_Synchronous_Adaptor_Grammars.html">232 acl-2011-Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.073), (1, -0.039), (2, 0.021), (3, 0.036), (4, 0.004), (5, -0.007), (6, 0.022), (7, -0.002), (8, -0.017), (9, 0.072), (10, -0.008), (11, 0.033), (12, 0.087), (13, 0.155), (14, -0.028), (15, 0.05), (16, -0.006), (17, 0.128), (18, 0.127), (19, -0.094), (20, 0.122), (21, 0.38), (22, 0.391), (23, 0.113), (24, 0.474), (25, 0.161), (26, 0.012), (27, -0.171), (28, 0.079), (29, 0.043), (30, -0.051), (31, 0.048), (32, 0.099), (33, 0.029), (34, -0.067), (35, 0.043), (36, -0.126), (37, 0.017), (38, 0.029), (39, 0.113), (40, 0.052), (41, -0.016), (42, -0.016), (43, 0.007), (44, 0.075), (45, -0.014), (46, 0.062), (47, -0.019), (48, -0.054), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97701359 <a title="56-lsi-1" href="./acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers.html">56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</a></p>
<p>Author: Sujith Ravi ; Kevin Knight</p><p>Abstract: We introduce a novel Bayesian approach for deciphering complex substitution ciphers. Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. Bayesian inference is performed on our model using an efficient sampling technique. We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before.</p><p>2 0.78821039 <a title="56-lsi-2" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>Author: Sujith Ravi ; Kevin Knight</p><p>Abstract: In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from nonparallel text.</p><p>3 0.25893068 <a title="56-lsi-3" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>4 0.24960034 <a title="56-lsi-4" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>5 0.23549761 <a title="56-lsi-5" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>6 0.1911965 <a title="56-lsi-6" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>7 0.18769141 <a title="56-lsi-7" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>8 0.18314373 <a title="56-lsi-8" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>9 0.16887026 <a title="56-lsi-9" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>10 0.16733773 <a title="56-lsi-10" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>11 0.16045284 <a title="56-lsi-11" href="./acl-2011-Temporal_Evaluation.html">294 acl-2011-Temporal Evaluation</a></p>
<p>12 0.15463969 <a title="56-lsi-12" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>13 0.15089932 <a title="56-lsi-13" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>14 0.14337642 <a title="56-lsi-14" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>15 0.14129007 <a title="56-lsi-15" href="./acl-2011-Nonparametric_Bayesian_Machine_Transliteration_with_Synchronous_Adaptor_Grammars.html">232 acl-2011-Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars</a></p>
<p>16 0.13968547 <a title="56-lsi-16" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>17 0.13850446 <a title="56-lsi-17" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>18 0.13236353 <a title="56-lsi-18" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<p>19 0.13127494 <a title="56-lsi-19" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>20 0.12908989 <a title="56-lsi-20" href="./acl-2011-Exploiting_Readymades_in_Linguistic_Creativity%3A_A_System_Demonstration_of_the_Jigsaw_Bard.html">125 acl-2011-Exploiting Readymades in Linguistic Creativity: A System Demonstration of the Jigsaw Bard</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.02), (17, 0.048), (26, 0.012), (36, 0.011), (37, 0.048), (39, 0.025), (41, 0.523), (55, 0.017), (59, 0.032), (72, 0.019), (91, 0.027), (96, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94441336 <a title="56-lda-1" href="./acl-2011-Contrasting_Multi-Lingual_Prosodic_Cues_to_Predict_Verbal_Feedback_for_Rapport.html">83 acl-2011-Contrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback for Rapport</a></p>
<p>Author: Siwei Wang ; Gina-Anne Levow</p><p>Abstract: Verbal feedback is an important information source in establishing interactional rapport. However, predicting verbal feedback across languages is challenging due to languagespecific differences, inter-speaker variation, and the relative sparseness and optionality of verbal feedback. In this paper, we employ an approach combining classifier weighting and SMOTE algorithm oversampling to improve verbal feedback prediction in Arabic, English, and Spanish dyadic conversations. This approach improves the prediction of verbal feedback, up to 6-fold, while maintaining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration.</p><p>2 0.93913424 <a title="56-lda-2" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>Author: Yu Hong ; Jianfeng Zhang ; Bin Ma ; Jianmin Yao ; Guodong Zhou ; Qiaoming Zhu</p><p>Abstract: Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entitytype consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction. 1</p><p>3 0.91663438 <a title="56-lda-3" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>Author: Antske Fokkens</p><p>Abstract: When designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. Moreover, because analyses interact, the choices made by the engineer influence the possibilities available in further grammar development. The order in which phenomena are treated may therefore have a major impact on the resulting grammar. This paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. Iargue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. The idea is illustrated through a comparative study of auxiliary structures in HPSG-based grammars for German and Dutch. Auxiliaries form a central phenomenon of German and Dutch and are likely to influence many components of the grammar. This study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation.</p><p>4 0.90438688 <a title="56-lda-4" href="./acl-2011-K-means_Clustering_with_Feature_Hashing.html">189 acl-2011-K-means Clustering with Feature Hashing</a></p>
<p>Author: Hajime Senuma</p><p>Abstract: One of the major problems of K-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task.</p><p>5 0.9001075 <a title="56-lda-5" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>Author: Jagadeesh Jagarlamudi ; Hal Daume III ; Raghavendra Udupa</p><p>Abstract: Mapping documents into an interlingual representation can help bridge the language barrier of a cross-lingual corpus. Previous approaches use aligned documents as training data to learn an interlingual representation, making them sensitive to the domain of the training data. In this paper, we learn an interlingual representation in an unsupervised manner using only a bilingual dictionary. We first use the bilingual dictionary to find candidate document alignments and then use them to find an interlingual representation. Since the candidate alignments are noisy, we de- velop a robust learning algorithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain.</p><p>6 0.89806694 <a title="56-lda-6" href="./acl-2011-Nonparametric_Bayesian_Machine_Transliteration_with_Synchronous_Adaptor_Grammars.html">232 acl-2011-Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars</a></p>
<p>7 0.89625931 <a title="56-lda-7" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>same-paper 8 0.88945293 <a title="56-lda-8" href="./acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers.html">56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</a></p>
<p>9 0.81004125 <a title="56-lda-9" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>10 0.61028087 <a title="56-lda-10" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>11 0.60826385 <a title="56-lda-11" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>12 0.59094644 <a title="56-lda-12" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>13 0.58916551 <a title="56-lda-13" href="./acl-2011-Modeling_Wisdom_of_Crowds_Using_Latent_Mixture_of_Discriminative_Experts.html">223 acl-2011-Modeling Wisdom of Crowds Using Latent Mixture of Discriminative Experts</a></p>
<p>14 0.5763973 <a title="56-lda-14" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>15 0.56529218 <a title="56-lda-15" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>16 0.55573314 <a title="56-lda-16" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>17 0.55560398 <a title="56-lda-17" href="./acl-2011-A_Generative_Entity-Mention_Model_for_Linking_Entities_with_Knowledge_Base.html">12 acl-2011-A Generative Entity-Mention Model for Linking Entities with Knowledge Base</a></p>
<p>18 0.5480656 <a title="56-lda-18" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>19 0.54120427 <a title="56-lda-19" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>20 0.53789163 <a title="56-lda-20" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
