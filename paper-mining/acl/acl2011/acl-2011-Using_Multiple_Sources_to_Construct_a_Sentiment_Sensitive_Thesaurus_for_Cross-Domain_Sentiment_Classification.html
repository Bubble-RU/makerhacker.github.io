<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-332" href="#">acl2011-332</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</h1>
<br/><p>Source: <a title="acl-2011-332-pdf" href="http://aclweb.org/anthology//P/P11/P11-1014.pdf">pdf</a></p><p>Author: Danushka Bollegala ; David Weir ; John Carroll</p><p>Abstract: We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automat- ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.</p><p>Reference: <a title="acl-2011-332-reference" href="../acl2011_reference/acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp Abstract  We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. [sent-5, score-1.333]
</p><p>2 We automat-  ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. [sent-6, score-1.592]
</p><p>3 The created thesaurus is then used to expand feature vectors to train a binary classifier. [sent-7, score-0.53]
</p><p>4 Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. [sent-8, score-0.836]
</p><p>5 Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products. [sent-9, score-1.045]
</p><p>6 , 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by  the author of the review. [sent-13, score-0.75]
</p><p>7 Supervised learning algorithms that require labeled data have been successfully used to build sentiment classifiers for a specific domain (Pang et al. [sent-25, score-0.899]
</p><p>8 However, sentiment is expressed differently in different domains, and it is costly to annotate data for each new domain in which we would like to apply a sentiment classifier. [sent-27, score-1.377]
</p><p>9 For example, in the domain of reviews about electronics products, the  words “durable” and “light” are used to express positive sentiment, whereas “expensive” and “short battery life” often indicate negative sentiment. [sent-28, score-0.678]
</p><p>10 On the other hand, if we consider the books domain the words “exciting” and “thriller” express positive sentiment, whereas the words “boring” and “lengthy” usually express negative sentiment. [sent-29, score-0.533]
</p><p>11 A classifier trained on one domain might not perform well on a different domain because it would fail to learn the sentiment of the unseen words. [sent-30, score-1.075]
</p><p>12 , 2007) focuses on the challenge of training a classifier from one or more domains (source domains) and applying the trained classifier in a different domain (target domain). [sent-32, score-0.502]
</p><p>13 A crossdomain sentiment classification system must overcome two main challenges. [sent-33, score-0.745]
</p><p>14 First, it must identify which source domain features are related to which target domain features. [sent-34, score-0.636]
</p><p>15 Ac s2s0o1ci1a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 132–141, garding the relatedness of source and target domain features. [sent-37, score-0.552]
</p><p>16 Following previous work, we define cross-  domain sentiment classification as the problem of learning a binary classifier (i. [sent-38, score-1.029]
</p><p>17 positive or negative sentiment) given a small set of labeled data for the source domain, and unlabeled data for both source and target domains. [sent-40, score-0.634]
</p><p>18 In this paper, we describe a cross-domain sentiment classification method using an automatically created sentiment sensitive thesaurus. [sent-42, score-1.383]
</p><p>19 We use labeled data from multiple source domains and unlabeled data from source and target domains to represent the distribution of features. [sent-43, score-0.848]
</p><p>20 Next, for each lexical element we measure its relatedness to other lexical elements and group related lexical elements to create a thesaurus. [sent-47, score-0.722]
</p><p>21 The thesaurus captures the relatedness among lexical elements that appear in source and target domains based on the contexts in which the lexical elements appear (their distributional context). [sent-48, score-1.143]
</p><p>22 A distinctive aspect of our approach is that, in addition to the usual co-occurrence features typically used in characterizing a word’s distributional context, we make use, where possible, of the sentiment label of a document: i. [sent-49, score-0.666]
</p><p>23 sentiment la-  bels form part of our context features. [sent-51, score-0.583]
</p><p>24 Our method can learn from a large amount of unlabeled data to leverage a robust cross-domain sentiment classifier. [sent-55, score-0.719]
</p><p>25 We model the cross-domain sentiment classification problem as one of feature expansion, where we append additional related features to feature vectors that represent source and target domain reviews in order to reduce the mismatch offeatures between the two domains. [sent-56, score-1.592]
</p><p>26 However, feature expansion techniques have not previously been applied to the task of cross-domain sentiment classification. [sent-59, score-0.784]
</p><p>27 In our method, we use the automatically created 133 thesaurus to expand feature vectors in a binary classifier at train and test times by introducing related  lexical elements from the thesaurus. [sent-60, score-0.761]
</p><p>28 Unlike previous work which attempts to learn a cross-domain classifier using a single source domain, we leverage data from multiple source domains to learn a robust classifier that generalizes across multiple domains. [sent-64, score-0.553]
</p><p>29 A Motivating Example  To explain the problem of cross-domain sentiment classification, consider the reviews shown in Table 1for the domains books and kitchen appliances. [sent-68, score-1.143]
</p><p>30 We have emphasized in boldface the words that express the sentiment of the authors of the reviews. [sent-70, score-0.649]
</p><p>31 We see that the words excellent, broad, high quality, interesting, and well researched are used to express positive sentiment in the books domain, whereas the word disappointed indicates negative sentiment. [sent-71, score-0.936]
</p><p>32 On the other hand, in the kitchen appliances domain the words thrilled, high quality, professional, energy saving, lean, and delicious express positive sentiment, whereas the words rust and disappointed express negative sentiment. [sent-72, score-0.853]
</p><p>33 Although high quality would express positive sentiment in both domains, and disappointed negative sentiment, it is unlikely that we would encounter well researched in kitchen appliances reviews, or rust or delicious in book reviews. [sent-73, score-1.225]
</p><p>34 Therefore, a model that is trained only using book reviews might not have any weights learnt for delicious or rust, which would make it difficult for this model to accurately classify reviews of kitchen appliances. [sent-74, score-0.685]
</p><p>35 Table 1: Positive (+) and negative (-) sentiment reviews in two different domains. [sent-75, score-0.877]
</p><p>36 Table 2: Generating lexical elements and sentiment features from a positive review sentence. [sent-76, score-1.01]
</p><p>37 For example, if we know that both excellent and delicious are positive sentiment words, then we can use this knowledge to expand a feature vector that contains the word delicious using the word excellent, thereby reducing the mismatch between features in a test instance and a trained model. [sent-78, score-1.135]
</p><p>38 Below we describe a method to construct a sentiment sensitive thesaurus for feature expansion. [sent-79, score-1.07]
</p><p>39 In particular, previous work has identified  adjectives as good indicators of sentiment (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000). [sent-85, score-0.583]
</p><p>40 Following previous work in cross-domain sentiment classification, we model a review as a bag of words. [sent-86, score-0.75]
</p><p>41 Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier (Blitzer et al. [sent-89, score-1.508]
</p><p>42 We note that it is possible to create lexical elements both from source domain labeled reviews as well as from unlabeled reviews in source and target domains. [sent-91, score-1.388]
</p><p>43 Second, from each source domain labeled review sentence in which u occurs, we create sentiment features by appending the label of the review to each lexical element we generate from that review. [sent-94, score-1.604]
</p><p>44 In Table 2, we use the notation “*P” to indicate posi-  tive sentiment features and “*N” to indicate negative sentiment features. [sent-96, score-1.268]
</p><p>45 The example sentence shown in Table 2 is selected from a positively labeled review, and generates positive sentiment features as shown in Table 2. [sent-97, score-0.787]
</p><p>46 In addition to word-level sentiment features, we replace words with their POS tags to create POS-level sentiment features. [sent-98, score-1.205]
</p><p>47 POS tags generalize the word-level sentiment features, thereby reducing feature sparseness. [sent-99, score-0.663]
</p><p>48 From the construction of the feature vector u described in the previous paragraph, it follows that w can be either a sentiment feature or another lexical element that co-occurs with u in some review sentence. [sent-102, score-1.143]
</p><p>49 (1)  Here, c(u, w) denotes the number of review sentences in which a lexical element u and a feature w co-occur, n and m respectively denote the total number of lexical elements and the total number of features, and N = Pin=1 Pjm=1 c(i,j). [sent-105, score-0.602]
</p><p>50 Next, for two lexical elements u and v (represented by feature vectors u and v, respectively), we compute the relatedness τ(v, u) of the feature v to the feature u as follows,  τ(v,u) =PPww∈∈{{x | f ( uv, x ) >>00}}f ( uu,,ww)). [sent-108, score-0.599]
</p><p>51 }Re tolatedness of a lexical element u to another lexical element v is the fraction of feature weights in the feature vector for the element u that also co-occur  with the features in the feature vector for the element v. [sent-110, score-1.024]
</p><p>52 Note that relatedness is an asymmetric measure by the definition given in Equation 2, and the relatedness τ(v, u) of an element v to another element u is not necessarily equal to τ(u, v), the relatedness of u to v. [sent-113, score-0.815]
</p><p>53 We use the relatedness measure defined in Equation 2 to construct a sentiment sensitive thesaurus in which, for each lexical element u we list lexical elements v that co-occur with u (i. [sent-114, score-1.512]
</p><p>54 In the remainder of the paper, we use the term base entry to refer to a lexical element u for which its related lexical elements v (referred to as the neighbors of u) are listed in the thesaurus. [sent-117, score-0.525]
</p><p>55 Note that relatedness values computed according to Equation 2 are sensitive to sentiment labels assigned to reviews in the source domain, because co-occurrences are computed over  both lexical and sentiment elements extracted from reviews. [sent-118, score-1.919]
</p><p>56 In other words, the relatedness of an element u to another element v depends upon the sentiment labels assigned to the reviews that generate u and v. [sent-119, score-1.26]
</p><p>57 This is an important fact that differentiates our sentiment-sensitive thesaurus from other distributional thesauri which do not consider sentiment information. [sent-120, score-0.94]
</p><p>58 4  Feature Expansion  Ourfeature expansion phase augments a feature vector with additional related features selected from the sentiment-sensitive thesaurus created in Section 3 to overcome the feature mismatch problem. [sent-124, score-0.685]
</p><p>59 This is particularly important because we would like to score base entries ui considering all the unigrams and bigrams that appear in a review d, instead of considering each unigram or bigram individually. [sent-133, score-0.591]
</p><p>60 To expand a vector, d, for a review d, we first rank the base entries, ui using the ranking score in Equation 3 and select the top k ranked base entries. [sent-134, score-0.594]
</p><p>61 Such words act as domain independent pivots and enable us to transfer the information regarding sentiment from one domain to another. [sent-163, score-1.039]
</p><p>62 Using the extended vectors d0 to represent reviews, we train a binary classifier from the source domain labeled reviews to predict positive and negative sentiment in reviews. [sent-164, score-1.523]
</p><p>63 We differentiate the appended base entries vdr from wi that existed in the original vector d (prior to expansion) by assigning different feature identifiers to the appended base entries. [sent-165, score-0.53]
</p><p>64 For example, a unigram excellent in a feature vector is differentiated from the base entry excellent by assigning the feature id, “BASE=excellent” to the latter. [sent-166, score-0.524]
</p><p>65 1 Dataset To evaluate our method we use the cross-domain sentiment classification dataset prepared by Blitzer et al. [sent-172, score-0.732]
</p><p>66 This dataset consists of Amazon product reviews for four different product types: books (B), DVDs (D), electronics (E) and kitchen appliances (K). [sent-174, score-0.563]
</p><p>67 This benchmark dataset has been used in much previous work on cross-domain sentiment classification and by evaluating on it we can directly compare our method against existing approaches. [sent-177, score-0.781]
</p><p>68 Following previous work, we randomly select 800 positive and 800 negative labeled reviews from each  ××  domain as training instances (i. [sent-178, score-0.702]
</p><p>69 Note that when we combine more than one source domain we limit the total number of source domain labeled reviews to 1600, balanced between the domains. [sent-186, score-0.955]
</p><p>70 For example, if we combine two source domains, then we select 400 positive and 400 negative labeled reviews from each domain giving (400 + 400) 2 = 1600. [sent-187, score-0.8]
</p><p>71 The evaluation metric is classification accuracy on a target domain, computed as the percentage of correctly classified target domain reviews out of the total number of reviews in the target domain. [sent-190, score-1.054]
</p><p>72 Each validation domain has 1000 positive and 1000 negative labeled  reviews, and 15000 unlabeled reviews. [sent-194, score-0.573]
</p><p>73 Using the validation domains as targets, we vary the number of top k ranked base entries (Equation 3) used for feature expansion during training (Traink) and testing (Testk), and measure the average classification 137  Figure 1: Feature expansion at train vs. [sent-195, score-0.852]
</p><p>74 3 Combining Multiple Sources Figure 2 shows the effect of combining multiple source domains to build a sentiment classifier for the electronics domain. [sent-204, score-1.012]
</p><p>75 We see that the kitchen domain is the single best source domain when adapting to the electronics target domain. [sent-205, score-0.752]
</p><p>76 To investigate the impact of the quantity of source domain labeled data on our method, we vary the amount of data from zero to 800 reviews, with equal  amounts of positive and negative labeled data. [sent-212, score-0.666]
</p><p>77 Note that source domain labeled data is used both to create the sentiment sensitive thesaurus as well as to train the sentiment classifier. [sent-214, score-2.029]
</p><p>78 The amount of unlabeled data is held constant, so that any change in classification accu138 BEKB+EB+KE+KB+E+K  Figure 5: Effect of target domain unlabeled data. [sent-217, score-0.681]
</p><p>79 To study the effect of source and target domain unlabeled data on the performance of our method, we create sentiment sensitive thesauri using different proportions of unlabeled data. [sent-227, score-1.431]
</p><p>80 Figure 4 shows classification accuracy on the DVD target domain when we vary the proportion of source domain unlabeled data (target domain’s unlabeled data is fixed). [sent-230, score-1.045]
</p><p>81 Likewise, Figure 5 shows the classification accuracy on the DVD target domain when we vary the proportion of the target domain’s unlabeled data (source domains’ unlabeled data is fixed). [sent-231, score-0.812]
</p><p>82 From Fig-  ures 4 and 5, we see that irrespective of the amount being used, there is a clear performance gain when we use unlabeled data from multiple source domains compared to using a single source domain. [sent-232, score-0.516]
</p><p>83 However, we could not observe a clear gain in performance when we increase the amount of the unlabeled data used to create the sentiment sensitive thesaurus. [sent-233, score-0.853]
</p><p>84 MethodKDEB  Table 3: Cross-domain sentiment classification accuracy. [sent-234, score-0.705]
</p><p>85 4  Cross-Domain Sentiment Classification  Table 3 compares our method against a number of  baselines and previous cross-domain sentiment classification techniques using the benchmark dataset. [sent-236, score-0.754]
</p><p>86 We simply train a binary classifier using unigrams and bigrams as features from the labeled reviews in the source domains and apply the trained classifier on the target domain. [sent-239, score-1.064]
</p><p>87 To evaluate the benefit of using sentiment features on our method, we give a NSS (non-sentiment sensitive) baseline in which we create a thesaurus without using any sentiment features. [sent-252, score-1.531]
</p><p>88 From Table 3, we see that our proposed method returns the best cross-domain sentiment classifica139 tion accuracy (shown in boldface) for the three domains kitchen appliances, DVDs, and electronics. [sent-254, score-0.871]
</p><p>89 The books domain has the lowest number of unlabeled reviews (around 5000) in the dataset. [sent-256, score-0.648]
</p><p>90 Because our method relies upon the availability of unlabeled data for the construction of a sentiment sensitive thesaurus, we believe that this accounts for our lack of performance on the books domain. [sent-257, score-0.883]
</p><p>91 We therefore conclude that using the sentiment sensitive thesaurus for feature expansion is useful for cross-domain sentiment classification. [sent-261, score-1.748]
</p><p>92 6  Related Work  Compared to single-domain sentiment classification, which has been studied extensively in previous work (Pang and Lee, 2008; Turney, 2002), crossdomain sentiment classification has only recently received attention in response to advances in the area of domain adaptation. [sent-264, score-1.539]
</p><p>93 Aue and Gammon (2005) report a number of empirical tests into domain adaptation of sentiment classifiers using an ensemble of classifiers. [sent-265, score-0.794]
</p><p>94 (2007) apply the structural correspondence learning (SCL) algorithm to train a crossdomain sentiment classifier. [sent-268, score-0.652]
</p><p>95 (2010) use structural feature alignment (SFA) to find an alignment between domain specific and domain independent features. [sent-276, score-0.502]
</p><p>96 The mutual information of a feature with domain labels is used to classify domain specific and domain independent features. [sent-277, score-0.75]
</p><p>97 7  Conclusions  We have described and evaluated a method to construct a sentiment-sensitive thesaurus to bridge the gap between source and target domains in cross-domain sentiment classification using multiple source domains. [sent-284, score-1.473]
</p><p>98 Experimental results using a benchmark dataset for cross-domain sentiment classification show that our proposed method can improve classification accuracy in a sentiment classi-  fier. [sent-285, score-1.515]
</p><p>99 Customizing sentiment classifiers to new domains: a case study. [sent-290, score-0.583]
</p><p>100 Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. [sent-298, score-0.583]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sentiment', 0.583), ('thesaurus', 0.286), ('reviews', 0.232), ('domain', 0.211), ('review', 0.167), ('relatedness', 0.167), ('domains', 0.151), ('element', 0.139), ('unlabeled', 0.136), ('classification', 0.122), ('expansion', 0.121), ('kitchen', 0.108), ('elements', 0.106), ('labeled', 0.105), ('base', 0.101), ('source', 0.098), ('sensitive', 0.095), ('delicious', 0.086), ('feature', 0.08), ('unigrams', 0.079), ('appliances', 0.079), ('target', 0.076), ('blitzer', 0.076), ('entries', 0.073), ('bigrams', 0.071), ('classifier', 0.07), ('entry', 0.069), ('ui', 0.069), ('books', 0.069), ('scl', 0.067), ('express', 0.066), ('excellent', 0.062), ('negative', 0.062), ('expand', 0.061), ('positive', 0.059), ('bekb', 0.058), ('disappointed', 0.058), ('rust', 0.058), ('sussex', 0.058), ('vdr', 0.058), ('lexical', 0.055), ('benchmark', 0.049), ('pang', 0.049), ('electronics', 0.048), ('expanding', 0.046), ('dvd', 0.044), ('pan', 0.044), ('distributional', 0.043), ('equation', 0.043), ('binary', 0.043), ('sfa', 0.042), ('crossdomain', 0.04), ('features', 0.04), ('create', 0.039), ('mismatch', 0.039), ('falmer', 0.039), ('nss', 0.039), ('researched', 0.039), ('testk', 0.039), ('kb', 0.039), ('appended', 0.039), ('spectral', 0.039), ('vector', 0.039), ('wj', 0.039), ('mutual', 0.037), ('asymmetric', 0.036), ('enables', 0.036), ('rasp', 0.034), ('pivots', 0.034), ('dvds', 0.034), ('traink', 0.034), ('vdk', 0.034), ('ranking', 0.034), ('select', 0.033), ('multiple', 0.033), ('ke', 0.033), ('eb', 0.033), ('numerous', 0.032), ('uk', 0.032), ('briscoe', 0.032), ('sarawagi', 0.032), ('vectors', 0.031), ('unigram', 0.031), ('thumbs', 0.031), ('train', 0.029), ('effect', 0.029), ('outlined', 0.029), ('accuracy', 0.029), ('ranked', 0.028), ('thesauri', 0.028), ('book', 0.027), ('dataset', 0.027), ('pointwise', 0.027), ('pin', 0.027), ('dimensions', 0.027), ('vary', 0.026), ('construct', 0.026), ('qiang', 0.026), ('brighton', 0.026), ('cheaper', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="332-tfidf-1" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>Author: Danushka Bollegala ; David Weir ; John Carroll</p><p>Abstract: We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automat- ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.</p><p>2 0.46815941 <a title="332-tfidf-2" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>Author: Andrew L. Maas ; Raymond E. Daly ; Peter T. Pham ; Dan Huang ; Andrew Y. Ng ; Christopher Potts</p><p>Abstract: Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semanticterm–documentinformation as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset , of movie reviews to serve as a more robust benchmark for work in this area.</p><p>3 0.37545317 <a title="332-tfidf-3" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>Author: Bin Lu ; Chenhao Tan ; Claire Cardie ; Benjamin K. Tsou</p><p>Abstract: Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8. 12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1</p><p>4 0.35915381 <a title="332-tfidf-4" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>Author: Yulan He ; Chenghua Lin ; Harith Alani</p><p>Abstract: Joint sentiment-topic (JST) model was previously proposed to detect sentiment and topic simultaneously from text. The only supervision required by JST model learning is domain-independent polarity word priors. In this paper, we modify the JST model by incorporating word polarity priors through modifying the topic-word Dirichlet priors. We study the polarity-bearing topics extracted by JST and show that by augmenting the original feature space with polarity-bearing topics, the in-domain supervised classifiers learned from augmented feature representation achieve the state-of-the-art performance of 95% on the movie review data and an average of 90% on the multi-domain sentiment dataset. Furthermore, using feature augmentation and selection according to the information gain criteria for cross-domain sentiment classification, our proposed approach performs either better or comparably compared to previous approaches. Nevertheless, our approach is much simpler and does not require difficult parameter tuning.</p><p>5 0.35403419 <a title="332-tfidf-5" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>Author: Long Jiang ; Mo Yu ; Ming Zhou ; Xiaohua Liu ; Tiejun Zhao</p><p>Abstract: Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-ofthe-art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification. 1</p><p>6 0.33242303 <a title="332-tfidf-6" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>7 0.32550234 <a title="332-tfidf-7" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<p>8 0.32314456 <a title="332-tfidf-8" href="./acl-2011-Semi-supervised_latent_variable_models_for_sentence-level_sentiment_analysis.html">279 acl-2011-Semi-supervised latent variable models for sentence-level sentiment analysis</a></p>
<p>9 0.28134492 <a title="332-tfidf-9" href="./acl-2011-PsychoSentiWordNet.html">253 acl-2011-PsychoSentiWordNet</a></p>
<p>10 0.28039604 <a title="332-tfidf-10" href="./acl-2011-Dr_Sentiment_Knows_Everything%21.html">105 acl-2011-Dr Sentiment Knows Everything!</a></p>
<p>11 0.26762694 <a title="332-tfidf-11" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>12 0.25250775 <a title="332-tfidf-12" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>13 0.24941604 <a title="332-tfidf-13" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>14 0.21844524 <a title="332-tfidf-14" href="./acl-2011-Is_Machine_Translation_Ripe_for_Cross-Lingual_Sentiment_Classification%3F.html">179 acl-2011-Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?</a></p>
<p>15 0.20730136 <a title="332-tfidf-15" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>16 0.15815169 <a title="332-tfidf-16" href="./acl-2011-Identifying_Noun_Product_Features_that_Imply_Opinions.html">159 acl-2011-Identifying Noun Product Features that Imply Opinions</a></p>
<p>17 0.1465227 <a title="332-tfidf-17" href="./acl-2011-Subjectivity_and_Sentiment_Analysis_of_Modern_Standard_Arabic.html">289 acl-2011-Subjectivity and Sentiment Analysis of Modern Standard Arabic</a></p>
<p>18 0.12933177 <a title="332-tfidf-18" href="./acl-2011-Liars_and_Saviors_in_a_Sentiment_Annotated_Corpus_of_Comments_to_Political_Debates.html">211 acl-2011-Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates</a></p>
<p>19 0.1208235 <a title="332-tfidf-19" href="./acl-2011-Effective_Measures_of_Domain_Similarity_for_Parsing.html">109 acl-2011-Effective Measures of Domain Similarity for Parsing</a></p>
<p>20 0.11440225 <a title="332-tfidf-20" href="./acl-2011-Automatically_Predicting_Peer-Review_Helpfulness.html">55 acl-2011-Automatically Predicting Peer-Review Helpfulness</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.27), (1, 0.415), (2, 0.395), (3, -0.109), (4, 0.112), (5, -0.019), (6, -0.017), (7, 0.002), (8, 0.023), (9, -0.024), (10, 0.173), (11, -0.067), (12, -0.011), (13, 0.02), (14, 0.106), (15, 0.098), (16, -0.055), (17, 0.02), (18, 0.031), (19, -0.006), (20, -0.009), (21, -0.12), (22, -0.005), (23, 0.003), (24, -0.012), (25, 0.034), (26, -0.057), (27, -0.028), (28, 0.086), (29, 0.008), (30, -0.033), (31, 0.051), (32, -0.034), (33, -0.059), (34, -0.053), (35, -0.015), (36, -0.094), (37, -0.014), (38, -0.028), (39, 0.041), (40, 0.036), (41, -0.066), (42, -0.029), (43, 0.053), (44, 0.035), (45, 0.045), (46, 0.008), (47, 0.037), (48, -0.041), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97341979 <a title="332-lsi-1" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>Author: Danushka Bollegala ; David Weir ; John Carroll</p><p>Abstract: We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automat- ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.</p><p>2 0.88670999 <a title="332-lsi-2" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>Author: Andrew L. Maas ; Raymond E. Daly ; Peter T. Pham ; Dan Huang ; Andrew Y. Ng ; Christopher Potts</p><p>Abstract: Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semanticterm–documentinformation as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset , of movie reviews to serve as a more robust benchmark for work in this area.</p><p>3 0.88035959 <a title="332-lsi-3" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>Author: Yulan He ; Chenghua Lin ; Harith Alani</p><p>Abstract: Joint sentiment-topic (JST) model was previously proposed to detect sentiment and topic simultaneously from text. The only supervision required by JST model learning is domain-independent polarity word priors. In this paper, we modify the JST model by incorporating word polarity priors through modifying the topic-word Dirichlet priors. We study the polarity-bearing topics extracted by JST and show that by augmenting the original feature space with polarity-bearing topics, the in-domain supervised classifiers learned from augmented feature representation achieve the state-of-the-art performance of 95% on the movie review data and an average of 90% on the multi-domain sentiment dataset. Furthermore, using feature augmentation and selection according to the information gain criteria for cross-domain sentiment classification, our proposed approach performs either better or comparably compared to previous approaches. Nevertheless, our approach is much simpler and does not require difficult parameter tuning.</p><p>4 0.85305798 <a title="332-lsi-4" href="./acl-2011-Semi-supervised_latent_variable_models_for_sentence-level_sentiment_analysis.html">279 acl-2011-Semi-supervised latent variable models for sentence-level sentiment analysis</a></p>
<p>Author: Oscar Tackstrom ; Ryan McDonald</p><p>Abstract: We derive two variants of a semi-supervised model for fine-grained sentiment analysis. Both models leverage abundant natural supervision in the form of review ratings, as well as a small amount of manually crafted sentence labels, to learn sentence-level sentiment classifiers. The proposed model is a fusion of a fully supervised structured conditional model and its partially supervised counterpart. This allows for highly efficient estimation and inference algorithms with rich feature definitions. We describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence-level sentiment analysis compared to all baselines. 1 Sentence-level sentiment analysis In this paper, we demonstrate how combining coarse-grained and fine-grained supervision benefits sentence-level sentiment analysis an important task in the field of opinion classification and retrieval (Pang and Lee, 2008). Typical supervised learning approaches to sentence-level sentiment analysis rely on sentence-level supervision. While such fine-grained supervision rarely exist naturally, and thus requires labor intensive manual annotation effort (Wiebe et al., 2005), coarse-grained supervision is naturally abundant in the form of online review ratings. This coarse-grained supervision is, of course, less informative compared to fine-grained supervision, however, by combining a small amount of sentence-level supervision with a large amount of document-level supervision, we are able to substantially improve on the sentence-level classification task. Our work combines two strands of research: models for sentiment analysis that take document structure into account; – 569 Ryan McDonald Google, Inc., New York ryanmcd@ google com . and models that use latent variables to learn unobserved phenomena from that which can be observed. Exploiting document structure for sentiment analysis has attracted research attention since the early work of Pang and Lee (2004), who performed minimal cuts in a sentence graph to select subjective sentences. McDonald et al. (2007) later showed that jointly learning fine-grained (sentence) and coarsegrained (document) sentiment improves predictions at both levels. More recently, Yessenalina et al. (2010) described how sentence-level latent variables can be used to improve document-level prediction and Nakagawa et al. (2010) used latent variables over syntactic dependency trees to improve sentence-level prediction, using only labeled sentences for training. In a similar vein, Sauper et al. (2010) integrated generative content structure models with discriminative models for multi-aspect sentiment summarization and ranking. These approaches all rely on the availability of fine-grained annotations, but Ta¨ckstro¨m and McDonald (201 1) showed that latent variables can be used to learn fine-grained sentiment using only coarse-grained supervision. While this model was shown to beat a set of natural baselines with quite a wide margin, it has its shortcomings. Most notably, due to the loose constraints provided by the coarse supervision, it tends to only predict the two dominant fine-grained sentiment categories well for each document sentiment category, so that almost all sentences in positive documents are deemed positive or neutral, and vice versa for negative documents. As a way of overcoming these shortcomings, we propose to fuse a coarsely supervised model with a fully supervised model. Below, we describe two ways of achieving such a combined model in the framework of structured conditional latent variable models. Contrary to (generative) topic models (Mei et al., 2007; Titov and Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o.c?i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 569–574, Figure 1: a) Factor graph of the fully observed graphical model. b) Factor graph of the corresponding latent variable model. During training, shaded nodes are observed, while non-shaded nodes are unobserved. The input sentences si are always observed. Note that there are no factors connecting the document node, yd, with the input nodes, s, so that the sentence-level variables, ys, in effect form a bottleneck between the document sentiment and the input sentences. McDonald, 2008; Lin and He, 2009), structured conditional models can handle rich and overlapping features and allow for exact inference and simple gradient based estimation. The former models are largely orthogonal to the one we propose in this work and combining their merits might be fruitful. As shown by Sauper et al. (2010), it is possible to fuse generative document structure models and task specific structured conditional models. While we do model document structure in terms of sentiment transitions, we do not model topical structure. An interesting avenue for future work would be to extend the model of Sauper et al. (2010) to take coarse-grained taskspecific supervision into account, while modeling fine-grained task-specific aspects with latent variables. Note also that the proposed approach is orthogonal to semi-supervised and unsupervised induction of context independent (prior polarity) lexicons (Turney, 2002; Kim and Hovy, 2004; Esuli and Sebastiani, 2009; Rao and Ravichandran, 2009; Velikovich et al., 2010). The output of such models could readily be incorporated as features in the proposed model. 1.1 Preliminaries Let d be a document consisting of n sentences, s = (si)in=1, with a document–sentence-sequence pair denoted d = (d, s). Let yd = (yd, ys) denote random variables1 the document level sentiment, yd, and the sequence of sentence level sentiment, = (ysi)in=1 . – ys 1We are abusing notation throughout by using the same symbols to refer to random variables and their particular assignments. 570 In what follows, we assume that we have access to two training sets: a small set of fully labeled instances, DF = {(dj, and a large set of ydj)}jm=f1, coarsely labeled instances DC = {(dj, yjd)}jm=fm+fm+c1. Furthermore, we assume that yd and all yis take values in {POS, NEG, NEU}. We focus on structured conditional models in the exponential family, with the standard parametrization pθ(yd,ys|s) = expnhφ(yd,ys,s),θi − Aθ(s)o</p><p>5 0.81041431 <a title="332-lsi-5" href="./acl-2011-Aspect_Ranking%3A_Identifying_Important_Product_Aspects_from_Online_Consumer_Reviews.html">45 acl-2011-Aspect Ranking: Identifying Important Product Aspects from Online Consumer Reviews</a></p>
<p>Author: Jianxing Yu ; Zheng-Jun Zha ; Meng Wang ; Tat-Seng Chua</p><p>Abstract: In this paper, we dedicate to the topic of aspect ranking, which aims to automatically identify important product aspects from online consumer reviews. The important aspects are identified according to two observations: (a) the important aspects of a product are usually commented by a large number of consumers; and (b) consumers’ opinions on the important aspects greatly influence their overall opinions on the product. In particular, given consumer reviews of a product, we first identify the product aspects by a shallow dependency parser and determine consumers’ opinions on these aspects via a sentiment classifier. We then develop an aspect ranking algorithm to identify the important aspects by simultaneously considering the aspect frequency and the influence of consumers’ opinions given to each aspect on their overall opinions. The experimental results on 11 popular products in four domains demonstrate the effectiveness of our approach. We further apply the aspect ranking results to the application ofdocumentlevel sentiment classification, and improve the performance significantly.</p><p>6 0.76893491 <a title="332-lsi-6" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>7 0.75675541 <a title="332-lsi-7" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>8 0.75207412 <a title="332-lsi-8" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>9 0.71484756 <a title="332-lsi-9" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>10 0.6776827 <a title="332-lsi-10" href="./acl-2011-Is_Machine_Translation_Ripe_for_Cross-Lingual_Sentiment_Classification%3F.html">179 acl-2011-Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?</a></p>
<p>11 0.67400593 <a title="332-lsi-11" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>12 0.64961553 <a title="332-lsi-12" href="./acl-2011-PsychoSentiWordNet.html">253 acl-2011-PsychoSentiWordNet</a></p>
<p>13 0.64942867 <a title="332-lsi-13" href="./acl-2011-Content_Models_with_Attitude.html">82 acl-2011-Content Models with Attitude</a></p>
<p>14 0.63615209 <a title="332-lsi-14" href="./acl-2011-Dr_Sentiment_Knows_Everything%21.html">105 acl-2011-Dr Sentiment Knows Everything!</a></p>
<p>15 0.62514627 <a title="332-lsi-15" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>16 0.59444058 <a title="332-lsi-16" href="./acl-2011-Subjectivity_and_Sentiment_Analysis_of_Modern_Standard_Arabic.html">289 acl-2011-Subjectivity and Sentiment Analysis of Modern Standard Arabic</a></p>
<p>17 0.52770227 <a title="332-lsi-17" href="./acl-2011-Automatically_Predicting_Peer-Review_Helpfulness.html">55 acl-2011-Automatically Predicting Peer-Review Helpfulness</a></p>
<p>18 0.46292219 <a title="332-lsi-18" href="./acl-2011-Liars_and_Saviors_in_a_Sentiment_Annotated_Corpus_of_Comments_to_Political_Debates.html">211 acl-2011-Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates</a></p>
<p>19 0.46221727 <a title="332-lsi-19" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>20 0.43197531 <a title="332-lsi-20" href="./acl-2011-Semi-supervised_condensed_nearest_neighbor_for_part-of-speech_tagging.html">278 acl-2011-Semi-supervised condensed nearest neighbor for part-of-speech tagging</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.014), (17, 0.075), (26, 0.028), (37, 0.23), (39, 0.056), (41, 0.056), (53, 0.024), (55, 0.026), (59, 0.035), (72, 0.033), (88, 0.178), (91, 0.029), (96, 0.139)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90846491 <a title="332-lda-1" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>Author: Ivan Titov</p><p>Abstract: We consider a semi-supervised setting for domain adaptation where only unlabeled data is available for the target domain. One way to tackle this problem is to train a generative model with latent variables on the mixture of data from the source and target domains. Such a model would cluster features in both domains and ensure that at least some of the latent variables are predictive of the label on the source domain. The danger is that these predictive clusters will consist of features specific to the source domain only and, consequently, a classifier relying on such clusters would perform badly on the target domain. We introduce a constraint enforcing that marginal distributions of each cluster (i.e., each latent variable) do not vary significantly across domains. We show that this constraint is effec- tive on the sentiment classification task (Pang et al., 2002), resulting in scores similar to the ones obtained by the structural correspondence methods (Blitzer et al., 2007) without the need to engineer auxiliary tasks.</p><p>same-paper 2 0.90192783 <a title="332-lda-2" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>Author: Danushka Bollegala ; David Weir ; John Carroll</p><p>Abstract: We describe a sentiment classification method that is applicable when we do not have any labeled data for a target domain but have some labeled data for multiple other domains, designated as the source domains. We automat- ically create a sentiment sensitive thesaurus using both labeled and unlabeled data from multiple source domains to find the association between words that express similar sentiments in different domains. The created thesaurus is then used to expand feature vectors to train a binary classifier. Unlike previous cross-domain sentiment classification methods, our method can efficiently learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products.</p><p>3 0.84501958 <a title="332-lda-3" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>Author: Mark-Jan Nederhof ; Giorgio Satta</p><p>Abstract: We present a method for the computation of prefix probabilities for synchronous contextfree grammars. Our framework is fairly general and relies on the combination of a simple, novel grammar transformation and standard techniques to bring grammars into normal forms.</p><p>4 0.83987826 <a title="332-lda-4" href="./acl-2011-Which_Noun_Phrases_Denote_Which_Concepts%3F.html">334 acl-2011-Which Noun Phrases Denote Which Concepts?</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: Resolving polysemy and synonymy is required for high-quality information extraction. We present ConceptResolver, a component for the Never-Ending Language Learner (NELL) (Carlson et al., 2010) that handles both phenomena by identifying the latent concepts that noun phrases refer to. ConceptResolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data. Domain knowledge (the ontology) guides concept creation by defining a set of possible semantic types for concepts. Word sense induction is performed by inferring a set of semantic types for each noun phrase. Synonym detection exploits redundant informa- tion to train several domain-specific synonym classifiers in a semi-supervised fashion. When ConceptResolver is run on NELL’s knowledge base, 87% of the word senses it creates correspond to real-world concepts, and 85% of noun phrases that it suggests refer to the same concept are indeed synonyms.</p><p>5 0.83942831 <a title="332-lda-5" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>Author: David McClosky ; Mihai Surdeanu ; Christopher Manning</p><p>Abstract: Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a “crime” event can cause a “investigation” event, which can lead to an “arrest” event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP’09 shared task with a F1 score of 53.5% in development and 48.6% in testing.</p><p>6 0.83608937 <a title="332-lda-6" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>7 0.8343516 <a title="332-lda-7" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>8 0.82896614 <a title="332-lda-8" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>9 0.82829183 <a title="332-lda-9" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>10 0.81785744 <a title="332-lda-10" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>11 0.81694627 <a title="332-lda-11" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>12 0.81381679 <a title="332-lda-12" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>13 0.81341785 <a title="332-lda-13" href="./acl-2011-Is_Machine_Translation_Ripe_for_Cross-Lingual_Sentiment_Classification%3F.html">179 acl-2011-Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?</a></p>
<p>14 0.81227767 <a title="332-lda-14" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>15 0.81154072 <a title="332-lda-15" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>16 0.81068933 <a title="332-lda-16" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>17 0.80978513 <a title="332-lda-17" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>18 0.80436325 <a title="332-lda-18" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>19 0.80433166 <a title="332-lda-19" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>20 0.80420196 <a title="332-lda-20" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
