<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 acl-2011-Identification of Domain-Specific Senses in a Machine-Readable Dictionary</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-158" href="#">acl2011-158</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>158 acl-2011-Identification of Domain-Specific Senses in a Machine-Readable Dictionary</h1>
<br/><p>Source: <a title="acl-2011-158-pdf" href="http://aclweb.org/anthology//P/P11/P11-2097.pdf">pdf</a></p><p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki</p><p>Abstract: This paper focuses on domain-specific senses and presents a method for assigning category/domain label to each sense of words in a dictionary. The method first identifies each sense of a word in the dictionary to its corresponding category. We used a text classification technique to select appropriate senses for each domain. Then, senses were scored by computing the rank scores. We used Markov Random Walk (MRW) model. The method was tested on English and Japanese resources, WordNet 3.0 and EDR Japanese dictionary. For evaluation of the method, we compared English results with the Subject Field Codes (SFC) resources. We also compared each English and Japanese results to the first sense heuristics in the WSD task. These results suggest that identification of domain-specific senses (IDSS) may actually be of benefit.</p><p>Reference: <a title="acl-2011-158-reference" href="../acl2011_reference/acl-2011-Identification_of_Domain-Specific_Senses_in_a_Machine-Readable_Dictionary_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 j p Abstract This paper focuses on domain-specific senses and presents a method for assigning category/domain label to each sense of words in a dictionary. [sent-4, score-0.801]
</p><p>2 The method first identifies each sense of a word in the dictionary to its corresponding category. [sent-5, score-0.233]
</p><p>3 We used a text classification technique to select appropriate senses for each domain. [sent-6, score-0.599]
</p><p>4 Then, senses were scored by computing the rank scores. [sent-7, score-0.602]
</p><p>5 We also compared each English and Japanese results to the first sense heuristics in the WSD task. [sent-12, score-0.223]
</p><p>6 These results  suggest that identification of domain-specific senses (IDSS) may actually be of benefit. [sent-13, score-0.598]
</p><p>7 1 Introduction Domain-specific sense of a word is crucial information for many NLP tasks and their applications, such as Word Sense Disambiguation (WSD) and Information Retrieval (IR). [sent-14, score-0.194]
</p><p>8 presented a method to find predominant noun senses automatically using a thesaurus acquired from raw textual corpora and the WordNet similarity package (McCarthy et al. [sent-16, score-0.676]
</p><p>9 , to try to capture changes in ranking of senses for documents from different domains. [sent-28, score-0.65]
</p><p>10 Domain adaptation is also an approach for focussing on domain-specific senses and used in the WSD task (Chand and Ng, 2007; Zhong et al. [sent-29, score-0.619]
</p><p>11 proposed a supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus having examples from the Brown corpus and Wall Street Journal corpus. [sent-33, score-0.217]
</p><p>12 They used active learning, countmerging, and predominant sense estimation in order to save target annotation effort. [sent-34, score-0.275]
</p><p>13 They showed that for the set of nouns which have different predominant senses between the training and target domains, the annotation effort was reduced up to 29%. [sent-35, score-0.678]
</p><p>14 presented a method of supervised domain adaptation (Agirre and Lacalle, 2009). [sent-38, score-0.149]
</p><p>15 They made use of unlabeled data with SVM (Vapnik, 1995), a combination of kernels and SVM, and showed that domain adaptation is an important technique for  WSD systems. [sent-39, score-0.149]
</p><p>16 The major motivation for domain adaptation is that the sense distribution depends on the domain in which a word is used. [sent-40, score-0.466]
</p><p>17 In the context of dictionary-based approach, the first sense heuristic applied to WordNet is often used as a baseline for supervised WSD systems (Cotton et al. [sent-42, score-0.257]
</p><p>18 , 1998), as the senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al. [sent-43, score-0.656]
</p><p>19 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 552–557, drawback in the first sense heuristic applied to the WordNet is the small size of the SemCor corpus. [sent-47, score-0.257]
</p><p>20 Therefore, senses that do not occur in SemCor are often ordered arbitrarily. [sent-48, score-0.592]
</p><p>21 More seriously, the decision is not based on the domain but on the frequency of SemCor data. [sent-49, score-0.098]
</p><p>22 0 synsets were annotated with Subject Field Codes (SFC) by a procedure that exploits the WordNet structure (Magnini and Cavaglia, 2000; Bentivogli et al. [sent-52, score-0.062]
</p><p>23 The results showed that 96% ofthe WordNet synsets of the noun  hierarchy could have been annotated using 115 different SFC, while identification of the domain labels for word senses was required a considerable amount of hand-labeling. [sent-54, score-0.785]
</p><p>24 In this paper, we focus on domain-specific senses and propose a method for assigning category/domain label to each sense of words in a dictionary. [sent-55, score-0.801]
</p><p>25 Our approach is automated, and requires only documents assigned to domains/categories, such as Reuters corpus, and a dictionary with gloss text, such as WordNet. [sent-56, score-0.304]
</p><p>26 Therefore, it can be applied easily to a new domain, sense inventory or different languages, given sufficient documents. [sent-57, score-0.194]
</p><p>27 2  Identification of Domain-Specific Senses  Our approach, IDSS consists of two steps: selection of senses and computation of rank scores. [sent-58, score-0.602]
</p><p>28 1 Selection of senses The first step to find domain-specific senses is to select appropriate senses for each domain. [sent-60, score-1.704]
</p><p>29 Let D be a domain set, and S be a set of senses that the word w ∈ W has. [sent-68, score-0.666]
</p><p>30 For each sense s ∈ S, and for each d ∈ D, we applied hw soernds replacement, i. [sent-75, score-0.194]
</p><p>31 , we replaced w in the training documents assigning to the domain d with its gloss text in a dictionary. [sent-77, score-0.367]
</p><p>32 553 All the training and test documents are tagged by a part-of-speech tagger, and represented as term vectors with frequency. [sent-78, score-0.115]
</p><p>33 If the classification accuracy of the domain d is equal or higher than that without word replacement, the sense s of a word w is judged to be a candidate sense in the domain d. [sent-83, score-0.641]
</p><p>34 2 Computation of rank scores We note that text classification accuracy used in selection of senses depends on the number of words consisting gloss in a dictionary. [sent-86, score-0.806]
</p><p>35 As a result, many of the classification accuracy with word replacement were equal to those without word replacement1 . [sent-88, score-0.137]
</p><p>36 Then in the second procedure, we scored senses by using MRW model. [sent-89, score-0.568]
</p><p>37 Given a set of senses Sd in the domain d, Gd = (Sd, E) is a graph reflecting the relationships between senses in the set. [sent-90, score-1.234]
</p><p>38 Each sense si in Sd is a gloss text assigned from a dictionary. [sent-91, score-0.421]
</p><p>39 Each edge eij in E is associated with an affinity weight f(i → j) ibnet Ewe iesn a senses si a wnidt sj (i j). [sent-93, score-0.674]
</p><p>40 The transition probability from si to sj is then defined by normalizing the corresponding  ×  =  affinity weight p(i → j) =P|kSf=d(1i|→f(ji→)k), if Σf? [sent-97, score-0.143]
</p><p>41 We used the row-normalized matrix Uij = (Uij) |Sd| |Sd| to describe G with each entry corre-  sponding to the transition probability, where Uij = p(i → j). [sent-99, score-0.089]
</p><p>42 The matrix form of the saliency score Score(si) can be formulated in a recursive form as in the MRW model: = + where = [Score(si)]|Sd|×1 is a vector of saliency scores for the senses. [sent-101, score-0.134]
</p><p>43 The final transition matrix is given by the formula (1), and each score of the sense in a specific domain is obtained by the principal eigenvector of the new transition matrix M. [sent-110, score-0.47]
</p><p>44 We note that the matrix M is a high-dimensional space. [sent-114, score-0.052]
</p><p>45 We selected the topmost K% senses according to rank score for each domain and make a sensedomain list. [sent-116, score-0.739]
</p><p>46 For each word w in a document, find the sense s that has the highest score within the list. [sent-117, score-0.194]
</p><p>47 If a domain with the highest score of the sense s and a domain in a document appearing w match, s is regarded as a domain-specific sense of the word w. [sent-118, score-0.645]
</p><p>48 0 We assigned Reuters categories to each sense of words in WordNet 3. [sent-121, score-0.299]
</p><p>49 The Reuters documents are organized into 126 categories (Rose et al. [sent-123, score-0.152]
</p><p>50 We selected 20 categories consisting a variety of genres. [sent-125, score-0.109]
</p><p>51 We used one month of documents, from 20th Aug to 19th Sept 1996 to train the SVM model. [sent-126, score-0.034]
</p><p>52 Similarly, we classified the following one month of documents into these 20 categories. [sent-127, score-0.116]
</p><p>53 All documents were tagged by Tree Tagger (Schmid, 1995). [sent-128, score-0.115]
</p><p>54 For each category, we collected noun words with more than five frequencies from one-  year Reuters corpus. [sent-130, score-0.066]
</p><p>55 The training data is used to estimate K according to rank score, and test data is used to test the method using the estimated value K. [sent-132, score-0.034]
</p><p>56 , the total number of words and senses, and the number of selected senses (Select S) that the classification accuracy of each domain was equal or higher than the result without word replacement. [sent-139, score-0.786]
</p><p>57 There are no existing sense-tagged data for these 20 categories that could be used for evaluation. [sent-141, score-0.07]
</p><p>58 Therefore, we selected a limited number of words and evaluated these words qualitatively. [sent-142, score-0.039]
</p><p>59 Table 3 shows the results of  12 Reuters categories that could be corresponded to SFC labels. [sent-146, score-0.107]
</p><p>60 In Table 3, “Reuters” shows categories, and “IDSS” shows the number of senses assigned by our approach. [sent-147, score-0.603]
</p><p>61 “SFC” refers to the number of senses appearing in the SFC resource. [sent-148, score-0.654]
</p><p>62 “S & R” denotes the number of senses appearing in both SFC and Reuters corpus. [sent-149, score-0.629]
</p><p>63 “Prec” is a ratio of correct assignments by “IDSS” divided by the total number of “IDSS” assignments. [sent-150, score-0.056]
</p><p>64 We manually evaluated senses not appearing in SFC resource. [sent-151, score-0.629]
</p><p>65 Therefore, recall denotes a ratio of the number of senses matched in our approach and SFC divided by the total number of senses appearing in both SFC and Reuters. [sent-153, score-1.253]
</p><p>66 Examining the result of text classification by word replacement, the former was 0. [sent-157, score-0.055]
</p><p>67 One reason is related to the length of the gloss in WordNet: the average number of words consisting the gloss assigned to “weather” was 8. [sent-160, score-0.331]
</p><p>68 IDSS depends on the size of gloss text in WordNet. [sent-163, score-0.173]
</p><p>69 Efficacy can be improved if we can assign gloss sentences to WordNet based on corpus statistics. [sent-164, score-0.148]
</p><p>70 In the WSD task, a first sense heuristic is often applied because of its powerful and needless of expensive hand-annotated data sets. [sent-166, score-0.281]
</p><p>71 We thus compared the results obtained by our method to those obtained by the first sense heuristic. [sent-167, score-0.194]
</p><p>72 For each of the 12 categories, we randomly picked up 10 words from the senses assigned by our approach. [sent-168, score-0.631]
</p><p>73 For each word, we CatTrainTestF-scoreCatTrainTestF-score  CatWordsSensesS sensesCatWordsSensesS senses  ReutersIDSSSFCS&RRecPrec;  Table 3: The results against SFC resource selected 10 sentences from the documents belonging to each corresponding category. [sent-169, score-0.746]
</p><p>74 “Sense” refers to the number of average senses par a word. [sent-172, score-0.62]
</p><p>75 648, while the result obtained by the first sense heuristic was 0. [sent-174, score-0.281]
</p><p>76 Table 555 4 also shows that overall performance obtained by our method was better than that with the first sense  heuristic in all categories. [sent-176, score-0.257]
</p><p>77 2  EDR dictionary  We assigned categories from Japanese Mainichi newspapers to each sense of words in EDR Japanese dictionary 4. [sent-178, score-0.409]
</p><p>78 We selected 4 categories, each of which has sufficient number of documents. [sent-180, score-0.039]
</p><p>79 All documents were tagged by a morphological analyzer Chasen (Matsumoto et al. [sent-181, score-0.115]
</p><p>80 We used 10,000 documents for each category from 1991 to 2000 year to train SVM model. [sent-183, score-0.153]
</p><p>81 We classified other 600 documents from the same period into one of these four categories. [sent-184, score-0.082]
</p><p>82 Table 5 shows categories and F-score (Baseline) by SVM. [sent-185, score-0.07]
</p><p>83 html CatSenseCorrectWIDSroSngPrecCorrectFirWstr soenngsePrec  Table  4: IDSS  against the first sense heuristic (WordNet)  CatPrecisionRecallF-score  IS Encp tioe rn tocametioynal. [sent-195, score-0.295]
</p><p>84 97 807598 02 Table 5: Text classification performance (Baseline)  CatWordsSensesS sensesPrec  IES nc pto ie rn tnocametiyonal3 4, 71 628504 971 2917, 59206296812 1 9013, 50764371 741. [sent-198, score-0.098]
</p><p>85 642 Table 6: The # of selected senses (EDR) for each category and evaluated these senses qualitatively. [sent-200, score-1.207]
</p><p>86 The average precision for four categories was  0. [sent-201, score-0.07]
</p><p>87 In the WSD task, we randomly picked up 30 words from the senses assigned by our method. [sent-203, score-0.631]
</p><p>88 For each word, we selected 10 sentences from the documents belonging to each corresponding category. [sent-204, score-0.147]
</p><p>89 As we can see from Table 7 that IDSS was also better than the first sense heuristics in Japanese data. [sent-206, score-0.223]
</p><p>90 For the first sense heuristics, there was no significant difference between English and Japanese, while the number of senses par a word in Japanese resource was 3. [sent-207, score-0.82]
</p><p>91 , the 556  CatSenseIDSSFirst sense  IES nc ptoie rn tnoscametioynal24 2. [sent-212, score-0.261]
</p><p>92 593 Table 7: IDSS against the first sense heuristic (EDR) small size of the EDR corpus. [sent-218, score-0.257]
</p><p>93 Therefore, there are  many senses that do not occur in the corpus. [sent-219, score-0.568]
</p><p>94 In fact, there are 62,460 nouns which appeared in both EDR and Mainichi newspapers (from 1991 to 2000 year), 164,761 senses in all. [sent-220, score-0.629]
</p><p>95 Of these, there are 114,267 senses not appearing in the EDR corpus. [sent-221, score-0.629]
</p><p>96 This also demonstrates that automatic IDSS is more effective than the frequency-based first sense heuristics. [sent-222, score-0.194]
</p><p>97 4  Conclusion  We presented a method for assigning categories to each sense of words in a machine-readable dictionary. [sent-223, score-0.303]
</p><p>98 Moreover, the result of WSD obtained by our method outperformed against the first sense heuristic in both English and Japanese. [sent-228, score-0.281]
</p><p>99 Future work will include: (i) applying the method to other part-of-speech words, (ii) comparing the method with existing other automated method, and (iii) extending the method to find domain-specific senses with unknown words. [sent-229, score-0.568]
</p><p>100 Domain adaptation with active learning for word sense disambiguation. [sent-275, score-0.245]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('senses', 0.568), ('sfc', 0.375), ('idss', 0.295), ('sense', 0.194), ('edr', 0.189), ('reuters', 0.167), ('wsd', 0.155), ('wordnet', 0.151), ('gloss', 0.148), ('semcor', 0.142), ('sd', 0.11), ('yamanashi', 0.107), ('japanese', 0.101), ('domain', 0.098), ('magnini', 0.089), ('mccarthy', 0.086), ('documents', 0.082), ('predominant', 0.081), ('buitelaar', 0.08), ('mrw', 0.08), ('replacement', 0.08), ('mainichi', 0.071), ('categories', 0.07), ('uij', 0.065), ('svm', 0.063), ('heuristic', 0.063), ('synsets', 0.062), ('appearing', 0.061), ('catwordssensess', 0.054), ('cavaglia', 0.054), ('chasen', 0.054), ('cotton', 0.054), ('netlib', 0.054), ('matrix', 0.052), ('adaptation', 0.051), ('fukumoto', 0.047), ('chand', 0.047), ('zhong', 0.047), ('agirre', 0.046), ('codes', 0.044), ('si', 0.044), ('bentivogli', 0.043), ('koeling', 0.043), ('lacalle', 0.043), ('saliency', 0.041), ('weather', 0.041), ('miller', 0.041), ('assigning', 0.039), ('year', 0.039), ('dictionary', 0.039), ('interdisciplinary', 0.039), ('weeds', 0.039), ('prec', 0.039), ('selected', 0.039), ('rn', 0.038), ('corresponded', 0.037), ('war', 0.037), ('transition', 0.037), ('medicine', 0.036), ('brin', 0.036), ('assigned', 0.035), ('month', 0.034), ('rank', 0.034), ('tagged', 0.033), ('category', 0.032), ('affinity', 0.032), ('newspapers', 0.032), ('divided', 0.031), ('resource', 0.031), ('classification', 0.031), ('identification', 0.03), ('sj', 0.03), ('nouns', 0.029), ('nc', 0.029), ('heuristics', 0.029), ('resources', 0.028), ('picked', 0.028), ('ies', 0.028), ('graduate', 0.027), ('par', 0.027), ('noun', 0.027), ('ut', 0.027), ('equal', 0.026), ('rose', 0.026), ('belonging', 0.026), ('ratio', 0.025), ('matsumoto', 0.025), ('depends', 0.025), ('refers', 0.025), ('result', 0.024), ('ordered', 0.024), ('publically', 0.024), ('forner', 0.024), ('uts', 0.024), ('tributed', 0.024), ('hypertextual', 0.024), ('needless', 0.024), ('rno', 0.024), ('anp', 0.024), ('customization', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="158-tfidf-1" href="./acl-2011-Identification_of_Domain-Specific_Senses_in_a_Machine-Readable_Dictionary.html">158 acl-2011-Identification of Domain-Specific Senses in a Machine-Readable Dictionary</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki</p><p>Abstract: This paper focuses on domain-specific senses and presents a method for assigning category/domain label to each sense of words in a dictionary. The method first identifies each sense of a word in the dictionary to its corresponding category. We used a text classification technique to select appropriate senses for each domain. Then, senses were scored by computing the rank scores. We used Markov Random Walk (MRW) model. The method was tested on English and Japanese resources, WordNet 3.0 and EDR Japanese dictionary. For evaluation of the method, we compared English results with the Subject Field Codes (SFC) resources. We also compared each English and Japanese results to the first sense heuristics in the WSD task. These results suggest that identification of domain-specific senses (IDSS) may actually be of benefit.</p><p>2 0.32382366 <a title="158-tfidf-2" href="./acl-2011-Towards_Tracking_Semantic_Change_by_Visual_Analytics.html">307 acl-2011-Towards Tracking Semantic Change by Visual Analytics</a></p>
<p>Author: Christian Rohrdantz ; Annette Hautli ; Thomas Mayer ; Miriam Butt ; Daniel A. Keim ; Frans Plank</p><p>Abstract: This paper presents a new approach to detecting and tracking changes in word meaning by visually modeling and representing diachronic development in word contexts. Previous studies have shown that computational models are capable of clustering and disambiguating senses, a more recent trend investigates whether changes in word meaning can be tracked by automatic methods. The aim of our study is to offer a new instrument for investigating the diachronic development of word senses in a way that allows for a better understanding of the nature of semantic change in general. For this purpose we combine techniques from the field of Visual Analytics with unsupervised methods from Natural Language Processing, allowing for an interactive visual exploration of semantic change.</p><p>3 0.31637478 <a title="158-tfidf-3" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>Author: Tim Van de Cruys ; Marianna Apidianaki</p><p>Abstract: In this paper, we present a unified model for the automatic induction of word senses from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory. The induction step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions; in a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions. The model is evaluated on the SEMEVAL-20 10 word sense induction and disambiguation task, on which it reaches stateof-the-art results.</p><p>4 0.19005263 <a title="158-tfidf-4" href="./acl-2011-Which_Noun_Phrases_Denote_Which_Concepts%3F.html">334 acl-2011-Which Noun Phrases Denote Which Concepts?</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: Resolving polysemy and synonymy is required for high-quality information extraction. We present ConceptResolver, a component for the Never-Ending Language Learner (NELL) (Carlson et al., 2010) that handles both phenomena by identifying the latent concepts that noun phrases refer to. ConceptResolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data. Domain knowledge (the ontology) guides concept creation by defining a set of possible semantic types for concepts. Word sense induction is performed by inferring a set of semantic types for each noun phrase. Synonym detection exploits redundant informa- tion to train several domain-specific synonym classifiers in a semi-supervised fashion. When ConceptResolver is run on NELL’s knowledge base, 87% of the word senses it creates correspond to real-world concepts, and 85% of noun phrases that it suggests refer to the same concept are indeed synonyms.</p><p>5 0.18505079 <a title="158-tfidf-5" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>Author: Dirk Hovy ; Ashish Vaswani ; Stephen Tratz ; David Chiang ; Eduard Hovy</p><p>Abstract: We present a preliminary study on unsupervised preposition sense disambiguation (PSD), comparing different models and training techniques (EM, MAP-EM with L0 norm, Bayesian inference using Gibbs sampling). To our knowledge, this is the first attempt at unsupervised preposition sense disambiguation. Our best accuracy reaches 56%, a significant improvement (at p <.001) of 16% over the most-frequent-sense baseline.</p><p>6 0.17270699 <a title="158-tfidf-6" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>7 0.17164683 <a title="158-tfidf-7" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>8 0.1668023 <a title="158-tfidf-8" href="./acl-2011-Disambiguating_temporal-contrastive_connectives_for_machine_translation.html">96 acl-2011-Disambiguating temporal-contrastive connectives for machine translation</a></p>
<p>9 0.10419847 <a title="158-tfidf-9" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>10 0.077634864 <a title="158-tfidf-10" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>11 0.073111065 <a title="158-tfidf-11" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>12 0.067568608 <a title="158-tfidf-12" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>13 0.066754743 <a title="158-tfidf-13" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>14 0.066371642 <a title="158-tfidf-14" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>15 0.061409831 <a title="158-tfidf-15" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>16 0.056345224 <a title="158-tfidf-16" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>17 0.054284405 <a title="158-tfidf-17" href="./acl-2011-Effective_Measures_of_Domain_Similarity_for_Parsing.html">109 acl-2011-Effective Measures of Domain Similarity for Parsing</a></p>
<p>18 0.050662432 <a title="158-tfidf-18" href="./acl-2011-Is_Machine_Translation_Ripe_for_Cross-Lingual_Sentiment_Classification%3F.html">179 acl-2011-Is Machine Translation Ripe for Cross-Lingual Sentiment Classification?</a></p>
<p>19 0.049638145 <a title="158-tfidf-19" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>20 0.049208481 <a title="158-tfidf-20" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, 0.05), (2, -0.056), (3, -0.011), (4, 0.001), (5, -0.042), (6, 0.148), (7, 0.062), (8, -0.039), (9, -0.025), (10, 0.017), (11, -0.253), (12, 0.248), (13, 0.049), (14, -0.003), (15, -0.193), (16, 0.24), (17, 0.239), (18, -0.033), (19, 0.201), (20, 0.071), (21, -0.118), (22, 0.01), (23, 0.076), (24, -0.073), (25, 0.046), (26, 0.026), (27, -0.06), (28, -0.055), (29, 0.081), (30, -0.05), (31, 0.079), (32, -0.001), (33, 0.031), (34, -0.099), (35, -0.034), (36, -0.091), (37, -0.008), (38, -0.006), (39, -0.047), (40, -0.002), (41, -0.006), (42, 0.047), (43, 0.031), (44, 0.033), (45, -0.022), (46, -0.023), (47, -0.018), (48, 0.016), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96902782 <a title="158-lsi-1" href="./acl-2011-Identification_of_Domain-Specific_Senses_in_a_Machine-Readable_Dictionary.html">158 acl-2011-Identification of Domain-Specific Senses in a Machine-Readable Dictionary</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki</p><p>Abstract: This paper focuses on domain-specific senses and presents a method for assigning category/domain label to each sense of words in a dictionary. The method first identifies each sense of a word in the dictionary to its corresponding category. We used a text classification technique to select appropriate senses for each domain. Then, senses were scored by computing the rank scores. We used Markov Random Walk (MRW) model. The method was tested on English and Japanese resources, WordNet 3.0 and EDR Japanese dictionary. For evaluation of the method, we compared English results with the Subject Field Codes (SFC) resources. We also compared each English and Japanese results to the first sense heuristics in the WSD task. These results suggest that identification of domain-specific senses (IDSS) may actually be of benefit.</p><p>2 0.91567975 <a title="158-lsi-2" href="./acl-2011-Towards_Tracking_Semantic_Change_by_Visual_Analytics.html">307 acl-2011-Towards Tracking Semantic Change by Visual Analytics</a></p>
<p>Author: Christian Rohrdantz ; Annette Hautli ; Thomas Mayer ; Miriam Butt ; Daniel A. Keim ; Frans Plank</p><p>Abstract: This paper presents a new approach to detecting and tracking changes in word meaning by visually modeling and representing diachronic development in word contexts. Previous studies have shown that computational models are capable of clustering and disambiguating senses, a more recent trend investigates whether changes in word meaning can be tracked by automatic methods. The aim of our study is to offer a new instrument for investigating the diachronic development of word senses in a way that allows for a better understanding of the nature of semantic change in general. For this purpose we combine techniques from the field of Visual Analytics with unsupervised methods from Natural Language Processing, allowing for an interactive visual exploration of semantic change.</p><p>3 0.82015848 <a title="158-lsi-3" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>Author: Tim Van de Cruys ; Marianna Apidianaki</p><p>Abstract: In this paper, we present a unified model for the automatic induction of word senses from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory. The induction step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions; in a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions. The model is evaluated on the SEMEVAL-20 10 word sense induction and disambiguation task, on which it reaches stateof-the-art results.</p><p>4 0.75520873 <a title="158-lsi-4" href="./acl-2011-Which_Noun_Phrases_Denote_Which_Concepts%3F.html">334 acl-2011-Which Noun Phrases Denote Which Concepts?</a></p>
<p>Author: Jayant Krishnamurthy ; Tom Mitchell</p><p>Abstract: Resolving polysemy and synonymy is required for high-quality information extraction. We present ConceptResolver, a component for the Never-Ending Language Learner (NELL) (Carlson et al., 2010) that handles both phenomena by identifying the latent concepts that noun phrases refer to. ConceptResolver performs both word sense induction and synonym resolution on relations extracted from text using an ontology and a small amount of labeled data. Domain knowledge (the ontology) guides concept creation by defining a set of possible semantic types for concepts. Word sense induction is performed by inferring a set of semantic types for each noun phrase. Synonym detection exploits redundant informa- tion to train several domain-specific synonym classifiers in a semi-supervised fashion. When ConceptResolver is run on NELL’s knowledge base, 87% of the word senses it creates correspond to real-world concepts, and 85% of noun phrases that it suggests refer to the same concept are indeed synonyms.</p><p>5 0.65810424 <a title="158-lsi-5" href="./acl-2011-Disambiguating_temporal-contrastive_connectives_for_machine_translation.html">96 acl-2011-Disambiguating temporal-contrastive connectives for machine translation</a></p>
<p>Author: Thomas Meyer</p><p>Abstract: Temporal–contrastive discourse connectives (although, while, since, etc.) signal various types ofrelations between clauses such as temporal, contrast, concession and cause. They are often ambiguous and therefore difficult to translate from one language to another. We discuss several new and translation-oriented experiments for the disambiguation of a specific subset of discourse connectives in order to correct some of the translation errors made by current statistical machine translation systems.</p><p>6 0.63788623 <a title="158-lsi-6" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>7 0.57268375 <a title="158-lsi-7" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>8 0.43761161 <a title="158-lsi-8" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>9 0.43209839 <a title="158-lsi-9" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>10 0.41243035 <a title="158-lsi-10" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>11 0.36429739 <a title="158-lsi-11" href="./acl-2011-Word_Maturity%3A_Computational_Modeling_of_Word_Knowledge.html">341 acl-2011-Word Maturity: Computational Modeling of Word Knowledge</a></p>
<p>12 0.35975489 <a title="158-lsi-12" href="./acl-2011-Even_the_Abstract_have_Color%3A_Consensus_in_Word-Colour_Associations.html">120 acl-2011-Even the Abstract have Color: Consensus in Word-Colour Associations</a></p>
<p>13 0.33854049 <a title="158-lsi-13" href="./acl-2011-Unsupervised_Decomposition_of_a_Document_into_Authorial_Components.html">319 acl-2011-Unsupervised Decomposition of a Document into Authorial Components</a></p>
<p>14 0.33170983 <a title="158-lsi-14" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>15 0.30184054 <a title="158-lsi-15" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>16 0.30114621 <a title="158-lsi-16" href="./acl-2011-Subjective_Natural_Language_Problems%3A_Motivations%2C_Applications%2C_Characterizations%2C_and_Implications.html">288 acl-2011-Subjective Natural Language Problems: Motivations, Applications, Characterizations, and Implications</a></p>
<p>17 0.29764101 <a title="158-lsi-17" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>18 0.26850444 <a title="158-lsi-18" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<p>19 0.26362041 <a title="158-lsi-19" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>20 0.26086158 <a title="158-lsi-20" href="./acl-2011-Local_and_Global_Algorithms_for_Disambiguation_to_Wikipedia.html">213 acl-2011-Local and Global Algorithms for Disambiguation to Wikipedia</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.23), (5, 0.021), (17, 0.037), (26, 0.02), (37, 0.154), (39, 0.031), (41, 0.052), (55, 0.018), (59, 0.041), (72, 0.036), (91, 0.028), (96, 0.109), (97, 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.825167 <a title="158-lda-1" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>Author: Kenneth Hild ; Umut Orhan ; Deniz Erdogmus ; Brian Roark ; Barry Oken ; Shalini Purwar ; Hooman Nezamfar ; Melanie Fried-Oken</p><p>Abstract: Event related potentials (ERP) corresponding to stimuli in electroencephalography (EEG) can be used to detect the intent of a person for brain computer interfaces (BCI). This paradigm is widely used to build letter-byletter text input systems using BCI. Nevertheless using a BCI-typewriter depending only on EEG responses will not be sufficiently accurate for single-trial operation in general, and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed. Hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed. In this demonstration we will present a BCI system for typing that integrates a stochastic language model with ERP classification to achieve speedups, via the rapid serial visual presentation (RSVP) paradigm.</p><p>2 0.80562991 <a title="158-lda-2" href="./acl-2011-A_Comprehensive_Dictionary_of_Multiword_Expressions.html">6 acl-2011-A Comprehensive Dictionary of Multiword Expressions</a></p>
<p>Author: Kosho Shudo ; Akira Kurahone ; Toshifumi Tanabe</p><p>Abstract: It has been widely recognized that one of the most difficult and intriguing problems in natural language processing (NLP) is how to cope with idiosyncratic multiword expressions. This paper presents an overview of the comprehensive dictionary (JDMWE) of Japanese multiword expressions. The JDMWE is characterized by a large notational, syntactic, and semantic diversity of contained expressions as well as a detailed description of their syntactic functions, structures, and flexibilities. The dictionary contains about 104,000 expressions, potentially 750,000 expressions. This paper shows that the JDMWE’s validity can be supported by comparing the dictionary with a large-scale Japanese N-gram frequency dataset, namely the LDC2009T08, generated by Google Inc. (Kudo et al. 2009). 1</p><p>same-paper 3 0.76345068 <a title="158-lda-3" href="./acl-2011-Identification_of_Domain-Specific_Senses_in_a_Machine-Readable_Dictionary.html">158 acl-2011-Identification of Domain-Specific Senses in a Machine-Readable Dictionary</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki</p><p>Abstract: This paper focuses on domain-specific senses and presents a method for assigning category/domain label to each sense of words in a dictionary. The method first identifies each sense of a word in the dictionary to its corresponding category. We used a text classification technique to select appropriate senses for each domain. Then, senses were scored by computing the rank scores. We used Markov Random Walk (MRW) model. The method was tested on English and Japanese resources, WordNet 3.0 and EDR Japanese dictionary. For evaluation of the method, we compared English results with the Subject Field Codes (SFC) resources. We also compared each English and Japanese results to the first sense heuristics in the WSD task. These results suggest that identification of domain-specific senses (IDSS) may actually be of benefit.</p><p>4 0.70934027 <a title="158-lda-4" href="./acl-2011-Types_of_Common-Sense_Knowledge_Needed_for_Recognizing_Textual_Entailment.html">315 acl-2011-Types of Common-Sense Knowledge Needed for Recognizing Textual Entailment</a></p>
<p>Author: Peter LoBue ; Alexander Yates</p><p>Abstract: Understanding language requires both linguistic knowledge and knowledge about how the world works, also known as common-sense knowledge. We attempt to characterize the kinds of common-sense knowledge most often involved in recognizing textual entailments. We identify 20 categories of common-sense knowledge that are prevalent in textual entailment, many of which have received scarce attention from researchers building collections of knowledge.</p><p>5 0.70634675 <a title="158-lda-5" href="./acl-2011-Learning_Sub-Word_Units_for_Open_Vocabulary_Speech_Recognition.html">203 acl-2011-Learning Sub-Word Units for Open Vocabulary Speech Recognition</a></p>
<p>Author: Carolina Parada ; Mark Dredze ; Abhinav Sethy ; Ariya Rastrow</p><p>Abstract: Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of subword units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.</p><p>6 0.69162738 <a title="158-lda-6" href="./acl-2011-Sentiment_Analysis_of_Citations_using_Sentence_Structure-Based_Features.html">281 acl-2011-Sentiment Analysis of Citations using Sentence Structure-Based Features</a></p>
<p>7 0.67676079 <a title="158-lda-7" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>8 0.66776133 <a title="158-lda-8" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>9 0.62830949 <a title="158-lda-9" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>10 0.62771487 <a title="158-lda-10" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>11 0.62701333 <a title="158-lda-11" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>12 0.62572891 <a title="158-lda-12" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>13 0.62447119 <a title="158-lda-13" href="./acl-2011-Which_Noun_Phrases_Denote_Which_Concepts%3F.html">334 acl-2011-Which Noun Phrases Denote Which Concepts?</a></p>
<p>14 0.62366712 <a title="158-lda-14" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>15 0.62307954 <a title="158-lda-15" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>16 0.62048608 <a title="158-lda-16" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>17 0.61853433 <a title="158-lda-17" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>18 0.61711061 <a title="158-lda-18" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>19 0.61707646 <a title="158-lda-19" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>20 0.61633962 <a title="158-lda-20" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
