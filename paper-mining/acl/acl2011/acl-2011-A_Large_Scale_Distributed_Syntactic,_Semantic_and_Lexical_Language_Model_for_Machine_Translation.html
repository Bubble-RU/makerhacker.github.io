<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-17" href="#">acl2011-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</h1>
<br/><p>Source: <a title="acl-2011-17-pdf" href="http://aclweb.org/anthology//P/P11/P11-1021.pdf">pdf</a></p><p>Author: Ming Tan ; Wenli Zhou ; Lei Zheng ; Shaojun Wang</p><p>Abstract: This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-the- art parsing-based machine translation system.</p><p>Reference: <a title="acl-2011-17-reference" href="../acl2011_reference/acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. [sent-9, score-0.805]
</p><p>2 , 2007; Zhang, 2008) have shown that using an immense distributed computing paradigm, up to 6grams can be trained on up to billions and trillions of words, yielding consistent system improvements, but Zhang (2008) did not observe much improvement beyond 6-grams. [sent-13, score-0.09]
</p><p>3 Although the Markov chains 201 are efficient at encoding local word interactions, the n-gram model clearly ignores the rich syntactic and semantic structures that constrain natural languages. [sent-14, score-0.077]
</p><p>4 (2006) integrated n-gram, structured language model (SLM) (Chelba and Jelinek, 2000) and probabilistic latent semantic analysis (PLSA) (Hofmann, 2001) under the directed MRF framework (Wang et al. [sent-19, score-0.112]
</p><p>5 , 2005) and studied the stochastic properties for the composite language model. [sent-20, score-0.365]
</p><p>6 They derived a generalized inside-outside algorithm to train the composite language model from a general EM (Dempster et al. [sent-21, score-0.399]
</p><p>7 In this paper, we study the same composite language model. [sent-24, score-0.365]
</p><p>8 , 2006), we train this composite model by a convergent N-best list approximate EM algorithm that has linear time complexity and a follow-up EM algorithm to improve word prediction power. [sent-26, score-0.568]
</p><p>9 We conduct comprehensive experiments on corpora with 44 million tokens, 230 million tokens, and 1. [sent-27, score-0.138]
</p><p>10 3 billion  tokens and compare perplexity results with n-grams (n=3,4,5 respectively) on these three corpora, we obtain drastic perplexity reductions. [sent-28, score-0.463]
</p><p>11 2 Composite language model The n-gram language model is essentially a word predictor that given its entire document history it predicts next word wk+1 based on the last n-1 words with probability p(wk+1|wkk−n+2) where wkk−n+2 = wk−n+2 , · · · , wk . [sent-32, score-0.516]
</p><p>12 The SLM is based on statistical parsing techniques that allow syntactic analysis of sentences; it assigns a probability p(W, T) to every sentence W and every possible binary parse T. [sent-34, score-0.163]
</p><p>13 The terminals of T are the words of W with POS  tags, and the nodes of T are annotated with phrase headwords and non-terminal labels. [sent-35, score-0.103]
</p><p>14 Let W be a sentence of length n words to which we have prepended the sentence beginning marker   and appended the sentence end marker   so that w0 =  and wn+1 = . [sent-36, score-0.102]
</p><p>15 Let Wk = w0, · · · , wk be the word k-prefix of the sentence the ,w··or·d ,sw from the beginning of the sentence up to the current position k and WkTk the word-parse k-prefix. [sent-37, score-0.375]
</p><p>16 A word-parse k-prefix has a set of exposed heads h−m, · · · , h−1, with each head being a pair (headword, no,n·-t·e·rm ,hinal label), or in the case of a root-only tree (word, POS tag). [sent-38, score-0.088]
</p><p>17 htea partial parse Tk from –  Tk−1, wk, and tk in a series of moves ending with NULL, where a parse move a is made with probability p(a|h−1); a ∈ A={(unary, NTlabel), (adjoinleft, NTlabel), (adjoin-right, NTlabel), null}. [sent-42, score-0.368]
</p><p>18 Since only one pair of (d, w) is being observed, as a result, ntlhye joint probability )m isod beeli isg a mixture of log-Plinear model with the expression p(d, w) = p(d) Pg p(w|g)p(g|d). [sent-46, score-0.068]
</p><p>19 Typically, the number of documPentsp a(nwd| vocabulary size are much larger than the sizeP oflatent semantic class variables. [sent-47, score-0.075]
</p><p>20 Thus, latent semantic class variables function as bottleneck variables to constrain word occurrences in documents. [sent-48, score-0.043]
</p><p>21 When combining n-gram, m order SLM and PLSA models together to build a composite generative language model under the directed MRF paradigm (Wang et al. [sent-49, score-0.475]
</p><p>22 The parameter +fo2r WORD-PREDICTOR in the composite n-gram/m-SLM/PLSA language model becomes p(wk+1 |wkk−n+2h−−1mgk+1). [sent-52, score-0.399]
</p><p>23 The resulting composite language −mno+d2el −hmas an even more complex dependency structure but with more expressive power than the original SLM. [sent-53, score-0.365]
</p><p>24 Figure 1 illustrates the structure of a composite n-gram/mSLM/PLSA language model. [sent-54, score-0.365]
</p><p>25 The composite n-gram/m-SLM/PLSA language model can be formulated as a directed MRF model (Wang et al. [sent-55, score-0.468]
</p><p>26 Figure 1: A composite n-gram/m-SLM/PLSA language model where the hidden information is the parse tree T and semantic content g. [sent-107, score-0.537]
</p><p>27 3 Training algorithm Under the composite n-gram/m-SLM/PLSA  lan-  guage model, the likelihood of a training corpus D, ag ucaoglleec mtioodne lo,f t dhoec liukmeelinhtoso, cda onf b ae rwairinttienng caos  L(D,p) =dY∈D Yl XGl XTlPp(Wl,Tl,Gl|d)! [sent-109, score-0.4]
</p><p>28 (1)  where (Wl, Tl , Gl , d) denote the joint sequence of the lth sentence Wl with its parse tree structure Tl and semantic annotation string Gl in document d. [sent-111, score-0.3]
</p><p>29 The objective of maximum likelihood estimation is to maximize the likelihood L(D, p) respect to miso tdoel m parameters. [sent-117, score-0.07]
</p><p>30 F loikr a given sentence, istsp parse tree and semantic content are hidden and the number of parse trees grows faster than exponential with sentence length, Wang et al. [sent-118, score-0.298]
</p><p>31 1 N-best list approximate EM Similar to SLM (Chelba and Jelinek, 2000), we adopt an N-best list approximate EM re-estimation with modular modifications to seamlessly incorporate the effect of n-gram and PLSA components. [sent-122, score-0.214]
</p><p>32 N-best list search: For each sentence W in document d, find N-best parse trees,  T | ′lN de  TNl= argmT′alNxnXGlTl∈XT′lNPp(Wl,Tl,Gl|d),||T′lN|| = No and denote TN as the collection of N-best list parse etrneoetse f Tor sentences over entire corpus D punardseer mtreoedse flo parameter p. [sent-126, score-0.411]
</p><p>33 Due to space constraints, we omit the proof of the convergence of the N-best list approximate EM algorithm which uses Zangwill’s  global convergence theorem (Zangwill, 1969). [sent-130, score-0.169]
</p><p>34 Each stack contains hypotheses (partial parses) that have been constructed by the same number of WORD-PREDICTOR and the same number of CONSTRUCTOR operations. [sent-132, score-0.058]
</p><p>35 A stack vector consists of ,th··e· ·o ,rdgered set of stacks containing partial parses with the same number of WORD-PREDICTOR operations but different number of CONSTRUCTOR operations. [sent-134, score-0.166]
</p><p>36 In WORD-PREDICTOR and TAGGER operations, some hypotheses are discarded due to the maximum number of hypotheses the stack can contain at any given time. [sent-135, score-0.058]
</p><p>37 In CONSTRUCTOR  operation, the resulting hypotheses are discarded due to either finite stack size or the log-probability threshold: the maximum tolerable difference between the log-probability score of the top-most hypothesis and the bottom-most hypothesis at any given state of the stack. [sent-136, score-0.058]
</p><p>38 204 EM update: Once we have the N-best parse trees for each sentence in document d and N-best topics for document d, we derive the EM algorithm to estimate model parameters. [sent-137, score-0.379]
</p><p>39 In E-step, we compute the expected count of each model parameter over sentence Wl in document d in the training corpus D. [sent-138, score-0.214]
</p><p>40 IZ FEoRr, t hthee WnuOmRbDerof possible semantic annotation sequences is exponential, we use forward-backward recursive formulas that are similar to those in hidden Markov models to compute the expected counts. [sent-140, score-0.085]
</p><p>41 We define the forward vector αl (g|d) to be αlk+1(g|d) = X Pp(Wkl, Tkl, wkk−n+2wk+1h−1g, Glk|d) XGlk  that can be recursively computed in a forward manner, where Wkl is the word k-prefix for sentence Wl,  Tkl is the parse for k-prefix. [sent-141, score-0.129]
</p><p>42 In M-step, Pthe rTecursive line|adr) interpolation scheme (Jelinek and Mercer, 1981) is used to obtain a smooth probability estimate for each model component, WORD-PREDICTOR, TAGGER, and CONSTRUCTOR. [sent-144, score-0.068]
</p><p>43 The TAGGER and CONSTRUCTOR are conditional probabilistic models of the type p(u|z1 , · · · , zn) where u, z1, · · · , zn belong t toy a emi px(eud|z se,t· ·of· words, POS tags, NTtags, CONSTRUCTOR actions (u only), and z1, · · · , zn form a linear Markov chain. [sent-145, score-0.118]
</p><p>44 tional probabilistic model where there are three kinds of context and g, each forms a linear Markov chain−. [sent-149, score-0.064]
</p><p>45 3 Distributed architecture When using very large corpora to train our composite language model, both the data and the parameters  can’t be stored in a single machine, so we have to resort to distributed computing. [sent-158, score-0.455]
</p><p>46 The topic of large scale distributed language models is relatively new, and existing works are restricted to n-grams only (Brants et al. [sent-159, score-0.09]
</p><p>47 Even though all use distributed architectures that follow the client-server paradigm, the real implementations are in fact different. [sent-163, score-0.09]
</p><p>48 (2007) store training corpora in suffix arrays such that one sub-corpus per server serves raw counts and test sentences are loaded in a client. [sent-166, score-0.144]
</p><p>49 This implies that when computing the language model probability of a sentence in a client, all servers need to be contacted for each ngram request. [sent-167, score-0.265]
</p><p>50 and make it suitable to perform iterations  of N-best list approximate EM algorithm, see Figure 2. [sent-171, score-0.107]
</p><p>51 The corpus is divided and loaded into a number of clients. [sent-172, score-0.044]
</p><p>52 We use a public available parser to parse the sentences in each client to get the initial counts for etc. [sent-173, score-0.199]
</p><p>53 of the servers by hashing through the word  w−1  (or  h−1) and its topic g, finish the Reduce part. [sent-175, score-0.165]
</p><p>54 This is the initialization of the N-best list approximate EM step. [sent-176, score-0.107]
</p><p>55 Each client then calls the servers for parameters to perform synchronous multi-stack search for each sentence to get the N-best list parse trees. [sent-177, score-0.411]
</p><p>56 Again, the expected count for a particular parameter of w−−n1+1wh−−1mg at the clients are computed, thus we fi−nnis+h1 a Map part, then summed up and stored in one of the servers by hashing through the word w−1 (or h−1) and its topic g, thus we finish the Reduce part. [sent-178, score-0.326]
</p><p>57 Similarly, we use a distributed architecture as in Figure 2 to perform the follow-up EM algorithm to re-estimate WORD-PREDICTOR. [sent-180, score-0.09]
</p><p>58 4 Experimental results We have trained our language models using three different training sets: one has 44 million tokens, another has 230 million tokens, and the other has 1. [sent-181, score-0.138]
</p><p>59 An independent test set which has 354 k tokens is chosen. [sent-183, score-0.121]
</p><p>60 7 million tokens for the 44 million tokens training corpus, 13. [sent-185, score-0.38]
</p><p>61 T Similar to SLM (Chelba and Jelinek, 2000), after the parses undergo headword percolation and binarization, each model component of WORDPREDICTOR, TAGGER, and CONSTRUCTOR is initialized from a set of parsed sentences. [sent-215, score-0.064]
</p><p>62 For the 44 and 230 million tokens corpora, all sentences are automatically parsed and used to initialize model parameters, while for 1. [sent-217, score-0.224]
</p><p>63 3 billion tokens corpus, we parse the sentences from a portion of the corpus that contain 230 million tokens, then use them to initialize model parameters. [sent-218, score-0.435]
</p><p>64 The parser at ”openNLP” is trained by Upenn treebank with 1million tokens and there is a mismatch between Upenn treebank and LDC English Gigaword corpus. [sent-219, score-0.121]
</p><p>65 If we have access to a large cluster of machines with Hadoop installed that are powerful enough to process a billion tokens level corpus, we just need to specify a map function and a reduce function etc. [sent-222, score-0.273]
</p><p>66 Instead, we have access to a supercomputer at a supercomputer center with MPI installed that has more than 1000 core processors usable. [sent-225, score-0.202]
</p><p>67 We use up to 1000 core processors to train the composite language models for 1. [sent-229, score-0.413]
</p><p>68 3 billion tokens corpus where 900 core processors are  used to store the parameters alone. [sent-230, score-0.34]
</p><p>69 We decide to use linearly smoothed trigram as the baseline model for 44 million token corpus, linearly smoothed 4-gram as the baseline model for 230 million token corpus, and linearly smoothed 5-gram as the baseline model for 1. [sent-231, score-0.24]
</p><p>70 Model size is a big issue, we have to keep only a small set of topics due to the consideration in both computational time and resource demand. [sent-233, score-0.055]
</p><p>71 Table 2 shows the perplexity results and computation time of composite n-gram/PLSA language models that are trained on three corpora when the pre-defined number of total topics is 200 but different numbers of most likely topics are kept for each document in PLSA, the rest are pruned. [sent-234, score-0.637]
</p><p>72 3 billion tokens corpus, 400 cores have to be used to keep top 5 most likely topics. [sent-236, score-0.237]
</p><p>73 For composite tri207 gram/PLSA model trained on 44M tokens corpus, the computation time increases drastically with less than 5% percent perplexity improvement. [sent-237, score-0.617]
</p><p>74 So in the following experiments, we keep top 5 topics for each document from total 200 topics and all other 195 topics are pruned. [sent-238, score-0.23]
</p><p>75 All composite language models are first trained by performing N-best list approximate EM algorithm until convergence, then EM algorithm for a second stage of parameter re-estimation for WORDPREDICTOR and SEMANTIZER until convergence. [sent-239, score-0.472]
</p><p>76 We fix the size of topics in PLSA to be 200 and then prune to 5 in the experiments, where the unpruned 5 topics in general account for 70% probability in p(g|d). [sent-240, score-0.144]
</p><p>77 r T a variety oofw dsif cfoermepnrte mheondseilvse s puecrhas composite n-gram/m-SLM, n-gram/PLSA, mSLM/PLSA, their linear combinations, etc. [sent-242, score-0.395]
</p><p>78 In Table 3, for composite n-gram/m-SLM model (n = 3, m = 2 and n = 4, m = 3) trained on 44 million tokens and 230 million tokens, we cut off its fractional expected counts that are less than a threshold 0. [sent-245, score-0.779]
</p><p>79 3 billion tokens corpus, we have to both aggressively prune the param-  eters of WORD-PREDICTOR and shrink the order of n-gram and m-SLM in order to store them in a supercomputer having 1000 cores. [sent-248, score-0.351]
</p><p>80 In particular, for composite 5-gram/4-SLM model, its size is too big to store, thus we use its approximation, a linear combination of 5-gram/2-SLM and 2-gram/4-SLM, and for 5-gram/2-SLM or 2-gram/4-SLM, again we cut off its fractional expected counts that are less than a threshold 0. [sent-249, score-0.516]
</p><p>81 For composite 4SLM/PLSA model, we cut off its fractional expected counts that are less than a threshold 0. [sent-251, score-0.486]
</p><p>82 For composite 4-SLM/PLSA model or its linear combination with models, we ignore all the tags and use only the words in the 4 head words. [sent-253, score-0.429]
</p><p>83 3,mB=4REDTUIOCN-  Table 3: Perplexity results for various language models on test corpus, where  + denotes  linear combination, / denotes  composite model; n denotes the order of n-gram and m denotes the order of SLM; the topic nodes are pruned from  200 to 5. [sent-256, score-0.395]
</p><p>84 The composite n-gram/m-SLM/PLSA model gives significant perplexity reductions over baseline n-grams, n = 3, 4, 5 and m-SLMs, m = 2, 3, 4. [sent-258, score-0.496]
</p><p>85 The majority of gains comes from PLSA component, but when adding SLM component into n-gram/PLSA, there is a further 10% relative perplexity reduction. [sent-259, score-0.097]
</p><p>86 We have applied our composite 5-gram/2SLM+2-gram/4-SLM+5-gram/PLSA language model that is trained by 1. [sent-260, score-0.399]
</p><p>87 3 billion word corpus for the task of re-ranking the N-best list in statistical machine translation. [sent-261, score-0.177]
</p><p>88 We used the same 1000-best list that is used by Zhang et al. [sent-262, score-0.061]
</p><p>89 This 208  list was generated on 919 sentences from the MT03 Chinese-English evaluation set by Hiero (Chiang, 2005; Chiang, 2007), a state-of-the-art parsing-based translation model. [sent-264, score-0.093]
</p><p>90 Its decoder uses a trigram language model trained with modified Kneser-Ney smoothing (Kneser and Ney, 1995) on a 200 million tokens corpus. [sent-265, score-0.224]
</p><p>91 Each translation has 11 features and language model is one of them. [sent-266, score-0.066]
</p><p>92 , 2002) by MERT (Och, 2003), a remaining single piece is used to re-rank the 1000-best list and obtain the BLEU score. [sent-270, score-0.061]
</p><p>93 It is expected that putting the our composite language into a one pass decoder of both phrase-based (Koehn et al. [sent-281, score-0.407]
</p><p>94 209 The composite 5-gram/2-SLM+2-gram/4-SLM+5gram/PLSA language model improves both significantly. [sent-290, score-0.399]
</p><p>95 (2003) integrated Charniak’s language model with the syntaxbased translation model Yamada and Knight proposed (2001) to rescore a tree-to-string translation forest, whereas we use only our language model for N-best list re-ranking. [sent-292, score-0.227]
</p><p>96 The difference between  human judgments and BLEU scores indicate that closer agreement may be possible by incorporating syntactic structure and semantic information into the BLEU score evaluation. [sent-294, score-0.043]
</p><p>97 This modification will lead to a better metric and such information can be provided by our composite language models. [sent-297, score-0.365]
</p><p>98 5  Conclusion  As far as we know, this is the first work of building a complex large scale distributed language model with a principled approach that is more powerful than ngrams when both trained on a very large corpus with up to a billion tokens. [sent-299, score-0.24]
</p><p>99 We believe our results still hold on web scale corpora that have trillion tokens, since the composite language model effectively encodes long range dependencies of natural language that n-gram is not viable to consider. [sent-300, score-0.399]
</p><p>100 Exploiting syntactic, semantic and lexical regularities in language modeling via directed Markov random fields. [sent-460, score-0.078]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('composite', 0.365), ('wk', 0.307), ('wl', 0.252), ('constructor', 0.235), ('tl', 0.219), ('jelinek', 0.213), ('wkk', 0.176), ('slm', 0.162), ('chelba', 0.152), ('gl', 0.144), ('em', 0.131), ('servers', 0.127), ('gk', 0.125), ('plsa', 0.124), ('tokens', 0.121), ('billion', 0.116), ('tk', 0.114), ('semantizer', 0.109), ('wkl', 0.109), ('bleu', 0.108), ('headwords', 0.103), ('perplexity', 0.097), ('parse', 0.095), ('wordpredictor', 0.091), ('xgl', 0.091), ('distributed', 0.09), ('exposed', 0.088), ('clients', 0.08), ('pp', 0.077), ('tkl', 0.072), ('million', 0.069), ('document', 0.065), ('mpi', 0.064), ('lth', 0.063), ('list', 0.061), ('client', 0.059), ('dy', 0.059), ('supercomputer', 0.059), ('stack', 0.058), ('tagger', 0.055), ('mapreduce', 0.055), ('store', 0.055), ('topics', 0.055), ('ntlabel', 0.054), ('ldc', 0.052), ('charniak', 0.05), ('hiero', 0.048), ('hadoop', 0.048), ('processors', 0.048), ('stacks', 0.048), ('yl', 0.047), ('approximate', 0.046), ('counts', 0.045), ('markov', 0.044), ('mrf', 0.044), ('loaded', 0.044), ('zn', 0.044), ('chiang', 0.044), ('gigaword', 0.044), ('brants', 0.043), ('semantic', 0.043), ('expected', 0.042), ('ln', 0.042), ('predictor', 0.042), ('emami', 0.041), ('paradigm', 0.041), ('wang', 0.04), ('count', 0.039), ('lk', 0.038), ('finish', 0.038), ('contacted', 0.036), ('glk', 0.036), ('installed', 0.036), ('multistack', 0.036), ('txnl', 0.036), ('zangwill', 0.036), ('directed', 0.035), ('upenn', 0.035), ('opennlp', 0.035), ('synchronous', 0.035), ('likelihood', 0.035), ('model', 0.034), ('sentence', 0.034), ('readability', 0.034), ('fractional', 0.034), ('probability', 0.034), ('tn', 0.032), ('translation', 0.032), ('vocabulary', 0.032), ('papineni', 0.032), ('wright', 0.032), ('convergent', 0.032), ('drastic', 0.032), ('twh', 0.032), ('xpp', 0.032), ('convergence', 0.031), ('trees', 0.031), ('linear', 0.03), ('partial', 0.03), ('parses', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="17-tfidf-1" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>Author: Ming Tan ; Wenli Zhou ; Lei Zheng ; Shaojun Wang</p><p>Abstract: This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-the- art parsing-based machine translation system.</p><p>2 0.11531541 <a title="17-tfidf-2" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>3 0.11319056 <a title="17-tfidf-3" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>4 0.11008333 <a title="17-tfidf-4" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve transla- , tion quality and collectively achieve up to 1 BLEU point over a competitive baseline.</p><p>5 0.090765379 <a title="17-tfidf-5" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>6 0.079613626 <a title="17-tfidf-6" href="./acl-2011-Phrase-Based_Translation_Model_for_Question_Retrieval_in_Community_Question_Answer_Archives.html">245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</a></p>
<p>7 0.078838632 <a title="17-tfidf-7" href="./acl-2011-Learning_to_Transform_and_Select_Elementary_Trees_for_Improved_Syntax-based_Machine_Translations.html">206 acl-2011-Learning to Transform and Select Elementary Trees for Improved Syntax-based Machine Translations</a></p>
<p>8 0.077409379 <a title="17-tfidf-8" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>9 0.069875717 <a title="17-tfidf-9" href="./acl-2011-Prefix_Probability_for_Probabilistic_Synchronous_Context-Free_Grammars.html">250 acl-2011-Prefix Probability for Probabilistic Synchronous Context-Free Grammars</a></p>
<p>10 0.068807341 <a title="17-tfidf-10" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>11 0.067071125 <a title="17-tfidf-11" href="./acl-2011-Binarized_Forest_to_String_Translation.html">61 acl-2011-Binarized Forest to String Translation</a></p>
<p>12 0.066437893 <a title="17-tfidf-12" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>13 0.065393932 <a title="17-tfidf-13" href="./acl-2011-Crowdsourcing_Translation%3A_Professional_Quality_from_Non-Professionals.html">90 acl-2011-Crowdsourcing Translation: Professional Quality from Non-Professionals</a></p>
<p>14 0.063563749 <a title="17-tfidf-14" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>15 0.063528031 <a title="17-tfidf-15" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>16 0.063129306 <a title="17-tfidf-16" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>17 0.062977791 <a title="17-tfidf-17" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>18 0.062353399 <a title="17-tfidf-18" href="./acl-2011-Improving_Decoding_Generalization_for_Tree-to-String_Translation.html">166 acl-2011-Improving Decoding Generalization for Tree-to-String Translation</a></p>
<p>19 0.061778605 <a title="17-tfidf-19" href="./acl-2011-Structural_Topic_Model_for_Latent_Topical_Structure_Analysis.html">287 acl-2011-Structural Topic Model for Latent Topical Structure Analysis</a></p>
<p>20 0.060862362 <a title="17-tfidf-20" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.192), (1, -0.064), (2, 0.004), (3, 0.006), (4, -0.0), (5, -0.033), (6, -0.049), (7, 0.021), (8, -0.001), (9, 0.057), (10, -0.014), (11, 0.004), (12, 0.024), (13, 0.007), (14, 0.029), (15, 0.03), (16, -0.076), (17, 0.036), (18, -0.017), (19, 0.026), (20, 0.045), (21, 0.0), (22, 0.081), (23, -0.014), (24, 0.042), (25, -0.065), (26, -0.004), (27, -0.024), (28, -0.037), (29, 0.011), (30, -0.02), (31, 0.008), (32, 0.016), (33, 0.021), (34, 0.034), (35, -0.043), (36, 0.064), (37, -0.063), (38, 0.061), (39, -0.051), (40, 0.033), (41, 0.036), (42, 0.077), (43, 0.006), (44, 0.009), (45, -0.032), (46, -0.044), (47, 0.009), (48, -0.05), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91544628 <a title="17-lsi-1" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>Author: Ming Tan ; Wenli Zhou ; Lei Zheng ; Shaojun Wang</p><p>Abstract: This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-the- art parsing-based machine translation system.</p><p>2 0.75011206 <a title="17-lsi-2" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: In this paper, with a belief that a language model that embraces a larger context provides better prediction ability, we present two extensions to standard n-gram language models in statistical machine translation: a backward language model that augments the conventional forward language model, and a mutual information trigger model which captures long-distance dependencies that go beyond the scope of standard n-gram language models. We integrate the two proposed models into phrase-based statistical machine translation and conduct experiments on large-scale training data to investigate their effectiveness. Our experimental results show that both models are able to significantly improve transla- , tion quality and collectively achieve up to 1 BLEU point over a competitive baseline.</p><p>3 0.73772126 <a title="17-lsi-3" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>4 0.68926144 <a title="17-lsi-4" href="./acl-2011-Generalized_Interpolation_in_Decision_Tree_LM.html">142 acl-2011-Generalized Interpolation in Decision Tree LM</a></p>
<p>Author: Denis Filimonov ; Mary Harper</p><p>Abstract: In the face of sparsity, statistical models are often interpolated with lower order (backoff) models, particularly in Language Modeling. In this paper, we argue that there is a relation between the higher order and the backoff model that must be satisfied in order for the interpolation to be effective. We show that in n-gram models, the relation is trivially held, but in models that allow arbitrary clustering of context (such as decision tree models), this relation is generally not satisfied. Based on this insight, we also propose a generalization of linear interpolation which significantly improves the performance of a decision tree language model.</p><p>5 0.68072736 <a title="17-lsi-5" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>Author: Lane Schwartz ; Chris Callison-Burch ; William Schuler ; Stephen Wu</p><p>Abstract: This paper describes a novel technique for incorporating syntactic knowledge into phrasebased machine translation through incremental syntactic parsing. Bottom-up and topdown parsers typically require a completed string as input. This requirement makes it difficult to incorporate them into phrase-based translation, which generates partial hypothesized translations from left-to-right. Incremental syntactic language models score sentences in a similar left-to-right fashion, and are therefore a good mechanism for incorporat- ing syntax into phrase-based translation. We give a formal definition of one such lineartime syntactic language model, detail its relation to phrase-based decoding, and integrate the model with the Moses phrase-based translation system. We present empirical results on a constrained Urdu-English translation task that demonstrate a significant BLEU score improvement and a large decrease in perplexity.</p><p>6 0.64332283 <a title="17-lsi-6" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>7 0.64262551 <a title="17-lsi-7" href="./acl-2011-A_Scalable_Probabilistic_Classifier_for_Language_Modeling.html">24 acl-2011-A Scalable Probabilistic Classifier for Language Modeling</a></p>
<p>8 0.63725901 <a title="17-lsi-8" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>9 0.63362336 <a title="17-lsi-9" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>10 0.62683266 <a title="17-lsi-10" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>11 0.60520095 <a title="17-lsi-11" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>12 0.60378373 <a title="17-lsi-12" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>13 0.60227239 <a title="17-lsi-13" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<p>14 0.59055585 <a title="17-lsi-14" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>15 0.58393216 <a title="17-lsi-15" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>16 0.57945418 <a title="17-lsi-16" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>17 0.56480485 <a title="17-lsi-17" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>18 0.56222552 <a title="17-lsi-18" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>19 0.56130624 <a title="17-lsi-19" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>20 0.55496228 <a title="17-lsi-20" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.022), (17, 0.059), (26, 0.04), (37, 0.096), (39, 0.062), (41, 0.045), (55, 0.054), (59, 0.037), (72, 0.033), (91, 0.038), (96, 0.127), (99, 0.292)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75645751 <a title="17-lda-1" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>Author: Ming Tan ; Wenli Zhou ; Lei Zheng ; Shaojun Wang</p><p>Abstract: This paper presents an attempt at building a large scale distributed composite language model that simultaneously accounts for local word lexical information, mid-range sentence syntactic structure, and long-span document semantic content under a directed Markov random field paradigm. The composite language model has been trained by performing a convergent N-best list approximate EM algorithm that has linear time complexity and a followup EM algorithm to improve word prediction power on corpora with up to a billion tokens and stored on a supercomputer. The large scale distributed composite language model gives drastic perplexity reduction over ngrams and achieves significantly better translation quality measured by the BLEU score and “readability” when applied to the task of re-ranking the N-best list from a state-of-the- art parsing-based machine translation system.</p><p>2 0.66987091 <a title="17-lda-2" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>Author: S.R.K Branavan ; David Silver ; Regina Barzilay</p><p>Abstract: This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built- . in AI of Civilization II. 1</p><p>3 0.55379206 <a title="17-lda-3" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>Author: Shane Bergsma ; David Yarowsky ; Kenneth Church</p><p>Abstract: Resolving coordination ambiguity is a classic hard problem. This paper looks at coordination disambiguation in complex noun phrases (NPs). Parsers trained on the Penn Treebank are reporting impressive numbers these days, but they don’t do very well on this problem (79%). We explore systems trained using three types of corpora: (1) annotated (e.g. the Penn Treebank), (2) bitexts (e.g. Europarl), and (3) unannotated monolingual (e.g. Google N-grams). Size matters: (1) is a million words, (2) is potentially billions of words and (3) is potentially trillions of words. The unannotated monolingual data is helpful when the ambiguity can be resolved through associations among the lexical items. The bilingual data is helpful when the ambiguity can be resolved by the order of words in the translation. We train separate classifiers with monolingual and bilingual features and iteratively improve them via achieves data and pervised tations. co-training. The co-trained classifier close to 96% accuracy on Treebank makes 20% fewer errors than a susystem trained with Treebank anno-</p><p>4 0.55290616 <a title="17-lda-4" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>5 0.55227351 <a title="17-lda-5" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>Author: Yee Seng Chan ; Dan Roth</p><p>Abstract: In this paper, we observe that there exists a second dimension to the relation extraction (RE) problem that is orthogonal to the relation type dimension. We show that most of these second dimensional structures are relatively constrained and not difficult to identify. We propose a novel algorithmic approach to RE that starts by first identifying these structures and then, within these, identifying the semantic type of the relation. In the real RE problem where relation arguments need to be identified, exploiting these structures also allows reducing pipelined propagated errors. We show that this RE framework provides significant improvement in RE performance.</p><p>6 0.55201602 <a title="17-lda-6" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>7 0.55135781 <a title="17-lda-7" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>8 0.55090916 <a title="17-lda-8" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>9 0.55058652 <a title="17-lda-9" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>10 0.54959065 <a title="17-lda-10" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>11 0.54951531 <a title="17-lda-11" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<p>12 0.54917574 <a title="17-lda-12" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>13 0.54892886 <a title="17-lda-13" href="./acl-2011-Ordering_Prenominal_Modifiers_with_a_Reranking_Approach.html">237 acl-2011-Ordering Prenominal Modifiers with a Reranking Approach</a></p>
<p>14 0.54890716 <a title="17-lda-14" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>15 0.54818916 <a title="17-lda-15" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>16 0.54817355 <a title="17-lda-16" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>17 0.54754639 <a title="17-lda-17" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>18 0.54699588 <a title="17-lda-18" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>19 0.54668081 <a title="17-lda-19" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>20 0.54654622 <a title="17-lda-20" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
