<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-13" href="#">acl2011-13</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</h1>
<br/><p>Source: <a title="acl-2011-13-pdf" href="http://aclweb.org/anthology//P/P11/P11-1091.pdf">pdf</a></p><p>Author: Zhuowei Bao ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. For that task, this paper presents an algorithm that is based on statistics from the corpus data (rather than the query log). This algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability (e.g., email subject vs. email body), and can handle complex spelling errors like splitting and merging of words. An experimental study shows the superiority of the algorithm over existing alternatives in the email domain.</p><p>Reference: <a title="acl-2011-13-reference" href="../acl2011_reference/acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. [sent-7, score-0.772]
</p><p>2 email body), and can handle complex spelling errors like splitting and merging of words. [sent-12, score-0.659]
</p><p>3 An experimental study shows the superiority of the algorithm over existing alternatives in the email domain. [sent-13, score-0.322]
</p><p>4 1 Introduction An abundance of applications require spelling correction, which (at the high level) is the following  task. [sent-14, score-0.272]
</p><p>5 The user intends to type a chunk q of text, but types instead the chunk s that contains spelling errors (which we discuss in detail later), due to uncareful typing or lack of knowledge of the exact spelling of q. [sent-15, score-0.634]
</p><p>6 The focus of this paper is on the special case where q is a search query, and where s instead of q is submitted to a search engine (with the goal of retrieving documents that match the search query q). [sent-18, score-0.35]
</p><p>7 Spelling correction for search queries is important, because a significant portion of posed queries may be misspelled (Cucerzan and Brill, 2004). [sent-19, score-0.731]
</p><p>8 Effective 905 spelling correction has a major effect on the experience and effort of the user, who is otherwise required to ensure the exact spellings of her queries. [sent-20, score-0.45]
</p><p>9 Furthermore, it is critical when the exact spelling is unknown (e. [sent-21, score-0.272]
</p><p>10 1 Spelling Errors The more common and studied type of spelling error  is word-to-word error: a single word w is misspelled into another single word w0. [sent-25, score-0.462]
</p><p>11 The specific spelling errors involved include omission of a character (e. [sent-26, score-0.3]
</p><p>12 Addi→tiown al ( common spelling errors are splitting of a word, and merging two (or more) words: • att ach ment → att achment • emai latt achment → emai l att achment P•art of our experiments, as well as most of our examples, are from the domain of (personal) email search. [sent-36, score-1.35]
</p><p>13 An email from the Enron email collection (Klimt and Yang, 2004) is shown in Figure 1. [sent-37, score-0.591]
</p><p>14 s adeep kohl i exce l at ach ment l s andeep kohl i exce l att achment  →  (1)  In this example, correction entails fixing sadeep, splitting exce l at ach, fixing exce l merging l l, at ach ment, and fixing at achment . [sent-39, score-1.956]
</p><p>15 Beyond the  complexity of errors, this example also illustrates other challenges in spelling correction for search. [sent-40, score-0.45]
</p><p>16 We need to identify not only that sadeep is misspelled, but also that kohli is correctly spelled. [sent-41, score-0.279]
</p><p>17 Ac s2s0o1ci1a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 905–914,  For example, in kohli coupons the user may very well mean kohl s coupons if Sandeep Kohli has nothing to do with coupons (in contrast to the store chain Kohl’s). [sent-45, score-0.621]
</p><p>18 A similar example is the word nail, which is a legitimate English word, but in the con-  text of email the query nail box is likely to be a misspelling of mail box (unless nail boxes are indeed relevant to the user’s email collection). [sent-46, score-0.871]
</p><p>19 Finally, while the word kohli is relevant to some email users (e. [sent-47, score-0.509]
</p><p>20 2 Domain Knowledge The common approach to spelling correction utilizes statistical information (Kernighan et al. [sent-51, score-0.45]
</p><p>21 As another example, the fact that the expression sandeep kohli is frequent in the domain increases our confidence in s adeep kohl i → s andeep kohl i(rather than, e. [sent-57, score-0.919]
</p><p>22 One can further note that, in→ email search, the fact that Sandeep Kohli sent multiple excel attachments in-  creases our confidence in excell → excel. [sent-60, score-0.353]
</p><p>23 However, while query logs are abundant in the context of Web search, in many 906 other search applications (e. [sent-65, score-0.249]
</p><p>24 email search, desktop search, and even small-enterprise search) query logs are too scarce to provide statistical information that is sufficient for effective spelling correction. [sent-67, score-0.76]
</p><p>25 Even an email provider of a massive scale (such as GMail) may need to rely on the (possibly tiny) query log of the single user at hand, due to privacy or security concerns; moreover, as noted earlier about kohli, the statistics of one user may be relevant to one user, while irrelevant to another. [sent-68, score-0.556]
</p><p>26 The focus of this paper is on spelling correction for search applications like the above, where querylog analysis is impossible or undesirable (with email search being a prominent example). [sent-69, score-0.867]
</p><p>27 As shown in Figure 1,  the corpus data may very well contain misspelled words (like query logs do), and such noise is a part of the challenge. [sent-75, score-0.344]
</p><p>28 Relying on the corpus has been shown to be successful in spelling correction for text cleaning (Schierle et al. [sent-76, score-0.45]
</p><p>29 3  Contribution and Outline  As said above, our goal is to devise spelling correction that relies on the corpus. [sent-80, score-0.45]
</p><p>30 The major question is how to effectively exploit that information while addressing the various types of spelling errors such as those discussed in Section 1. [sent-85, score-0.3]
</p><p>31 2  Spelling-Correction Algorithm  In this section, we describe our algorithm for spelling correction. [sent-91, score-0.272]
</p><p>32 Recall that given a search query s of a user who intends to phrase q, the goal is to find q. [sent-92, score-0.278]
</p><p>33 For example, in email search such a document is an email with a title, a body, one or more recipients, and so on. [sent-94, score-0.633]
</p><p>34 As conventional in spelling correction, we devise a scoring function scoreD (r | s) that estimates our confidence in r being the (cro r|r se)cti tohnat o esf s (i. [sent-95, score-0.272]
</p><p>35 1 Word-Level Correction We first handle a restriction of our problem, where the search query is a single word w (rather than a general sequence s of words). [sent-112, score-0.216]
</p><p>36 b Tinhaetio funn octfi tohne s scoreD(w0 | w)  =def X  af  scoref  (w0 | w) :  · scoref(w0 | w)  f∈XWFD  As a concrete example, the features ofWFD we used in the email domain are listed in Table 1; the resulting scoref (w0 | w) is in the spirit of the noisy channel model (Kernighan est i al. [sent-124, score-0.429]
</p><p>37 Rather than manually tuning the parameters af, we learned them using the well known Support Vector Machine, abbreviated SVM (Cortes and Vapnik, 1995), as also done by Schaback and Li (2007) for spelling correction. [sent-127, score-0.272]
</p><p>38 2 Query-Level Correction: MaxPaths We now describe our algorithm, MaxPaths, for spelling correction. [sent-132, score-0.272]
</p><p>39 The input is a (possibly mis-  spelled) search query s = s1, . [sent-133, score-0.216]
</p><p>40 mFo +r tw 1}o, w woerd dse w1, w2 ∈ Σ∗, the word w1w2 ∈ Σ∗ is obtained by concatenating w1 and scoreD  No∈te Σthat for the search query s = s1, . [sent-146, score-0.216]
</p><p>41 For example, if s1 = sadeep and s2 = kohl i, cth. [sent-151, score-0.266]
</p><p>42 e nF s corresponds to the query sadeep kohli while bsc is the word sadeepkohli; furthermore, bsc [1,7) = sadeep. [sent-152, score-0.686]
</p><p>43 1  Plausible Tokens  To support merging and splitting, we first identify the possible tokens of the given query s. [sent-156, score-0.212]
</p><p>44 For example, in excellatach ment we would like to identify excel land atach ment as tokens, since those are indeed the tokens that the user has in mind. [sent-157, score-0.297]
</p><p>45 Intuitively, w is plausible fifo irt sios an original token of s, or we have a high confidence in our word-level suggestion to correct w (note that the suggested correction for w can be w itself). [sent-161, score-0.339]
</p><p>46 Observe that a plausible token is not necessarily induced by any plausible tokenization; in that case, the plausible token is useless to us. [sent-178, score-0.265]
</p><p>47 As a concrete example, for the query excellatach ment, the sp-tokens in our implementation include excel latach, ment, exce l l,and at achment . [sent-180, score-0.513]
</p><p>48 2 Correction Graph In the next step (line 2 in Algorithm 1), we construct the correction graph, which we denote by GD (s) . [sent-184, score-0.21]
</p><p>49 This example shows the actual execution of our implementation within email search, where s is the query s adeep kohl i exce l at ach ment ; for l clarity of presentation, we omitted a few sp-tokens and suggested corrections. [sent-189, score-0.982]
</p><p>50 Observe that some of the corrections in the table are actually misspelled words (as those naturally occur in the corpus). [sent-190, score-0.188]
</p><p>51 a eno gdrea hw1, w10i to a tnaoidnes hw2, w20i eifd w2 immediately foodlelow hws w1 ini bqc ; nino doeth hewr words, GD (s) has an edge from hw1, w10i  iton hw2, w20i wrdshe,n Geve(rs )th heares aenxis etd ignedi cfreosm i, j and k, tsouc hhw that w1 = bsc [i,j) haenrde w2 = bsc [j,k) . [sent-194, score-0.258]
</p><p>52 jsa nisd ae nep de pk o h lie e x xc ce el plte n ctea t mst ae tacne acht che m e dn t Figure 2: The graph GD  (s)  For example, Figure 2 shows GD (s) for the query s adeep kohl iu exce l at ach ment, with l the sp-tokens w and the sets topD (w) being those of Table 2. [sent-196, score-0.683]
</p><p>53 sA),n easy wobserviat hiaosn niso that, sgionicneg we gceosns inid Ger only strongly plausible tokens, if P is full then w1 · · · wk = bsc ; in that case, the se-  quence w10 , . [sent-203, score-0.222]
</p><p>54 , wk0 ·isw a suggestion fhoart spelling correction, and we denote it by crc(P) . [sent-206, score-0.368]
</p><p>55 The corrections crc(Pi), for iG = 1 2, are j aideep kohl i exce l , lent ment and s andeep kohl i exce l att achement , respectively. [sent-208, score-0.994]
</p><p>56 As an example, in the email domain we would like crl(kohli, excel) to be high if Kohli sent many emails with excel attachments. [sent-227, score-0.436]
</p><p>57 An example of such a node is in the grey box of Figure 2, where sandeep kohli is a bigram. [sent-245, score-0.39]
</p><p>58 Observe that sandeep kohli may be deemed an important bi910 gram because it occurs as a sender of an email, and not necessarily because it is frequent. [sent-246, score-0.486]
</p><p>59 For example, consider the query text ile import expert, and assume that both text i import le and import export (with an “o” rather than an “e”) are frequent bigrams. [sent-248, score-0.493]
</p><p>60 However, only one of these two options can hold true, and we would like t ext i import export to be rewarded only le once—for the bigram import export. [sent-251, score-0.247]
</p><p>61 This is achieved in our approach, since a full path in GD (s)  may cvoendta inin o euirth aeprp a onaocdhe, fsoinrc text ile import or a node for import export, but it cannot contain nodes for both of these bigrams. [sent-252, score-0.249]
</p><p>62 The focus of our experimental study is on personal email search; later on (Section 3. [sent-262, score-0.283]
</p><p>63 Our dataset (for the email domain) is obtained from the Enron email collection (Bekkerman et al. [sent-266, score-0.591]
</p><p>64 We refer to the three email collections by the last names of their owners: Farmer, Kaminski and Kitchen. [sent-269, score-0.283]
</p><p>65 We gave each 10-email set to a unique human subject that was asked to phrase two search queries for each email: one for the entire email content (general query), and the other for the From and X-From fields (sender query). [sent-273, score-0.512]
</p><p>66 , spelling correction), and the collected queries have no spelling errors. [sent-280, score-0.706]
</p><p>67 For generating spelling errors, we implemented a typo generator. [sent-281, score-0.325]
</p><p>68 1 This generator extends an online typo generator (Seobook, 2010) that produces a variety of spelling errors, including skipped letter, doubled letter, reversed letter, skipped space (merge), missed key and inserted key; in addition, our generator produces inserted space (split). [sent-282, score-0.436]
</p><p>69 For each collected query (and for each considered value of p) we generated 5 misspelled queries, and thereby obtained 250 instances of misspelled general queries and 250 instances of misspelled sender queries. [sent-284, score-0.95]
</p><p>70 The second alternative is  the spelling correction provided by Google. [sent-288, score-0.45]
</p><p>71 However, we were not able to do so with Google, as we used remote access via its Java API (Google, 2010); hence, the Google tool is un1The queries and our typo generator are publicly available at https : / / dbappserv . [sent-290, score-0.252]
</p><p>72 Hence, by default accuracy means  the number of properly corrected queries (within the top-k suggestions) divided by the number of the misspelled queries. [sent-301, score-0.354]
</p><p>73 Since MaxPaths and WordWise involve parameter learning (SVM), the results for them are consistently obtained by performing 5-folder cross validation over each collection of misspelled queries. [sent-304, score-0.187]
</p><p>74 Figure 4(b) shows the accuracy of top-k suggestions for sender queries. [sent-313, score-0.238]
</p><p>75 Overall, the results are similar to those of Figure 4(a), except that top-1 of both  WordWise and MaxPaths has a higher accuracy in sender queries than in general queries. [sent-314, score-0.345]
</p><p>76 This is due to the fact that the dictionaries of person names and email addresses extracted from the X-From and From fields, respectively, provide strong features for the scoring function, since a sender query refers to these two fields. [sent-315, score-0.585]
</p><p>77 3 Impact of Error Probability We now study the impact of the complexity of spelling errors on our algorithm. [sent-318, score-0.3]
</p><p>78 We got similar results for the sender queries (and for each of the three users). [sent-324, score-0.315]
</p><p>79 , the email collection of one user) on a second corpus (e. [sent-330, score-0.308]
</p><p>80 , the email collection of another user), rather than learning the parameters again over the second corpus. [sent-332, score-0.308]
</p><p>81 Figures 5(a) and 5(b) show the accuracy of the top-1 suggestion for general queries and sender queries, respectively, with varying error probabilities. [sent-336, score-0.437]
</p><p>82 5  Accuracy for Correct Queries  Next, we study the accuracy on correct queries, where the task is to recognize the given query as correct by returning it as the top suggestion. [sent-341, score-0.204]
</p><p>83 Since Jazzy is based on edit distance, it almost always gives the input query as the top suggestion; the misses of Jazzy are for queries that contain a word that is not the corpus. [sent-344, score-0.363]
</p><p>84 Google (having no access to the domain) also performs well, partly because it returns the input query if no reasonable suggestion is found. [sent-346, score-0.213]
</p><p>85 6 Applicability to Large-Scale Site Search Up to now, our focus has been on email search,  which represents a restricted (closed) domain with specialized knowledge (e. [sent-348, score-0.323]
</p><p>86 We manually collected 1348 misspelled queries from the log of search issued against developerWorks (www . [sent-356, score-0.391]
</p><p>87 To facilitate the manual collection of these queries, we inspected each query with two or fewer search results, after applying a random permutation to those queries. [sent-359, score-0.241]
</p><p>88 , an email user) can be applied to sim913 100%  Google  Jazzy  WordWise  MaxPaths  824600000%%%%% Top 1  Top 3  Top 10  Figure 6: Accuracy for site search ilar domains (e. [sent-370, score-0.38]
</p><p>89 , other email users) with essentially no loss in performance (Section 3. [sent-372, score-0.283]
</p><p>90 Fourth, even when applied to large (open) domains, it achieves a comparable performance to the state-of-the-art Google spelling correction (Section 3. [sent-376, score-0.45]
</p><p>91 4  Conclusions  We presented the algorithm MaxPaths for spelling correction in domain-centric search. [sent-379, score-0.45]
</p><p>92 This algorithm relies primarily on corpus statistics and domain knowledge (rather than on query logs). [sent-380, score-0.189]
</p><p>93 It can handle a variety of spelling errors, and can incorporate different levels of spelling reliability among different parts of the corpus. [sent-381, score-0.544]
</p><p>94 Our experimental study demonstrates the superiority of MaxPaths over existing alternatives in the domain of email search, and indicates its effectiveness beyond that domain. [sent-382, score-0.362]
</p><p>95 For instance, if kohli is of type person, and phone is highly correlated with person instances, then phone is highly correlated with kohli even if the two words do not  frequently  co-occur. [sent-389, score-0.452]
</p><p>96 Learning a spelling error model from search query logs. [sent-395, score-0.516]
</p><p>97 Automatic categorization of email into folders: Benchmark experiments on Enron and Sri Corpora. [sent-402, score-0.283]
</p><p>98 A spelling correction program based on a noisy channel model. [sent-468, score-0.45]
</p><p>99 Exploring distributional similarity based models for query spelling correction. [sent-489, score-0.421]
</p><p>100 From spelling correction to text cleaning - using context information. [sent-522, score-0.45]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('maxpaths', 0.466), ('email', 0.283), ('spelling', 0.272), ('kohli', 0.226), ('kohl', 0.213), ('correction', 0.178), ('gd', 0.175), ('misspelled', 0.162), ('queries', 0.162), ('exce', 0.16), ('jazzy', 0.153), ('sender', 0.153), ('query', 0.149), ('bsc', 0.129), ('kaminski', 0.12), ('wordwise', 0.12), ('cd', 0.115), ('achment', 0.107), ('crl', 0.107), ('sandeep', 0.107), ('crc', 0.097), ('import', 0.097), ('scored', 0.096), ('toppathsd', 0.093), ('topd', 0.08), ('plausible', 0.071), ('excel', 0.07), ('farmer', 0.07), ('search', 0.067), ('adeep', 0.067), ('suggestion', 0.064), ('user', 0.062), ('paths', 0.061), ('att', 0.058), ('def', 0.058), ('ment', 0.057), ('suggestions', 0.055), ('andeep', 0.053), ('export', 0.053), ('sadeep', 0.053), ('schierle', 0.053), ('scoref', 0.053), ('typo', 0.053), ('ach', 0.053), ('google', 0.052), ('misspelling', 0.044), ('emails', 0.043), ('graph', 0.041), ('enron', 0.041), ('domain', 0.04), ('coupons', 0.04), ('kimelfeld', 0.04), ('klimt', 0.04), ('wfd', 0.04), ('whitespace', 0.04), ('merging', 0.039), ('alternatives', 0.039), ('generator', 0.037), ('splitting', 0.037), ('cucerzan', 0.036), ('kernighan', 0.035), ('logs', 0.033), ('node', 0.033), ('nail', 0.032), ('denote', 0.032), ('merges', 0.031), ('spelled', 0.03), ('site', 0.03), ('accuracy', 0.03), ('hw', 0.029), ('error', 0.028), ('errors', 0.028), ('edit', 0.027), ('aideep', 0.027), ('atachment', 0.027), ('bekkerman', 0.027), ('excellatach', 0.027), ('fagin', 0.027), ('lent', 0.027), ('mitton', 0.027), ('zhuowei', 0.027), ('fixing', 0.027), ('corrections', 0.026), ('token', 0.026), ('raghavan', 0.026), ('li', 0.025), ('top', 0.025), ('collection', 0.025), ('box', 0.024), ('splits', 0.024), ('tokens', 0.024), ('schaback', 0.023), ('almaden', 0.023), ('desktop', 0.023), ('emai', 0.023), ('hwk', 0.023), ('ibm', 0.023), ('www', 0.023), ('brill', 0.022), ('full', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000013 <a title="13-tfidf-1" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>Author: Zhuowei Bao ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. For that task, this paper presents an algorithm that is based on statistics from the corpus data (rather than the query log). This algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability (e.g., email subject vs. email body), and can handle complex spelling errors like splitting and merging of words. An experimental study shows the superiority of the algorithm over existing alternatives in the email domain.</p><p>2 0.21571964 <a title="13-tfidf-2" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>Author: Ziqi Wang ; Gu Xu ; Hang Li ; Ming Zhang</p><p>Abstract: This paper proposes a new method for approximate string search, specifically candidate generation in spelling error correction, which is a task as follows. Given a misspelled word, the system finds words in a dictionary, which are most “similar” to the misspelled word. The paper proposes a probabilistic approach to the task, which is both accurate and efficient. The approach includes the use of a log linear model, a method for training the model, and an algorithm for finding the top k candidates. The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word. The learning method employs the criterion in candidate generation as loss function. The retrieval algorithm is efficient and is guaranteed to find the optimal k candidates. Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings.</p><p>3 0.15019222 <a title="13-tfidf-3" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>Author: Michael Bendersky ; W. Bruce Croft ; David A. Smith</p><p>Abstract: W. Bruce Croft Dept. of Computer Science University of Massachusetts Amherst, MA cro ft @ c s .uma s s .edu David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@ c s .umas s .edu articles or web pages). As previous research shows, these differences severely limit the applicability of Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an impor- tant part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.</p><p>4 0.14962758 <a title="13-tfidf-4" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>Author: Y. Albert Park ; Roger Levy</p><p>Abstract: Automated grammar correction techniques have seen improvement over the years, but there is still much room for increased performance. Current correction techniques mainly focus on identifying and correcting a specific type of error, such as verb form misuse or preposition misuse, which restricts the corrections to a limited scope. We introduce a novel technique, based on a noisy channel model, which can utilize the whole sentence context to determine proper corrections. We show how to use the EM algorithm to learn the parameters of the noise model, using only a data set of erroneous sentences, given the proper language model. This frees us from the burden of acquiring a large corpora of corrected sentences. We also present a cheap and efficient way to provide automated evaluation re- sults for grammar corrections by using BLEU and METEOR, in contrast to the commonly used manual evaluations.</p><p>5 0.14897221 <a title="13-tfidf-5" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<p>Author: Bo Pang ; Ravi Kumar</p><p>Abstract: Web search is an information-seeking activity. Often times, this amounts to a user seeking answers to a question. However, queries, which encode user’s information need, are typically not expressed as full-length natural language sentences in particular, as questions. Rather, they consist of one or more text fragments. As humans become more searchengine-savvy, do natural-language questions still have a role to play in web search? Through a systematic, large-scale study, we find to our surprise that as time goes by, web users are more likely to use questions to express their search intent. —</p><p>6 0.13146828 <a title="13-tfidf-6" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>7 0.12707053 <a title="13-tfidf-7" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>8 0.12523954 <a title="13-tfidf-8" href="./acl-2011-Jigs_and_Lures%3A_Associating_Web_Queries_with_Structured_Entities.html">181 acl-2011-Jigs and Lures: Associating Web Queries with Structured Entities</a></p>
<p>9 0.11893678 <a title="13-tfidf-9" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>10 0.10924271 <a title="13-tfidf-10" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>11 0.080831692 <a title="13-tfidf-11" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>12 0.071216524 <a title="13-tfidf-12" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>13 0.068162568 <a title="13-tfidf-13" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>14 0.063776158 <a title="13-tfidf-14" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>15 0.063681975 <a title="13-tfidf-15" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>16 0.062901661 <a title="13-tfidf-16" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>17 0.058536798 <a title="13-tfidf-17" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>18 0.057044704 <a title="13-tfidf-18" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>19 0.056754321 <a title="13-tfidf-19" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>20 0.054446317 <a title="13-tfidf-20" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.134), (1, 0.042), (2, -0.066), (3, 0.045), (4, -0.116), (5, -0.162), (6, -0.039), (7, -0.188), (8, 0.085), (9, -0.005), (10, -0.015), (11, -0.024), (12, -0.007), (13, 0.072), (14, -0.021), (15, 0.092), (16, 0.005), (17, -0.03), (18, 0.002), (19, 0.029), (20, -0.057), (21, 0.025), (22, 0.042), (23, 0.005), (24, 0.008), (25, -0.045), (26, -0.016), (27, 0.019), (28, 0.007), (29, 0.021), (30, 0.03), (31, 0.01), (32, 0.033), (33, -0.03), (34, -0.078), (35, -0.052), (36, -0.047), (37, 0.121), (38, -0.06), (39, 0.017), (40, 0.053), (41, -0.001), (42, 0.044), (43, 0.077), (44, -0.026), (45, 0.015), (46, -0.073), (47, -0.039), (48, 0.05), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94911665 <a title="13-lsi-1" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>Author: Zhuowei Bao ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. For that task, this paper presents an algorithm that is based on statistics from the corpus data (rather than the query log). This algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability (e.g., email subject vs. email body), and can handle complex spelling errors like splitting and merging of words. An experimental study shows the superiority of the algorithm over existing alternatives in the email domain.</p><p>2 0.69726878 <a title="13-lsi-2" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>Author: Ziqi Wang ; Gu Xu ; Hang Li ; Ming Zhang</p><p>Abstract: This paper proposes a new method for approximate string search, specifically candidate generation in spelling error correction, which is a task as follows. Given a misspelled word, the system finds words in a dictionary, which are most “similar” to the misspelled word. The paper proposes a probabilistic approach to the task, which is both accurate and efficient. The approach includes the use of a log linear model, a method for training the model, and an algorithm for finding the top k candidates. The log linear model is defined as a conditional probability distribution of a corrected word and a rule set for the correction conditioned on the misspelled word. The learning method employs the criterion in candidate generation as loss function. The retrieval algorithm is efficient and is guaranteed to find the optimal k candidates. Experimental results on large scale data show that the proposed approach improves upon existing methods in terms of accuracy in different settings.</p><p>3 0.68348336 <a title="13-lsi-3" href="./acl-2011-Search_in_the_Lost_Sense_of_%22Query%22%3A_Question_Formulation_in_Web_Search_Queries_and_its_Temporal_Changes.html">271 acl-2011-Search in the Lost Sense of "Query": Question Formulation in Web Search Queries and its Temporal Changes</a></p>
<p>Author: Bo Pang ; Ravi Kumar</p><p>Abstract: Web search is an information-seeking activity. Often times, this amounts to a user seeking answers to a question. However, queries, which encode user’s information need, are typically not expressed as full-length natural language sentences in particular, as questions. Rather, they consist of one or more text fragments. As humans become more searchengine-savvy, do natural-language questions still have a role to play in web search? Through a systematic, large-scale study, we find to our surprise that as time goes by, web users are more likely to use questions to express their search intent. —</p><p>4 0.6806106 <a title="13-lsi-4" href="./acl-2011-Jigs_and_Lures%3A_Associating_Web_Queries_with_Structured_Entities.html">181 acl-2011-Jigs and Lures: Associating Web Queries with Structured Entities</a></p>
<p>Author: Patrick Pantel ; Ariel Fuxman</p><p>Abstract: We propose methods for estimating the probability that an entity from an entity database is associated with a web search query. Association is modeled using a query entity click graph, blending general query click logs with vertical query click logs. Smoothing techniques are proposed to address the inherent data sparsity in such graphs, including interpolation using a query synonymy model. A large-scale empirical analysis of the smoothing techniques, over a 2-year click graph collected from a commercial search engine, shows significant reductions in modeling error. The association models are then applied to the task of recommending products to web queries, by annotating queries with products from a large catalog and then mining query- product associations through web search session analysis. Experimental analysis shows that our smoothing techniques improve coverage while keeping precision stable, and overall, that our top-performing model affects 9% of general web queries with 94% precision.</p><p>5 0.67733866 <a title="13-lsi-5" href="./acl-2011-Joint_Annotation_of_Search_Queries.html">182 acl-2011-Joint Annotation of Search Queries</a></p>
<p>Author: Michael Bendersky ; W. Bruce Croft ; David A. Smith</p><p>Abstract: W. Bruce Croft Dept. of Computer Science University of Massachusetts Amherst, MA cro ft @ c s .uma s s .edu David A. Smith Dept. of Computer Science University of Massachusetts Amherst, MA dasmith@ c s .umas s .edu articles or web pages). As previous research shows, these differences severely limit the applicability of Marking up search queries with linguistic annotations such as part-of-speech tags, capitalization, and segmentation, is an impor- tant part of query processing and understanding in information retrieval systems. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing NLP tools. To address this challenge, we propose a probabilistic approach for performing joint query annotation. First, we derive a robust set of unsupervised independent annotations, using queries and pseudo-relevance feedback. Then, we stack additional classifiers on the independent annotations, and exploit the dependencies between them to further improve the accuracy, even with a very limited amount of available training data. We evaluate our method using a range of queries extracted from a web search log. Experimental results verify the effectiveness of our approach for both short keyword queries, and verbose natural language queries.</p><p>6 0.65081578 <a title="13-lsi-6" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<p>7 0.6173954 <a title="13-lsi-7" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>8 0.58416951 <a title="13-lsi-8" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>9 0.58352053 <a title="13-lsi-9" href="./acl-2011-Query_Weighting_for_Ranking_Model_Adaptation.html">256 acl-2011-Query Weighting for Ranking Model Adaptation</a></p>
<p>10 0.53238159 <a title="13-lsi-10" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>11 0.50346339 <a title="13-lsi-11" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>12 0.49364212 <a title="13-lsi-12" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>13 0.45007125 <a title="13-lsi-13" href="./acl-2011-A_Speech-based_Just-in-Time_Retrieval_System_using_Semantic_Search.html">26 acl-2011-A Speech-based Just-in-Time Retrieval System using Semantic Search</a></p>
<p>14 0.44722041 <a title="13-lsi-14" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>15 0.44496453 <a title="13-lsi-15" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>16 0.43974471 <a title="13-lsi-16" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>17 0.43582195 <a title="13-lsi-17" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>18 0.42423746 <a title="13-lsi-18" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<p>19 0.42320213 <a title="13-lsi-19" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>20 0.40722755 <a title="13-lsi-20" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.021), (16, 0.014), (17, 0.039), (26, 0.048), (36, 0.348), (37, 0.071), (39, 0.03), (41, 0.052), (55, 0.03), (59, 0.03), (72, 0.025), (91, 0.033), (96, 0.124), (97, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73576206 <a title="13-lda-1" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>Author: Zhuowei Bao ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Spelling correction for keyword-search queries is challenging in restricted domains such as personal email (or desktop) search, due to the scarcity of query logs, and due to the specialized nature of the domain. For that task, this paper presents an algorithm that is based on statistics from the corpus data (rather than the query log). This algorithm, which employs a simple graph-based approach, can incorporate different types of data sources with different levels of reliability (e.g., email subject vs. email body), and can handle complex spelling errors like splitting and merging of words. An experimental study shows the superiority of the algorithm over existing alternatives in the email domain.</p><p>2 0.63597679 <a title="13-lda-2" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<p>Author: Jui-Yu Weng ; Cheng-Lun Yang ; Bo-Nian Chen ; Yen-Kai Wang ; Shou-De Lin</p><p>Abstract: This paper presents a system to summarize a Microblog post and its responses with the goal to provide readers a more constructive and concise set of information for efficient digestion. We introduce a novel two-phase summarization scheme. In the first phase, the post plus its responses are classified into four categories based on the intention, interrogation, sharing, discussion and chat. For each type of post, in the second phase, we exploit different strategies, including opinion analysis, response pair identification, and response relevancy detection, to summarize and highlight critical information to display. This system provides an alternative thinking about machinesummarization: by utilizing AI approaches, computers are capable of constructing deeper and more user-friendly abstraction. 1</p><p>3 0.52270222 <a title="13-lda-3" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>Author: Yulan He ; Chenghua Lin ; Harith Alani</p><p>Abstract: Joint sentiment-topic (JST) model was previously proposed to detect sentiment and topic simultaneously from text. The only supervision required by JST model learning is domain-independent polarity word priors. In this paper, we modify the JST model by incorporating word polarity priors through modifying the topic-word Dirichlet priors. We study the polarity-bearing topics extracted by JST and show that by augmenting the original feature space with polarity-bearing topics, the in-domain supervised classifiers learned from augmented feature representation achieve the state-of-the-art performance of 95% on the movie review data and an average of 90% on the multi-domain sentiment dataset. Furthermore, using feature augmentation and selection according to the information gain criteria for cross-domain sentiment classification, our proposed approach performs either better or comparably compared to previous approaches. Nevertheless, our approach is much simpler and does not require difficult parameter tuning.</p><p>4 0.46308839 <a title="13-lda-4" href="./acl-2011-MemeTube%3A_A_Sentiment-based_Audiovisual_System_for_Analyzing_and_Displaying_Microblog_Messages.html">218 acl-2011-MemeTube: A Sentiment-based Audiovisual System for Analyzing and Displaying Microblog Messages</a></p>
<p>Author: Cheng-Te Li ; Chien-Yuan Wang ; Chien-Lin Tseng ; Shou-De Lin</p><p>Abstract: Micro-blogging services provide platforms for users to share their feelings and ideas on the move. In this paper, we present a search-based demonstration system, called MemeTube, to summarize the sentiments of microblog messages in an audiovisual manner. MemeTube provides three main functions: (1) recognizing the sentiments of messages (2) generating music melody automatically based on detected sentiments, and (3) produce an animation of real-time piano playing for audiovisual display. Our MemeTube system can be accessed via: http://mslab.csie.ntu.edu.tw/memetube/ .</p><p>5 0.45074528 <a title="13-lda-5" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>Author: Sujith Ravi ; Kevin Knight</p><p>Abstract: In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from nonparallel text.</p><p>6 0.44744545 <a title="13-lda-6" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>7 0.44192943 <a title="13-lda-7" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>8 0.44031414 <a title="13-lda-8" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<p>9 0.43949428 <a title="13-lda-9" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>10 0.43852913 <a title="13-lda-10" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>11 0.43838286 <a title="13-lda-11" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>12 0.43776411 <a title="13-lda-12" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>13 0.43775076 <a title="13-lda-13" href="./acl-2011-Language-independent_compound_splitting_with_morphological_operations.html">193 acl-2011-Language-independent compound splitting with morphological operations</a></p>
<p>14 0.43706957 <a title="13-lda-14" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>15 0.43698284 <a title="13-lda-15" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>16 0.43669224 <a title="13-lda-16" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>17 0.43648502 <a title="13-lda-17" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>18 0.43506414 <a title="13-lda-18" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>19 0.4347958 <a title="13-lda-19" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>20 0.43440944 <a title="13-lda-20" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
