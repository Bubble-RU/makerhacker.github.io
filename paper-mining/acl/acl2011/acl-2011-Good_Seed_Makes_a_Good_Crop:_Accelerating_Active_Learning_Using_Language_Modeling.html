<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-145" href="#">acl2011-145</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</h1>
<br/><p>Source: <a title="acl-2011-145-pdf" href="http://aclweb.org/anthology//P/P11/P11-2002.pdf">pdf</a></p><p>Author: Dmitriy Dligach ; Martha Palmer</p><p>Abstract: Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation.</p><p>Reference: <a title="acl-2011-145-reference" href="../acl2011_reference/acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 D l igach@  co l orado  Abstract Active Learning (AL) is typically initialized with a small seed of examples selected randomly. [sent-2, score-0.509]
</p><p>2 However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. [sent-3, score-0.14]
</p><p>3 Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. [sent-4, score-0.962]
</p><p>4 The evaluation is conducted in the context of word sense disambiguation. [sent-5, score-0.148]
</p><p>5 edu  Active learning (AL) (Settles, 2009) has become a popular research field due to its potential benefits: it can lead to drastic reductions in the amount of annotation that is necessary for training a highly accurate statistical classifier. [sent-7, score-0.031]
</p><p>6 Unlike in a random sampling approach, where unlabeled data is selected for annotation randomly, AL delegates the selection of unlabeled data to the classifier. [sent-8, score-0.544]
</p><p>7 In a typical AL setup, a classifier is trained on a small sample of the data (usually selected randomly), known as the seed examples. [sent-9, score-0.339]
</p><p>8 The classifier is subsequently applied to a pool of unlabeled data with the purpose of selecting additional examples that the classifier views as informative. [sent-10, score-0.564]
</p><p>9 The selected data is annotated and the cycle is repeated, allowing the learner to quickly refine the decision boundary between the classes. [sent-11, score-0.165]
</p><p>10 Unfortunately, AL is susceptible to a shortcoming known as the missed cluster effect (Sch u¨tze et al. [sent-12, score-0.131]
</p><p>11 , 2006) and its special case called the missed class 6 Martha Palmer Department of Linguistics University of Colorado at Boulder Martha . [sent-13, score-0.231]
</p><p>12 The missed cluster effect is a consequence of the fact that seed examples influence the direction the learner takes in its exploration of the instance space. [sent-17, score-0.641]
</p><p>13 Whenever the seed does not contain the examples of a certain cluster that is representative of a group of examples in the data, the learner may become overconfident about the class membership of this cluster (particularly if it lies far from the decision boundary). [sent-18, score-0.897]
</p><p>14 As a result, the learner spends a lot of time exploring one region of the instance space at the expense of missing another. [sent-19, score-0.091]
</p><p>15 This problem can become especially severe, when the class distribution in the data is skewed: a randomly selected seed may not adequately represent all the classes or even miss certain classes altogether. [sent-20, score-0.62]
</p><p>16 Consider a binary classification task where rare class examples constitute 5% of the data (a frequent scenario in e. [sent-21, score-0.787]
</p><p>17 If 10 examples are chosen randomly for seeding AL, the probability that none of the rare class examples will make it to the seed is 60% 1. [sent-24, score-1.405]
</p><p>18 Thus, there is a high probability that AL would stall, selecting only the examples of the predominant class over the course  of many iterations. [sent-25, score-0.52]
</p><p>19 At the same time, if we had a way to ensure that examples of the rare class were present in the seed, AL would be able to select the examples of both classes, efficiently clarifying the decision boundary and ultimately producing an accurate classifier. [sent-26, score-1.085]
</p><p>20 (2009) simulated these scenarios using manually constructed seed sets. [sent-28, score-0.22]
</p><p>21 They demonstrated that seeding AL with a data set that is artificially enriched with rare class examples indeed leads to a higher learning rate comparing to randomly 1Calculated using Binomial distribution Proceedings oPfo thretla 4n9dt,h O Arnengouanl, JMueneeti 1n9g- o2f4 t,h 2e01 A1ss. [sent-29, score-1.077]
</p><p>22 cc ia2t0i1o1n A fosrs Cocoiamtipounta fotiro Cnaolm Lpinugtauti sotnicasl: Lshinogrtupiastpicesrs, pages 6–10, sampled and predominant class enriched seeds. [sent-31, score-0.307]
</p><p>23 In this paper, we propose a simple automatic approach for selecting the seeds that are rich in the examples of the rare class. [sent-32, score-0.759]
</p><p>24 We then demonstrate that this approach to seed selection accelerates AL. [sent-33, score-0.378]
</p><p>25 2  Approach  Language Model (LM) Sampling is a simple unsu-  pervised technique for selecting unlabeled data that is enriched with rare class examples. [sent-35, score-0.832]
</p><p>26 LM sampling involves training a LM on a corpus of unlabeled candidate examples and selecting the examples with low LM probability. [sent-36, score-0.748]
</p><p>27 Dligach and Palmer (2009) used this technique in the context of word sense disambiguation and showed that rare sense examples tend to concentrate among the examples with low probability. [sent-37, score-1.224]
</p><p>28 Unfortunately these authors provided a limited evaluation of this technique: they looked at its effectiveness only at a single selection size. [sent-38, score-0.065]
</p><p>29 We provide a more convincing evaluation in which the effectiveness of this approach is examined for all sizes of the selected data. [sent-39, score-0.102]
</p><p>30 However, for datasets with a skewed distribution of classes, rare class examples may end up being underrepresented. [sent-41, score-0.861]
</p><p>31 We propose to use LM sampling for seed selection, which captures more examples of rare classes than random selection, thus leading to a faster learning progress. [sent-42, score-1.215]
</p><p>32 1 Data  For our evaluation, we needed a dataset that is characterized by a skewed class distribution. [sent-44, score-0.217]
</p><p>33 A large word sense annotated corpus has recently been released by the OntoNotes (Hovy et al. [sent-46, score-0.148]
</p><p>34 For clarity of evaluation, we identify a set of verbs that satisfy three criteria: (1) the number of senses is two, (2) the number of annotated examples is at least 100, (3) the proportion of the rare sense is at most 20%. [sent-49, score-0.823]
</p><p>35 The following 25 verbs satisfy these criteria: account, add, admit, allow, announce, approve, compare, demand, exist, expand, expect, explain, focus, include, invest, issue, point, promote, protect, receive, remain, re7  place, strengthen, wait, wonder. [sent-50, score-0.031]
</p><p>36 The average number of examples for these verbs is 232. [sent-51, score-0.194]
</p><p>37 In supervised word sense disambiguation, a single model per word is typically trained and that is the approach we take. [sent-52, score-0.148]
</p><p>38 Thus, we conduct our evaluation using 25 different data sets. [sent-53, score-0.054]
</p><p>39 We report the averages across these 25 data sets. [sent-54, score-0.049]
</p><p>40 In our evaluation, we use a state-of-theart word sense disambiguation system (Dligach and Palmer, 2008), that utilizes rich linguistic features to  capture the contexts of ambiguous words. [sent-55, score-0.198]
</p><p>41 2  Rare Sense Retrieval  The success of our approach to seeding AL hinges on the ability of LM sampling to discover rare class examples better than random sampling. [sent-57, score-1.252]
</p><p>42 In this experiment, we demonstrate that LM sampling outperforms random sampling for every selection size. [sent-58, score-0.595]
</p><p>43 For each verb we conduct an experiment in which we select the instances of this verb using both methods. [sent-59, score-0.309]
</p><p>44 We measure the recall of the rare sense, which we calculate as the ratio of the number of selected rare sense examples to the total number of rare sense examples for this verb. [sent-60, score-2.198]
</p><p>45 For each verb, we compute the LM probability for each instance of this verb and sort the instances by probability. [sent-62, score-0.107]
</p><p>46 In the course of the experiment, we select one example with the smallest probability and move it to the set of selected examples. [sent-63, score-0.136]
</p><p>47 We then measure the recall of the rare sense for the selected examples. [sent-64, score-0.729]
</p><p>48 We con-  tinue in this fashion until all the examples have been selected. [sent-65, score-0.194]
</p><p>49 We use random sampling as a baseline, which is obtained by continuously selecting a single example randomly. [sent-66, score-0.357]
</p><p>50 We continue until all the examples have been selected. [sent-67, score-0.194]
</p><p>51 At the end of the experiment, we have produced two recall curves, which measure the recall of the rare sense retrieval for this verb at various sizes of selected data. [sent-68, score-0.925]
</p><p>52 Due to the lack of space, we do not show the plots that display these curves for individual verbs. [sent-69, score-0.14]
</p><p>53 Instead, in Figure 1 we display the curves that are averaged across all verbs. [sent-70, score-0.14]
</p><p>54 At every selection size, LM sampling results in a higher recall of the rare sense. [sent-71, score-0.827]
</p><p>55 The average difference across all selection sizes is 11%. [sent-72, score-0.101]
</p><p>56 Figure 1: Average recall of rare sense retrieval for LM and random sampling by relative size of training set 3. [sent-73, score-1.011]
</p><p>57 3 Classic and Selectively Seeded AL In this experiment, we seed AL using LM sampling and compare how this selectively seeded AL performs in comparison with classic (randomly-seeded) AL. [sent-74, score-0.901]
</p><p>58 Our experimental setup is typical for an active learning study. [sent-75, score-0.098]
</p><p>59 We split the set of annotated examples for a verb into 90% and 10% parts. [sent-76, score-0.255]
</p><p>60 The 90% part is used as a pool of unlabeled data. [sent-77, score-0.188]
</p><p>61 We begin classic AL by randomly selecting 10% of the examples from the pool to use as seeds. [sent-79, score-0.632]
</p><p>62 We then repeatedly apply the model to the remaining examples in the pool: on each iteration of AL, we draw a single most informative example from the pool. [sent-81, score-0.194]
</p><p>63 The informativeness is estimated using prediction margin (Schein and Ungar, 2007), which is computed as |P(c1 |x) − P(c2 |x) |, where c1 and c2 are the two |mPo(sct probable (ccla|sxse)s|, o wf example x according to the model. [sent-82, score-0.029]
</p><p>64 The selected example is moved to the training set. [sent-83, score-0.066]
</p><p>65 On each iteration, we also keep track of how accurately the current model classifies the held out test set. [sent-84, score-0.073]
</p><p>66 In parallel, we conduct a selectively seeded AL experiment that is identical to the classic one but with one crucial difference: instead of selecting the seed examples randomly, we select them using LM sampling by identifying 10% of the examples from the pool with the smallest LM probability. [sent-85, score-1.664]
</p><p>67 We also produce a random sampling curve to be used as a baseline. [sent-86, score-0.355]
</p><p>68 At the end of this experiment we have ob8  tained three learning curves: for classic AL, for selectively seeded AL, and for the random sampling baseline. [sent-87, score-0.797]
</p><p>69 The final learning curves for each verb are produced by averaging the learning curves from ten different trials. [sent-88, score-0.281]
</p><p>70 Figure 2 presents the average accuracy of selectively seeded AL (top curve), classic AL (middle curve) and the random sampling baseline (bottom curve) at various fractions of the total size of the training set. [sent-89, score-0.809]
</p><p>71 The size of zero corresponds to a training set consisting only of the seed examples. [sent-90, score-0.253]
</p><p>72 The size of one corresponds to a training set consisting of all the examples in the pool labeled. [sent-91, score-0.349]
</p><p>73 The accuracy at a given size was averaged across all 25 verbs. [sent-92, score-0.065]
</p><p>74 It is clear that LM-seeded AL accelerates learning: it reaches the same performance as classic AL with less training data. [sent-93, score-0.312]
</p><p>75 LM-seeded AL also reaches a higher classification accuracy (if stopped at its peak). [sent-94, score-0.122]
</p><p>76 The difference between the classic and LM-seeded curves is statistically significant (p = 0. [sent-96, score-0.299]
</p><p>77 4 Why LM Seeding Produces Better Results For random sampling, the system achieves its best accuracy, 94. [sent-101, score-0.063]
</p><p>78 4%, when the entire pool of unlabeled examples is labeled. [sent-102, score-0.382]
</p><p>79 The goal of a typical AL study is to demonstrate that the same accuracy can be 2We compute the average area under the curve for each type of AL and use Wilcoxon signed rank test to test whether the difference between the averages is significant. [sent-103, score-0.186]
</p><p>80 For example, in our case, classic AL reaches the best random sampling accuracy with only about 5% of the data. [sent-105, score-0.563]
</p><p>81 However, it is interesting to notice that LM-seeded AL actually reaches a higher accuracy, 95%, during early stages of learning (at 15% of the total training set size). [sent-106, score-0.09]
</p><p>82 We believe this phenomenon takes place due to overfitting the predominant class: as the model receives new data (and therefore more and more examples of the predominant class), it begins to mislabel more and more examples of the rare class. [sent-107, score-1.117]
</p><p>83 However, instead of measuring the accuracy on the test set, we resort to different metrics that reflect how accurately the classifier labels the instances ofthe rare class in the held out test set. [sent-113, score-0.797]
</p><p>84 These metrics are the recall and precision for the rare class. [sent-114, score-0.515]
</p><p>85 Recall is the ratio of the correctly labeled examples of the rare class and the total number of instances of  the rare class. [sent-115, score-1.35]
</p><p>86 Precision is the ratio of the correctly labeled examples of the rare class and the number of instances labeled as that class. [sent-116, score-0.934]
</p><p>87 Figure 3: Rare sense classification recall Observe that for LM-seeded AL, the recall peaks at first and begins to decline later. [sent-118, score-0.314]
</p><p>88 Thus the classifier makes progressively more errors on the rare class as more labeled examples are being received. [sent-119, score-0.874]
</p><p>89 9  Figure 4: Rare sense classification precision This is consistent with our hypothesis that the classifier overfits the predominant class. [sent-120, score-0.308]
</p><p>90 When all the data is labeled, the recall decreases from about 13% to only 7%, an almost 50% drop. [sent-121, score-0.065]
</p><p>91 The reason that the system achieved a higher level of recall at first is due to the fact that AL was seeded with LM selected data, which has a higher content of rare classes (as we demonstrated in the first experiment). [sent-122, score-0.87]
</p><p>92 The avail-  ability of the extra examples of the rare class allows the classifier to label the instances of this class in the test set more accurately, which in turn boosts the overall accuracy. [sent-123, score-1.029]
</p><p>93 4  Conclusion and Future Work  We introduced a novel approach to seeding AL, in which the seeds are selected from the examples with low LM probability. [sent-124, score-0.452]
</p><p>94 This approach selects more rare class examples than random sampling, resulting in more rapid learning and, more importantly, leading to a classifier that performs better on rare class examples. [sent-125, score-1.496]
</p><p>95 As a consequence of this, the overall classification accuracy is higher than that for classic AL. [sent-126, score-0.286]
</p><p>96 This should result in better performance on the rare classes, which is currently still low. [sent-128, score-0.45]
</p><p>97 We also plan to experiment with other unsupervised techniques, such as clustering and outlier detection, that can lead to better retrieval of rare classes. [sent-129, score-0.537]
</p><p>98 An empirical study of the behavior of active learning for word sense disambiguation. [sent-136, score-0.246]
</p><p>99 Addressing the curse of  imbalanced training sets: one-sided selection. [sent-164, score-0.029]
</p><p>100 On proper unit selection in active learning: co-selection effects for named entity recognition. [sent-197, score-0.163]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rare', 0.45), ('al', 0.323), ('lm', 0.228), ('seed', 0.22), ('sampling', 0.218), ('examples', 0.194), ('classic', 0.189), ('seeded', 0.161), ('seeding', 0.153), ('sense', 0.148), ('class', 0.143), ('dligach', 0.143), ('pool', 0.122), ('selectively', 0.113), ('curves', 0.11), ('predominant', 0.107), ('active', 0.098), ('ontonotes', 0.094), ('tomanek', 0.093), ('missed', 0.088), ('schein', 0.086), ('palmer', 0.086), ('martha', 0.08), ('dmitriy', 0.076), ('selecting', 0.076), ('curve', 0.074), ('skewed', 0.074), ('kubat', 0.07), ('classes', 0.07), ('unlabeled', 0.066), ('selected', 0.066), ('selection', 0.065), ('recall', 0.065), ('random', 0.063), ('ungar', 0.062), ('accelerates', 0.062), ('reaches', 0.061), ('verb', 0.061), ('learner', 0.06), ('weischedel', 0.057), ('enriched', 0.057), ('weiss', 0.054), ('conduct', 0.054), ('classifier', 0.053), ('experiment', 0.053), ('randomly', 0.051), ('disambiguation', 0.05), ('averages', 0.049), ('morristown', 0.048), ('colorado', 0.047), ('instances', 0.046), ('sch', 0.045), ('nj', 0.043), ('cluster', 0.043), ('hovy', 0.042), ('technique', 0.04), ('boundary', 0.039), ('ramshaw', 0.039), ('seeds', 0.039), ('criteria', 0.037), ('held', 0.037), ('tze', 0.037), ('accurately', 0.036), ('smallest', 0.036), ('consequence', 0.036), ('sizes', 0.036), ('begins', 0.036), ('retrieval', 0.034), ('labeled', 0.034), ('select', 0.034), ('ratio', 0.033), ('size', 0.033), ('accuracy', 0.032), ('demonstrate', 0.031), ('drastic', 0.031), ('spends', 0.031), ('unifying', 0.031), ('mpo', 0.031), ('hinges', 0.031), ('clarifying', 0.031), ('rarity', 0.031), ('boulder', 0.031), ('satisfy', 0.031), ('companion', 0.03), ('display', 0.03), ('higher', 0.029), ('phenomenon', 0.029), ('pervasive', 0.029), ('orado', 0.029), ('informativeness', 0.029), ('approve', 0.029), ('wilcoxon', 0.029), ('originates', 0.029), ('curse', 0.029), ('jinying', 0.029), ('laws', 0.029), ('lyle', 0.029), ('udo', 0.029), ('invest', 0.029), ('protect', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="145-tfidf-1" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>Author: Dmitriy Dligach ; Martha Palmer</p><p>Abstract: Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation.</p><p>2 0.28788769 <a title="145-tfidf-2" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>Author: Ines Rehbein ; Josef Ruppenhofer</p><p>Abstract: Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when ap- plied to noisy data.</p><p>3 0.21367958 <a title="145-tfidf-3" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>Author: Emmanuel Prochasson ; Pascale Fung</p><p>Abstract: We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.</p><p>4 0.13413069 <a title="145-tfidf-4" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>Author: Mitesh M. Khapra ; Salil Joshi ; Arindam Chatterjee ; Pushpak Bhattacharyya</p><p>Abstract: Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L1) can benefit from the annotation work done in a resource rich language (L2) via parameter projection. However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L1 is used to annotate the untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.</p><p>5 0.12765089 <a title="145-tfidf-5" href="./acl-2011-On-line_Language_Model_Biasing_for_Statistical_Machine_Translation.html">233 acl-2011-On-line Language Model Biasing for Statistical Machine Translation</a></p>
<p>Author: Sankaranarayanan Ananthakrishnan ; Rohit Prasad ; Prem Natarajan</p><p>Abstract: The language model (LM) is a critical component in most statistical machine translation (SMT) systems, serving to establish a probability distribution over the hypothesis space. Most SMT systems use a static LM, independent of the source language input. While previous work has shown that adapting LMs based on the input improves SMT performance, none of the techniques has thus far been shown to be feasible for on-line systems. In this paper, we develop a novel measure of cross-lingual similarity for biasing the LM based on the test input. We also illustrate an efficient on-line implementation that supports integration with on-line SMT systems by transferring much of the computational load off-line. Our approach yields significant reductions in target perplexity compared to the static LM, as well as consistent improvements in SMT performance across language pairs (English-Dari and English-Pashto).</p><p>6 0.1168511 <a title="145-tfidf-6" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>7 0.11562113 <a title="145-tfidf-7" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>8 0.10879067 <a title="145-tfidf-8" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>9 0.096570134 <a title="145-tfidf-9" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>10 0.091275677 <a title="145-tfidf-10" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>11 0.082615189 <a title="145-tfidf-11" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>12 0.080449507 <a title="145-tfidf-12" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>13 0.080385298 <a title="145-tfidf-13" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>14 0.077641331 <a title="145-tfidf-14" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>15 0.071013063 <a title="145-tfidf-15" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>16 0.070437342 <a title="145-tfidf-16" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>17 0.070302814 <a title="145-tfidf-17" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>18 0.069745608 <a title="145-tfidf-18" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>19 0.069552884 <a title="145-tfidf-19" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>20 0.069083504 <a title="145-tfidf-20" href="./acl-2011-Ranking_Class_Labels_Using_Query_Sessions.html">258 acl-2011-Ranking Class Labels Using Query Sessions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, 0.034), (2, -0.052), (3, 0.017), (4, -0.014), (5, -0.012), (6, 0.119), (7, -0.001), (8, -0.041), (9, 0.042), (10, -0.036), (11, -0.145), (12, 0.135), (13, 0.104), (14, -0.007), (15, -0.074), (16, 0.001), (17, 0.056), (18, 0.067), (19, -0.001), (20, 0.062), (21, -0.02), (22, 0.083), (23, 0.056), (24, -0.02), (25, 0.055), (26, 0.119), (27, 0.161), (28, 0.073), (29, -0.065), (30, 0.011), (31, -0.063), (32, -0.077), (33, -0.065), (34, 0.13), (35, 0.049), (36, 0.113), (37, -0.073), (38, 0.082), (39, 0.0), (40, 0.129), (41, -0.063), (42, -0.177), (43, -0.092), (44, -0.162), (45, 0.093), (46, 0.121), (47, -0.034), (48, -0.041), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97765768 <a title="145-lsi-1" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>Author: Dmitriy Dligach ; Martha Palmer</p><p>Abstract: Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation.</p><p>2 0.84926087 <a title="145-lsi-2" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>Author: Ines Rehbein ; Josef Ruppenhofer</p><p>Abstract: Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when ap- plied to noisy data.</p><p>3 0.76652586 <a title="145-lsi-3" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>4 0.70023441 <a title="145-lsi-4" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>Author: Mitesh M. Khapra ; Salil Joshi ; Arindam Chatterjee ; Pushpak Bhattacharyya</p><p>Abstract: Recent work on bilingual Word Sense Disambiguation (WSD) has shown that a resource deprived language (L1) can benefit from the annotation work done in a resource rich language (L2) via parameter projection. However, this method assumes the presence of sufficient annotated data in one resource rich language which may not always be possible. Instead, we focus on the situation where there are two resource deprived languages, both having a very small amount of seed annotated data and a large amount of untagged data. We then use bilingual bootstrapping, wherein, a model trained using the seed annotated data of L1 is used to annotate the untagged data of L2 and vice versa using parameter projection. The untagged instances of L1 and L2 which get annotated with high confidence are then added to the seed data of the respective languages and the above process is repeated. Our experiments show that such a bilingual bootstrapping algorithm when evaluated on two different domains with small seed sizes using Hindi (L1) and Marathi (L2) as the language pair performs better than monolingual bootstrapping and significantly reduces annotation cost.</p><p>5 0.51030713 <a title="145-lsi-5" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>Author: Mariet Theune ; Ruud Koolen ; Emiel Krahmer ; Sander Wubben</p><p>Abstract: In this paper we investigate how much data is required to train an algorithm for attribute selection, a subtask of Referring Expressions Generation (REG). To enable comparison between different-sized training sets, a systematic training method was developed. The results show that depending on the complexity of the domain, training on 10 to 20 items may already lead to a good performance.</p><p>6 0.4973599 <a title="145-lsi-6" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>7 0.48328272 <a title="145-lsi-7" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>8 0.46940494 <a title="145-lsi-8" href="./acl-2011-A_Scalable_Probabilistic_Classifier_for_Language_Modeling.html">24 acl-2011-A Scalable Probabilistic Classifier for Language Modeling</a></p>
<p>9 0.45128375 <a title="145-lsi-9" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>10 0.45121843 <a title="145-lsi-10" href="./acl-2011-Confidence-Weighted_Learning_of_Factored_Discriminative_Language_Models.html">78 acl-2011-Confidence-Weighted Learning of Factored Discriminative Language Models</a></p>
<p>11 0.44177687 <a title="145-lsi-11" href="./acl-2011-Integrating_history-length_interpolation_and_classes_in_language_modeling.html">175 acl-2011-Integrating history-length interpolation and classes in language modeling</a></p>
<p>12 0.44024381 <a title="145-lsi-12" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>13 0.43238938 <a title="145-lsi-13" href="./acl-2011-Even_the_Abstract_have_Color%3A_Consensus_in_Word-Colour_Associations.html">120 acl-2011-Even the Abstract have Color: Consensus in Word-Colour Associations</a></p>
<p>14 0.41801983 <a title="145-lsi-14" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>15 0.41458666 <a title="145-lsi-15" href="./acl-2011-Improving_Classification_of_Medical_Assertions_in_Clinical_Notes.html">165 acl-2011-Improving Classification of Medical Assertions in Clinical Notes</a></p>
<p>16 0.41457301 <a title="145-lsi-16" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>17 0.41301051 <a title="145-lsi-17" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>18 0.40517995 <a title="145-lsi-18" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>19 0.4050132 <a title="145-lsi-19" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>20 0.40340561 <a title="145-lsi-20" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.059), (17, 0.046), (19, 0.196), (26, 0.014), (34, 0.015), (37, 0.065), (39, 0.057), (41, 0.09), (55, 0.07), (59, 0.052), (72, 0.032), (91, 0.091), (96, 0.139)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94461453 <a title="145-lda-1" href="./acl-2011-Exploiting_Readymades_in_Linguistic_Creativity%3A_A_System_Demonstration_of_the_Jigsaw_Bard.html">125 acl-2011-Exploiting Readymades in Linguistic Creativity: A System Demonstration of the Jigsaw Bard</a></p>
<p>Author: Tony Veale ; Yanfen Hao</p><p>Abstract: Large lexical resources, such as corpora and databases of Web ngrams, are a rich source of pre-fabricated phrases that can be reused in many different contexts. However, one must be careful in how these resources are used, and noted writers such as George Orwell have argued that the use of canned phrases encourages sloppy thinking and results in poor communication. Nonetheless, while Orwell prized home-made phrases over the readymade variety, there is a vibrant movement in modern art which shifts artistic creation from the production of novel artifacts to the clever reuse of readymades or objets trouvés. We describe here a system that makes creative reuse of the linguistic readymades in the Google ngrams. Our system, the Jigsaw Bard, thus owes more to Marcel Duchamp than to George Orwell. We demonstrate how textual readymades can be identified and harvested on a large scale, and used to drive a modest form of linguistic creativity. 1</p><p>same-paper 2 0.81923437 <a title="145-lda-2" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>Author: Dmitriy Dligach ; Martha Palmer</p><p>Abstract: Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation.</p><p>3 0.79472947 <a title="145-lda-3" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>Author: Yuval Marton ; Nizar Habash ; Owen Rambow</p><p>Abstract: We explore the contribution of morphological features both lexical and inflectional to dependency parsing of Arabic, a morphologically rich language. Using controlled experiments, we find that definiteness, person, number, gender, and the undiacritzed lemma are most helpful for parsing on automatically tagged input. We further contrast the contribution of form-based and functional features, and show that functional gender and number (e.g., “broken plurals”) and the related rationality feature improve over form-based features. It is the first time functional morphological features are used for Arabic NLP. – –</p><p>4 0.78872895 <a title="145-lda-4" href="./acl-2011-Learning_Condensed_Feature_Representations_from_Large_Unsupervised_Data_Sets_for_Supervised_Learning.html">199 acl-2011-Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning</a></p>
<p>Author: Jun Suzuki ; Hideki Isozaki ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative ‘condensed feature representations’ from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III. ,</p><p>5 0.72436804 <a title="145-lda-5" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>Author: Ines Rehbein ; Josef Ruppenhofer</p><p>Abstract: Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when ap- plied to noisy data.</p><p>6 0.71843076 <a title="145-lda-6" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>7 0.71092975 <a title="145-lda-7" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>8 0.70334023 <a title="145-lda-8" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>9 0.70301938 <a title="145-lda-9" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>10 0.70082915 <a title="145-lda-10" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>11 0.69867283 <a title="145-lda-11" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>12 0.69428241 <a title="145-lda-12" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>13 0.69237101 <a title="145-lda-13" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>14 0.69154572 <a title="145-lda-14" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>15 0.69092679 <a title="145-lda-15" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>16 0.69089127 <a title="145-lda-16" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>17 0.68992329 <a title="145-lda-17" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>18 0.68991852 <a title="145-lda-18" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>19 0.68991435 <a title="145-lda-19" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>20 0.68971419 <a title="145-lda-20" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
