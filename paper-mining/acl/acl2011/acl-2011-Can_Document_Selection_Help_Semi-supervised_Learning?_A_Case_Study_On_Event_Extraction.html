<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-65" href="#">acl2011-65</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</h1>
<br/><p>Source: <a title="acl-2011-65-pdf" href="http://aclweb.org/anthology//P/P11/P11-2045.pdf">pdf</a></p><p>Author: Shasha Liao ; Ralph Grishman</p><p>Abstract: Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 1</p><p>Reference: <a title="acl-2011-65-reference" href="../acl2011_reference/acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract Annotating training data for event extraction is tedious and labor-intensive. [sent-4, score-0.759]
</p><p>2 Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. [sent-5, score-0.685]
</p><p>3 In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. [sent-6, score-0.201]
</p><p>4 Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. [sent-7, score-0.332]
</p><p>5 We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve  instance selection. [sent-8, score-0.357]
</p><p>6 3% in role labeling through IR and an additional 1. [sent-11, score-0.141]
</p><p>7 1  Introduction  The goal of event extraction is to identify instances of a class of events in text. [sent-14, score-0.897]
</p><p>8 In addition to identifying the event itself, it also identifies all of the participants and attributes of each event; these are the entities that are involved in that event. [sent-15, score-0.701]
</p><p>9 The same event might be presented in various expressions, and an expression might represent different events in different contexts. [sent-16, score-0.754]
</p><p>10 edu Moreover, for each event type, the event participants and attributes may also appear in multiple forms and exemplars of the different forms may be required. [sent-19, score-1.301]
</p><p>11 Thus, event extraction is a difficult task and requires substantial training data. [sent-20, score-0.721]
</p><p>12 However, annotating events for training is a tedious task. [sent-21, score-0.229]
</p><p>13 Annotators need to read the  whole sentence, possibly several sentences, to decide whether there is a specific event or not, and then need to identify the event participants (like Agent and Patient), and attributes (like place and time) to complete an event annotation. [sent-22, score-1.874]
</p><p>14 As a result, for event extraction tasks like MUC4, MUC6 (MUC 1995) and ACE2005, from one to several hundred annotated documents were needed. [sent-23, score-0.77]
</p><p>15 Although traditional self-training on normal newswire does not work well for this specific task, we managed to use information retrieval (IR) to select a better corpus for bootstrapping. [sent-25, score-0.148]
</p><p>16 Also, taking advantage of properties of this corpus, cross-document inference is applied to obtain more “informative” probabilities. [sent-26, score-0.075]
</p><p>17 To the best of our knowledge, we are the first to apply information retrieval and global inference to semi-supervised learning for event extraction. [sent-27, score-0.806]
</p><p>18 2  Task Description  Automatic Content Extraction (ACE) defines an event as a specific occurrence involving 260  Proce dings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o. [sent-28, score-0.599]
</p><p>19 i ac t2io0n1 fo Ar Csso cmiaptuiotanti fo nra Clo Lminpgu tiast i ocns:aslh Loirntpgaupisetrics , pages 260–265,  1  participants ; it annotates 8 types and 33 subtypes of events. [sent-30, score-0.077]
</p><p>20 2 We first present some ACE terminology to understand this task more easily:  Event mention3: a phrase or sentence within which an event is described, including one trigger and an arbitrary number of arguments. [sent-31, score-0.921]
</p><p>21  Event trigger: the main word that most clearly expresses an event occurrence. [sent-32, score-0.599]
</p><p>22  Event mention arguments (roles): the entity mentions that are involved in an event mention, and their relation to the event. [sent-33, score-0.953]
</p><p>23 Here is an example: (1) Bob Cole was killed in France today; he was attacked… Table 1 shows the results of the preprocessing, including name identification, entity mention classification and coreference, and time stamping. [sent-34, score-0.253]
</p><p>24 An example of entities and entity mentions and their types  Table 2. [sent-38, score-0.119]
</p><p>25 An example of event triggers and roles  1http://projects. [sent-39, score-0.833]
</p><p>26 pdf 2 In this paper, we treat the event subtypes  separately, and no type hierarchy is considered. [sent-45, score-0.662]
</p><p>27 3 Note that we do not deal with event mention coreference in this paper, so each event mention is treated separately. [sent-46, score-1.473]
</p><p>28 For event extraction, there are several studies on bootstrapping from a seed pattern set. [sent-48, score-0.717]
</p><p>29 Riloff (1996) initiated the idea of using document relevance for extracting new patterns, and Yangarber et al. [sent-49, score-0.078]
</p><p>30 (2000, 2003) incorporated this into a bootstrapping approach, extended by Surdeanu et al. [sent-50, score-0.118]
</p><p>31 However, these approaches were focused on finding instances of a scenario/event type rather than on argument role labeling. [sent-54, score-0.309]
</p><p>32 Starting from a set of documents classified for relevance, Patwardhan and Riloff (2007) created a  self-trained relevant sentence classifier and automatically learned domain-relevant extraction patterns. [sent-55, score-0.26]
</p><p>33 Liu (2009) proposed the BEAR system, which tagged both the events and their roles. [sent-56, score-0.209]
</p><p>34 The idea of sense consistency was first introduced and extended to operate across related documents by (Yarowsky, 1995). [sent-58, score-0.123]
</p><p>35 , 2007) applied cross-document inference to correct local extraction results for disease name, location and start/end time. [sent-61, score-0.161]
</p><p>36 Mann (2007) encoded specific inference rules to improve extraction of information about CEOs (name, start year, end year). [sent-62, score-0.161]
</p><p>37 Later, Ji and Grishman (2008) employed a rule-based approach to propagate consistent triggers and arguments across topic-related documents. [sent-63, score-0.315]
</p><p>38 Liao and Grishman (2010a) use a statistical model to infer the cross-event  information within a document to improve event extraction. [sent-65, score-0.651]
</p><p>39 This system extracts events independently for each sentence, because the definition of event mention arguments in ACE constrains them to appear in the same sentence. [sent-68, score-1.014]
</p><p>40 In the training process, for every event mention in the ACE training corpus, patterns are constructed based on the sequences of constituent heads separating the trigger and arguments. [sent-70, score-1.146]
</p><p>41 A set of Maximum Entropy based classifiers are also trained:  Argument Classifier: to distinguish arguments of a potential trigger from non-arguments. [sent-71, score-0.472]
</p><p>42  Role Classifier: to classify arguments by argument role. [sent-72, score-0.288]
</p><p>43 We use the same features as the argument classifier. [sent-73, score-0.138]
</p><p>44  Reportable-Event Classifier (Trigger  Classifier): Given a potential trigger, an event type, and a set of arguments, to determine whether there is a reportable event mention. [sent-74, score-1.257]
</p><p>45 In the test procedure, each document is scanned for instances of triggers from the training corpus. [sent-75, score-0.31]
</p><p>46 When an instance is found, the system tries to match the environment of the trigger against the set of patterns associated with that trigger. [sent-76, score-0.365]
</p><p>47 If this pattern-matching process succeeds, the argument classifier is applied to the entity mentions in the sentence to assign the possible arguments; for any argument passing that classifier, the role classifier is used to assign a role to it. [sent-77, score-0.724]
</p><p>48 Finally, once all arguments have been assigned, the reportable-event classifier is applied to the potential event mention; if the result is successful, this event mention is reported. [sent-78, score-1.547]
</p><p>49 5  Our Approach  In self-training, a classifier is first trained with a small amount of labeled data. [sent-79, score-0.089]
</p><p>50 The classifier is then used to classify the unlabeled data. [sent-80, score-0.089]
</p><p>51 Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. [sent-81, score-0.203]
</p><p>52 As a result, the criterion for selecting the most confident examples is critical to the effectiveness of self-training. [sent-83, score-0.136]
</p><p>53 To acquire confident samples, we need to first decide how to evaluate the confidence for each event. [sent-84, score-0.162]
</p><p>54 However, as an event contains one trigger and an arbitrary number of roles, a confident event might contain unconfident arguments. [sent-85, score-1.656]
</p><p>55 Thus, instead of taking the whole event, we select a partial event, containing one confident trigger and its most confident argument, to feed back to the training system. [sent-86, score-0.63]
</p><p>56 In each iteration, we  from from from added  the the the the  most confident   pairs training data, and re-trained the system. [sent-88, score-0.203]
</p><p>57 The newly added samples do not improve the system performance; instead, its performance stays stable, and even gets worse after several iterations. [sent-91, score-0.084]
</p><p>58 First, the classifier uses its own predictions to train itself, and so a classification mistake can reinforce itself. [sent-93, score-0.089]
</p><p>59 This is particularly true for event extraction, due to its relatively poor performance, compared to other NLP tasks, like Named Entity Recognition, parsing, or part-of-speech tagging, where self-training has been more successful. [sent-94, score-0.599]
</p><p>60 Figure 1 shows that the precision using the original training data is not very good: while precision improves with increasing classifier threshold, about 1/3 of the  roles are still incorrectly tagged at a threshold of 0. [sent-95, score-0.248]
</p><p>61 9) Another problem of self-training is that nothing “novel” is added because the most confident examples are those frequently seen in the training data and might not provide “new” information. [sent-110, score-0.203]
</p><p>62 Co-training has had some success in training (binary) semantic relation extractors for some relations, where the two views correspond to the arguments of the relation and the context of these arguments (Agichtein and Gravano 2000). [sent-114, score-0.363]
</p><p>63 However, it has had less success for event extraction because event arguments may participate in multiple events in a corpus and individual event instances may omit some arguments. [sent-115, score-2.245]
</p><p>64 2  Self-training on Information Retrieval Selected Corpus (ST_IR) To address the first problem (low precision of extracted events), we tried to select a corpus where the baseline system can tag the instances with greater confidence. [sent-117, score-0.081]
</p><p>65 (Ji and Grishman 2008) have observed that the events in a cluster of documents on the same topics as documents in the training corpus can be tagged more confidently. [sent-118, score-0.505]
</p><p>66 Thus, we believe that bootstrapping on a corpus of topic-related documents should perform better than a regular newswire corpus. [sent-119, score-0.278]
</p><p>67 org/indri/ related documents for each annotated document in the training corpus. [sent-123, score-0.173]
</p><p>68 The query is event-based to insure that related documents contain the same events. [sent-124, score-0.111]
</p><p>69 For each training document, we construct an INDRI query from the triggers and arguments. [sent-125, score-0.201]
</p><p>70 Only names and nominal arguments will be used; pronouns appearing as arguments are not included. [sent-127, score-0.327]
</p><p>71 For each argument we also add other names coreferential with the argument. [sent-128, score-0.208]
</p><p>72 3  Self-training (ST_GI)  using  Global  Inference  Although bootstrapping on related documents can solve the problem of “confidence” to some extent, the “novelty” problem still remains: the top-ranked extracted events will be too similar to  those in the training corpus. [sent-130, score-0.394]
</p><p>73 To address this problem, we propose to use a simple form of global inference based on the special characteristics of related-topic documents. [sent-131, score-0.164]
</p><p>74 Previous studies pointed out that information from wider scope, at the document or cross-document level, could provide non-local information to aid event extraction (Ji and Grishman 2008, Liao and Grishman 2010a). [sent-132, score-0.737]
</p><p>75 There are two common assumptions within a cluster of related documents (Ji and Grishman 2008):  Trigger Consistency Per Cluster: if one instance of a word triggers an event, other instances of the same word will trigger events of the same type. [sent-133, score-0.874]
</p><p>76  Role Consistency Per Cluster: if one entity appears as an argument of multiple events of the same type in a cluster of related documents, it should be assigned the same role each time. [sent-134, score-0.556]
</p><p>77 Based on these assumptions, if a trigger/role has a low probability from the baseline system, but a high one from global inference, it means that the local context of this trigger/role tag is not  frequently seen in the training data, but the tag is still confident. [sent-135, score-0.189]
</p><p>78 Thus, we can confidently add it to the training data and it can provide novel information which the samples confidently tagged by the baseline system cannot provide. [sent-136, score-0.291]
</p><p>79 263  To start, the baseline system extracts a set of events and estimates the probability that a particular instance of a word triggers an event of that type, and the probability that it takes a particular argument. [sent-137, score-1.023]
</p><p>80 The global inference process then begins by collecting all the confident triggers and arguments from a cluster of related documents. [sent-138, score-0.705]
</p><p>81 5 For each trigger word and event type, it records the highest probability (over all instances of that word in the cluster) that the word triggers an event of that type. [sent-139, score-1.782]
</p><p>82 For each argument, within-document and cross-document coreference6 are used to collect all instances of that entity; we then compute the maximum probability (over all instances) of that argument playing a particular role in a particular event type. [sent-140, score-0.922]
</p><p>83 7 For example, if the entity “Iraq” is tagged confidently (probability > 0. [sent-142, score-0.174]
</p><p>84 9) as the “Attacker” role somewhere in a cluster, and there is another instance where from local information it is only tagged with 0. [sent-143, score-0.142]
</p><p>85 1 probability to be an “Attacker” role, we use probability of 0. [sent-144, score-0.08]
</p><p>86 In this way, a trigger pair containing this argument is more likely to be added into the training data through bootstrapping, because we have global evidence that this role probability is high, although its local confidence is low. [sent-146, score-0.77]
</p><p>87 6  Results  We randomly chose 20 newswire texts from the ACE 2005 training corpora (from March to May of 2003) as our evaluation set, and used the 5 In our experiment, only triggers and roles with probability higher than 0. [sent-148, score-0.442]
</p><p>88 2005), and a simple rule-based cross-document coreference system, where entities sharing the same names will be treated as coreferential across documents. [sent-151, score-0.15]
</p><p>89 7 If a word or argument has multiple tags (different event types or roles) in a cluster, and the difference in the probabilities of the two tags is less than some threshold, we treat this as a “conflict” and do not use the conflicting information for global inference. [sent-152, score-0.826]
</p><p>90 remaining newswire texts as the original training data (83 documents). [sent-153, score-0.168]
</p><p>91 For self-training, we picked 10,000 consecutive newswire texts from the TDT5 corpus from for the ST experiment. [sent-154, score-0.132]
</p><p>92 For ST_IR and ST_GI, we retrieved the best N (using N = 25, which (Ji and Grishman 2008) found to work best) related texts for each training document from the English TDT5 corpus consisting of 278,108 news texts (from April to September of 2003). [sent-155, score-0.202]
</p><p>93 In total we retrieved 1650 texts; the IR system returned no texts or fewer than 25 texts for some training documents. [sent-156, score-0.15]
</p><p>94 In each iteration, we extract 500 trigger and argument pairs to add to the training data. [sent-157, score-0.496]
</p><p>95 Results (Table 3) show that bootstrapping on an event-based IR corpus can produce  20038  improvements on all three evaluations, global inference can yield further gains. [sent-158, score-0.282]
</p><p>96 Performance (F score) with different self-training strategies after 10 iterations 7  Conclusions and Future Work  We proposed a novel self-training process for event extraction that involves information retrieval (IR) and global inference to provide more accurate and informative instances. [sent-163, score-0.95]
</p><p>97 Experiments show that using an IR-selected corpus improves trigger labeling F score 1. [sent-164, score-0.375]
</p><p>98 Also, this bootstrapping involves processing a much  8  We selected all bootstrapping data from 2003 newswire, with the same genre and time period as ACE 2005 data to avoid possible influences of variations in the genre or time period on the bootstrapping. [sent-170, score-0.346]
</p><p>99 Also, we selected 10,000 documents because this size of corpus yielded a set of confidently-extracted events (probability > 0. [sent-171, score-0.24]
</p><p>100 Such pre-selection of documents may benefit bootstrapping for other NLP tasks as well, such as name and relation extraction. [sent-174, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('event', 0.599), ('trigger', 0.322), ('grishman', 0.221), ('triggers', 0.165), ('yangarber', 0.157), ('events', 0.155), ('arguments', 0.15), ('argument', 0.138), ('confident', 0.136), ('liao', 0.135), ('ace', 0.129), ('bootstrapping', 0.118), ('ji', 0.114), ('mention', 0.11), ('cluster', 0.09), ('roman', 0.089), ('global', 0.089), ('classifier', 0.089), ('ir', 0.089), ('indri', 0.088), ('role', 0.088), ('extraction', 0.086), ('documents', 0.085), ('newswire', 0.075), ('inference', 0.075), ('ralph', 0.072), ('roles', 0.069), ('heng', 0.064), ('shasha', 0.061), ('confidently', 0.061), ('entity', 0.059), ('attacker', 0.059), ('parg', 0.059), ('pevent', 0.059), ('prole', 0.059), ('proleoftrigger', 0.059), ('reportable', 0.059), ('strohman', 0.059), ('instances', 0.057), ('texts', 0.057), ('coreference', 0.055), ('tagged', 0.054), ('labeling', 0.053), ('document', 0.052), ('attacked', 0.052), ('killed', 0.052), ('bob', 0.048), ('cole', 0.048), ('retrieval', 0.043), ('patterns', 0.043), ('coreferential', 0.043), ('gupta', 0.043), ('probability', 0.04), ('participants', 0.04), ('agichtein', 0.039), ('tedious', 0.038), ('patwardhan', 0.038), ('consistency', 0.038), ('attributes', 0.037), ('subtypes', 0.037), ('training', 0.036), ('mentions', 0.035), ('stevenson', 0.034), ('surdeanu', 0.034), ('name', 0.032), ('today', 0.032), ('informative', 0.032), ('added', 0.031), ('traditional', 0.03), ('period', 0.03), ('mi', 0.029), ('year', 0.029), ('france', 0.029), ('samples', 0.029), ('riloff', 0.028), ('names', 0.027), ('views', 0.027), ('filtered', 0.026), ('type', 0.026), ('clive', 0.026), ('insure', 0.026), ('borovets', 0.026), ('exemplars', 0.026), ('gri', 0.026), ('initiated', 0.026), ('mateo', 0.026), ('mutations', 0.026), ('nyu', 0.026), ('confidence', 0.026), ('novel', 0.026), ('genre', 0.025), ('entities', 0.025), ('baseline', 0.024), ('metzler', 0.024), ('umass', 0.024), ('stays', 0.024), ('atem', 0.024), ('jordi', 0.024), ('shman', 0.024), ('turmo', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="65-tfidf-1" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>Author: Shasha Liao ; Ralph Grishman</p><p>Abstract: Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 1</p><p>2 0.6940468 <a title="65-tfidf-2" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>Author: Yu Hong ; Jianfeng Zhang ; Bin Ma ; Jianmin Yao ; Guodong Zhou ; Qiaoming Zhu</p><p>Abstract: Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entitytype consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction. 1</p><p>3 0.54074335 <a title="65-tfidf-3" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: The goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords. We propose a multilayered event extraction architecture that progressively “zooms in” on relevant information. Our extraction model includes a document genre classifier to recognize event narratives, two types of sentence classifiers, and noun phrase classifiers to extract role fillers. These modules are organized as a pipeline to gradually zero in on event-related information. We present results on the MUC-4 event extraction data set and show that this model performs better than previous systems.</p><p>4 0.45193619 <a title="65-tfidf-4" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>Author: David McClosky ; Mihai Surdeanu ; Christopher Manning</p><p>Abstract: Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a “crime” event can cause a “investigation” event, which can lead to an “arrest” event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP’09 shared task with a F1 score of 53.5% in development and 48.6% in testing.</p><p>5 0.25828654 <a title="65-tfidf-5" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>Author: Nathanael Chambers ; Dan Jurafsky</p><p>Abstract: Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.</p><p>6 0.24033235 <a title="65-tfidf-6" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>7 0.19432771 <a title="65-tfidf-7" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>8 0.17410961 <a title="65-tfidf-8" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>9 0.15853643 <a title="65-tfidf-9" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>10 0.14313932 <a title="65-tfidf-10" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>11 0.14110433 <a title="65-tfidf-11" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>12 0.13160732 <a title="65-tfidf-12" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>13 0.12756616 <a title="65-tfidf-13" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>14 0.11933091 <a title="65-tfidf-14" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>15 0.11729358 <a title="65-tfidf-15" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>16 0.11476373 <a title="65-tfidf-16" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>17 0.10884731 <a title="65-tfidf-17" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>18 0.10070223 <a title="65-tfidf-18" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>19 0.099141665 <a title="65-tfidf-19" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>20 0.097614057 <a title="65-tfidf-20" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.218), (1, 0.127), (2, -0.369), (3, 0.039), (4, 0.451), (5, 0.305), (6, -0.137), (7, -0.112), (8, 0.339), (9, 0.04), (10, -0.039), (11, 0.02), (12, 0.011), (13, 0.084), (14, 0.026), (15, 0.019), (16, 0.064), (17, 0.035), (18, -0.006), (19, 0.017), (20, 0.005), (21, -0.024), (22, 0.024), (23, 0.003), (24, -0.04), (25, 0.049), (26, 0.009), (27, 0.037), (28, 0.016), (29, -0.008), (30, 0.032), (31, 0.025), (32, 0.004), (33, -0.034), (34, -0.046), (35, -0.009), (36, 0.023), (37, -0.018), (38, 0.028), (39, 0.004), (40, -0.0), (41, -0.012), (42, -0.015), (43, 0.032), (44, 0.057), (45, -0.012), (46, 0.008), (47, -0.001), (48, -0.04), (49, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97716153 <a title="65-lsi-1" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>Author: Shasha Liao ; Ralph Grishman</p><p>Abstract: Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 1</p><p>2 0.96781963 <a title="65-lsi-2" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>Author: Yu Hong ; Jianfeng Zhang ; Bin Ma ; Jianmin Yao ; Guodong Zhou ; Qiaoming Zhu</p><p>Abstract: Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entitytype consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction. 1</p><p>3 0.95984405 <a title="65-lsi-3" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: The goal of our research is to improve event extraction by learning to identify secondary role filler contexts in the absence of event keywords. We propose a multilayered event extraction architecture that progressively “zooms in” on relevant information. Our extraction model includes a document genre classifier to recognize event narratives, two types of sentence classifiers, and noun phrase classifiers to extract role fillers. These modules are organized as a pipeline to gradually zero in on event-related information. We present results on the MUC-4 event extraction data set and show that this model performs better than previous systems.</p><p>4 0.85332865 <a title="65-lsi-4" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>Author: David McClosky ; Mihai Surdeanu ; Christopher Manning</p><p>Abstract: Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a “crime” event can cause a “investigation” event, which can lead to an “arrest” event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP’09 shared task with a F1 score of 53.5% in development and 48.6% in testing.</p><p>5 0.73375183 <a title="65-lsi-5" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<p>Author: Nathanael Chambers ; Dan Jurafsky</p><p>Abstract: Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.</p><p>6 0.5514617 <a title="65-lsi-6" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>7 0.51272523 <a title="65-lsi-7" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>8 0.44674858 <a title="65-lsi-8" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>9 0.38720876 <a title="65-lsi-9" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>10 0.37173921 <a title="65-lsi-10" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>11 0.36665398 <a title="65-lsi-11" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>12 0.34907195 <a title="65-lsi-12" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>13 0.33928218 <a title="65-lsi-13" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>14 0.32877919 <a title="65-lsi-14" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>15 0.32691982 <a title="65-lsi-15" href="./acl-2011-Classifying_arguments_by_scheme.html">68 acl-2011-Classifying arguments by scheme</a></p>
<p>16 0.31473896 <a title="65-lsi-16" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>17 0.3086836 <a title="65-lsi-17" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>18 0.3042205 <a title="65-lsi-18" href="./acl-2011-Knowledge_Base_Population%3A_Successful_Approaches_and_Challenges.html">191 acl-2011-Knowledge Base Population: Successful Approaches and Challenges</a></p>
<p>19 0.28988764 <a title="65-lsi-19" href="./acl-2011-Semi-supervised_Relation_Extraction_with_Large-scale_Word_Clustering.html">277 acl-2011-Semi-supervised Relation Extraction with Large-scale Word Clustering</a></p>
<p>20 0.28977501 <a title="65-lsi-20" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.034), (9, 0.011), (17, 0.066), (26, 0.023), (37, 0.102), (39, 0.038), (40, 0.168), (41, 0.165), (55, 0.024), (59, 0.072), (72, 0.038), (91, 0.059), (96, 0.113)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84840792 <a title="65-lda-1" href="./acl-2011-Semi-supervised_condensed_nearest_neighbor_for_part-of-speech_tagging.html">278 acl-2011-Semi-supervised condensed nearest neighbor for part-of-speech tagging</a></p>
<p>Author: Anders Sgaard</p><p>Abstract: This paper introduces a new training set condensation technique designed for mixtures of labeled and unlabeled data. It finds a condensed set of labeled and unlabeled data points, typically smaller than what is obtained using condensed nearest neighbor on the labeled data only, and improves classification accuracy. We evaluate the algorithm on semisupervised part-of-speech tagging and present the best published result on the Wall Street Journal data set.</p><p>same-paper 2 0.84717673 <a title="65-lda-2" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>Author: Shasha Liao ; Ralph Grishman</p><p>Abstract: Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 1</p><p>3 0.78659433 <a title="65-lda-3" href="./acl-2011-K-means_Clustering_with_Feature_Hashing.html">189 acl-2011-K-means Clustering with Feature Hashing</a></p>
<p>Author: Hajime Senuma</p><p>Abstract: One of the major problems of K-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task.</p><p>4 0.78292716 <a title="65-lda-4" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>Author: Antske Fokkens</p><p>Abstract: When designing grammars of natural language, typically, more than one formal analysis can account for a given phenomenon. Moreover, because analyses interact, the choices made by the engineer influence the possibilities available in further grammar development. The order in which phenomena are treated may therefore have a major impact on the resulting grammar. This paper proposes to tackle this problem by using metagrammar development as a methodology for grammar engineering. Iargue that metagrammar engineering as an approach facilitates the systematic exploration of grammars through comparison of competing analyses. The idea is illustrated through a comparative study of auxiliary structures in HPSG-based grammars for German and Dutch. Auxiliaries form a central phenomenon of German and Dutch and are likely to influence many components of the grammar. This study shows that a special auxiliary+verb construction significantly improves efficiency compared to the standard argument-composition analysis for both parsing and generation.</p><p>5 0.7828126 <a title="65-lda-5" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>6 0.77934813 <a title="65-lda-6" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>7 0.77263182 <a title="65-lda-7" href="./acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers.html">56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</a></p>
<p>8 0.76340765 <a title="65-lda-8" href="./acl-2011-Contrasting_Multi-Lingual_Prosodic_Cues_to_Predict_Verbal_Feedback_for_Rapport.html">83 acl-2011-Contrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback for Rapport</a></p>
<p>9 0.761401 <a title="65-lda-9" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>10 0.76136798 <a title="65-lda-10" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>11 0.76048279 <a title="65-lda-11" href="./acl-2011-Nonparametric_Bayesian_Machine_Transliteration_with_Synchronous_Adaptor_Grammars.html">232 acl-2011-Nonparametric Bayesian Machine Transliteration with Synchronous Adaptor Grammars</a></p>
<p>12 0.7571705 <a title="65-lda-12" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>13 0.74904978 <a title="65-lda-13" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>14 0.74416363 <a title="65-lda-14" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>15 0.74085915 <a title="65-lda-15" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>16 0.73870754 <a title="65-lda-16" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>17 0.73635828 <a title="65-lda-17" href="./acl-2011-Modeling_Wisdom_of_Crowds_Using_Latent_Mixture_of_Discriminative_Experts.html">223 acl-2011-Modeling Wisdom of Crowds Using Latent Mixture of Discriminative Experts</a></p>
<p>18 0.73615265 <a title="65-lda-18" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>19 0.72873288 <a title="65-lda-19" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>20 0.72637171 <a title="65-lda-20" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
