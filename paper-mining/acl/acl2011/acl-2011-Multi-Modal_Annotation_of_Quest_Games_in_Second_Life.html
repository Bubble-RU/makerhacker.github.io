<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-226" href="#">acl2011-226</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</h1>
<br/><p>Source: <a title="acl-2011-226-pdf" href="http://aclweb.org/anthology//P/P11/P11-1018.pdf">pdf</a></p><p>Author: Sharon Gower Small ; Jennifer Strommer-Galley ; Tomek Strzalkowski</p><p>Abstract: We describe an annotation tool developed to assist in the creation of multimodal actioncommunication corpora from on-line massively multi-player games, or MMGs. MMGs typically involve groups of players (5-30) who control their perform various activities (questing, competing, fighting, etc.) and communicate via chat or speech using assumed screen names. We collected a corpus of 48 group quests in Second Life that jointly involved 206 players who generated over 30,000 messages in quasisynchronous chat during approximately 140 hours of recorded action. Multiple levels of coordinated annotation of this corpus (dialogue, movements, touch, gaze, wear, etc) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. The annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously. avatars1, 1</p><p>Reference: <a title="acl-2011-226-reference" href="../acl2011_reference/acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We describe an annotation tool developed to assist in the creation of multimodal actioncommunication corpora from on-line massively multi-player games, or MMGs. [sent-4, score-0.241]
</p><p>2 MMGs typically involve groups of players (5-30) who control their perform various activities (questing, competing, fighting, etc. [sent-5, score-0.341]
</p><p>3 ) and communicate via chat or speech using assumed screen names. [sent-6, score-0.409]
</p><p>4 We collected a corpus of 48 group quests in Second Life that jointly involved 206 players who generated over 30,000 messages in quasisynchronous chat during approximately 140 hours of recorded action. [sent-7, score-1.087]
</p><p>5 Multiple levels of coordinated annotation of this corpus (dialogue, movements, touch, gaze, wear, etc) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. [sent-8, score-0.205]
</p><p>6 The annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously. [sent-9, score-0.316]
</p><p>7 avatars1,  1  Introduction  The aim of our project is to predict the real world characteristics of players of massively-multiplayer online games, such as Second Life (SL). [sent-10, score-0.342]
</p><p>8 We sought to predict actual player attributes like age or education levels, and personality traits including leadership or conformity. [sent-11, score-0.13]
</p><p>9 Our task was to do so using only the behaviors, communication, and interaction among the players produced during game play. [sent-12, score-0.377]
</p><p>10 To do so, we logged all players’ avatar movements, 1 All avatar names seen in this protect players’ identities. [sent-13, score-1.036]
</p><p>11 paper  have been changed to  171 “touch events” (putting on or taking off clothing items, for example), and their public chat messages (i. [sent-14, score-0.515]
</p><p>12 , messages that can be seen by all players in the group). [sent-16, score-0.387]
</p><p>13 Given the complex nature of interpreting chat in an online game environment, we required a tool that would allow annotators to have a synchronized view of both the event action as well as the chat utterances. [sent-17, score-1.411]
</p><p>14 This would allow our annotators to correlate the events and the chat by marking them simultaneously. [sent-18, score-0.645]
</p><p>15 More importantly, being able to  view game events enables more accurate chat annotation; and conversely, viewing chat utterances helps to interpret the significance of certain events in the game, e. [sent-19, score-1.233]
</p><p>16 ” could be simply a response (rejection) to a request from another player; however, when the game action is viewed and the speaker is seen attempting to enter a building without success, another interpretation may arise (an assertion, a call for help, etc. [sent-23, score-0.156]
</p><p>17 The Real World (RW) characteristics of SL players (and other on-line games) may be inferred to varying degrees from the appearance of their avatars, the behaviors they engage in, as well as from their on-line chat communications. [sent-25, score-0.751]
</p><p>18 For example, the avatar gender generally matches the gender of the owner; on the other hand, vocabulary choices in chat are rather poor predictors of a player’s age, even though such correlation is generally seen in real life conversation. [sent-26, score-1.05]
</p><p>19 We generated a corpus of chat and movement data from 48 quests comprised of 206 participants who generated over 30,000  2 An online Virtual World developed and launched in 2003, by Linden Lab, San Francisco, CA. [sent-28, score-0.606]
</p><p>20 Ac s2s0o1ci1a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 171–179, messages and approximately 140 hours of recorded action. [sent-32, score-0.272]
</p><p>21 We required an annotation tool to help us efficiently annotate dialogue acts and communication links in chat utterances as well as avatar movements from such a large corpus. [sent-33, score-1.477]
</p><p>22 Moreover, we required correlation between these two dimensions of chat and movement since movement and other actions may be both causes and effects of verbal communication. [sent-34, score-0.653]
</p><p>23 We developed a multimodal event and chat annotation tool (called RAT, the Relational Annotation Tool), which will simultaneously display a 2D rendering of all movement activity recorded during our Second Life studies, synchronized with the chat utterances. [sent-35, score-1.469]
</p><p>24 In this way both chat and movements can be annotated simultaneously: the avatar movement actions can be reviewed while making dialogue act annotations. [sent-36, score-1.237]
</p><p>25 The multi-modal tool DAT (Core and Allen, 1997) was developed to assist testing of the DAMSL annotation scheme. [sent-45, score-0.241]
</p><p>26 With DAT, annotators were able to listen to the actual dialogues as well as view the transcripts. [sent-46, score-0.127]
</p><p>27 While these tools are all highly effective for their respective tasks, ours is unique in its synchronized view of both event action and chat utterances. [sent-47, score-0.788]
</p><p>28 ti or NVivo, a few studies have annotated chat using custom-built tools. [sent-49, score-0.409]
</p><p>29 One approach uses computer-mediated discourse analysis approaches and the Dynamic Topic Analysis tool (Herring, 2003; Herring & Nix; 1997; StromerGalley & Martison, 2009), which allows annotators  to track a specific phenomenon of online interaction in chat: topic shifts during an interaction. [sent-50, score-0.179]
</p><p>30 We are unaware of any other tools that facilitate the simultaneous playback of multi-modes of communication and behavior. [sent-54, score-0.119]
</p><p>31 3  Second Life Experiments  To generate player data, we rented an island in  Second Life and developed an approximately two hour quest, the Case of the Missing Moonstone. [sent-55, score-0.288]
</p><p>32 We recruited Second Life players in-game through advertising and setting up a shop that interested players could browse. [sent-57, score-0.616]
</p><p>33 Once all players arrived, the main quest began, progressing through five geographic areas in the island. [sent-60, score-0.551]
</p><p>34 Players were accompanied by a “training sergeant”, a researcher using a robot avatar, that followed players through the quest and provided hints when groups became stymied along their investigation but otherwise had little interaction with the group. [sent-61, score-0.526]
</p><p>35 We followed Yee and Bailenson’s (2008) technical approach for logging player behavior. [sent-67, score-0.13]
</p><p>36 To get a sense of the volume of data generated, 206 players generated over 30,000 messages into the group’s public chat from the 48 sessions. [sent-68, score-0.796]
</p><p>37 The avatar logger was implemented to record each avatar’s location through their (x,y,z) coordinates, recorded at two second intervals. [sent-70, score-0.642]
</p><p>38 A tool was needed that would allow annotators to see the textual transcripts of the chat while at the same 173 rately annotate those messages, we needed annotators to have as much information about the context as possible. [sent-74, score-0.744]
</p><p>39 The 2D map coupled with the  events information made it easier to understand. [sent-75, score-0.147]
</p><p>40 For example, in the quest, players in a specific zone, encounter a dead, maimed body. [sent-76, score-0.336]
</p><p>41 As annotators assigned codes to the chat, they would sometimes encounter exclamations, such as “ew” or “gross”. [sent-77, score-0.127]
</p><p>42 Annotators would use the 2D map and the location of the exclaiming avatar to determine if the exclamation was a result of their location (in the zone with the dead body) or because of something said or done by another player. [sent-78, score-0.689]
</p><p>43 Location of avatars on the 2D map synchronized with chat was also helpful for annotators when attempting to disambiguate communicative links. [sent-79, score-0.813]
</p><p>44 If player A says “You see that scribbling on the wall? [sent-81, score-0.13]
</p><p>45 ” the annotator needs to use the 2D map to see who the player is speaking to. [sent-82, score-0.251]
</p><p>46 If player A and player C are both standing in that subzone, then the annotator can make a reasonable assumption that player A is directing the question to player C, and not player B who is located in a different subzone. [sent-83, score-0.736]
</p><p>47 Second, we annotated coordinated avatar movement actions (such as following each other into a building or into a room), and the only way to readily identify such complex events was through the 2D map of avatar movements. [sent-84, score-1.313]
</p><p>48 The overall RAT interface, Figure 2, allows the annotator to simultaneously view all modes of representation. [sent-85, score-0.14]
</p><p>49 The left hand panel is the 2D representation of the action (section 4. [sent-87, score-0.212]
</p><p>50 The upper right hand panel displays the chat and event transcripts (section 4. [sent-89, score-0.685]
</p><p>51 Conversely, an overly abstract representation would not be of significant value in the annotation process. [sent-96, score-0.128]
</p><p>52 We decided to represent each area separately as each group moves between the areas together, and it was therefore never necessary to display more than one area at a time. [sent-99, score-0.172]
</p><p>53 Even though annotators visited the island to familiarize themselves with the layout, many mansion rooms were labeled to help  the annotator recall the layout of the building, and minimize error of annotation based on flawed recall. [sent-107, score-0.431]
</p><p>54 Finally, the exact time of the action that is currently being represented is displayed in the lower left hand corner. [sent-108, score-0.156]
</p><p>55 Figure 3: Second Life overview map  Figure 4: 2D representation of Second Life action inside the Mansion/Manor  Figure 5: Second Life view of Mansion exterior Avatar location was recorded in our log files as an (x,y,z) coordinate at a two second interval. [sent-109, score-0.319]
</p><p>56 Avatars 175 were represented in our 2D panel as moving solid color circles, using the x and y coordinates. [sent-110, score-0.141]
</p><p>57 A color coded avatar key was displayed below the 2D rep-  resentation. [sent-111, score-0.637]
</p><p>58 This key related the full name of every avatar to its colored circle representation. [sent-112, score-0.496]
</p><p>59 The z coordinate was used to determine if the avatar was on the second floor of a building. [sent-113, score-0.496]
</p><p>60 If the z value indicated an avatar was on a second floor, their icon was modified to include the number “2” for the duration of their time on the second floor. [sent-114, score-0.496]
</p><p>61 Using this we were able to represent which direction the avatar was looking by a small black dot on their colored circle. [sent-116, score-0.496]
</p><p>62 As the annotators stepped through the chat and event annotation, the action would move forward, in synchronized step in the 2D map. [sent-117, score-0.854]
</p><p>63 In this way at any given time the annotator could see the avatar action corresponding to the chat and event transcripts appearing in the right panels. [sent-118, score-1.253]
</p><p>64 The annotator had the option to step forward or backward through the data at any step interval, where each step corresponded to a two second increment or decrement, to provide maximum flexibility to the annotator in viewing and reviewing the actions and communications to be annotated. [sent-119, score-0.21]
</p><p>65 Additionally, “Play” and “Stop” buttons were added to the tool so the annotator may simply watch the action play forward rather than manually stepping through. [sent-120, score-0.253]
</p><p>66 2  The Chat & Event Panel  Avatar utterances along with logged Second Life events were displayed in the Chat and Event Panel (Figure 6). [sent-122, score-0.319]
</p><p>67 Utterances and events were each displayed in their own column. [sent-123, score-0.181]
</p><p>68 Time was recorded for every utterance and event, and this was displayed in the first column of the Chat and Event Panel. [sent-124, score-0.205]
</p><p>69 All avatar names in the utterances and events were color coded, where the colors corresponded to the avatar color used in the 2D panel. [sent-125, score-1.278]
</p><p>70 This panel was synchronized with the 2D Representation panel and as the annotator stepped through the game action on the 2D display, the associated utterances and events populated the Chat and Event panel. [sent-126, score-0.759]
</p><p>71 3  The Annotator Panels  The Annotator Panels (Figures 7 and 10) contains all features needed for the annotator to quickly  annotate the events and dialogue. [sent-128, score-0.23]
</p><p>72 Annotators could choose from a number of categories to label each dialogue utterance. [sent-129, score-0.117]
</p><p>73 Coding categories included communicative links, dialogue acts, and selected multi-avatar actions. [sent-130, score-0.158]
</p><p>74 A more detailed description of the chat annotation scheme is available in (Shaikh et al. [sent-132, score-0.513]
</p><p>75 1 Communicative Links One of the challenges in multi-party dialogue is to establish which user an utterance is directed towards. [sent-136, score-0.173]
</p><p>76 Communicative link annotation allows for accurate mapping of dialogue dynamics in the multiparty setting, and is a critical component of tracking  such social phenomena as disagreements and leadership. [sent-139, score-0.255]
</p><p>77 2 Dialogue Acts We developed a hierarchy of 19 dialogue acts for annotating the functional aspect of the utterance in 176 the discussion. [sent-142, score-0.262]
</p><p>78 , 1997), but greatly reduced and also tuned significantly towards dialogue pragmatics and away from more surface characteristics of utterances. [sent-144, score-0.151]
</p><p>79 In particular, we ask our annotators what is the pragmatic function of each utterance within the dialogue, a decision that often depends upon how earlier utterances were classified. [sent-145, score-0.249]
</p><p>80 A subzone is a building, a room within a building, or any other identifiable area within the playable spaces of the quest, e. [sent-153, score-0.293]
</p><p>81 The subzone was determined based on the avatar(s) (x,y,z) coordinates and the known subzone boundaries. [sent-156, score-0.468]
</p><p>82 4 Multi-avatar events As mentioned, in addition to chat we also were interested in having the annotators record composite  events involving multiple avatars over a span of time and space. [sent-160, score-0.915]
</p><p>83 While the design of the RAT tool will support annotation of any event of interest with only slight modifications, for our purposes, we were interested in annotating two types of events that we considered significant for our research hypotheses. [sent-161, score-0.477]
</p><p>84 The first type of event was the multiavatar entry (or exit) into a sub-zone, including the order in which the avatars moved. [sent-162, score-0.325]
</p><p>85 Figure 8 shows an example of a “Moves into Subzone” annotation as displayed in the Chat & Event Panel. [sent-163, score-0.173]
</p><p>86 Figure 9 shows the corresponding series of progressive moments in time portraying entry into the Bank subzone as represented in RAT. [sent-164, score-0.332]
</p><p>87 In the annotation, each avatar name is recorded in order of its entry into the subzone (here, the Bank). [sent-165, score-0.817]
</p><p>88 Additionally, we record the subzone name and the time the event is completed3. [sent-166, score-0.399]
</p><p>89 The second type of event we annotated was the “follow X” event, i. [sent-167, score-0.15]
</p><p>90 , when one or more avatars appeared to be following one another within a subzone. [sent-169, score-0.149]
</p><p>91 These two types of events were of particular interest because we hypothesized that players who are leaders are likely to enter first into a subzone and be followed around once inside. [sent-170, score-0.635]
</p><p>92 In addition, support for annotation of other types  of composite events can be added as needed; for example, group forming and splitting, or certain 3 We are also able to record the start time of any event but for our purposes we were only concerned with the end time. [sent-171, score-0.432]
</p><p>93 A “Moves Into Subzone” event is annotated by recording the ordinal (1, 2, 3, etc. [sent-175, score-0.15]
</p><p>94 Similarly, a “Follows” event is coded as avatar group “A” follows group “B’, where each group will contain one or more avatars. [sent-177, score-0.774]
</p><p>95 Two students were hired and trained for approximately 60 hours, during which time they learned how to use the annotation tool and the categories and rules for the annotation process. [sent-180, score-0.327]
</p><p>96 Annotators spent roughly 7 hours marking up of agreement), the two students then anno- the movements and chat messages per 2. [sent-184, score-0.661]
</p><p>97 Our tool was used to accurately and simultaneously annotate over 30,000 messages and  approximately 140 hours of action. [sent-191, score-0.33]
</p><p>98 For each hour spent annotating, our annotators were able to tag approximately 170 utterances as well as 36 minutes of action. [sent-192, score-0.268]
</p><p>99 The function allowing for the synchronized playback of the chat and movement data coupled with the 2D map increased comprehension of utterances and behavior of the players during the quest, improving validity and reliability of the results. [sent-194, score-1.073]
</p><p>100 MPC:  A Multi-party chat corpus for modeling social phenomena in discourse. [sent-245, score-0.443]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('avatar', 0.496), ('chat', 0.409), ('players', 0.308), ('quest', 0.218), ('subzone', 0.215), ('event', 0.15), ('avatars', 0.149), ('life', 0.145), ('player', 0.13), ('dialogue', 0.117), ('events', 0.112), ('annotation', 0.104), ('movement', 0.103), ('panel', 0.101), ('annotators', 0.099), ('utterances', 0.094), ('mansion', 0.087), ('action', 0.087), ('annotator', 0.086), ('rat', 0.084), ('synchronized', 0.08), ('tool', 0.08), ('recorded', 0.08), ('messages', 0.079), ('movements', 0.074), ('hours', 0.074), ('game', 0.069), ('displayed', 0.069), ('quests', 0.066), ('math', 0.063), ('panels', 0.058), ('utterance', 0.056), ('island', 0.055), ('albany', 0.05), ('virtual', 0.045), ('herring', 0.044), ('playback', 0.044), ('shaikh', 0.044), ('logged', 0.044), ('area', 0.042), ('communication', 0.041), ('communicative', 0.041), ('touch', 0.04), ('damsl', 0.04), ('color', 0.04), ('approximately', 0.039), ('games', 0.039), ('zone', 0.038), ('coordinates', 0.038), ('actions', 0.038), ('room', 0.036), ('hour', 0.036), ('map', 0.035), ('social', 0.034), ('tools', 0.034), ('record', 0.034), ('characteristics', 0.034), ('bailenson', 0.033), ('cassidy', 0.033), ('cslu', 0.033), ('emu', 0.033), ('exterior', 0.033), ('moments', 0.033), ('portraying', 0.033), ('stahl', 0.033), ('tomek', 0.033), ('coordinated', 0.033), ('activities', 0.033), ('group', 0.032), ('requests', 0.032), ('coded', 0.032), ('objects', 0.032), ('location', 0.032), ('annotate', 0.032), ('annotating', 0.031), ('teams', 0.031), ('moves', 0.031), ('acts', 0.03), ('studying', 0.029), ('assist', 0.029), ('exclamation', 0.029), ('nix', 0.029), ('strzalkowski', 0.029), ('stepped', 0.029), ('encounter', 0.028), ('view', 0.028), ('developed', 0.028), ('dead', 0.027), ('clothing', 0.027), ('entry', 0.026), ('simultaneously', 0.026), ('coding', 0.026), ('progressive', 0.025), ('inventories', 0.025), ('alpha', 0.025), ('bank', 0.025), ('areas', 0.025), ('marking', 0.025), ('transcripts', 0.025), ('representation', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="226-tfidf-1" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>Author: Sharon Gower Small ; Jennifer Strommer-Galley ; Tomek Strzalkowski</p><p>Abstract: We describe an annotation tool developed to assist in the creation of multimodal actioncommunication corpora from on-line massively multi-player games, or MMGs. MMGs typically involve groups of players (5-30) who control their perform various activities (questing, competing, fighting, etc.) and communicate via chat or speech using assumed screen names. We collected a corpus of 48 group quests in Second Life that jointly involved 206 players who generated over 30,000 messages in quasisynchronous chat during approximately 140 hours of recorded action. Multiple levels of coordinated annotation of this corpus (dialogue, movements, touch, gaze, wear, etc) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. The annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously. avatars1, 1</p><p>2 0.15043724 <a title="226-tfidf-2" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>3 0.14372991 <a title="226-tfidf-3" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>Author: Yu Hong ; Jianfeng Zhang ; Bin Ma ; Jianmin Yao ; Guodong Zhou ; Qiaoming Zhu</p><p>Abstract: Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entitytype consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction. 1</p><p>4 0.13128985 <a title="226-tfidf-4" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>Author: David McClosky ; Mihai Surdeanu ; Christopher Manning</p><p>Abstract: Nested event structures are a common occurrence in both open domain and domain specific extraction tasks, e.g., a “crime” event can cause a “investigation” event, which can lead to an “arrest” event. However, most current approaches address event extraction with highly local models that extract each event and argument independently. We propose a simple approach for the extraction of such structures by taking the tree of event-argument relations and using it directly as the representation in a reranking dependency parser. This provides a simple framework that captures global properties of both nested and flat event structures. We explore a rich feature space that models both the events to be parsed and context from the original supporting text. Our approach obtains competitive results in the extraction of biomedical events from the BioNLP’09 shared task with a F1 score of 53.5% in development and 48.6% in testing.</p><p>5 0.12526065 <a title="226-tfidf-5" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>6 0.12501891 <a title="226-tfidf-6" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>7 0.11933091 <a title="226-tfidf-7" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>8 0.11758123 <a title="226-tfidf-8" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>9 0.11185513 <a title="226-tfidf-9" href="./acl-2011-Dr_Sentiment_Knows_Everything%21.html">105 acl-2011-Dr Sentiment Knows Everything!</a></p>
<p>10 0.10834925 <a title="226-tfidf-10" href="./acl-2011-Disentangling_Chat_with_Local_Coherence_Models.html">101 acl-2011-Disentangling Chat with Local Coherence Models</a></p>
<p>11 0.10594748 <a title="226-tfidf-11" href="./acl-2011-PsychoSentiWordNet.html">253 acl-2011-PsychoSentiWordNet</a></p>
<p>12 0.096078739 <a title="226-tfidf-12" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>13 0.091409683 <a title="226-tfidf-13" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>14 0.084324501 <a title="226-tfidf-14" href="./acl-2011-Language_Use%3A_What_can_it_tell_us%3F.html">194 acl-2011-Language Use: What can it tell us?</a></p>
<p>15 0.081516504 <a title="226-tfidf-15" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>16 0.077184811 <a title="226-tfidf-16" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>17 0.073837593 <a title="226-tfidf-17" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>18 0.072043851 <a title="226-tfidf-18" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>19 0.06886816 <a title="226-tfidf-19" href="./acl-2011-IMASS%3A_An_Intelligent_Microblog_Analysis_and_Summarization_System.html">156 acl-2011-IMASS: An Intelligent Microblog Analysis and Summarization System</a></p>
<p>20 0.067754202 <a title="226-tfidf-20" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.116), (1, 0.087), (2, -0.074), (3, 0.0), (4, -0.055), (5, 0.25), (6, -0.061), (7, -0.058), (8, 0.117), (9, 0.008), (10, -0.007), (11, 0.033), (12, 0.043), (13, 0.016), (14, 0.019), (15, 0.012), (16, 0.098), (17, 0.056), (18, -0.053), (19, 0.01), (20, 0.062), (21, 0.078), (22, -0.02), (23, -0.021), (24, -0.007), (25, -0.053), (26, -0.026), (27, 0.021), (28, -0.015), (29, -0.067), (30, -0.005), (31, -0.083), (32, 0.076), (33, 0.033), (34, 0.051), (35, 0.031), (36, 0.028), (37, 0.042), (38, -0.037), (39, -0.022), (40, 0.092), (41, 0.06), (42, 0.013), (43, -0.109), (44, -0.082), (45, -0.006), (46, 0.044), (47, 0.057), (48, -0.017), (49, -0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96721214 <a title="226-lsi-1" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>Author: Sharon Gower Small ; Jennifer Strommer-Galley ; Tomek Strzalkowski</p><p>Abstract: We describe an annotation tool developed to assist in the creation of multimodal actioncommunication corpora from on-line massively multi-player games, or MMGs. MMGs typically involve groups of players (5-30) who control their perform various activities (questing, competing, fighting, etc.) and communicate via chat or speech using assumed screen names. We collected a corpus of 48 group quests in Second Life that jointly involved 206 players who generated over 30,000 messages in quasisynchronous chat during approximately 140 hours of recorded action. Multiple levels of coordinated annotation of this corpus (dialogue, movements, touch, gaze, wear, etc) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. The annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously. avatars1, 1</p><p>2 0.59791768 <a title="226-lsi-2" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>Author: S.R.K Branavan ; David Silver ; Regina Barzilay</p><p>Abstract: This paper presents a novel approach for leveraging automatically extracted textual knowledge to improve the performance of control applications such as games. Our ultimate goal is to enrich a stochastic player with highlevel guidance expressed in text. Our model jointly learns to identify text that is relevant to a given game state in addition to learning game strategies guided by the selected text. Our method operates in the Monte-Carlo search framework, and learns both text analysis and game strategies based only on environment feedback. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 27% absolute improvement and winning over 78% of games when playing against the built- . in AI of Civilization II. 1</p><p>3 0.57469648 <a title="226-lsi-3" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>4 0.52538669 <a title="226-lsi-4" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>Author: Fabrizio Morbini ; Kenji Sagae</p><p>Abstract: Individual utterances often serve multiple communicative purposes in dialogue. We present a data-driven approach for identification of multiple dialogue acts in single utterances in the context of dialogue systems with limited training data. Our approach results in significantly increased understanding of user intent, compared to two strong baselines.</p><p>5 0.51894468 <a title="226-lsi-5" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>Author: Paul Piwek ; Svetlana Stoyanchev</p><p>Abstract: This short paper introduces an implemented and evaluated monolingual Text-to-Text generation system. The system takes monologue and transforms it to two-participant dialogue. After briefly motivating the task of monologue-to-dialogue generation, we describe the system and present an evaluation in terms of fluency and accuracy.</p><p>6 0.4963724 <a title="226-lsi-6" href="./acl-2011-Multimodal_Menu-based_Dialogue_with_Speech_Cursor_in_DICO_II%2B.html">227 acl-2011-Multimodal Menu-based Dialogue with Speech Cursor in DICO II+</a></p>
<p>7 0.48948732 <a title="226-lsi-7" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>8 0.47758391 <a title="226-lsi-8" href="./acl-2011-Recognizing_Authority_in_Dialogue_with_an_Integer_Linear_Programming_Constrained_Model.html">260 acl-2011-Recognizing Authority in Dialogue with an Integer Linear Programming Constrained Model</a></p>
<p>9 0.47157335 <a title="226-lsi-9" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>10 0.46155342 <a title="226-lsi-10" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>11 0.46150222 <a title="226-lsi-11" href="./acl-2011-Using_Cross-Entity_Inference_to_Improve_Event_Extraction.html">328 acl-2011-Using Cross-Entity Inference to Improve Event Extraction</a></p>
<p>12 0.45172521 <a title="226-lsi-12" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>13 0.45163664 <a title="226-lsi-13" href="./acl-2011-Semantic_Information_and_Derivation_Rules_for_Robust_Dialogue_Act_Detection_in_a_Spoken_Dialogue_System.html">272 acl-2011-Semantic Information and Derivation Rules for Robust Dialogue Act Detection in a Spoken Dialogue System</a></p>
<p>14 0.43505108 <a title="226-lsi-14" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>15 0.42856836 <a title="226-lsi-15" href="./acl-2011-French_TimeBank%3A_An_ISO-TimeML_Annotated_Reference_Corpus.html">138 acl-2011-French TimeBank: An ISO-TimeML Annotated Reference Corpus</a></p>
<p>16 0.42470866 <a title="226-lsi-16" href="./acl-2011-Entrainment_in_Speech_Preceding_Backchannels..html">118 acl-2011-Entrainment in Speech Preceding Backchannels.</a></p>
<p>17 0.41542074 <a title="226-lsi-17" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>18 0.40283176 <a title="226-lsi-18" href="./acl-2011-PsychoSentiWordNet.html">253 acl-2011-PsychoSentiWordNet</a></p>
<p>19 0.402753 <a title="226-lsi-19" href="./acl-2011-Turn-Taking_Cues_in_a_Human_Tutoring_Corpus.html">312 acl-2011-Turn-Taking Cues in a Human Tutoring Corpus</a></p>
<p>20 0.39737597 <a title="226-lsi-20" href="./acl-2011-Dr_Sentiment_Knows_Everything%21.html">105 acl-2011-Dr Sentiment Knows Everything!</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.281), (5, 0.046), (17, 0.036), (26, 0.041), (31, 0.011), (37, 0.041), (39, 0.054), (41, 0.079), (53, 0.011), (55, 0.023), (59, 0.033), (72, 0.046), (88, 0.02), (91, 0.03), (96, 0.129), (97, 0.025), (98, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81178576 <a title="226-lda-1" href="./acl-2011-Multi-Modal_Annotation_of_Quest_Games_in_Second_Life.html">226 acl-2011-Multi-Modal Annotation of Quest Games in Second Life</a></p>
<p>Author: Sharon Gower Small ; Jennifer Strommer-Galley ; Tomek Strzalkowski</p><p>Abstract: We describe an annotation tool developed to assist in the creation of multimodal actioncommunication corpora from on-line massively multi-player games, or MMGs. MMGs typically involve groups of players (5-30) who control their perform various activities (questing, competing, fighting, etc.) and communicate via chat or speech using assumed screen names. We collected a corpus of 48 group quests in Second Life that jointly involved 206 players who generated over 30,000 messages in quasisynchronous chat during approximately 140 hours of recorded action. Multiple levels of coordinated annotation of this corpus (dialogue, movements, touch, gaze, wear, etc) are required in order to support development of automated predictors of selected real-life social and demographic characteristics of the players. The annotation tool presented in this paper was developed to enable efficient and accurate annotation of all dimensions simultaneously. avatars1, 1</p><p>2 0.63652158 <a title="226-lda-2" href="./acl-2011-A_New_Dataset_and_Method_for_Automatically_Grading_ESOL_Texts.html">20 acl-2011-A New Dataset and Method for Automatically Grading ESOL Texts</a></p>
<p>Author: Helen Yannakoudakis ; Ted Briscoe ; Ben Medlock</p><p>Abstract: We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.</p><p>3 0.52975184 <a title="226-lda-3" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>4 0.52650928 <a title="226-lda-4" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>Author: Donald Metzler ; Eduard Hovy ; Chunliang Zhang</p><p>Abstract: Paraphrase generation is an important task that has received a great deal of interest recently. Proposed data-driven solutions to the problem have ranged from simple approaches that make minimal use of NLP tools to more complex approaches that rely on numerous language-dependent resources. Despite all of the attention, there have been very few direct empirical evaluations comparing the merits of the different approaches. This paper empirically examines the tradeoffs between simple and sophisticated paraphrase harvesting approaches to help shed light on their strengths and weaknesses. Our evaluation reveals that very simple approaches fare surprisingly well and have a number of distinct advantages, including strong precision, good coverage, and low redundancy.</p><p>5 0.52503306 <a title="226-lda-5" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>Author: Sameer Singh ; Amarnag Subramanya ; Fernando Pereira ; Andrew McCallum</p><p>Abstract: Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach.</p><p>6 0.5242272 <a title="226-lda-6" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>7 0.52369195 <a title="226-lda-7" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>8 0.52317339 <a title="226-lda-8" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>9 0.52315986 <a title="226-lda-9" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>10 0.52227771 <a title="226-lda-10" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>11 0.52141023 <a title="226-lda-11" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>12 0.52121031 <a title="226-lda-12" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>13 0.52011597 <a title="226-lda-13" href="./acl-2011-MACAON_An_NLP_Tool_Suite_for_Processing_Word_Lattices.html">215 acl-2011-MACAON An NLP Tool Suite for Processing Word Lattices</a></p>
<p>14 0.51975083 <a title="226-lda-14" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<p>15 0.51973927 <a title="226-lda-15" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>16 0.51969063 <a title="226-lda-16" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>17 0.51898569 <a title="226-lda-17" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>18 0.51891977 <a title="226-lda-18" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>19 0.51868463 <a title="226-lda-19" href="./acl-2011-I_Thou_Thee%2C_Thou_Traitor%3A_Predicting_Formal_vs._Informal_Address_in_English_Literature.html">157 acl-2011-I Thou Thee, Thou Traitor: Predicting Formal vs. Informal Address in English Literature</a></p>
<p>20 0.51847684 <a title="226-lda-20" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
