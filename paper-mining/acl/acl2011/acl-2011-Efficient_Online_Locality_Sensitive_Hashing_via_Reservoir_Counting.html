<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 acl-2011-Efficient Online Locality Sensitive Hashing via Reservoir Counting</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-113" href="#">acl2011-113</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 acl-2011-Efficient Online Locality Sensitive Hashing via Reservoir Counting</h1>
<br/><p>Source: <a title="acl-2011-113-pdf" href="http://aclweb.org/anthology//P/P11/P11-2004.pdf">pdf</a></p><p>Author: Benjamin Van Durme ; Ashwin Lall</p><p>Abstract: We describe a novel mechanism called Reservoir Counting for application in online Locality Sensitive Hashing. This technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.</p><p>Reference: <a title="acl-2011-113-reference" href="../acl2011_reference/acl-2011-Efficient_Online_Locality_Sensitive_Hashing_via_Reservoir_Counting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Efficient Online Locality Sensitive Hashing via Reservoir Counting  Benjamin Van Durme HLTCOE Johns Hopkins University Abstract We describe a novel mechanism called Reservoir Counting for application in online Locality Sensitive Hashing. [sent-1, score-0.113]
</p><p>2 This technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint. [sent-2, score-0.286]
</p><p>3 1 Introduction Feature vectors based on lexical co-occurrence are often of a high dimension, d. [sent-3, score-0.036]
</p><p>4 This leads to O(d) operations to calculate cosine similarity, a fundamental tool in distributional semantics. [sent-4, score-0.112]
</p><p>5 This is improved in practice through the use of data structures that exploit feature sparsity, leading to an expected O(f) operations, where f is the number of unique features  we expect to have non-zero entries in a given vector. [sent-5, score-0.066]
</p><p>6 Such LSH bit signatures are constructed using the following hash function, where ∈ Rd is a vector in the original feature space, ahnerde er ~v is ∈ randomly drawn from N(0, 1)d:  v  h(~ v) =? [sent-8, score-0.246]
</p><p>7 d, which leads to faster pair-wise comparisons bbe ? [sent-12, score-0.038]
</p><p>8 twe de,n w vectors, adnsd t a lfoaswteerr memory footprint. [sent-13, score-0.062]
</p><p>9 Van Durme and Lall (2010) observed1 that if the feature values are additive over a dataset (e. [sent-14, score-0.052]
</p><p>10 , when collecting word co-occurrence frequencies), then these signatures may be constructed online by unrolling the dot-product into a series of local operations: ri = Σt~ vt · ri, where vt represents features oabtisoenrsv:e v~ d· locally at ti· rm~e t in a data-stream. [sent-16, score-0.213]
</p><p>11 Since updates may be done locally, feature vectors do not need to be stored explicitly. [sent-17, score-0.227]
</p><p>12 This directly leads to significant space savings, as only one counter is needed for each of the b running sums. [sent-18, score-0.214]
</p><p>13 In this work we focus on the following observation: the counters used to store the running sums may themselves be an inefficient use of space, in that they may be amenable to compression through approximation. [sent-19, score-0.039]
</p><p>14 2 Since the accuracy ofthis LSH routine is a function of b, then if we were able to reduce the online requirements of each counter, we might afford a larger number of projections. [sent-20, score-0.075]
</p><p>15 Even if a chance of approximation error were introduced for each hash function, this may be justified in greater  v·  overall fidelity from the resultant increase in b. [sent-21, score-0.212]
</p><p>16 We show experimentally that this leads to greater accuracy approximations at the same memory cost, or similar accuracy approximations at a significantly reduced cost. [sent-28, score-0.146]
</p><p>17 This result is relevant to work in large-scale distributional semantics (Bhagat and Ravichandran, 2008; Van Durme and Lall, 2009; Pantel et al. [sent-29, score-0.041]
</p><p>18 We then employ an integer-valued projection matrix in order to work with an integer-valued stream of online updates, which is reduced (implicitly) to a stream of positive and negative unit updates. [sent-35, score-0.77]
</p><p>19 The sign of the sum of these updates is approximated through a novel twist on Reservoir Sampling. [sent-36, score-0.247]
</p><p>20 When computed explicitly this leads to an impractical mechanism linear in each feature value update. [sent-37, score-0.146]
</p><p>21 To ensure our counter can (approximately) add and subtract in constant time, we then derive expressions for the expected value of each step of the update. [sent-38, score-0.232]
</p><p>22 Unit Projection Rather than construct a projection matrix from N(0, 1), a matrix randomly populated with entries from the set {−1, 0, 1} will suffice, dw withit quality dependent on {th−e r,e0l,at1iv}e w proportion of these elements. [sent-40, score-0.209]
</p><p>23 If we let p be the percent probability mass allocated to zeros, then we create a discrete projection matrix by sampling from the  (1−2p  1−2p  multinomial: : −1,p : 0, : +1). [sent-41, score-0.198]
</p><p>24 Henceforth we assume this discrete projection matrix, with p = 0. [sent-44, score-0.093]
</p><p>25 3 The use of such sparse projections was first proposed by Achlioptas (2003), then extended by Li et al. [sent-46, score-0.092]
</p><p>26 19  Figure 1: With b = 256, mean absolute error in cosine  approximation when using a projection based on N(0, 1), compared to {−1, 0, 1}. [sent-49, score-0.153]
</p><p>27 Unit Stream Based on a unit projection, we can view an online counter as summing over a stream drawn from {−1, 1}: each projected feature value udnrarowlnle fdr oinmto { i−ts1 (positive or negative) unary representation. [sent-50, score-0.604]
</p><p>28 Reservoir Sampling We can maintain a uniform sample of size k over a stream of unknown length as follows. [sent-52, score-0.263]
</p><p>29 Accept the first k elements into an reservoir (array) of size k. [sent-53, score-0.836]
</p><p>30 Each following element at position n is accepted with probability whereupon an element currently in the reservoir is evicted, and replaced with the just accepted item. [sent-54, score-0.905]
</p><p>31 Reservoir sampling is a folklore algorithm that was extended by Vitter (1985) to allow for multiple updates. [sent-56, score-0.036]
</p><p>32 nk,  Reservoir Counting If we are sampling over a stream drawn from just two values, we can implic-  itly represent the reservoir by counting only the frequency of one or the other elements. [sent-57, score-1.133]
</p><p>33 counter, s, for tracking the number of 1values currently in the reservoir. [sent-59, score-0.03]
</p><p>34 5 When a negative value is accepted, we decrement the counter with probability ks . [sent-60, score-0.306]
</p><p>35 When a positive update is accepted, we increment the counter with probability (1−ks). [sent-61, score-0.311]
</p><p>36 This reflects an update evicting e pirthoebra an teyle (m1−ent of the same sign, which has no effect on the makeup of the reservoir, or decreasing/increasing the number of 1’s currently sampled. [sent-62, score-0.096]
</p><p>37 An approximate sum of all values seen up to position n is then simply: n(2ks − 1). [sent-63, score-0.058]
</p><p>38 While this value is potentially interesting in fut−ure 1 applications, here we are only concerned with its sign. [sent-64, score-0.073]
</p><p>39 Parallel Reservoir Counting On its own this counting mechanism hardly appears useful: as it is dependent on knowing n, then we might just as well  sum the elements of the stream directly, counting in whatever space we would otherwise use in maintaining the value of n. [sent-65, score-0.695]
</p><p>40 However, if we have a set of tied streams that we process in parallel,6 then we only need to track n once, across b different streams, each with their own reservoir. [sent-66, score-0.126]
</p><p>41 When dealing with parallel streams resulting from different random projections of the same vector, we cannot assume these will be strictly tied. [sent-67, score-0.218]
</p><p>42 Some projections will cancel out heavier elements than others, leading to update streams of different lengths once elements are unrolled into their (positive or negative) unary representation. [sent-68, score-0.431]
</p><p>43 In practice we have found that tracking the mean value of n across b streams is sufficient. [sent-69, score-0.174]
</p><p>44 5 zeroed matrix, we can update n by one half the magnitude of each observed value, as on average half the projections will cancel out any given element. [sent-71, score-0.275]
</p><p>45 This step can be found in Algorithm 2, lines 8 and 9. [sent-72, score-0.028]
</p><p>46 Example To make concrete what we have covered to this point, consider a given feature vector of dimensionality d = 3, say: [3, 2, 1]. [sent-73, score-0.028]
</p><p>47 This might be projected into b = 4, vectors: [3, 0, 0], [0, -2, 1], [0, 0, 1], and [-3, 2, 0]. [sent-74, score-0.036]
</p><p>48 When viewed as  positive/negative, loosely-tied unit streams, they respectively have length n: 3, 3, 1, and 5, with mean length 3. [sent-75, score-0.062]
</p><p>49 The goal of reservoir counting is to efficiently keep track of an approximation of their sums (here: 3, -1, 1, and -1), while the underlying feature 5E. [sent-76, score-1.039]
</p><p>50 , a reservoir of size k = 255 requires an 8-bit integer. [sent-78, score-0.785]
</p><p>51 6Tied in the sense that each stream is of the same length, e. [sent-79, score-0.209]
</p><p>52 A k = 3 reservoir used for the last projected vector, [-3, 2, 0], might reasonably contain two values of -1, and one value of 1. [sent-86, score-0.857]
</p><p>53 7 Represented explicitly as a vector, the reservoir would thus be in the arrangement: [1, -1, -1], [-1, 1, -1], or [-1, -1, 1]. [sent-87, score-0.755]
</p><p>54 These are functionally equivalent: we only need to know that one of the k = 3 elements is positive. [sent-88, score-0.051]
</p><p>55 Expected Number ofSamples Traversing m consecutive values of either 1or −1 in the unit stream sshecouutlidv e be v thought eoifth as seeing positive or negative m as a feature update. [sent-89, score-0.402]
</p><p>56 For a reservoir of size k, let A(m, n, k) be the number of samples accepted when traversing the stream from position n + 1to n + m. [sent-90, score-1.115]
</p><p>57 A is non-deterministic: it represents the results of flipping m consecutive coins, where each coin is increasingly biased towards rejection. [sent-91, score-0.036]
</p><p>58 Rather than computing A explicitly, which is linear in m, we will instead use the expected number of updates, A0(m, n, k) = E[A(m, n, k)], which can be computed in constant time. [sent-92, score-0.038]
</p><p>59 For example, consider m = 30, encountered at  position n = 100, with a reservoir of k = 10. [sent-94, score-0.755]
</p><p>60 As the reservoir is a discre)te ≈ set of bins, pflreasct oifon 1a. [sent-97, score-0.755]
</p><p>61 l portions of a sample are resolved by a coin flip: if a = k loge(n+nm), then accept u = dae samples with probability (a bac ), aepndt u = bac samples −  with  γ,  probabili  7Other options are: three -1’s, or oneP -1 and two 1’s. [sent-98, score-0.401]
</p><p>62 8With x a positive integer, H(x) = Pix=1 1/x ≈ loge (x) + where γ is Euler’s constant. [sent-99, score-0.126]
</p><p>63 These steps are found in lines 3 and 4 of Algorithm 1. [sent-101, score-0.028]
</p><p>64 See Table 1 for simulation results using a variety of parameters. [sent-102, score-0.024]
</p><p>65 Expected Reservoir Change We now discuss how to simulate many independent updates of the same type to the reservoir counter, e. [sent-103, score-0.918]
</p><p>66 : five updates of 1, or three updates of -1, using a single estimate. [sent-105, score-0.326]
</p><p>67 Consider a situation in which we have a reservoir of size k with some current value of s, 0 s k, and we ew kis wh ttoh perform u independent updates. [sent-106, score-0.827]
</p><p>68 W ke, adnednote by Uk0(s, u) the expected value of the reservoir after these u updates have taken place. [sent-107, score-0.998]
</p><p>69 Since a single update leads to no change with probability ks, we can write the following recurrence for Uk0:  ≤ ≤  Uk0(s,u) =ksUk0(s,u−1)+kk − sUk0(s+1,u−1), with the boundary condition: for all s, Uk0(s, 0) = s. [sent-108, score-0.192]
</p><p>70 Solving the above recurrence, we get that the expected value of the reservoir after these updates is:  Uk0(s,u) = k + (s − k)? [sent-109, score-0.998]
</p><p>71 The case for negative updates follows similarly (see lines 7 and 8 of Algorithm 1). [sent-112, score-0.233]
</p><p>72 Hence, instead of simulating u independent updates of the same type to the reservoir, we simply update it to this expected value, where fractional updates are handled similarly as when estimating the number of accepts. [sent-113, score-0.492]
</p><p>73 These steps are found in lines 5 through 9 of Algorithm 1, and as seen in Fig. [sent-114, score-0.028]
</p><p>74 3, which shows the use of reservoir counting in Online Locality Sensitive Hashing (as made explicit in Algorithm 2), as compared to the method described by Van Durme  and Lall (2010). [sent-117, score-0.888]
</p><p>75 The total amount of space required when using this counting scheme is b log2 (k + 1) + 32: b reservoirs, and a 32 bit integer to track n. [sent-118, score-0.283]
</p><p>76 This is compared to b 32 bit floating point values, as is standard. [sent-119, score-0.092]
</p><p>77 Note that our scheme comes away with similar levels of accuracy, often at half the memory cost, while requiring larger b to account for the chance of approximation errors in individual reservoir counters. [sent-120, score-0.901]
</p><p>78 21  Figure 2: Results of simulating many iterations of U0,  for k = 255, and various values of s and u. [sent-121, score-0.056]
</p><p>79 Algorithm 1 RESERVOIRUPDATE(n,k,m,σ,s) Parameters: n : size of stream so far k : size of reservoir, also maximum value of s m : magnitude of update σ : sign of update s : current value of reservoir 1: if m = 0 or σ = 0 then 2: Return without doing an? [sent-122, score-1.35]
</p><p>80 standard counting mechanisms (blue), as measured by the amount of total memory required to the resultant error. [sent-129, score-0.229]
</p><p>81 Algorithm 2 COMPUTESIGNATURE(S,k,b,p) Parameters: S : bit array of size b k : size of each reservoir b : number of projections p : percentage of zeros in projection, p ∈ [0, 1] 1: Initialize b reservoirs R[1, . [sent-130, score-1.065]
</p><p>82 , b], each represented by a log2 (k + 1)-bit unsigned integer  2: Initialize b hash functions hi (w) that map features w to elements in a vector made up of −1 and 1 each twoit ehl proportion , aonrd m m0a date proportion p. [sent-133, score-0.273]
</p><p>83 time, where a high-throughput streaming application that  is not concerned with online memory requirements will not have reason to consider the developments in this article. [sent-139, score-0.27]
</p><p>84 The approach given here is motivated by cases where data is not flooding in at breakneck speed, and resource considerations are dominated by a large number of unique elements for which we are maintaining signatures. [sent-140, score-0.082]
</p><p>85 Dark or light states correspond to a prediction of a running sum being positive or negative. [sent-143, score-0.071]
</p><p>86 States are numerically labeled to reflect the similarity to  a small bit integer data type, one that never overflows. [sent-144, score-0.131]
</p><p>87 Assuming a fixed probability of a positive versus negative update, then in expectation the state of the chain should correspond to the sign. [sent-146, score-0.139]
</p><p>88 However if we are concerned with the global statistic, as we are here, then the assumption of a fixed probability update precludes the analysis of streaming sources that contain local irregularities. [sent-147, score-0.255]
</p><p>89 9 In distributional semantics, consider a feature stream formed by sequentially reading the n-gram resource of Brants and Franz (2006). [sent-148, score-0.278]
</p><p>90 The pair: (the dog : 3,502,485), can be viewed as a feature value pair: (leftWord= ’the ’ : 3,502,485), with respect to online signature generation for the word dog. [sent-149, score-0.185]
</p><p>91 Rather than viewing this feature repeatedly, spread over a large corpus, the update happens just once, with large magnitude. [sent-150, score-0.124]
</p><p>92 Reservoir Counting, representing an online uniform sample, is agnostic to the ordering of elements in the stream. [sent-153, score-0.126]
</p><p>93 4  Conclusion  We have presented a novel approximation scheme we call Reservoir Counting, motivated here by a desire for greater space efficiency in Online Locality Sensitive Hashing. [sent-154, score-0.084]
</p><p>94 Going beyond our results provided for synthetic data, future work will explore applications of this technique, such as in experiments with streaming social media like Twitter. [sent-155, score-0.102]
</p><p>95 ,1,1,-1,-1,-1), is overall positive, but locally negative at the end. [sent-160, score-0.074]
</p><p>96 Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. [sent-190, score-0.06]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reservoir', 0.755), ('stream', 0.209), ('updates', 0.163), ('counter', 0.152), ('counting', 0.133), ('durme', 0.118), ('hash', 0.118), ('lall', 0.111), ('streaming', 0.102), ('streams', 0.102), ('update', 0.096), ('projection', 0.093), ('projections', 0.092), ('locality', 0.089), ('bac', 0.089), ('loge', 0.089), ('online', 0.075), ('lsh', 0.072), ('van', 0.065), ('unit', 0.062), ('memory', 0.062), ('accepted', 0.062), ('approximation', 0.06), ('ashwin', 0.059), ('bit', 0.058), ('elements', 0.051), ('sign', 0.05), ('sensitive', 0.047), ('integer', 0.044), ('dae', 0.044), ('goemans', 0.044), ('reservoirs', 0.044), ('reservoirupdate', 0.044), ('ks', 0.044), ('matrix', 0.043), ('negative', 0.042), ('signatures', 0.042), ('value', 0.042), ('distributional', 0.041), ('benjamin', 0.04), ('signature', 0.04), ('sums', 0.039), ('bsc', 0.039), ('cancel', 0.039), ('goyal', 0.039), ('indyk', 0.039), ('petrovic', 0.039), ('expected', 0.038), ('leads', 0.038), ('mechanism', 0.038), ('positive', 0.037), ('bergsma', 0.037), ('ravichandran', 0.037), ('projected', 0.036), ('sampling', 0.036), ('coin', 0.036), ('vectors', 0.036), ('kenneth', 0.035), ('accept', 0.035), ('chain', 0.034), ('ping', 0.034), ('bm', 0.034), ('floating', 0.034), ('resultant', 0.034), ('sum', 0.034), ('operations', 0.033), ('simulating', 0.032), ('recurrence', 0.032), ('vt', 0.032), ('locally', 0.032), ('concerned', 0.031), ('maintaining', 0.031), ('zeros', 0.031), ('hb', 0.031), ('savings', 0.031), ('tracking', 0.03), ('size', 0.03), ('traversing', 0.03), ('deepak', 0.03), ('proportion', 0.03), ('pantel', 0.029), ('samples', 0.029), ('hashing', 0.029), ('shane', 0.029), ('bhagat', 0.029), ('similarity', 0.029), ('lines', 0.028), ('feature', 0.028), ('sketch', 0.028), ('cos', 0.027), ('probability', 0.026), ('array', 0.025), ('random', 0.024), ('simulation', 0.024), ('track', 0.024), ('values', 0.024), ('sample', 0.024), ('space', 0.024), ('half', 0.024), ('approximations', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="113-tfidf-1" href="./acl-2011-Efficient_Online_Locality_Sensitive_Hashing_via_Reservoir_Counting.html">113 acl-2011-Efficient Online Locality Sensitive Hashing via Reservoir Counting</a></p>
<p>Author: Benjamin Van Durme ; Ashwin Lall</p><p>Abstract: We describe a novel mechanism called Reservoir Counting for application in online Locality Sensitive Hashing. This technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.</p><p>2 0.10945757 <a title="113-tfidf-2" href="./acl-2011-Semi-Supervised_SimHash_for_Efficient_Document_Similarity_Search.html">276 acl-2011-Semi-Supervised SimHash for Efficient Document Similarity Search</a></p>
<p>Author: Qixia Jiang ; Maosong Sun</p><p>Abstract: Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance. 1</p><p>3 0.095409237 <a title="113-tfidf-3" href="./acl-2011-K-means_Clustering_with_Feature_Hashing.html">189 acl-2011-K-means Clustering with Feature Hashing</a></p>
<p>Author: Hajime Senuma</p><p>Abstract: One of the major problems of K-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task.</p><p>4 0.085847117 <a title="113-tfidf-4" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>Author: Adam Pauls ; Dan Klein</p><p>Abstract: N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.</p><p>5 0.041080557 <a title="113-tfidf-5" href="./acl-2011-Confidence-Weighted_Learning_of_Factored_Discriminative_Language_Models.html">78 acl-2011-Confidence-Weighted Learning of Factored Discriminative Language Models</a></p>
<p>Author: Viet Ha Thuc ; Nicola Cancedda</p><p>Abstract: Language models based on word surface forms only are unable to benefit from available linguistic knowledge, and tend to suffer from poor estimates for rare features. We propose an approach to overcome these two limitations. We use factored features that can flexibly capture linguistic regularities, and we adopt confidence-weighted learning, a form of discriminative online learning that can better take advantage of a heavy tail of rare features. Finally, we extend the confidence-weighted learning to deal with label noise in training data, a common case with discriminative lan- guage modeling.</p><p>6 0.040870946 <a title="113-tfidf-6" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>7 0.039410714 <a title="113-tfidf-7" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>8 0.038017735 <a title="113-tfidf-8" href="./acl-2011-Latent_Semantic_Word_Sense_Induction_and_Disambiguation.html">198 acl-2011-Latent Semantic Word Sense Induction and Disambiguation</a></p>
<p>9 0.037878204 <a title="113-tfidf-9" href="./acl-2011-Using_Multiple_Sources_to_Construct_a_Sentiment_Sensitive_Thesaurus_for_Cross-Domain_Sentiment_Classification.html">332 acl-2011-Using Multiple Sources to Construct a Sentiment Sensitive Thesaurus for Cross-Domain Sentiment Classification</a></p>
<p>10 0.036918182 <a title="113-tfidf-10" href="./acl-2011-An_Empirical_Evaluation_of_Data-Driven_Paraphrase_Generation_Techniques.html">37 acl-2011-An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques</a></p>
<p>11 0.035740055 <a title="113-tfidf-11" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>12 0.035413798 <a title="113-tfidf-12" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>13 0.034942843 <a title="113-tfidf-13" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>14 0.034507342 <a title="113-tfidf-14" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>15 0.034410339 <a title="113-tfidf-15" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>16 0.034005128 <a title="113-tfidf-16" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>17 0.033301402 <a title="113-tfidf-17" href="./acl-2011-Unsupervised_Part-of-Speech_Tagging_with_Bilingual_Graph-Based_Projections.html">323 acl-2011-Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</a></p>
<p>18 0.033283096 <a title="113-tfidf-18" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>19 0.032836497 <a title="113-tfidf-19" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>20 0.032543737 <a title="113-tfidf-20" href="./acl-2011-Partial_Parsing_from_Bitext_Projections.html">243 acl-2011-Partial Parsing from Bitext Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.102), (1, 0.016), (2, -0.01), (3, 0.003), (4, -0.014), (5, -0.029), (6, 0.007), (7, 0.007), (8, -0.012), (9, -0.005), (10, 0.009), (11, 0.01), (12, 0.024), (13, 0.03), (14, -0.02), (15, 0.016), (16, -0.054), (17, 0.008), (18, 0.002), (19, -0.028), (20, 0.069), (21, -0.031), (22, 0.047), (23, -0.029), (24, -0.05), (25, -0.08), (26, 0.002), (27, 0.015), (28, 0.062), (29, -0.06), (30, 0.11), (31, 0.033), (32, 0.125), (33, 0.148), (34, 0.008), (35, -0.056), (36, 0.042), (37, -0.028), (38, -0.009), (39, 0.012), (40, -0.027), (41, 0.033), (42, -0.024), (43, -0.012), (44, -0.006), (45, 0.022), (46, 0.055), (47, -0.013), (48, -0.001), (49, -0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90831405 <a title="113-lsi-1" href="./acl-2011-Efficient_Online_Locality_Sensitive_Hashing_via_Reservoir_Counting.html">113 acl-2011-Efficient Online Locality Sensitive Hashing via Reservoir Counting</a></p>
<p>Author: Benjamin Van Durme ; Ashwin Lall</p><p>Abstract: We describe a novel mechanism called Reservoir Counting for application in online Locality Sensitive Hashing. This technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.</p><p>2 0.87858623 <a title="113-lsi-2" href="./acl-2011-K-means_Clustering_with_Feature_Hashing.html">189 acl-2011-K-means Clustering with Feature Hashing</a></p>
<p>Author: Hajime Senuma</p><p>Abstract: One of the major problems of K-means is that one must use dense vectors for its centroids, and therefore it is infeasible to store such huge vectors in memory when the feature space is high-dimensional. We address this issue by using feature hashing (Weinberger et al., 2009), a dimension-reduction technique, which can reduce the size of dense vectors while retaining sparsity of sparse vectors. Our analysis gives theoretical motivation and justification for applying feature hashing to Kmeans, by showing how much will the objective of K-means be (additively) distorted. Furthermore, to empirically verify our method, we experimented on a document clustering task.</p><p>3 0.87671244 <a title="113-lsi-3" href="./acl-2011-Semi-Supervised_SimHash_for_Efficient_Document_Similarity_Search.html">276 acl-2011-Semi-Supervised SimHash for Efficient Document Similarity Search</a></p>
<p>Author: Qixia Jiang ; Maosong Sun</p><p>Abstract: Searching documents that are similar to a query document is an important component in modern information retrieval. Some existing hashing methods can be used for efficient document similarity search. However, unsupervised hashing methods cannot incorporate prior knowledge for better hashing. Although some supervised hashing methods can derive effective hash functions from prior knowledge, they are either computationally expensive or poorly discriminative. This paper proposes a novel (semi-)supervised hashing method named Semi-Supervised SimHash (S3H) for high-dimensional data similarity search. The basic idea of S3H is to learn the optimal feature weights from prior knowledge to relocate the data such that similar data have similar hash codes. We evaluate our method with several state-of-the-art methods on two large datasets. All the results show that our method gets the best performance. 1</p><p>4 0.63746876 <a title="113-lsi-4" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>Author: Adam Pauls ; Dan Klein</p><p>Abstract: N-gram language models are a major resource bottleneck in machine translation. In this paper, we present several language model implementations that are both highly compact and fast to query. Our fastest implementation is as fast as the widely used SRILM while requiring only 25% of the storage. Our most compact representation can store all 4 billion n-grams and associated counts for the Google n-gram corpus in 23 bits per n-gram, the most compact lossless representation to date, and even more compact than recent lossy compression techniques. We also discuss techniques for improving query speed during decoding, including a simple but novel language model caching technique that improves the query speed of our language models (and SRILM) by up to 300%.</p><p>5 0.4639689 <a title="113-lsi-5" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>Author: Hakan Ceylan ; Rada Mihalcea</p><p>Abstract: We introduce a new publicly available tool that implements efficient indexing and retrieval of large N-gram datasets, such as the Web1T 5-gram corpus. Our tool indexes the entire Web1T dataset with an index size of only 100 MB and performs a retrieval of any N-gram with a single disk access. With an increased index size of 420 MB and duplicate data, it also allows users to issue wild card queries provided that the wild cards in the query are contiguous. Furthermore, we also implement some of the smoothing algorithms that are designed specifically for large datasets and are shown to yield better language models than the traditional ones on the Web1T 5gram corpus (Yuret, 2008). We demonstrate the effectiveness of our tool and the smoothing algorithms on the English Lexical Substi- tution task by a simple implementation that gives considerable improvement over a basic language model.</p><p>6 0.40546247 <a title="113-lsi-6" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>7 0.40187997 <a title="113-lsi-7" href="./acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution.html">212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</a></p>
<p>8 0.38920346 <a title="113-lsi-8" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>9 0.3848466 <a title="113-lsi-9" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>10 0.38080192 <a title="113-lsi-10" href="./acl-2011-Unsupervised_Decomposition_of_a_Document_into_Authorial_Components.html">319 acl-2011-Unsupervised Decomposition of a Document into Authorial Components</a></p>
<p>11 0.3656958 <a title="113-lsi-11" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<p>12 0.35623017 <a title="113-lsi-12" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>13 0.34617814 <a title="113-lsi-13" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>14 0.3444559 <a title="113-lsi-14" href="./acl-2011-Optimal_Head-Driven_Parsing_Complexity_for_Linear_Context-Free_Rewriting_Systems.html">234 acl-2011-Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems</a></p>
<p>15 0.33547044 <a title="113-lsi-15" href="./acl-2011-Tier-based_Strictly_Local_Constraints_for_Phonology.html">303 acl-2011-Tier-based Strictly Local Constraints for Phonology</a></p>
<p>16 0.33366477 <a title="113-lsi-16" href="./acl-2011-%2811-06-spirl%29.html">1 acl-2011-(11-06-spirl)</a></p>
<p>17 0.33239079 <a title="113-lsi-17" href="./acl-2011-full-for-print.html">342 acl-2011-full-for-print</a></p>
<p>18 0.33187985 <a title="113-lsi-18" href="./acl-2011-SystemT%3A_A_Declarative_Information_Extraction_System.html">291 acl-2011-SystemT: A Declarative Information Extraction System</a></p>
<p>19 0.33130097 <a title="113-lsi-19" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>20 0.3248989 <a title="113-lsi-20" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (5, 0.018), (17, 0.066), (26, 0.025), (37, 0.083), (39, 0.036), (41, 0.099), (53, 0.012), (55, 0.06), (59, 0.032), (72, 0.022), (75, 0.287), (91, 0.026), (96, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94229549 <a title="113-lda-1" href="./acl-2011-Tier-based_Strictly_Local_Constraints_for_Phonology.html">303 acl-2011-Tier-based Strictly Local Constraints for Phonology</a></p>
<p>Author: Jeffrey Heinz ; Chetan Rawal ; Herbert G. Tanner</p><p>Abstract: Beginning with Goldsmith (1976), the phonological tier has a long history in phonological theory to describe non-local phenomena. This paper defines a class of formal languages, the Tier-based Strictly Local languages, which begin to describe such phenomena. Then this class is located within the Subregular Hierarchy (McNaughton and Papert, 1971). It is found that these languages contain the Strictly Local languages, are star-free, are incomparable with other known sub-star-free classes, and have other interesting properties.</p><p>same-paper 2 0.79007632 <a title="113-lda-2" href="./acl-2011-Efficient_Online_Locality_Sensitive_Hashing_via_Reservoir_Counting.html">113 acl-2011-Efficient Online Locality Sensitive Hashing via Reservoir Counting</a></p>
<p>Author: Benjamin Van Durme ; Ashwin Lall</p><p>Abstract: We describe a novel mechanism called Reservoir Counting for application in online Locality Sensitive Hashing. This technique allows for significant savings in the streaming setting, allowing for maintaining a larger number of signatures, or an increased level of approximation accuracy at a similar memory footprint.</p><p>3 0.76774114 <a title="113-lda-3" href="./acl-2011-The_Arabic_Online_Commentary_Dataset%3A_an_Annotated_Dataset_of_Informal_Arabic_with_High_Dialectal_Content.html">299 acl-2011-The Arabic Online Commentary Dataset: an Annotated Dataset of Informal Arabic with High Dialectal Content</a></p>
<p>Author: Omar F. Zaidan ; Chris Callison-Burch</p><p>Abstract: The written form of Arabic, Modern Standard Arabic (MSA), differs quite a bit from the spoken dialects of Arabic, which are the true “native” languages of Arabic speakers used in daily life. However, due to MSA’s prevalence in written form, almost all Arabic datasets have predominantly MSA content. We present the Arabic Online Commentary Dataset, a 52M-word monolingual dataset rich in dialectal content, and we describe our long-term annotation effort to identify the dialect level (and dialect itself) in each sentence of the dataset. So far, we have labeled 108K sentences, 41% of which as having dialectal content. We also present experimental results on the task of automatic dialect identification, using the collected labels for training and evaluation.</p><p>4 0.6532501 <a title="113-lda-4" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>Author: Nguyen Bach ; Fei Huang ; Yaser Al-Onaizan</p><p>Abstract: State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict wordlevel and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improve- ments between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.</p><p>5 0.64248848 <a title="113-lda-5" href="./acl-2011-Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation.html">72 acl-2011-Collecting Highly Parallel Data for Paraphrase Evaluation</a></p>
<p>Author: David Chen ; William Dolan</p><p>Abstract: A lack of standard datasets and evaluation metrics has prevented the field of paraphrasing from making the kind of rapid progress enjoyed by the machine translation community over the last 15 years. We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale. The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates. In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.</p><p>6 0.55909324 <a title="113-lda-6" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>7 0.55675435 <a title="113-lda-7" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>8 0.55542022 <a title="113-lda-8" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>9 0.55135787 <a title="113-lda-9" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>10 0.55048728 <a title="113-lda-10" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>11 0.55034125 <a title="113-lda-11" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>12 0.54966789 <a title="113-lda-12" href="./acl-2011-Ordering_Prenominal_Modifiers_with_a_Reranking_Approach.html">237 acl-2011-Ordering Prenominal Modifiers with a Reranking Approach</a></p>
<p>13 0.54877651 <a title="113-lda-13" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>14 0.54871601 <a title="113-lda-14" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>15 0.54660076 <a title="113-lda-15" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>16 0.54459941 <a title="113-lda-16" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>17 0.54434389 <a title="113-lda-17" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>18 0.54362798 <a title="113-lda-18" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>19 0.54351318 <a title="113-lda-19" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>20 0.54307342 <a title="113-lda-20" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
