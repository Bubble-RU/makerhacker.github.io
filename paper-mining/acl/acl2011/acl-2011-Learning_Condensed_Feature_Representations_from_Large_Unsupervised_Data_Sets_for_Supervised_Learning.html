<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 acl-2011-Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-199" href="#">acl2011-199</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>199 acl-2011-Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning</h1>
<br/><p>Source: <a title="acl-2011-199-pdf" href="http://aclweb.org/anthology//P/P11/P11-2112.pdf">pdf</a></p><p>Author: Jun Suzuki ; Hideki Isozaki ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative ‘condensed feature representations’ from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III. ,</p><p>Reference: <a title="acl-2011-199-reference" href="../acl2011_reference/acl-2011-Learning_Condensed_Feature_Representations_from_Large_Unsupervised_Data_Sets_for_Supervised_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 i o z aki hideki s  Abstract This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. [sent-4, score-0.418]
</p><p>2 We use unsupervised data to generate informative ‘condensed feature representations’ from the original feature set used in supervised NLP systems. [sent-5, score-0.491]
</p><p>3 The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. [sent-6, score-0.212]
</p><p>4 72 with 344 features for CoNLL-2003 NER data, and UAS 93. [sent-10, score-0.046]
</p><p>5 ,  1 Introduction In the last decade, supervised learning has become a standard way to train the models of many natural language processing (NLP) systems. [sent-13, score-0.189]
</p><p>6 One simple but powerful approach for further enhancing the performance is to utilize a large amount of unsupervised data to supplement supervised data. [sent-14, score-0.203]
</p><p>7 j p ab simple and general framework, like the iCWR approach, to enhance existing state-of-the-art super-  vised NLP systems. [sent-23, score-0.02]
</p><p>8 The differences between the iCWR approach and our method are as follows; suppose F is the original feature set used in supervised learning, sC t hise t ohrei gCinWalR f efeatauturere s set, saendd i Hn s iusp ptherev new fleeaatrunrien gs,e Ct generated by our mree stehto,d a. [sent-24, score-0.337]
</p><p>9 n Then, w thieth n ethwe iCWR approach, C is induced independently from F, WanRd aupspedro ianc ahd,d Citi iosn i to uFce eind supervised learning, i. [sent-25, score-0.166]
</p><p>10 m In F co wnitrthas tth, ein help mofe an existing mireocdtleyl already t rfraoinmed F by supervised learning wistiitnhg F, oadnedl auslreeda diny place odf b bFy sinu supervised learning. [sent-31, score-0.399]
</p><p>11 The largest contribution of our method is that it offers an architecture that can drastically reduce the number of features, i. [sent-32, score-0.022]
</p><p>12 , from 10M features in F to less than 1K features in H by constructing F‘c toond leenssse tdh afena t1uKre representations (COFER)’, which is a new and very unique property that cannot be matched by previous semi-supervised learning methods including the iCWR approach. [sent-34, score-0.156]
</p><p>13 As a  result, NLP systems that are both compact and highperformance can be built by retraining the model with the obtained condensed feature set H. [sent-36, score-0.511]
</p><p>14 2  Condensed Feature Representations  Let us first define the condensed feature set H. [sent-37, score-0.457]
</p><p>15 Let sNu paenrdv iMsed represent ,t Fhe, ,n thuem obreirgsi onfalfe feaatutruerse ein s eFt. [sent-40, score-0.025]
</p><p>16 n Wdeens aesds ufmeaetu Mre hm ∈ aHnd i sg cnhearraalcly636  Proceedings ofP thoer t4l9atnhd A, Onrnuegaoln M,e Jeuntineg 19 o-f2 t4h,e 2 A0s1s1o. [sent-45, score-0.118]
</p><p>17 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 636–641,  feature set. [sent-47, score-0.11]
</p><p>18 terized as a set of features in F, that is, hm = Sm twerhiezreed Sm ⊆ eFt. [sent-50, score-0.164]
</p><p>19 o fW fee assume nth Fat, e thacath original feature fn ∈ F⊆ maps, Wate most, mtoe one c eoanchde onrsiegdin nfaelat feuraehm. [sent-51, score-0.681]
</p><p>20 Th∈isF assumption prevents tew coo ncdoenndseendse fdea fteuraetures from containing the same original feature, and some original features from not being mapped to any condensed feature. [sent-52, score-0.442]
</p><p>21 The ,v walhueer eo mf 6=eacmh conde∪nsed featu⊆reF i hs cldal. [sent-54, score-0.021]
</p><p>22 culated by summing the valu∪es of the original features assigned to it. [sent-55, score-0.084]
</p><p>23 Formally, let X and Y represent th ases sets do tfo oa il t possible inputs Xan adn outputs eofa target task, respectively. [sent-56, score-0.047]
</p><p>24 ⊆W Ye  =  ∪Mm=∩1  write the n-th feature function of the original features, whose value is determined by x and y, as fn(x, y), where n ∈ {1, . [sent-58, score-0.148]
</p><p>25 3  Learnin∑g COFERs  The remaining part of our method consists of the way to map the original features into the condensed features. [sent-74, score-0.404]
</p><p>26 For this purpose, we define the feature potency, which is evaluated by employing an existing 637 supervised model with unsupervised data sets. [sent-75, score-0.34]
</p><p>27 Figure 1shows a brief sketch of the process to construct the condensed features described in this section. [sent-76, score-0.366]
</p><p>28 1 Self-taught-style feature potency estimation We assume that we have a model trained by supervised learning, which we call the ‘base supervised model’, and the original feature set F that is used imn otdhee ’ b,a asen supervised amlo fdeaetlu. [sent-78, score-1.158]
</p><p>29 e W see tco Fns thidaetr a case  where the base supervised model is a (log-)linear model, and use the following equation to select the best output ˆy given x:  yˆ=ayrg∈Ym(xa)x∑nN=1wnfn(x,y),  (1)  where wn is a model parameter (or weight) of fn. [sent-79, score-0.242]
</p><p>30 To simplify the explanation, we define function r(x, y), where r(x, y) returns 1if y = ˆy is obtained from the base supervised model given x, and 0 otherwise. [sent-81, score-0.239]
</p><p>31 We also define V+ (fn) and VD− (fn) as shown in Figure 2 where D represents Dthe unsupervised idnat Fai sgeutr. [sent-83, score-0.064]
</p><p>32 e V+ (fn) measures the positive correlation with the besDt output ˆy given by the base supervised model since this is the summation of all the (weighted) feature values used in the estimation of the one best output ˆy over all x in the unsupervised data D. [sent-84, score-0.488]
</p><p>33 NDext, we define VD (fn) as the feature potency of fn: VD(fn) = − VD−(fn). [sent-87, score-0.486]
</p><p>34 V+(fn)  An i)nt −uit VivDe explanation of VD(fn) is as follows; if |VD(fn) | is large, the distribution of fn has either a large positive or negative rcoibrurteiloatnio onf wfith the best output ˆy given by the base supervised model. [sent-88, score-0.757]
</p><p>35 This implies that fn is an informative and potent feature in the model. [sent-89, score-0.655]
</p><p>36 Then, the distribution of fn has very small (or no) correlation to determine ˆy if |VD(fn) | sism zero or near zero. [sent-90, score-0.515]
</p><p>37 Itnio tnhi tso case, fn can y b ief e |vValuate)d| as an uninformative feature in the model. [sent-91, score-0.625]
</p><p>38 From this perspective, we treat VD(fn) as a measure of feature potency in terms of the base supervised model. [sent-92, score-0.671]
</p><p>39 The essence of this idea, evaluating features against each other on a certain model, is widely used in the context of semi-supervised learning, i. [sent-93, score-0.046]
</p><p>40 Our method is rough and a much simpler framework for implementing this fundamental idea of semi-supervised learning developed for NLP tasks. [sent-96, score-0.023]
</p><p>41 In fact, we apply the framework by incorporating a feature merging and elimination architecture to obtain effective condensed feature sets for supervised learning. [sent-98, score-0.728]
</p><p>42 2 Feature potency discounting To discount low potency values, we redefine feature potency as VD0(fn) instead of VD(fn) as follows:  VD0(fn)=l0o g [R n −+C ]− liof g −[A Cn ]≤ ifR RCn −<−ARAn ≤−  0 Dand VD∗(fn) D= bδVD0(fn)c oDtherwise, wDhere δ is a positive user-specified co)ncst oatnhte. [sent-100, score-1.157]
</p><p>43 ThDis ca)lc ∈u Nlati wonh can Nbe = seen as mapping 1ea,2ch,. [sent-108, score-0.018]
</p><p>44 ture into a discrete (integer) space with respect to VD0(fn). [sent-112, score-0.021]
</p><p>45 4 Condensed feature construction Suppose we have M different quantized feature potency values in VD∗(fn) for all n, which we rewrite as {um}mM=1. [sent-115, score-0.62]
</p><p>46 ThDen, we define Sm as a set of fn awsh o{use quantized feature potency value is um. [sent-116, score-1.052]
</p><p>47 As described in Section 2, we define the m-th condensed feature hm(x, y) as the summation of all the original features fn assigned to Sm. [sent-117, score-1.09]
</p><p>48 Obviously, the upper bound of the number of condensed features is the number of original features. [sent-120, score-0.404]
</p><p>49 from the condensed features, we discard feature fn for all n if un = 0. [sent-122, score-0.988]
</p><p>50 1, a feature has small (or no) effect in achieving the best output decision in the base supervised model if its potency is near 0. [sent-124, score-0.701]
</p><p>51 Additionally, we also utilize the ‘quantized’ feature potency values themselves as a new feature. [sent-127, score-0.459]
</p><p>52 The reason behind is that they are also very informative for supervised learning. [sent-128, score-0.196]
</p><p>53 For this purpose, we define φ(x, y) as φ(x, y) = We then use φ(x, y) as th∑e (M + 1)-th feature of our condensed feature s∑et. [sent-130, score-0.567]
</p><p>54 As a result, the condensed feature set obtained with our method is represented as H = {h1(x, y) , . [sent-131, score-0.43]
</p><p>55 We emphasize that once are de∑termined by supervised learning,  {wm}mM=+11  we can calculate wm0 in a preliminary step before the test phase. [sent-138, score-0.189]
</p><p>56 The number of features for our method is essentially M even if we add φ. [sent-140, score-0.046]
</p><p>57 5 Application to Structured Prediction Tasks We modify our method to better suit structured prediction problems in terms of calculation cost. [sent-142, score-0.078]
</p><p>58 For a structured prediction problem, it is usual to decompose or factorize output structure y into a set of local sub-structures z to reduce the calculation cost and to cope with the sparsity of the output space Y. [sent-143, score-0.138]
</p><p>59 i sW a part oef z output y, assuming that output y is constructed by a set of local substructures. [sent-146, score-0.06]
</p><p>60 Then formally, the n-th feature is written as fn(x, z), and fn(x, y) = ∑z∈y fn(x, z) holds. [sent-147, score-0.11]
</p><p>61 We define Z(x) as the set of all local substruWcteur deesf possibly generated ftor o fal al y oicna Y(x). [sent-149, score-0.045]
</p><p>62 This means that our feature potency estimation can be applied to the structured prediction problem at low cost. [sent-153, score-0.544]
</p><p>63 6 Efficient feature potency computation Our feature potency estimation described in Section 3. [sent-155, score-0.953]
</p><p>64 This is because Rn and An can be calculated by the summation of a data-wise calculation (map phase), and VD∗(fn) can be calculated independently by each featuDre (reduce phase). [sent-158, score-0.062]
</p><p>65 We emphasize that our feature potency estimation can be performed in a ‘single’ map-reduce process. [sent-159, score-0.517]
</p><p>66 4  Experiments  We conducted experiments on two different NLP tasks, namely NER and dependency parsing. [sent-160, score-0.057]
</p><p>67 , NER (Ando and Zhang, 2005; Suzuki and Isozaki, 2008) and dependency parsing (Koo et al. [sent-163, score-0.093]
</p><p>68 For the supervised datasets, we used CoNLL’03 (Tjong Kim Sang and De Meulder, 2003) shared task data for NER, and the Penn Treebank III 639 (PTB) corpus (Marcus et al. [sent-167, score-0.166]
</p><p>69 72 billion token text data as unsupervised data following the instructions given in (Suzuki et al. [sent-170, score-0.037]
</p><p>70 The iCWR approach yields the state-of-the-art re-  sults with both dependency parsing data derived from PTB-III (Koo et al. [sent-174, score-0.093]
</p><p>71 By comparing COFER with iCWR we can clarify its effectiveness in terms of providing better features for supervised learning. [sent-177, score-0.212]
</p><p>72 We use the term active features to refer to features whose corresponding model parameter is non-zero after supervised learning. [sent-178, score-0.316]
</p><p>73 It is wellknown that we can discard non-active features from the trained model without any loss after finishing supervised learning. [sent-179, score-0.234]
</p><p>74 Finally, we compared the performance in terms of the number of active features in the model given by supervised learning. [sent-180, score-0.27]
</p><p>75 We note here that the number of active features for COFER is the number of features hm if wm0 = 0, which is not wm = 0 for a fair comparison. [sent-181, score-0.352]
</p><p>76 Unlike COFER, iCWR does not have any architecture to winnow the original feature set used in supervised learning. [sent-182, score-0.336]
</p><p>77 For a fair comparison, we prepared L1-regularized supervised learning algorithms, which try to reduce the non-zero parameters in a model. [sent-183, score-0.236]
</p><p>78 Specifically, we utilized L1-regularized CRF (L1CRF) optimized by OWL-QN (Andrew and Gao, 2007) for NER, and the online struc-  tured output learning version of FOBOS (Duchi and Singer, 2009; Tsuruoka et al. [sent-184, score-0.073]
</p><p>79 , 2001) optimized by LBFGS (Liu and Nocedal, 1989) (L2CRF) for NER, and the online structured output learning version of the Passive-Aggressive algorithm (ostPA) (Crammer et al. [sent-187, score-0.084]
</p><p>80 , 2006) for dependency parsing to illustrate the baseline performance regardless of the active feature number. [sent-188, score-0.261]
</p><p>81 2 Settings for COFER We utilized baseline supervised learning models as the base supervised models of COFER. [sent-190, score-0.421]
</p><p>82 size of active features in the trained model on the development sets In addition, we also report the results when we treat iCWR as COFER’s base supervised models (iCWR+COFER). [sent-207, score-0.316]
</p><p>83 Suppose we have K different feature types, which are often defined by feature templates, i. [sent-210, score-0.22]
</p><p>84 In our experiments, we restrict the merging of features during the condensed feature construction process if and only if the features are the same feature type. [sent-213, score-0.632]
</p><p>85 As a result, COFER essentially consists of K different condensed feature sets. [sent-214, score-0.43]
</p><p>86 The numbers offeature types K were 79 and 30 for our NER and dependency parsing experiments, respectively. [sent-215, score-0.093]
</p><p>87 We note that this kind of feature partition by their types is widely used in the context of semi-supervised learning (Ando and Zhang, 2005; Suzuki and Isozaki, 2008). [sent-216, score-0.133]
</p><p>88 3  Results and Discussion  Figure 3 displays the performance on the development set with respect to the number of active features in the trained models given by each supervised learning algorithm. [sent-218, score-0.293]
</p><p>89 In both NER and dependency parsing experiments, COFER significantly outperformed iCWR. [sent-219, score-0.093]
</p><p>90 Moreover, COFER was surprisingly robust in relation to the number of active features in the model. [sent-220, score-0.104]
</p><p>91 These results reveal that COFER provides effective feature sets for certain NLP tasks. [sent-221, score-0.11]
</p><p>92 We summarize the noteworthy results in Figure 3, and also the performance of recent top-line systems for NER and dependency parsing in Table 1. [sent-222, score-0.123]
</p><p>93 AF: the size of active features in the trained model. [sent-247, score-0.104]
</p><p>94 ) supervised learning systems even though it uses far fewer active features. [sent-248, score-0.247]
</p><p>95 5  Conclusion  This paper introduced the idea of condensed feature representations (COFER) as a simple and general framework that can enhance the performance of existing supervised NLP systems. [sent-258, score-0.657]
</p><p>96 We also proposed a method that efficiently constructs condensed feature sets through discrete feature potency estimation over unsupervised data. [sent-259, score-0.982]
</p><p>97 We demonstrated that COFER based on our feature potency estimation can offer informative dense and low-dimensional feature spaces for supervised learning, which is theoretically preferable to the sparse and high-dimensional feature spaces often used in many NLP tasks. [sent-260, score-1.024]
</p><p>98 Existing NLP systems can be made more compact with higher performance by retraining their models with our condensed features. [sent-261, score-0.35]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fn', 0.515), ('potency', 0.349), ('cofer', 0.334), ('condensed', 0.32), ('vd', 0.27), ('icwr', 0.25), ('supervised', 0.166), ('ner', 0.147), ('hm', 0.118), ('feature', 0.11), ('suzuki', 0.09), ('sm', 0.086), ('isozaki', 0.081), ('mm', 0.078), ('xx', 0.074), ('wm', 0.065), ('xy', 0.065), ('ando', 0.06), ('active', 0.058), ('dependency', 0.057), ('koo', 0.056), ('nlp', 0.052), ('highperformance', 0.051), ('quantized', 0.051), ('hideki', 0.049), ('features', 0.046), ('base', 0.046), ('turian', 0.043), ('cwr', 0.042), ('erfw', 0.042), ('valu', 0.042), ('representations', 0.041), ('jun', 0.039), ('original', 0.038), ('unsupervised', 0.037), ('duchi', 0.037), ('parsing', 0.036), ('estimation', 0.035), ('spaces', 0.035), ('dy', 0.034), ('summation', 0.034), ('tsuruoka', 0.032), ('mapreduce', 0.032), ('druck', 0.032), ('structured', 0.031), ('retraining', 0.03), ('noteworthy', 0.03), ('informative', 0.03), ('output', 0.03), ('rn', 0.029), ('dean', 0.029), ('ntt', 0.029), ('sang', 0.029), ('calculation', 0.028), ('adn', 0.028), ('prepared', 0.028), ('tjong', 0.028), ('define', 0.027), ('icml', 0.026), ('ein', 0.025), ('uas', 0.025), ('dense', 0.025), ('ratinov', 0.025), ('carreras', 0.023), ('suppose', 0.023), ('xavier', 0.023), ('yoram', 0.023), ('emphasize', 0.023), ('learning', 0.023), ('architecture', 0.022), ('terry', 0.022), ('discard', 0.022), ('andrew', 0.021), ('entity', 0.021), ('eo', 0.021), ('un', 0.021), ('discrete', 0.021), ('enhance', 0.02), ('utilized', 0.02), ('ichi', 0.02), ('sw', 0.02), ('lafferty', 0.02), ('offer', 0.019), ('co', 0.019), ('let', 0.019), ('fair', 0.019), ('crammer', 0.019), ('crf', 0.019), ('prediction', 0.019), ('afnlp', 0.019), ('xan', 0.018), ('xfi', 0.018), ('aive', 0.018), ('nfd', 0.018), ('fal', 0.018), ('asen', 0.018), ('zw', 0.018), ('fat', 0.018), ('fee', 0.018), ('wonh', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="199-tfidf-1" href="./acl-2011-Learning_Condensed_Feature_Representations_from_Large_Unsupervised_Data_Sets_for_Supervised_Learning.html">199 acl-2011-Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning</a></p>
<p>Author: Jun Suzuki ; Hideki Isozaki ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative ‘condensed feature representations’ from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III. ,</p><p>2 0.14660951 <a title="199-tfidf-2" href="./acl-2011-Semi-supervised_condensed_nearest_neighbor_for_part-of-speech_tagging.html">278 acl-2011-Semi-supervised condensed nearest neighbor for part-of-speech tagging</a></p>
<p>Author: Anders Sgaard</p><p>Abstract: This paper introduces a new training set condensation technique designed for mixtures of labeled and unlabeled data. It finds a condensed set of labeled and unlabeled data points, typically smaller than what is obtained using condensed nearest neighbor on the labeled data only, and improves classification accuracy. We evaluate the algorithm on semisupervised part-of-speech tagging and present the best published result on the Wall Street Journal data set.</p><p>3 0.10334685 <a title="199-tfidf-3" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>Author: Stefan Rud ; Massimiliano Ciaramita ; Jens Muller ; Hinrich Schutze</p><p>Abstract: We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries.</p><p>4 0.094467521 <a title="199-tfidf-4" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>Author: Nan Duan ; Mu Li ; Ming Zhou</p><p>Abstract: This paper presents hypothesis mixture decoding (HM decoding), a new decoding scheme that performs translation reconstruction using hypotheses generated by multiple translation systems. HM decoding involves two decoding stages: first, each component system decodes independently, with the explored search space kept for use in the next step; second, a new search space is constructed by composing existing hypotheses produced by all component systems using a set of rules provided by the HM decoder itself, and a new set of model independent features are used to seek the final best translation from this new search space. Few assumptions are made by our approach about the underlying component systems, enabling us to leverage SMT models based on arbitrary paradigms. We compare our approach with several related techniques, and demonstrate significant BLEU improvements in large-scale Chinese-to-English translation tasks.</p><p>5 0.077038437 <a title="199-tfidf-5" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>Author: Yuval Marton ; Nizar Habash ; Owen Rambow</p><p>Abstract: We explore the contribution of morphological features both lexical and inflectional to dependency parsing of Arabic, a morphologically rich language. Using controlled experiments, we find that definiteness, person, number, gender, and the undiacritzed lemma are most helpful for parsing on automatically tagged input. We further contrast the contribution of form-based and functional features, and show that functional gender and number (e.g., “broken plurals”) and the related rationality feature improve over form-based features. It is the first time functional morphological features are used for Arabic NLP. – –</p><p>6 0.073864333 <a title="199-tfidf-6" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>7 0.068265356 <a title="199-tfidf-7" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>8 0.066885844 <a title="199-tfidf-8" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>9 0.066096336 <a title="199-tfidf-9" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>10 0.060562182 <a title="199-tfidf-10" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>11 0.053263541 <a title="199-tfidf-11" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>12 0.050546408 <a title="199-tfidf-12" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>13 0.050408602 <a title="199-tfidf-13" href="./acl-2011-Exploiting_Morphology_in_Turkish_Named_Entity_Recognition_System.html">124 acl-2011-Exploiting Morphology in Turkish Named Entity Recognition System</a></p>
<p>14 0.047527574 <a title="199-tfidf-14" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>15 0.04487687 <a title="199-tfidf-15" href="./acl-2011-A_Scalable_Probabilistic_Classifier_for_Language_Modeling.html">24 acl-2011-A Scalable Probabilistic Classifier for Language Modeling</a></p>
<p>16 0.044740267 <a title="199-tfidf-16" href="./acl-2011-P11-2093_k2opt.pdf.html">238 acl-2011-P11-2093 k2opt.pdf</a></p>
<p>17 0.044625297 <a title="199-tfidf-17" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>18 0.042770032 <a title="199-tfidf-18" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>19 0.042107034 <a title="199-tfidf-19" href="./acl-2011-K-means_Clustering_with_Feature_Hashing.html">189 acl-2011-K-means Clustering with Feature Hashing</a></p>
<p>20 0.04198445 <a title="199-tfidf-20" href="./acl-2011-Bootstrapping_coreference_resolution_using_word_associations.html">63 acl-2011-Bootstrapping coreference resolution using word associations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.119), (1, 0.001), (2, -0.022), (3, -0.059), (4, -0.007), (5, -0.028), (6, 0.049), (7, 0.002), (8, -0.01), (9, 0.052), (10, 0.034), (11, 0.029), (12, -0.004), (13, -0.025), (14, 0.012), (15, 0.014), (16, -0.026), (17, 0.006), (18, 0.007), (19, -0.039), (20, -0.056), (21, -0.105), (22, 0.022), (23, 0.033), (24, 0.005), (25, -0.064), (26, 0.017), (27, -0.053), (28, 0.004), (29, 0.049), (30, 0.005), (31, 0.044), (32, -0.001), (33, 0.039), (34, 0.013), (35, 0.065), (36, 0.021), (37, -0.066), (38, 0.035), (39, 0.081), (40, 0.14), (41, 0.048), (42, -0.062), (43, -0.069), (44, 0.077), (45, -0.071), (46, 0.062), (47, 0.061), (48, -0.09), (49, 0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92109597 <a title="199-lsi-1" href="./acl-2011-Learning_Condensed_Feature_Representations_from_Large_Unsupervised_Data_Sets_for_Supervised_Learning.html">199 acl-2011-Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning</a></p>
<p>Author: Jun Suzuki ; Hideki Isozaki ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative ‘condensed feature representations’ from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III. ,</p><p>2 0.77379632 <a title="199-lsi-2" href="./acl-2011-Semi-supervised_condensed_nearest_neighbor_for_part-of-speech_tagging.html">278 acl-2011-Semi-supervised condensed nearest neighbor for part-of-speech tagging</a></p>
<p>Author: Anders Sgaard</p><p>Abstract: This paper introduces a new training set condensation technique designed for mixtures of labeled and unlabeled data. It finds a condensed set of labeled and unlabeled data points, typically smaller than what is obtained using condensed nearest neighbor on the labeled data only, and improves classification accuracy. We evaluate the algorithm on semisupervised part-of-speech tagging and present the best published result on the Wall Street Journal data set.</p><p>3 0.62136137 <a title="199-lsi-3" href="./acl-2011-Improving_Classification_of_Medical_Assertions_in_Clinical_Notes.html">165 acl-2011-Improving Classification of Medical Assertions in Clinical Notes</a></p>
<p>Author: Youngjun Kim ; Ellen Riloff ; Stephane Meystre</p><p>Abstract: We present an NLP system that classifies the assertion type of medical problems in clinical notes used for the Fourth i2b2/VA Challenge. Our classifier uses a variety of linguistic features, including lexical, syntactic, lexicosyntactic, and contextual features. To overcome an extremely unbalanced distribution of assertion types in the data set, we focused our efforts on adding features specifically to improve the performance of minority classes. As a result, our system reached 94. 17% micro-averaged and 79.76% macro-averaged F1-measures, and showed substantial recall gains on the minority classes. 1</p><p>4 0.58634371 <a title="199-lsi-4" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>Author: Stefan Rud ; Massimiliano Ciaramita ; Jens Muller ; Hinrich Schutze</p><p>Abstract: We use search engine results to address a particularly difficult cross-domain language processing task, the adaptation of named entity recognition (NER) from news text to web queries. The key novelty of the method is that we submit a token with context to a search engine and use similar contexts in the search results as additional information for correctly classifying the token. We achieve strong gains in NER performance on news, in-domain and out-of-domain, and on web queries.</p><p>5 0.57200408 <a title="199-lsi-5" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao ; Kang Liu ; Li Cai</p><p>Abstract: In this paper, we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing. Conventional selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous work to wordto-word selectional preferences by using webscale data. Experiments show that web-scale data improves statistical dependency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance.</p><p>6 0.56205481 <a title="199-lsi-6" href="./acl-2011-Confidence-Weighted_Learning_of_Factored_Discriminative_Language_Models.html">78 acl-2011-Confidence-Weighted Learning of Factored Discriminative Language Models</a></p>
<p>7 0.55951548 <a title="199-lsi-7" href="./acl-2011-A_Scalable_Probabilistic_Classifier_for_Language_Modeling.html">24 acl-2011-A Scalable Probabilistic Classifier for Language Modeling</a></p>
<p>8 0.523148 <a title="199-lsi-8" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>9 0.51299971 <a title="199-lsi-9" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>10 0.51238716 <a title="199-lsi-10" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>11 0.50375748 <a title="199-lsi-11" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>12 0.50259691 <a title="199-lsi-12" href="./acl-2011-Discovering_Sociolinguistic_Associations_with_Structured_Sparsity.html">97 acl-2011-Discovering Sociolinguistic Associations with Structured Sparsity</a></p>
<p>13 0.50002474 <a title="199-lsi-13" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>14 0.49807927 <a title="199-lsi-14" href="./acl-2011-Hypothesis_Mixture_Decoding_for_Statistical_Machine_Translation.html">155 acl-2011-Hypothesis Mixture Decoding for Statistical Machine Translation</a></p>
<p>15 0.48268795 <a title="199-lsi-15" href="./acl-2011-Automatically_Predicting_Peer-Review_Helpfulness.html">55 acl-2011-Automatically Predicting Peer-Review Helpfulness</a></p>
<p>16 0.47325903 <a title="199-lsi-16" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>17 0.46252179 <a title="199-lsi-17" href="./acl-2011-A_New_Dataset_and_Method_for_Automatically_Grading_ESOL_Texts.html">20 acl-2011-A New Dataset and Method for Automatically Grading ESOL Texts</a></p>
<p>18 0.45474193 <a title="199-lsi-18" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>19 0.45392123 <a title="199-lsi-19" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>20 0.45126063 <a title="199-lsi-20" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.012), (17, 0.031), (19, 0.275), (26, 0.01), (31, 0.013), (37, 0.142), (39, 0.05), (41, 0.073), (55, 0.039), (59, 0.023), (72, 0.058), (91, 0.044), (96, 0.12), (97, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91154104 <a title="199-lda-1" href="./acl-2011-Exploiting_Readymades_in_Linguistic_Creativity%3A_A_System_Demonstration_of_the_Jigsaw_Bard.html">125 acl-2011-Exploiting Readymades in Linguistic Creativity: A System Demonstration of the Jigsaw Bard</a></p>
<p>Author: Tony Veale ; Yanfen Hao</p><p>Abstract: Large lexical resources, such as corpora and databases of Web ngrams, are a rich source of pre-fabricated phrases that can be reused in many different contexts. However, one must be careful in how these resources are used, and noted writers such as George Orwell have argued that the use of canned phrases encourages sloppy thinking and results in poor communication. Nonetheless, while Orwell prized home-made phrases over the readymade variety, there is a vibrant movement in modern art which shifts artistic creation from the production of novel artifacts to the clever reuse of readymades or objets trouvés. We describe here a system that makes creative reuse of the linguistic readymades in the Google ngrams. Our system, the Jigsaw Bard, thus owes more to Marcel Duchamp than to George Orwell. We demonstrate how textual readymades can be identified and harvested on a large scale, and used to drive a modest form of linguistic creativity. 1</p><p>same-paper 2 0.79224223 <a title="199-lda-2" href="./acl-2011-Learning_Condensed_Feature_Representations_from_Large_Unsupervised_Data_Sets_for_Supervised_Learning.html">199 acl-2011-Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning</a></p>
<p>Author: Jun Suzuki ; Hideki Isozaki ; Masaaki Nagata</p><p>Abstract: This paper proposes a novel approach for effectively utilizing unsupervised data in addition to supervised data for supervised learning. We use unsupervised data to generate informative ‘condensed feature representations’ from the original feature set used in supervised NLP systems. The main contribution of our method is that it can offer dense and low-dimensional feature spaces for NLP tasks while maintaining the state-ofthe-art performance provided by the recently developed high-performance semi-supervised learning technique. Our method matches the results of current state-of-the-art systems with very few features, i.e., F-score 90.72 with 344 features for CoNLL-2003 NER data, and UAS 93.55 with 12.5K features for dependency parsing data derived from PTB-III. ,</p><p>3 0.72452652 <a title="199-lda-3" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>Author: Yuval Marton ; Nizar Habash ; Owen Rambow</p><p>Abstract: We explore the contribution of morphological features both lexical and inflectional to dependency parsing of Arabic, a morphologically rich language. Using controlled experiments, we find that definiteness, person, number, gender, and the undiacritzed lemma are most helpful for parsing on automatically tagged input. We further contrast the contribution of form-based and functional features, and show that functional gender and number (e.g., “broken plurals”) and the related rationality feature improve over form-based features. It is the first time functional morphological features are used for Arabic NLP. – –</p><p>4 0.66819608 <a title="199-lda-4" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>Author: Dmitriy Dligach ; Martha Palmer</p><p>Abstract: Active Learning (AL) is typically initialized with a small seed of examples selected randomly. However, when the distribution of classes in the data is skewed, some classes may be missed, resulting in a slow learning progress. Our contribution is twofold: (1) we show that an unsupervised language modeling based technique is effective in selecting rare class examples, and (2) we use this technique for seeding AL and demonstrate that it leads to a higher learning rate. The evaluation is conducted in the context of word sense disambiguation.</p><p>5 0.6275295 <a title="199-lda-5" href="./acl-2011-Creative_Language_Retrieval%3A_A_Robust_Hybrid_of_Information_Retrieval_and_Linguistic_Creativity.html">89 acl-2011-Creative Language Retrieval: A Robust Hybrid of Information Retrieval and Linguistic Creativity</a></p>
<p>Author: Tony Veale</p><p>Abstract: Information retrieval (IR) and figurative language processing (FLP) could scarcely be more different in their treatment of language and meaning. IR views language as an open-ended set of mostly stable signs with which texts can be indexed and retrieved, focusing more on a text’s potential relevance than its potential meaning. In contrast, FLP views language as a system of unstable signs that can be used to talk about the world in creative new ways. There is another key difference: IR is practical, scalable and robust, and in daily use by millions of casual users. FLP is neither scalable nor robust, and not yet practical enough to migrate beyond the lab. This paper thus presents a mutually beneficial hybrid of IR and FLP, one that enriches IR with new operators to enable the non-literal retrieval of creative expressions, and which also transplants FLP into a robust, scalable framework in which practical applications of linguistic creativity can be implemented. 1</p><p>6 0.60630631 <a title="199-lda-6" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>7 0.60364568 <a title="199-lda-7" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>8 0.60112345 <a title="199-lda-8" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>9 0.59937662 <a title="199-lda-9" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>10 0.59846944 <a title="199-lda-10" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>11 0.598423 <a title="199-lda-11" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>12 0.59757817 <a title="199-lda-12" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>13 0.59665024 <a title="199-lda-13" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>14 0.59475392 <a title="199-lda-14" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>15 0.5936383 <a title="199-lda-15" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>16 0.59290648 <a title="199-lda-16" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>17 0.5928756 <a title="199-lda-17" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>18 0.59286582 <a title="199-lda-18" href="./acl-2011-Automatically_Extracting_Polarity-Bearing_Topics_for_Cross-Domain_Sentiment_Classification.html">54 acl-2011-Automatically Extracting Polarity-Bearing Topics for Cross-Domain Sentiment Classification</a></p>
<p>19 0.59281707 <a title="199-lda-19" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>20 0.59249294 <a title="199-lda-20" href="./acl-2011-Subjectivity_and_Sentiment_Analysis_of_Modern_Standard_Arabic.html">289 acl-2011-Subjectivity and Sentiment Analysis of Modern Standard Arabic</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
