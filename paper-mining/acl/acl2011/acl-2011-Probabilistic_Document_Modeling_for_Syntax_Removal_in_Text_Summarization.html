<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-251" href="#">acl2011-251</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</h1>
<br/><p>Source: <a title="acl-2011-251-pdf" href="http://aclweb.org/anthology//P/P11/P11-2113.pdf">pdf</a></p><p>Author: William M. Darling ; Fei Song</p><p>Abstract: Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.</p><p>Reference: <a title="acl-2011-251-reference" href="../acl2011_reference/acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 wdarl ing@uogue lph ca Abstract Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. [sent-3, score-0.495]
</p><p>2 To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. [sent-4, score-0.073]
</p><p>3 The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. [sent-5, score-0.509]
</p><p>4 We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the  syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. [sent-6, score-0.935]
</p><p>5 Therefore, automatic text summarization, which aims at providing a shorter representation of the salient parts of a large amount of information, has been steadily growing in both importance and popularity over the last several years. [sent-11, score-0.071]
</p><p>6 The summarization tracks at the Document Understanding Conference (DUC), and its successor the Text Analysis Conference (TAC)1 ,have helped fuel this interest by hosting yearly competitions to promote the advancement of automatic text summarization methods. [sent-12, score-0.519]
</p><p>7 The tasks at the DUC and TAC involve taking a set of documents as input and outputting a short summary (either 100 or 250 words, depending on the year) containing what the system deems to be the most important information contained in the original documents. [sent-13, score-0.067]
</p><p>8 While a system matching human performance will likely require deep language understanding, most existing systems use an extractive, rather than abstractive, approach whereby the most salient sentences are extracted from the original documents and strung together to form an output summary. [sent-14, score-0.042]
</p><p>9 2  In this paper, we present a summarization model based on (Griffiths et al. [sent-15, score-0.248]
</p><p>10 We show that a simple model that separates syntax and content words and uses the content distribution as a representative model of the important words in a document set can achieve high performance in multi-document summarization, competitive with state-of-the-art summarization systems. [sent-17, score-1.331]
</p><p>11 gov/t ac 2NLP techniques such as sentence compression are often used, but this is far from abstractive summarization. [sent-20, score-0.048]
</p><p>12 (2006) describe SumBasic, a simple, yet high-performing summarization system based on term frequency. [sent-25, score-0.249]
</p><p>13 While the methodology underlying SumBasic departs very little from the pioneering summarization work performed at IBM in the 1950’s (Luhn, 1958), methods based on simple word  statistics continue to outperform more complicated approaches to automatic summarization. [sent-26, score-0.244]
</p><p>14 The SumBasic algorithm uses the empirical unigram probability distribution of the non-stop-words in the input such that for each word w, p(w) = nNw where nw is the number of occurrences of word w and N is the total number of words in the input. [sent-29, score-0.252]
</p><p>15 Sentences are then scored based on a composition function CF(·) that composes the score for the sentence tbioasned C on ·i)ts t hcaotn ctaoimnepdo swesor thdes. [sent-30, score-0.072]
</p><p>16 cTohree most commonly used composition function adds the probabilities of the words in a sentence together, and then divides by the number of words in that sentence. [sent-31, score-0.129]
</p><p>17 However, to reduce redundancy, once a sentence has been chosen for summary inclusion, the probability distribution is recalculated such that any word that appears in the chosen sentence has its probability diminished. [sent-32, score-0.266]
</p><p>18 Sentences are continually marked for inclusion until the summary word-limit is reached. [sent-33, score-0.068]
</p><p>19 Despite its simplicity, SumBasic continues to be one of the top summarization performers in both manual and auto-  matic evaluations (Nenkova et al. [sent-34, score-0.303]
</p><p>20 (2005) describe a composite generative model that combines syntax and semantics. [sent-38, score-0.306]
</p><p>21 The semantic portion of the model is similar to Latent Dirichlet Allocation and models long-range thematic word dependencies with a set of topics, while short-range (sentence-wide) word dependencies are modeled with syntax classes using a Hidden Markov Model. [sent-39, score-0.423]
</p><p>22 The model has an HMM at its base where 3A system based on SumBasic was one of the top performers at the Text Analysis Conference 2010 summarization track. [sent-40, score-0.307]
</p><p>23 643 one of its syntax classes is replaced with an LDAlike topic model. [sent-41, score-0.445]
</p><p>24 When the model is in the semantic class state, it chooses a topic from the given document’s topic distribution, samples a word from that topic’s word distribution, and generates it. [sent-42, score-0.324]
</p><p>25 Otherwise, the model samples a word from the current syntax class in the HMM and outputs that word. [sent-43, score-0.346]
</p><p>26 (2006) show that using term frequency is a powerful approach to modeling human summarization. [sent-45, score-0.071]
</p><p>27 Nevertheless, for SumBasic to perform well, stop-words must be removed from the composition scoring function. [sent-46, score-0.092]
</p><p>28 Because these words add nothing to the content of a summary, if they were not removed for the scoring calculation, the sentence scores would no longer provide a good fit with sentences that a human summarizer would find salient. [sent-47, score-0.336]
</p><p>29 However, by simply removing pre-selected words from a list, we will inevitably miss words that in different contexts would be considered noncontent words. [sent-48, score-0.084]
</p><p>30 In contrast, if too many words are removed, the opposite problem appears and we may remove important information that would be useful in determining sentence scores. [sent-49, score-0.072]
</p><p>31 These problems are referred to as undercoverage and overcoverage, respectively. [sent-50, score-0.067]
</p><p>32 To alleviate this problem, we would like to put less probability mass for our document set probability distribution on non-content words and more on words with strong semantic meaning. [sent-51, score-0.532]
</p><p>33 One approach that could achieve this would be to build sep-  arate stopword lists for specific domains, and there are approaches to automatically build such lists (Lo et al. [sent-52, score-0.139]
</p><p>34 Another approach would be to use a part-of-speech (POS) tagger on each sentence and ignore all non-noun words because high-content words are almost exclusively nouns. [sent-55, score-0.2]
</p><p>35 One could also include verbs, adverbs, adjectives, or any combination thereof, and therefore solve some ofthe contextbased problems associated with using a stopword list. [sent-56, score-0.168]
</p><p>36 Nevertheless, this approach introduces deeper context-related problems of its own (a noun, for example, is not always a content word). [sent-57, score-0.201]
</p><p>37 A separate ap-  Figure 1: Graphical model depiction of our content and syntax summarization method. [sent-58, score-0.772]
</p><p>38 There are D document sets, M documents in each set, NM words in document M, and C syntax classes. [sent-59, score-0.673]
</p><p>39 proach would be to model the syntax and semantic words used in a document collection in an HMM, as in Griffiths et al. [sent-60, score-0.557]
</p><p>40 (2005), and use the semantic class as the content-word distribution for summarization. [sent-61, score-0.184]
</p><p>41 Our approach to summarization builds on SumBasic, and combines it with a similar approach to separating content and syntax distributions as that described in (Griffiths et al. [sent-62, score-0.872]
</p><p>42 Like  (Haghighi and Vanderwende, 2009), (Daum ´e and Marcu, 2006), and (Barzilay and Lee, 2004), we model words as being generated from latent distributions. [sent-64, score-0.073]
</p><p>43 However, instead of background, content, and document-specific distributions, we model all words in a document set as being there for one of only two purposes: a semantic (content) purpose, or a syntactic (functional) purpose. [sent-65, score-0.282]
</p><p>44 We model the syntax class distributions using an HMM and model the content words using a simple language model. [sent-66, score-0.749]
</p><p>45 The principal difference between our generative model and the one described in (Griffiths et al. [sent-67, score-0.075]
</p><p>46 , 2005) is that we simplify the model by assuming that each document is generated solely from one topic distribution that is shared throughout each document set. [sent-68, score-0.611]
</p><p>47 This results in a smoothed language model for each document set’s content distribution where the counts from content words (as determined through inference) are used to determine their probability, and the syntax words are essentially discarded. [sent-69, score-1.083]
</p><p>48 Therefore, our model describes the process of generating a document as traversing an HMM and 644  . [sent-70, score-0.209]
</p><p>49 The left and right states show the top words for those syntax classes while the middle state shows the top words for the given document set’s content distribution. [sent-73, score-0.835]
</p><p>50 emitting either a content word from a single topic’s (document set’s) content word distribution, or a syntax word from one of C corpus-wide syntax classes where C is a parameter input to the algorithm. [sent-74, score-1.011]
</p><p>51 More specifically, a document is generated as follows: 1. [sent-75, score-0.178]
</p><p>52 Choose a topic z corresponding to the given document set (z = {z1, . [sent-76, score-0.289]
</p><p>53 , zk} where k is the nduocmubmeer notf sdeotc (uzm =ent { szets to summarize. [sent-79, score-0.027]
</p><p>54 For each word wi in document d (a) Draw ci from π(ci−1)  (b) If ci = 1, then draw wi from wise draw wi from φ(ci)  ζ(z),  other-  Each class ci and topic z correspond to multinomial distributions over words, and transitions between classes follow the transition distribution . [sent-81, score-1.233]
</p><p>55 When ci = 1, a content word is emitted from the topic word distribution for the given document set z. [sent-82, score-0.751]
</p><p>56 Otherwise, a syntax word is emitted from the corpus-wide syntax word distribution φ(ci) . [sent-83, score-0.709]
</p><p>57 The word distributions and transition vectors are all drawn from Dirichlet priors. [sent-84, score-0.129]
</p><p>58 A graphical model depiction of this distribution is shown in Figure 1. [sent-85, score-0.192]
</p><p>59 A portion of an example HMM (from the DUC 2006 dataset) is shown in Figure 2 with the most probable words in the content class in the middle and two syntax classes on either side of it. [sent-86, score-0.682]
</p><p>60 1 Inference Because the posterior probability of the content (document set) word distributions and syntax class word distributions cannot be solved analytically, as with many topic modeling approaches, we appeal to an approximation. [sent-88, score-0.967]
</p><p>61 , 1999)), or more specifically, “collapsed” Gibbs sampling where the multinomial parameters are integrated out. [sent-93, score-0.044]
</p><p>62 4 We ran our sampler for between 500 and 5,000 iterations (though the distributions would typically converge by 1,000 iterations), and chose between 5 and 10 (with negligible changes in results) for the cardinality of the classes set C. [sent-94, score-0.188]
</p><p>63 We leave optimizing the number of syntax classes, or determining them directly from the data, for future work. [sent-95, score-0.305]
</p><p>64 2 Summarization Here we describe how we use the estimated topic and syntax distributions to perform extractive multidocument summarization. [sent-97, score-0.588]
</p><p>65 We follow the SumBasic algorithm, but replace the empirical unigram distribution of the document set with the learned topic distributions for the given documents. [sent-98, score-0.585]
</p><p>66 This models the effect of not only ignoring stop-words, but also reduces the amount of probability mass in the distribution placed on functional words that serve no semantic purpose and that would likely be less useful in a summary. [sent-99, score-0.323]
</p><p>67 Because this is a fully probabilistic model, we do not entirely “ignore” stop-words; in-  stead, the model forces the probability mass of these words to the syntax classes. [sent-100, score-0.431]
</p><p>68 For a given document set to be summarized, each sentence is assigned a score corresponding to the average probability of the words contained within it: Score(S) = |S1| Pw∈S p(w). [sent-101, score-0.263]
</p><p>69 In our mPodel, SyntaxSum, p(wi) = p(wi |ζ(z) ), where ζ(z) is a multinomial distribution over ζthe corpus’ fixed vocabulary that puts high probabilities on content words that are used often in the given document set and low probabilities on words that are more important in other syntax classes. [sent-103, score-0.895]
</p><p>70 The middle node in Figure 2 is a true representation of the top words in the ζ(z) distribution for document set 43 in the DUC 2006 dataset. [sent-104, score-0.371]
</p><p>71 marization metric for unigram (R-1), bigram (R-2), and skip-4 bigram (R-SU4) recall both with and without (-s) stopwords removed (Lin, 2004). [sent-118, score-0.213]
</p><p>72 We tested our models on the popular DUC 2006 dataset which aids in model comparison and also on the more recent TAC 2010 dataset. [sent-119, score-0.031]
</p><p>73 Our approach is compared to using an a priori stopword list, and using a POStagger to build distributions of words coming from only a subset of the parts-of-speech. [sent-122, score-0.364]
</p><p>74 1 SumBasic To cogently demonstrate the effect of ignoring nonsemantic words in term frequency-based summarization, we implemented two initial versions of SumBasic. [sent-124, score-0.101]
</p><p>75 The first, SB-, does not ignore stopwords while the second, SumBasic, ignores all stopwords from a list included in the Python NLTK library. [sent-125, score-0.275]
</p><p>76 6 For SumBasic without stop-word removal (SB-), we obtain 3. [sent-126, score-0.036]
</p><p>77 7 With stop-words removed from the sentence scoring calculation (SumBasic), our results increase to 5. [sent-129, score-0.077]
</p><p>78 2 POS Tagger Because the content distributions learned from our model seem to favor almost exclusively nouns (see Figure 2), another approach to building a semantically strong word distribution for determining salient sentences in summarization might be to ignore all words except nouns. [sent-139, score-0.982]
</p><p>79 This would avoid most stopwords (many ofwhich are modeled as their own part-of-speech) and would serve as a simpler approach to finding important content. [sent-140, score-0.163]
</p><p>80 Nevertheless, adjectives and verbs also often carry important semantic information. [sent-141, score-0.127]
</p><p>81 Therefore, we ran a POS tagger over the input sentences and tried selecting sentences based on word distributions that included only nouns; nouns and verbs; nouns and adjectives; and nouns, verbs, and adjectives. [sent-142, score-0.291]
</p><p>82 In each case, this approach performs either worse than or no better than SumBasic using a priori stopword removal. [sent-143, score-0.193]
</p><p>83 The nouns and adjectives distribution did the best, whereas the nouns and verbs were the worst. [sent-144, score-0.337]
</p><p>84 Due to space constraints, we omit full TAC 2010 results but R-2 and R-SU4 results without stopwords improved from SumBasic’s 7. [sent-152, score-0.112]
</p><p>85 ζ(z)  5  Conclusions and Future Work  This paper has described using a domainindependent document modeling approach of avoiding low-content syntax words in an NLP task where high-content semantic words should be the principal focus. [sent-157, score-0.651]
</p><p>86 Specifically, we have shown that we can increase summarization performance by modeling the document set probability distribution 646  using a hybrid LDA-HMM content and syntax model. [sent-158, score-1.066]
</p><p>87 This is a very flexible approach to finding content words and works well for increasing performance of simple statistics-based text summarization. [sent-160, score-0.243]
</p><p>88 It could also, however, prove to be useful in any other NLP task where stopwords should be removed. [sent-161, score-0.112]
</p><p>89 Some future work includes applying this model to areas such as topic tracking and text segmentation, and coherently adjusting it to fit an n-gram modeling approach. [sent-162, score-0.181]
</p><p>90 Catching the drift: Probabilistic content models, with applications to generation and summarization. [sent-168, score-0.201]
</p><p>91 Automatically building a stopword list for an information retrieval system. [sent-205, score-0.139]
</p><p>92 A compositional context sensitive multidocument summarizer: exploring the factors that influence summarization. [sent-218, score-0.036]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sumbasic', 0.601), ('syntax', 0.275), ('summarization', 0.217), ('content', 0.201), ('duc', 0.197), ('document', 0.178), ('tac', 0.153), ('rouge', 0.141), ('stopword', 0.139), ('guelph', 0.134), ('distributions', 0.129), ('distribution', 0.113), ('stopwords', 0.112), ('topic', 0.111), ('griffiths', 0.11), ('ci', 0.102), ('hmm', 0.09), ('nenkova', 0.088), ('darling', 0.067), ('gilks', 0.067), ('overcoverage', 0.067), ('undercoverage', 0.067), ('nouns', 0.064), ('performers', 0.059), ('classes', 0.059), ('wi', 0.059), ('unigram', 0.054), ('priori', 0.054), ('ofwhich', 0.051), ('adjectives', 0.051), ('ignore', 0.051), ('separating', 0.05), ('abstractive', 0.048), ('nltk', 0.048), ('depiction', 0.048), ('removed', 0.047), ('emitted', 0.046), ('lucy', 0.046), ('summarizer', 0.046), ('verbs', 0.045), ('composition', 0.045), ('principal', 0.044), ('stone', 0.044), ('multinomial', 0.044), ('probability', 0.043), ('salient', 0.042), ('vanderwende', 0.042), ('words', 0.042), ('topics', 0.04), ('class', 0.04), ('mass', 0.04), ('modeling', 0.039), ('nevertheless', 0.039), ('draw', 0.038), ('middle', 0.038), ('summary', 0.038), ('monte', 0.037), ('extractive', 0.037), ('carlo', 0.036), ('multidocument', 0.036), ('removal', 0.036), ('tagger', 0.034), ('markov', 0.033), ('term', 0.032), ('lo', 0.032), ('rd', 0.032), ('canada', 0.031), ('semantic', 0.031), ('daum', 0.031), ('exclusively', 0.031), ('semantically', 0.031), ('model', 0.031), ('calculation', 0.03), ('inclusion', 0.03), ('determining', 0.03), ('analytically', 0.029), ('alliance', 0.029), ('nnw', 0.029), ('yearly', 0.029), ('lph', 0.029), ('iadh', 0.029), ('outputting', 0.029), ('contextbased', 0.029), ('hosting', 0.029), ('recalculated', 0.029), ('szpakowicz', 0.029), ('growing', 0.029), ('haghighi', 0.029), ('portion', 0.027), ('functional', 0.027), ('ignoring', 0.027), ('barzilay', 0.027), ('advancement', 0.027), ('matic', 0.027), ('departs', 0.027), ('nologies', 0.027), ('postgraduate', 0.027), ('composes', 0.027), ('stan', 0.027), ('notf', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="251-tfidf-1" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>Author: William M. Darling ; Fei Song</p><p>Abstract: Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.</p><p>2 0.1749087 <a title="251-tfidf-2" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>Author: Risa Kitajima ; Ichiro Kobayashi</p><p>Abstract: Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic.</p><p>3 0.17386283 <a title="251-tfidf-3" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.</p><p>4 0.16663174 <a title="251-tfidf-4" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>Author: Dong Wang ; Yang Liu</p><p>Abstract: This paper presents a pilot study of opinion summarization on conversations. We create a corpus containing extractive and abstractive summaries of speaker’s opinion towards a given topic using 88 telephone conversations. We adopt two methods to perform extractive summarization. The first one is a sentence-ranking method that linearly combines scores measured from different aspects including topic relevance, subjectivity, and sentence importance. The second one is a graph-based method, which incorporates topic and sentiment information, as well as additional information about sentence-to-sentence relations extracted based on dialogue structure. Our evaluation results show that both methods significantly outperform the baseline approach that extracts the longest utterances. In particular, we find that incorporating dialogue structure in the graph-based method contributes to the improved system performance.</p><p>5 0.1387914 <a title="251-tfidf-5" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>Author: Ivan Vulic ; Wim De Smet ; Marie-Francine Moens</p><p>Abstract: A topic model outputs a set of multinomial distributions over words for each topic. In this paper, we investigate the value of bilingual topic models, i.e., a bilingual Latent Dirichlet Allocation model for finding translations of terms in comparable corpora without using any linguistic resources. Experiments on a document-aligned English-Italian Wikipedia corpus confirm that the developed methods which only use knowledge from word-topic distributions outperform methods based on similarity measures in the original word-document space. The best results, obtained by combining knowledge from wordtopic distributions with similarity measures in the original space, are also reported.</p><p>6 0.1293803 <a title="251-tfidf-6" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>7 0.12816215 <a title="251-tfidf-7" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>8 0.12798893 <a title="251-tfidf-8" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>9 0.127729 <a title="251-tfidf-9" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>10 0.11940276 <a title="251-tfidf-10" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>11 0.11769835 <a title="251-tfidf-11" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>12 0.11682552 <a title="251-tfidf-12" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>13 0.11377214 <a title="251-tfidf-13" href="./acl-2011-Structural_Topic_Model_for_Latent_Topical_Structure_Analysis.html">287 acl-2011-Structural Topic Model for Latent Topical Structure Analysis</a></p>
<p>14 0.1110047 <a title="251-tfidf-14" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>15 0.10862881 <a title="251-tfidf-15" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>16 0.10199249 <a title="251-tfidf-16" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>17 0.099672578 <a title="251-tfidf-17" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>18 0.095798656 <a title="251-tfidf-18" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>19 0.089208618 <a title="251-tfidf-19" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>20 0.081870221 <a title="251-tfidf-20" href="./acl-2011-Sentence_Ordering_Driven_by_Local_and_Global_Coherence_for_Summary_Generation.html">280 acl-2011-Sentence Ordering Driven by Local and Global Coherence for Summary Generation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.19), (1, 0.095), (2, -0.07), (3, 0.127), (4, -0.05), (5, -0.091), (6, -0.122), (7, 0.248), (8, -0.006), (9, 0.042), (10, -0.089), (11, 0.061), (12, -0.062), (13, 0.012), (14, -0.01), (15, -0.045), (16, 0.011), (17, 0.036), (18, -0.025), (19, 0.05), (20, -0.01), (21, 0.017), (22, 0.037), (23, -0.015), (24, 0.008), (25, -0.004), (26, 0.016), (27, -0.075), (28, -0.002), (29, -0.022), (30, 0.005), (31, -0.031), (32, 0.053), (33, -0.089), (34, 0.001), (35, 0.016), (36, 0.042), (37, -0.021), (38, 0.029), (39, 0.016), (40, -0.006), (41, 0.038), (42, 0.003), (43, -0.014), (44, -0.031), (45, 0.014), (46, -0.046), (47, -0.04), (48, 0.026), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95299536 <a title="251-lsi-1" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>Author: William M. Darling ; Fei Song</p><p>Abstract: Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.</p><p>2 0.902704 <a title="251-lsi-2" href="./acl-2011-Discovery_of_Topically_Coherent_Sentences_for_Extractive_Summarization.html">98 acl-2011-Discovery of Topically Coherent Sentences for Extractive Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Extractive methods for multi-document summarization are mainly governed by information overlap, coherence, and content constraints. We present an unsupervised probabilistic approach to model the hidden abstract concepts across documents as well as the correlation between these concepts, to generate topically coherent and non-redundant summaries. Based on human evaluations our models generate summaries with higher linguistic quality in terms of coherence, readability, and redundancy compared to benchmark systems. Although our system is unsupervised and optimized for topical coherence, we achieve a 44.1 ROUGE on the DUC-07 test set, roughly in the range of state-of-the-art supervised models.</p><p>3 0.77375668 <a title="251-lsi-3" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<p>Author: Risa Kitajima ; Ichiro Kobayashi</p><p>Abstract: Recently, several latent topic analysis methods such as LSI, pLSI, and LDA have been widely used for text analysis. However, those methods basically assign topics to words, but do not account for the events in a document. With this background, in this paper, we propose a latent topic extracting method which assigns topics to events. We also show that our proposed method is useful to generate a document summary based on a latent topic.</p><p>4 0.76405102 <a title="251-lsi-4" href="./acl-2011-Towards_a_Framework_for_Abstractive_Summarization_of_Multimodal_Documents.html">308 acl-2011-Towards a Framework for Abstractive Summarization of Multimodal Documents</a></p>
<p>Author: Charles Greenbacker</p><p>Abstract: We propose a framework for generating an abstractive summary from a semantic model of a multimodal document. We discuss the type of model required, the means by which it can be constructed, how the content of the model is rated and selected, and the method of realizing novel sentences for the summary. To this end, we introduce a metric called information density used for gauging the importance of content obtained from text and graphical sources.</p><p>5 0.75098032 <a title="251-lsi-5" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>Author: Yves Petinot ; Kathleen McKeown ; Kapil Thadani</p><p>Abstract: We investigate the relevance of hierarchical topic models to represent the content of Web gists. We focus our attention on DMOZ, a popular Web directory, and propose two algorithms to infer such a model from its manually-curated hierarchy of categories. Our first approach, based on information-theoretic grounds, uses an algorithm similar to recursive feature selection. Our second approach is fully Bayesian and derived from the more general model, hierarchical LDA. We evaluate the performance of both models against a flat 1-gram baseline and show improvements in terms of perplexity over held-out data.</p><p>6 0.73371977 <a title="251-lsi-6" href="./acl-2011-Comparative_News_Summarization_Using_Linear_Programming.html">76 acl-2011-Comparative News Summarization Using Linear Programming</a></p>
<p>7 0.72400278 <a title="251-lsi-7" href="./acl-2011-Structural_Topic_Model_for_Latent_Topical_Structure_Analysis.html">287 acl-2011-Structural Topic Model for Latent Topical Structure Analysis</a></p>
<p>8 0.70897901 <a title="251-lsi-8" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>9 0.70812654 <a title="251-lsi-9" href="./acl-2011-A_Pilot_Study_of_Opinion_Summarization_in_Conversations.html">21 acl-2011-A Pilot Study of Opinion Summarization in Conversations</a></p>
<p>10 0.70356715 <a title="251-lsi-10" href="./acl-2011-SciSumm%3A_A_Multi-Document_Summarization_System_for_Scientific_Articles.html">270 acl-2011-SciSumm: A Multi-Document Summarization System for Scientific Articles</a></p>
<p>11 0.66245747 <a title="251-lsi-11" href="./acl-2011-Automatic_Assessment_of_Coverage_Quality_in_Intelligence_Reports.html">47 acl-2011-Automatic Assessment of Coverage Quality in Intelligence Reports</a></p>
<p>12 0.64500612 <a title="251-lsi-12" href="./acl-2011-A_Class_of_Submodular_Functions_for_Document_Summarization.html">4 acl-2011-A Class of Submodular Functions for Document Summarization</a></p>
<p>13 0.64122659 <a title="251-lsi-13" href="./acl-2011-Learning_From_Collective_Human_Behavior_to_Introduce_Diversity_in_Lexical_Choice.html">201 acl-2011-Learning From Collective Human Behavior to Introduce Diversity in Lexical Choice</a></p>
<p>14 0.63235635 <a title="251-lsi-14" href="./acl-2011-Query_Snowball%3A_A_Co-occurrence-based_Approach_to_Multi-document_Summarization_for_Question_Answering.html">255 acl-2011-Query Snowball: A Co-occurrence-based Approach to Multi-document Summarization for Question Answering</a></p>
<p>15 0.6293745 <a title="251-lsi-15" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>16 0.62404126 <a title="251-lsi-16" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>17 0.61069506 <a title="251-lsi-17" href="./acl-2011-Automatic_Labelling_of_Topic_Models.html">52 acl-2011-Automatic Labelling of Topic Models</a></p>
<p>18 0.60797679 <a title="251-lsi-18" href="./acl-2011-Using_Bilingual_Information_for_Cross-Language_Document_Summarization.html">326 acl-2011-Using Bilingual Information for Cross-Language Document Summarization</a></p>
<p>19 0.59686911 <a title="251-lsi-19" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>20 0.58729732 <a title="251-lsi-20" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.021), (12, 0.18), (17, 0.05), (26, 0.019), (37, 0.071), (39, 0.082), (41, 0.073), (55, 0.045), (59, 0.058), (72, 0.03), (91, 0.05), (96, 0.211), (97, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92782599 <a title="251-lda-1" href="./acl-2011-Contrasting_Opposing_Views_of_News_Articles_on_Contentious_Issues.html">84 acl-2011-Contrasting Opposing Views of News Articles on Contentious Issues</a></p>
<p>Author: Souneil Park ; Kyung Soon Lee ; Junehwa Song</p><p>Abstract: We present disputant relation-based method for classifying news articles on contentious issues. We observe that the disputants of a contention are an important feature for understanding the discourse. It performs unsupervised classification on news articles based on disputant relations, and helps readers intuitively view the articles through the opponent-based frame. The readers can attain balanced understanding on the contention, free from a specific biased view. We applied a modified version of HITS algorithm and an SVM classifier trained with pseudo-relevant data for article analysis. 1</p><p>2 0.8809486 <a title="251-lda-2" href="./acl-2011-Reordering_Constraint_Based_on_Document-Level_Context.html">263 acl-2011-Reordering Constraint Based on Document-Level Context</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: One problem with phrase-based statistical machine translation is the problem of longdistance reordering when translating between languages with different word orders, such as Japanese-English. In this paper, we propose a method of imposing reordering constraints using document-level context. As the documentlevel context, we use noun phrases which significantly occur in context documents containing source sentences. Given a source sentence, zones which cover the noun phrases are used as reordering constraints. Then, in decoding, reorderings which violate the zones are restricted. Experiment results for patent translation tasks show a significant improvement of 1.20% BLEU points in JapaneseEnglish translation and 1.41% BLEU points in English-Japanese translation.</p><p>same-paper 3 0.84948933 <a title="251-lda-3" href="./acl-2011-Probabilistic_Document_Modeling_for_Syntax_Removal_in_Text_Summarization.html">251 acl-2011-Probabilistic Document Modeling for Syntax Removal in Text Summarization</a></p>
<p>Author: William M. Darling ; Fei Song</p><p>Abstract: Statistical approaches to automatic text summarization based on term frequency continue to perform on par with more complex summarization methods. To compute useful frequency statistics, however, the semantically important words must be separated from the low-content function words. The standard approach of using an a priori stopword list tends to result in both undercoverage, where syntactical words are seen as semantically relevant, and overcoverage, where words related to content are ignored. We present a generative probabilistic modeling approach to building content distributions for use with statistical multi-document summarization where the syntax words are learned directly from the data with a Hidden Markov Model and are thereby deemphasized in the term frequency statistics. This approach is compared to both a stopword-list and POS-tagging approach and our method demonstrates improved coverage on the DUC 2006 and TAC 2010 datasets using the ROUGE metric.</p><p>4 0.81793427 <a title="251-lda-4" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>Author: Joseph Reisinger ; Marius Pasca</p><p>Abstract: We develop a novel approach to the semantic analysis of short text segments and demonstrate its utility on a large corpus of Web search queries. Extracting meaning from short text segments is difficult as there is little semantic redundancy between terms; hence methods based on shallow semantic analysis may fail to accurately estimate meaning. Furthermore search queries lack explicit syntax often used to determine intent in question answering. In this paper we propose a hybrid model of semantic analysis combining explicit class-label extraction with a latent class PCFG. This class-label correlation (CLC) model admits a robust parallel approximation, allowing it to scale to large amounts of query data. We demonstrate its performance in terms of (1) its predicted label accuracy on polysemous queries and (2) its ability to accurately chunk queries into base constituents.</p><p>5 0.81359977 <a title="251-lda-5" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>Author: Phil Blunsom ; Trevor Cohn</p><p>Abstract: In this work we address the problem of unsupervised part-of-speech induction by bringing together several strands of research into a single model. We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman-Yor processes prior, providing an elegant and principled means of incorporating lexical characteristics. Central to our approach is a new type-based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages.</p><p>6 0.8114922 <a title="251-lda-6" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>7 0.81094575 <a title="251-lda-7" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>8 0.81056988 <a title="251-lda-8" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>9 0.80862653 <a title="251-lda-9" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<p>10 0.80855447 <a title="251-lda-10" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>11 0.80825168 <a title="251-lda-11" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>12 0.80583376 <a title="251-lda-12" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>13 0.80573726 <a title="251-lda-13" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>14 0.80543089 <a title="251-lda-14" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>15 0.80485249 <a title="251-lda-15" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>16 0.80419421 <a title="251-lda-16" href="./acl-2011-A_Statistical_Tree_Annotator_and_Its_Applications.html">28 acl-2011-A Statistical Tree Annotator and Its Applications</a></p>
<p>17 0.80359685 <a title="251-lda-17" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>18 0.80183971 <a title="251-lda-18" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>19 0.80111283 <a title="251-lda-19" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>20 0.80095184 <a title="251-lda-20" href="./acl-2011-A_Latent_Topic_Extracting_Method_based_on_Events_in_a_Document_and_its_Application.html">18 acl-2011-A Latent Topic Extracting Method based on Events in a Document and its Application</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
