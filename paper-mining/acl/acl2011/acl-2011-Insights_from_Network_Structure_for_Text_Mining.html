<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>174 acl-2011-Insights from Network Structure for Text Mining</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-174" href="#">acl2011-174</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>174 acl-2011-Insights from Network Structure for Text Mining</h1>
<br/><p>Source: <a title="acl-2011-174-pdf" href="http://aclweb.org/anthology//P/P11/P11-1162.pdf">pdf</a></p><p>Author: Zornitsa Kozareva ; Eduard Hovy</p><p>Abstract: Text mining and data harvesting algorithms have become popular in the computational linguistics community. They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications.</p><p>Reference: <a title="acl-2011-174-reference" href="../acl2011_reference/acl-2011-Insights_from_Network_Structure_for_Text_Mining_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Text mining and data harvesting algorithms have become popular in the computational linguistics community. [sent-2, score-0.33]
</p><p>2 They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. [sent-3, score-0.502]
</p><p>3 They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. [sent-4, score-0.239]
</p><p>4 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al. [sent-8, score-0.511]
</p><p>5 They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the 1616 seed(s). [sent-13, score-0.27]
</p><p>6 Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. [sent-17, score-0.395]
</p><p>7 This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. [sent-18, score-0.346]
</p><p>8 Text mining is a process of network traversal, and faces the standard problems of handling cycles, ranking search alternatives, estimating yield maxima, etc. [sent-20, score-0.37]
</p><p>9 The computational properties of large networks and large network traversal have been studied intensively (Sabidussi, 1966; Freeman, 1979; Watts and Strogatz, 1998) and especially, over the past years, in the context of the world wide web (Page et al. [sent-21, score-0.814]
</p><p>10 It sometimes explains why text mining algo-  1These networks are generally far larger and more densely interconnected than the world wide web’s network of pages and hyperlinks. [sent-28, score-0.696]
</p><p>11 In Section 3 we describe the general harvesting procedure, and follow with an examination of the various statistical properties of implicit semantic networks in Section 4, using our implemented harvester to provide illustrative statistics. [sent-33, score-0.834]
</p><p>12 While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. [sent-43, score-0.284]
</p><p>13 Researchers outside computational linguistics have studied complex networks such as the World Wide Web, the Social Web, the network of scientific papers, among others. [sent-46, score-0.677]
</p><p>14 They have investigated the properties of these text-based networks with the objective of understanding their structure and applying this knowledge to determine node importance/centrality, connectivity, growth and decay of interest, etc. [sent-47, score-0.544]
</p><p>15 However, no-one has studied the properties of the text-based semantic networks induced by semantic relations between terms with the objective of understanding their structure and applying this knowledge to improve concept discovery. [sent-58, score-0.981]
</p><p>16 Most relevant  to this theme is the work of Steyvers and Tenenbaum (Steyvers and Tenenbaum, 2004), who studied three manually built lexical networks (association norms, WordNet, and Roget’s Thesaurus (Roget, 191 1)) and proposed a model of the growth of the semantic structure over time. [sent-59, score-0.606]
</p><p>17 These networks are limited to the semantic relations among nouns. [sent-60, score-0.622]
</p><p>18 In this paper we take a step further to explore the statistical properties of semantic networks relating proper names, nouns, verbs, and adjectives. [sent-61, score-0.585]
</p><p>19 We implement a general harvesting procedure and show its results for these word types. [sent-63, score-0.249]
</p><p>20 A fundamental difference with the work of (Steyvers and Tenenbaum, 2004) is that we study very large semantic networks built ‘naturally’ by (millions of) users rather than ‘artificially’ by a small set of experts. [sent-64, score-0.541]
</p><p>21 The large networks capture the semantic intuitions and knowledge of the collective mass. [sent-65, score-0.541]
</p><p>22 3  Inducing Semantic Networks in the Web  Text mining algorithms such as those mentioned above raise certain questions, such as: Why are some seed terms more powerful (provide a greater yield) than others? [sent-67, score-0.293]
</p><p>23 On the face of it, one would need to know the structure of the network a priori to be able to provide answers. [sent-72, score-0.253]
</p><p>24 For example, in the text mining community, (Kozareva and Hovy, 2010b) have shown that one can obtain a quite accurate estimate of the eventual yield of a pattern and seed after only five steps of harvesting. [sent-74, score-0.395]
</p><p>25 They do not provide an answer, but research from the network community does. [sent-76, score-0.258]
</p><p>26 To illustrate the properties of networks of the kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. [sent-77, score-1.214]
</p><p>27 Since the goal of this paper is not the development of a new text harvesting algorithm, we implemented a version of an existing one: the so-called DAP (doubly-anchored pattern) algorithm (Kozareva et al. [sent-78, score-0.249]
</p><p>28 , 2007), (5) can be formulated to learn semantic lexicons and relations for noun, verb and verb+preposition syntactic constructions; (6) functions equally well in different languages. [sent-81, score-0.262]
</p><p>29 Next we describe the knowledge harvesting procedure and the construction of the text-mined semantic networks. [sent-82, score-0.43]
</p><p>30 1 Harvesting to Induce Semantic Networks  For a given semantic class of interest say singers, the algorithm starts with a seed example ofthe class, say 1618 Madonna. [sent-84, score-0.324]
</p><p>31 The seed term is inserted in the lexicosyntactic pattern “class such as seed and *”, which learns on the position of the ∗ new terms of type clelaasrns. [sent-85, score-0.473]
</p><p>32 The output of the algorithm is a set of terms for the semantic class. [sent-87, score-0.25]
</p><p>33 The algorithm is implemented as a breadth-first search and its mechanism is described as follows:  The output of the knowledge harvesting algorithm is a network of semantic terms interconnected by the semantic relation captured in the pattern. [sent-88, score-0.967]
</p><p>34 We can represent the traversed (implicit) network as a directed graph G(V, E) with nodes V (|V | = n) adnirde edges E(|E| = m). [sent-89, score-0.457]
</p><p>35 For example, given the sentence (where the pattern is in italics and the extracted term is underlined) “He loves singers such as Madonna and Michael Jackson”, two nodes Madonna and Michael Jackson with an edge e=(Madonna, Michael Jackson) would be created in the graph G. [sent-95, score-0.455]
</p><p>36 The starting seed term Madonna is shown in red color and the harvested terms are in blue. [sent-97, score-0.371]
</p><p>37 2  Data  We harvested data from the Web for a representative selection of semantic classes and relations, of  ! [sent-99, score-0.309]
</p><p>38 , 2005; Pasca, 2007; Kozareva and Hovy, 2010a):  •  •  •  •  •  semantic classes that can be learned using difsfeemrenatn tsicee cdlsa (e. [sent-116, score-0.252]
</p><p>39 , “singers es luecahr as M usaindgon dnifaand *” and “singers such as Placido Domingo and *”); semantic classes that are expressed through difsfeemrenatn lexico-syntactic patterns (e. [sent-118, score-0.31]
</p><p>40 , “expensive aenridz *n car”, “dogs run and *”); semantic relations with more complex lexicosyntactic structure (e. [sent-122, score-0.293]
</p><p>41 In total, we collected 10GB of data which was partof-speech tagged with Treetagger (Schmid, 1994) and used for the semantic term extraction. [sent-131, score-0.243]
</p><p>42 Table 1 summarizes the number of nodes and edges learned for each semantic network using pattern Pi and the initial seed shown in italics. [sent-132, score-0.799]
</p><p>43 4  Statistical Properties of Text-Mined Semantic Networks  In this section we apply a range of relevant measures from the network analysis community to the networks described above. [sent-134, score-0.618]
</p><p>44 It measures the degree to which the network structure determines the importance of a node in the network (Sabidussi, 1966; Freeman, 1979). [sent-137, score-0.673]
</p><p>45 We explore the effect of two centrality measures: indegree and outdegree. [sent-138, score-0.333]
</p><p>46 The indegree of a node u denoted as indegree(u)=P(v, u) considers the  sum of all incoming edges to uP and captures the ability of a semantic term to be dPiscovered by other semantic terms. [sent-139, score-0.847]
</p><p>47 The Poutdegree of a node u denoted as outdegree(u)=P(u, v) considers the number of outgoing edges of Pthe node u and measures the ability of a semantic tPerm to discover new terms. [sent-140, score-0.525]
</p><p>48 Since harvesting algorithms are notorious for extracting erroneous information, we use the two centrality measures to rerank the harvested elements. [sent-142, score-0.437]
</p><p>49 Table 2 shows the accuracy2 of the singer semantic terms at different ranks using the in and out degree measures. [sent-143, score-0.395]
</p><p>50 shows that for the text-mined semantic networks, the ability of a term to discover new terms is more important than the ability to be discovered. [sent-146, score-0.432]
</p><p>51 Table 3 shows the top and bottom 10 terms of the semantic class. [sent-153, score-0.25]
</p><p>52 The nodes with high outdegree correspond to famous or contemporary singers. [sent-155, score-0.54]
</p><p>53 Potentially, knowing which terms have a high outdegree allows one to rerank candidate seeds for more effective harvesting. [sent-157, score-0.528]
</p><p>54 , 2000) and social networks like Orkut and Flickr, the textmined semantic networks also exhibit a power-law distribution. [sent-161, score-0.943]
</p><p>55 This means that while a few terms have a significantly high degree, the majority of the semantic terms have small degree. [sent-162, score-0.319]
</p><p>56 Figure 2 shows the indegree and outdegree distributions for different semantic classes, lexico-syntactic patterns, and languages (English and Spanish). [sent-163, score-0.836]
</p><p>57 It is interesting to note that the indegree powerlaw exponents for all semantic networks fall within the same range (γin ≈ 2. [sent-174, score-0.836]
</p><p>58 4), and similarly for the outdegree exponents (γout ≈ n1 . [sent-175, score-0.435]
</p><p>59 However, tthhee values of the indegree and≈ outdegree exponents differ from each other. [sent-177, score-0.677]
</p><p>60 The difference in the distributions can be explained by the link asymmetry of semantic terms: A discovering B does not necessarily mean that B will discover A. [sent-180, score-0.266]
</p><p>61 In the text-mined semantic networks, this asymmetry is caused by patterns of language use, such as the fact that people use first adjectives of the  size and then of the color (e. [sent-181, score-0.315]
</p><p>62 3  Sparsity  Another relevant property of the semantic networks concerns sparsity. [sent-186, score-0.541]
</p><p>63 Sparsity can e bex -aslesom captured through t ihse density o Sfp tahreAll networks have low density which suggests −th1a)t the networks exhibit a sparse connectivity pattern. [sent-190, score-0.841]
</p><p>64 Similar behavior was reported for the WordNet and Roget’s semantic networks (Steyvers and Tenenbaum, 2004). [sent-192, score-0.541]
</p><p>65 4  Connectedness  For every network, we computed the strongly connected component (SCC) such that for all nodes (semantic terms) in the SCC, there is a path from any node to another node in the SCC considering the direction of the edges between the nodes. [sent-202, score-0.493]
</p><p>66 Unlike WordNet and Roget’s semantic networks where the SCC consists 96% of all semantic terms, in the text-mined semantic networks only 12 to 55% of the terms are in the SCC. [sent-205, score-1.332]
</p><p>67 This shows that not all nodes can reach (discover) every other node in the network. [sent-206, score-0.267]
</p><p>68 5 Path Lengths and Diameter Next, we describe the properties of the shortest paths between the semantic terms in the SCC. [sent-211, score-0.294]
</p><p>69 The diameter of the SCC is calculated as the maximum distance over all pairs of nodes (u, v), such that a node v is reachable from node u. [sent-215, score-0.497]
</p><p>70 Table 5 shows the average distance and the diameter of the semantic networks. [sent-216, score-0.302]
</p><p>71 The diameter shows the maximum number of steps necessary to reach from any node to any other, while the average distance shows the number of steps necessary on average. [sent-219, score-0.298]
</p><p>72 Overall, all networks have very short average path lengths and small diameters that are consistent with Watt’s finding for small-world networks. [sent-220, score-0.392]
</p><p>73 Therefore, the yield of harvesting seeds can be predicted within five steps explaining (Kozareva and Hovy, 2010b; Vyas et al. [sent-221, score-0.392]
</p><p>74 We also compute for any randomly selected node in the semantic network on average how many hops (steps) are necessary to reach from one node to another. [sent-223, score-0.722]
</p><p>75 6 Clustering The clustering coefficient (C) is another measure to study the connectivity structure of the networks (Watts and Strogatz, 1998). [sent-226, score-0.583]
</p><p>76 The clustering coefficient C for the whole semantic network is the Paverage clustering coefficient of all its nodes, C=1n P Ci. [sent-230, score-0.681]
</p><p>77 The value of the clustering coefficient rangesP between [0, 1] , where 0 indicates that  the nodes doP not have neighbors which are themselves connected, while 1indicates that all nodes are connected. [sent-231, score-0.455]
</p><p>78 Table 6 shows the clustering coefficient for all text-mined semantic networks together with the number of closed and open triads3. [sent-232, score-0.68]
</p><p>79 Similarly, we are interested in understanding how the nodes of the semantic networks connect to each other. [sent-237, score-0.76]
</p><p>80 JDD is approximated by the degree correlation function knn which maps the outdegree and the average 3A triad is three nodes that are connected by either two (open triad) or three (closed triad) directed ties. [sent-240, score-0.8]
</p><p>81 CClosedTriadsOpenTriads  indegree of all nodes connected to a node with that outdegree. [sent-242, score-0.555]
</p><p>82 High values of knn indicate that high-degree nodes tend to connect to other highdegree nodes (forming a “core” in the network), while lower values of knn suggest that the highdegree nodes tend to connect to low-degree ones. [sent-243, score-0.806]
</p><p>83 The figure  plots the outdegree and the average indegree of the semantic terms in the networks on a log-log scale. [sent-245, score-1.234]
</p><p>84 We can see that for all networks the high-degree nodes tend to connect to other high-degree ones. [sent-246, score-0.579]
</p><p>85 8 Assortivity The property of the nodes to connect to other nodes with similar degrees can be captured through the assortivity coefficient r (Newman, 2003). [sent-249, score-0.55]
</p><p>86 d Aes pteosndit vtoe caossnonreticvti tyo nooefdfeics eonf s mimeialnars degree, while negative coefficient means that nodes are likely to connect to nodes with degree very different from their own. [sent-252, score-0.558]
</p><p>87 We find that the assortivitiy coefficient of our semantic networks is positive, ranging from 0. [sent-253, score-0.633]
</p><p>88 In this respect, the semantic networks differ from the Web, which has a negative assortivity (Newman, 2003). [sent-256, score-0.622]
</p><p>89 5  Discussion  The above studies show that many of the properties discovered of the network formed by the web hold also for the networks induced by semantic relations in text mining applications, for various semantic classes, semantic relations, and languages. [sent-259, score-1.415]
</p><p>90 The small-world phenomenon, for example, holds that any node is connected to any other node in at most six steps. [sent-261, score-0.264]
</p><p>91 5 the semantic networks also exhibit this phenomenon, we can explain the observation of (Kozareva and Hovy, 2010b) that one can quite accurately predict the relative ‘goodness’ of a seed term (its eventual total yield and the number of steps required to obtain that) within five harvesting steps. [sent-263, score-1.11]
</p><p>92 We have shown that due  1623 to the strongly connected components in text mining networks, not all elements within the harvested graph can discover each other. [sent-264, score-0.316]
</p><p>93 This implies that harvesting algorithms have to be started with several seeds to obtain adequate Recall (Vyas et al. [sent-265, score-0.326]
</p><p>94 We have shown that centrality measures can be used successfully to rank harvested terms to guide the network traversal, and to validate the correctness of the harvested terms. [sent-267, score-0.576]
</p><p>95 6  Conclusion  In this paper we describe the implicit ‘hidden’ semantic network graph structure induced over the text of the web and other sources by the semantic relations people use in sentences. [sent-269, score-0.856]
</p><p>96 We describe how term harvesting patterns whose seed terms are harvested and then applied recursively can be used to discover these semantic term networks. [sent-270, score-0.975]
</p><p>97 Learning arguments and supertypes of semantic relations using recursive patterns. [sent-347, score-0.262]
</p><p>98 Espresso: Leveraging generic patterns for automatically harvesting semantic relations. [sent-384, score-0.488]
</p><p>99 The large-scale structure of semantic networks: Statistical analyses and a model of semantic growth. [sent-446, score-0.393]
</p><p>100 Ranking scientific publications using a simple model of network traffic. [sent-463, score-0.283]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('outdegree', 0.382), ('networks', 0.36), ('harvesting', 0.249), ('indegree', 0.242), ('network', 0.222), ('semantic', 0.181), ('nodes', 0.158), ('kozareva', 0.153), ('seed', 0.143), ('singers', 0.141), ('node', 0.109), ('scc', 0.106), ('madonna', 0.106), ('broder', 0.101), ('hops', 0.101), ('tenenbaum', 0.101), ('harvested', 0.097), ('coefficient', 0.092), ('centrality', 0.091), ('degree', 0.089), ('web', 0.084), ('roget', 0.082), ('diameter', 0.082), ('mining', 0.081), ('relations', 0.081), ('assortivity', 0.081), ('seeds', 0.077), ('pasca', 0.073), ('traversal', 0.07), ('terms', 0.069), ('steyvers', 0.066), ('vyas', 0.065), ('knn', 0.065), ('zornitsa', 0.065), ('term', 0.062), ('scientific', 0.061), ('connect', 0.061), ('triad', 0.06), ('watts', 0.06), ('patterns', 0.058), ('singer', 0.056), ('pattern', 0.056), ('discover', 0.054), ('connectivity', 0.053), ('exponents', 0.053), ('etzioni', 0.052), ('eventual', 0.049), ('clauset', 0.049), ('hovy', 0.048), ('newman', 0.047), ('riloff', 0.047), ('clustering', 0.047), ('pagerank', 0.046), ('girju', 0.046), ('jackson', 0.046), ('connected', 0.046), ('eduard', 0.045), ('pantel', 0.044), ('properties', 0.044), ('social', 0.042), ('kleinberg', 0.042), ('suchanek', 0.042), ('cantantes', 0.04), ('difsfeemrenatn', 0.04), ('gasser', 0.04), ('highdegree', 0.04), ('huafeng', 0.04), ('jdd', 0.04), ('preiss', 0.04), ('radicchi', 0.04), ('sabidussi', 0.04), ('sayyadi', 0.04), ('strogatz', 0.04), ('distance', 0.039), ('edges', 0.039), ('graph', 0.038), ('adjectives', 0.038), ('people', 0.038), ('community', 0.036), ('freeman', 0.035), ('bombs', 0.035), ('ranking', 0.035), ('discovery', 0.035), ('studied', 0.034), ('density', 0.034), ('steps', 0.034), ('ability', 0.033), ('shepherd', 0.033), ('interconnected', 0.033), ('path', 0.032), ('citation', 0.032), ('yield', 0.032), ('relation', 0.032), ('wordnet', 0.032), ('distributions', 0.031), ('structure', 0.031), ('classes', 0.031), ('kempe', 0.031), ('xie', 0.031), ('scientists', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="174-tfidf-1" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>Author: Zornitsa Kozareva ; Eduard Hovy</p><p>Abstract: Text mining and data harvesting algorithms have become popular in the computational linguistics community. They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications.</p><p>2 0.15558675 <a title="174-tfidf-2" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>Author: Apoorv Agarwal</p><p>Abstract: In my thesis, Ipropose to build a system that would enable extraction of social interactions from texts. To date Ihave defined a comprehensive set of social events and built a preliminary system that extracts social events from news articles. Iplan to improve the performance of my current system by incorporating semantic information. Using domain adaptation techniques, Ipropose to apply my system to a wide range of genres. By extracting linguistic constructs relevant to social interactions, I will be able to empirically analyze different kinds of linguistic constructs that people use to express social interactions. Lastly, I will attempt to make convolution kernels more scalable and interpretable.</p><p>3 0.15196121 <a title="174-tfidf-3" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>Author: Ahmed Hassan ; Amjad AbuJbara ; Rahul Jha ; Dragomir Radev</p><p>Abstract: We present a method for identifying the positive or negative semantic orientation of foreign words. Identifying the semantic orientation of words has numerous applications in the areas of text classification, analysis of product review, analysis of responses to surveys, and mining online discussions. Identifying the semantic orientation of English words has been extensively studied in literature. Most of this work assumes the existence of resources (e.g. Wordnet, seeds, etc) that do not exist in foreign languages. In this work, we describe a method based on constructing a multilingual network connecting English and foreign words. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results.</p><p>4 0.12138372 <a title="174-tfidf-4" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>5 0.1047722 <a title="174-tfidf-5" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>6 0.10461446 <a title="174-tfidf-6" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>7 0.099754088 <a title="174-tfidf-7" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>8 0.09952607 <a title="174-tfidf-8" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>9 0.089341938 <a title="174-tfidf-9" href="./acl-2011-Machine_Translation_System_Combination_by_Confusion_Forest.html">217 acl-2011-Machine Translation System Combination by Confusion Forest</a></p>
<p>10 0.087184116 <a title="174-tfidf-10" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>11 0.087025195 <a title="174-tfidf-11" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>12 0.081785552 <a title="174-tfidf-12" href="./acl-2011-End-to-End_Relation_Extraction_Using_Distant_Supervision_from_External_Semantic_Repositories.html">114 acl-2011-End-to-End Relation Extraction Using Distant Supervision from External Semantic Repositories</a></p>
<p>13 0.078549907 <a title="174-tfidf-13" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>14 0.078244902 <a title="174-tfidf-14" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>15 0.077687576 <a title="174-tfidf-15" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>16 0.076795429 <a title="174-tfidf-16" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>17 0.074653879 <a title="174-tfidf-17" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>18 0.074313648 <a title="174-tfidf-18" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>19 0.073836043 <a title="174-tfidf-19" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>20 0.069664121 <a title="174-tfidf-20" href="./acl-2011-Template-Based_Information_Extraction_without_the_Templates.html">293 acl-2011-Template-Based Information Extraction without the Templates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, 0.064), (2, -0.085), (3, 0.005), (4, 0.013), (5, -0.021), (6, -0.011), (7, -0.034), (8, -0.066), (9, -0.089), (10, -0.018), (11, -0.051), (12, 0.052), (13, 0.038), (14, -0.073), (15, -0.164), (16, -0.016), (17, -0.111), (18, -0.0), (19, -0.051), (20, 0.015), (21, 0.065), (22, 0.032), (23, 0.085), (24, 0.055), (25, -0.073), (26, -0.01), (27, 0.189), (28, 0.003), (29, 0.029), (30, 0.011), (31, -0.04), (32, -0.023), (33, -0.036), (34, 0.133), (35, -0.068), (36, -0.022), (37, 0.012), (38, -0.06), (39, 0.05), (40, -0.003), (41, -0.154), (42, -0.062), (43, 0.03), (44, -0.016), (45, 0.063), (46, -0.023), (47, 0.004), (48, -0.011), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93769377 <a title="174-lsi-1" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>Author: Zornitsa Kozareva ; Eduard Hovy</p><p>Abstract: Text mining and data harvesting algorithms have become popular in the computational linguistics community. They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications.</p><p>2 0.69641274 <a title="174-lsi-2" href="./acl-2011-Identifying_the_Semantic_Orientation_of_Foreign_Words.html">162 acl-2011-Identifying the Semantic Orientation of Foreign Words</a></p>
<p>Author: Ahmed Hassan ; Amjad AbuJbara ; Rahul Jha ; Dragomir Radev</p><p>Abstract: We present a method for identifying the positive or negative semantic orientation of foreign words. Identifying the semantic orientation of words has numerous applications in the areas of text classification, analysis of product review, analysis of responses to surveys, and mining online discussions. Identifying the semantic orientation of English words has been extensively studied in literature. Most of this work assumes the existence of resources (e.g. Wordnet, seeds, etc) that do not exist in foreign languages. In this work, we describe a method based on constructing a multilingual network connecting English and foreign words. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results.</p><p>3 0.665923 <a title="174-lsi-3" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>4 0.62088424 <a title="174-lsi-4" href="./acl-2011-Clairlib%3A_A_Toolkit_for_Natural_Language_Processing%2C_Information_Retrieval%2C_and_Network_Analysis.html">67 acl-2011-Clairlib: A Toolkit for Natural Language Processing, Information Retrieval, and Network Analysis</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In this paper we present Clairlib, an opensource toolkit for Natural Language Processing, Information Retrieval, and Network Analysis. Clairlib provides an integrated framework intended to simplify a number of generic tasks within and across those three areas. It has a command-line interface, a graphical interface, and a documented API. Clairlib is compatible with all the common platforms and operating systems. In addition to its own functionality, it provides interfaces to external software and corpora. Clairlib comes with a comprehensive documentation and a rich set of tutorials and visual demos.</p><p>5 0.55684102 <a title="174-lsi-5" href="./acl-2011-Social_Network_Extraction_from_Texts%3A_A_Thesis_Proposal.html">286 acl-2011-Social Network Extraction from Texts: A Thesis Proposal</a></p>
<p>Author: Apoorv Agarwal</p><p>Abstract: In my thesis, Ipropose to build a system that would enable extraction of social interactions from texts. To date Ihave defined a comprehensive set of social events and built a preliminary system that extracts social events from news articles. Iplan to improve the performance of my current system by incorporating semantic information. Using domain adaptation techniques, Ipropose to apply my system to a wide range of genres. By extracting linguistic constructs relevant to social interactions, I will be able to empirically analyze different kinds of linguistic constructs that people use to express social interactions. Lastly, I will attempt to make convolution kernels more scalable and interpretable.</p><p>6 0.53710753 <a title="174-lsi-6" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>7 0.53590214 <a title="174-lsi-7" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>8 0.52387792 <a title="174-lsi-8" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>9 0.51854408 <a title="174-lsi-9" href="./acl-2011-Typed_Graph_Models_for_Learning_Latent_Attributes_from_Names.html">314 acl-2011-Typed Graph Models for Learning Latent Attributes from Names</a></p>
<p>10 0.51006317 <a title="174-lsi-10" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>11 0.50415289 <a title="174-lsi-11" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<p>12 0.49592569 <a title="174-lsi-12" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>13 0.46865413 <a title="174-lsi-13" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>14 0.46378279 <a title="174-lsi-14" href="./acl-2011-NULEX%3A_An_Open-License_Broad_Coverage_Lexicon.html">229 acl-2011-NULEX: An Open-License Broad Coverage Lexicon</a></p>
<p>15 0.45406288 <a title="174-lsi-15" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>16 0.45310867 <a title="174-lsi-16" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>17 0.45032385 <a title="174-lsi-17" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>18 0.44581023 <a title="174-lsi-18" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>19 0.43769857 <a title="174-lsi-19" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>20 0.43453625 <a title="174-lsi-20" href="./acl-2011-Types_of_Common-Sense_Knowledge_Needed_for_Recognizing_Textual_Entailment.html">315 acl-2011-Types of Common-Sense Knowledge Needed for Recognizing Textual Entailment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.028), (9, 0.017), (17, 0.05), (26, 0.031), (34, 0.232), (37, 0.062), (39, 0.039), (41, 0.048), (55, 0.027), (59, 0.066), (72, 0.033), (91, 0.064), (96, 0.182), (97, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83093387 <a title="174-lda-1" href="./acl-2011-Insights_from_Network_Structure_for_Text_Mining.html">174 acl-2011-Insights from Network Structure for Text Mining</a></p>
<p>Author: Zornitsa Kozareva ; Eduard Hovy</p><p>Abstract: Text mining and data harvesting algorithms have become popular in the computational linguistics community. They employ patterns that specify the kind of information to be harvested, and usually bootstrap either the pattern learning or the term harvesting process (or both) in a recursive cycle, using data learned in one step to generate more seeds for the next. They therefore treat the source text corpus as a network, in which words are the nodes and relations linking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications.</p><p>2 0.80889642 <a title="174-lda-2" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>Author: Ines Rehbein ; Josef Ruppenhofer</p><p>Abstract: Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised classification. While various simulation studies for a number of NLP tasks have shown that AL works well on goldstandard data, there is some doubt whether the approach can be successful when applied to noisy, real-world data sets. This paper presents a thorough evaluation of the impact of annotation noise on AL and shows that systematic noise resulting from biased coder decisions can seriously harm the AL process. We present a method to filter out inconsistent annotations during AL and show that this makes AL far more robust when ap- plied to noisy data.</p><p>3 0.78228641 <a title="174-lda-3" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>Author: Colin Cherry ; Shane Bergsma</p><p>Abstract: Graph-based dependency parsing can be sped up significantly if implausible arcs are eliminated from the search-space before parsing begins. State-of-the-art methods for arc filtering use separate classifiers to make pointwise decisions about the tree; they label tokens with roles such as root, leaf, or attaches-tothe-left, and then filter arcs accordingly. Because these classifiers overlap substantially in their filtering consequences, we propose to train them jointly, so that each classifier can focus on the gaps of the others. We integrate the various pointwise decisions as latent variables in a single arc-level SVM classifier. This novel framework allows us to combine nine pointwise filters, and adjust their sensitivity using a shared threshold based on arc length. Our system filters 32% more arcs than the independently-trained classifiers, without reducing filtering speed. This leads to faster parsing with no reduction in accuracy.</p><p>4 0.69225109 <a title="174-lda-4" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>Author: Raphael Hoffmann ; Congle Zhang ; Xiao Ling ; Luke Zettlemoyer ; Daniel S. Weld</p><p>Abstract: Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web’s natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multiinstance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint — for example they cannot extract the pair Founded ( Jobs Apple ) and CEO-o f ( Jobs Apple ) . , , This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extrac- , tion model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Freebase. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.</p><p>5 0.69190598 <a title="174-lda-5" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>Author: Ryan Gabbard ; Marjorie Freedman ; Ralph Weischedel</p><p>Abstract: As an alternative to requiring substantial supervised relation training data, many have explored bootstrapping relation extraction from a few seed examples. Most techniques assume that the examples are based on easily spotted anchors, e.g., names or dates. Sentences in a corpus which contain the anchors are then used to induce alternative ways of expressing the relation. We explore whether coreference can improve the learning process. That is, if the algorithm considered examples such as his sister, would accuracy be improved? With coreference, we see on average a 2-fold increase in F-Score. Despite using potentially errorful machine coreference, we see significant increase in recall on all relations. Precision increases in four cases and decreases in six.</p><p>6 0.68980348 <a title="174-lda-6" href="./acl-2011-Fine-Grained_Class_Label_Markup_of_Search_Queries.html">137 acl-2011-Fine-Grained Class Label Markup of Search Queries</a></p>
<p>7 0.68812168 <a title="174-lda-7" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>8 0.68760765 <a title="174-lda-8" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>9 0.68686301 <a title="174-lda-9" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>10 0.68645275 <a title="174-lda-10" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>11 0.68568802 <a title="174-lda-11" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>12 0.68494809 <a title="174-lda-12" href="./acl-2011-Putting_it_Simply%3A_a_Context-Aware_Approach_to_Lexical_Simplification.html">254 acl-2011-Putting it Simply: a Context-Aware Approach to Lexical Simplification</a></p>
<p>13 0.68469733 <a title="174-lda-13" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>14 0.68368709 <a title="174-lda-14" href="./acl-2011-Jointly_Learning_to_Extract_and_Compress.html">187 acl-2011-Jointly Learning to Extract and Compress</a></p>
<p>15 0.68359691 <a title="174-lda-15" href="./acl-2011-Learning_to_Win_by_Reading_Manuals_in_a_Monte-Carlo_Framework.html">207 acl-2011-Learning to Win by Reading Manuals in a Monte-Carlo Framework</a></p>
<p>16 0.68158078 <a title="174-lda-16" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>17 0.68131065 <a title="174-lda-17" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>18 0.68023372 <a title="174-lda-18" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>19 0.6799252 <a title="174-lda-19" href="./acl-2011-Nonlinear_Evidence_Fusion_and_Propagation_for_Hyponymy_Relation_Mining.html">231 acl-2011-Nonlinear Evidence Fusion and Propagation for Hyponymy Relation Mining</a></p>
<p>20 0.67948127 <a title="174-lda-20" href="./acl-2011-Entity_Set_Expansion_using_Topic_information.html">117 acl-2011-Entity Set Expansion using Topic information</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
