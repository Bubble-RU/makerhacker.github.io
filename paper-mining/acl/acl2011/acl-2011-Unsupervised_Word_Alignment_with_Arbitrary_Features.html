<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-325" href="#">acl2011-325</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</h1>
<br/><p>Source: <a title="acl-2011-325-pdf" href="http://aclweb.org/anthology//P/P11/P11-1042.pdf">pdf</a></p><p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>Reference: <a title="acl-2011-325-reference" href="../acl2011_reference/acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('align', 0.408), ('arbit', 0.213), ('rk', 0.179), ('transl', 0.173), ('discrimin', 0.168), ('brown', 0.154), ('mo', 0.152), ('tj', 0.146), ('overlap', 0.146), ('parallel', 0.135), ('tord', 0.122), ('hclark', 0.122), ('wof', 0.122), ('nasmi', 0.122), ('abandon', 0.122), ('impos', 0.113), ('cpd', 0.113), ('xexp', 0.113), ('ramet', 0.113), ('attract', 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="325-tfidf-1" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>2 0.34086531 <a title="325-tfidf-2" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>3 0.30040163 <a title="325-tfidf-3" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>4 0.28659409 <a title="325-tfidf-4" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>5 0.22909565 <a title="325-tfidf-5" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>Author: Emmanuel Prochasson ; Pascale Fung</p><p>Abstract: We present a first known result of high precision rare word bilingual extraction from comparable corpora, using aligned comparable documents and supervised classification. We incorporate two features, a context-vector similarity and a co-occurrence model between words in aligned documents in a machine learning approach. We test our hypothesis on different pairs of languages and corpora. We obtain very high F-Measure between 80% and 98% for recognizing and extracting correct translations for rare terms (from 1to 5 occurrences). Moreover, we show that our system can be trained on a pair of languages and test on a different pair of languages, obtaining a F-Measure of 77% for the classification of Chinese-English translations using a training corpus of Spanish-French. Our method is therefore even potentially applicable to low resources languages without training data.</p><p>6 0.22612332 <a title="325-tfidf-6" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>7 0.20123941 <a title="325-tfidf-7" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>8 0.19840704 <a title="325-tfidf-8" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>9 0.19836837 <a title="325-tfidf-9" href="./acl-2011-Simple_English_Wikipedia%3A_A_New_Text_Simplification_Task.html">283 acl-2011-Simple English Wikipedia: A New Text Simplification Task</a></p>
<p>10 0.16797556 <a title="325-tfidf-10" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>11 0.16245504 <a title="325-tfidf-11" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>12 0.16213925 <a title="325-tfidf-12" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>13 0.15749498 <a title="325-tfidf-13" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>14 0.1573834 <a title="325-tfidf-14" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>15 0.15441853 <a title="325-tfidf-15" href="./acl-2011-Effective_Use_of_Function_Words_for_Rule_Generalization_in_Forest-Based_Translation.html">110 acl-2011-Effective Use of Function Words for Rule Generalization in Forest-Based Translation</a></p>
<p>16 0.15120593 <a title="325-tfidf-16" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>17 0.15067478 <a title="325-tfidf-17" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>18 0.14857903 <a title="325-tfidf-18" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>19 0.14699478 <a title="325-tfidf-19" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>20 0.1452882 <a title="325-tfidf-20" href="./acl-2011-An_exponential_translation_model_for_target_language_morphology.html">44 acl-2011-An exponential translation model for target language morphology</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.298), (1, -0.193), (2, -0.18), (3, -0.155), (4, -0.02), (5, 0.029), (6, -0.075), (7, -0.061), (8, 0.141), (9, 0.229), (10, 0.077), (11, -0.093), (12, -0.031), (13, -0.051), (14, -0.053), (15, 0.026), (16, 0.048), (17, -0.008), (18, 0.017), (19, -0.031), (20, 0.018), (21, 0.03), (22, 0.021), (23, -0.025), (24, 0.018), (25, -0.027), (26, 0.014), (27, -0.028), (28, -0.022), (29, -0.026), (30, -0.031), (31, -0.043), (32, 0.045), (33, 0.016), (34, -0.015), (35, 0.02), (36, -0.047), (37, 0.011), (38, -0.006), (39, -0.034), (40, -0.016), (41, -0.032), (42, 0.058), (43, 0.022), (44, 0.034), (45, -0.025), (46, -0.006), (47, 0.054), (48, 0.028), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95577431 <a title="325-lsi-1" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>2 0.95027202 <a title="325-lsi-2" href="./acl-2011-Gappy_Phrasal_Alignment_By_Agreement.html">141 acl-2011-Gappy Phrasal Alignment By Agreement</a></p>
<p>Author: Mohit Bansal ; Chris Quirk ; Robert Moore</p><p>Abstract: We propose a principled and efficient phraseto-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semiMarkov model, word-to-phrase and phraseto-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include “gappy phrases” (such as French ne ? pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime.</p><p>3 0.94739026 <a title="325-lsi-3" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>4 0.93620336 <a title="325-lsi-4" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>Author: John DeNero ; Klaus Macherey</p><p>Abstract: Unsupervised word alignment is most often modeled as a Markov process that generates a sentence f conditioned on its translation e. A similar model generating e from f will make different alignment predictions. Statistical machine translation systems combine the predictions of two directional models, typically using heuristic combination procedures like grow-diag-final. This paper presents a graphical model that embeds two directional aligners into a single model. Inference can be performed via dual decomposition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations.</p><p>5 0.91712999 <a title="325-lsi-5" href="./acl-2011-Bayesian_Word_Alignment_for_Statistical_Machine_Translation.html">57 acl-2011-Bayesian Word Alignment for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Murat Saraclar</p><p>Abstract: In this work, we compare the translation performance of word alignments obtained via Bayesian inference to those obtained via expectation-maximization (EM). We propose a Gibbs sampler for fully Bayesian inference in IBM Model 1, integrating over all possible parameter values in finding the alignment distribution. We show that Bayesian inference outperforms EM in all of the tested language pairs, domains and data set sizes, by up to 2.99 BLEU points. We also show that the proposed method effectively addresses the well-known rare word problem in EM-estimated models; and at the same time induces a much smaller dictionary of bilingual word-pairs. .t r</p><p>6 0.88464284 <a title="325-lsi-6" href="./acl-2011-Reordering_Modeling_using_Weighted_Alignment_Matrices.html">265 acl-2011-Reordering Modeling using Weighted Alignment Matrices</a></p>
<p>7 0.85491991 <a title="325-lsi-7" href="./acl-2011-Optimal_and_Syntactically-Informed_Decoding_for_Monolingual_Phrase-Based_Alignment.html">235 acl-2011-Optimal and Syntactically-Informed Decoding for Monolingual Phrase-Based Alignment</a></p>
<p>8 0.84824228 <a title="325-lsi-8" href="./acl-2011-Dealing_with_Spurious_Ambiguity_in_Learning_ITG-based_Word_Alignment.html">93 acl-2011-Dealing with Spurious Ambiguity in Learning ITG-based Word Alignment</a></p>
<p>9 0.78941554 <a title="325-lsi-9" href="./acl-2011-An_Unsupervised_Model_for_Joint_Phrase_Alignment_and_Extraction.html">43 acl-2011-An Unsupervised Model for Joint Phrase Alignment and Extraction</a></p>
<p>10 0.78549689 <a title="325-lsi-10" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>11 0.76755607 <a title="325-lsi-11" href="./acl-2011-Word_Alignment_via_Submodular_Maximization_over_Matroids.html">340 acl-2011-Word Alignment via Submodular Maximization over Matroids</a></p>
<p>12 0.75881797 <a title="325-lsi-12" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>13 0.75076348 <a title="325-lsi-13" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>14 0.7371121 <a title="325-lsi-14" href="./acl-2011-Discriminative_Feature-Tied_Mixture_Modeling_for_Statistical_Machine_Translation.html">100 acl-2011-Discriminative Feature-Tied Mixture Modeling for Statistical Machine Translation</a></p>
<p>15 0.69680411 <a title="325-lsi-15" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>16 0.66207796 <a title="325-lsi-16" href="./acl-2011-Rare_Word_Translation_Extraction_from_Aligned_Comparable_Documents.html">259 acl-2011-Rare Word Translation Extraction from Aligned Comparable Documents</a></p>
<p>17 0.64909464 <a title="325-lsi-17" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>18 0.64450032 <a title="325-lsi-18" href="./acl-2011-Simple_English_Wikipedia%3A_A_New_Text_Simplification_Task.html">283 acl-2011-Simple English Wikipedia: A New Text Simplification Task</a></p>
<p>19 0.61879259 <a title="325-lsi-19" href="./acl-2011-ParaSense_or_How_to_Use_Parallel_Corpora_for_Word_Sense_Disambiguation.html">240 acl-2011-ParaSense or How to Use Parallel Corpora for Word Sense Disambiguation</a></p>
<p>20 0.59815758 <a title="325-lsi-20" href="./acl-2011-Corpus_Expansion_for_Statistical_Machine_Translation_with_Semantic_Role_Label_Substitution_Rules.html">87 acl-2011-Corpus Expansion for Statistical Machine Translation with Semantic Role Label Substitution Rules</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.076), (31, 0.074), (38, 0.192), (53, 0.087), (69, 0.067), (78, 0.193), (79, 0.077), (90, 0.079), (97, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94446677 <a title="325-lda-1" href="./acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers.html">56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</a></p>
<p>Author: Sujith Ravi ; Kevin Knight</p><p>Abstract: We introduce a novel Bayesian approach for deciphering complex substitution ciphers. Our method uses a decipherment model which combines information from letter n-gram language models as well as word dictionaries. Bayesian inference is performed on our model using an efficient sampling technique. We evaluate the quality of the Bayesian decipherment output on simple and homophonic letter substitution ciphers and show that unlike a previous approach, our method consistently produces almost 100% accurate decipherments. The new method can be applied on more complex substitution ciphers and we demonstrate its utility by cracking the famous Zodiac-408 cipher in a fully automated fashion, which has never been done before.</p><p>same-paper 2 0.84711605 <a title="325-lda-2" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>Author: Chris Dyer ; Jonathan H. Clark ; Alon Lavie ; Noah A. Smith</p><p>Abstract: We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, nonindependent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs.</p><p>3 0.842134 <a title="325-lda-3" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>Author: Bo Han ; Timothy Baldwin</p><p>Abstract: Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</p><p>4 0.8245939 <a title="325-lda-4" href="./acl-2011-PsychoSentiWordNet.html">253 acl-2011-PsychoSentiWordNet</a></p>
<p>Author: Amitava Das</p><p>Abstract: Sentiment analysis is one of the hot demanding research areas since last few decades. Although a formidable amount of research has been done but still the existing reported solutions or available systems are far from perfect or to meet the satisfaction level of end user's. The main issue may be there are many conceptual rules that govern sentiment, and there are even more clues (possibly unlimited) that can convey these concepts from realization to verbalization of a human being. Human psychology directly relates to the unrevealed clues; govern the sentiment realization of us. Human psychology relates many things like social psychology, culture, pragmatics and many more endless intelligent aspects of civilization. Proper incorporation of human psychology into computational sentiment knowledge representation may solve the problem. PsychoSentiWordNet is an extension over SentiWordNet that holds human psychological knowledge and sentiment knowledge simultaneously. 1</p><p>5 0.81011593 <a title="325-lda-5" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>Author: Colin Cherry ; Shane Bergsma</p><p>Abstract: Graph-based dependency parsing can be sped up significantly if implausible arcs are eliminated from the search-space before parsing begins. State-of-the-art methods for arc filtering use separate classifiers to make pointwise decisions about the tree; they label tokens with roles such as root, leaf, or attaches-tothe-left, and then filter arcs accordingly. Because these classifiers overlap substantially in their filtering consequences, we propose to train them jointly, so that each classifier can focus on the gaps of the others. We integrate the various pointwise decisions as latent variables in a single arc-level SVM classifier. This novel framework allows us to combine nine pointwise filters, and adjust their sensitivity using a shared threshold based on arc length. Our system filters 32% more arcs than the independently-trained classifiers, without reducing filtering speed. This leads to faster parsing with no reduction in accuracy.</p><p>6 0.80758375 <a title="325-lda-6" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>7 0.80641246 <a title="325-lda-7" href="./acl-2011-Deciphering_Foreign_Language.html">94 acl-2011-Deciphering Foreign Language</a></p>
<p>8 0.799263 <a title="325-lda-8" href="./acl-2011-Dr_Sentiment_Knows_Everything%21.html">105 acl-2011-Dr Sentiment Knows Everything!</a></p>
<p>9 0.79589593 <a title="325-lda-9" href="./acl-2011-Why_Initialization_Matters_for_IBM_Model_1%3A_Multiple_Optima_and_Non-Strict_Convexity.html">335 acl-2011-Why Initialization Matters for IBM Model 1: Multiple Optima and Non-Strict Convexity</a></p>
<p>10 0.79039705 <a title="325-lda-10" href="./acl-2011-Model-Based_Aligner_Combination_Using_Dual_Decomposition.html">221 acl-2011-Model-Based Aligner Combination Using Dual Decomposition</a></p>
<p>11 0.78737217 <a title="325-lda-11" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<p>12 0.78102112 <a title="325-lda-12" href="./acl-2011-A_Scalable_Probabilistic_Classifier_for_Language_Modeling.html">24 acl-2011-A Scalable Probabilistic Classifier for Language Modeling</a></p>
<p>13 0.77829903 <a title="325-lda-13" href="./acl-2011-Extracting_Social_Power_Relationships_from_Natural_Language.html">133 acl-2011-Extracting Social Power Relationships from Natural Language</a></p>
<p>14 0.77761364 <a title="325-lda-14" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>15 0.77750635 <a title="325-lda-15" href="./acl-2011-Predicting_Relative_Prominence_in_Noun-Noun_Compounds.html">249 acl-2011-Predicting Relative Prominence in Noun-Noun Compounds</a></p>
<p>16 0.77679259 <a title="325-lda-16" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>17 0.77597541 <a title="325-lda-17" href="./acl-2011-A_Hierarchical_Model_of_Web_Summaries.html">14 acl-2011-A Hierarchical Model of Web Summaries</a></p>
<p>18 0.77590203 <a title="325-lda-18" href="./acl-2011-Interactive_Topic_Modeling.html">178 acl-2011-Interactive Topic Modeling</a></p>
<p>19 0.77475792 <a title="325-lda-19" href="./acl-2011-Ordering_Prenominal_Modifiers_with_a_Reranking_Approach.html">237 acl-2011-Ordering Prenominal Modifiers with a Reranking Approach</a></p>
<p>20 0.77444535 <a title="325-lda-20" href="./acl-2011-Towards_Tracking_Semantic_Change_by_Visual_Analytics.html">307 acl-2011-Towards Tracking Semantic Change by Visual Analytics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
