<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>321 acl-2011-Unsupervised Discovery of Rhyme Schemes</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-321" href="#">acl2011-321</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>321 acl-2011-Unsupervised Discovery of Rhyme Schemes</h1>
<br/><p>Source: <a title="acl-2011-321-pdf" href="http://aclweb.org/anthology//P/P11/P11-2014.pdf">pdf</a></p><p>Author: Sravana Reddy ; Kevin Knight</p><p>Abstract: This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation.</p><p>Reference: <a title="acl-2011-321-reference" href="../acl2011_reference/acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation. [sent-3, score-1.64]
</p><p>2 1 Introduction Rhyming stanzas of poetry are characterized by rhyme schemes, patterns that specify how the lines in the stanza rhyme with one another. [sent-4, score-2.184]
</p><p>3 The question we raise in this paper is: can we infer the rhyme scheme of a stanza given no information about pronunciations or rhyming relations among words? [sent-5, score-1.512]
</p><p>4 Background  A rhyme scheme is represented as a  string corresponding to the sequence of lines that comprise the stanza, in which rhyming lines are denoted by the same letter. [sent-6, score-1.287]
</p><p>5 For example, the limerick’s rhyme scheme is aabba, indicating that the 1st, 2nd, and 5th lines rhyme, as do the the 3rd and 4th. [sent-7, score-0.873]
</p><p>6 ‘Culturomics ’ The field of digital humanities ‘isC growing, sw’i tThh a ffioecldus on isgtaittaislti chsu mtoa ntritaiceks cultural and literary trends (partially spurred by projects like the Google Books Ngrams1). [sent-9, score-0.128]
</p><p>7 com/ 77  Kevin Knight Information Sciences Institute University of Southern California Marina del Rey, CA 90292 knight @ i i s . [sent-12, score-0.041]
</p><p>8 edu  Rhyming corpora could be extremely useful for large-scale statistical analyses of poetic texts. [sent-13, score-0.055]
</p><p>9 •  Historical Linguistics/Study of Dialects Rhymes olf a word in poetry of a given time period or dialect region provide clues about its pronunciation in that time or dialect, a fact that is often taken advantage of by linguists (Wyld, 1923). [sent-14, score-0.474]
</p><p>10 One could automate this task given enough annotated data. [sent-15, score-0.023]
</p><p>11 An obvious approach to finding rhyme schemes is to use word pronunciations and a definition of rhyme, in which case the problem is fairly easy. [sent-16, score-0.972]
</p><p>12 However, we favor an unsupervised solution that utilizes no external knowledge for several reasons. [sent-17, score-0.055]
</p><p>13 The definition of rhyme varies across poetic tTrahdeit dioefnisn taiondn languages, arnieds may sin pcoluedtiec  slant rhymes like gate/mat, ‘sight rhymes’ like word/sword, assonance/consonance like shore/ alone, leaves/lance, etc. [sent-20, score-1.158]
</p><p>14 •  Pronunciations and spelling conventions change over ntisme. [sent-21, score-0.022]
</p><p>15 Words that rhymed historically may not anymore, like prove and love or proued and beloued. [sent-22, score-0.048]
</p><p>16 –  2  Related Work  There have been a number of recent papers on the automated annotation, analysis, or translation of poProceedings ofP tohretl 4an9tdh, O Anrneguoanl, M Jueentein 19g- o2f4 t,h 2e0 A1s1s. [sent-23, score-0.036]
</p><p>17 cc ia2t0io1n1 f Aors Cocoimatpiounta ftoiorn Caolm Lipnugtuaitsiotincasl:s Lhionrgtpuaisptiecrs , pages 77–82, etry. [sent-25, score-0.02]
</p><p>18 (2010) use a finite state transducer to infer the syllable-stress assignments in lines of poetry under metrical constraints. [sent-27, score-0.426]
</p><p>19 (2010) incorporate constraints on meter and rhyme (where the stress and rhyming information is derived from a pronunciation dictionary) into a machine translation system. [sent-29, score-1.276]
</p><p>20 Jiang and Zhou (2008) develop a system to generate the second line of a Chinese couplet given the first. [sent-30, score-0.048]
</p><p>21 A few researchers have also explored the problem of poetry generation under some  constraints (Manurung et al. [sent-31, score-0.276]
</p><p>22 There has also been some work on computational approaches to characterizing rhymes (Byrd and Chodorow, 1985) and global properties of the rhyme network (Sonderegger, 2011) in English. [sent-35, score-1.067]
</p><p>23 To the best ofour knowledge, there has been no language-independent computational work on finding rhyme schemes. [sent-36, score-0.804]
</p><p>24 3 Finding Stanza Rhyme Schemes A collection of rhyming poetry inevitably contains repetition of rhyming pairs. [sent-37, score-0.976]
</p><p>25 For example, the word trees will often rhyme with breeze across different stanzas, even those with different rhyme schemes and written by different authors. [sent-38, score-1.65]
</p><p>26 This is partly due to sparsity of rhymes many words that have no rhymes at all, and many others have only a handful, forcing poets to reuse rhyming pairs. [sent-39, score-0.96]
</p><p>27 In this section, we describe an unsupervised algorithm to infer rhyme schemes that harnesses this repetition, based on a model of stanza generation. [sent-40, score-1.2]
</p><p>28 Pick a rhyme scheme r of length n with probability P(r). [sent-43, score-0.804]
</p><p>29 For each i ∈ [1, n] , pick a word sequence, choosing t hie ∈last [21 ,wno],rd p xi as afo wlloorwds :s (a) If, according to r, the line does not rhyme with any previous line in the stanza, pick a word xi from a vocabulary of line-end words with probability P(xi). [sent-45, score-1.074]
</p><p>30 (b) If the ith line rhymes with some previous line(s) j according to r, choose a word xi that  ith  2A rhyme may span more than one word in a line – for example, laureate. [sent-46, score-1.248]
</p><p>31 An extension of our model could include a latent variable that selects the entire rhyming portion of a line. [sent-53, score-0.324]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rhyme', 0.76), ('rhyming', 0.324), ('rhymes', 0.288), ('poetry', 0.253), ('stanza', 0.252), ('schemes', 0.098), ('pronunciations', 0.075), ('meter', 0.072), ('stanzas', 0.072), ('lines', 0.069), ('pronunciation', 0.06), ('poetic', 0.055), ('dialect', 0.055), ('repetition', 0.05), ('line', 0.048), ('pick', 0.047), ('xi', 0.046), ('scheme', 0.044), ('chicago', 0.038), ('infer', 0.035), ('uren', 0.032), ('olf', 0.032), ('cago', 0.032), ('harnesses', 0.032), ('afo', 0.032), ('breeze', 0.032), ('isc', 0.032), ('metrical', 0.032), ('mtoa', 0.032), ('rwe', 0.032), ('growing', 0.031), ('ramakrishnan', 0.029), ('byrd', 0.029), ('nin', 0.029), ('byron', 0.029), ('ith', 0.029), ('anymore', 0.027), ('greene', 0.027), ('sight', 0.027), ('reddy', 0.027), ('historically', 0.026), ('qj', 0.026), ('tthh', 0.026), ('tory', 0.026), ('inevitably', 0.025), ('dialects', 0.024), ('knight', 0.024), ('constraints', 0.023), ('humanities', 0.023), ('automate', 0.023), ('forcing', 0.023), ('unsupervised', 0.023), ('sin', 0.022), ('raise', 0.022), ('ofour', 0.022), ('finding', 0.022), ('conventions', 0.022), ('chodorow', 0.022), ('handful', 0.022), ('reuse', 0.022), ('love', 0.022), ('historical', 0.021), ('transducer', 0.021), ('southern', 0.021), ('aors', 0.021), ('lhionrgtpuaisptiecrs', 0.021), ('cultural', 0.021), ('comprise', 0.021), ('stress', 0.021), ('marina', 0.021), ('region', 0.02), ('linguists', 0.02), ('ye', 0.02), ('anrneguoanl', 0.02), ('caolm', 0.02), ('cocoimatpiounta', 0.02), ('ftoiorn', 0.02), ('jueentein', 0.02), ('lipnugtuaitsiotincasl', 0.02), ('tohretl', 0.02), ('literary', 0.019), ('tnh', 0.019), ('rey', 0.019), ('characterizing', 0.019), ('period', 0.018), ('books', 0.018), ('characterized', 0.018), ('sw', 0.017), ('definition', 0.017), ('del', 0.017), ('digital', 0.017), ('utilizes', 0.017), ('il', 0.016), ('varies', 0.016), ('trends', 0.016), ('translation', 0.016), ('clues', 0.016), ('assignments', 0.016), ('favor', 0.015), ('partly', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="321-tfidf-1" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>Author: Sravana Reddy ; Kevin Knight</p><p>Abstract: This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation.</p><p>2 0.032510854 <a title="321-tfidf-2" href="./acl-2011-The_Arabic_Online_Commentary_Dataset%3A_an_Annotated_Dataset_of_Informal_Arabic_with_High_Dialectal_Content.html">299 acl-2011-The Arabic Online Commentary Dataset: an Annotated Dataset of Informal Arabic with High Dialectal Content</a></p>
<p>Author: Omar F. Zaidan ; Chris Callison-Burch</p><p>Abstract: The written form of Arabic, Modern Standard Arabic (MSA), differs quite a bit from the spoken dialects of Arabic, which are the true “native” languages of Arabic speakers used in daily life. However, due to MSA’s prevalence in written form, almost all Arabic datasets have predominantly MSA content. We present the Arabic Online Commentary Dataset, a 52M-word monolingual dataset rich in dialectal content, and we describe our long-term annotation effort to identify the dialect level (and dialect itself) in each sentence of the dataset. So far, we have labeled 108K sentences, 41% of which as having dialectal content. We also present experimental results on the task of automatic dialect identification, using the collected labels for training and evaluation.</p><p>3 0.022851035 <a title="321-tfidf-3" href="./acl-2011-Contrasting_Multi-Lingual_Prosodic_Cues_to_Predict_Verbal_Feedback_for_Rapport.html">83 acl-2011-Contrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback for Rapport</a></p>
<p>Author: Siwei Wang ; Gina-Anne Levow</p><p>Abstract: Verbal feedback is an important information source in establishing interactional rapport. However, predicting verbal feedback across languages is challenging due to languagespecific differences, inter-speaker variation, and the relative sparseness and optionality of verbal feedback. In this paper, we employ an approach combining classifier weighting and SMOTE algorithm oversampling to improve verbal feedback prediction in Arabic, English, and Spanish dyadic conversations. This approach improves the prediction of verbal feedback, up to 6-fold, while maintaining a high overall accuracy. Analyzing highly weighted features highlights widespread use of pitch, with more varied use of intensity and duration.</p><p>4 0.022283088 <a title="321-tfidf-4" href="./acl-2011-How_Much_Can_We_Gain_from_Supervised_Word_Alignment%3F.html">152 acl-2011-How Much Can We Gain from Supervised Word Alignment?</a></p>
<p>Author: Jinxi Xu ; Jinying Chen</p><p>Abstract: Word alignment is a central problem in statistical machine translation (SMT). In recent years, supervised alignment algorithms, which improve alignment accuracy by mimicking human alignment, have attracted a great deal of attention. The objective of this work is to explore the performance limit of supervised alignment under the current SMT paradigm. Our experiments used a manually aligned ChineseEnglish corpus with 280K words recently released by the Linguistic Data Consortium (LDC). We treated the human alignment as the oracle of supervised alignment. The result is surprising: the gain of human alignment over a state of the art unsupervised method (GIZA++) is less than 1point in BLEU. Furthermore, we showed the benefit of improved alignment becomes smaller with more training data, implying the above limit also holds for large training conditions. 1</p><p>5 0.02217856 <a title="321-tfidf-5" href="./acl-2011-Classifying_arguments_by_scheme.html">68 acl-2011-Classifying arguments by scheme</a></p>
<p>Author: Vanessa Wei Feng ; Graeme Hirst</p><p>Abstract: Argumentation schemes are structures or templates for various kinds of arguments. Given the text of an argument with premises and conclusion identified, we classify it as an instance ofone offive common schemes, using features specific to each scheme. We achieve accuracies of 63–91% in one-against-others classification and 80–94% in pairwise classification (baseline = 50% in both cases).</p><p>6 0.018190654 <a title="321-tfidf-6" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>7 0.016745185 <a title="321-tfidf-7" href="./acl-2011-Latent_Class_Transliteration_based_on_Source_Language_Origin.html">197 acl-2011-Latent Class Transliteration based on Source Language Origin</a></p>
<p>8 0.016726743 <a title="321-tfidf-8" href="./acl-2011-Unsupervised_Word_Alignment_with_Arbitrary_Features.html">325 acl-2011-Unsupervised Word Alignment with Arbitrary Features</a></p>
<p>9 0.016481174 <a title="321-tfidf-9" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>10 0.015930071 <a title="321-tfidf-10" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>11 0.015916059 <a title="321-tfidf-11" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>12 0.015635222 <a title="321-tfidf-12" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>13 0.015046688 <a title="321-tfidf-13" href="./acl-2011-Learning_Word_Vectors_for_Sentiment_Analysis.html">204 acl-2011-Learning Word Vectors for Sentiment Analysis</a></p>
<p>14 0.014805673 <a title="321-tfidf-14" href="./acl-2011-How_do_you_pronounce_your_name%3F_Improving_G2P_with_transliterations.html">153 acl-2011-How do you pronounce your name? Improving G2P with transliterations</a></p>
<p>15 0.014036186 <a title="321-tfidf-15" href="./acl-2011-Extracting_Paraphrases_from_Definition_Sentences_on_the_Web.html">132 acl-2011-Extracting Paraphrases from Definition Sentences on the Web</a></p>
<p>16 0.013845243 <a title="321-tfidf-16" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>17 0.013603355 <a title="321-tfidf-17" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>18 0.013512611 <a title="321-tfidf-18" href="./acl-2011-Phrase-Based_Translation_Model_for_Question_Retrieval_in_Community_Question_Answer_Archives.html">245 acl-2011-Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives</a></p>
<p>19 0.012799337 <a title="321-tfidf-19" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>20 0.012698709 <a title="321-tfidf-20" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.039), (1, -0.003), (2, 0.002), (3, 0.01), (4, -0.006), (5, 0.0), (6, 0.014), (7, -0.011), (8, -0.01), (9, 0.011), (10, -0.002), (11, 0.006), (12, 0.001), (13, 0.03), (14, 0.004), (15, -0.006), (16, -0.023), (17, 0.004), (18, -0.002), (19, 0.021), (20, 0.001), (21, 0.018), (22, 0.005), (23, 0.029), (24, -0.01), (25, -0.003), (26, 0.011), (27, -0.019), (28, 0.001), (29, -0.017), (30, -0.011), (31, 0.003), (32, 0.034), (33, -0.004), (34, 0.029), (35, -0.018), (36, -0.022), (37, -0.017), (38, 0.008), (39, -0.027), (40, 0.012), (41, 0.003), (42, -0.009), (43, 0.029), (44, -0.013), (45, -0.013), (46, -0.022), (47, 0.014), (48, -0.011), (49, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80123967 <a title="321-lsi-1" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>Author: Sravana Reddy ; Kevin Knight</p><p>Abstract: This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation.</p><p>2 0.47001269 <a title="321-lsi-2" href="./acl-2011-Hierarchical_Reinforcement_Learning_and_Hidden_Markov_Models_for_Task-Oriented_Natural_Language_Generation.html">149 acl-2011-Hierarchical Reinforcement Learning and Hidden Markov Models for Task-Oriented Natural Language Generation</a></p>
<p>Author: Nina Dethlefs ; Heriberto Cuayahuitl</p><p>Abstract: Surface realisation decisions in language generation can be sensitive to a language model, but also to decisions of content selection. We therefore propose the joint optimisation of content selection and surface realisation using Hierarchical Reinforcement Learning (HRL). To this end, we suggest a novel reward function that is induced from human data and is especially suited for surface realisation. It is based on a generation space in the form of a Hidden Markov Model (HMM). Results in terms of task success and human-likeness suggest that our unified approach performs better than greedy or random baselines.</p><p>3 0.44687182 <a title="321-lsi-3" href="./acl-2011-Underspecifying_and_Predicting_Voice_for_Surface_Realisation_Ranking.html">317 acl-2011-Underspecifying and Predicting Voice for Surface Realisation Ranking</a></p>
<p>Author: Sina Zarriess ; Aoife Cahill ; Jonas Kuhn</p><p>Abstract: This paper addresses a data-driven surface realisation model based on a large-scale reversible grammar of German. We investigate the relationship between the surface realisation performance and the character of the input to generation, i.e. its degree of underspecification. We extend a syntactic surface realisation system, which can be trained to choose among word order variants, such that the candidate set includes active and passive variants. This allows us to study the interaction of voice and word order alternations in realistic German corpus data. We show that with an appropriately underspecified input, a linguistically informed realisation model trained to regenerate strings from the underlying semantic representation achieves 91.5% accuracy (over a baseline of 82.5%) in the prediction of the original voice. 1</p><p>4 0.44317922 <a title="321-lsi-4" href="./acl-2011-Automatic_Headline_Generation_using_Character_Cross-Correlation.html">51 acl-2011-Automatic Headline Generation using Character Cross-Correlation</a></p>
<p>Author: Fahad Alotaiby</p><p>Abstract: Arabic language is a morphologically complex language. Affixes and clitics are regularly attached to stems which make direct comparison between words not practical. In this paper we propose a new automatic headline generation technique that utilizes character cross-correlation to extract best headlines and to overcome the Arabic language complex morphology. The system that uses character cross-correlation achieves ROUGE-L score of 0. 19384 while the exact word matching scores only 0. 17252 for the same set of documents. 1</p><p>5 0.44271389 <a title="321-lsi-5" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>Author: Kenneth Hild ; Umut Orhan ; Deniz Erdogmus ; Brian Roark ; Barry Oken ; Shalini Purwar ; Hooman Nezamfar ; Melanie Fried-Oken</p><p>Abstract: Event related potentials (ERP) corresponding to stimuli in electroencephalography (EEG) can be used to detect the intent of a person for brain computer interfaces (BCI). This paradigm is widely used to build letter-byletter text input systems using BCI. Nevertheless using a BCI-typewriter depending only on EEG responses will not be sufficiently accurate for single-trial operation in general, and existing systems utilize many-trial schemes to achieve accuracy at the cost of speed. Hence incorporation of a language model based prior or additional evidence is vital to improve accuracy and speed. In this demonstration we will present a BCI system for typing that integrates a stochastic language model with ERP classification to achieve speedups, via the rapid serial visual presentation (RSVP) paradigm.</p><p>6 0.43257695 <a title="321-lsi-6" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<p>7 0.42904869 <a title="321-lsi-7" href="./acl-2011-A_Large_Scale_Distributed_Syntactic%2C_Semantic_and_Lexical_Language_Model_for_Machine_Translation.html">17 acl-2011-A Large Scale Distributed Syntactic, Semantic and Lexical Language Model for Machine Translation</a></p>
<p>8 0.41411722 <a title="321-lsi-8" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<p>9 0.40682384 <a title="321-lsi-9" href="./acl-2011-A_Hierarchical_Pitman-Yor_Process_HMM_for_Unsupervised_Part_of_Speech_Induction.html">15 acl-2011-A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction</a></p>
<p>10 0.38721642 <a title="321-lsi-10" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>11 0.38423276 <a title="321-lsi-11" href="./acl-2011-Enhancing_Language_Models_in_Statistical_Machine_Translation_with_Backward_N-grams_and_Mutual_Information_Triggers.html">116 acl-2011-Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers</a></p>
<p>12 0.3829214 <a title="321-lsi-12" href="./acl-2011-An_Empirical_Investigation_of_Discounting_in_Cross-Domain_Language_Models.html">38 acl-2011-An Empirical Investigation of Discounting in Cross-Domain Language Models</a></p>
<p>13 0.37762469 <a title="321-lsi-13" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>14 0.37076581 <a title="321-lsi-14" href="./acl-2011-Contrasting_Multi-Lingual_Prosodic_Cues_to_Predict_Verbal_Feedback_for_Rapport.html">83 acl-2011-Contrasting Multi-Lingual Prosodic Cues to Predict Verbal Feedback for Rapport</a></p>
<p>15 0.36819565 <a title="321-lsi-15" href="./acl-2011-Generalized_Interpolation_in_Decision_Tree_LM.html">142 acl-2011-Generalized Interpolation in Decision Tree LM</a></p>
<p>16 0.36735833 <a title="321-lsi-16" href="./acl-2011-Classifying_arguments_by_scheme.html">68 acl-2011-Classifying arguments by scheme</a></p>
<p>17 0.36469468 <a title="321-lsi-17" href="./acl-2011-Models_and_Training_for_Unsupervised_Preposition_Sense_Disambiguation.html">224 acl-2011-Models and Training for Unsupervised Preposition Sense Disambiguation</a></p>
<p>18 0.35907322 <a title="321-lsi-18" href="./acl-2011-The_Arabic_Online_Commentary_Dataset%3A_an_Annotated_Dataset_of_Informal_Arabic_with_High_Dialectal_Content.html">299 acl-2011-The Arabic Online Commentary Dataset: an Annotated Dataset of Informal Arabic with High Dialectal Content</a></p>
<p>19 0.35372382 <a title="321-lsi-19" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>20 0.34898803 <a title="321-lsi-20" href="./acl-2011-Using_Deep_Morphology_to_Improve_Automatic_Error_Detection_in_Arabic_Handwriting_Recognition.html">329 acl-2011-Using Deep Morphology to Improve Automatic Error Detection in Arabic Handwriting Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.48), (5, 0.017), (17, 0.017), (26, 0.013), (37, 0.025), (39, 0.032), (41, 0.036), (55, 0.025), (59, 0.03), (72, 0.053), (75, 0.015), (91, 0.033), (96, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74415189 <a title="321-lda-1" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>Author: Sravana Reddy ; Kevin Knight</p><p>Abstract: This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation.</p><p>2 0.72578806 <a title="321-lda-2" href="./acl-2011-Extracting_and_Classifying_Urdu_Multiword_Expressions.html">134 acl-2011-Extracting and Classifying Urdu Multiword Expressions</a></p>
<p>Author: Annette Hautli ; Sebastian Sulger</p><p>Abstract: This paper describes a method for automatically extracting and classifying multiword expressions (MWEs) for Urdu on the basis of a relatively small unannotated corpus (around 8.12 million tokens). The MWEs are extracted by an unsupervised method and classified into two distinct classes, namely locations and person names. The classification is based on simple heuristics that take the co-occurrence of MWEs with distinct postpositions into account. The resulting classes are evaluated against a hand-annotated gold standard and achieve an f-score of 0.5 and 0.746 for locations and persons, respectively. A target application is the Urdu ParGram grammar, where MWEs are needed to generate a more precise syntactic and semantic analysis.</p><p>3 0.63642794 <a title="321-lda-3" href="./acl-2011-Lost_in_Translation%3A_Authorship_Attribution_using_Frame_Semantics.html">214 acl-2011-Lost in Translation: Authorship Attribution using Frame Semantics</a></p>
<p>Author: Steffen Hedegaard ; Jakob Grue Simonsen</p><p>Abstract: We investigate authorship attribution using classifiers based on frame semantics. The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts. Our results suggest (i) that frame-based classifiers are usable for author attribution of both translated and untranslated texts; (ii) that framebased classifiers generally perform worse than the baseline classifiers for untranslated texts, but (iii) perform as well as, or superior to the baseline classifiers on translated texts; (iv) that—contrary to current belief—naïve clas- sifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.</p><p>4 0.53726172 <a title="321-lda-4" href="./acl-2011-Predicting_Relative_Prominence_in_Noun-Noun_Compounds.html">249 acl-2011-Predicting Relative Prominence in Noun-Noun Compounds</a></p>
<p>Author: Taniya Mishra ; Srinivas Bangalore</p><p>Abstract: There are several theories regarding what influences prominence assignment in English noun-noun compounds. We have developed corpus-driven models for automatically predicting prominence assignment in noun-noun compounds using feature sets based on two such theories: the informativeness theory and the semantic composition theory. The evaluation of the prediction models indicate that though both of these theories are relevant, they account for different types of variability in prominence assignment.</p><p>5 0.51472133 <a title="321-lda-5" href="./acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging.html">112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.</p><p>6 0.51051128 <a title="321-lda-6" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>7 0.31325716 <a title="321-lda-7" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>8 0.25808236 <a title="321-lda-8" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>9 0.25793493 <a title="321-lda-9" href="./acl-2011-Local_Histograms_of_Character_N-grams_for_Authorship_Attribution.html">212 acl-2011-Local Histograms of Character N-grams for Authorship Attribution</a></p>
<p>10 0.24559724 <a title="321-lda-10" href="./acl-2011-Unsupervised_Decomposition_of_a_Document_into_Authorial_Components.html">319 acl-2011-Unsupervised Decomposition of a Document into Authorial Components</a></p>
<p>11 0.24528858 <a title="321-lda-11" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>12 0.24355757 <a title="321-lda-12" href="./acl-2011-Towards_Style_Transformation_from_Written-Style_to_Audio-Style.html">306 acl-2011-Towards Style Transformation from Written-Style to Audio-Style</a></p>
<p>13 0.243478 <a title="321-lda-13" href="./acl-2011-Language_of_Vandalism%3A_Improving_Wikipedia_Vandalism_Detection_via_Stylometric_Analysis.html">195 acl-2011-Language of Vandalism: Improving Wikipedia Vandalism Detection via Stylometric Analysis</a></p>
<p>14 0.2424165 <a title="321-lda-14" href="./acl-2011-Prototyping_virtual_instructors_from_human-human_corpora.html">252 acl-2011-Prototyping virtual instructors from human-human corpora</a></p>
<p>15 0.24017502 <a title="321-lda-15" href="./acl-2011-Data-oriented_Monologue-to-Dialogue_Generation.html">91 acl-2011-Data-oriented Monologue-to-Dialogue Generation</a></p>
<p>16 0.23943257 <a title="321-lda-16" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>17 0.23927064 <a title="321-lda-17" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>18 0.23731753 <a title="321-lda-18" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>19 0.23696813 <a title="321-lda-19" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>20 0.23635876 <a title="321-lda-20" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
