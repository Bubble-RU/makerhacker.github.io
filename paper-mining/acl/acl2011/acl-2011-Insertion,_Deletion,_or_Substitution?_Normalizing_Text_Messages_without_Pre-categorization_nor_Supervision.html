<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-172" href="#">acl2011-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</h1>
<br/><p>Source: <a title="acl-2011-172-pdf" href="http://aclweb.org/anthology//P/P11/P11-2013.pdf">pdf</a></p><p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>Reference: <a title="acl-2011-172-reference" href="../acl2011_reference/acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ful i ang ewielnigu@ u,s y abnog gsl lc}h@ Abstract Most text message normalization approaches are based on supervised learning and rely on human labeled training data. [sent-6, score-0.219]
</p><p>2 In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. [sent-7, score-0.534]
</p><p>3 In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. [sent-8, score-0.535]
</p><p>4 Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. [sent-9, score-1.471]
</p><p>5 To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level  . [sent-10, score-0.274]
</p><p>6 Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21. [sent-12, score-0.566]
</p><p>7 16% over jazzy spell checker on the two test sets respectively). [sent-14, score-0.381]
</p><p>8 1 Introduction Recent years have witnessed the explosive growth of text message usage, including the mobile phone text messages (SMS), chat logs, emails, and status updates from the social network websites such as Twitter and Facebook. [sent-15, score-0.27]
</p><p>9 These text message collections serve as valuable information sources, yet the nonstandard contents within them often degrade  hclotm. [sent-16, score-0.662]
</p><p>10 the existing language processing systems, calling the need of text normalization before applying the traditional information extraction, retrieval, sentiment analysis (Celikyilmaz et al. [sent-21, score-0.113]
</p><p>11 Text message normalization is also of crucial importance for building text-tospeech (TTS) systems, which need to determine pronunciation for nonstandard words. [sent-23, score-0.757]
</p><p>12 Text message normalization aims to replace the non-standard tokens that carry significant meanings with the context-appropriate standard English words. [sent-24, score-0.344]
</p><p>13 This is a very challenging task due to the vast amount and wide variety of existing nonstandard tokens. [sent-25, score-0.534]
</p><p>14 We found more than 4 million distinct out-of-vocabulary tokens in the English tweets of the Edinburgh Twitter corpus (see Section 2. [sent-26, score-0.209]
</p><p>15 Table 1 shows examples of nonstandard tokens originated from the word “together”. [sent-28, score-0.704]
</p><p>16 We can see that some variants can be generated by dropping letters from the original word (“tgthr”) or substituting letters with digit (“2gether”); however, many variants are generated by combining the letter insertion, deletion, and substitution operations (“toqethaa”, “2gthr”). [sent-29, score-0.982]
</p><p>17 This shows that it is difficult to  divide the nonstandard tokens into exclusive categories. [sent-30, score-0.681]
</p><p>18 Among the literature of text normalization 71  Proceedings ofP tohretl 4an9tdh, O Anrneguoanl, M Jueentein 19g- o2f4 t,h 2e0 A1s1s. [sent-31, score-0.113]
</p><p>19 cc ia2t0io1n1 f Aors Cocoimatpiounta ftoiorn Caolm Lipnugtuaitsiotincasl:s Lhionrgtpuaisptiecrs , pages 71–76, (for text messages or other domains), Sproat et al. [sent-33, score-0.101]
</p><p>20 (2001), Cook and Stevenson (2009) employed the noisy channel model to find the most probable word sequence given the observed noisy message. [sent-34, score-0.219]
</p><p>21 Their approaches first classified the nonstandard tokens into various categories (e. [sent-35, score-0.681]
</p><p>22 , abbreviation, stylistic variation, prefix-clipping), then calculated the posterior probability of the nonstandard tokens based on each category. [sent-37, score-0.681]
</p><p>23 (2009), Pennell and Liu (2010) focused on modeling word abbreviations formed by dropping characters from the original word. [sent-41, score-0.097]
</p><p>24 Toutanova and Moore (2002) addressed the phonetic substitution problem by extending the initial letter-to-phone model. [sent-42, score-0.067]
</p><p>25 (2008) viewed the text message normalization as a statistical machine translation process from the texting language to standard English. [sent-45, score-0.26]
</p><p>26 In this paper, we propose a general letter transformation approach that normalizes nonstandard tokens without categorizing them. [sent-49, score-1.254]
</p><p>27 A large set of noisy training word pairs were automatically collected via a novel web-based approach and aligned at the character level for model training. [sent-50, score-0.283]
</p><p>28 Results show that our system significantly outperformed the jazzy spell checker and the state-of-the-art deletion-based abbreviation system, and also demonstrated good cross-domain portability. [sent-52, score-0.487]
</p><p>29 1 General Framework Given a noisy text message T, our goal is to normalize it into a standard English word sequence S. [sent-54, score-0.258]
</p><p>30 Under the noisy channel model, this is equivalent to finding the sequence Sˆ that maximizes p(S|T) : Sˆ = argmaxSp(S|T) = argmaxS(Qip(Ti|Si))p(S) where  we assume  that each non-standard token Ti  is dependent on only one English word Si, that is, we are not considering acronyms (e. [sent-56, score-0.232]
</p><p>31 We formulate the process of generating a nonstandard token Ti from dictionary word Si using a letter transformation model, and use the model confidence as the probability p(Ti |Si). [sent-60, score-1.261]
</p><p>32 To form a nonstandard token, each letter in the dictionary word can be labeled with: (a) one of the 0-9 digits; (b) one of the 26 characters including itself; (c) the null character “-”; (d) a letter combination. [sent-62, score-1.542]
</p><p>33 This transformation process from dictionary words to nonstandard tokens will be learned automatically through a sequence labeling framework that integrates character-, phonetic-, and syllable-level information. [sent-63, score-1.049]
</p><p>34 In general, the letter transformation approach will handle the nonstandard tokens listed in Table 2 yet without explicitly categorizing them. [sent-64, score-1.254]
</p><p>35 Note for the tokens with letter repetition, we first generate a set of variants by varying the repetitive letters (e. [sent-65, score-0.772]
</p><p>36 But in this study we are treating each letter in the English word separately and not considering the phrase-level transformation. [sent-68, score-0.413]
</p><p>37 (5634217) gatlpsyerhbnatolepirgshctoevimacpvtbhsieauorncbt aiewrno/- rfow(1/)tdi(g6)tpl4bhu0egivtmoare,gvthwsineuam,rb2hkctm3oniaergd,3tyhcoe5su,krtmioel0pdur,ntioq  Table 2: Nonstandard tokens that can be processed by the unified letter transformation approach. [sent-69, score-0.682]
</p><p>38 2 Web based Data Collection w/o Supervision We propose to automatically collect training data (annotate nonstandard words with the corresponding English forms) using a web-based approach, therefore avoiding the expensive human annotation. [sent-71, score-0.568]
</p><p>39 The English tweets were extracted using the TextCat language identification toolkit (Cavnar and Trenkle, 1994), and tokenized into a sequence of clean tokens consisting of letters, digits, and apostrophe. [sent-74, score-0.276]
</p><p>40 For the out-of-vocabulary (OOV) tokens consisting of letters and apostrophe, we form n Google queries for each of them in the form of either “w1 w2 w3” OOV or OOV “w1 w2 w3” , where w1 to w3 are consecutive context words extracted from the tweets that contain this OOV. [sent-75, score-0.391]
</p><p>41 The first 32 returned snippets for each query are parsed and the words in boldface that are different from both the OOV and the context words are collected as candidate normalized words. [sent-77, score-0.122]
</p><p>42 Among them, we further select the words that have longer common character sequence with the OOV than with the context words, and pair each of them with the OOV to form the training pairs. [sent-78, score-0.096]
</p><p>43 For the OOV tokens consisting of both letters and digits, we use simple rules to recover possible original words. [sent-79, score-0.329]
</p><p>44 In addition, we add 932 word pairs of chat slangs and their normalized word forms collected from InternetSlang. [sent-82, score-0.272]
</p><p>45 These noisy training pairs were further expanded 73  and purged. [sent-84, score-0.108]
</p><p>46 We remove the data pairs whose word candidate is not in the CMU dictionary. [sent-87, score-0.101]
</p><p>47 We also remove the pairs whose word candidate and OOV are simply inflections of each other, e. [sent-88, score-0.101]
</p><p>48 In total, this procedure generated 62,907 training word pairs including 20,880 unique candidate words and 46,356 unique OOVs. [sent-91, score-0.135]
</p><p>49 3 Automatic Letter-level Alignment Given a training pair (Si, Ti) consisting of a word Si and its nonstandard variant Ti, we propose a procedure to align each letter in Si with zero, one, or more letters/digits in Ti. [sent-93, score-1.06]
</p><p>50 First we align the letters of the  longest common sequence between the dictionary word and the variant (which gives letter-to-letter correspondence in those common subsequences). [sent-94, score-0.457]
</p><p>51 Then for the letter chunks in between each of the obtained alignments, we process them based on the following three cases: (a) (many-to-0): a chunk in the dictionary word needs to be aligned to zero letters in the variant. [sent-95, score-0.901]
</p><p>52 In this case, we map each letter in the chunk to “-” (e. [sent-96, score-0.525]
</p><p>53 (b) (0-to-many): zero letters in the dictionary word need to be aligned to a letter/digit chunk in the variant. [sent-99, score-0.489]
</p><p>54 In this case, if the first letter in the chunk can be combined with the previous letter to form a digraph (such as “wh” when aligning “sandwich” to “sandwhich”), we combine these two letters. [sent-100, score-0.941]
</p><p>55 The remaining letters, or the entire chunk when the first letter does not form a digraph with the previous letter, are put together with the following aligned letter in the variant. [sent-101, score-0.987]
</p><p>56 (c) (many-to-many): non-zero letters in the dictionary word need to be aligned to a chunk in the variant. [sent-102, score-0.489]
</p><p>57 Similar to (b), the first letter in the vari-  ant chunk is merged with the previous alignment if they form a digraph. [sent-103, score-0.543]
</p><p>58 Then we map the chunk in the dictionary word to the chunk in the variant as one alignment, e. [sent-104, score-0.461]
</p><p>59 2Please contact the first author for the collected word pairs. [sent-107, score-0.074]
</p><p>60 To eliminate possible noisy training pairs, such as (“you”, “haveu”), we keep all data pairs containing digits, but remove the data pairs with chunks involving three letters or more in either the dictionary word or the variant. [sent-109, score-0.501]
</p><p>61 For the chunk alignments in the remaining pairs, we sequentially align the letters (e. [sent-110, score-0.354]
</p><p>62 Note that for those 1-to-2 alignments, we align the single letter in the dictionary word to a two-letter combination in the variant. [sent-113, score-0.611]
</p><p>63 We limit to the top 5 most frequent letter combinations, which are “ck”, “ey”, “ie”, “ou”, “wh”, and the pairs involving other combinations are removed. [sent-114, score-0.432]
</p><p>64 After applying the letter alignment to the collected noisy training word pairs, we obtained 298,160 letter-level alignments. [sent-115, score-0.569]
</p><p>65 Some example  alignments and corresponding word pairs are: e → ’ ’ (have, hav) q → k (iraq, irak) e → a (another, anotha) q → g (iraq, irag) e → 3 a (online, 0nlin3) w → w→h (watch, whatch) 2. [sent-116, score-0.101]
</p><p>66 4 Sequence Labeling Model for P(Ti |Si) For a letter sequence Si, we use the conditional random fields (CRF) model to perform sequence tagging to generate its variant Ti. [sent-117, score-0.536]
</p><p>67 To train the model, we first align the collected dictionary word and its variant at the letter level, then construct a feature vector for each letter in the dictionary word, using its mapped character as the reference label. [sent-118, score-1.296]
</p><p>68 , 2007) to map each letter to multiple phonemes (1-to-2 alignment). [sent-125, score-0.411]
</p><p>69 • Syllable-level features SRyellalatibvlee position oufr ethse current syllable in the 74 word; two binary features indicating whether the character is at the beginning or the end of the current syllable. [sent-127, score-0.088]
</p><p>70 The English hyphenation dictionary (Hindson, 2006) is used to mark all the syllable information. [sent-128, score-0.23]
</p><p>71 The trained CRF model can be applied to any English word to generate its variants with probabilities. [sent-129, score-0.102]
</p><p>72 3 Experiments We evaluate the system performance on both Twitter and SMS message test sets. [sent-130, score-0.106]
</p><p>73 It consists of 303 distinct nonstandard tokens and their corresponding dictionary words. [sent-133, score-0.831]
</p><p>74 We developed our own Twitter message test set consisting of 6,150 tweets manually annotated via the Amazon Mechanical Turk. [sent-134, score-0.194]
</p><p>75 3 to 6 turkers were required to convert the nonstandard tokens in  the tweets to the standard English words. [sent-135, score-0.743]
</p><p>76 We extract the nonstandard tokens whose most frequently normalized word consists of letters/digits/apostrophe, and is different from the token itself. [sent-136, score-0.785]
</p><p>77 This results in 3,802 distinct nonstandard tokens that we use as the test set. [sent-137, score-0.681]
</p><p>78 Similar to prior work, we use isolated nonstandard tokens without any context, that is, the LM probabilities P(S) are based on unigrams. [sent-140, score-0.681]
</p><p>79 The first one is a comprehensive list of chat slangs, abbreviations, and acronyms collected by InternetSlang. [sent-142, score-0.125]
</p><p>80 The second is the word-abbreviation lookup table generated by the supervised deletion-based abbreviation approach proposed in (Pennell and Liu, 2010). [sent-144, score-0.216]
</p><p>81 It contains 477,941 (word, abbreviation) pairs automatically generated for 54,594 CMU dictionary words. [sent-145, score-0.226]
</p><p>82 The third is the jazzy spell checker based on the Aspell algorithm (Idzelis, 2005). [sent-146, score-0.381]
</p><p>83 For each nonstandard token, the system is considered correct if any of the corresponding standard words is among the n-best output from the system. [sent-149, score-0.534]
</p><p>84 For the system “LetterTran (All)”, we first generate a lookup table by applying the trained CRF model to the CMU dictionary to generate up to  ××  30 variants for each dictionary word. [sent-165, score-0.48]
</p><p>85 3 To make the comparison more meaningful, we also trim our lookup table to the same size as the deletion table, namely “LetterTran (Trim)”. [sent-166, score-0.202]
</p><p>86 The trimming was performed by selecting the most frequent dictionary words and their generated variants until the length limit is reached. [sent-167, score-0.238]
</p><p>87 Since the string similarity and letter switching algorithms implemented in jazzy can compensate the letter transformation model, we also investigate combining it with our approach, “LetterTran(All) + Jazzy”. [sent-170, score-1.155]
</p><p>88 The deletion table has modest performance given the fact that it covers only deletion-based abbreviations and letter repetitions (see Section 2. [sent-174, score-0.505]
</p><p>89 This is because it handles different ways of forming nonstandard tokens in an unified framework. [sent-177, score-0.708]
</p><p>90 Taking the Twitter test set for an example, the lookup table generated by “LetterTran” covered 69. [sent-178, score-0.139]
</p><p>91 The test tokens that were not covered by the “LetterTrans” model include those generated by accidentally switching and inserting letters (e. [sent-181, score-0.395]
</p><p>92 Adding the output from jazzy compensates these problems and boosts the 1-best accuracy, achieving 21. [sent-184, score-0.228]
</p><p>93 16% absolute performance gain respectively on the Twitter and SMS test sets, as compared to using jazzy only. [sent-186, score-0.228]
</p><p>94 When combined with the jazzy module, it achieved 62. [sent-188, score-0.228]
</p><p>95 4 Conclusion In this paper, we proposed a generic letter transformation approach for text message normalization without pre-categorizing the nonstandard tokens into insertion, deletion, substitution, etc. [sent-194, score-1.408]
</p><p>96 We also avoided the expensive and time consuming hand labeling process by automatically collecting a large set of noisy training pairs. [sent-195, score-0.181]
</p><p>97 In the future, we would like to compare our method with a statistical machine translation approach performed at the letter level, evaluate the system using sentences by incorporating context word information, and consider many-to-one letter transformation in the model. [sent-197, score-0.921]
</p><p>98 5 Acknowledgments The authors thank Deana Pennell for sharing the look-up table generated using the deletion-based abbreviation approach. [sent-198, score-0.14]
</p><p>99 A hybrid rule/model-based finite-state framework for normalizing sms messages. [sent-207, score-0.227]
</p><p>100 Conditional random fields: Probabilistic models for segmenting and labeling sequence data. [sent-240, score-0.077]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nonstandard', 0.534), ('letter', 0.39), ('jazzy', 0.228), ('lettertran', 0.188), ('sms', 0.18), ('twitter', 0.175), ('letters', 0.156), ('dictionary', 0.15), ('tokens', 0.147), ('si', 0.138), ('transformation', 0.118), ('chunk', 0.114), ('oov', 0.111), ('message', 0.106), ('abbreviation', 0.106), ('pennell', 0.094), ('normalization', 0.091), ('ti', 0.089), ('spell', 0.085), ('cook', 0.085), ('choudhury', 0.083), ('messages', 0.079), ('deletion', 0.079), ('lookup', 0.076), ('edinburgh', 0.069), ('checker', 0.068), ('noisy', 0.066), ('categorizing', 0.065), ('crf', 0.064), ('tweets', 0.062), ('jiampojamarn', 0.057), ('slangs', 0.057), ('digits', 0.055), ('character', 0.055), ('stevenson', 0.054), ('variants', 0.054), ('collected', 0.051), ('align', 0.048), ('normalizing', 0.047), ('bday', 0.047), ('cauz', 0.047), ('cavnar', 0.047), ('deana', 0.047), ('digraph', 0.047), ('hyphenation', 0.047), ('pleeeaas', 0.047), ('trim', 0.047), ('token', 0.046), ('aligned', 0.046), ('consuming', 0.045), ('substitution', 0.043), ('insertion', 0.042), ('pairs', 0.042), ('chat', 0.041), ('sequence', 0.041), ('petrovic', 0.041), ('beaufort', 0.041), ('birthday', 0.041), ('coz', 0.041), ('texting', 0.041), ('alignment', 0.039), ('variant', 0.039), ('sittichai', 0.038), ('dropping', 0.038), ('kobus', 0.038), ('labeling', 0.036), ('candidate', 0.036), ('alignments', 0.036), ('abbreviations', 0.036), ('cmu', 0.036), ('normalized', 0.035), ('generated', 0.034), ('expensive', 0.034), ('iraq', 0.034), ('acronyms', 0.033), ('bosch', 0.033), ('syllable', 0.033), ('celikyilmaz', 0.031), ('covered', 0.029), ('switching', 0.029), ('sproat', 0.028), ('yang', 0.027), ('unified', 0.027), ('pronunciation', 0.026), ('consisting', 0.026), ('generate', 0.025), ('phonetic', 0.024), ('wh', 0.023), ('aw', 0.023), ('someone', 0.023), ('integrates', 0.023), ('channel', 0.023), ('unigram', 0.023), ('word', 0.023), ('text', 0.022), ('fei', 0.022), ('lafferty', 0.022), ('chunks', 0.022), ('hlt', 0.022), ('map', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="172-tfidf-1" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>2 0.29087633 <a title="172-tfidf-2" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>Author: Bo Han ; Timothy Baldwin</p><p>Abstract: Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</p><p>3 0.18435964 <a title="172-tfidf-3" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>4 0.10470065 <a title="172-tfidf-4" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>Author: Edward Benson ; Aria Haghighi ; Regina Barzilay</p><p>Abstract: We present a novel method for record extraction from social streams such as Twitter. Unlike typical extraction setups, these environments are characterized by short, one sentence messages with heavily colloquial speech. To further complicate matters, individual messages may not express the full relation to be uncovered, as is often assumed in extraction tasks. We develop a graphical model that addresses these problems by learning a latent set of records and a record-message alignment simultaneously; the output of our model is a set of canonical records, the values of which are consistent with aligned messages. We demonstrate that our approach is able to accurately induce event records from Twitter messages, evaluated against events from a local city guide. Our method achieves significant error reduction over baseline methods.1</p><p>5 0.10417611 <a title="172-tfidf-5" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.</p><p>6 0.099235132 <a title="172-tfidf-6" href="./acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers.html">56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</a></p>
<p>7 0.09405116 <a title="172-tfidf-7" href="./acl-2011-Learning_Sub-Word_Units_for_Open_Vocabulary_Speech_Recognition.html">203 acl-2011-Learning Sub-Word Units for Open Vocabulary Speech Recognition</a></p>
<p>8 0.088651881 <a title="172-tfidf-8" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>9 0.076529406 <a title="172-tfidf-9" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>10 0.072605185 <a title="172-tfidf-10" href="./acl-2011-Domain_Adaptation_for_Machine_Translation_by_Mining_Unseen_Words.html">104 acl-2011-Domain Adaptation for Machine Translation by Mining Unseen Words</a></p>
<p>11 0.072057247 <a title="172-tfidf-11" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>12 0.071333125 <a title="172-tfidf-12" href="./acl-2011-Identifying_Sarcasm_in_Twitter%3A_A_Closer_Look.html">160 acl-2011-Identifying Sarcasm in Twitter: A Closer Look</a></p>
<p>13 0.06837824 <a title="172-tfidf-13" href="./acl-2011-Why_Press_Backspace%3F_Understanding_User_Input_Behaviors_in_Chinese_Pinyin_Input_Method.html">336 acl-2011-Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method</a></p>
<p>14 0.068162568 <a title="172-tfidf-14" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>15 0.066353776 <a title="172-tfidf-15" href="./acl-2011-From_Bilingual_Dictionaries_to_Interlingual_Document_Representations.html">139 acl-2011-From Bilingual Dictionaries to Interlingual Document Representations</a></p>
<p>16 0.066145457 <a title="172-tfidf-16" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>17 0.062612392 <a title="172-tfidf-17" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>18 0.057021365 <a title="172-tfidf-18" href="./acl-2011-A_Mobile_Touchable_Application_for_Online_Topic_Graph_Extraction_and_Exploration_of_Web_Content.html">19 acl-2011-A Mobile Touchable Application for Online Topic Graph Extraction and Exploration of Web Content</a></p>
<p>19 0.056673218 <a title="172-tfidf-19" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>20 0.056445889 <a title="172-tfidf-20" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.147), (1, 0.033), (2, 0.034), (3, 0.028), (4, -0.023), (5, -0.016), (6, 0.056), (7, -0.117), (8, -0.012), (9, 0.14), (10, -0.168), (11, 0.166), (12, 0.128), (13, 0.023), (14, -0.073), (15, -0.046), (16, -0.03), (17, 0.013), (18, 0.062), (19, -0.012), (20, 0.014), (21, -0.001), (22, 0.045), (23, -0.02), (24, 0.061), (25, -0.016), (26, 0.001), (27, -0.029), (28, -0.031), (29, -0.012), (30, -0.01), (31, -0.083), (32, 0.048), (33, 0.01), (34, -0.109), (35, -0.033), (36, -0.083), (37, 0.192), (38, 0.024), (39, -0.029), (40, -0.025), (41, 0.093), (42, -0.038), (43, 0.067), (44, 0.013), (45, -0.026), (46, 0.025), (47, 0.074), (48, 0.063), (49, -0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92074788 <a title="172-lsi-1" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>2 0.88319051 <a title="172-lsi-2" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>Author: Bo Han ; Timothy Baldwin</p><p>Abstract: Twitter provides access to large volumes of data in real time, but is notoriously noisy, hampering its utility for NLP. In this paper, we target out-of-vocabulary words in short text messages and propose a method for identifying and normalising ill-formed words. Our method uses a classifier to detect ill-formed words, and generates correction candidates based on morphophonemic similarity. Both word similarity and context are then exploited to select the most probable correction candidate for the word. The proposed method doesn’t require any annotations, and achieves state-of-the-art performance over an SMS corpus and a novel dataset based on Twitter.</p><p>3 0.66512626 <a title="172-lsi-3" href="./acl-2011-Learning_Sub-Word_Units_for_Open_Vocabulary_Speech_Recognition.html">203 acl-2011-Learning Sub-Word Units for Open Vocabulary Speech Recognition</a></p>
<p>Author: Carolina Parada ; Mark Dredze ; Abhinav Sethy ; Ariya Rastrow</p><p>Abstract: Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of subword units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.</p><p>4 0.64993131 <a title="172-lsi-4" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>Author: Kevin Gimpel ; Nathan Schneider ; Brendan O'Connor ; Dipanjan Das ; Daniel Mills ; Jacob Eisenstein ; Michael Heilman ; Dani Yogatama ; Jeffrey Flanigan ; Noah A. Smith</p><p>Abstract: We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.</p><p>5 0.52853233 <a title="172-lsi-5" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>Author: Zhonghua Qu ; Yang Liu</p><p>Abstract: The number of users on Twitter has drastically increased in the past years. However, Twitter does not have an effective user grouping mechanism. Therefore tweets from other users can quickly overrun and become inconvenient to read. In this paper, we propose methods to help users group the people they follow using their provided seeding users. Two sources of information are used to build sub-systems: textural information captured by the tweets sent by users, and social connections among users. We also propose a measure of fitness to determine which subsystem best represents the seed users and use it for target user ranking. Our experiments show that our proposed framework works well and that adaptively choosing the appropriate sub-system for group suggestion results in increased accuracy.</p><p>6 0.51320058 <a title="172-lsi-6" href="./acl-2011-Event_Discovery_in_Social_Media_Feeds.html">121 acl-2011-Event Discovery in Social Media Feeds</a></p>
<p>7 0.50268424 <a title="172-lsi-7" href="./acl-2011-Identifying_Sarcasm_in_Twitter%3A_A_Closer_Look.html">160 acl-2011-Identifying Sarcasm in Twitter: A Closer Look</a></p>
<p>8 0.45446804 <a title="172-lsi-8" href="./acl-2011-Topical_Keyphrase_Extraction_from_Twitter.html">305 acl-2011-Topical Keyphrase Extraction from Twitter</a></p>
<p>9 0.45185101 <a title="172-lsi-9" href="./acl-2011-Improved_Modeling_of_Out-Of-Vocabulary_Words_Using_Morphological_Classes.html">163 acl-2011-Improved Modeling of Out-Of-Vocabulary Words Using Morphological Classes</a></p>
<p>10 0.43049198 <a title="172-lsi-10" href="./acl-2011-A_Fast_and_Accurate_Method_for_Approximate_String_Search.html">11 acl-2011-A Fast and Accurate Method for Approximate String Search</a></p>
<p>11 0.39760268 <a title="172-lsi-11" href="./acl-2011-An_ERP-based_Brain-Computer_Interface_for_text_entry_using_Rapid_Serial_Visual_Presentation_and_Language_Modeling.html">35 acl-2011-An ERP-based Brain-Computer Interface for text entry using Rapid Serial Visual Presentation and Language Modeling</a></p>
<p>12 0.38376597 <a title="172-lsi-12" href="./acl-2011-The_impact_of_language_models_and_loss_functions_on_repair_disfluency_detection.html">301 acl-2011-The impact of language models and loss functions on repair disfluency detection</a></p>
<p>13 0.38195455 <a title="172-lsi-13" href="./acl-2011-Predicting_Clicks_in_a_Vocabulary_Learning_System.html">248 acl-2011-Predicting Clicks in a Vocabulary Learning System</a></p>
<p>14 0.3790724 <a title="172-lsi-14" href="./acl-2011-Simple_supervised_document_geolocation_with_geodesic_grids.html">285 acl-2011-Simple supervised document geolocation with geodesic grids</a></p>
<p>15 0.37775943 <a title="172-lsi-15" href="./acl-2011-Recognizing_Named_Entities_in_Tweets.html">261 acl-2011-Recognizing Named Entities in Tweets</a></p>
<p>16 0.37336093 <a title="172-lsi-16" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>17 0.35616863 <a title="172-lsi-17" href="./acl-2011-A_Graph_Approach_to_Spelling_Correction_in_Domain-Centric_Search.html">13 acl-2011-A Graph Approach to Spelling Correction in Domain-Centric Search</a></p>
<p>18 0.35233018 <a title="172-lsi-18" href="./acl-2011-C-Feel-It%3A_A_Sentiment_Analyzer_for_Micro-blogs.html">64 acl-2011-C-Feel-It: A Sentiment Analyzer for Micro-blogs</a></p>
<p>19 0.33967313 <a title="172-lsi-19" href="./acl-2011-Bayesian_Inference_for_Zodiac_and_Other_Homophonic_Ciphers.html">56 acl-2011-Bayesian Inference for Zodiac and Other Homophonic Ciphers</a></p>
<p>20 0.33769521 <a title="172-lsi-20" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (5, 0.016), (17, 0.057), (20, 0.017), (26, 0.025), (29, 0.224), (37, 0.085), (39, 0.026), (41, 0.107), (55, 0.032), (59, 0.038), (72, 0.056), (88, 0.013), (91, 0.048), (92, 0.052), (96, 0.104), (97, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75826538 <a title="172-lda-1" href="./acl-2011-Insertion%2C_Deletion%2C_or_Substitution%3F_Normalizing_Text_Messages_without_Pre-categorization_nor_Supervision.html">172 acl-2011-Insertion, Deletion, or Substitution? Normalizing Text Messages without Pre-categorization nor Supervision</a></p>
<p>Author: Fei Liu ; Fuliang Weng ; Bingqing Wang ; Yang Liu</p><p>Abstract: Most text message normalization approaches are based on supervised learning and rely on human labeled training data. In addition, the nonstandard words are often categorized into different types and specific models are designed to tackle each type. In this paper, we propose a unified letter transformation approach that requires neither pre-categorization nor human supervision. Our approach models the generation process from the dictionary words to nonstandard tokens under a sequence labeling framework, where each letter in the dictionary word can be retained, removed, or substituted by other letters/digits. To avoid the expensive and time consuming hand labeling process, we automatically collected a large set of noisy training pairs using a novel webbased approach and performed character-level . alignment for model training. Experiments on both Twitter and SMS messages show that our system significantly outperformed the stateof-the-art deletion-based abbreviation system and the jazzy spell checker (absolute accuracy gain of 21.69% and 18. 16% over jazzy spell checker on the two test sets respectively).</p><p>2 0.70080251 <a title="172-lda-2" href="./acl-2011-Subjectivity_and_Sentiment_Analysis_of_Modern_Standard_Arabic.html">289 acl-2011-Subjectivity and Sentiment Analysis of Modern Standard Arabic</a></p>
<p>Author: Muhammad Abdul-Mageed ; Mona Diab ; Mohammed Korayem</p><p>Abstract: Although Subjectivity and Sentiment Analysis (SSA) has been witnessing a flurry of novel research, there are few attempts to build SSA systems for Morphologically-Rich Languages (MRL). In the current study, we report efforts to partially fill this gap. We present a newly developed manually annotated corpus ofModern Standard Arabic (MSA) together with a new polarity lexicon.The corpus is a collection of newswire documents annotated on the sentence level. We also describe an automatic SSA tagging system that exploits the annotated data. We investigate the impact of different levels ofpreprocessing settings on the SSA classification task. We show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance.</p><p>3 0.63845247 <a title="172-lda-3" href="./acl-2011-An_Efficient_Indexer_for_Large_N-Gram_Corpora.html">36 acl-2011-An Efficient Indexer for Large N-Gram Corpora</a></p>
<p>Author: Hakan Ceylan ; Rada Mihalcea</p><p>Abstract: We introduce a new publicly available tool that implements efficient indexing and retrieval of large N-gram datasets, such as the Web1T 5-gram corpus. Our tool indexes the entire Web1T dataset with an index size of only 100 MB and performs a retrieval of any N-gram with a single disk access. With an increased index size of 420 MB and duplicate data, it also allows users to issue wild card queries provided that the wild cards in the query are contiguous. Furthermore, we also implement some of the smoothing algorithms that are designed specifically for large datasets and are shown to yield better language models than the traditional ones on the Web1T 5gram corpus (Yuret, 2008). We demonstrate the effectiveness of our tool and the smoothing algorithms on the English Lexical Substi- tution task by a simple implementation that gives considerable improvement over a basic language model.</p><p>4 0.62796378 <a title="172-lda-4" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<p>Author: Shasha Liao ; Ralph Grishman</p><p>Abstract: Annotating training data for event extraction is tedious and labor-intensive. Most current event extraction tasks rely on hundreds of annotated documents, but this is often not enough. In this paper, we present a novel self-training strategy, which uses Information Retrieval (IR) to collect a cluster of related documents as the resource for bootstrapping. Also, based on the particular characteristics of this corpus, global inference is applied to provide more confident and informative data selection. We compare this approach to self-training on a normal newswire corpus and show that IR can provide a better corpus for bootstrapping and that global inference can further improve instance selection. We obtain gains of 1.7% in trigger labeling and 2.3% in role labeling through IR and an additional 1.1% in trigger labeling and 1.3% in role labeling by applying global inference. 1</p><p>5 0.62549084 <a title="172-lda-5" href="./acl-2011-An_Affect-Enriched_Dialogue_Act_Classification_Model_for_Task-Oriented_Dialogue.html">33 acl-2011-An Affect-Enriched Dialogue Act Classification Model for Task-Oriented Dialogue</a></p>
<p>Author: Kristy Boyer ; Joseph Grafsgaard ; Eun Young Ha ; Robert Phillips ; James Lester</p><p>Abstract: Dialogue act classification is a central challenge for dialogue systems. Although the importance of emotion in human dialogue is widely recognized, most dialogue act classification models make limited or no use of affective channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dialogue that models facial expressions of users, in particular, facial expressions related to confusion. The findings indicate that the affectenriched classifiers perform significantly better for distinguishing user requests for feedback and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively leverage affective channels to improve dialogue act classification. 1</p><p>6 0.62028074 <a title="172-lda-6" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>7 0.61586678 <a title="172-lda-7" href="./acl-2011-Large-Scale_Cross-Document_Coreference_Using_Distributed_Inference_and_Hierarchical_Models.html">196 acl-2011-Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models</a></p>
<p>8 0.61322415 <a title="172-lda-8" href="./acl-2011-Lexical_Normalisation_of_Short_Text_Messages%3A_Makn_Sens_a_%23twitter.html">208 acl-2011-Lexical Normalisation of Short Text Messages: Makn Sens a #twitter</a></p>
<p>9 0.60949415 <a title="172-lda-9" href="./acl-2011-Faster_and_Smaller_N-Gram_Language_Models.html">135 acl-2011-Faster and Smaller N-Gram Language Models</a></p>
<p>10 0.60933185 <a title="172-lda-10" href="./acl-2011-Joint_Identification_and_Segmentation_of_Domain-Specific_Dialogue_Acts_for_Conversational_Dialogue_Systems.html">185 acl-2011-Joint Identification and Segmentation of Domain-Specific Dialogue Acts for Conversational Dialogue Systems</a></p>
<p>11 0.60787898 <a title="172-lda-11" href="./acl-2011-An_Error_Analysis_of_Relation_Extraction_in_Social_Media_Documents.html">40 acl-2011-An Error Analysis of Relation Extraction in Social Media Documents</a></p>
<p>12 0.60383797 <a title="172-lda-12" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>13 0.60315919 <a title="172-lda-13" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>14 0.60117424 <a title="172-lda-14" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>15 0.60004622 <a title="172-lda-15" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>16 0.59997869 <a title="172-lda-16" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>17 0.59932721 <a title="172-lda-17" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>18 0.59915799 <a title="172-lda-18" href="./acl-2011-Peeling_Back_the_Layers%3A_Detecting_Event_Role_Fillers_in_Secondary_Contexts.html">244 acl-2011-Peeling Back the Layers: Detecting Event Role Fillers in Secondary Contexts</a></p>
<p>19 0.59869778 <a title="172-lda-19" href="./acl-2011-Lexically-Triggered_Hidden_Markov_Models_for_Clinical_Document_Coding.html">209 acl-2011-Lexically-Triggered Hidden Markov Models for Clinical Document Coding</a></p>
<p>20 0.59850621 <a title="172-lda-20" href="./acl-2011-Algorithm_Selection_and_Model_Adaptation_for_ESL_Correction_Tasks.html">32 acl-2011-Algorithm Selection and Model Adaptation for ESL Correction Tasks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
