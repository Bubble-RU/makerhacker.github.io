<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-79" href="#">acl2011-79</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</h1>
<br/><p>Source: <a title="acl-2011-79-pdf" href="http://aclweb.org/anthology//P/P11/P11-1149.pdf">pdf</a></p><p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>Reference: <a title="acl-2011-79-reference" href="../acl2011_reference/acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  ,  ,  Abstract Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. [sent-4, score-0.254]
</p><p>2 We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. [sent-6, score-0.395]
</p><p>3 The algorithm takes a self training approach driven by confidence estimation. [sent-7, score-0.834]
</p><p>4 The term semantic parsing has been used ambiguously to refer to several semantic tasks (e. [sent-11, score-0.362]
</p><p>5 Unlike shallow semantic analysis tasks, the output ofa semantic parser is complete and unambiguous to the extent it can be understood or even executed by a computer system. [sent-15, score-0.331]
</p><p>6 Our model compensates for the lack of training data by employing a self training protocol based on identifying high confidence self labeled examples and using them to retrain the model. [sent-21, score-1.024]
</p><p>7 We base our approach on a simple observation: semantic parsing is a difficult structured prediction task, which requires learning a complex model, however identifying good predictions can be done with a far simpler model capturing repeating patterns in the predicted data. [sent-22, score-0.77]
</p><p>8 We present several simple, yet highly effective confidence measures capturing such patterns, and show how to use them to train a semantic parser without manually annotated sentences. [sent-23, score-0.786]
</p><p>9 Our basic premise, that predictions with high confidence score are of high quality, is further used to improve the performance of the unsupervised trainProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. [sent-24, score-0.928]
</p><p>10 Our learning algorithm takes an EMlike iterative approach, in which the predictions of the previous stage are used to bias the model. [sent-27, score-0.266]
</p><p>11 We show that by using confidence estimation as a proxy for the model’s prediction quality, the learning algorithm can identify a better model compared to the default convergence criterion. [sent-29, score-0.962]
</p><p>12 Our experimental results show that using our approach we are able to train a good semantic parser without annotated data, and that using a confidence score to identify good models results in a significant performance improve-  ment. [sent-33, score-0.759]
</p><p>13 2 Semantic Parsing We formulate semantic parsing as a structured prediction problem, mapping a NL input sentence (denoted x), to its highest ranking MR (denoted z). [sent-34, score-0.544]
</p><p>14 The prediction function, mapping a sentence to its corresponding MR, is formalized as follows:  z = Fw(x) = ayr∈gY,mz∈aZxwTΦ(x,y,z)  (1)  Where Φ is a feature function defined over an input sentence x, alignment y and output z. [sent-44, score-0.334]
</p><p>15 y z  count( state( traverse( river( const(colorado))))  Figure 1: Example of an input sentence (x), meaning representation (z) and the alignment between the two (y) for the Geoquery domain ference problem based on Φ and w is what compromises our semantic parser. [sent-48, score-0.268]
</p><p>16 In practice the parsing decision is decomposed into smaller decisions (Sec. [sent-49, score-0.294]
</p><p>17 1 Target Meaning Representation The output of the semantic parser is a logical formula, grounding the semantics of the input sentence in the domain language (i. [sent-58, score-0.442]
</p><p>18 In practice this decision is decomposed into smaller decisions, capturing local mapping of input tokens to logical fragments and their composition into larger fragments. [sent-73, score-0.568]
</p><p>19 The first type of decisions are encoded directly by the alignment (y) between the input tokens and their corresponding predicates. [sent-76, score-0.225]
</p><p>20 1: river ( const ( colorado ) ) is a composition of two predicates river and const ( colorado ) . [sent-82, score-0.567]
</p><p>21 We denote by si the i-th output predicate composition in z, by si−1 (si) the composition ofthe (i− 1)th predicate on the i-th) predicate asnitdio by y(si) (tih−e i1n)-put word corresponding to that predicate according to the alignment y. [sent-87, score-0.557]
</p><p>22 3  Unsupervised Semantic Parsing  Our learning framework takes a self training ap-  proach in which the learner is iteratively trained over its own predictions. [sent-88, score-0.264]
</p><p>23 Successful application of this approach depends heavily on two important factors - how to select high quality examples to train the model on, and how to define the learning objective so that learning can halt once a good model is found. [sent-89, score-0.406]
</p><p>24 In this work we suggest to address both of the above concerns by approximating the quality of the model’s predictions using a confidence measure computed over the statistics of the self generated predictions. [sent-91, score-0.89]
</p><p>25 Output structures which fall close to the center of mass of these statistics will receive a high confidence score. [sent-92, score-0.687]
</p><p>26 The first issue is addressed by using examples assigned a high confidence score to train the model, acting as labeled examples. [sent-93, score-0.726]
</p><p>27 We also note that since the confidence score pro1488 vides a good indication for the model’s prediction  performance, it can be used to approximate the overall model performance, by observing the model’s total confidence score over all its predictions. [sent-94, score-1.403]
</p><p>28 This allows us to set a performance driven goal for our learning process - return the model maximizing the confidence score over all predictions. [sent-95, score-0.82]
</p><p>29 We describe the details of integrating the confidence score into the learning framework in Sec. [sent-96, score-0.706]
</p><p>30 , wTΦ(x, y, z)) as an indication of correctness is a natural choice, we argue and show empirically, that unsupervised learning driven by confidence estimation results in a better performing model. [sent-101, score-1.066]
</p><p>31 The statistics used for confidence estimation are different than those used by the model to create the output structures, and can therefore capture additional information unobserved by the prediction model. [sent-105, score-0.928]
</p><p>32 The success of our learning procedure hinges on finding good confidence measures, whose confidence prediction correlates well with the true quality of the prediction. [sent-108, score-1.302]
</p><p>33 The ability of unsupervised confidence estimation to provide high quality confidence predictions can be explained by the observation that prominent prediction patterns are more likely to be correct. [sent-109, score-1.677]
</p><p>34 Our specific choice of confidence measures is guided by the intuition that unlike structure prediction (i. [sent-111, score-0.698]
</p><p>35 , solving the inference problem) which requires taking statistics over complex and intricate patterns, identifying high quality predictions can be done using much simpler patterns that are significantly easier to capture. [sent-113, score-0.408]
</p><p>36 1), we then explain the rational behind confidence estimation over self-generated data and introduce the confidence measures used in our experiments (Sec. [sent-120, score-1.229]
</p><p>37 1 Unsupervised Confidence-Driven Learning Our learning framework works in an EM-like manner, iterating between two stages: making predictions based on its current set of parameters and then retraining the model using a subset of the predictions, assigned high confidence. [sent-127, score-0.324]
</p><p>38 The learning process “discovers” new high confidence training examples to add to its training set over multiple iterations, and converges when the model no longer adds new training examples. [sent-128, score-0.819]
</p><p>39 We follow the observation that confidence estimation can be used to approximate the performance of the entire model and return the model with the highest overall prediction confidence. [sent-132, score-0.867]
</p><p>40 The set of top confidence examples (for either correct or incorrect prediction), at iteration iof the algorithm, is denoted The exact nature of the confidence computation is discussed in Sec. [sent-141, score-1.204]
</p><p>41 The algorithm iterates between these two stages, at each iteration it adds more self-annotated examples to its training set, learning therefore converges when no new examples are added (line 11). [sent-144, score-0.319]
</p><p>42 The algorithm keeps track of the models it trained at each stage throughout this process, and returns the one with the highest averaged overall confidence score  Scionf. [sent-145, score-0.635]
</p><p>43 At each stage, the overall confidence score is computed by averaging over all the confidence scores of the predictions made at that stage. [sent-147, score-1.262]
</p><p>44 2 Unsupervised Confidence Estimation Confidence estimation is calculated over a batch of input (x) - output (z) pairs. [sent-149, score-0.236]
</p><p>45 Confidence estimation is done by computing the statistics of these decisions, over the entire set of predicted structures. [sent-153, score-0.238]
</p><p>46 In the rest of this section we introduce the confidence measures used by our system. [sent-154, score-0.58]
</p><p>47 This can be considered as an abstraction of the prediction model: we collapse the intricate feature representation into 1Since we commit to the max-score output prediction, rather than summing over all possibilities, we require a reasonable initialization point. [sent-156, score-0.26]
</p><p>48 high level decisions and take statistics over these decisions. [sent-159, score-0.212]
</p><p>49 Since it takes statistics over considerably  less variables than the actual prediction model, we expect this model to make reliable confidence predictions. [sent-160, score-0.798]
</p><p>50 We consider two variations of this approach, the first constructs a unigram model over the first order decisions and the second a bigram model over the second order decisions. [sent-161, score-0.317]
</p><p>51 Given a set of structure predictions S, we com-  pute this proportion for each structure (denoted as Prop(x, z)) and calculate the average proportion over the entire set (denoted as AvProp(S)). [sent-164, score-0.292]
</p><p>52 The algorithm decomposes each predicted formula and its corresponding input sentence into a feature vector Φ(x, y, z) normalized by the size of the input sentence |x|, and assigns a binary label to this vector2. [sent-174, score-0.404]
</p><p>53 1, and use the confidence score to select the top ranking examples as positive examples, and the bottom ranking examples as negative examples. [sent-177, score-0.861]
</p><p>54 The second is a structured learning algorithm which considers learning as a ranking problem, i. [sent-179, score-0.276]
</p><p>55 The structured learning algorithm can directly use the top ranking predictions of the model (line 8 in Alg. [sent-182, score-0.389]
</p><p>56 1 is an inference procedure selecting the top ranked output logical formula. [sent-187, score-0.303]
</p><p>57 1) is decomposed into smaller decisions, capturing mapping of input to-  kens to logical fragments (first order) and their composition into larger fragments (second order). [sent-195, score-0.562]
</p><p>58 We encode a first-order decision as αcs, a binary variable indicating that constituent c is aligned with the logical symbol s . [sent-196, score-0.321]
</p><p>59 A second-order decision βcs,dt, is encoded as a binary variable indicating that the symbol t (associated with constituent d) is an argument of a function s (associated with constituent c). [sent-197, score-0.213]
</p><p>60 First-order decision features Φ1 Determining if a logical symbol is aligned with a specific constituent depends mostly on lexical information. [sent-206, score-0.266]
</p><p>61 , (Zettlemoyer and Collins, 2005)) we create a small lexicon, mapping logical symbols to surface forms. [sent-209, score-0.213]
</p><p>62 , 1990) and add features which measure the lexical similarity between a constituent and a logical symbol’s surface forms (as defined by the lexicon). [sent-212, score-0.212]
</p><p>63 We compare several confidence measures and  analyze their properties. [sent-222, score-0.58]
</p><p>64 Our learning system had access only to the NL questions, and the logical forms were only used to evaluate the system’s performance. [sent-234, score-0.225]
</p><p>65 SCORE: An unsupervised framework using the model’s internal prediction score (wTΦ(x, y, z)) for confidence estimation. [sent-243, score-0.882]
</p><p>66 The reported score was obtained by selecting the model at the training iteration with the highest overall confidence score (see line 12 in Alg. [sent-247, score-0.803]
</p><p>67 While our approach is based on assessing the correctness os the model’s predictions according to unsupervised confidence estimation, their framework is provided with external supervision for these decisions, indicating if the predicted structures are correct. [sent-251, score-1.015]
</p><p>68 We compare three self-trained systems, ALL EXAMPLES, PREDICTIONSCORE and COMBINED, which differ 4While unsupervised learning for various semantic tasks has been widely discussed, this is the first attempt to tackle this task. [sent-261, score-0.306]
</p><p>69 1492 in their sample selection strategy, but all use confidence estimation for selecting the final semantic parsing model. [sent-264, score-0.92]
</p><p>70 These results show that training the model with training examples selected carefully will improve learning - as the best performance is achieved with perfect knowledge of the predictions correctness (RESPONSE BASED). [sent-274, score-0.331]
</p><p>71 The low performance of the PREDICTIONSCORE model is also not surprising, and it demonstrates one of the key principles in confidence estimation - the score should be comparable across predictions done over different inputs, and not the same input, as done in PREDICTIONSCORE model. [sent-277, score-0.891]
</p><p>72 (2) How does confidence driven sample selection  contribute to the learning process? [sent-278, score-0.742]
</p><p>73 Comparing the systems driven by confidence sample-selection to the ALL EXAMPLES approach uncovers an interesting tradeoff between training with more (noisy) data and selectively training the system with higher quality examples. [sent-279, score-0.69]
</p><p>74 We argue that carefully selecting high quality training examples will result in better performance. [sent-280, score-0.264]
</p><p>75 The empirical results indeed support our argument, as the best performing model (RESPONSE BASED) is achieved by sample selection with perfect knowledge of prediction correctness. [sent-281, score-0.243]
</p><p>76 We argue that different confidence measures capture different properties of the data, and hypothesize that combining their scores will improve the resulting model. [sent-284, score-0.627]
</p><p>77 Results show that using the COMBINED measure leads to an improved performance, better than any of the individual measures, suggesting that it can effectively exploit the properties ofeach confidence measure. [sent-288, score-0.535]
</p><p>78 (3) Can confidence measures serve as a good proxy for the model’s performance? [sent-290, score-0.624]
</p><p>79 We argue that by selecting the model that maximizes the averaged confidence score, a better model can be found. [sent-292, score-0.723]
</p><p>80 We compare the performance of the model selected using the confidence score to the performance of the final model considered by the learning algorithm (see Sec. [sent-295, score-0.8]
</p><p>81 Since these experiments required running the learning algorithm many times, we focused on the binary learning algorithm as it converges considerably faster. [sent-299, score-0.299]
</p><p>82 SCORE and PROPORTION models we used both their confidence prediction, and the simple UNIGRAM confidence score to evaluate model performance (the latter appear in parentheses in Tab. [sent-304, score-1.184]
</p><p>83 Results show that the over overall confidence score serves as a reliable proxy for the model performance - using UNIGRAM and BIGRAM the framework can select the best performing model, far better than the performance of the default model to which the system converged. [sent-306, score-0.824]
</p><p>84 273482  Table 4: Using confidence to approximate model performance. [sent-327, score-0.585]
</p><p>85 We compare the best result obtained in any of the learning algorithm iterations (Best), the result obtained by approximating the best result using the averaged prediction confidence (Conf. [sent-328, score-0.754]
</p><p>86 Results in parentheses are the result of using the UNIGRAM confidence to approximate the model’s performance. [sent-331, score-0.535]
</p><p>87 Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. [sent-344, score-0.359]
</p><p>88 While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. [sent-345, score-0.24]
</p><p>89 Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al. [sent-346, score-0.315]
</p><p>90 We estimate the quality of the predictions and use only high confidence examples for training. [sent-351, score-0.839]
</p><p>91 In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. [sent-358, score-0.834]
</p><p>92 This use of confidence estimation was explored in (Reichart et al. [sent-359, score-0.649]
</p><p>93 In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. [sent-361, score-0.218]
</p><p>94 7  Conclusions  We introduced an unsupervised learning algorithm  for semantic parsing, the first for this task to the best of our knowledge. [sent-362, score-0.342]
</p><p>95 To compensate for the lack of training data we use a self-training protocol, driven by unsupervised confidence estimation. [sent-363, score-0.764]
</p><p>96 We demonstrate empirically that our approach results in a high preforming semantic parser and show that confidence estimation plays a vital role in this success, both by identifying good training examples as well as identifying good over all performance, used to improve the final model selection. [sent-364, score-0.986]
</p><p>97 In future work we hope to further improve unsupervised semantic parsing performance. [sent-365, score-0.317]
</p><p>98 Particularly, we intend to explore new approaches for confidence estimation and their usage in the unsupervised and semi-supervised versions of the task. [sent-366, score-0.772]
</p><p>99 Speech recognition and utterance verification based on a generalized confidence score. [sent-460, score-0.535]
</p><p>100 Online learning of relaxed CCG grammars for parsing to logical form. [sent-620, score-0.301]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('confidence', 0.535), ('zettlemoyer', 0.165), ('logical', 0.16), ('nl', 0.152), ('mooney', 0.149), ('geoquery', 0.148), ('clarke', 0.135), ('predictions', 0.128), ('unsupervised', 0.123), ('si', 0.121), ('reichart', 0.12), ('self', 0.12), ('prediction', 0.118), ('semantic', 0.118), ('decisions', 0.115), ('estimation', 0.114), ('zelle', 0.109), ('driven', 0.106), ('const', 0.1), ('mr', 0.099), ('predictionscore', 0.099), ('composition', 0.089), ('examples', 0.088), ('proportion', 0.082), ('parsing', 0.076), ('river', 0.072), ('protocol', 0.072), ('tang', 0.072), ('input', 0.069), ('structured', 0.067), ('supervision', 0.066), ('predicted', 0.066), ('learning', 0.065), ('score', 0.064), ('collins', 0.063), ('supervised', 0.06), ('statistics', 0.058), ('combined', 0.058), ('titov', 0.057), ('goldwasser', 0.057), ('decomposes', 0.057), ('wt', 0.056), ('structures', 0.055), ('binary', 0.055), ('decision', 0.054), ('bigram', 0.054), ('output', 0.053), ('mapping', 0.053), ('constituent', 0.052), ('formula', 0.052), ('wong', 0.051), ('model', 0.05), ('refer', 0.05), ('avprop', 0.049), ('initialmodel', 0.049), ('intricate', 0.049), ('prolog', 0.049), ('scionf', 0.049), ('line', 0.049), ('inference', 0.049), ('quality', 0.049), ('decomposed', 0.049), ('unigram', 0.048), ('xl', 0.048), ('fragments', 0.048), ('response', 0.048), ('logic', 0.047), ('argue', 0.047), ('predicates', 0.046), ('blum', 0.046), ('denoted', 0.046), ('capturing', 0.046), ('measures', 0.045), ('proxy', 0.044), ('reader', 0.044), ('colorado', 0.044), ('ranking', 0.043), ('chang', 0.043), ('parser', 0.042), ('framework', 0.042), ('converges', 0.042), ('inductive', 0.042), ('alignment', 0.041), ('selecting', 0.041), ('predicate', 0.041), ('representation', 0.04), ('prop', 0.04), ('caruana', 0.04), ('roth', 0.04), ('performing', 0.039), ('high', 0.039), ('geography', 0.038), ('ueffing', 0.038), ('takes', 0.037), ('indication', 0.037), ('selection', 0.036), ('algorithm', 0.036), ('patterns', 0.036), ('kate', 0.036), ('branavan', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="79-tfidf-1" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>2 0.19393864 <a title="79-tfidf-2" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>3 0.18409163 <a title="79-tfidf-3" href="./acl-2011-Goodness%3A_A_Method_for_Measuring_Machine_Translation_Confidence.html">146 acl-2011-Goodness: A Method for Measuring Machine Translation Confidence</a></p>
<p>Author: Nguyen Bach ; Fei Huang ; Yaser Al-Onaizan</p><p>Abstract: State-of-the-art statistical machine translation (MT) systems have made significant progress towards producing user-acceptable translation output. However, there is still no efficient way for MT systems to inform users which words are likely translated correctly and how confident it is about the whole sentence. We propose a novel framework to predict wordlevel and sentence-level MT errors with a large number of novel features. Experimental results show that the MT error prediction accuracy is increased from 69.1 to 72.2 in F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improve- ments between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity.</p><p>4 0.1639501 <a title="79-tfidf-4" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>Author: Percy Liang ; Michael Jordan ; Dan Klein</p><p>Abstract: Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.</p><p>5 0.12958959 <a title="79-tfidf-5" href="./acl-2011-Confidence-Weighted_Learning_of_Factored_Discriminative_Language_Models.html">78 acl-2011-Confidence-Weighted Learning of Factored Discriminative Language Models</a></p>
<p>Author: Viet Ha Thuc ; Nicola Cancedda</p><p>Abstract: Language models based on word surface forms only are unable to benefit from available linguistic knowledge, and tend to suffer from poor estimates for rare features. We propose an approach to overcome these two limitations. We use factored features that can flexibly capture linguistic regularities, and we adopt confidence-weighted learning, a form of discriminative online learning that can better take advantage of a heavy tail of rare features. Finally, we extend the confidence-weighted learning to deal with label noise in training data, a common case with discriminative lan- guage modeling.</p><p>6 0.096068092 <a title="79-tfidf-6" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>7 0.095686577 <a title="79-tfidf-7" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>8 0.094352692 <a title="79-tfidf-8" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>9 0.090222768 <a title="79-tfidf-9" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>10 0.08976052 <a title="79-tfidf-10" href="./acl-2011-Consistent_Translation_using_Discriminative_Learning_-_A_Translation_Memory-inspired_Approach.html">81 acl-2011-Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach</a></p>
<p>11 0.08847817 <a title="79-tfidf-11" href="./acl-2011-Semi-supervised_latent_variable_models_for_sentence-level_sentiment_analysis.html">279 acl-2011-Semi-supervised latent variable models for sentence-level sentiment analysis</a></p>
<p>12 0.085511841 <a title="79-tfidf-12" href="./acl-2011-Word_Alignment_Combination_over_Multiple_Word_Segmentation.html">339 acl-2011-Word Alignment Combination over Multiple Word Segmentation</a></p>
<p>13 0.084583364 <a title="79-tfidf-13" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>14 0.083531223 <a title="79-tfidf-14" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>15 0.08136417 <a title="79-tfidf-15" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>16 0.081252657 <a title="79-tfidf-16" href="./acl-2011-Global_Learning_of_Typed_Entailment_Rules.html">144 acl-2011-Global Learning of Typed Entailment Rules</a></p>
<p>17 0.080973983 <a title="79-tfidf-17" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<p>18 0.079776973 <a title="79-tfidf-18" href="./acl-2011-Domain_Adaptation_by_Constraining_Inter-Domain_Variability_of_Latent_Feature_Representation.html">103 acl-2011-Domain Adaptation by Constraining Inter-Domain Variability of Latent Feature Representation</a></p>
<p>19 0.079177119 <a title="79-tfidf-19" href="./acl-2011-AM-FM%3A_A_Semantic_Framework_for_Translation_Quality_Assessment.html">2 acl-2011-AM-FM: A Semantic Framework for Translation Quality Assessment</a></p>
<p>20 0.078961492 <a title="79-tfidf-20" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.268), (1, 0.001), (2, -0.069), (3, -0.052), (4, 0.005), (5, -0.017), (6, 0.048), (7, 0.021), (8, -0.029), (9, 0.009), (10, 0.084), (11, -0.013), (12, 0.043), (13, 0.044), (14, -0.085), (15, -0.014), (16, -0.074), (17, -0.141), (18, 0.021), (19, -0.01), (20, 0.026), (21, 0.016), (22, -0.059), (23, 0.02), (24, 0.016), (25, -0.034), (26, -0.02), (27, -0.034), (28, 0.028), (29, 0.042), (30, -0.032), (31, 0.02), (32, -0.012), (33, 0.02), (34, 0.019), (35, 0.081), (36, 0.008), (37, -0.051), (38, 0.095), (39, -0.072), (40, 0.048), (41, -0.036), (42, -0.032), (43, -0.007), (44, -0.014), (45, -0.096), (46, 0.066), (47, -0.068), (48, -0.036), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96533012 <a title="79-lsi-1" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>2 0.76577264 <a title="79-lsi-2" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>Author: Percy Liang ; Michael Jordan ; Dan Klein</p><p>Abstract: Compositional question answering begins by mapping questions to logical forms, but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms. In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs. In tackling this challenging learning problem, we introduce a new semantic representation which highlights a parallel between dependency syntax and efficient evaluation of logical forms. On two standard semantic parsing benchmarks (GEO and JOBS), our system obtains the highest published accuracies, despite requiring no annotated logical forms.</p><p>3 0.72014654 <a title="79-lsi-3" href="./acl-2011-A_Bayesian_Model_for_Unsupervised_Semantic_Parsing.html">3 acl-2011-A Bayesian Model for Unsupervised Semantic Parsing</a></p>
<p>Author: Ivan Titov ; Alexandre Klementiev</p><p>Abstract: We propose a non-parametric Bayesian model for unsupervised semantic parsing. Following Poon and Domingos (2009), we consider a semantic parsing setting where the goal is to (1) decompose the syntactic dependency tree of a sentence into fragments, (2) assign each of these fragments to a cluster of semantically equivalent syntactic structures, and (3) predict predicate-argument relations between the fragments. We use hierarchical PitmanYor processes to model statistical dependencies between meaning representations of predicates and those of their arguments, as well as the clusters of their syntactic realizations. We develop a modification of the MetropolisHastings split-merge sampler, resulting in an efficient inference algorithm for the model. The method is experimentally evaluated by us- ing the induced semantic representation for the question answering task in the biomedical domain.</p><p>4 0.69409221 <a title="79-lsi-4" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>Author: Michael Mohler ; Razvan Bunescu ; Rada Mihalcea</p><p>Abstract: In this work we address the task of computerassisted assessment of short student answers. We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. We also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers.</p><p>5 0.66931337 <a title="79-lsi-5" href="./acl-2011-Confidence-Weighted_Learning_of_Factored_Discriminative_Language_Models.html">78 acl-2011-Confidence-Weighted Learning of Factored Discriminative Language Models</a></p>
<p>Author: Viet Ha Thuc ; Nicola Cancedda</p><p>Abstract: Language models based on word surface forms only are unable to benefit from available linguistic knowledge, and tend to suffer from poor estimates for rare features. We propose an approach to overcome these two limitations. We use factored features that can flexibly capture linguistic regularities, and we adopt confidence-weighted learning, a form of discriminative online learning that can better take advantage of a heavy tail of rare features. Finally, we extend the confidence-weighted learning to deal with label noise in training data, a common case with discriminative lan- guage modeling.</p><p>6 0.66875982 <a title="79-lsi-6" href="./acl-2011-Unsupervised_Learning_of_Semantic_Relation_Composition.html">322 acl-2011-Unsupervised Learning of Semantic Relation Composition</a></p>
<p>7 0.65065813 <a title="79-lsi-7" href="./acl-2011-Unsupervised_Discovery_of_Domain-Specific_Knowledge_from_Text.html">320 acl-2011-Unsupervised Discovery of Domain-Specific Knowledge from Text</a></p>
<p>8 0.63050401 <a title="79-lsi-8" href="./acl-2011-Combining_Indicators_of_Allophony.html">74 acl-2011-Combining Indicators of Allophony</a></p>
<p>9 0.62242591 <a title="79-lsi-9" href="./acl-2011-A_Simple_Measure_to_Assess_Non-response.html">25 acl-2011-A Simple Measure to Assess Non-response</a></p>
<p>10 0.60400802 <a title="79-lsi-10" href="./acl-2011-full-for-print.html">342 acl-2011-full-for-print</a></p>
<p>11 0.59799379 <a title="79-lsi-11" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>12 0.59691578 <a title="79-lsi-12" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>13 0.58652633 <a title="79-lsi-13" href="./acl-2011-In-domain_Relation_Discovery_with_Meta-constraints_via_Posterior_Regularization.html">170 acl-2011-In-domain Relation Discovery with Meta-constraints via Posterior Regularization</a></p>
<p>14 0.57877874 <a title="79-lsi-14" href="./acl-2011-Temporal_Restricted_Boltzmann_Machines_for_Dependency_Parsing.html">295 acl-2011-Temporal Restricted Boltzmann Machines for Dependency Parsing</a></p>
<p>15 0.57792252 <a title="79-lsi-15" href="./acl-2011-Learning_Condensed_Feature_Representations_from_Large_Unsupervised_Data_Sets_for_Supervised_Learning.html">199 acl-2011-Learning Condensed Feature Representations from Large Unsupervised Data Sets for Supervised Learning</a></p>
<p>16 0.57657427 <a title="79-lsi-16" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>17 0.57457536 <a title="79-lsi-17" href="./acl-2011-Does_Size_Matter_-_How_Much_Data_is_Required_to_Train_a_REG_Algorithm%3F.html">102 acl-2011-Does Size Matter - How Much Data is Required to Train a REG Algorithm?</a></p>
<p>18 0.57388669 <a title="79-lsi-18" href="./acl-2011-Computing_and_Evaluating_Syntactic_Complexity_Features_for_Automated_Scoring_of_Spontaneous_Non-Native_Speech.html">77 acl-2011-Computing and Evaluating Syntactic Complexity Features for Automated Scoring of Spontaneous Non-Native Speech</a></p>
<p>19 0.56882864 <a title="79-lsi-19" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>20 0.56815553 <a title="79-lsi-20" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.019), (17, 0.044), (26, 0.015), (37, 0.101), (39, 0.028), (41, 0.055), (55, 0.035), (59, 0.06), (72, 0.032), (91, 0.391), (96, 0.152), (97, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94726646 <a title="79-lda-1" href="./acl-2011-ConsentCanvas%3A_Automatic_Texturing_for_Improved_Readability_in_End-User_License_Agreements.html">80 acl-2011-ConsentCanvas: Automatic Texturing for Improved Readability in End-User License Agreements</a></p>
<p>Author: Oliver Schneider ; Alex Garnett</p><p>Abstract: We present ConsentCanvas, a system which structures and “texturizes” End-User License Agreement (EULA) documents to be more readable. The system aims to help users better understand the terms under which they are providing their informed consent. ConsentCanvas receives unstructured text documents as input and uses unsupervised natural language processing methods to embellish the source document using a linked stylesheet. Unlike similar usable security projects which employ summarization techniques, our system preserves the contents of the source document, minimizing the cognitive and legal burden for both the end user and the licensor. Our system does not require a corpus for training. 1</p><p>2 0.93508595 <a title="79-lda-2" href="./acl-2011-Fully_Unsupervised_Word_Segmentation_with_BVE_and_MDL.html">140 acl-2011-Fully Unsupervised Word Segmentation with BVE and MDL</a></p>
<p>Author: Daniel Hewlett ; Paul Cohen</p><p>Abstract: Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</p><p>3 0.93288845 <a title="79-lda-3" href="./acl-2011-HITS-based_Seed_Selection_and_Stop_List_Construction_for_Bootstrapping.html">148 acl-2011-HITS-based Seed Selection and Stop List Construction for Bootstrapping</a></p>
<p>Author: Tetsuo Kiso ; Masashi Shimbo ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: In bootstrapping (seed set expansion), selecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method.</p><p>same-paper 4 0.91810793 <a title="79-lda-4" href="./acl-2011-Confidence_Driven_Unsupervised_Semantic_Parsing.html">79 acl-2011-Confidence Driven Unsupervised Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Roi Reichart ; James Clarke ; Dan Roth</p><p>Abstract: Current approaches for semantic parsing take a supervised approach requiring a considerable amount of training data which is expensive and difficult to obtain. This supervision bottleneck is one of the major difficulties in scaling up semantic parsing. We argue that a semantic parser can be trained effectively without annotated data, and introduce an unsupervised learning algorithm. The algorithm takes a self training approach driven by confidence estimation. Evaluated over Geoquery, a standard dataset for this task, our system achieved 66% accuracy, compared to 80% of its fully supervised counterpart, demonstrating the promise of unsupervised approaches for this task.</p><p>5 0.86914432 <a title="79-lda-5" href="./acl-2011-Two_Easy_Improvements_to_Lexical_Weighting.html">313 acl-2011-Two Easy Improvements to Lexical Weighting</a></p>
<p>Author: David Chiang ; Steve DeNeefe ; Michael Pust</p><p>Abstract: We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems.</p><p>6 0.78321582 <a title="79-lda-6" href="./acl-2011-EdIt%3A_A_Broad-Coverage_Grammar_Checker_Using_Pattern_Grammar.html">108 acl-2011-EdIt: A Broad-Coverage Grammar Checker Using Pattern Grammar</a></p>
<p>7 0.71184635 <a title="79-lda-7" href="./acl-2011-Good_Seed_Makes_a_Good_Crop%3A_Accelerating_Active_Learning_Using_Language_Modeling.html">145 acl-2011-Good Seed Makes a Good Crop: Accelerating Active Learning Using Language Modeling</a></p>
<p>8 0.69924593 <a title="79-lda-8" href="./acl-2011-Coreference_for_Learning_to_Extract_Relations%3A_Yes_Virginia%2C_Coreference_Matters.html">86 acl-2011-Coreference for Learning to Extract Relations: Yes Virginia, Coreference Matters</a></p>
<p>9 0.69720519 <a title="79-lda-9" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>10 0.66453147 <a title="79-lda-10" href="./acl-2011-Together_We_Can%3A_Bilingual_Bootstrapping_for_WSD.html">304 acl-2011-Together We Can: Bilingual Bootstrapping for WSD</a></p>
<p>11 0.66012919 <a title="79-lda-11" href="./acl-2011-Learning_Dependency-Based_Compositional_Semantics.html">200 acl-2011-Learning Dependency-Based Compositional Semantics</a></p>
<p>12 0.65633404 <a title="79-lda-12" href="./acl-2011-P11-5002_k2opt.pdf.html">239 acl-2011-P11-5002 k2opt.pdf</a></p>
<p>13 0.65055835 <a title="79-lda-13" href="./acl-2011-Parsing_the_Internal_Structure_of_Words%3A_A_New_Paradigm_for_Chinese_Word_Segmentation.html">241 acl-2011-Parsing the Internal Structure of Words: A New Paradigm for Chinese Word Segmentation</a></p>
<p>14 0.64219332 <a title="79-lda-14" href="./acl-2011-Evaluating_the_Impact_of_Coder_Errors_on_Active_Learning.html">119 acl-2011-Evaluating the Impact of Coder Errors on Active Learning</a></p>
<p>15 0.6386863 <a title="79-lda-15" href="./acl-2011-Interactive_Group_Suggesting_for_Twitter.html">177 acl-2011-Interactive Group Suggesting for Twitter</a></p>
<p>16 0.61868823 <a title="79-lda-16" href="./acl-2011-Model-Portability_Experiments_for_Textual_Temporal_Analysis.html">222 acl-2011-Model-Portability Experiments for Textual Temporal Analysis</a></p>
<p>17 0.61552781 <a title="79-lda-17" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>18 0.61475748 <a title="79-lda-18" href="./acl-2011-Grammatical_Error_Correction_with_Alternating_Structure_Optimization.html">147 acl-2011-Grammatical Error Correction with Alternating Structure Optimization</a></p>
<p>19 0.6109522 <a title="79-lda-19" href="./acl-2011-Knowledge-Based_Weak_Supervision_for_Information_Extraction_of_Overlapping_Relations.html">190 acl-2011-Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></p>
<p>20 0.60798758 <a title="79-lda-20" href="./acl-2011-Simple_Unsupervised_Grammar_Induction_from_Raw_Text_with_Cascaded_Finite_State_Models.html">284 acl-2011-Simple Unsupervised Grammar Induction from Raw Text with Cascaded Finite State Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
