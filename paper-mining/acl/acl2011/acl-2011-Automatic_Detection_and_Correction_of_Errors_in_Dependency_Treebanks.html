<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-48" href="#">acl2011-48</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</h1>
<br/><p>Source: <a title="acl-2011-48-pdf" href="http://aclweb.org/anthology//P/P11/P11-2060.pdf">pdf</a></p><p>Author: Alexander Volokh ; Gunter Neumann</p><p>Abstract: Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 1</p><p>Reference: <a title="acl-2011-48-reference" href="../acl2011_reference/acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Errors in Dependency TreeGünter Neumann DFKI  Stuhlsatzenhausweg 3 66123 Saarbrücken, Germany neumann@ dfki . [sent-3, score-0.148]
</p><p>2 de Abstract Annotated corpora are essential for almost all NLP applications. [sent-4, score-0.04]
</p><p>3 Whereas they are expected  to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. [sent-5, score-0.071]
</p><p>4 Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. [sent-7, score-0.311]
</p><p>5 Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. [sent-8, score-0.453]
</p><p>6 At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. [sent-9, score-0.784]
</p><p>7 1  Introduction  Treebanks and other annotated corpora have become essential for almost all NLP applications. [sent-10, score-0.08]
</p><p>8 Papers about corpora like the Penn Treebank [1] have thousands of citations, since most of the algorithms profit from annotated data during the development and testing and thus are widely used in the field. [sent-11, score-0.08]
</p><p>9 Treebanks are therefore expected to be of a very  high quality in order to guarantee reliability for their theoretical and practical uses. [sent-12, score-0.128]
</p><p>10 The construction of an annotated corpus involves a lot of work performed by large groups. [sent-13, score-0.092]
</p><p>11 However, despite the fact that a lot of human post-editing and automatic quality assurance is done, errors can not be avoided completely [5]. [sent-14, score-0.458]
</p><p>12 346 In this paper we propose an approach for finding and correcting errors in dependency treebanks. [sent-15, score-0.474]
</p><p>13 We apply our method to the English dependency corpus – conversion of the Penn Treebank to the dependency format done by Richard Johansson and Mihai Surdeanu [2] for the CoNLL shared tasks [3]. [sent-16, score-0.526]
</p><p>14 This is probably the most used dependency corpus, since English is the most popular language among the researchers. [sent-17, score-0.243]
</p><p>15 Still we are able to find a considerable amount of errors in it. [sent-18, score-0.342]
</p><p>16 They are able to find a similar number of errors in different corpora, however, as our investigation shows, the overlap between our results is quite small and the approaches are rather complementary. [sent-20, score-0.342]
</p><p>17 2  Related Work  Surprisingly, we were not able to find a lot of work on the topic of error detection in treebanks. [sent-21, score-0.399]
</p><p>18 Some organisers of shared tasks usually try to guarantee a certain quality of the used data, but the quality control is usually performed manually. [sent-22, score-0.414]
</p><p>19 in the already mentioned CoNLL task the organisers analysed a large amount of dependency treebanks for different languages [4], described problems they have encountered and forwarded them to the developers of the corresponding corpora. [sent-25, score-0.636]
</p><p>20 The only work, that we were able to find, which involved automatic quality control, was done by the already mentioned group around Detmar Meurers. [sent-26, score-0.178]
</p><p>21 This work includes numerous publications concerning finding errors in phrase structures [5] as well as in dependency treebanks [6]. [sent-27, score-0.68]
</p><p>22 The approach is based on the concept of “variation detection”, first introduced in [7]. [sent-28, score-0.041]
</p><p>23 i ac t2io0n11 fo Ar Cssoocmiaptuiotanti foonra Clo Lminpguutiast i ocns:aslh Loirntpgaupisetrics , pages 346–350, method for evaluating the automatic error detection. [sent-31, score-0.093]
</p><p>24 We will perform a similar evaluation for the precision of our approach. [sent-32, score-0.043]
</p><p>25 For space reasons, we will not be able to elaborately present this method and advise to read the referred work, However, we think that we should at least briefly explain its idea. [sent-34, score-0.068]
</p><p>26 The idea behind “variation detection” is to find strings, which occur multiple times in the corpus, but which have varying annotations. [sent-35, score-0.043]
</p><p>27 This can obviously have only two reasons: either the strings are ambiguous and can have different structures, depending on the meaning, or the annotation is erroneous in at least one of the cases. [sent-36, score-0.22]
</p><p>28 The idea can be adapted to dependency structures as well, by analysing the possible dependency relations between same words. [sent-37, score-0.486]
</p><p>29 Again different dependencies can be either the result of ambiguity or errors. [sent-38, score-0.108]
</p><p>30 We take the English dependency treebank and train models with two different state of the art parsers: the graph-  based MSTParser [9] and the transition-based MaltParser [10]. [sent-40, score-0.301]
</p><p>31 The idea behind this step is that we basically try to reproduce the gold standard, since parsing the data seen during the training is very easy (a similar idea in the area of POS tagging is very broadly described in [8]). [sent-42, score-0.23]
</p><p>32 Indeed both parsers achieve accuracies between 98% and 99% UAS (Unlabeled Attachment Score), which is defined as the proportion of correctly identified dependency relations. [sent-43, score-0.452]
</p><p>33 The reason why the parsers are not able to achieve 100% is on the one hand the fact that some of the phenomena are too rare and are not captured by their models. [sent-44, score-0.277]
</p><p>34 On the other hand, in many other cases parsers do make correct predictions, but the gold standard they are evaluated against is wrong. [sent-45, score-0.502]
</p><p>35 We have investigated the latter case, namely when both parsers predict dependencies different from the gold standard (we do not consider the correctness of the dependency label). [sent-46, score-0.808]
</p><p>36 Additionally, considering the  accuracies of 98-99% the chance that both parsers, which have different foundations, make an erroneous decision simultaneously is very small and therefore these cases are the most likely candidates when looking for errors. [sent-48, score-0.186]
</p><p>37 5  Automatic Correction of Errors  In this section we propose our algorithm for automatic correction of errors, which consists out of the following steps: 1. [sent-49, score-0.132]
</p><p>38 cases where two parsers deliver results different to gold-standard. [sent-52, score-0.313]
</p><p>39 Substitution of the annotation of the error candidates by the annotation proposed by one of the parsers (in our case MSTParser). [sent-54, score-0.494]
</p><p>40 The modifications are only kept for those cases when the modified annotation is identical with the one predicted by the third parser and undone in other cases. [sent-60, score-0.173]
</p><p>41 For the English dependency treebank we have  identified 6743 error candidates, which is about 0. [sent-61, score-0.355]
</p><p>42 The third dependency parser, which is used is MDParser1 - a fast transition-based parser. [sent-63, score-0.243]
</p><p>43 We subsitute the gold standard by MSTParser and not MaltParser in order not to give an advantage to a parser with similar basics (both MDParser and MDParser are transition-based). [sent-64, score-0.248]
</p><p>44 During this experiment we have found out that the result of MDParser significantly improves: it is able to correctly recgonize 3535 more dependencies than before the substitution of the gold standard. [sent-65, score-0.41]
</p><p>45 2077 annotations remain wrong independently of the changes in the gold standard. [sent-66, score-0.321]
</p><p>46 113 1 of the relations become wrong with the changed gold standard, whereas they were correct with the old unchanged version. [sent-67, score-0.346]
</p><p>47 We then undo the changes to the gold standard when the wrong cases remained wrong and when the correct cases became wrong. [sent-68, score-0.597]
</p><p>48 We suggest that the 3535 dependencies which became correct after the change in gold standard are 1 h ttp://mdparser. [sent-69, score-0.404]
</p><p>49 de / errors, since a) two state of the art parsers deliver a result which differs from the gold standard and b) a third parser confirms that by delivering exactly the  same result as the proposed change. [sent-72, score-0.516]
</p><p>50 However, the exact precision of the approach can probably be computed only by manual investigation of all corrected dependencies. [sent-73, score-0.043]
</p><p>51 6  Estimating the Overall Number Of Errors  The previous section tries to evaluate the precision of the approach for the identified error candidates. [sent-74, score-0.097]
</p><p>52 However, it remains unclear how many of the errors are found and how many errors can be still expected in the corpus. [sent-75, score-0.462]
</p><p>53 Therefore in this section we will describe our attempt to evaluate the recall of the proposed method. [sent-76, score-0.041]
</p><p>54 We have taken sentences of different lengths from the corpus and provided them with a “gold standard” annotation which was completely (=100%) erroneous. [sent-78, score-0.089]
</p><p>55 We have achieved that by substituting the original annotation by the annotation of a different sentence of the same length from the corpus, which did not contain dependency edges which would overlap with the original annotation. [sent-79, score-0.421]
</p><p>56 g consider the fol-  lowing sentence in the (slightly simplified) CoNLL format: 1 2 3 4 5 6 7 8 9  Not all those who wrote oppose the changes . [sent-81, score-0.199]
</p><p>57 3  P  This way we know that we have introduced a well-formed dependency tree (since its annotation belonged to a different tree before) to the corpus and the exact number of errors (since randomly correct dependencies are impossible). [sent-85, score-0.712]
</p><p>58 In case of our example 9 errors are introduced to the corpus. [sent-86, score-0.272]
</p><p>59 In our experiment we have introduced sentences of different lengths with overall 1350 tokens. [sent-87, score-0.086]
</p><p>60 We have then counted how many of these 1350 errors could be found. [sent-89, score-0.27]
</p><p>61 That means that despite the fact that the training data contained some incorrectly annotated tokens, the parsers were able to annotate them differently. [sent-92, score-0.317]
</p><p>62 Therefore we suggest that the recall of our method is close to the value of 0. [sent-93, score-0.041]
</p><p>63 However, of course we do not know whether the randomly introduced errors in our experiment are similar to those which occur in real treebanks. [sent-95, score-0.317]
</p><p>64 7  Comparison with Variation Detection  The interesting question which naturally arises at this point is whether the errors we find are the same as those found by the method of variation detection. [sent-96, score-0.447]
</p><p>65 In order for variation detection to be applicable the frequency counts for both relations must be available and the counts for the dependency proposed by the parsers should ideally greatly outweigh the frequency of the gold standard, which would be a great indication of an error. [sent-98, score-1.096]
</p><p>66 For the 3535 dependencies that we classify as errors the variation detection method works only 934 times (39. [sent-99, score-0.694]
</p><p>67 These are the cases when the gold standard is obviously wrong and occurs only few times, most often - once, whereas the parsers propose much more frequent dependencies. [sent-101, score-0.698]
</p><p>68 In all other cases the counts suggest that the variation detection would not work, since both dependencies have frequent counts or the correct dependency is even  outweighed by the incorrect one. [sent-102, score-0.89]
</p><p>69 8  Examples  We will provide some of the example errors, which we are able to find with our approach. [sent-103, score-0.111]
</p><p>70 Therefore we will provide the sentence strings and briefly compare the gold standard dependency annotation of a certain dependency within these sentences. [sent-104, score-0.866]
</p><p>71 Together, the two stocks wreaked havoc among takeover stock traders, and caused a 7. [sent-105, score-0.078]
</p><p>72 3% drop in the DOW Jones Transportation Average, second in size only to the stock-market crash of Oct. [sent-106, score-0.089]
</p><p>73 In this sentence the gold standard suggests the dependency relation market  the , whereas the parsers correctly recognise the dependency crash  the . [sent-108, score-1.182]
</p><p>74 Both dependencies have very high counts and therefore the variation detection would not work well in this scenario. [sent-109, score-0.513]
</p><p>75 In this sentence the gold standard suggests points at , whereas the parsers predict was  at . [sent-111, score-0.535]
</p><p>76 The gold standard suggestion occurs only once whereas the temporal dependency  was  at occurs 11 times in the corpus. [sent-112, score-0.647]
</p><p>77 This is an example of an error which could be found with the variation detection as well. [sent-113, score-0.409]
</p><p>78 2 million commission – for “Portrait of a Man as Mars ”. [sent-116, score-0.089]
</p><p>79 In this sentence the gold standard suggests the dependency relation $  a , whereas the parsers correctly recognise the dependency commission a . [sent-117, score-1.182]
</p><p>80 The interesting fact is that the relation $  a is actually much more frequent than commission a , e. [sent-118, score-0.089]
</p><p>81 as in the sentence he cought up an additional $1 billion or so. [sent-120, score-0.039]
</p><p>82 ( $  an ) So the variation detection alone would not suffice in this case. [sent-121, score-0.355]
</p><p>83 9  Conclusion  The quality of treebanks is of an extreme importance for the community. [sent-122, score-0.277]
</p><p>84 Nevertheless, errors can be found even in the most popular and widely-used 349 resources. [sent-123, score-0.231]
</p><p>85 In this paper we have presented an approach for automatic detection and correction of errors and compared it to the only other work we have found in this field. [sent-124, score-0.545]
</p><p>86 Our results show that both  approaches are rather complementary and find different types of errors. [sent-125, score-0.043]
</p><p>87 We have only analysed the errors in the headmodifier annotation of the dependency relations in the English dependency treebank. [sent-126, score-0.904]
</p><p>88 In fact, in the area of POS tagging a similar strategy of using the same data for training and testing in order to detect inconsistencies has proven to be very efficient [8]. [sent-130, score-0.065]
</p><p>89 However, the method lacked means for automatic correction of the possibly inconsistent annotations. [sent-131, score-0.171]
</p><p>90 Additionally, the method off course can as well be applied to different corpora in different languages. [sent-132, score-0.04]
</p><p>91 It is even more difficult to estimate the recall of our method, since the overall number of errors in a corpus is unknown. [sent-134, score-0.272]
</p><p>92 We have described an experiment which to our mind is a good attempt to evaluate the recall of our approach. [sent-135, score-0.086]
</p><p>93 On the one hand the recall we have achieved in this experiment is rather low (0. [sent-136, score-0.086]
</p><p>94 459), which means that our method would definitely not  guarantee to find all errors in a corpus. [sent-137, score-0.331]
</p><p>95 On the other hand it has a very high precision and thus is in any case beneficial, since the quality of the treebanks increases with the removal of errors. [sent-138, score-0.32]
</p><p>96 Additionally, the low recall suggests that treebanks contain an even larger number of errors, which could not be found. [sent-139, score-0.247]
</p><p>97 The overall number of errors thus seems to be over 1% of the total size of a corpus, which is expected to be of a very high quality. [sent-140, score-0.231]
</p><p>98 A fact that one has to be aware of when working with annotated resources and which we would like to emphasize with our paper. [sent-141, score-0.04]
</p><p>99 CoNLL-X  shared task on multilingual dependency parsing. [sent-152, score-0.283]
</p><p>100 In LREC 2006 workshop on Quality assurance and quality measurement for language and speech resources. [sent-156, score-0.136]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nmod', 0.289), ('dependency', 0.243), ('errors', 0.231), ('parsers', 0.209), ('treebanks', 0.206), ('gold', 0.189), ('detection', 0.182), ('mdparser', 0.178), ('variation', 0.173), ('detmar', 0.157), ('dfki', 0.148), ('meurers', 0.145), ('maltparser', 0.125), ('sbj', 0.119), ('mstparser', 0.111), ('dickinson', 0.108), ('dependencies', 0.108), ('correction', 0.093), ('cken', 0.089), ('crash', 0.089), ('oppose', 0.089), ('organisers', 0.089), ('commission', 0.089), ('annotation', 0.089), ('erroneous', 0.088), ('markus', 0.083), ('wrong', 0.079), ('stuhlsatzenhausweg', 0.079), ('whereas', 0.078), ('additionally', 0.076), ('dt', 0.074), ('pdt', 0.072), ('recognise', 0.072), ('quality', 0.071), ('neumann', 0.068), ('able', 0.068), ('assurance', 0.065), ('inconsistencies', 0.065), ('buchholz', 0.062), ('joakim', 0.061), ('erwin', 0.059), ('deliver', 0.059), ('analysed', 0.059), ('nns', 0.059), ('vbp', 0.059), ('standard', 0.059), ('treebank', 0.058), ('wrote', 0.057), ('obj', 0.057), ('sabine', 0.057), ('guarantee', 0.057), ('vbd', 0.056), ('saarbr', 0.054), ('error', 0.054), ('changes', 0.053), ('candidates', 0.053), ('lot', 0.052), ('rb', 0.051), ('surdeanu', 0.051), ('wp', 0.051), ('counts', 0.05), ('mihai', 0.05), ('became', 0.048), ('conll', 0.046), ('johansson', 0.046), ('cases', 0.045), ('control', 0.045), ('experiment', 0.045), ('find', 0.043), ('strings', 0.043), ('precision', 0.043), ('try', 0.041), ('recall', 0.041), ('introduced', 0.041), ('germany', 0.041), ('shared', 0.04), ('annotated', 0.04), ('corpora', 0.04), ('automatic', 0.039), ('counted', 0.039), ('bmwi', 0.039), ('fkz', 0.039), ('ordo', 0.039), ('techwatch', 0.039), ('theseus', 0.039), ('outweighed', 0.039), ('adriane', 0.039), ('cought', 0.039), ('forwarded', 0.039), ('gulsen', 0.039), ('headmodifier', 0.039), ('irregularities', 0.039), ('kubler', 0.039), ('lacked', 0.039), ('stocks', 0.039), ('takeover', 0.039), ('undone', 0.039), ('penn', 0.039), ('occurs', 0.039), ('nivre', 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="48-tfidf-1" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>Author: Alexander Volokh ; Gunter Neumann</p><p>Abstract: Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 1</p><p>2 0.19430453 <a title="48-tfidf-2" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>Author: Nathan Green</p><p>Abstract: Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation. .</p><p>3 0.1642139 <a title="48-tfidf-3" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>Author: Yue Zhang ; Joakim Nivre</p><p>Abstract: Transition-based dependency parsers generally use heuristic decoding algorithms but can accommodate arbitrarily rich feature representations. In this paper, we show that we can improve the accuracy of such parsers by considering even richer feature sets than those employed in previous systems. In the standard Penn Treebank setup, our novel features improve attachment score form 91.4% to 92.9%, giving the best results so far for transitionbased parsing and rivaling the best results overall. For the Chinese Treebank, they give a signficant improvement of the state of the art. An open source release of our parser is freely available.</p><p>4 0.16152008 <a title="48-tfidf-4" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao ; Kang Liu ; Li Cai</p><p>Abstract: In this paper, we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing. Conventional selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous work to wordto-word selectional preferences by using webscale data. Experiments show that web-scale data improves statistical dependency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance.</p><p>5 0.14700057 <a title="48-tfidf-5" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>Author: Jinho D. Choi ; Martha Palmer</p><p>Abstract: This paper suggests two ways of improving transition-based, non-projective dependency parsing. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. Second, we present a bootstrapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-theart performance with respect to other parsing approaches evaluated on the same data set.</p><p>6 0.14612441 <a title="48-tfidf-6" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>7 0.14385183 <a title="48-tfidf-7" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>8 0.13835688 <a title="48-tfidf-8" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>9 0.13660119 <a title="48-tfidf-9" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>10 0.12867969 <a title="48-tfidf-10" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>11 0.12704217 <a title="48-tfidf-11" href="./acl-2011-Automated_Whole_Sentence_Grammar_Correction_Using_a_Noisy_Channel_Model.html">46 acl-2011-Automated Whole Sentence Grammar Correction Using a Noisy Channel Model</a></p>
<p>12 0.1227081 <a title="48-tfidf-12" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>13 0.11004216 <a title="48-tfidf-13" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>14 0.10523692 <a title="48-tfidf-14" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>15 0.098639689 <a title="48-tfidf-15" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>16 0.095653422 <a title="48-tfidf-16" href="./acl-2011-Event_Extraction_as_Dependency_Parsing.html">122 acl-2011-Event Extraction as Dependency Parsing</a></p>
<p>17 0.088404194 <a title="48-tfidf-17" href="./acl-2011-Improving_Arabic_Dependency_Parsing_with_Form-based_and_Functional_Morphological_Features.html">164 acl-2011-Improving Arabic Dependency Parsing with Form-based and Functional Morphological Features</a></p>
<p>18 0.086452246 <a title="48-tfidf-18" href="./acl-2011-Scaling_up_Automatic_Cross-Lingual_Semantic_Role_Annotation.html">269 acl-2011-Scaling up Automatic Cross-Lingual Semantic Role Annotation</a></p>
<p>19 0.085344031 <a title="48-tfidf-19" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>20 0.083195575 <a title="48-tfidf-20" href="./acl-2011-A_Discriminative_Model_for_Joint_Morphological_Disambiguation_and_Dependency_Parsing.html">10 acl-2011-A Discriminative Model for Joint Morphological Disambiguation and Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, -0.023), (2, -0.084), (3, -0.216), (4, -0.059), (5, -0.034), (6, 0.114), (7, 0.023), (8, 0.079), (9, -0.027), (10, -0.055), (11, -0.047), (12, 0.013), (13, -0.072), (14, -0.012), (15, 0.116), (16, 0.049), (17, -0.043), (18, -0.028), (19, -0.042), (20, -0.162), (21, -0.004), (22, 0.002), (23, 0.008), (24, 0.097), (25, -0.04), (26, 0.02), (27, -0.0), (28, -0.051), (29, -0.071), (30, -0.015), (31, -0.035), (32, 0.072), (33, 0.028), (34, 0.014), (35, -0.032), (36, -0.081), (37, 0.059), (38, 0.041), (39, -0.01), (40, -0.014), (41, 0.068), (42, -0.049), (43, -0.007), (44, -0.056), (45, 0.024), (46, 0.067), (47, 0.033), (48, 0.055), (49, -0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9792397 <a title="48-lsi-1" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>Author: Alexander Volokh ; Gunter Neumann</p><p>Abstract: Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 1</p><p>2 0.81039965 <a title="48-lsi-2" href="./acl-2011-Neutralizing_Linguistically_Problematic_Annotations_in_Unsupervised_Dependency_Parsing_Evaluation.html">230 acl-2011-Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation</a></p>
<p>Author: Roy Schwartz ; Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon.</p><p>3 0.76229745 <a title="48-lsi-3" href="./acl-2011-Exploiting_Web-Derived_Selectional_Preference_to_Improve_Statistical_Dependency_Parsing.html">127 acl-2011-Exploiting Web-Derived Selectional Preference to Improve Statistical Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao ; Kang Liu ; Li Cai</p><p>Abstract: In this paper, we present a novel approach which incorporates the web-derived selectional preferences to improve statistical dependency parsing. Conventional selectional preference learning methods have usually focused on word-to-class relations, e.g., a verb selects as its subject a given nominal class. This paper extends previous work to wordto-word selectional preferences by using webscale data. Experiments show that web-scale data improves statistical dependency parsing, particularly for long dependency relationships. There is no data like more data, performance improves log-linearly with the number of parameters (unique N-grams). More importantly, when operating on new domains, we show that using web-derived selectional preferences is essential for achieving robust performance.</p><p>4 0.74581748 <a title="48-lsi-4" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>Author: Nathan Green</p><p>Abstract: Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure’s effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation. .</p><p>5 0.73097074 <a title="48-lsi-5" href="./acl-2011-Web-Scale_Features_for_Full-Scale_Parsing.html">333 acl-2011-Web-Scale Features for Full-Scale Parsing</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: Counts from large corpora (like the web) can be powerful syntactic cues. Past work has used web counts to help resolve isolated ambiguities, such as binary noun-verb PP attachments and noun compound bracketings. In this work, we first present a method for generating web count features that address the full range of syntactic attachments. These features encode both surface evidence of lexical affinities as well as paraphrase-based cues to syntactic structure. We then integrate our features into full-scale dependency and constituent parsers. We show relative error reductions of7.0% over the second-order dependency parser of McDonald and Pereira (2006), 9.2% over the constituent parser of Petrov et al. (2006), and 3.4% over a non-local constituent reranker.</p><p>6 0.72878313 <a title="48-lsi-6" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>7 0.71489865 <a title="48-lsi-7" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>8 0.71275443 <a title="48-lsi-8" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>9 0.69891101 <a title="48-lsi-9" href="./acl-2011-Partial_Parsing_from_Bitext_Projections.html">243 acl-2011-Partial Parsing from Bitext Projections</a></p>
<p>10 0.66495204 <a title="48-lsi-10" href="./acl-2011-Better_Automatic_Treebank_Conversion_Using_A_Feature-Based_Approach.html">59 acl-2011-Better Automatic Treebank Conversion Using A Feature-Based Approach</a></p>
<p>11 0.65648633 <a title="48-lsi-11" href="./acl-2011-Improving_Dependency_Parsing_with_Semantic_Classes.html">167 acl-2011-Improving Dependency Parsing with Semantic Classes</a></p>
<p>12 0.65138102 <a title="48-lsi-12" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>13 0.61986244 <a title="48-lsi-13" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>14 0.58559948 <a title="48-lsi-14" href="./acl-2011-Creating_a_manually_error-tagged_and_shallow-parsed_learner_corpus.html">88 acl-2011-Creating a manually error-tagged and shallow-parsed learner corpus</a></p>
<p>15 0.58368701 <a title="48-lsi-15" href="./acl-2011-Temporal_Restricted_Boltzmann_Machines_for_Dependency_Parsing.html">295 acl-2011-Temporal Restricted Boltzmann Machines for Dependency Parsing</a></p>
<p>16 0.57060349 <a title="48-lsi-16" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>17 0.56528753 <a title="48-lsi-17" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>18 0.55416042 <a title="48-lsi-18" href="./acl-2011-Dynamic_Programming_Algorithms_for_Transition-Based_Dependency_Parsers.html">107 acl-2011-Dynamic Programming Algorithms for Transition-Based Dependency Parsers</a></p>
<p>19 0.55179942 <a title="48-lsi-19" href="./acl-2011-Grammatical_Error_Correction_with_Alternating_Structure_Optimization.html">147 acl-2011-Grammatical Error Correction with Alternating Structure Optimization</a></p>
<p>20 0.5243237 <a title="48-lsi-20" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.055), (11, 0.239), (17, 0.044), (26, 0.034), (37, 0.148), (39, 0.059), (41, 0.062), (55, 0.017), (59, 0.029), (72, 0.055), (91, 0.034), (96, 0.144), (97, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84538496 <a title="48-lda-1" href="./acl-2011-Finding_Deceptive_Opinion_Spam_by_Any_Stretch_of_the_Imagination.html">136 acl-2011-Finding Deceptive Opinion Spam by Any Stretch of the Imagination</a></p>
<p>Author: Myle Ott ; Yejin Choi ; Claire Cardie ; Jeffrey T. Hancock</p><p>Abstract: Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming targets of opinion spam. While recent work has focused primarily on manually identifiable instances of opinion spam, in this work we study deceptive opinion spam—fictitious opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate on our gold-standard opinion spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.</p><p>same-paper 2 0.82056755 <a title="48-lda-2" href="./acl-2011-Automatic_Detection_and_Correction_of_Errors_in_Dependency_Treebanks.html">48 acl-2011-Automatic Detection and Correction of Errors in Dependency Treebanks</a></p>
<p>Author: Alexander Volokh ; Gunter Neumann</p><p>Abstract: Annotated corpora are essential for almost all NLP applications. Whereas they are expected to be of a very high quality because of their importance for the followup developments, they still contain a considerable number of errors. With this work we want to draw attention to this fact. Additionally, we try to estimate the amount of errors and propose a method for their automatic correction. Whereas our approach is able to find only a portion of the errors that we suppose are contained in almost any annotated corpus due to the nature of the process of its creation, it has a very high precision, and thus is in any case beneficial for the quality of the corpus it is applied to. At last, we compare it to a different method for error detection in treebanks and find out that the errors that we are able to detect are mostly different and that our approaches are complementary. 1</p><p>3 0.72315377 <a title="48-lda-3" href="./acl-2011-Joint_Bilingual_Sentiment_Classification_with_Unlabeled_Parallel_Corpora.html">183 acl-2011-Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora</a></p>
<p>Author: Bin Lu ; Chenhao Tan ; Claire Cardie ; Benjamin K. Tsou</p><p>Abstract: Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource-rich languages to resource-poor languages. We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data. We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language. Experiments on multiple data sets show that the proposed approach (1) outperforms the monolingual baselines, significantly improving the accuracy for both languages by 3.44%-8. 12%; (2) outperforms two standard approaches for leveraging unlabeled data; and (3) produces (albeit smaller) performance gains when employing pseudo-parallel data from machine translation engines. 1</p><p>4 0.71191162 <a title="48-lda-4" href="./acl-2011-Unsupervised_Bilingual_Morpheme_Segmentation_and_Alignment_with_Context-rich_Hidden_Semi-Markov_Models.html">318 acl-2011-Unsupervised Bilingual Morpheme Segmentation and Alignment with Context-rich Hidden Semi-Markov Models</a></p>
<p>Author: Jason Naradowsky ; Kristina Toutanova</p><p>Abstract: This paper describes an unsupervised dynamic graphical model for morphological segmentation and bilingual morpheme alignment for statistical machine translation. The model extends Hidden Semi-Markov chain models by using factored output nodes and special structures for its conditional probability distributions. It relies on morpho-syntactic and lexical source-side information (part-of-speech, morphological segmentation) while learning a morpheme segmentation over the target language. Our model outperforms a competitive word alignment system in alignment quality. Used in a monolingual morphological segmentation setting it substantially improves accuracy over previous state-of-the-art models on three Arabic and Hebrew datasets.</p><p>5 0.70660269 <a title="48-lda-5" href="./acl-2011-Coherent_Citation-Based_Summarization_of_Scientific_Papers.html">71 acl-2011-Coherent Citation-Based Summarization of Scientific Papers</a></p>
<p>Author: Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In citation-based summarization, text written by several researchers is leveraged to identify the important aspects of a target paper. Previous work on this problem focused almost exclusively on its extraction aspect (i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper). Meanwhile, the fluency of the produced summaries has been mostly ignored. For example, diversity, readability, cohesion, and ordering of the sentences included in the summary have not been thoroughly considered. This resulted in noisy and confusing summaries. In this work, we present an approach for producing readable and cohesive citation-based summaries. Our experiments show that the pro- posed approach outperforms several baselines in terms of both extraction quality and fluency.</p><p>6 0.68999779 <a title="48-lda-6" href="./acl-2011-Target-dependent_Twitter_Sentiment_Classification.html">292 acl-2011-Target-dependent Twitter Sentiment Classification</a></p>
<p>7 0.68673956 <a title="48-lda-7" href="./acl-2011-Piggyback%3A_Using_Search_Engines_for_Robust_Cross-Domain_Named_Entity_Recognition.html">246 acl-2011-Piggyback: Using Search Engines for Robust Cross-Domain Named Entity Recognition</a></p>
<p>8 0.68628204 <a title="48-lda-8" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>9 0.68146956 <a title="48-lda-9" href="./acl-2011-Using_Large_Monolingual_and_Bilingual_Corpora_to_Improve_Coordination_Disambiguation.html">331 acl-2011-Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation</a></p>
<p>10 0.67974412 <a title="48-lda-10" href="./acl-2011-Collective_Classification_of_Congressional_Floor-Debate_Transcripts.html">73 acl-2011-Collective Classification of Congressional Floor-Debate Transcripts</a></p>
<p>11 0.67890525 <a title="48-lda-11" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>12 0.67825061 <a title="48-lda-12" href="./acl-2011-An_Algorithm_for_Unsupervised_Transliteration_Mining_with_an_Application_to_Word_Alignment.html">34 acl-2011-An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment</a></p>
<p>13 0.6781283 <a title="48-lda-13" href="./acl-2011-Using_Derivation_Trees_for_Treebank_Error_Detection.html">330 acl-2011-Using Derivation Trees for Treebank Error Detection</a></p>
<p>14 0.67768222 <a title="48-lda-14" href="./acl-2011-Data_point_selection_for_cross-language_adaptation_of_dependency_parsers.html">92 acl-2011-Data point selection for cross-language adaptation of dependency parsers</a></p>
<p>15 0.67725968 <a title="48-lda-15" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>16 0.67659885 <a title="48-lda-16" href="./acl-2011-Translationese_and_Its_Dialects.html">311 acl-2011-Translationese and Its Dialects</a></p>
<p>17 0.67645168 <a title="48-lda-17" href="./acl-2011-Coreference_Resolution_with_World_Knowledge.html">85 acl-2011-Coreference Resolution with World Knowledge</a></p>
<p>18 0.67529202 <a title="48-lda-18" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>19 0.6751554 <a title="48-lda-19" href="./acl-2011-Joint_Training_of_Dependency_Parsing_Filters_through_Latent_Support_Vector_Machines.html">186 acl-2011-Joint Training of Dependency Parsing Filters through Latent Support Vector Machines</a></p>
<p>20 0.67474544 <a title="48-lda-20" href="./acl-2011-Bootstrapping_coreference_resolution_using_word_associations.html">63 acl-2011-Bootstrapping coreference resolution using word associations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
