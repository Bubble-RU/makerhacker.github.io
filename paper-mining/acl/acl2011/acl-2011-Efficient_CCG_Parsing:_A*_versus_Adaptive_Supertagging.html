<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2011" href="../home/acl2011_home.html">acl2011</a> <a title="acl-2011-112" href="#">acl2011-112</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</h1>
<br/><p>Source: <a title="acl-2011-112-pdf" href="http://aclweb.org/anthology//P/P11/P11-1158.pdf">pdf</a></p><p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.</p><p>Reference: <a title="acl-2011-112-reference" href="../acl2011_reference/acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk @ Abstract We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). [sent-5, score-0.127]
</p><p>2 First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. [sent-6, score-0.386]
</p><p>3 Next we consider several variants on A*, a classic exact  search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. [sent-7, score-0.234]
</p><p>4 In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. [sent-8, score-0.206]
</p><p>5 By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. [sent-9, score-0.21]
</p><p>6 Combining A* with adaptive supertagging decreases CPU time by 15% for our best model. [sent-10, score-0.562]
</p><p>7 Even with practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. [sent-12, score-0.273]
</p><p>8 Where a typical Penn Treebank grammar  1577 A* versus Adaptive Supertagging Adam Lopez HLTCOE Johns Hopkins University alope z @ c s . [sent-13, score-0.101]
</p><p>9 edu may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained nearly 1600. [sent-15, score-0.101]
</p><p>10 The same grammar assigns an average of 26 lexical categories per word, resulting in a very large space of possible derivations. [sent-16, score-0.211]
</p><p>11 The most successful strategy to date for efficient parsing of CCG is to first prune the set of lexical categories considered for each word, using the output of a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). [sent-17, score-0.316]
</p><p>12 However, pruning means approximate search: if a lexical category used by the highest probability derivation is pruned,  the parser will not find that derivation (§2). [sent-19, score-0.31]
</p><p>13 Since the supertagger leln nfoortc fiens no grammaticality )c. [sent-20, score-0.217]
</p><p>14 o Sninstcraein thtes it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). [sent-21, score-0.147]
</p><p>15 Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). [sent-22, score-0.469]
</p><p>16 We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from suProceedingPso orftla thned 4,9 Otrhe Agonnn,u Jauln Mee 1e9t-i2ng4, o 2f0 t1h1e. [sent-27, score-0.151]
</p><p>17 Our empirical results show that on the unpruned set of lexical categories, heuristics employed for context-free grammars show substantial speedups in hardware-independent metrics of parser effort (§4). [sent-31, score-0.247]
</p><p>18 2  CCG and Parsing Algorithms  CCG is a lexicalized grammar formalism encoding for each word lexical categories which are either basic (eg. [sent-34, score-0.211]
</p><p>19 Complex lexical categories specify the number and directionality of arguments. [sent-36, score-0.11]
</p><p>20 In parsing, adjacent spans are combined using a small number of binary combinatory rules like forward application or composition on the spanning categories (Steedman, 2000; Fowler and Penn, 2010). [sent-38, score-0.143]
</p><p>21 1 Adaptive Supertagging Supertagging (Bangalore and Joshi, 1999) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. [sent-43, score-0.11]
</p><p>22 Once the supertagger has been run, lexical categories that apply to each word in the input sentence are pruned to contain only those with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004). [sent-44, score-0.823]
</p><p>23 The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. [sent-45, score-0.249]
</p><p>24 Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a sentence-spanning derivation from the tag sequences preferred by the supertagger, since it  does not enforce grammaticality. [sent-46, score-0.116]
</p><p>25 A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004). [sent-47, score-0.532]
</p><p>26 It is based on a step function over supertagger beam ratios, relaxing the pruning threshold for lexical categories whenever the parser fails to find an analysis. [sent-48, score-0.537]
</p><p>27 However, the technique is inherently approximate: it will return a lower probability parse under the parsing model if a higher probability parse can only be constructed from a supertag sequence returned by a subsequent iteration. [sent-51, score-0.257]
</p><p>28 In this way it prioritizes speed over accuracy, although the tradeoff can be modified by adjusting the beam step function. [sent-52, score-0.223]
</p><p>29 2 A* Parsing Irrespective of whether lexical categories are pruned in advance using the output of a supertagger, the CCG parsers we are aware of all use some variant of the CKY algorithm. [sent-54, score-0.233]
</p><p>30 A* search is an agenda-based best-first graph search algorithm which finds the lowest cost parse exactly without necessarily traversing the entire search space (Klein and Manning, 2003). [sent-57, score-0.271]
</p><p>31 Instead, they are processed from a priority queue, which orders them by the product of their inside probability and a heuristic estimate of their outside probability. [sent-59, score-0.379]
</p><p>32 Provided that the heuristic never underestimates the true outside probability (i. [sent-60, score-0.282]
</p><p>33 1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. [sent-67, score-0.164]
</p><p>34 We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. [sent-69, score-0.233]
</p><p>35 For supertagging we used Dennis Mehay’s implementation, which follows Clark 1Indeed, all of the past work on A* parsing that we are aware of uses generative parsers (Pauls and Klein, 2009b, inter alia). [sent-72, score-0.525]
</p><p>36 2  1579 Due to differences in smoothing of the  supertagging and parsing models, we occasionally drop supertags returned by the supertagger because they do not appear in the parsing model 3. [sent-74, score-0.835]
</p><p>37 1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported beyond the speedups (Clark, 2002) We ran experiments to understand the time/accuracy tradeoff of adaptive supertagging, and to serve as baselines. [sent-82, score-0.289]
</p><p>38 Adaptive supertagging is parametrized by a beam size β and a dictionary cutoff k that bounds the number of lexical categories considered for each word (Clark and Curran, 2007). [sent-83, score-0.601]
</p><p>39 Table 3 shows both the standard beam levels (AST) used for the C&C; parser and looser beam levels: AST-covA, a simple extension of AST with increased coverage and AST-covB, also increasing coverage but with better performance for the HWDep model. [sent-84, score-0.246]
</p><p>40 Parsing results for the AST settings (Tables 4 and 5) confirm that it improves speed by an order of magnitude over a baseline parser without AST. [sent-85, score-0.11]
</p><p>41 This is because the parser prunes more aggressively as the search space increases. [sent-87, score-0.144]
</p><p>42 Expansion probability Head probability Non-head probability Lexical probability  p(exp|P) p(H|P, exp) p(S|P, exp, H) p(w|P)  exp ∈  {leaf, unary,  left-head,  right-head}  eHx ips ∈th {el heeafa,d u daughter S  is the non-head daughter  Table 1: Factorisation of the PCFG model. [sent-92, score-0.347]
</p><p>43 4 Table 4: Results on CCGbank section 00 when applying adaptive supertagging (AST) to two models of a generative CCG parser. [sent-196, score-0.532]
</p><p>44 0 Table 5: Results on CCGbank section 23 when applying adaptive supertagging (AST) to two models of a CCG parser. [sent-269, score-0.532]
</p><p>45 9  Table 6: Breakdown of the number of sentences parsed for the HWDep (AST) model (see Table 4) at each of the supertagger beam levels from the most to the least restrictive setting. [sent-286, score-0.299]
</p><p>46 we can perform exhaustive search without pruning7, and for which we could parse without failure at all of the tested beam settings. [sent-287, score-0.235]
</p><p>47 We then measured the log probability ofthe highest probability parse found under a variety of beam settings, relative to the log probability of the unpruned exact parse, along with the labeled F-Score of the Viterbi parse under these settings (Figure 2). [sent-288, score-0.382]
</p><p>48 In other words, the supertagger can actually “fix” a bad parsing model by restricting it to  a small portion of the search space. [sent-290, score-0.372]
</p><p>49 The next question is whether we can exploit this larger search space without paying as high a cost in efficiency. [sent-292, score-0.11]
</p><p>50 1581  Supertagger beam  Figure 2: Log-probability of parses relative to exact solution vs. [sent-294, score-0.124]
</p><p>51 Following (Klein and Manning, 2003) we restrict our experiments to sentences on which we can perform exact search via using the same subset of section 00 as in §3. [sent-297, score-0.104]
</p><p>52 We measure the number of edges pushed (Pauls and Klein, 2009a) and edges popped, corresponding to the insert/decrease-key operations and remove operation of the priority queue, respectively. [sent-302, score-0.352]
</p><p>53 It gives a bound on the outside probability of a nonterminal P with iwords to the right and j words to the left, and can be computed from a grammar using a simple dynamic program. [sent-309, score-0.249]
</p><p>54 The parsers are tested with and without adaptive supertagging where the former can be seen as performing exact search (via A*) over the pruned  search space created by AST. [sent-310, score-0.821]
</p><p>55 Table 7 shows that A* with the SX heuristic decreases the number of edges pushed by up to 39% on the unpruned search space. [sent-311, score-0.424]
</p><p>56 Although encouraging, this is not as impressive as the 95% speedup obtained by Klein and Manning (2003) with this heuristic on their CFG. [sent-312, score-0.099]
</p><p>57 On the other hand, the NULL heuristic works better for CCG than for CFG, with speedups of 29% and 11%, respectively. [sent-313, score-0.153]
</p><p>58 These results carry over to the AST setting which shows that A* can improve search even on the highly pruned search graph. [sent-314, score-0.195]
</p><p>59 Since there are many more categories in the CCG grammar we might have expected the SX heuristic to work better than for a CFG. [sent-316, score-0.279]
</p><p>60 We can measure how well a heuristic bounds the true cost in 8The NULL parser is a special case of A*, also called uniform cost search, which in the case of parsing corresponds to Knuth’s algorithm (Knuth, 1977; Klein and Manning, 2001), the extension of Dijkstra’s algorithm to hypergraphs. [sent-318, score-0.434]
</p><p>61 The figure aggregates the ratio of the difference between the estimated outside cost and true outside costs relative to the true cost across the development set. [sent-320, score-0.372]
</p><p>62 terms of slack: the difference between the true and estimated outside cost. [sent-321, score-0.138]
</p><p>63 Lower slack means that the heuristic bounds the true cost better and guides us to the exact solution more quickly. [sent-322, score-0.321]
</p><p>64 Figure 3 plots the average slack for the SX heuristic against the num-  ber of words in the outside context. [sent-323, score-0.27]
</p><p>65 Comparing this with an analysis of the same heuristic when applied to a CFG by Klein and Manning (2003), we find that it is less effective in our setting9. [sent-324, score-0.099]
</p><p>66 There is a steep increase in slack for outside contexts with size more than one. [sent-325, score-0.171]
</p><p>67 The main reason for this is because a single word in the outside context is in many cases the full stop at the end ofthe sentence, which is very predictable. [sent-326, score-0.103]
</p><p>68 However for longer spans the flexibility of CCG to analyze spans in many different ways means that the outside estimate for a nonterminal can be based on many high probability outside derivations which do not bound the true probability very well. [sent-327, score-0.331]
</p><p>69 2 Hardware-Independent Results: HWDep Lexicalization in the HWDep model makes the precomputed SX estimate impractical, so for this model we designed two hierarchical A* (HA*) variants based on simple grammar projections of the model. [sent-329, score-0.211]
</p><p>70 The basic idea of HA* is to compute Viterbi inside probabilities using the easier-to-parse projected 9Specifically, we refer to Figure 9 of their paper which slightly different representation of estimate sharpness  uses a  Parser  Edges pushed  Edges popped  Traversals  SENX UHL 2 0S43t. [sent-330, score-0.166]
</p><p>71 T3171870%3 0 Table 7: Exhaustive search (EXH), A* with no heuristic (NULL) and with the SX heuristic in terms of millions of edges pushed, popped and traversals computed using the PCFG grammar with and without adaptive supertagging. [sent-336, score-0.704]
</p><p>72 grammar, use these to compute Viterbi outside probabilities for the simple grammar, and then use these as outside estimates for the true grammar; all computations are prioritized in a single agenda following the algorithm of Felzenszwalb and McAllester (2007) and Pauls and Klein (2009a). [sent-337, score-0.278]
</p><p>73 We designed two simple grammar projections, each simplifying the HWDep model: PCFGP ro j completely removes lexicalization and projects the grammar to a PCFG, while as LexcatP ro j removes only the headwords but retains the lexical categories. [sent-338, score-0.262]
</p><p>74 Figure 4 compares exhaustive search, A* with no heuristic (NULL), and HA*. [sent-339, score-0.153]
</p><p>75 We find that A* NULL saves about 44% of edges pushed which  makes it slightly more effective than for the PCFG model. [sent-341, score-0.184]
</p><p>76 However, the effort to compute the grammar projections outweighs their benefit. [sent-342, score-0.184]
</p><p>77 We suspect that this is due to the large difference between the target grammar and the projection: The PCFG projection is a simple grammar and so we improve the probability of a traversal less often than in the target grammar. [sent-343, score-0.306]
</p><p>78 First, the projection requires about as much work to compute as the target grammar without a heuristic (NULL). [sent-345, score-0.259]
</p><p>79 This is espe-  cially true in real implementations because the savings in edges processed by an agenda parser come at a cost: operations on the priority queue data structure can add significant runtime. [sent-349, score-0.385]
</p><p>80 10 We carried out timing experiments on the best A* parsers for each model (SX and NULL for PCFG and HWDep, respectively), comparing them with our CKY implementation and the agenda-based CKY simulation EXH; we used the same data as in §3. [sent-353, score-0.147]
</p><p>81 and without adaptive supertagging average over ten runs, while Table 9 reports F-scores. [sent-357, score-0.532]
</p><p>82 Although the  timing results of the agenda-based parsers track the hardware-independent metrics, they start at a significant disadvantage to exhaustive CKY with a simple control loop. [sent-359, score-0.201]
</p><p>83 org  heap  implementation  at  Figure 4: Comparsion between a CKY simulation (EXH), A* with no heuristic (NULL), hierarchical A* (HA*) using two grammar projections for standard search (left) and AST (right). [sent-363, score-0.345]
</p><p>84 The breakdown of the inside/outside edges for the grammar projection as well as the target grammar shows that the projections, serving as the heuristic estimates for the  target grammar, are costly to compute. [sent-364, score-0.457]
</p><p>85 Standard PCFG HWDep  AST PCFG HWDep  CKY5362448934143 EXH 1251 26889 41 155 A* NULL 1032 21830 36 121 A* SX 889 34 -  Table 8: Parsing time in seconds of CKY and agendabased parsers with and without adaptive supertagging. [sent-365, score-0.257]
</p><p>86 8 -  Table 9: Labelled F-score of exact CKY and agendabased parsers with/without adaptive supertagging. [sent-380, score-0.299]
</p><p>87 Although this decreases the time required to obtain the highest accuracy, it is still a substantial tradeoff in speed compared with AST. [sent-383, score-0.113]
</p><p>88 On the other hand, the AST tradeoff improves significantly: by combining AST with A* we observe 1584  a decrease in running time of 15% for the A* NULL parser of the HWDep model over CKY. [sent-384, score-0.137]
</p><p>89 6  Conclusion and Future Work  Adaptive supertagging is a strong technique for efficient CCG parsing. [sent-386, score-0.414]
</p><p>90 However, for better models, the efficiency gains of adaptive supertagging come at the cost of accuracy. [sent-388, score-0.614]
</p><p>91 One way to look at this is that the supertagger has good precision with respect to the parser’s search space, but low recall. [sent-389, score-0.279]
</p><p>92 For instance, we might combine both parsing and supertagging models in a principled way to exploit these observations, eg. [sent-390, score-0.473]
</p><p>93 by making the supertagger output a soft constraint on the parser rather than a hard constraint. [sent-391, score-0.299]
</p><p>94 To our knowledge, we are the first to measure A* parsing speed both in terms of running time and commonly used hardware-independent metrics. [sent-393, score-0.121]
</p><p>95 It  is clear from our results that the gains from A* do not come as easily for CCG as for CFG, and that agenda-based algorithms like A* must make very large reductions in the number of edges processed to result in realtime savings, due to the added expense of keeping a priority queue. [sent-394, score-0.197]
</p><p>96 However, we have shown that A* can yield real improvements even over the highly optimized technique ofadaptive supertagging: in this pruned search space, a 44% reduction in the number of edges pushed results in a 15% speedup in CPU time. [sent-395, score-0.317]
</p><p>97 Furthermore, just as A* can be combined with adaptive supertagging, it should also combine easily with other search-space pruning methods, such as those of Djordjevic et al. [sent-396, score-0.198]
</p><p>98 In future work we plan to examine better A* heuristics for CCG, and to look at principled approaches to combine the strengths of A*, adaptive supertagging, and other techniques to the best advantage. [sent-400, score-0.152]
</p><p>99 Wide-coverage efficient statistical parsing with CCG and log-linear mod-  els. [sent-438, score-0.127]
</p><p>100 A formal basis for the heuristic determination of minimum cost paths. [sent-483, score-0.147]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hwdep', 0.409), ('supertagging', 0.38), ('ccg', 0.283), ('ast', 0.249), ('supertagger', 0.217), ('pcfg', 0.199), ('sx', 0.159), ('cky', 0.157), ('adaptive', 0.152), ('exh', 0.125), ('clark', 0.11), ('curran', 0.11), ('outside', 0.103), ('pauls', 0.103), ('grammar', 0.101), ('heuristic', 0.099), ('hockenmaier', 0.099), ('edges', 0.097), ('timing', 0.095), ('parsing', 0.093), ('steedman', 0.092), ('cpu', 0.089), ('pushed', 0.087), ('klein', 0.085), ('beam', 0.082), ('parser', 0.082), ('categories', 0.079), ('null', 0.075), ('ccgbank', 0.072), ('priority', 0.071), ('pruned', 0.071), ('slack', 0.068), ('combinatory', 0.064), ('search', 0.062), ('projection', 0.059), ('exp', 0.059), ('prioritizes', 0.058), ('tradeoff', 0.055), ('speedups', 0.054), ('daughter', 0.054), ('exhaustive', 0.054), ('np', 0.054), ('agendabased', 0.053), ('projections', 0.052), ('parsers', 0.052), ('supertags', 0.052), ('wp', 0.051), ('categorial', 0.05), ('unpruned', 0.049), ('cost', 0.048), ('traversals', 0.047), ('popped', 0.047), ('pruning', 0.046), ('cp', 0.046), ('probability', 0.045), ('cfg', 0.045), ('felzenszwalb', 0.043), ('fowler', 0.043), ('exact', 0.042), ('ha', 0.039), ('headword', 0.039), ('manning', 0.038), ('derivation', 0.037), ('agenda', 0.037), ('parse', 0.037), ('penn', 0.036), ('brodal', 0.036), ('ctop', 0.036), ('dijkstra', 0.036), ('djordjevic', 0.036), ('ehx', 0.036), ('factorisation', 0.036), ('fibonacci', 0.036), ('heaps', 0.036), ('knuth', 0.036), ('lexcat', 0.036), ('true', 0.035), ('queue', 0.034), ('efficiency', 0.034), ('efficient', 0.034), ('inside', 0.032), ('category', 0.032), ('auli', 0.031), ('kummerfeld', 0.031), ('heap', 0.031), ('lexical', 0.031), ('effort', 0.031), ('labelled', 0.031), ('viterbi', 0.031), ('decreases', 0.03), ('bangalore', 0.029), ('bounds', 0.029), ('variants', 0.029), ('headwords', 0.029), ('precomputed', 0.029), ('failures', 0.029), ('processed', 0.029), ('speed', 0.028), ('uf', 0.027), ('ur', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="112-tfidf-1" href="./acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging.html">112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.</p><p>2 0.49730116 <a title="112-tfidf-2" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.</p><p>3 0.4065299 <a title="112-tfidf-3" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>Author: Yue Zhang ; Stephen Clark</p><p>Abstract: CCGs are directly compatible with binarybranching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C; parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C.; Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.</p><p>4 0.13613625 <a title="112-tfidf-4" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>Author: Nathan Bodenstab ; Aaron Dunlop ; Keith Hall ; Brian Roark</p><p>Abstract: Efficient decoding for syntactic parsing has become a necessary research area as statistical grammars grow in accuracy and size and as more NLP applications leverage syntactic analyses. We review prior methods for pruning and then present a new framework that unifies their strengths into a single approach. Using a log linear model, we learn the optimal beam-search pruning parameters for each CYK chart cell, effectively predicting the most promising areas of the model space to explore. We demonstrate that our method is faster than coarse-to-fine pruning, exemplified in both the Charniak and Berkeley parsers, by empirically comparing our parser to the Berkeley parser using the same grammar and under identical operating conditions.</p><p>5 0.13610165 <a title="112-tfidf-5" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>Author: Nathan Bodenstab ; Kristy Hollingshead ; Brian Roark</p><p>Abstract: We present a novel pruning method for context-free parsing that increases efficiency by disallowing phrase-level unary productions in CKY chart cells spanning a single word. Our work is orthogonal to recent work on “closing” chart cells, which has focused on multi-word constituents, leaving span-1 chart cells unpruned. We show that a simple discriminative classifier can learn with high accuracy which span-1 chart cells to close to phrase-level unary productions. Eliminating these unary productions from the search can have a large impact on downstream processing, depending on implementation details of the search. We apply our method to four parsing architectures and demonstrate how it is complementary to the cell-closing paradigm, as well as other pruning methods such as coarse-to-fine, agenda, and beam-search pruning.</p><p>6 0.11776032 <a title="112-tfidf-6" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>7 0.080650769 <a title="112-tfidf-7" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>8 0.073635556 <a title="112-tfidf-8" href="./acl-2011-Effects_of_Noun_Phrase_Bracketing_in_Dependency_Parsing_and_Machine_Translation.html">111 acl-2011-Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation</a></p>
<p>9 0.067466021 <a title="112-tfidf-9" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>10 0.067109391 <a title="112-tfidf-10" href="./acl-2011-Incremental_Syntactic_Language_Models_for_Phrase-based_Translation.html">171 acl-2011-Incremental Syntactic Language Models for Phrase-based Translation</a></p>
<p>11 0.066962376 <a title="112-tfidf-11" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>12 0.066081814 <a title="112-tfidf-12" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<p>13 0.065066732 <a title="112-tfidf-13" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>14 0.064684875 <a title="112-tfidf-14" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>15 0.063021809 <a title="112-tfidf-15" href="./acl-2011-Learning_Hierarchical_Translation_Structure_with_Linguistic_Annotations.html">202 acl-2011-Learning Hierarchical Translation Structure with Linguistic Annotations</a></p>
<p>16 0.06099125 <a title="112-tfidf-16" href="./acl-2011-Language_of_Vandalism%3A_Improving_Wikipedia_Vandalism_Detection_via_Stylometric_Analysis.html">195 acl-2011-Language of Vandalism: Improving Wikipedia Vandalism Detection via Stylometric Analysis</a></p>
<p>17 0.057649039 <a title="112-tfidf-17" href="./acl-2011-Relation_Guided_Bootstrapping_of_Semantic_Lexicons.html">262 acl-2011-Relation Guided Bootstrapping of Semantic Lexicons</a></p>
<p>18 0.056580521 <a title="112-tfidf-18" href="./acl-2011-A_Word-Class_Approach_to_Labeling_PSCFG_Rules_for_Machine_Translation.html">29 acl-2011-A Word-Class Approach to Labeling PSCFG Rules for Machine Translation</a></p>
<p>19 0.055970542 <a title="112-tfidf-19" href="./acl-2011-An_Ensemble_Model_that_Combines_Syntactic_and_Semantic_Clustering_for_Discriminative_Dependency_Parsing.html">39 acl-2011-An Ensemble Model that Combines Syntactic and Semantic Clustering for Discriminative Dependency Parsing</a></p>
<p>20 0.055785351 <a title="112-tfidf-20" href="./acl-2011-Exact_Decoding_of_Syntactic_Translation_Models_through_Lagrangian_Relaxation.html">123 acl-2011-Exact Decoding of Syntactic Translation Models through Lagrangian Relaxation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2011_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.155), (1, -0.082), (2, -0.046), (3, -0.249), (4, -0.037), (5, -0.062), (6, -0.071), (7, 0.052), (8, -0.011), (9, -0.07), (10, 0.01), (11, 0.125), (12, 0.073), (13, -0.066), (14, -0.072), (15, 0.102), (16, 0.117), (17, -0.079), (18, 0.005), (19, -0.098), (20, 0.213), (21, 0.087), (22, 0.191), (23, -0.065), (24, -0.318), (25, 0.27), (26, 0.093), (27, 0.008), (28, 0.04), (29, 0.052), (30, -0.079), (31, -0.152), (32, -0.095), (33, 0.0), (34, -0.169), (35, -0.019), (36, 0.045), (37, -0.012), (38, -0.067), (39, -0.007), (40, -0.114), (41, -0.022), (42, -0.013), (43, 0.074), (44, -0.035), (45, 0.121), (46, -0.092), (47, 0.086), (48, -0.133), (49, -0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94676703 <a title="112-lsi-1" href="./acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging.html">112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.</p><p>2 0.88708889 <a title="112-lsi-2" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: Via an oracle experiment, we show that the upper bound on accuracy of a CCG parser is significantly lowered when its search space is pruned using a supertagger, though the supertagger also prunes many bad parses. Inspired by this analysis, we design a single model with both supertagging and parsing features, rather than separating them into distinct models chained together in a pipeline. To overcome the resulting increase in complexity, we experiment with both belief propagation and dual decomposition approaches to inference, the first empirical comparison of these algorithms that we are aware of on a structured natural language processing problem. On CCGbank we achieve a labelled dependency F-measure of 88.8% on gold POS tags, and 86.7% on automatic part-of-speeoch tags, the best reported results for this task.</p><p>3 0.75770152 <a title="112-lsi-3" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>Author: Yue Zhang ; Stephen Clark</p><p>Abstract: CCGs are directly compatible with binarybranching bottom-up parsing algorithms, in particular CKY and shift-reduce algorithms. While the chart-based approach has been the dominant approach for CCG, the shift-reduce method has been little explored. In this paper, we develop a shift-reduce CCG parser using a discriminative model and beam search, and compare its strengths and weaknesses with the chart-based C&C; parser. We study different errors made by the two parsers, and show that the shift-reduce parser gives competitive accuracies compared to C&C.; Considering our use of a small beam, and given the high ambiguity levels in an automatically-extracted grammar and the amount of information in the CCG lexical categories which form the shift actions, this is a surprising result.</p><p>4 0.54668319 <a title="112-lsi-4" href="./acl-2011-The_Surprising_Variance_in_Shortest-Derivation_Parsing.html">300 acl-2011-The Surprising Variance in Shortest-Derivation Parsing</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: We investigate full-scale shortest-derivation parsing (SDP), wherein the parser selects an analysis built from the fewest number of training fragments. Shortest derivation parsing exhibits an unusual range of behaviors. At one extreme, in the fully unpruned case, it is neither fast nor accurate. At the other extreme, when pruned with a coarse unlexicalized PCFG, the shortest derivation criterion becomes both fast and surprisingly effective, rivaling more complex weighted-fragment approaches. Our analysis includes an investigation of tie-breaking and associated dynamic programs. At its best, our parser achieves an accuracy of 87% F1 on the English WSJ task with minimal annotation, and 90% F1 with richer annotation.</p><p>5 0.48858854 <a title="112-lsi-5" href="./acl-2011-Optimistic_Backtracking_-_A_Backtracking_Overlay_for_Deterministic_Incremental_Parsing.html">236 acl-2011-Optimistic Backtracking - A Backtracking Overlay for Deterministic Incremental Parsing</a></p>
<p>Author: Gisle Ytrestl</p><p>Abstract: This paper describes a backtracking strategy for an incremental deterministic transitionbased parser for HPSG. The method could theoretically be implemented on any other transition-based parser with some adjustments. In this paper, the algorithm is evaluated on CuteForce, an efficient deterministic shiftreduce HPSG parser. The backtracking strategy may serve to improve existing parsers, or to assess if a deterministic parser would benefit from backtracking as a strategy to improve parsing.</p><p>6 0.38377053 <a title="112-lsi-6" href="./acl-2011-Unary_Constraints_for_Efficient_Context-Free_Parsing.html">316 acl-2011-Unary Constraints for Efficient Context-Free Parsing</a></p>
<p>7 0.38172516 <a title="112-lsi-7" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>8 0.35474181 <a title="112-lsi-8" href="./acl-2011-Joint_Hebrew_Segmentation_and_Parsing_using_a_PCFGLA_Lattice_Parser.html">184 acl-2011-Joint Hebrew Segmentation and Parsing using a PCFGLA Lattice Parser</a></p>
<p>9 0.30952483 <a title="112-lsi-9" href="./acl-2011-Metagrammar_engineering%3A_Towards_systematic_exploration_of_implemented_grammars.html">219 acl-2011-Metagrammar engineering: Towards systematic exploration of implemented grammars</a></p>
<p>10 0.30283588 <a title="112-lsi-10" href="./acl-2011-Dual_Decomposition___for_Natural_Language_Processing_.html">106 acl-2011-Dual Decomposition   for Natural Language Processing </a></p>
<p>11 0.29187211 <a title="112-lsi-11" href="./acl-2011-Types_of_Common-Sense_Knowledge_Needed_for_Recognizing_Textual_Entailment.html">315 acl-2011-Types of Common-Sense Knowledge Needed for Recognizing Textual Entailment</a></p>
<p>12 0.28668556 <a title="112-lsi-12" href="./acl-2011-Clause_Restructuring_For_SMT_Not_Absolutely_Helpful.html">69 acl-2011-Clause Restructuring For SMT Not Absolutely Helpful</a></p>
<p>13 0.28529006 <a title="112-lsi-13" href="./acl-2011-Getting_the_Most_out_of_Transition-based_Dependency_Parsing.html">143 acl-2011-Getting the Most out of Transition-based Dependency Parsing</a></p>
<p>14 0.28176385 <a title="112-lsi-14" href="./acl-2011-Optimal_Head-Driven_Parsing_Complexity_for_Linear_Context-Free_Rewriting_Systems.html">234 acl-2011-Optimal Head-Driven Parsing Complexity for Linear Context-Free Rewriting Systems</a></p>
<p>15 0.27800727 <a title="112-lsi-15" href="./acl-2011-Reversible_Stochastic_Attribute-Value_Grammars.html">267 acl-2011-Reversible Stochastic Attribute-Value Grammars</a></p>
<p>16 0.2566185 <a title="112-lsi-16" href="./acl-2011-Language-Independent_Parsing_with_Empty_Elements.html">192 acl-2011-Language-Independent Parsing with Empty Elements</a></p>
<p>17 0.25192606 <a title="112-lsi-17" href="./acl-2011-Transition-based_Dependency_Parsing_with_Rich_Non-local_Features.html">309 acl-2011-Transition-based Dependency Parsing with Rich Non-local Features</a></p>
<p>18 0.2504586 <a title="112-lsi-18" href="./acl-2011-The_ACL_Anthology_Searchbench.html">298 acl-2011-The ACL Anthology Searchbench</a></p>
<p>19 0.24721652 <a title="112-lsi-19" href="./acl-2011-Insertion_Operator_for_Bayesian_Tree_Substitution_Grammars.html">173 acl-2011-Insertion Operator for Bayesian Tree Substitution Grammars</a></p>
<p>20 0.24697615 <a title="112-lsi-20" href="./acl-2011-Judging_Grammaticality_with_Tree_Substitution_Grammar_Derivations.html">188 acl-2011-Judging Grammaticality with Tree Substitution Grammar Derivations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2011_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.302), (5, 0.044), (17, 0.05), (26, 0.021), (37, 0.086), (39, 0.051), (41, 0.061), (55, 0.019), (59, 0.042), (72, 0.014), (91, 0.042), (96, 0.102), (98, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84639579 <a title="112-lda-1" href="./acl-2011-Extracting_and_Classifying_Urdu_Multiword_Expressions.html">134 acl-2011-Extracting and Classifying Urdu Multiword Expressions</a></p>
<p>Author: Annette Hautli ; Sebastian Sulger</p><p>Abstract: This paper describes a method for automatically extracting and classifying multiword expressions (MWEs) for Urdu on the basis of a relatively small unannotated corpus (around 8.12 million tokens). The MWEs are extracted by an unsupervised method and classified into two distinct classes, namely locations and person names. The classification is based on simple heuristics that take the co-occurrence of MWEs with distinct postpositions into account. The resulting classes are evaluated against a hand-annotated gold standard and achieve an f-score of 0.5 and 0.746 for locations and persons, respectively. A target application is the Urdu ParGram grammar, where MWEs are needed to generate a more precise syntactic and semantic analysis.</p><p>2 0.83660549 <a title="112-lda-2" href="./acl-2011-Unsupervised_Discovery_of_Rhyme_Schemes.html">321 acl-2011-Unsupervised Discovery of Rhyme Schemes</a></p>
<p>Author: Sravana Reddy ; Kevin Knight</p><p>Abstract: This paper describes an unsupervised, language-independent model for finding rhyme schemes in poetry, using no prior knowledge about rhyme or pronunciation.</p><p>3 0.78362703 <a title="112-lda-3" href="./acl-2011-Lost_in_Translation%3A_Authorship_Attribution_using_Frame_Semantics.html">214 acl-2011-Lost in Translation: Authorship Attribution using Frame Semantics</a></p>
<p>Author: Steffen Hedegaard ; Jakob Grue Simonsen</p><p>Abstract: We investigate authorship attribution using classifiers based on frame semantics. The purpose is to discover whether adding semantic information to lexical and syntactic methods for authorship attribution will improve them, specifically to address the difficult problem of authorship attribution of translated texts. Our results suggest (i) that frame-based classifiers are usable for author attribution of both translated and untranslated texts; (ii) that framebased classifiers generally perform worse than the baseline classifiers for untranslated texts, but (iii) perform as well as, or superior to the baseline classifiers on translated texts; (iv) that—contrary to current belief—naïve clas- sifiers based on lexical markers may perform tolerably on translated texts if the combination of author and translator is present in the training set of a classifier.</p><p>same-paper 4 0.74262375 <a title="112-lda-4" href="./acl-2011-Efficient_CCG_Parsing%3A_A%2A_versus_Adaptive_Supertagging.html">112 acl-2011-Efficient CCG Parsing: A* versus Adaptive Supertagging</a></p>
<p>Author: Michael Auli ; Adam Lopez</p><p>Abstract: We present a systematic comparison and combination of two orthogonal techniques for efficient parsing of Combinatory Categorial Grammar (CCG). First we consider adaptive supertagging, a widely used approximate search technique that prunes most lexical categories from the parser’s search space using a separate sequence model. Next we consider several variants on A*, a classic exact search technique which to our knowledge has not been applied to more expressive grammar formalisms like CCG. In addition to standard hardware-independent measures of parser effort we also present what we believe is the first evaluation of A* parsing on the more realistic but more stringent metric of CPU time. By itself, A* substantially reduces parser effort as measured by the number of edges considered during parsing, but we show that for CCG this does not always correspond to improvements in CPU time over a CKY baseline. Combining A* with adaptive supertagging decreases CPU time by 15% for our best model.</p><p>5 0.71741933 <a title="112-lda-5" href="./acl-2011-Predicting_Relative_Prominence_in_Noun-Noun_Compounds.html">249 acl-2011-Predicting Relative Prominence in Noun-Noun Compounds</a></p>
<p>Author: Taniya Mishra ; Srinivas Bangalore</p><p>Abstract: There are several theories regarding what influences prominence assignment in English noun-noun compounds. We have developed corpus-driven models for automatically predicting prominence assignment in noun-noun compounds using feature sets based on two such theories: the informativeness theory and the semantic composition theory. The evaluation of the prediction models indicate that though both of these theories are relevant, they account for different types of variability in prominence assignment.</p><p>6 0.69000655 <a title="112-lda-6" href="./acl-2011-Age_Prediction_in_Blogs%3A_A_Study_of_Style%2C_Content%2C_and_Online_Behavior_in_Pre-_and_Post-Social_Media_Generations.html">31 acl-2011-Age Prediction in Blogs: A Study of Style, Content, and Online Behavior in Pre- and Post-Social Media Generations</a></p>
<p>7 0.57005167 <a title="112-lda-7" href="./acl-2011-A_Comparison_of_Loopy_Belief_Propagation_and_Dual_Decomposition_for_Integrated_CCG_Supertagging_and_Parsing.html">5 acl-2011-A Comparison of Loopy Belief Propagation and Dual Decomposition for Integrated CCG Supertagging and Parsing</a></p>
<p>8 0.52101123 <a title="112-lda-8" href="./acl-2011-Shift-Reduce_CCG_Parsing.html">282 acl-2011-Shift-Reduce CCG Parsing</a></p>
<p>9 0.50980449 <a title="112-lda-9" href="./acl-2011-A_Probabilistic_Modeling_Framework_for_Lexical_Entailment.html">22 acl-2011-A Probabilistic Modeling Framework for Lexical Entailment</a></p>
<p>10 0.50580674 <a title="112-lda-10" href="./acl-2011-Turn-Taking_Cues_in_a_Human_Tutoring_Corpus.html">312 acl-2011-Turn-Taking Cues in a Human Tutoring Corpus</a></p>
<p>11 0.49343011 <a title="112-lda-11" href="./acl-2011-Identifying_Word_Translations_from_Comparable_Corpora_Using_Latent_Topic_Models.html">161 acl-2011-Identifying Word Translations from Comparable Corpora Using Latent Topic Models</a></p>
<p>12 0.48198533 <a title="112-lda-12" href="./acl-2011-Part-of-Speech_Tagging_for_Twitter%3A_Annotation%2C_Features%2C_and_Experiments.html">242 acl-2011-Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments</a></p>
<p>13 0.48155069 <a title="112-lda-13" href="./acl-2011-Using_Bilingual_Parallel_Corpora_for_Cross-Lingual_Textual_Entailment.html">327 acl-2011-Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment</a></p>
<p>14 0.47862816 <a title="112-lda-14" href="./acl-2011-Exploiting_Syntactico-Semantic_Structures_for_Relation_Extraction.html">126 acl-2011-Exploiting Syntactico-Semantic Structures for Relation Extraction</a></p>
<p>15 0.47714269 <a title="112-lda-15" href="./acl-2011-Learning_to_Grade_Short_Answer_Questions_using_Semantic_Similarity_Measures_and_Dependency_Graph_Alignments.html">205 acl-2011-Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments</a></p>
<p>16 0.47708362 <a title="112-lda-16" href="./acl-2011-Unsupervised_Semantic_Role_Induction_via_Split-Merge_Clustering.html">324 acl-2011-Unsupervised Semantic Role Induction via Split-Merge Clustering</a></p>
<p>17 0.47674006 <a title="112-lda-17" href="./acl-2011-Beam-Width_Prediction_for_Efficient_Context-Free_Parsing.html">58 acl-2011-Beam-Width Prediction for Efficient Context-Free Parsing</a></p>
<p>18 0.47581714 <a title="112-lda-18" href="./acl-2011-Language_of_Vandalism%3A_Improving_Wikipedia_Vandalism_Detection_via_Stylometric_Analysis.html">195 acl-2011-Language of Vandalism: Improving Wikipedia Vandalism Detection via Stylometric Analysis</a></p>
<p>19 0.47510368 <a title="112-lda-19" href="./acl-2011-Exploring_Entity_Relations_for_Named_Entity_Disambiguation.html">128 acl-2011-Exploring Entity Relations for Named Entity Disambiguation</a></p>
<p>20 0.47476935 <a title="112-lda-20" href="./acl-2011-Can_Document_Selection_Help_Semi-supervised_Learning%3F_A_Case_Study_On_Event_Extraction.html">65 acl-2011-Can Document Selection Help Semi-supervised Learning? A Case Study On Event Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
