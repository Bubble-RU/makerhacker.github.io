<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>262 acl-2010-Word Alignment with Synonym Regularization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-262" href="#">acl2010-262</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>262 acl-2010-Word Alignment with Synonym Regularization</h1>
<br/><p>Source: <a title="acl-2010-262-pdf" href="http://aclweb.org/anthology//P/P10/P10-2025.pdf">pdf</a></p><p>Author: Hiroyuki Shindo ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality.</p><p>Reference: <a title="acl-2010-262-reference" href="../acl2010_reference/acl-2010-Word_Alignment_with_Synonym_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan { shindo , a . [sent-2, score-0.055]
</p><p>2 jp  Abstract We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. [sent-11, score-1.474]
</p><p>3 Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. [sent-12, score-1.207]
</p><p>4 We design a generative model for word alignment that uses synonym information as a regularization term. [sent-13, score-1.296]
</p><p>5 The experimental results show that our proposed method significantly improves word alignment quality. [sent-14, score-0.484]
</p><p>6 1 Introduction  Word alignment is an essential step in most phrase and syntax based statistical machine translation (SMT). [sent-15, score-0.367]
</p><p>7 It is an inference problem of word correspondences between different languages given parallel sentence pairs. [sent-16, score-0.117]
</p><p>8 Accurate word alignment can induce high quality phrase detection and translation probability, which leads to a significant improvement in SMT performance. [sent-17, score-0.424]
</p><p>9 Many word alignment approaches based on generative models have been proposed and they learn from bilingual sentences in an unsupervised manner (Vogel et al. [sent-18, score-0.799]
</p><p>10 One way to improve word alignment quality is to add linguistic knowledge derived from a monolingual corpus. [sent-20, score-0.545]
</p><p>11 This monolingual knowledge makes it easier to determine corresponding words correctly. [sent-21, score-0.121]
</p><p>12 For instance, functional words in one language tend to correspond to functional words in another language (Deng and Gao, 2007), and the syntactic dependency ofwords in each language can help the alignment process (Ma et al. [sent-22, score-0.447]
</p><p>13 It has been shown that such grammatical  information works as a constraint in word alignment models and improves word alignment quality. [sent-24, score-0.848]
</p><p>14 A large number of monolingual lexical semantic resources such as WordNet (Miller, 1995) have been constructed in more than fifty languages (Sagot and Fiser, 2008). [sent-25, score-0.121]
</p><p>15 Synonym information is particularly helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. [sent-27, score-1.207]
</p><p>16 In this paper, we explore a method for using synonym information effectively to improve word alignment quality. [sent-28, score-1.108]
</p><p>17 In general, synonym relations are defined in terms of word sense, not in terms of word form. [sent-29, score-0.75]
</p><p>18 In other words, synonym relations are usually context or domain dependent. [sent-30, score-0.636]
</p><p>19 For instance, ‘head’ and ‘chief’ are synonyms in contexts referring to working environment, while ‘head’ and ‘forefront’ are synonyms in contexts referring to physical positions. [sent-31, score-0.184]
</p><p>20 Therefore, it is easy to imagine that simply replacing all occurrences of ‘chief’ and ‘forefront’ with ‘head’ do sometimes harm with word alignment accuracy, and we have to model either the context or senses of words. [sent-33, score-0.491]
</p><p>21 We propose a novel method that incorporates synonyms from monolingual resources in a bilingual word alignment model. [sent-34, score-0.917]
</p><p>22 We formulate a synonym pair generative model with a topic variable and use this model as a regularization term with a bilingual word alignment model. [sent-35, score-1.713]
</p><p>23 The topic variable in our synonym model is helpful for disambiguating the meanings of synonyms. [sent-36, score-0.868]
</p><p>24 We extend HM-BiTAM, which is a HMM-based word alignment model with a latent topic, with a novel synonym pair generative model. [sent-37, score-1.326]
</p><p>25 We applied the proposed method to an English-French word alignment task and successfully improved the word 137  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-38, score-0.541]
</p><p>26 c C2o0n1f0er Aenscseoc Sihatoirotn P faopre Crso,m papguetsat 1io3n7a–l1 L4i1n,guistics  Figure 1: Graphical model of HM-BiTAM alignment quality. [sent-40, score-0.398]
</p><p>27 2  Bilingual Word Alignment Model  In this section, we review a conventional generative word alignment model, HM-BiTAM (Zhao and Xing, 2008). [sent-41, score-0.545]
</p><p>28 HM-BiTAM is a bilingual generative model with topic z, alignment a and topic weight vec-  tor θ as latent variables. [sent-42, score-1.013]
</p><p>29 Topic variables such as ‘science’ or ‘economy’ assigned to individual sentences help to disambiguate the meanings of words. [sent-43, score-0.055]
</p><p>30 HM-BiTAM assumes that the nth bilingual sentence pair, (En, Fn), is generated under a given latent topic zn ∈ {1, . [sent-44, score-0.528]
</p><p>31 In this framework, all of the bilingual sentence pairs {E, F} = {(En, are generated as efo pllaoiwrss {. [sent-54, score-0.344]
</p><p>32 For each sentence pair (En , Fn) (a) zn ∼ Multinomial (θ) : sample the topic (b) en,i:In |zn ∼ p (En |zn ; β ): sample English  twoopircds z fnrom a monolingual unigram model given (c) For each position jn = 1, . [sent-57, score-0.561]
</p><p>33 ajn ∼ p (ajn |ajn −1 ; T ): sample an alignment∼ ∼lin pk( ajn f|raom a first order Markov process ii. [sent-61, score-0.357]
</p><p>34 fjn ∼ p (fjn |En , ajn , zn ; B ): sample a target∼ ∼w por(df fj|nE given an aligned source word and topic where alignment  ajn  = idenotes source word  ei  and target word fjn are aligned. [sent-62, score-1.257]
</p><p>35 α is a parameter over the topic weight vector θ, β = {βk,e} is tteher source ew tooprdic probability given βthe = k {thβ topic: p (e |z = k ). [sent-63, score-0.14]
</p><p>36 B = {Bf,e,k} represents the word translation probability from {e to }f under the kth topic: p (f |e, z = k ). [sent-64, score-0.057]
</p><p>37 The total likelihood of bilingual sentence pairs {E, F} can be obtained by marginalizing out lat{eEnt, Fva}ria cbalnes b z, a taanidn eθd,  p(F,E;Ψ) =∑z∑a? [sent-68, score-0.368]
</p><p>38 In thisw model, we can iβn,fTer, wBo}rd i alignment a by maximizing the likelihood above. [sent-70, score-0.367]
</p><p>39 1 Synonym Pair Generative Model We design a generative model for synonym pairs {f, f′} in language F, which assumes that the  synonyms are cgoulalegcete Fd ,fr womhi monolingual linguistic resources. [sent-72, score-1.069]
</p><p>40 We assume that each synonym pair (f, f′) is generated independently given the same ‘sense’ s. [sent-73, score-0.694]
</p><p>41 Under this assumption, the probability of synonym pair (f, f′) can be formulated as, p(f,f′)  ∝  ∑p(f |s)p(f′|s)p(s). [sent-74, score-0.694]
</p><p>42 (2) ∑s  We define a pair (e, k) as a representation of the sense s, where e and k are a word in a different language E and a latent topic, respectively. [sent-75, score-0.217]
</p><p>43 It has been shown that a word e in a different language is an appropriate representation of s in synonym modeling (Bannard and Callison-Burch, 2005). [sent-76, score-0.693]
</p><p>44 We assume that adding a latent topic k for the sense is very useful for disambiguating word meaning, and thus that (e, k) gives us a good approximation of s. [sent-77, score-0.301]
</p><p>45 Under this assumption, the synonym pair generative model can be defined as follows. [sent-78, score-0.823]
</p><p>46 2  Wored Alignment with Synonym Regularization In this section, we extend the bilingual generative model (HM-BiTAM) with our synonym pair model. [sent-81, score-1.041]
</p><p>47 Our expectation is that synonym pairs 138  Figure 2: Graphical model of synonym pair generative process  correspond to the same word in a different language, thus they make it easy to infer accurate word alignment. [sent-82, score-1.69]
</p><p>48 HM-BiTAM and the synonym model share parameters in order to incorporate monolingual synonym information into the bilingual word alignment model. [sent-83, score-2.066]
</p><p>49 (5)  Overall, we ree-define the synonym pair model with the HM-BiTAM parameter set Ψ, p({f,f′} ;Ψ)  ∝{∑k}′1αk′(f∏,f′)∑k,eαkβk,eBf,e,kBf′,e,k. [sent-91, score-0.755]
</p><p>50 2 shows a graphical model of the synonym pair generative process. [sent-93, score-0.871]
</p><p>51 We estimate the parameter values to maximize the likelihood of HM-  BiTAM with respect to bilingual sentences and that of the synonym model with respect to synonym pairs collected from monolingual resources. [sent-94, score-1.825]
</p><p>52 Namely, the parameter estimate, Ψˆ, is computed as Ψˆ = argΨmax{logp(F,E;  Ψ) + ζ logp({f,f′} ;Ψ)} , (7)  where ζ is a regularization weight that should be set for training. [sent-95, score-0.137]
</p><p>53 7 to constrain parameter set Ψ and avoid overfitting for the bilingual word alignment model. [sent-97, score-0.699]
</p><p>54 1 Experimental Setting For an empirical evaluation of the proposed method, we used a bilingual parallel corpus of English-French Hansards (Mihalcea and Pedersen, 2003). [sent-102, score-0.278]
</p><p>55 The corpus consists of over 1 million sen-  tence pairs, which include 447 manually wordaligned sentences. [sent-103, score-0.054]
</p><p>56 We selected 100 sentence pairs randomly from the manually word-aligned sentences as development data for tuning the regularization weight ζ, and used the 347 remaining sentence pairs as evaluation data. [sent-104, score-0.383]
</p><p>57 We also randomly selected 10k, 50k, and 100k sized sentence pairs from the corpus as additional training data. [sent-105, score-0.126]
</p><p>58 We ran the unsupervised training of our proposed word alignment model on the additional training data and the 347 sentence pairs of the evaluation data. [sent-106, score-0.616]
</p><p>59 Note that manual word alignment of the 347 sentence pairs was not used for the unsupervised training. [sent-107, score-0.55]
</p><p>60 After the unsupervised training, we evaluated the word alignment performance of our proposed method by comparing the manual word alignment of the 347 sentence pairs with the prediction provided by the trained model. [sent-108, score-1.034]
</p><p>61 We collected English and French synonym pairs from WordNet 2. [sent-109, score-0.765]
</p><p>62 We  selected synonym pairs where both words were included in the bilingual training set. [sent-114, score-0.945]
</p><p>63 We compared the word alignment performance of our model with that of GIZA++ 1. [sent-115, score-0.455]
</p><p>64 We trained the word alignment in two directions: English to French, and French to English. [sent-121, score-0.424]
</p><p>65 The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). [sent-122, score-0.39]
</p><p>66 We evaluated these results for precision, recall, Fmeasure and alignment error rate (AER), which are standard metrics for word alignment accuracy (Och and Ney, 2000). [sent-123, score-0.791]
</p><p>67 html 139  10kPrecision Recall F-measure AER  HMGI-ZBAiPT+AroMposw seitdathnd SaR rHd 0 . [sent-126, score-0.083]
</p><p>68 1 269 08937 50kPrecision Recall F-measure AER  HMGI-ZBAiPT+AroMposw seitdathnd SaR rHd 0 . [sent-130, score-0.083]
</p><p>69 1 46154026 100kPrecision Recall F-measure AER  HMGI-ZBAiPT+AroMposw seitdathnd SaR rHd 0 . [sent-134, score-0.083]
</p><p>70 1 120346 Table 1: Comparison of word alignment accuracy. [sent-138, score-0.424]
</p><p>71 2  Results and Discussion  Table 1 shows the word alignment accuracy of the three methods trained with 10k, 50k, and 100k additional sentence pairs. [sent-142, score-0.459]
</p><p>72 For all settings, our proposed method outperformed other conventional methods. [sent-143, score-0.115]
</p><p>73 This result shows that synonym information is effective for improving word alignment quality as we expected. [sent-144, score-1.06]
</p><p>74 1, the main idea of our proposed method is to introduce latent topics for modeling synonym pairs, and then to utilize the synonym pair model for the regularization of word alignment models. [sent-146, score-2.083]
</p><p>75 We expect the latent topics to be useful for modeling polysemous words included in synonym pairs and to enable us to incorporate synonym information effectively into word alignment models. [sent-147, score-1.977]
</p><p>76 To confirm the effect of the synonym pair model with latent topics, we also tested GIZA++ and HM-  BiTAM with what we call Synonym Replacement Heuristics (SRH), where all of the synonym pairs in the bilingual training sentences were simply replaced with a representative word. [sent-148, score-1.8]
</p><p>77 For instance, the words ‘sick’ and ‘ill’ in the bilingual sentences  # vocabularies10k50k100k  EFrnegnlcish w s it a th n d S a R rd H 19850475 3 79 5812 70612908372 5724 213273089 217 97084  Table 2: The number of vocabularies in the 10k, 50k and 100k data sets. [sent-149, score-0.306]
</p><p>78 As shown in Table 2, the number of vocabularies in the English and French data sets decreased as a result of employing the SRH. [sent-151, score-0.039]
</p><p>79 We assume that the SRH mitigated the overfitting of these models into low-frequency word pairs in bilingual sentences, and then improved the word alignment performance. [sent-154, score-0.841]
</p><p>80 The SRH regards all of the different words coupled with the same word in the synonym pairs as synonyms. [sent-155, score-0.784]
</p><p>81 For instance, the words ‘head’, ‘chief’ and ‘forefront’ in the bilingual sentences are replaced with ‘chief’, since (‘head’, ‘chief’) and (‘head’, ‘forefront’) are synonyms. [sent-156, score-0.269]
</p><p>82 Obviously, (‘chief’, ‘forefront’) are not synonyms, which is detrimented to word alignment. [sent-157, score-0.057]
</p><p>83 The proposed method consistently outperformed GIZA++ and HM-BiTAM with the SRH in 10k, 50k and 100k data sets in F-measure. [sent-158, score-0.092]
</p><p>84 The synonym pair model in our proposed method can automatically learn that (‘head’, ‘chief’) and (‘head’, ‘forefront’) are individual synonyms with different meanings by assigning these pairs to different topics. [sent-159, score-0.999]
</p><p>85 By sharing latent topics between the synonym pair model and the word alignment model, the synonym information incorporated in  the synonym pair model is used directly for training word alignment model. [sent-160, score-3.065]
</p><p>86 The experimental results show that our proposed method was effective in improving the performance of the word alignment model by using synonym pairs including such ambiguous synonym words. [sent-161, score-1.878]
</p><p>87 As shown in Table 1, using a large number of additional sentence pairs improved the performance of all the models. [sent-163, score-0.126]
</p><p>88 In all our experimental settings, all the additional sen140  tence pairs and the evaluation data were selected from the Hansards data set. [sent-164, score-0.121]
</p><p>89 These experimental results show that a larger number of sentence pairs was more effective in improving word alignment performance when the sentence pairs were collected from a homogeneous data source. [sent-165, score-0.758]
</p><p>90 However, in practice, it might be difficult to collect a large number of such homogeneous sentence pairs for a specific target domain and language pair. [sent-166, score-0.17]
</p><p>91 One direction for future work is to confirm the effect  of the proposed method when training the word alignment model by using a large number of sentence pairs collected from various data sources including many topics for a specific language pair. [sent-167, score-0.731]
</p><p>92 5  Conclusions and Future Work  We proposed a novel framework that incorporates synonyms from monolingual linguistic resources in a word alignment generative model. [sent-168, score-0.807]
</p><p>93 This approach utilizes both bilingual and monolingual synonym resources effectively for word alignment. [sent-169, score-1.055]
</p><p>94 Our proposed method uses a latent topic for bilingual sentences and monolingual synonym pairs, which is helpful in terms of word sense disambiguation. [sent-170, score-1.356]
</p><p>95 Our proposed method improved word alignment quality with both small and large data sets. [sent-171, score-0.484]
</p><p>96 Future work will involve examining the proposed method for different language pairs such as English-Chinese and EnglishJapanese and evaluating the impact of our proposed method on SMT performance. [sent-172, score-0.211]
</p><p>97 We will also apply our proposed method to a larger data sets  of multiple domains since we can expect a further improvement in word alignment accuracy if we use more bilingual sentences and more monolingual knowledge. [sent-173, score-0.883]
</p><p>98 In Proceedings of the HLT-NAACL 2003 Workshop on building and using parallel texts: data driven machine translation and beyond-Volume 3, page 10. [sent-224, score-0.049]
</p><p>99 A systematic comparison  of various statistical alignment models. [sent-245, score-0.367]
</p><p>100 complete data: with application to scoring graphical model structures. [sent-275, score-0.079]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('synonym', 0.636), ('alignment', 0.367), ('bilingual', 0.218), ('chief', 0.193), ('forefront', 0.193), ('ajn', 0.165), ('srh', 0.155), ('monolingual', 0.121), ('topic', 0.11), ('regularization', 0.107), ('generative', 0.098), ('synonyms', 0.092), ('pairs', 0.091), ('xing', 0.088), ('zn', 0.086), ('aromposw', 0.083), ('fjn', 0.083), ('rhd', 0.083), ('seitdathnd', 0.083), ('latent', 0.079), ('giza', 0.079), ('zhao', 0.074), ('bitam', 0.072), ('aer', 0.067), ('sagot', 0.066), ('jn', 0.066), ('sar', 0.062), ('french', 0.058), ('pair', 0.058), ('word', 0.057), ('fiser', 0.055), ('hmbitam', 0.055), ('shindo', 0.055), ('topics', 0.052), ('och', 0.052), ('head', 0.05), ('ney', 0.049), ('vogel', 0.048), ('sick', 0.048), ('bannard', 0.048), ('graphical', 0.048), ('homogeneous', 0.044), ('lat', 0.044), ('en', 0.042), ('wolf', 0.041), ('hansards', 0.041), ('fn', 0.04), ('vocabularies', 0.039), ('ntt', 0.039), ('wordnet', 0.038), ('collected', 0.038), ('incorporates', 0.037), ('expect', 0.036), ('imagine', 0.036), ('sentence', 0.035), ('proposed', 0.035), ('deng', 0.033), ('bernardo', 0.032), ('disambiguating', 0.032), ('variational', 0.032), ('outperformed', 0.032), ('meanings', 0.031), ('smt', 0.031), ('model', 0.031), ('parameter', 0.03), ('association', 0.03), ('logp', 0.03), ('lab', 0.03), ('fraser', 0.03), ('tence', 0.03), ('helpful', 0.028), ('sample', 0.027), ('overfitting', 0.027), ('functional', 0.027), ('replaced', 0.027), ('mihalcea', 0.027), ('correspond', 0.026), ('rd', 0.025), ('method', 0.025), ('morristown', 0.025), ('parallel', 0.025), ('sentences', 0.024), ('miller', 0.024), ('republic', 0.024), ('marginalizing', 0.024), ('ill', 0.024), ('admixture', 0.024), ('wordaligned', 0.024), ('entitled', 0.024), ('reparameterizing', 0.024), ('aaki', 0.024), ('nagat', 0.024), ('nagata', 0.024), ('mitigated', 0.024), ('dawid', 0.024), ('page', 0.024), ('sense', 0.023), ('effectively', 0.023), ('conventional', 0.023), ('heuristics', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="262-tfidf-1" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>Author: Hiroyuki Shindo ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality.</p><p>2 0.23682754 <a title="262-tfidf-2" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>3 0.17949231 <a title="262-tfidf-3" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>Author: Zhanyi Liu ; Haifeng Wang ; Hua Wu ; Sheng Li</p><p>Abstract: This paper proposes to use monolingual collocations to improve Statistical Machine Translation (SMT). We make use of the collocation probabilities, which are estimated from monolingual corpora, in two aspects, namely improving word alignment for various kinds of SMT systems and improving phrase table for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1</p><p>4 0.17310935 <a title="262-tfidf-4" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>5 0.16538343 <a title="262-tfidf-5" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>6 0.16493978 <a title="262-tfidf-6" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>7 0.16089009 <a title="262-tfidf-7" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>8 0.15708953 <a title="262-tfidf-8" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>9 0.14985582 <a title="262-tfidf-9" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>10 0.12152546 <a title="262-tfidf-10" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>11 0.1194073 <a title="262-tfidf-11" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>12 0.1097828 <a title="262-tfidf-12" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>13 0.100233 <a title="262-tfidf-13" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>14 0.090268359 <a title="262-tfidf-14" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>15 0.089275144 <a title="262-tfidf-15" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>16 0.087029509 <a title="262-tfidf-16" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>17 0.084070571 <a title="262-tfidf-17" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>18 0.078047007 <a title="262-tfidf-18" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>19 0.075219706 <a title="262-tfidf-19" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>20 0.071306929 <a title="262-tfidf-20" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.193), (1, -0.193), (2, -0.086), (3, 0.021), (4, 0.125), (5, 0.098), (6, -0.117), (7, 0.059), (8, 0.127), (9, -0.108), (10, -0.088), (11, -0.093), (12, -0.063), (13, 0.103), (14, 0.007), (15, -0.048), (16, 0.031), (17, -0.081), (18, 0.031), (19, -0.147), (20, -0.028), (21, -0.109), (22, 0.036), (23, 0.017), (24, -0.047), (25, 0.0), (26, -0.033), (27, -0.01), (28, -0.05), (29, -0.065), (30, -0.069), (31, 0.036), (32, -0.02), (33, -0.069), (34, 0.065), (35, 0.048), (36, 0.053), (37, -0.02), (38, -0.002), (39, -0.066), (40, 0.03), (41, -0.093), (42, -0.001), (43, 0.096), (44, 0.006), (45, 0.035), (46, 0.008), (47, 0.006), (48, -0.018), (49, -0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96100998 <a title="262-lsi-1" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>Author: Hiroyuki Shindo ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality.</p><p>2 0.70737493 <a title="262-lsi-2" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>3 0.70224684 <a title="262-lsi-3" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>4 0.69251597 <a title="262-lsi-4" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>5 0.65626645 <a title="262-lsi-5" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>6 0.6477738 <a title="262-lsi-6" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>7 0.62418342 <a title="262-lsi-7" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>8 0.617562 <a title="262-lsi-8" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>9 0.61021721 <a title="262-lsi-9" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>10 0.60039496 <a title="262-lsi-10" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>11 0.55741578 <a title="262-lsi-11" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>12 0.53881484 <a title="262-lsi-12" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>13 0.52471691 <a title="262-lsi-13" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>14 0.48822555 <a title="262-lsi-14" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>15 0.46893731 <a title="262-lsi-15" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>16 0.43950668 <a title="262-lsi-16" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>17 0.4212504 <a title="262-lsi-17" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>18 0.4102723 <a title="262-lsi-18" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>19 0.39215016 <a title="262-lsi-19" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>20 0.38657528 <a title="262-lsi-20" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.296), (14, 0.034), (25, 0.042), (39, 0.011), (42, 0.022), (59, 0.111), (73, 0.046), (76, 0.012), (78, 0.014), (83, 0.06), (84, 0.02), (98, 0.23)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82756376 <a title="262-lda-1" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>Author: Peter Prettenhofer ; Benno Stein</p><p>Abstract: We present a new approach to crosslanguage text classification that builds on structural correspondence learning, a recently proposed theory for domain adaptation. The approach uses unlabeled documents, along with a simple word translation oracle, in order to induce taskspecific, cross-lingual word correspondences. We report on analyses that reveal quantitative insights about the use of unlabeled data and the complexity of interlanguage correspondence modeling. We conduct experiments in the field of cross-language sentiment classification, employing English as source language, and German, French, and Japanese as target languages. The results are convincing; they demonstrate both the robustness and the competitiveness of the presented ideas.</p><p>same-paper 2 0.80053288 <a title="262-lda-2" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>Author: Hiroyuki Shindo ; Akinori Fujino ; Masaaki Nagata</p><p>Abstract: We present a novel framework for word alignment that incorporates synonym knowledge collected from monolingual linguistic resources in a bilingual probabilistic model. Synonym information is helpful for word alignment because we can expect a synonym to correspond to the same word in a different language. We design a generative model for word alignment that uses synonym information as a regularization term. The experimental results show that our proposed method significantly improves word alignment quality.</p><p>3 0.70497596 <a title="262-lda-3" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>Author: Nancy Ide ; Collin Baker ; Christiane Fellbaum ; Rebecca Passonneau</p><p>Abstract: The Manually Annotated Sub-Corpus (MASC) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the American National Corpus. The MASC infrastructure enables the incorporation of contributed annotations into a single, usable format that can then be analyzed as it is or ported to any of a variety of other formats. MASC includes data from a much wider variety of genres than existing multiply-annotated corpora of English, and the project is committed to a fully open model of distribution, without restriction, for all data and annotations produced or contributed. As such, MASC is the first large-scale, open, communitybased effort to create much needed language resources for NLP. This paper describes the MASC project, its corpus and annotations, and serves as a call for contributions of data and annotations from the language processing community.</p><p>4 0.66600567 <a title="262-lda-4" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>Author: David Jurgens ; Keith Stevens</p><p>Abstract: We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</p><p>5 0.66520077 <a title="262-lda-5" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>Author: Wenbin Jiang ; Qun Liu</p><p>Abstract: In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this clas- , sifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.</p><p>6 0.66400695 <a title="262-lda-6" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>7 0.6621173 <a title="262-lda-7" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>8 0.65787667 <a title="262-lda-8" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>9 0.65776384 <a title="262-lda-9" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>10 0.65540826 <a title="262-lda-10" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>11 0.65352052 <a title="262-lda-11" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>12 0.65342784 <a title="262-lda-12" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>13 0.65341115 <a title="262-lda-13" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>14 0.64937043 <a title="262-lda-14" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>15 0.6479634 <a title="262-lda-15" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>16 0.64604217 <a title="262-lda-16" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>17 0.64597374 <a title="262-lda-17" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>18 0.64576608 <a title="262-lda-18" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>19 0.64570713 <a title="262-lda-19" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>20 0.64496458 <a title="262-lda-20" href="./acl-2010-Learning_Phrase-Based_Spelling_Error_Models_from_Clickthrough_Data.html">164 acl-2010-Learning Phrase-Based Spelling Error Models from Clickthrough Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
