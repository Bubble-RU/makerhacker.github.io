<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-220" href="#">acl2010-220</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</h1>
<br/><p>Source: <a title="acl-2010-220-pdf" href="http://aclweb.org/anthology//P/P10/P10-1021.pdf">pdf</a></p><p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>Reference: <a title="acl-2010-220-reference" href="../acl2010_reference/acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk, Abstract The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. [sent-10, score-0.584]
</p><p>2 Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. [sent-12, score-0.312]
</p><p>3 In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model. [sent-13, score-0.94]
</p><p>4 The first type is semantic prediction, as evidenced in semantic priming: a word that is preceded by a semantically related prime or a semantically congruous sentence fragment is processed faster (Stanovich and West 1981 ; van Berkum et al. [sent-22, score-0.457]
</p><p>5 Comprehenders are faster at naming words that are syntactically compatible with prior context, even when they bear no semantic relationship to the context (Wright and Garrett 1984). [sent-32, score-0.319]
</p><p>6 Thus, human language processing takes advantage of the constraints imposed by the preceding semantic and syntactic context to derive expectations about the upcoming input. [sent-34, score-0.561]
</p><p>7 Probably the best known measure of syntactic expectation is surprisal (Hale 2001) which can be coarsely defined as the negative log proba-  bility of word wt given the preceding words, typically computed using a probabilistic context-free grammar. [sent-37, score-0.727]
</p><p>8 Modeling work on semantic constraint focuses on the degree to which a word is related to its preceding context. [sent-38, score-0.4]
</p><p>9 The assumption is that words carry prior semantic expectations which are updated upon seeing the next word. [sent-43, score-0.327]
</p><p>10 Expectations are represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. [sent-44, score-0.437]
</p><p>11 In this paper, we investigate a model of prediction that is incremental and takes into account syntactic as well as semantic constraint. [sent-52, score-0.459]
</p><p>12 The model essentially integrates the predictions of an incremental parser (Roark 2001) together with those of a semantic space model (Mitchell and Lapata 2009). [sent-53, score-0.588]
</p><p>13 The latter creates meaning representations compositionally, and therefore builds semantic expectations for word sequences (e. [sent-54, score-0.364]
</p><p>14 2009); however, the semantic component of these models is limited to semantic role in-  formation, rather than attempting to build a full semantic representation for a sentence. [sent-58, score-0.599]
</p><p>15 The proposed model simultaneously captures semantic and syntactic effects in a single measure which we empirically show is predictive of processing difficulty as manifested in eyemovements. [sent-61, score-0.587]
</p><p>16 2  Models of Processing Difficulty  As described in Section 1, reading times provide an insight into the various cognitive activities that contribute to the overall processing difficulty involved in comprehending a written text. [sent-62, score-0.387]
</p><p>17 To quantify and understand the overall cognitive load associated with processing a word in context, we will break that load down into a sum of terms representing distinct computational costs (semantic and syntactic). [sent-63, score-0.29]
</p><p>18 For example, surprisal can be thought of as measuring the cost of dealing with unexpected input. [sent-64, score-0.386]
</p><p>19 When a word conforms to the language processor’s expectations, surprisal is low,  and the cognitive load associated with processing that input will also be low. [sent-65, score-0.502]
</p><p>20 In contrast, unexpected words will have a high surprisal and a high cognitive cost. [sent-66, score-0.456]
</p><p>21 However, high-level syntactic and semantic factors are only one source of cognitive costs. [sent-67, score-0.459]
</p><p>22 A sizable proportion of the variance in reading times is accounted for by costs associated with low-level features ofthe stimuli, e. [sent-68, score-0.387]
</p><p>23 We begin by looking at the low-level costs and move on to consider the costs associated with syntactic and semantic constraint. [sent-76, score-0.524]
</p><p>24 These include previous fixation (indicating whether or not the previous word has been fixated), launch distance (how many characters intervene between the current fixation and the previous fixation), and landing position (which letter in the word the fixation landed on). [sent-82, score-0.682]
</p><p>25 Information about the sequential context of a word can also influence reading times. [sent-83, score-0.304]
</p><p>26 Mc197  Donald and Shillcock (2003) show that forward and backward transitional probabilities are predictive of first fixation and first pass durations: the higher the transitional probability, the shorter the fixation time. [sent-84, score-0.593]
</p><p>27 Backward transitional probability is essentially the conditional probability of a word given its immediately preceding word, P(wk|wk−1 ). [sent-85, score-0.305]
</p><p>28 2 Syntactic Constraint As mentioned earlier, surprisal (Hale 2001 ; Levy 2008) is one of the best known models of processing difficulty associated with syntactic constraint, and has been previously applied to the modeling of reading times (Demberg and Keller 2008; Ferrara Boston et al. [sent-88, score-0.83]
</p><p>29 The basic idea is that the processing costs relating to the expectations of the language processor can be expressed in terms of the probabilities assigned by some form of language model to the input. [sent-91, score-0.395]
</p><p>30 The original formulation of surprisal (Hale 2001) used a probabilistic parser to calculate these probabilities, as the emphasis was on the processing costs incurred when parsing structurally ambiguous garden path sentences. [sent-101, score-0.563]
</p><p>31 For instance, unlexicalized surprisal can be easily derived by substituting the words in Equation (1)  with parts of speech (Demberg and Keller 2008). [sent-109, score-0.386]
</p><p>32 3  Semantic Constraint  Distributional models of meaning have been commonly used to quantify the semantic relation between a word and its context in computational studies of lexical processing. [sent-112, score-0.322]
</p><p>33 As LSA is one the best known semantic space models it comes as no surprise that it has been used to analyze semantic constraint. [sent-124, score-0.471]
</p><p>34 (2008) measure the similarity between the next word and its preceding context under the assumption that high similarity indicates high semantic constraint (i. [sent-126, score-0.505]
</p><p>35 They oper198  ationalize preceding contexts in two ways, either as the word immediately preceding the next word as the sentence fragment preceding it. [sent-131, score-0.29]
</p><p>36 (2008) analyze reading times on the French part of the Dundee corpus (Kennedy and Pynte 2005) and find that word-level LSA similarities are predictive of first fixation and first pass durations, whereas sentence-level LSA is only predictive of first pass duration (i. [sent-135, score-0.753]
</p><p>37 The only other model of semantic constraint we are aware of is Incremental Contextual Distinctiveness (ICD, McDonald 2000; McDonald and Brew 2004). [sent-143, score-0.344]
</p><p>38 ICD assumes that words carry prior semantic expectations which are updated upon seeing the next word. [sent-144, score-0.327]
</p><p>39 Context is represented by a vector of probabilities which reflects the likely location in semantic space of the upcoming word. [sent-145, score-0.437]
</p><p>40 In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008). [sent-149, score-0.344]
</p><p>41 Importantly, composition models are not defined with a specific semantic space in mind, they could easily be adapted to LSA, or simple co-occurrence vectors, or more sophisticated semantic representations (e. [sent-154, score-0.619]
</p><p>42 2007), although admittedly some composition functions may be better suited for particular semantic spaces. [sent-157, score-0.294]
</p><p>43 In this paper we evaluate additive and compositional models in their ability to capture semantic prediction. [sent-159, score-0.366]
</p><p>44 Under this framework, word meaning is represented as a probability distribution over a set of latent topics, essentially a vector whose dimensions correspond to topics and values to the probability of the word given these topics. [sent-173, score-0.329]
</p><p>45 In contrast to more standard semantic space models where word senses are conflated into a single representation, topics have an intuitive correspondence to coarse-grained sense distinctions. [sent-176, score-0.333]
</p><p>46 3  Integrating Semantic Constraint into Surprisal  The treatment of semantic and syntactic constraint in models of processing difficulty has been somewhat inconsistent. [sent-177, score-0.475]
</p><p>47 While surprisal is a theoretically well-motivated measure, formalizing the idea of linguistic processing being highly predictive in terms of probabilistic language models, the measurement of semantic constraint in terms of vector similarities lacks a clear motivation. [sent-178, score-0.811]
</p><p>48 Moreover, the two approaches, surprisal and similarity, produce mathematically different types of measures. [sent-179, score-0.386]
</p><p>49 Formally, it would be preferable to have a single approach to capturing constraint and the obvious solution is to derive some form of semantic surprisal rather than sticking with similarity. [sent-180, score-0.724]
</p><p>50 This can be achieved by turning a vector model of semantic similarity into a probabilistic language  model. [sent-181, score-0.292]
</p><p>51 We focus here on the model of Mitchell and Lapata (2009) which tackles the issue of the composition of semantic vectors and also integrates the output of an incremental parser. [sent-185, score-0.538]
</p><p>52 , using LDA or a simpler semantic space model) and for combining them into a representation of the prior context h (e. [sent-189, score-0.332]
</p><p>53 , using additive or multiplicative functions) produces dis-  tinct models of semantic composition. [sent-191, score-0.509]
</p><p>54 They also linearly interpolate this semantic language model with the output of an incremental parser, which computes the following probability: p(w|h) = λp1 (w|h) + (1 λ)p2(w|h) −  (9)  where p1(w|h) is computed as in Equation (7) and p2(w|(hw) his) )co ism cpoumtepdu bteyd dth aes parser. [sent-194, score-0.33]
</p><p>55 Equation (9) essentially defines a language model which combines semantic, syntactic and n-gram structure, and Mitchell and Lapata (2009) demonstrate that it improves further upon a semantic language model in terms of perplexity. [sent-198, score-0.424]
</p><p>56 We argue that the probabilities from this model give us a means to model the incrementally and predictivity of the language processor in a manner that integrates both syntactic and semantic constraints. [sent-199, score-0.54]
</p><p>57 Converting these probabilities to surprisal should result in a single measure ofthe processing cost associated with semantic and syntactic expectations. [sent-200, score-0.765]
</p><p>58 Finally, because our focus is the influence of semantic context, we selected only content words whose prior sentential context contained at least two further content words. [sent-211, score-0.317]
</p><p>59 We integrated our compositional models with a trigram model which we also trained on BLLIP. [sent-236, score-0.295]
</p><p>60 101510  Table 1: Coefficients of the baseline LME model for total reading time likelihood ratio is significant, then this indicates that the new factor significantly improves model fit. [sent-254, score-0.402]
</p><p>61 Rather than model raw reading times, we model times on the log scale. [sent-257, score-0.458]
</p><p>62 Firstly, the raw reading times tend to have a skew distribution and taking logs produces something closer to normal, which is preferable for modeling. [sent-259, score-0.307]
</p><p>63 Secondly, the regression equation makes more sense on the log scale as the contribution of each term to raw reading  time is multiplicative rather than additive. [sent-260, score-0.445]
</p><p>64 In particular, the intercept term for each participant now represents a multiplicative factor by which that participant is slower or faster. [sent-262, score-0.432]
</p><p>65 5  Results  We computed separate mixed effects models for three dependent variables, namely first fixation duration, first pass duration, and total reading time. [sent-263, score-0.512]
</p><p>66 Our strategy was to first construct a baseline model of low-level factors influencing reading time, and then to take the residuals from that model as the dependent variable in subsequent analyses. [sent-265, score-0.526]
</p><p>67 In this way we removed the effects of low-level factors before investigating the factors associated with syntactic and semantic constraint. [sent-266, score-0.576]
</p><p>68 This avoids problems with collinearity between low-level factors and the factors we are interested in (e. [sent-267, score-0.383]
</p><p>69 00262  Table 2: Coefficients of LME models including simple semantic space (SSS) or Latent Dirichlet Allocation (LDA) as factors; ∗∗∗p < . [sent-275, score-0.287]
</p><p>70 001 quency, launch distance, landing position, and the reading time for the last fixated word, and its parameter estimates are given in Table 1. [sent-276, score-0.423]
</p><p>71 Before investigating whether an integrated model of semantic and syntactic constraint improves the goodness of fit over the baseline, we examined the influence of semantic constraint alone. [sent-279, score-0.869]
</p><p>72 ’s (2008) finding, we were also interested in assessing whether the underlying semantic representation (simple semantic space or LDA) and composition function (additive versus multiplicative) modulate reading times differentially. [sent-282, score-0.791]
</p><p>73 We built an LME model that predicted the residual reading times of the baseline model using the similarity scores from our composition models as factors. [sent-283, score-0.526]
</p><p>74 We then carried out a χ2 test on the likelihood ratio of a model only containing the random factor and the intercept, and a model also containing the semantic factor (cosine similarity). [sent-284, score-0.45]
</p><p>75 The addition of the semantic factor significantly improves model fit for both the simple semantic space and LDA. [sent-285, score-0.626]
</p><p>76 This result is observed for both additive and multiplicative composition functions. [sent-286, score-0.388]
</p><p>77 Before evaluating our integrated surprisal mea-  sure, we evaluated its components individually in order to tease their contributions apart. [sent-288, score-0.474]
</p><p>78 For example, it may be the case that syntactic surprisal is an overwhelmingly better predictor of reading time than semantic surprisal, however we would not be able to detect this by simply adding a factor based on Equation (9) to the baseline model. [sent-289, score-0.94]
</p><p>79 Considering the trigram model first, we find that adding this factor to the model gives a significant improvement in fit. [sent-303, score-0.293]
</p><p>80 A(1s− −faλr) as LDA is concerned, the additive model significantly improves model fit, whereas the multiplicative one does not. [sent-306, score-0.39]
</p><p>81 These results mirror the findings of Mitchell and Lapata (2009), who report that a multiplicative composition function produced the lowest perplexity for the simple semantic space model, whereas an additive function gave the best perplexity for the LDA space. [sent-307, score-0.628]
</p><p>82 0000864107∗∗∗∗∗∗ Table 4: Coefficients of LME models with integrated surprisal measure (based on SSS or LDA) as factor  all four variants of our semantic constraint measure. [sent-312, score-0.946]
</p><p>83 Finally, we built a separate LME model where we added the integrated surprisal measure (see Equation (9)) to the model only containing the random factor and the intercept (see Table 4). [sent-313, score-0.845]
</p><p>84 We did this separately for all four versions of the integrated surprisal measure (SSS, LDA; additive, multiplicative). [sent-314, score-0.534]
</p><p>85 This correlation is taken care of by residualizing, which isolates the two factors: word frequency is part of the baseline model, while trigram probability is part of the separate models that we fit on the residuals. [sent-326, score-0.31]
</p><p>86 740 Table 5: Intercorrelations between model factors 6  Discussion  In this paper we investigated the contributions of syntactic and semantic constraint in modeling processing difficulty. [sent-359, score-0.549]
</p><p>87 Our work departs from previous approaches in that we propose a single measure which integrates syntactic and semantic factors. [sent-360, score-0.375]
</p><p>88 Evaluation on an eye-tracking corpus shows that our measure predicts reading time better than a baseline model that captures low-level factors in reading (word length, landing position, etc. [sent-361, score-0.734]
</p><p>89 Crucially, we were able to show that the semantic component of our measure improves reading time predictions over and above a model that includes  syntactic measures (based on a trigram model and incremental parser). [sent-363, score-0.843]
</p><p>90 This means that semantic costs are a significant predictor of reading time in addition to the well-known syntactic surprisal. [sent-364, score-0.607]
</p><p>91 An open issue is whether a single, integrated measure (as evaluated in Table 4) fits the eyemovement data significantly better than separate measures for trigram, syntactic, and semantic surprisal (as evaluated in Table 3. [sent-365, score-0.718]
</p><p>92 Finally, an integrated measure requires less parameters; our definition of surprisal in 12 is simply the sum of the trigram, syntactic, and semantic components. [sent-369, score-0.718]
</p><p>93 In doing so, we were able to compare different syntactic and semantic costs on the same footing. [sent-372, score-0.394]
</p><p>94 Previous analyses of semantic constraint have been conducted on different eye-tracking corpora (Dundee and Embra Corpus) and on different languages (English, French). [sent-373, score-0.329]
</p><p>95 Moreover, comparisons of the individual contributions of syntactic and semantic factors were generally absent from the literature. [sent-374, score-0.389]
</p><p>96 Our analysis showed that both of these factors can be captured by our integrated surprisal measure which is uniformly probabilistic and thus preferable to modeling semantic and syntactic costs disjointly using a mixture of probabilistic and nonprobabilistic measures. [sent-375, score-1.103]
</p><p>97 A key objective for future work will be to investigate models that integrate semantic constraint with syntactic predictions more tightly. [sent-382, score-0.415]
</p><p>98 At the same time, the semantic model should have access to syntactic information, i. [sent-386, score-0.32]
</p><p>99 Parsing costs as predictors of reading difficulty: An evaluation using the Potsdam Sentence Corpus. [sent-455, score-0.343]
</p><p>100 A distributional model of semantic context effects in lexical processing. [sent-516, score-0.347]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('surprisal', 0.386), ('reading', 0.213), ('semantic', 0.184), ('lme', 0.183), ('fixation', 0.149), ('multiplicative', 0.143), ('additive', 0.135), ('lda', 0.134), ('pynte', 0.133), ('collinearity', 0.133), ('costs', 0.13), ('factors', 0.125), ('intercept', 0.122), ('lsa', 0.113), ('composition', 0.11), ('trigram', 0.104), ('constraint', 0.104), ('expectations', 0.096), ('duration', 0.095), ('keller', 0.095), ('upcoming', 0.09), ('incremental', 0.09), ('log', 0.089), ('integrated', 0.088), ('demberg', 0.087), ('predictive', 0.085), ('wk', 0.081), ('syntactic', 0.08), ('factor', 0.077), ('mcdonald', 0.077), ('launch', 0.076), ('oglo', 0.076), ('residuals', 0.076), ('sss', 0.076), ('coefficients', 0.07), ('lapata', 0.07), ('cognitive', 0.07), ('fit', 0.069), ('roark', 0.067), ('fixated', 0.067), ('landing', 0.067), ('dundee', 0.067), ('mitchell', 0.066), ('preceding', 0.066), ('effects', 0.062), ('difficulty', 0.06), ('measure', 0.06), ('hale', 0.058), ('processor', 0.058), ('cognition', 0.057), ('altmann', 0.057), ('transitional', 0.057), ('icd', 0.057), ('intercorrelations', 0.057), ('space', 0.056), ('wn', 0.056), ('model', 0.056), ('psycholinguistic', 0.056), ('probabilities', 0.055), ('vector', 0.052), ('integrates', 0.051), ('staub', 0.05), ('preferable', 0.05), ('latent', 0.049), ('pad', 0.049), ('prediction', 0.049), ('essentially', 0.048), ('griffiths', 0.048), ('frank', 0.048), ('eye', 0.048), ('prior', 0.047), ('parser', 0.047), ('vectors', 0.047), ('models', 0.047), ('word', 0.046), ('fixations', 0.046), ('clifton', 0.046), ('tensor', 0.046), ('participant', 0.045), ('context', 0.045), ('times', 0.044), ('probability', 0.044), ('representing', 0.044), ('faster', 0.043), ('gaze', 0.043), ('movement', 0.042), ('landauer', 0.042), ('pass', 0.041), ('analyses', 0.041), ('matrix', 0.041), ('nested', 0.041), ('sentential', 0.041), ('fitted', 0.041), ('brew', 0.041), ('bllip', 0.041), ('rayner', 0.039), ('priming', 0.039), ('representations', 0.038), ('semantics', 0.038), ('bellegarda', 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="220-tfidf-1" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>2 0.34490615 <a title="220-tfidf-2" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>Author: Frank Keller</p><p>Abstract: We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, specifically selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed.</p><p>3 0.27482259 <a title="220-tfidf-3" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>Author: Stephen Wu ; Asaf Bachrach ; Carlos Cardenas ; William Schuler</p><p>Abstract: Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.</p><p>4 0.25396737 <a title="220-tfidf-4" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>Author: Amit Dubey</p><p>Abstract: Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. Our simulations show that a Weakly Interactive cognitive architecture can explain data which had been provided as evidence for the Strongly Interactive hypothesis.</p><p>5 0.19795726 <a title="220-tfidf-5" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>Author: Klinton Bicknell ; Roger Levy</p><p>Abstract: A number of results in the study of realtime sentence comprehension have been explained by computational models as resulting from the rational use of probabilistic linguistic information. Many times, these hypotheses have been tested in reading by linking predictions about relative word difficulty to word-aggregated eye tracking measures such as go-past time. In this paper, we extend these results by asking to what extent reading is well-modeled as rational behavior at a finer level of analysis, predicting not aggregate measures, but the duration and location of each fixation. We present a new rational model of eye movement control in reading, the central assumption of which is that eye move- ment decisions are made to obtain noisy visual information as the reader performs Bayesian inference on the identities of the words in the sentence. As a case study, we present two simulations demonstrating that the model gives a rational explanation for between-word regressions.</p><p>6 0.17867193 <a title="220-tfidf-6" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>7 0.16049971 <a title="220-tfidf-7" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>8 0.13663986 <a title="220-tfidf-8" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>9 0.13075341 <a title="220-tfidf-9" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>10 0.11913411 <a title="220-tfidf-10" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>11 0.1117034 <a title="220-tfidf-11" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>12 0.11142611 <a title="220-tfidf-12" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>13 0.094070293 <a title="220-tfidf-13" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>14 0.088934928 <a title="220-tfidf-14" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>15 0.088369861 <a title="220-tfidf-15" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>16 0.087351233 <a title="220-tfidf-16" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>17 0.086302176 <a title="220-tfidf-17" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>18 0.079258069 <a title="220-tfidf-18" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>19 0.076838501 <a title="220-tfidf-19" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>20 0.076702699 <a title="220-tfidf-20" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.266), (1, 0.101), (2, 0.029), (3, -0.096), (4, -0.001), (5, -0.025), (6, 0.022), (7, -0.077), (8, 0.143), (9, -0.019), (10, -0.117), (11, 0.097), (12, 0.291), (13, 0.286), (14, -0.163), (15, 0.299), (16, 0.037), (17, -0.012), (18, -0.155), (19, 0.002), (20, -0.116), (21, 0.048), (22, 0.066), (23, 0.15), (24, 0.045), (25, 0.058), (26, -0.026), (27, -0.16), (28, 0.007), (29, -0.014), (30, -0.015), (31, -0.038), (32, 0.004), (33, 0.013), (34, 0.054), (35, -0.01), (36, -0.044), (37, 0.016), (38, 0.078), (39, -0.076), (40, -0.017), (41, -0.029), (42, 0.041), (43, -0.032), (44, 0.065), (45, 0.028), (46, 0.018), (47, 0.015), (48, -0.003), (49, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92695075 <a title="220-lsi-1" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>2 0.90602314 <a title="220-lsi-2" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>Author: Stephen Wu ; Asaf Bachrach ; Carlos Cardenas ; William Schuler</p><p>Abstract: Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.</p><p>3 0.90335506 <a title="220-lsi-3" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>Author: Frank Keller</p><p>Abstract: We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, specifically selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed.</p><p>4 0.80838066 <a title="220-lsi-4" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>Author: Klinton Bicknell ; Roger Levy</p><p>Abstract: A number of results in the study of realtime sentence comprehension have been explained by computational models as resulting from the rational use of probabilistic linguistic information. Many times, these hypotheses have been tested in reading by linking predictions about relative word difficulty to word-aggregated eye tracking measures such as go-past time. In this paper, we extend these results by asking to what extent reading is well-modeled as rational behavior at a finer level of analysis, predicting not aggregate measures, but the duration and location of each fixation. We present a new rational model of eye movement control in reading, the central assumption of which is that eye move- ment decisions are made to obtain noisy visual information as the reader performs Bayesian inference on the identities of the words in the sentence. As a case study, we present two simulations demonstrating that the model gives a rational explanation for between-word regressions.</p><p>5 0.77727258 <a title="220-lsi-5" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>Author: Amit Dubey</p><p>Abstract: Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. Our simulations show that a Weakly Interactive cognitive architecture can explain data which had been provided as evidence for the Strongly Interactive hypothesis.</p><p>6 0.55085021 <a title="220-lsi-6" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>7 0.4997547 <a title="220-lsi-7" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>8 0.42944843 <a title="220-lsi-8" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>9 0.42619035 <a title="220-lsi-9" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>10 0.40641829 <a title="220-lsi-10" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>11 0.40123144 <a title="220-lsi-11" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>12 0.3963235 <a title="220-lsi-12" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>13 0.39252159 <a title="220-lsi-13" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>14 0.39225832 <a title="220-lsi-14" href="./acl-2010-Modeling_Norms_of_Turn-Taking_in_Multi-Party_Conversation.html">173 acl-2010-Modeling Norms of Turn-Taking in Multi-Party Conversation</a></p>
<p>15 0.38864356 <a title="220-lsi-15" href="./acl-2010-Online_Generation_of_Locality_Sensitive_Hash_Signatures.html">183 acl-2010-Online Generation of Locality Sensitive Hash Signatures</a></p>
<p>16 0.38275406 <a title="220-lsi-16" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>17 0.36246502 <a title="220-lsi-17" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>18 0.35922596 <a title="220-lsi-18" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>19 0.35853642 <a title="220-lsi-19" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<p>20 0.34820965 <a title="220-lsi-20" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.018), (25, 0.035), (37, 0.044), (42, 0.015), (44, 0.012), (59, 0.091), (73, 0.033), (78, 0.057), (83, 0.101), (84, 0.389), (98, 0.108)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96883255 <a title="220-lda-1" href="./acl-2010-GernEdiT_-_The_GermaNet_Editing_Tool.html">126 acl-2010-GernEdiT - The GermaNet Editing Tool</a></p>
<p>Author: Verena Henrich ; Erhard Hinrichs</p><p>Abstract: GernEdiT (short for: GermaNet Editing Tool) offers a graphical interface for the lexicographers and developers of GermaNet to access and modify the underlying GermaNet resource. GermaNet is a lexical-semantic wordnet that is modeled after the Princeton WordNet for English. The traditional lexicographic development of GermaNet was error prone and time-consuming, mainly due to a complex underlying data format and no opportunity of automatic consistency checks. GernEdiT replaces the earlier development by a more userfriendly tool, which facilitates automatic checking of internal consistency and correctness of the linguistic resource. This paper pre- sents all these core functionalities of GernEdiT along with details about its usage and usability. 1</p><p>2 0.951846 <a title="220-lda-2" href="./acl-2010-Estimating_Strictly_Piecewise_Distributions.html">103 acl-2010-Estimating Strictly Piecewise Distributions</a></p>
<p>Author: Jeffrey Heinz ; James Rogers</p><p>Abstract: Strictly Piecewise (SP) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data.</p><p>same-paper 3 0.8705163 <a title="220-lda-3" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>4 0.84806269 <a title="220-lda-4" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>Author: Michael Connor ; Yael Gertner ; Cynthia Fisher ; Dan Roth</p><p>Abstract: A fundamental step in sentence comprehension involves assigning semantic roles to sentence constituents. To accomplish this, the listener must parse the sentence, find constituents that are candidate arguments, and assign semantic roles to those constituents. Each step depends on prior lexical and syntactic knowledge. Where do children learning their first languages begin in solving this problem? In this paper we focus on the parsing and argumentidentification steps that precede Semantic Role Labeling (SRL) training. We combine a simplified SRL with an unsupervised HMM part of speech tagger, and experiment with psycholinguisticallymotivated ways to label clusters resulting from the HMM so that they can be used to parse input for the SRL system. The results show that proposed shallow representations of sentence structure are robust to reductions in parsing accuracy, and that the contribution of alternative representations of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argumentidentification stages.</p><p>5 0.77338547 <a title="220-lda-5" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>Author: Georgios Paltoglou ; Mike Thelwall</p><p>Abstract: Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.</p><p>6 0.71709251 <a title="220-lda-6" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>7 0.69106627 <a title="220-lda-7" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>8 0.67817426 <a title="220-lda-8" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>9 0.60082358 <a title="220-lda-9" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>10 0.57466686 <a title="220-lda-10" href="./acl-2010-String_Extension_Learning.html">217 acl-2010-String Extension Learning</a></p>
<p>11 0.56614596 <a title="220-lda-11" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>12 0.55013567 <a title="220-lda-12" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>13 0.54437792 <a title="220-lda-13" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>14 0.5410676 <a title="220-lda-14" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>15 0.53816628 <a title="220-lda-15" href="./acl-2010-Expanding_Verb_Coverage_in_Cyc_with_VerbNet.html">108 acl-2010-Expanding Verb Coverage in Cyc with VerbNet</a></p>
<p>16 0.53741533 <a title="220-lda-16" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>17 0.53560024 <a title="220-lda-17" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>18 0.53392011 <a title="220-lda-18" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>19 0.53375536 <a title="220-lda-19" href="./acl-2010-Multilingual_Pseudo-Relevance_Feedback%3A_Performance_Study_of_Assisting_Languages.html">177 acl-2010-Multilingual Pseudo-Relevance Feedback: Performance Study of Assisting Languages</a></p>
<p>20 0.52918231 <a title="220-lda-20" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
