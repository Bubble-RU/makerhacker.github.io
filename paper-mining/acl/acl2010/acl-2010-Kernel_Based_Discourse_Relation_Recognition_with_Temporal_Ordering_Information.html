<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-155" href="#">acl2010-155</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</h1>
<br/><p>Source: <a title="acl-2010-155-pdf" href="http://aclweb.org/anthology//P/P10/P10-1073.pdf">pdf</a></p><p>Author: WenTing Wang ; Jian Su ; Chew Lim Tan</p><p>Abstract: Syntactic knowledge is important for discourse relation recognition. Yet only heuristically selected flat paths and 2-level production rules have been used to incorporate such information so far. In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations. The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well. University of Singapore Singapore 117417 sg tacl @ comp .nus .edu . sg 1</p><p>Reference: <a title="acl-2010-155-reference" href="../acl2010_reference/acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 2  ,  Abstract Syntactic knowledge is important for discourse relation recognition. [sent-4, score-0.582]
</p><p>2 In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. [sent-6, score-1.814]
</p><p>3 These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification  for both explicit and implicit relations. [sent-7, score-1.734]
</p><p>4 The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. [sent-8, score-0.869]
</p><p>5 We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. [sent-9, score-1.269]
</p><p>6 Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2. [sent-10, score-1.466]
</p><p>7 (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. [sent-19, score-0.427]
</p><p>8 This shows the importance of syntactic knowledge to discourse analysis. [sent-20, score-0.512]
</p><p>9 Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. [sent-26, score-0.811]
</p><p>10 Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains un-  clear what kinds of syntactic heuristics are effective for discourse analysis. [sent-28, score-0.643]
</p><p>11 In this paper we propose using tree kernel based method to automatically mine the syntactic 710 ProceedinUgspp osfa tlhae, 4S8wthed Aennn,u 1a1l-1 M6e Jeutilnyg 2 o0f1 t0h. [sent-32, score-0.686]
</p><p>12 c As2s0o1c0ia Atisosnoc foiart Cionom fopru Ctaotmiopnuatla Lti onngaulis Lti cnsg,u piasgtiecs 710–719, information from the parse trees for discourse analysis, applying kernel function to the parse tree structures directly. [sent-34, score-1.214]
</p><p>13 These structural syntactic features, together with other flat features are then incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification. [sent-35, score-1.345]
</p><p>14 The experiment shows that tree kernel is able to effectively incorporate syntactic structural information and produce statistical significant improvements over flat syntactic path feature for the recognition of both explicit and implicit relation in Penn Dis-  course Treebank (PDTB; Prasad et al. [sent-36, score-1.694]
</p><p>15 We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further work on a higher dimensional space for possible better discrimination. [sent-38, score-1.253]
</p><p>16 Section 3 gives the related work on tree kernel approach in NLP and its difference with production rules, and also linguistic study on tense and discourse anaphor. [sent-43, score-1.21]
</p><p>17 Section 4 introduces the frame work for discourse recognition, as well as the baseline feature space and the SVM classifier. [sent-44, score-0.473]
</p><p>18 2  We conclude our works in Sec-  Penn Discourse Tree Bank  The Penn Discourse Treebank (PDTB) is the largest available annotated corpora of discourse relations (Prasad et al. [sent-48, score-0.519]
</p><p>19 The PDTB models discourse relation in the predicate-argument view, where a discourse connective (e. [sent-50, score-1.342]
</p><p>20 The argument that the discourse connective syntactically bounds to is called Arg2, and the other argument is called Arg1. [sent-53, score-0.828]
</p><p>21 The PDTB provides annotations for both explicit and implicit discourse relations. [sent-54, score-0.816]
</p><p>22 An explicit relation is triggered by an explicit connective. [sent-55, score-0.477]
</p><p>23 Example (1) shows an explicit Contrast relation signaled by the discourse connective ‘but’ ’. [sent-56, score-1.076]
</p><p>24 The annotators insert a connective expression that best conveys the inferred implicit relation between adjacent sentences within the same paragraph. [sent-66, score-0.789]
</p><p>25 In Example (2), the annotators select ‘because ’ as the most appropriate connective to express the inferred Causal relation between the sentences. [sent-67, score-0.53]
</p><p>26 There is one special label AltLex pre-defined for cases where the insertion of an Implicit connective to express an inferred relation led to a redundancy in the expression of the relation. [sent-68, score-0.53]
</p><p>27 In Example (3), the Causal relation derived between sentences is alternatively lexicalized by some non-connective expression shown in square brackets, so no implicit connective is inserted. [sent-69, score-0.745]
</p><p>28 , 2001) as in Example (4); and (b) No relation where no discourse or entity-based coherence relation can be inferred between adjacent sentences. [sent-83, score-0.81]
</p><p>29 In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). [sent-96, score-0.525]
</p><p>30 This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. [sent-97, score-0.505]
</p><p>31 Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001 ; Moschitti, 2004) and relation extraction (Zelenko et al. [sent-98, score-0.62]
</p><p>32 However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. [sent-101, score-0.623]
</p><p>33 (2009) has explored the 2-level production rules for discourse analysis. [sent-103, score-0.513]
</p><p>34 ) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better  discrimination. [sent-117, score-0.772]
</p><p>35 1 used by 2-level production rules and convolution tree  kernel approaches. [sent-126, score-0.782]
</p><p>36 Our observation on temporal ordering information is in line with the above, which is also incorporated in our discourse analyzer. [sent-161, score-0.8]
</p><p>37 During training, for each discourse relation encountered, a positive instance is created by pairing the two arguments. [sent-164, score-0.582]
</p><p>38 During resolution, (a) clauses within same sentence and sentences within three-sentence spans are paired to form an explicit testing instance; and (b) neighboring sentences within three-sentence spans are paired to form an implicit testing instance. [sent-167, score-0.52]
</p><p>39 The instance is presented to each explicit or implicit relation classifier which then returns a class label with a confidence value indicating the likelihood that the candidate pair holds a particular discourse relation. [sent-168, score-0.971]
</p><p>40 All these base features have been proved effective for discourse analysis in previous work. [sent-173, score-0.574]
</p><p>41 2 Support Vector Machine In theory, any discriminative learning algorithm is applicable to learn the classifier for discourse analysis. [sent-176, score-0.427]
</p><p>42 > One advantage of SVM is that we can use tree kernel approach to capture syntactic parse tree information in a particular high-dimension space. [sent-213, score-0.968]
</p><p>43 In the next section, we will discuss how to use kernel to incorporate the more complex structure  01. [sent-214, score-0.411]
</p><p>44 5  Incorporating Information  Structural  Syntactic  A parse tree that covers both discourse arguments could provide us much syntactic information related to the pair. [sent-216, score-0.889]
</p><p>45 Both the syntactic flat path connecting connective and arguments and the 2-level production rules in the parse tree used in previous study can be directly described by the tree structure. [sent-217, score-1.301]
</p><p>46 Other syntactic knowledge that may be helpful for discourse resolution could also be implicitly represented in the tree. [sent-218, score-0.512]
</p><p>47 Therefore, by comparing the common sub-structures between two trees we can find out to which level two trees contain similar syntactic information, which can be done using a convolution tree kernel. [sent-219, score-0.503]
</p><p>48 The value returned from the tree kernel reflects the similarity between two instances in syntax. [sent-220, score-0.596]
</p><p>49 However, in many cases two discourse arguments do not occur in the same sentence. [sent-225, score-0.516]
</p><p>50 To present their syntactic properties and relations in a single tree structure, we construct a syntax tree for each paragraph by attaching the parsing trees of all its sentences to an upper paragraph node. [sent-226, score-0.728]
</p><p>51 In this paper, we only consider discourse relations within 3 sentences, which only occur within each pa1 In our task, the result of ? [sent-227, score-0.519]
</p><p>52 Our 3-sentence spans cover 95% discourse relation cases in PDTB v2. [sent-232, score-0.635]
</p><p>53 Having obtained the parse tree of a paragraph, we shall consider how to select the appropriate portion of the tree as the structured feature for a  given instance. [sent-234, score-0.621]
</p><p>54 In our study, we examine three structured features that contain different substructures of the paragraph parse tree: Min-Expansion This feature records the minimal structure covering both arguments and connective word in the parse tree. [sent-237, score-0.818]
</p><p>55 ” Simple-Expansion Min-Expansion could, to some degree, describe the syntactic relationships between the connective and arguments. [sent-246, score-0.418]
</p><p>56 Min-Expansion tree built from golden standard parse tree for the explicit disc ourse relation in Example (5). [sent-249, score-0.836]
</p><p>57 Simple-Expansion tree for the explicit discourse relation in Example (5). [sent-252, score-0.945]
</p><p>58 Full-Expansion tree for the explicit discourse relation in Example (5). [sent-263, score-0.945]
</p><p>59 2  Convolution Parse Tree Kernel  Given the parse tree defined above, we use the same convolution tree kernel as described in (Collins and Duffy, 2002) and (Moschitti, 2004). [sent-265, score-0.984]
</p><p>60 To solve the computational issue, a tree kernel function is introduced to calculate the dot product between the above high dimensional vectors efficiently. [sent-322, score-0.597]
</p><p>61 The parse tree kernel counts the number of common sub-trees as the syntactic similarity measure between two instances. [sent-413, score-0.741]
</p><p>62 3  Composite Tree Kernel  Besides the above convolution parse tree kernel ? [sent-419, score-0.782]
</p><p>63 to capture other flat features, such as base features (described in Table 1) and temporal ordering information (described in Section 6). [sent-436, score-0.636]
</p><p>64 In our study, the composite kernel is defined in the following way:  ? [sent-437, score-0.431]
</p><p>65 6  Using Temporal tion  Ordering  Informa-  In our discourse analyzer, we also add in temporal information to be used as features to predict discourse relations. [sent-472, score-1.087]
</p><p>66 This is because both our observations and some linguistic studies (Webber, 1988) show that temporal ordering information including tense, aspectual and event orders  between two arguments may constrain the discourse relation type. [sent-473, score-1.212]
</p><p>67 For example, the connective 715  word is the same in both Example (6) and (7), but the tense shift from progressive form in clause 6. [sent-474, score-0.496]
</p><p>68 b, indicating that the twisting occurred during the state of running the marathon, usually signals a temporal discourse relation; while in Example (7), both clauses are in past tense and it is marked as a Causal relation. [sent-476, score-0.787]
</p><p>69 Inspired by the linguistic model from Webber (1988) as described in Section 3, we explore the temporal order of events in two adjacent sentences for discourse relation interpretation. [sent-485, score-0.804]
</p><p>70 We notice that the feasible temporal order of events differs for different discourse relations. [sent-521, score-0.618]
</p><p>71 Then the tense and temporal ordering  information is extracted as features for discourse relation recognition. [sent-558, score-1.094]
</p><p>72 edu/tarsqi/ 7  Experiments and Results  In this section we provide the results of a set of experiments focused on the task of simultaneous discourse identification and classification. [sent-561, score-0.545]
</p><p>73 Besides four top-level discourse relations, we also consider Entity and No relations described in Section 2. [sent-565, score-0.519]
</p><p>74 2 System with Structural Kernel Table 2 lists the performance of simultaneous identification and classification on level-1 discourse senses. [sent-612, score-0.545]
</p><p>75 We can see that all our tree kernels outperform the manually constructed flat path feature in all three groups including Explicit only, Implicit only and All relations, with the accuracy increasing by 1. [sent-618, score-0.516]
</p><p>76 6  Base + Manually selected flat path features Base + Tree kernel  70. [sent-625, score-0.598]
</p><p>77 Results of the syntactic structured kerne ls on level-1 discourse relation recognition. [sent-637, score-0.752]
</p><p>78 This proves that structural syntactic information has good predication power for discourse analysis in both explicit and implicit relations. [sent-641, score-0.993]
</p><p>79 However, Full-Expansion that includes more information in other branches may introduce too many details which are rather tangential to discourse recognition. [sent-644, score-0.427]
</p><p>80 It would be interesting to find how the structured information works for discourse relations whose arguments reside in different sentences. [sent-647, score-0.693]
</p><p>81 For this purpose, we test the accuracy for discourse relations with the two arguments occurring in the same sentence, one-sentence apart, and two-sentence apart. [sent-648, score-0.608]
</p><p>82 This observation suggests that the structured syntactic information is more helpful for intersentential discourse analysis. [sent-652, score-0.597]
</p><p>83 We find that due to the weak modeling of Entity relations, many Entity relations which are non-discourse relation instances are mis-identified as implicit Expansion relations. [sent-658, score-0.501]
</p><p>84 Results of the syntactic structured kernel fo r discourse relations recognition with argu-  m ents in different sentences apart. [sent-668, score-1.098]
</p><p>85 Results of the syntactic structured kern el for simultaneous discourse identification and c lassification subtasks. [sent-676, score-0.715]
</p><p>86 3  System with Temporal Ordering Information  To examine the effectiveness of our temporal ordering information, we perform experiments 717  on simultaneous identification and classification of level-1 discourse relations to compare with using only base feature set as baseline. [sent-678, score-1.131]
</p><p>87 It indicates that temporal ordering information can constrain the discourse relation types inferred within a clause(s)/sentence(s) pair for both explicit and implicit relations. [sent-686, score-1.403]
</p><p>88 Results of tense and temporal order information on level-1 discourse relations. [sent-693, score-0.745]
</p><p>89 We observe that although temporal ordering information is useful in both explicit and implicit relation recognition, the contributions of the spe-  cific information are quite different for the two cases. [sent-694, score-0.887]
</p><p>90 In our experiments, we use tense and aspectual information for explicit relations, while event ordering information is used for implicit relations. [sent-695, score-0.819]
</p><p>91 The reason is explicit connective itself provides a strong hint for explicit relation, so tense and aspectual analysis which yields a reliable result can provide additional constraints, thus can help explicit relation recognition. [sent-696, score-1.168]
</p><p>92 However, event ordering which would inevitably involve more noises will adversely affect the explicit relation recognition performance. [sent-697, score-0.59]
</p><p>93 On the other hand, for implicit relations with no explicit connective words, tense and aspectual information alone is not enough for discourse analysis. [sent-698, score-1.438]
</p><p>94 4  Overall Results  We also evaluate our model which combines base features, tree kernel and tense/temporal ordering information together on Explicit, Implicit and All Relations respectively. [sent-701, score-0.827]
</p><p>95 8  Conclusions and Future Works  The purpose of this paper is to explore how to make use of the structural syntactic knowledge to do discourse relation recognition. [sent-708, score-0.733]
</p><p>96 In previous work, syntactic information from parse trees is represented as a set of heuristically selected flat paths or 2-level production rules. [sent-709, score-0.479]
</p><p>97 However, the features defined this way may not necessarily capture all useful syntactic information provided by the parse trees for discourse analysis. [sent-710, score-0.71]
</p><p>98 Specifically, we directly utilize the syntactic parse tree as a structure feature, and then apply kernels to such a feature, together  with other normal features. [sent-712, score-0.453]
</p><p>99 In addition, we also propose to incorporate temporal ordering information to constrain the interpretation of discourse relations, which also demonstrate statistical significant improvements for discourse relation recognition, both explicit and implicit. [sent-715, score-1.629]
</p><p>100 Complexity of dependencies in discourse: are dependencies in discourse more complex than in syntax? [sent-763, score-0.427]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('discourse', 0.427), ('kernel', 0.368), ('connective', 0.333), ('pdtb', 0.259), ('implicit', 0.228), ('tree', 0.202), ('temporal', 0.191), ('explicit', 0.161), ('relation', 0.155), ('ordering', 0.152), ('tense', 0.127), ('convolution', 0.126), ('flat', 0.121), ('base', 0.105), ('relations', 0.092), ('arguments', 0.089), ('causal', 0.088), ('production', 0.086), ('parse', 0.086), ('structured', 0.085), ('syntactic', 0.085), ('webber', 0.084), ('event', 0.081), ('kernels', 0.08), ('simultaneous', 0.072), ('aspectual', 0.07), ('path', 0.067), ('structural', 0.066), ('duffy', 0.066), ('composite', 0.063), ('altlex', 0.055), ('paragraph', 0.051), ('moschitti', 0.05), ('constrain', 0.047), ('prasad', 0.046), ('feature', 0.046), ('identification', 0.046), ('trees', 0.045), ('incorporate', 0.043), ('children', 0.043), ('features', 0.042), ('inferred', 0.042), ('clauses', 0.042), ('recognition', 0.041), ('ben', 0.04), ('commanding', 0.037), ('igure', 0.037), ('marathon', 0.037), ('pettibone', 0.037), ('retailing', 0.037), ('saito', 0.037), ('simpleexpansion', 0.037), ('svm', 0.037), ('entity', 0.036), ('clause', 0.036), ('argument', 0.034), ('cents', 0.032), ('prn', 0.032), ('dinesh', 0.032), ('contingency', 0.032), ('knott', 0.032), ('besides', 0.031), ('collins', 0.031), ('nodes', 0.031), ('heuristically', 0.031), ('mine', 0.031), ('penn', 0.031), ('adjacent', 0.031), ('connecting', 0.03), ('spans', 0.03), ('incorporated', 0.03), ('yesterday', 0.03), ('zelenko', 0.03), ('pitler', 0.03), ('haussler', 0.03), ('went', 0.03), ('golden', 0.03), ('sorensen', 0.03), ('square', 0.029), ('neighboring', 0.029), ('recognizing', 0.028), ('james', 0.027), ('dimensional', 0.027), ('predication', 0.026), ('culotta', 0.026), ('cash', 0.026), ('oakes', 0.026), ('instances', 0.026), ('singapore', 0.026), ('objects', 0.026), ('improvements', 0.026), ('echihabi', 0.025), ('paths', 0.025), ('capture', 0.025), ('harder', 0.024), ('row', 0.024), ('attribution', 0.024), ('vapnik', 0.023), ('cover', 0.023), ('sg', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="155-tfidf-1" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>Author: WenTing Wang ; Jian Su ; Chew Lim Tan</p><p>Abstract: Syntactic knowledge is important for discourse relation recognition. Yet only heuristically selected flat paths and 2-level production rules have been used to incorporate such information so far. In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations. The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well. University of Singapore Singapore 117417 sg tacl @ comp .nus .edu . sg 1</p><p>2 0.33651033 <a title="155-tfidf-2" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>3 0.29017648 <a title="155-tfidf-3" href="./acl-2010-Discourse_Structure%3A_Theory%2C_Practice_and_Use.html">86 acl-2010-Discourse Structure: Theory, Practice and Use</a></p>
<p>Author: Bonnie Webber ; Markus Egg ; Valia Kordoni</p><p>Abstract: unkown-abstract</p><p>4 0.23501183 <a title="155-tfidf-4" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>Author: Jun Sun ; Min Zhang ; Chew Lim Tan</p><p>Abstract: We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1</p><p>5 0.21873838 <a title="155-tfidf-5" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>6 0.19510902 <a title="155-tfidf-6" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>7 0.12718035 <a title="155-tfidf-7" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>8 0.12384374 <a title="155-tfidf-8" href="./acl-2010-Temporal_Information_Processing_of_a_New_Language%3A_Fast_Porting_with_Minimal_Resources.html">225 acl-2010-Temporal Information Processing of a New Language: Fast Porting with Minimal Resources</a></p>
<p>9 0.11843517 <a title="155-tfidf-9" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>10 0.11473639 <a title="155-tfidf-10" href="./acl-2010-Learning_Script_Knowledge_with_Web_Experiments.html">165 acl-2010-Learning Script Knowledge with Web Experiments</a></p>
<p>11 0.096981578 <a title="155-tfidf-11" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>12 0.09378273 <a title="155-tfidf-12" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>13 0.092233099 <a title="155-tfidf-13" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>14 0.087654978 <a title="155-tfidf-14" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>15 0.087398142 <a title="155-tfidf-15" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>16 0.086374663 <a title="155-tfidf-16" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>17 0.084882818 <a title="155-tfidf-17" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>18 0.082148992 <a title="155-tfidf-18" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>19 0.081186257 <a title="155-tfidf-19" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>20 0.079593368 <a title="155-tfidf-20" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.237), (1, 0.05), (2, 0.112), (3, -0.072), (4, -0.116), (5, 0.094), (6, 0.032), (7, 0.022), (8, -0.192), (9, -0.123), (10, -0.062), (11, -0.109), (12, 0.083), (13, 0.083), (14, -0.039), (15, 0.21), (16, 0.098), (17, 0.028), (18, 0.309), (19, -0.139), (20, 0.21), (21, -0.102), (22, -0.185), (23, 0.067), (24, 0.073), (25, 0.091), (26, 0.077), (27, 0.039), (28, -0.003), (29, 0.205), (30, 0.078), (31, 0.198), (32, 0.094), (33, 0.026), (34, -0.006), (35, 0.039), (36, 0.101), (37, -0.086), (38, -0.071), (39, -0.101), (40, 0.046), (41, -0.052), (42, -0.043), (43, 0.127), (44, -0.048), (45, -0.051), (46, -0.048), (47, -0.045), (48, -0.009), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9736048 <a title="155-lsi-1" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>Author: WenTing Wang ; Jian Su ; Chew Lim Tan</p><p>Abstract: Syntactic knowledge is important for discourse relation recognition. Yet only heuristically selected flat paths and 2-level production rules have been used to incorporate such information so far. In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations. The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well. University of Singapore Singapore 117417 sg tacl @ comp .nus .edu . sg 1</p><p>2 0.70164353 <a title="155-lsi-2" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>3 0.65748483 <a title="155-lsi-3" href="./acl-2010-Discourse_Structure%3A_Theory%2C_Practice_and_Use.html">86 acl-2010-Discourse Structure: Theory, Practice and Use</a></p>
<p>Author: Bonnie Webber ; Markus Egg ; Valia Kordoni</p><p>Abstract: unkown-abstract</p><p>4 0.56644338 <a title="155-lsi-4" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>Author: Jun Sun ; Min Zhang ; Chew Lim Tan</p><p>Abstract: We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1</p><p>5 0.52636635 <a title="155-lsi-5" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>6 0.42549866 <a title="155-lsi-6" href="./acl-2010-Temporal_Information_Processing_of_a_New_Language%3A_Fast_Porting_with_Minimal_Resources.html">225 acl-2010-Temporal Information Processing of a New Language: Fast Porting with Minimal Resources</a></p>
<p>7 0.40638906 <a title="155-lsi-7" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>8 0.40458864 <a title="155-lsi-8" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>9 0.38062832 <a title="155-lsi-9" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>10 0.37421137 <a title="155-lsi-10" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>11 0.37328878 <a title="155-lsi-11" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>12 0.36334774 <a title="155-lsi-12" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>13 0.34507349 <a title="155-lsi-13" href="./acl-2010-Learning_Script_Knowledge_with_Web_Experiments.html">165 acl-2010-Learning Script Knowledge with Web Experiments</a></p>
<p>14 0.3199012 <a title="155-lsi-14" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>15 0.31170791 <a title="155-lsi-15" href="./acl-2010-Efficient_Inference_through_Cascades_of_Weighted_Tree_Transducers.html">95 acl-2010-Efficient Inference through Cascades of Weighted Tree Transducers</a></p>
<p>16 0.30927145 <a title="155-lsi-16" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>17 0.29589632 <a title="155-lsi-17" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>18 0.28392935 <a title="155-lsi-18" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>19 0.27386403 <a title="155-lsi-19" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>20 0.27033433 <a title="155-lsi-20" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.228), (14, 0.023), (16, 0.018), (25, 0.053), (28, 0.012), (39, 0.015), (42, 0.012), (44, 0.013), (59, 0.091), (73, 0.045), (78, 0.084), (83, 0.15), (84, 0.021), (98, 0.145)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95171678 <a title="155-lda-1" href="./acl-2010-The_Use_of_Formal_Language_Models_in_the_Typology_of_the_Morphology_of_Amerindian_Languages.html">234 acl-2010-The Use of Formal Language Models in the Typology of the Morphology of Amerindian Languages</a></p>
<p>Author: Andres Osvaldo Porta</p><p>Abstract: The aim of this work is to present some preliminary results of an investigation in course on the typology of the morphology of the native South American languages from the point of view of the formal language theory. With this object, we give two contrasting examples of descriptions of two Aboriginal languages finite verb forms morphology: Argentinean Quechua (quichua santiague n˜o) and Toba. The description of the morphology of the finite verb forms of Argentinean quechua, uses finite automata and finite transducers. In this case the construction is straightforward using two level morphology and then, describes in a very natural way the Argentinean Quechua morphology using a regular language. On the contrary, the Toba verbs morphology, with a system that simultaneously uses prefixes and suffixes, has not a natural description as regular language. Toba has a complex system of causative suffixes, whose successive applications determinate the use of prefixes belonging different person marking prefix sets. We adopt the solution of Creider et al. (1995) to naturally deal with this and other similar morphological processes which involve interactions between prefixes and suffixes and then we describe the toba morphology using linear context-free languages.1 .</p><p>2 0.91699398 <a title="155-lda-2" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<p>Author: Barbara McGillivray</p><p>Abstract: We present a system that automatically induces Selectional Preferences (SPs) for Latin verbs from two treebanks by using Latin WordNet. Our method overcomes some of the problems connected with data sparseness and the small size of the input corpora. We also suggest a way to evaluate the acquired SPs on unseen events extracted from other Latin corpora.</p><p>3 0.86940634 <a title="155-lda-3" href="./acl-2010-Semantics-Driven_Shallow_Parsing_for_Chinese_Semantic_Role_Labeling.html">207 acl-2010-Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling</a></p>
<p>Author: Weiwei Sun</p><p>Abstract: One deficiency of current shallow parsing based Semantic Role Labeling (SRL) methods is that syntactic chunks are too small to effectively group words. To partially resolve this problem, we propose semantics-driven shallow parsing, which takes into account both syntactic structures and predicate-argument structures. We also introduce several new “path” features to improve shallow parsing based SRL method. Experiments indicate that our new method obtains a significant improvement over the best reported Chinese SRL result.</p><p>same-paper 4 0.85569477 <a title="155-lda-4" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>Author: WenTing Wang ; Jian Su ; Chew Lim Tan</p><p>Abstract: Syntactic knowledge is important for discourse relation recognition. Yet only heuristically selected flat paths and 2-level production rules have been used to incorporate such information so far. In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations. The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well. University of Singapore Singapore 117417 sg tacl @ comp .nus .edu . sg 1</p><p>5 0.85539258 <a title="155-lda-5" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>Author: Jungi Kim ; Jin-Ji Li ; Jong-Hyeok Lee</p><p>Abstract: Subjectivity analysis is a rapidly growing field of study. Along with its applications to various NLP tasks, much work have put efforts into multilingual subjectivity learning from existing resources. Multilingual subjectivity analysis requires language-independent criteria for comparable outcomes across languages. This paper proposes to measure the multilanguage-comparability of subjectivity analysis tools, and provides meaningful comparisons of multilingual subjectivity analysis from various points of view.</p><p>6 0.78188217 <a title="155-lda-6" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>7 0.74918199 <a title="155-lda-7" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>8 0.74512023 <a title="155-lda-8" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>9 0.72489721 <a title="155-lda-9" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>10 0.7167356 <a title="155-lda-10" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>11 0.71540093 <a title="155-lda-11" href="./acl-2010-Finding_Cognate_Groups_Using_Phylogenies.html">116 acl-2010-Finding Cognate Groups Using Phylogenies</a></p>
<p>12 0.71388483 <a title="155-lda-12" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>13 0.71180385 <a title="155-lda-13" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>14 0.71169949 <a title="155-lda-14" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>15 0.70966029 <a title="155-lda-15" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>16 0.70749366 <a title="155-lda-16" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>17 0.70668221 <a title="155-lda-17" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>18 0.70641351 <a title="155-lda-18" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>19 0.70552206 <a title="155-lda-19" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>20 0.70455492 <a title="155-lda-20" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
