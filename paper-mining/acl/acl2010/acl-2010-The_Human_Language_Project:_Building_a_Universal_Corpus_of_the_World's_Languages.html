<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-226" href="#">acl2010-226</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</h1>
<br/><p>Source: <a title="acl-2010-226-pdf" href="http://aclweb.org/anthology//P/P10/P10-1010.pdf">pdf</a></p><p>Author: Steven Abney ; Steven Bird</p><p>Abstract: We present a grand challenge to build a corpus that will include all of the world’s languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. We propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language. We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many strands of activity described here, as their contribution to the global effort to document the world’s linguistic heritage before more languages fall silent.</p><p>Reference: <a title="acl-2010-226-reference" href="../acl2010_reference/acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract We present a grand challenge to build a corpus that will include all of the world’s languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. [sent-2, score-0.35]
</p><p>2 The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. [sent-3, score-0.251]
</p><p>3 We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many  strands of activity described here, as their contribution to the global effort to document the world’s linguistic heritage before more languages fall silent. [sent-5, score-0.283]
</p><p>4 1 Introduction The grand aim of linguistics is the construction of a universal theory of human language. [sent-6, score-0.295]
</p><p>5 To a computational linguist, it seems obvious that the first step is to collect significant amounts of primary data for a large variety of languages. [sent-7, score-0.207]
</p><p>6 The next generation will forgive us for the most egregious shortcomings in theory construction and technology development, but they will not forgive us if we fail to preserve vanishing primary language data in a form that enables future research. [sent-12, score-0.246]
</p><p>7 au we have non-negligible quantities of machinereadable data for only about 20–30 of the world’s 6,900 languages (Maxwell and Hughes, 2006). [sent-16, score-0.129]
</p><p>8 There has been a tremendous upsurge of interest in documentary linguistics, the field concerned with the the “creation, annotation, preservation, and dissemination of transparent records of a language” (Woodbury, 2010). [sent-18, score-0.139]
</p><p>9 However, documentary linguistics alone is not equal to the task. [sent-19, score-0.139]
</p><p>10 For example, no million-word machine-readable corpus exists for any endangered language, even though such a quantity would be necessary for wide-ranging investigation of the language once no speakers are available. [sent-20, score-0.139]
</p><p>11 The chances of constructing large-scale resources will be greatly improved if computational linguists contribute their expertise. [sent-21, score-0.145]
</p><p>12 This collaboration between linguists and computational linguists will extend beyond the construction of the Universal Corpus to its exploitation for both theoretical and technological ends. [sent-22, score-0.197]
</p><p>13 We envisage a new paradigm of universal linguistics, in which grammars of individual languages are built from the ground up, combining expert manual effort with the power tools of probabilistic language models and grammatical inference. [sent-23, score-0.486]
</p><p>14 A universal grammar captures redundancies which exist across languages, constituting a “universal linguistic prior,” and enabling us to identify the distinctive properties of specific languages and families. [sent-24, score-0.414]
</p><p>15 There are existing collections that contain multiple languages, but it is rare to have consistent formats and annotation across languages, and few such datasets contain more than a dozen or so languages. [sent-32, score-0.163]
</p><p>16 If we think of a multi-lingual corpus as consisting of an array of items, with columns representing languages and rows representing resource types, the usual focus is on “vertical” processing. [sent-33, score-0.241]
</p><p>17 However, we do not aim for a collection that is universal in the sense of encompassing all language documentation efforts. [sent-49, score-0.406]
</p><p>18 We contrast the proposed effort with general efforts to develop open resources, standards, and best practices. [sent-52, score-0.152]
</p><p>19 The large engineering question is how one can turn the size ofthe task—constructing MT systems for all the world’s languages simultaneously—to one’s advantage, and thereby consume dramatically less data per language. [sent-60, score-0.129]
</p><p>20 It is natural to think in terms of replicating the body of resources available for well-documented languages, and the pre-eminent resource for any language is a treebank. [sent-65, score-0.185]
</p><p>21 It is also notoriously difficult to obtain agreement about how parse trees should be defined in one language, much less in many languages si-  multaneously. [sent-67, score-0.129]
</p><p>22 The idea of producing treebanks for 6,900 languages is quixotic, to put it mildly. [sent-68, score-0.129]
</p><p>23 A treebank, arguably, represents a theoretical hypothesis about how interpretations could be constructed; the primary data is actually the interpretations themselves. [sent-71, score-0.156]
</p><p>24 However, if the language under consideration is anything other than English, then a translation into English (or some other reference language) is for most purposes a perfectly adequate meaning representation. [sent-75, score-0.135]
</p><p>25 One measure of  adequacy of a language digitization is the ability of a human—already fluent in a reference language—to acquire fluency in the digitized language using only archived material. [sent-78, score-0.223]
</p><p>26 Taking sentences in a reference language as the meaning representation, we arrive back at machine translation as the measure of success. [sent-81, score-0.135]
</p><p>27 The key resource that should be built for each language, then, is a collection of primary texts with translations into a reference language. [sent-83, score-0.533]
</p><p>28 Large volumes of primary texts will be useful even without translation for such tasks as language modeling and unsupervised learning of morphology. [sent-85, score-0.327]
</p><p>29 Note that, for maximally authentic primary texts, we assume the direction of translation will normally be from primary text to reference language, not the other way around. [sent-87, score-0.447]
</p><p>30 Another layer of the corpus consists of sentence and word alignments, required for training and evaluating machine translation systems, and for extracting bilingual lexicons. [sent-88, score-0.158]
</p><p>31 This supports the development of morphological analyzers, to preprocess primary texts to identify morpheme boundaries and recognize allomorphs, reducing the amount of data required for training an MT system. [sent-91, score-0.256]
</p><p>32 This most-refined target annotation corresponds to the interlinear glossed texts that are the de facto standard of anno-  tation in the documentary linguistics community. [sent-92, score-0.453]
</p><p>33 We postulate that interlinear glossed text is sufficiently fine-grained to serve our purposes. [sent-93, score-0.214]
</p><p>34 It invites efforts to enrich it by automatic means: for example, there has been work on parsing the English translations and using the word-by-word glosses to transfer the parse tree to the object language, effectively creating a treebank automatically (Xia and Lewis, 2007). [sent-94, score-0.174]
</p><p>35 At the same time, we believe that interlinear glossed text is sufficiently simple and well-understood to allow rapid construction ofresources, and to make cross-linguistic consistency a realistic goal. [sent-95, score-0.214]
</p><p>36 However, there is an important consequence: the primary texts will be permanently subject to new translation initiatives, which themselves will be subject to new alignment and glossing initiatives, in which each step is an instance of semisupervised learning (Abney,  2007). [sent-98, score-0.327]
</p><p>37 Covering as many languages as possible is the first priority. [sent-102, score-0.129]
</p><p>38 “Covering” languages means enabling machine processing seamlessly across languages. [sent-105, score-0.184]
</p><p>39 Secondary resources such as grammars and lexicons are important, but no substitute for primary data. [sent-118, score-0.229]
</p><p>40 One means of resource identification is to survey existing documentation for the language, including bibliographic references and locations of web resources. [sent-125, score-0.236]
</p><p>41 (1) Translations of primary documents into a reference language (possibly including commentary). [sent-142, score-0.22]
</p><p>42 All documents will be included in primary form, but the percentage of documents with manual annotation, or manually corrected annotation, decreases at increasingly fine-grained levels of annotation. [sent-146, score-0.156]
</p><p>43 Defining such methods for a large range of resource-poor languages is an interesting computational challenge. [sent-148, score-0.129]
</p><p>44 22), the following secondary resources should be secured if they are available: (1) A lexicon with glosses in a reference language. [sent-151, score-0.235]
</p><p>45 Conversely, large-scale data collection efforts by the Linguistic Data Consortium and the European Language Resources Association cover less than one percent of the world’s languages, with no evident plans for major expansion of coverage. [sent-159, score-0.131]
</p><p>46 Other efforts concern the definition and aggregation of language resource metadata, including OLAC, IMDI, and 91  CLARIN (Simons and Bird, 2003; Broeder and Wittenburg, 2006; V ´aradi et al. [sent-160, score-0.191]
</p><p>47 Initiatives to develop standard formats for linguistic annotations are orthogonal to our goals. [sent-162, score-0.163]
</p><p>48 Converting all data formats to an official standard, such as the RDF-based models  being developed by ISO Technical Committee 37 Sub-committee 4 Working Group 2, is simply impractical. [sent-164, score-0.163]
</p><p>49 These formats have onerous syntactic and semantic requirements that demand substantial further processing together with expert judgment, and threaten to crush the large-scale collaborative data collection effort we envisage, before it even gets off the ground. [sent-165, score-0.337]
</p><p>50 Instead, we opt for a very lightweight format, sketched in the next section, to minimize the effort of conversion and enable an immediate start. [sent-166, score-0.146]
</p><p>51 This does not limit the options of community members who desire richer formats, since they are free to invest the effort in enriching the existing data. [sent-167, score-0.212]
</p><p>52 3  A Simple Storage Model  Here we sketch a simple approach to storage of texts (including transcribed speech), bitexts, interlinear glossed text, and lexicons. [sent-169, score-0.371]
</p><p>53 However, behind the scenes these could be represented as a sequence of pairs of start and end offsets into a primary text or speech signal, or as a sequence of integers that reference an array of strings. [sent-172, score-0.22]
</p><p>54 In what follows, we focus on the minimal requirements for storing and disseminating aligned text, not the requirements for efficient in-memory data structures. [sent-184, score-0.201]
</p><p>55 : ID :  europarl  / swedi sh / ep-0  0 -0 1 -1 7  / 18  swd eng SENT : det g ¨al le r en ordnings fr˚ aga TRANS : thi s i a po int s o f order ALI GN : 1 -1 2 -2 3 -3 4 -4 4 -5 4 -6 PROVENANCE : pharaoh-v1 . [sent-188, score-0.155]
</p><p>56 The LANGS attribute identifies the source and reference language using ISO 639 The SENT attribute contains space-delimited tokens comprising a sentence. [sent-197, score-0.16]
</p><p>57 A provenance attribute records any automatic or manual processes which apply to the record, and a revision attribute contains the version number, timestamp, and username associated with the most recent modification of the record, and a rights attribute contains copyright and license information. [sent-199, score-0.446]
</p><p>58 A bilingual lexicon is an indispensable resource, whether provided as such, induced from a collection of aligned text, or created by merging contributed and induced lexicons. [sent-203, score-0.197]
</p><p>59 Thus, we take a bilingual lexicon to be a kind of text in which each record contains a single lexeme and its translation, represented using the LEX and TRANS attributes we have already introduced, e. [sent-206, score-0.14]
</p><p>60 2 / 0 4 19 LANGS : swd eng LEX : ordnings fr˚ aga TRANS : point o f order ID :  In sum, the Universal Corpus is represented as a massive store of records, each representing a single sentence or lexical entry, using a limited set of attributes. [sent-211, score-0.155]
</p><p>61 We can start immediately by leveraging existing infrastructure, and the voluntary effort of interested members of the language resources community. [sent-222, score-0.204]
</p><p>62 One possibility is to found a “Language Commons,” an open access repository of language resources hosted in the Internet Archive, with a lightweight method for community members to contribute data sets. [sent-223, score-0.386]
</p><p>63 Even someone who has specialized in a particular language or language family maintains an interest, we expect, in the universal question—the exploration of Language writ large. [sent-230, score-0.23]
</p><p>64 Data providers will find benefit in the availability of volunteers for crowd-sourcing, and tools for (semi-)automated quality control, refinement, and presentation of data. [sent-231, score-0.143]
</p><p>65 In return for the data that documentary linguistics can provide, computational linguistics has the potential to revolutionize the tools and practice of language documentation. [sent-234, score-0.193]
</p><p>66 The corpus provides an economy of scale for the development of literacy materials and tools for interactive language instruction, in support of language preservation and revitalization. [sent-236, score-0.303]
</p><p>67 For small languages, literacy in the mother tongue is often defended on the grounds that it provides the best route to literacy in the national language (Wagner, 1993, ch. [sent-237, score-0.24]
</p><p>68 An essential ingredient of any local literacy program is to have a substantial quantity of available texts that represent familiar topics including cultural heritage, folklore, personal narratives, and current events. [sent-239, score-0.22]
</p><p>69 Transition to literacy in a language of wider communication is aided when transitional materials are available (Waters, 1998, pp. [sent-240, score-0.187]
</p><p>70 web-based functionality for transcribing audio and video of oral literature, or setting up a translation service based on aligned texts for a low-density language, and collecting the improved translations suggested by users. [sent-257, score-0.499]
</p><p>71 An important reason for keeping the data model as lightweight as possible is to enable contributions from volunteers with little or no linguistic training. [sent-259, score-0.162]
</p><p>72 Two models are the volunteers who scan documents and correct OCR output in Project Gutenberg, or the undergraduate volunteers who have constructed Greek and Latin treebanks within Project Perseus (Crane, 2010). [sent-260, score-0.178]
</p><p>73 We also see the Universal Corpus as an excellent opportunity for undergraduates to participate in research, and for native speakers to participate in the preservation of their language. [sent-262, score-0.168]
</p><p>74 The collection protocol known as Basic Oral Language Documentation (BOLD) enables documentary linguists to collect 2–3 orders of magnitude more oral discourse than before (Bird, 2010). [sent-264, score-0.414]
</p><p>75 Linguists can equip local speakers to collect written texts, then to carefully “respeak” and orally translate the texts into a reference language. [sent-265, score-0.215]
</p><p>76 With suitable tools, incorporating active learning, local speakers could further curate bilingual texts and lexicons. [sent-266, score-0.187]
</p><p>77 The LDC and ELRA have a central role to play, given their track record in obtaining, curating, and publishing data with licenses that facilitate language technology development. [sent-269, score-0.163]
</p><p>78 They could give bilingual texts a distinct status within their collections, to facilitate discovery. [sent-275, score-0.187]
</p><p>79 However, in the spirit of starting small, and starting now, agencies could require that sponsored projects which collect texts and build lexicons contribute them to the Language Commons. [sent-278, score-0.151]
</p><p>80 After all, the most effective time to do translation, alignment, and lexicon  work is often at the point when primary data is first collected, and this extra work promises direct benefits to the individual project. [sent-279, score-0.156]
</p><p>81 Language resources on the web are one source—the Cr´u bad a´n project has identified resources for 400 languages, for example (Scannell, 2008); the New Testament of the Bible exists in about 1200 languages and contains of the order of 100k words. [sent-285, score-0.347]
</p><p>82 Other resources can be logged by community members using a public access wiki, with a metadata template to ensure key fields are elicited such as resource owner, license, ISO 639 language code(s), and data type. [sent-292, score-0.441]
</p><p>83 Editors with knowledge of particular language families will categorize documented resources relative to the needs of the project, using controlled vocabularies. [sent-297, score-0.187]
</p><p>84 This  involves examining a resource, determining the granularity and provenance of the segmentation and alignment, checking its ISO 639 classifications, assigning it to a logarithmic size category, documenting its format and layout, collecting sample files, and assigning a priority score. [sent-298, score-0.29]
</p><p>85 Funding may be required to buy the rights to the resource from its owner, as compensation for lost revenue from future data sales. [sent-301, score-0.237]
</p><p>86 The repository’s ingestion process is followed, and the resource metadata is updated. [sent-303, score-0.177]
</p><p>87 Sponsorship is sought for collecting bilingual texts in high priority languages. [sent-306, score-0.296]
</p><p>88 Software developers will inspect the file formats and identify high priority formats based on information about resource priorities and sizes. [sent-315, score-0.487]
</p><p>89 They will code a corpus reader, an open source reference implementation for convert-  ing between corpus formats and the storage model presented in section 3. [sent-316, score-0.284]
</p><p>90 For users, it would be ideal for all materials to be available under a single license that permits derivative works, commercial use, and redistribution, such as the Creative Commons Attribution License (CC-BY). [sent-322, score-0.175]
</p><p>91 Instead, we propose to distinguish between: (1) a digital Archive of contributed corpora that are stored in their original format and made available under a range of licenses, offering preservation and dissemination services to the language resources community at large (i. [sent-325, score-0.33]
</p><p>92 the Language Commons); and (2) the Universal Corpus, which is embodied as programmatic access to an evolving subset of materials from the archive under one of a small set of permissive licenses, licenses whose unions and intersections are understood (e. [sent-327, score-0.414]
</p><p>93 The Cor-  pus, but not the Archive, is limited to the formats that support automatic cross-linguistic processing. [sent-333, score-0.163]
</p><p>94 Conversely, since the primary interface to the Corpus is programmatic, it may include materials that are hosted in many different archives; it only needs to know how to access and deliver them to the user. [sent-334, score-0.275]
</p><p>95 We do not trivialize the work involved in converting documents to the formats of section 3, and in manually correcting the results of noisy automatic processes such as optical character recognition. [sent-337, score-0.163]
</p><p>96 The utter simplicity of the formats also widens the pool of potential volunteers for doing the manual work that is required. [sent-341, score-0.252]
</p><p>97 By avoiding linguistically delicate annotation, we can take advantage of motivated but untrained volunteers such as students and members of speaker communities. [sent-342, score-0.147]
</p><p>98 ” Today, language documentation is a high priority in mainstream linguistics. [sent-345, score-0.173]
</p><p>99 Collecting bilingual texts is an orthodox activity, and many alternative conceptions of a Human Language Project would likely include this as an early task. [sent-354, score-0.187]
</p><p>100 By rising to this, the greatest language challenge of our time, we enable multi-lingual technology development at a new scale, and simultaneously lay the foundations for a new science of empirical universal linguistics. [sent-359, score-0.23]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('universal', 0.23), ('archive', 0.185), ('formats', 0.163), ('primary', 0.156), ('endangered', 0.139), ('documentary', 0.139), ('langs', 0.136), ('languages', 0.129), ('documentation', 0.124), ('literacy', 0.12), ('interlinear', 0.114), ('provenance', 0.114), ('resource', 0.112), ('licenses', 0.11), ('license', 0.108), ('texts', 0.1), ('glossed', 0.1), ('oral', 0.1), ('commons', 0.1), ('archives', 0.091), ('bird', 0.091), ('digitization', 0.091), ('simons', 0.091), ('trans', 0.091), ('volunteers', 0.089), ('lex', 0.089), ('bilingual', 0.087), ('steven', 0.082), ('community', 0.081), ('rights', 0.08), ('genome', 0.08), ('efforts', 0.079), ('transcription', 0.078), ('lightweight', 0.073), ('effort', 0.073), ('resources', 0.073), ('project', 0.072), ('linguists', 0.072), ('translation', 0.071), ('documented', 0.068), ('archived', 0.068), ('initiatives', 0.068), ('swadesh', 0.068), ('format', 0.067), ('materials', 0.067), ('metadata', 0.065), ('grand', 0.065), ('eng', 0.065), ('reference', 0.064), ('preservation', 0.062), ('audio', 0.061), ('collecting', 0.06), ('iso', 0.059), ('members', 0.058), ('aligned', 0.058), ('storage', 0.057), ('world', 0.057), ('enabling', 0.055), ('bible', 0.055), ('elra', 0.055), ('tools', 0.054), ('participate', 0.053), ('collaboration', 0.053), ('record', 0.053), ('id', 0.052), ('collection', 0.052), ('secondary', 0.052), ('access', 0.052), ('collect', 0.051), ('repository', 0.049), ('priority', 0.049), ('requirements', 0.049), ('translations', 0.049), ('culture', 0.049), ('abney', 0.049), ('attribute', 0.048), ('digital', 0.047), ('families', 0.046), ('glosses', 0.046), ('gary', 0.046), ('aga', 0.045), ('aradi', 0.045), ('archiving', 0.045), ('broeder', 0.045), ('cieri', 0.045), ('clarin', 0.045), ('compensation', 0.045), ('curating', 0.045), ('disseminating', 0.045), ('engagement', 0.045), ('flarenet', 0.045), ('forgive', 0.045), ('gone', 0.045), ('himmelmann', 0.045), ('hutchins', 0.045), ('imdi', 0.045), ('leaner', 0.045), ('nikolaus', 0.045), ('olac', 0.045), ('ordnings', 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999975 <a title="226-tfidf-1" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>Author: Steven Abney ; Steven Bird</p><p>Abstract: We present a grand challenge to build a corpus that will include all of the world’s languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. We propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language. We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many strands of activity described here, as their contribution to the global effort to document the world’s linguistic heritage before more languages fall silent.</p><p>2 0.10612344 <a title="226-tfidf-2" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>Author: Nancy Ide ; Collin Baker ; Christiane Fellbaum ; Rebecca Passonneau</p><p>Abstract: The Manually Annotated Sub-Corpus (MASC) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the American National Corpus. The MASC infrastructure enables the incorporation of contributed annotations into a single, usable format that can then be analyzed as it is or ported to any of a variety of other formats. MASC includes data from a much wider variety of genres than existing multiply-annotated corpora of English, and the project is committed to a fully open model of distribution, without restriction, for all data and annotations produced or contributed. As such, MASC is the first large-scale, open, communitybased effort to create much needed language resources for NLP. This paper describes the MASC project, its corpus and annotations, and serves as a call for contributions of data and annotations from the language processing community.</p><p>3 0.10063537 <a title="226-tfidf-3" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>Author: Ivan Titov ; Mikhail Kozhevnikov</p><p>Abstract: We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations. A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts. We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts.</p><p>4 0.091879413 <a title="226-tfidf-4" href="./acl-2010-WebLicht%3A_Web-Based_LRT_Services_for_German.html">259 acl-2010-WebLicht: Web-Based LRT Services for German</a></p>
<p>Author: Erhard Hinrichs ; Marie Hinrichs ; Thomas Zastrow</p><p>Abstract: This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a webbased service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project1. WebLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services. WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.) Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service. 1</p><p>5 0.08553873 <a title="226-tfidf-5" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>Author: Shane Bergsma ; Emily Pitler ; Dekang Lin</p><p>Abstract: In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.</p><p>6 0.079857774 <a title="226-tfidf-6" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>7 0.077623017 <a title="226-tfidf-7" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>8 0.074846663 <a title="226-tfidf-8" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>9 0.073243469 <a title="226-tfidf-9" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>10 0.072578467 <a title="226-tfidf-10" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>11 0.072334126 <a title="226-tfidf-11" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>12 0.070536375 <a title="226-tfidf-12" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>13 0.069435418 <a title="226-tfidf-13" href="./acl-2010-BabelNet%3A_Building_a_Very_Large_Multilingual_Semantic_Network.html">44 acl-2010-BabelNet: Building a Very Large Multilingual Semantic Network</a></p>
<p>14 0.068279095 <a title="226-tfidf-14" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>15 0.068223149 <a title="226-tfidf-15" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>16 0.065455742 <a title="226-tfidf-16" href="./acl-2010-Tools_for_Multilingual_Grammar-Based_Translation_on_the_Web.html">235 acl-2010-Tools for Multilingual Grammar-Based Translation on the Web</a></p>
<p>17 0.064358003 <a title="226-tfidf-17" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>18 0.063204713 <a title="226-tfidf-18" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>19 0.061236352 <a title="226-tfidf-19" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>20 0.060750656 <a title="226-tfidf-20" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.225), (1, -0.003), (2, -0.08), (3, -0.04), (4, 0.021), (5, -0.03), (6, -0.014), (7, 0.023), (8, 0.014), (9, 0.0), (10, 0.019), (11, 0.089), (12, 0.002), (13, -0.025), (14, -0.074), (15, 0.053), (16, 0.054), (17, 0.022), (18, 0.095), (19, 0.035), (20, -0.083), (21, -0.12), (22, 0.053), (23, -0.096), (24, 0.0), (25, -0.009), (26, 0.009), (27, 0.138), (28, 0.047), (29, -0.115), (30, -0.146), (31, -0.038), (32, 0.058), (33, -0.076), (34, -0.005), (35, -0.084), (36, 0.062), (37, -0.019), (38, 0.008), (39, 0.021), (40, 0.004), (41, 0.046), (42, 0.068), (43, 0.098), (44, 0.025), (45, 0.04), (46, 0.031), (47, 0.061), (48, -0.017), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95443112 <a title="226-lsi-1" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>Author: Steven Abney ; Steven Bird</p><p>Abstract: We present a grand challenge to build a corpus that will include all of the world’s languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. We propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language. We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many strands of activity described here, as their contribution to the global effort to document the world’s linguistic heritage before more languages fall silent.</p><p>2 0.78943503 <a title="226-lsi-2" href="./acl-2010-WebLicht%3A_Web-Based_LRT_Services_for_German.html">259 acl-2010-WebLicht: Web-Based LRT Services for German</a></p>
<p>Author: Erhard Hinrichs ; Marie Hinrichs ; Thomas Zastrow</p><p>Abstract: This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a webbased service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project1. WebLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services. WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.) Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service. 1</p><p>3 0.78199154 <a title="226-lsi-3" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>Author: Nancy Ide ; Collin Baker ; Christiane Fellbaum ; Rebecca Passonneau</p><p>Abstract: The Manually Annotated Sub-Corpus (MASC) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the American National Corpus. The MASC infrastructure enables the incorporation of contributed annotations into a single, usable format that can then be analyzed as it is or ported to any of a variety of other formats. MASC includes data from a much wider variety of genres than existing multiply-annotated corpora of English, and the project is committed to a fully open model of distribution, without restriction, for all data and annotations produced or contributed. As such, MASC is the first large-scale, open, communitybased effort to create much needed language resources for NLP. This paper describes the MASC project, its corpus and annotations, and serves as a call for contributions of data and annotations from the language processing community.</p><p>4 0.67092055 <a title="226-lsi-4" href="./acl-2010-Tools_for_Multilingual_Grammar-Based_Translation_on_the_Web.html">235 acl-2010-Tools for Multilingual Grammar-Based Translation on the Web</a></p>
<p>Author: Aarne Ranta ; Krasimir Angelov ; Thomas Hallgren</p><p>Abstract: This is a system demo for a set of tools for translating texts between multiple languages in real time with high quality. The translation works on restricted languages, and is based on semantic interlinguas. The underlying model is GF (Grammatical Framework), which is an open-source toolkit for multilingual grammar implementations. The demo will cover up to 20 parallel languages. Two related sets of tools are presented: grammarian’s tools helping to build translators for new domains and languages, and translator’s tools helping to translate documents. The grammarian’s tools are designed to make it easy to port the technique to new applications. The translator’s tools are essential in the restricted language context, enabling the author to remain in the fragments recognized by the system. The tools that are demonstrated will be ap- plied and developed further in the European project MOLTO (Multilingual On-Line Translation) which has started in March 2010 and runs for three years. 1 Translation Needs for the Web The best-known translation tools on the web are Google translate1 and Systran2. They are targeted to consumers of web documents: users who want to find out what a given document is about. For this purpose, browsing quality is sufficient, since the user has intelligence and good will, and understands that she uses the translation at her own risk. Since Google and Systran translations can be grammatically and semantically flawed, they don’t reach publication quality, and cannot hence be used by the producers of web documents. For instance, the provider of an e-commerce site cannot take the risk that the product descriptions or selling conditions have errors that change the original intentions. There are very few automatic translation systems actually in use for producers of information. As already 1www .google . com/t rans l e at 2www. systransoft . com noted by Bar-Hillel (1964), machine translation is one of those AI-complete tasks that involves a trade-off between coverage and precision, and the current mainstream systems opt for coverage. This is also what web users expect: they want to be able to throw just anything at the translation system and get something useful back. Precision-oriented approaches, the prime example of which is METEO (Chandioux 1977), have not been popular in recent years. However, from the producer’s point of view, large coverage is not essential: unlike the consumer’s tools, their input is predictable, and can be restricted to very specific domains, and to content that the producers themselves are creating in the first place. But even in such tasks, two severe problems remain: • • The development cost problem: a large amount oTfh ew dorekv eisl onpemedeendt f coors building tmra:n asl laatorgrse afomr new domains and new languages. The authoring problem: since the method does nTohte ew aourkth foorri nalgl input, etmhe: :asu tihnocer othfe eth me source toexest of translation may need special training to write in a way that can be translated at all. These two problems have probably been the main obstacles to making high-quality restricted language translation more wide-spread in tasks where it would otherwise be applicable. We address these problems by providing tools that help developers of translation systems on the one hand, and authors and translators—i.e. the users of the systems—on the other. In the MOLTO project (Multilingual On-Line Translation)3, we have the goal to improve both the development and use of restricted language translation by an order of magnitude, as compared with the state of the art. As for development costs, this means that a system for many languages and with adequate quality can be built in a matter of days rather than months. As for authoring, this means that content production does not require the use of manuals or involve trial and error, both of which can easily make the work ten times slower than normal writing. In the proposed system demo, we will show how some of the building blocks for MOLTO can already now be used in web-based translators, although on a 3 www.molto-project .eu 66 UppsalaP,r Sowceeeddenin,g 1s3 o Jfu tlhye 2 A0C1L0. 2 ?c 01200 S1y0s Atesmso Dcieamtioonns ftorart Cioonms,p puatagteiso 6n6a–l7 L1in,guistics Figure 1: A multilingual GF grammar with reversible mappings from a common abstract syntax to the 15 languages currently available in the GF Resource Grammar Library. smaller scale as regards languages and application domains. A running demo system is available at http : / / grammat i cal framework .org : 4 1 9 6. 2 2 Multilingual Grammars The translation tools are based on GF, Grammatical Framework4 (Ranta 2004). GF is a grammar formalism—that is, a mathematical model of natural language, equipped with a formal notation for writing grammars and a computer program implementing parsing and generation which are declaratively defined by grammars. Thus GF is comparable with formalism such as HPSG (Pollard and Sag 1994), LFG (Bresnan 1982) or TAG (Joshi 1985). The novel feature of GF is the notion of multilingual grammars, which describe several languages simultaneously by using a common representation called abstract syntax; see Figure 1. In a multilingual GF grammar, meaning-preserving translation is provided as a composition of parsing and generation via the abstract syntax, which works as an interlingua. This model of translation is different from approaches based on other comparable grammar formalisms, such as synchronous TAGs (Shieber and Schabes 1990), Pargram (Butt & al. 2002, based on LFG), LINGO Matrix (Bender and Flickinger 2005, based on HPSG), and CLE (Core Language Engine, Alshawi 1992). These approaches use transfer rules between individual languages, separate for each pair of languages. Being interlingua-based, GF translation scales up linearly to new languages without the quadratic blowup of transfer-based systems. In transfer-based sys- 4www.grammaticalframework.org tems, as many as n(n − 1) components (transfer functtieomnss), are naeneyde ads nto( cover a)l cl language pairs nisnf bero tfhu ndci-rections. In an interlingua-based system, 2n + 1components are enough: the interlingua itself, plus translations in both directions between each language and the interlingua. However, in GF, n + 1 components are sufficient, because the mappings from the abstract syntax to each language (the concrete syntaxes) are reversible, i.e. usable for both generation and parsing. Multilingual GF grammars can be seen as an implementation of Curry’s distinction between tectogrammatical and phenogrammatical structure (Curry 1961). In GF, the tectogrammatical structure is called abstract syntax, following standard computer science terminology. It is defined by using a logical framework (Harper & al. 1993), whose mathematical basis is in the type theory of Martin-L o¨f (1984). Two things can be noted about this architecture, both showing im- provements over state-of-the-art grammar-based translation methods. First, the translation interlingua (the abstract syntax) is a powerful logical formalism, able to express semantical structures such as context-dependencies and anaphora (Ranta 1994). In particular, dependent types make it more expressive than the type theory used in Montague grammar (Montague 1974) and employed in the Rosetta translation project (Rosetta 1998). Second, GF uses a framework for interlinguas, rather than one universal interlingua. This makes the interlingual approach more light-weight and feasible than in systems assuming one universal interlingua, such as Rosetta and UNL, Universal Networking Language5 . It also gives more precision to special-purpose translation: the interlingua of a GF translation system (i.e. the abstract syntax of a multilingual grammar) can encode precisely those structures and distinctions that are relevant for the task at hand. Thus an interlingua for mathematical proofs (Hallgren and Ranta 2000) is different from one for commands for operating an MP3 player (Perera and Ranta 2007). The expressive power of the logical framework is sufficient for both kinds of tasks. One important source of inspiration for GF was the WYSIWYM system (Power and Scott 1998), which used domain-specific interlinguas and produced excellent quality in multilingual generation. But the generation components were hard-coded in the program, instead of being defined declaratively as in GF, and they were not usable in the direction of parsing. 3 Grammars and Ontologies Parallel to the first development efforts of GF in the late 1990’s, another framework idea was emerging in web technology: XML, Extensible Mark-up Language, which unlike HTML is not a single mark-up language but a framework for creating custom mark-up lan5www .undl .org 67 guages. The analogy between GF and XML was seen from the beginning, and GF was designed as a formalism for multilingual rendering of semantic content (Dymetman and al. 2000). XML originated as a format for structuring documents and structured data serialization, but a couple ofits descendants, RDF(S) and OWL, developed its potential to formally express the semantics of data and content, serving as the fundaments of the emerging Semantic Web. Practically any meaning representation format can be converted into GF’s abstract syntax, which can then be mapped to different target languages. In particular the OWL language can be seen as a syntactic sugar for a subset of Martin-L o¨f’s type theory so it is trivial to embed it in GF’s abstract syntax. The translation problem defined in terms of an ontology is radically different from the problem of translating plain text from one language to another. Many of the projects in which GF has been used involve precisely this: a meaning representation formalized as GF abstract syntax. Some projects build on previously existing meaning representation and address mathematical proofs (Hallgren and Ranta 2000), software specifications (Beckert & al. 2007), and mathematical exercises (the European project WebALT6). Other projects start with semantic modelling work to build meaning representations from scratch, most notably ones for dialogue systems (Perera and Ranta 2007) in the European project TALK7. Yet another project, and one closest to web translation, is the multilingual Wiki system presented in (Meza Moreno and Bringert 2008). In this system, users can add and modify reviews of restaurants in three languages (English, Spanish, and Swedish). Any change made in any of the languages gets automatically translated to the other languages. To take an example, the OWL-to-GF mapping trans- lates OWL’s classes to GF’s categories and OWL’s properties to GF’s functions that return propositions. As a running example in this and the next section, we will use the class of integers and the two-place property of being divisible (“x is divisible by y”). The correspondences are as follows: Clas s (pp : intege r . . . ) m catm integer Ob j e ctP roperty ( pp :div domain (pp : int ege r ) range ( pp :integer ) ) m funm div : int eger -> 4 int ege r -> prop Grammar Engineer’s Tools In the GF setting, building a multilingual translation system is equivalent to building a multilingual GF 6EDC-22253, webalt .math .he l inki . fi s 7IST-507802, 2004–2006, www .t alk-pro j e ct .org grammar, which in turn consists of two kinds of components: • a language-independent abstract syntax, giving tahe l snegmuaangtei-ci nmdeopdeenl dveinat tw ahbisctrha ctrtan ssylnattiaoxn, gisi performed; • for each language, a concrete syntax mapping abfstorrac eta syntax turaegese ,t oa strings ien s tyhnatta language. While abstract syntax construction is an extra task compared to many other kinds of translation methods, it is technically relatively simple, and its cost is moreover amortized as the system is extended to new languages. Concrete syntax construction can be much more demanding in terms of programming skills and linguistic knowledge, due to the complexity of natural languages. This task is where GF claims perhaps the highest advantage over other approaches to special-purpose grammars. The two main assets are: • • Programming language support: GF is a modern fPuroncgtriaomnaml programming language, w isith a a powerful type system and module system supporting modular and collaborative programming and reuse of code. RGL, the GF Resource Grammar Library, implementing Fthe R bsoausicrc linguistic dre Ltaiiblsr orfy l iamn-guages: inflectional morphology and syntactic combination functions. The RGL covers fifteen languages at the moment, shown in Figure 1; see also Khegai 2006, El Dada and Ranta 2007, Angelov 2008, Ranta 2009a,b, and Enache et al. 2010. To give an example of what the library provides, let us first consider the inflectional morphology. It is presented as a set of lexicon-building functions such as, in English, mkV : St r -> V i.e. function mkV, which takes a string (St r) as its argument and returns a verb (V) as its value. The verb is, internally, an inflection table containing all forms of a verb. The function mkV derives all these forms from its argument string, which is the infinitive form. It predicts all regular variations: (mkV</p><p>5 0.65712732 <a title="226-lsi-5" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>Author: Jungi Kim ; Jin-Ji Li ; Jong-Hyeok Lee</p><p>Abstract: Subjectivity analysis is a rapidly growing field of study. Along with its applications to various NLP tasks, much work have put efforts into multilingual subjectivity learning from existing resources. Multilingual subjectivity analysis requires language-independent criteria for comparable outcomes across languages. This paper proposes to measure the multilanguage-comparability of subjectivity analysis tools, and provides meaningful comparisons of multilingual subjectivity analysis from various points of view.</p><p>6 0.59648478 <a title="226-lsi-6" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>7 0.58765167 <a title="226-lsi-7" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>8 0.57572156 <a title="226-lsi-8" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>9 0.52920055 <a title="226-lsi-9" href="./acl-2010-GernEdiT_-_The_GermaNet_Editing_Tool.html">126 acl-2010-GernEdiT - The GermaNet Editing Tool</a></p>
<p>10 0.5249241 <a title="226-lsi-10" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>11 0.52109122 <a title="226-lsi-11" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>12 0.51815939 <a title="226-lsi-12" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>13 0.50947696 <a title="226-lsi-13" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>14 0.49548754 <a title="226-lsi-14" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>15 0.49507645 <a title="226-lsi-15" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>16 0.48315978 <a title="226-lsi-16" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>17 0.48254856 <a title="226-lsi-17" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>18 0.47943014 <a title="226-lsi-18" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>19 0.46391183 <a title="226-lsi-19" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>20 0.46389574 <a title="226-lsi-20" href="./acl-2010-SystemT%3A_An_Algebraic_Approach_to_Declarative_Information_Extraction.html">222 acl-2010-SystemT: An Algebraic Approach to Declarative Information Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.012), (14, 0.013), (25, 0.05), (39, 0.025), (42, 0.035), (44, 0.018), (59, 0.081), (71, 0.013), (72, 0.014), (73, 0.06), (76, 0.019), (78, 0.03), (80, 0.015), (83, 0.084), (84, 0.038), (97, 0.305), (98, 0.108)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82759392 <a title="226-lda-1" href="./acl-2010-Plot_Induction_and_Evolutionary_Search_for_Story_Generation.html">196 acl-2010-Plot Induction and Evolutionary Search for Story Generation</a></p>
<p>Author: Neil McIntyre ; Mirella Lapata</p><p>Abstract: In this paper we develop a story generator that leverages knowledge inherent in corpora without requiring extensive manual involvement. A key feature in our approach is the reliance on a story planner which we acquire automatically by recording events, their participants, and their precedence relationships in a training corpus. Contrary to previous work our system does not follow a generate-and-rank architecture. Instead, we employ evolutionary search techniques to explore the space of possible stories which we argue are well suited to the story generation task. Experiments on generating simple children’s stories show that our system outperforms pre- vious data-driven approaches.</p><p>same-paper 2 0.78564787 <a title="226-lda-2" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>Author: Steven Abney ; Steven Bird</p><p>Abstract: We present a grand challenge to build a corpus that will include all of the world’s languages, in a consistent structure that permits large-scale cross-linguistic processing, enabling the study of universal linguistics. The focal data types, bilingual texts and lexicons, relate each language to one of a set of reference languages. We propose that the ability to train systems to translate into and out of a given language be the yardstick for determining when we have successfully captured a language. We call on the computational linguistics community to begin work on this Universal Corpus, pursuing the many strands of activity described here, as their contribution to the global effort to document the world’s linguistic heritage before more languages fall silent.</p><p>3 0.76397657 <a title="226-lda-3" href="./acl-2010-Optimizing_Question_Answering_Accuracy_by_Maximizing_Log-Likelihood.html">189 acl-2010-Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</a></p>
<p>Author: Matthias H. Heie ; Edward W. D. Whittaker ; Sadaoki Furui</p><p>Abstract: In this paper we demonstrate that there is a strong correlation between the Question Answering (QA) accuracy and the log-likelihood of the answer typing component of our statistical QA model. We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question-and-answer pairs. Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters.</p><p>4 0.64684707 <a title="226-lda-4" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>Author: Omri Abend ; Ari Rappoport</p><p>Abstract: The core-adjunct argument distinction is a basic one in the theory of argument structure. The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategorization acquisition. This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms. This is the first work to tackle this task in a fully unsupervised scenario.</p><p>5 0.52895188 <a title="226-lda-5" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>Author: Nancy Ide ; Collin Baker ; Christiane Fellbaum ; Rebecca Passonneau</p><p>Abstract: The Manually Annotated Sub-Corpus (MASC) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the American National Corpus. The MASC infrastructure enables the incorporation of contributed annotations into a single, usable format that can then be analyzed as it is or ported to any of a variety of other formats. MASC includes data from a much wider variety of genres than existing multiply-annotated corpora of English, and the project is committed to a fully open model of distribution, without restriction, for all data and annotations produced or contributed. As such, MASC is the first large-scale, open, communitybased effort to create much needed language resources for NLP. This paper describes the MASC project, its corpus and annotations, and serves as a call for contributions of data and annotations from the language processing community.</p><p>6 0.51462257 <a title="226-lda-6" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>7 0.50743651 <a title="226-lda-7" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>8 0.50455272 <a title="226-lda-8" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>9 0.50332928 <a title="226-lda-9" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>10 0.50193572 <a title="226-lda-10" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>11 0.50057691 <a title="226-lda-11" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>12 0.50021386 <a title="226-lda-12" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>13 0.50003791 <a title="226-lda-13" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>14 0.49956423 <a title="226-lda-14" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>15 0.49914521 <a title="226-lda-15" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>16 0.49875683 <a title="226-lda-16" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>17 0.49872571 <a title="226-lda-17" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>18 0.49871755 <a title="226-lda-18" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>19 0.4976151 <a title="226-lda-19" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>20 0.49697793 <a title="226-lda-20" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
