<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-134" href="#">acl2010-134</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</h1>
<br/><p>Source: <a title="acl-2010-134-pdf" href="http://aclweb.org/anthology//P/P10/P10-2050.pdf">pdf</a></p><p>Author: Yejin Choi ; Claire Cardie</p><p>Abstract: Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes polarity and intensity. Our experimental results show that our proposed approach improves the performance over a baseline that does not — exploit hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate components.</p><p>Reference: <a title="acl-2010-134-reference" href="../acl2010_reference/acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu l  Abstract Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. [sent-3, score-1.216]
</p><p>2 Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. [sent-4, score-0.028]
</p><p>3 In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes polarity and intensity. [sent-5, score-1.819]
</p><p>4 Our experimental results show that our proposed approach improves the performance over a baseline that does not —  exploit hierarchical structure among the classes. [sent-6, score-0.095]
</p><p>5 In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate components. [sent-7, score-0.133]
</p><p>6 1 Introduction Automatic opinion recognition involves a number of related tasks, such as identifying expressions of opinion (e. [sent-8, score-1.097]
</p><p>7 Most previous work treats each subtask in isolation: opinion expression extraction (i. [sent-18, score-0.625]
</p><p>8 detecting the boundaries of opinion expressions) and opinion attribute classification (e. [sent-20, score-1.153]
</p><p>9 determining values for polarity and intensity) are tackled as separate steps in opinion recognition systems. [sent-22, score-0.888]
</p><p>10 Unfortunately, errors from individual components will propagate in systems with cascaded component architectures,  causing performance degradation in the end-toend system (e. [sent-23, score-0.044]
</p><p>11 (2006)) in our case, in the end-to-end opinion recognition system. [sent-26, score-0.516]
</p><p>12 In this paper, we apply a hierarchical parameter sharing technique (e. [sent-27, score-0.187]
</p><p>13 In particular, we aim to jointly identify the boundaries of opinion expressions as well as to determine two of their key attributes polarity and intensity. [sent-32, score-1.041]
</p><p>14 Experimental results show that our proposed approach improves the performance over the baseline that does not exploit the hierarchical structure among the classes. [sent-33, score-0.095]
</p><p>15 In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate systems. [sent-34, score-0.133]
</p><p>16 —  —  2  Hierarchical Sequential Learning  We define the problem of joint extraction of opinion expressions and their attributes as a sequence  tagging task as follows. [sent-35, score-0.832]
</p><p>17 , 9} are defined as conjunctive values of∈ polarity 9la}b ealrse and intensity labels, as shown in Table 1. [sent-45, score-0.581]
</p><p>18 Then the conditional probability p(y|x) for linear-chain tChReF cos nisd given as (Lafferty e pt( ayl|. [sent-46, score-0.026]
</p><p>19 In order to apply a hierarchical parameter sharing technique (e. [sent-50, score-0.187]
</p><p>20 eN soatme etha cto mthpeorecan be other variations of hierarchical construction. [sent-82, score-0.117]
</p><p>21 tnfoedanetpficonale raditsyetpicnaorgmautipesoh ne tanoctfhpan larbdameal,neti rnis-  For instance, if yi = 1, then  3  λ f(1, x, i)  = λOPINION  gO  Features  (OPINION, x, i)  +  λPOSITIVE  +  gP(POSITVE,  We first introduce definitions of key terms that will be used to describe features. [sent-90, score-0.081]
</p><p>22 = λN′O-OPINION,OPINION gO′(NO-OPINION, OPINION, x, i) • EXP-POLARITY, EXP-INTENSITY & EXP-SPAN: + λN′O-POLARITY, gP′(NO-POLARITY, NEUTRAL, x, i) Words in a given opinion expression often do + λN′O-INTENSITY, gS′ (NO-INTENSITY, i) not share the same prior-attributes. [sent-93, score-0.553]
</p><p>23 Such discontinuous distribution of features can make it harder to learn the desired opinion expresThis hierarchical construction of feature and sion boundaries. [sent-94, score-0.674]
</p><p>24 Therefore, we try to obtain weight vectors allows similar labels to share the expression-level attributes (EXP-POLARITY and same subcomponents of feature and weight vecEXP-INTENSITY) using simple heuristics. [sent-95, score-0.143]
</p><p>25 The text span with the same expression-level attributes are referred to as EXP-SPAN. [sent-100, score-0.094]
</p><p>26 1 Per-Token Features  Per-token features are defined in the form of gO (α, x, i) , gP (β, x, i) and gS (γ, x, i). [sent-102, score-0.044]
</p><p>27 Common Per-Token Features Following features are common for all class labels. [sent-104, score-0.067]
</p><p>28 The notation ⊗ indicates conjunctive operation of tTwhoe nvaoltuaetiso. [sent-105, score-0.04]
</p><p>29 • OPINION-LEXICON(xi): based on opinion lexicon (Wiebe et al. [sent-109, score-0.491]
</p><p>30 • PRIOR-POLARITY(xi) ⊗ PRIOR-INTENSITY(xi) • EXP-POLARITY(xi) ⊗ EXP-INTENSITY(xi) • EXP-POLARITY(xi) ⊗ EXP-INTENSITY(xi) ⊗ STEM(xi) • EXP-SPAN(xi): boolean to indicate whether xi is in an EXP-SPAN. [sent-112, score-0.498]
</p><p>31 •  EXP-POLARITY(xi) ⊗ EXP-INTENSITY(xi) ⊗ EXP-SPAN(xi)  Polarity Per-Token Features These features are included only for gO (α, x, i) and gP(β, x, i), which are the feature functions corresponding to the polarity-based classes. [sent-114, score-0.044]
</p><p>32 This feature en∈cod {epso sthiteiv en,u nmebuetrra o,f n positive, neutral, and negative EXP-POLARITY words respectively, in the current sentence. [sent-116, score-0.03]
</p><p>33 • PRIOR-INTENSITY(xi), EXP-INTENSITY(xi) • STEM(xi) ⊗ EXP-INTENSITY(xi)  COUNT-OF-STRONG, COUNT-OF-WEAK: the number of strong and weak EXP-INTENSITY words in the current sentence. [sent-118, score-0.03]
</p><p>34 • INTENSIFIER(xi): whether xi is an intensifier, such as “extremely”, “highly”, “really”. [sent-119, score-0.498]
</p><p>35 • STRONGMODAL(xi): whether xi is a strong modal verb, such as “must”, “can”, “will”. [sent-120, score-0.529]
</p><p>36 • WEAKMODAL(xi): whether xi is a weak modal verb, such as “may”, “could”, “would”. [sent-121, score-0.559]
</p><p>37 • DIMINISHER(xi): whether xi is a diminisher, such as “little”, “somewhat”, “less”. [sent-122, score-0.498]
</p><p>38 2 Transition Features Transition features are employed to help with boundary extraction as follows: •  ⊗  ⊗ ⊗  ⊗  ⊗  ⊗  Polarity Transition Features Polarity transition features are features that used only for gO′ (α, αˆ , x, i) and gP′(β, x, i). [sent-124, score-0.225]
</p><p>39 • PART-OF-SPEECH(xi) ⊗ PART-OF-SPEECH(xi+1) EXP-POLARITY(xi) • EXP-POLARITY(xi) ⊗ EXP-POLARITY(xi+1) Intensity Transition Features Intensity transition features are features that used only for gO′ (α, αˆ , x, i) and gS′ (γ, γˆ, x, i). [sent-125, score-0.137]
</p><p>40 Our gold standard opinion expressions cor1The MPQA corpus can be obtained http://nrrc. [sent-127, score-0.581]
</p><p>41 271  Method Descriptionr(%)Ppo (s%iti)ve f(%)r(%)N pe(u%tr)al f(%)r(%)N peg(a%ti)ve f(%) JJointialioP n tr wtyiwithlnohO-t uyt HHierarchy31. [sent-131, score-0.084]
</p><p>42 63 Table 2: Performance of Opinion Extraction with Correct Polarity Attribute  at  HighMediumLow  Method Descriptionr(%) p(%) f(%)r(%) p(%) f(%)r(%) p(%) f(%)  JJointialioP n tr wtyiwithlnohO-t uyt HHierarchy27. [sent-149, score-0.084]
</p><p>43 0 Table 4: Performance of Opinion Extraction respond to direct subjective expression and expressive subjective element (Wiebe et al. [sent-176, score-0.11]
</p><p>44 2 Our implementation of hierarchical sequential learning is based on the Mallet (McCallum, 2002) code for CRFs. [sent-178, score-0.194]
</p><p>45 We then combine the results from two separate CRFs by collecting all opinion entities extracted by both sequence taggers. [sent-183, score-0.541]
</p><p>46 5% of the polarity annotations correspond to both; hence, we merge both into the neutral. [sent-185, score-0.294]
</p><p>47 Similarly, for gold standard intensity, we merge extremely high into high. [sent-186, score-0.023]
</p><p>48 [Baseline-2] Joint without Hierarchy: Here we use simple linear-chain CRFs without exploiting the class hierarchy for the opinion recognition task. [sent-189, score-0.633]
</p><p>49 Joint with Hierarchy: Finally, we test the hierarchical sequential learning approach elaborated in Section 3. [sent-191, score-0.194]
</p><p>50 1 Evaluation Results We evaluate all experiments at the opinion entity level, i. [sent-193, score-0.491]
</p><p>51 at the level of each opinion expression rather than at the token level. [sent-195, score-0.553]
</p><p>52 Table 4 shows the performance of opinion extraction without matching any attribute. [sent-197, score-0.535]
</p><p>53 That is, an extracted opinion entity is counted as correct if it overlaps4 with a gold standard opinion expression, without checking the correctness of its attributes. [sent-198, score-0.982]
</p><p>54 Table 2 and 3 show the performance of opinion extraction with the correct polarity and intensity respectively. [sent-199, score-1.076]
</p><p>55 One might wonder whether the overlap matching scheme could allow a degenerative case where extracting  the entire test dataset as one giant opinion expression would yield 100% recall and precision. [sent-202, score-0.632]
</p><p>56 Because each sentence corresponds to a different test instance in our model, and because some sentences do not contain any opinion expression in the dataset, such degenerative case is not possible in our experiments. [sent-203, score-0.603]
</p><p>57 272  HIERARCHY performs the best, and the least effective one is BASELINE-1, which cascades two separately trained models. [sent-204, score-0.055]
</p><p>58 It is interesting that the simple sequential tagging approach even without exploiting the hierarchy (BASELINE-2) performs better than the cascaded approach (BASELINE-1). [sent-205, score-0.277]
</p><p>59 When evaluating with respect to the polarity attribute, the performance of the negative class is substantially higher than the that of other classes. [sent-206, score-0.324]
</p><p>60 This is not surprising as there is approximately twice as much data for the negative class. [sent-207, score-0.03]
</p><p>61 When evaluating with respect to the intensity attribute, the performance of the LOW class is substantially lower than that of other classes. [sent-208, score-0.293]
</p><p>62 This result reflects the fact that it is inherently harder to distinguish an opinion expression with low intensity from no opinion. [sent-209, score-0.867]
</p><p>63 In general, we observe that determining  correct intensity attributes is a much harder task than determining correct polarity attributes. [sent-210, score-0.827]
</p><p>64 Remind that neither of these models alone fully solve the joint task of extracting boundaries as well as determining two attributions simultaneously. [sent-219, score-0.214]
</p><p>65 We conclude from our experiments that the simple joint sequential tagging approach even without exploiting the hierarchy brings a better performance than combining two separately developed systems. [sent-221, score-0.352]
</p><p>66 In addition, our hierarchical joint sequential learning approach brings a further perfor-  mance gain over the simple joint sequential tagging method. [sent-222, score-0.469]
</p><p>67 5  Related Work  Although there have been much research for finegrained opinion analysis (e. [sent-223, score-0.542]
</p><p>68 The hierarchical parameter sharing technique used in this paper has been previously used by Zhao et al. [sent-234, score-0.187]
</p><p>69 (2008) employs this technique only to classify sentence-level attributes (polarity and intensity), without involving a much harder task of detecting boundaries of sub-sentential entities. [sent-237, score-0.267]
</p><p>70 6  Conclusion  We applied a hierarchical parameter sharing technique using Conditional Random Fields for finegrained opinion analysis. [sent-238, score-0.729]
</p><p>71 Our proposed approach jointly extract opinion expressions from unstructured text and determine their attributes polarity and intensity. [sent-239, score-1.005]
</p><p>72 Empirical results indicate that the simple joint sequential tagging approach even without exploiting the hierarchy brings a better performance than combining two separately developed systems. [sent-240, score-0.352]
</p><p>73 In addition, we found that the hierarchical joint sequential learning approach improves the performance over the simple joint sequential tagging method. [sent-241, score-0.433]
</p><p>74 (2005) evaluate only on known words that are in their opinion lexicon. [sent-258, score-0.491]
</p><p>75 (2005) simplifies the problem by combining neutral opinions and no opinions into the same class, while our system distinguishes the two. [sent-260, score-0.18]
</p><p>76 Recognizing Contextual Polarity: an exploration of features for phrase-level sentiment analysis. [sent-375, score-0.096]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xi', 0.498), ('opinion', 0.491), ('polarity', 0.271), ('intensity', 0.27), ('gp', 0.218), ('wilson', 0.193), ('gs', 0.14), ('wiebe', 0.102), ('sequential', 0.099), ('hierarchical', 0.095), ('attributes', 0.094), ('neutral', 0.092), ('expressions', 0.09), ('attribute', 0.084), ('yi', 0.081), ('go', 0.076), ('descriptionr', 0.075), ('diminisher', 0.075), ('intensifier', 0.075), ('determining', 0.074), ('choi', 0.068), ('hierarchy', 0.065), ('expression', 0.062), ('boundaries', 0.061), ('crfs', 0.058), ('cascading', 0.056), ('zhao', 0.055), ('stem', 0.055), ('sentiment', 0.052), ('cai', 0.051), ('finegrained', 0.051), ('sharing', 0.05), ('degenerative', 0.05), ('ishncrare', 0.05), ('jjointialiop', 0.05), ('positve', 0.05), ('strongmodal', 0.05), ('uyt', 0.05), ('weakmodal', 0.05), ('yty', 0.05), ('joint', 0.05), ('transition', 0.049), ('breck', 0.045), ('features', 0.044), ('extraction', 0.044), ('opinions', 0.044), ('cascaded', 0.044), ('harder', 0.044), ('technique', 0.042), ('kim', 0.041), ('tagging', 0.04), ('conjunctive', 0.04), ('cardie', 0.038), ('lafferty', 0.037), ('brings', 0.036), ('popescu', 0.036), ('mpqa', 0.035), ('hu', 0.034), ('tr', 0.034), ('jointly', 0.034), ('separately', 0.033), ('cunningham', 0.032), ('mallet', 0.032), ('fields', 0.032), ('gate', 0.031), ('modal', 0.031), ('extracts', 0.031), ('hofmann', 0.03), ('negative', 0.03), ('weak', 0.03), ('claire', 0.029), ('extracting', 0.029), ('exploiting', 0.029), ('recognizing', 0.029), ('treats', 0.028), ('vectors', 0.027), ('gi', 0.027), ('separate', 0.027), ('conditional', 0.026), ('detecting', 0.026), ('positive', 0.026), ('recognition', 0.025), ('medium', 0.025), ('unstructured', 0.025), ('subjective', 0.024), ('xp', 0.024), ('finkel', 0.024), ('etzioni', 0.023), ('sequence', 0.023), ('class', 0.023), ('merge', 0.023), ('liu', 0.022), ('strength', 0.022), ('corne', 0.022), ('ychoi', 0.022), ('subcomponents', 0.022), ('cascades', 0.022), ('remind', 0.022), ('ayl', 0.022), ('etha', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="134-tfidf-1" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>Author: Yejin Choi ; Claire Cardie</p><p>Abstract: Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes polarity and intensity. Our experimental results show that our proposed approach improves the performance over a baseline that does not — exploit hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate components.</p><p>2 0.4330577 <a title="134-tfidf-2" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>Author: Binyang Li ; Lanjun Zhou ; Shi Feng ; Kam-Fai Wong</p><p>Abstract: There is a growing research interest in opinion retrieval as on-line users’ opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user’s query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representa- , tion, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 1</p><p>3 0.35527474 <a title="134-tfidf-3" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>4 0.33465701 <a title="134-tfidf-4" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>5 0.2159576 <a title="134-tfidf-5" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>6 0.144263 <a title="134-tfidf-6" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>7 0.13127705 <a title="134-tfidf-7" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>8 0.09976349 <a title="134-tfidf-8" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>9 0.090058893 <a title="134-tfidf-9" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>10 0.087887801 <a title="134-tfidf-10" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>11 0.086773895 <a title="134-tfidf-11" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>12 0.085336655 <a title="134-tfidf-12" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>13 0.083519086 <a title="134-tfidf-13" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>14 0.073497064 <a title="134-tfidf-14" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>15 0.06763681 <a title="134-tfidf-15" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>16 0.066420719 <a title="134-tfidf-16" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>17 0.060260288 <a title="134-tfidf-17" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>18 0.055657145 <a title="134-tfidf-18" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>19 0.054975357 <a title="134-tfidf-19" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>20 0.054498523 <a title="134-tfidf-20" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.158), (1, 0.137), (2, -0.243), (3, 0.292), (4, -0.25), (5, 0.06), (6, -0.04), (7, 0.214), (8, 0.053), (9, -0.015), (10, 0.023), (11, -0.063), (12, 0.052), (13, 0.048), (14, 0.011), (15, -0.086), (16, -0.202), (17, 0.174), (18, 0.001), (19, 0.066), (20, 0.044), (21, 0.086), (22, -0.06), (23, 0.086), (24, 0.002), (25, -0.005), (26, -0.017), (27, 0.012), (28, 0.13), (29, 0.031), (30, -0.059), (31, -0.004), (32, 0.045), (33, -0.028), (34, 0.023), (35, 0.115), (36, -0.032), (37, -0.021), (38, 0.028), (39, -0.038), (40, -0.033), (41, -0.052), (42, 0.01), (43, -0.016), (44, -0.017), (45, -0.065), (46, -0.011), (47, 0.08), (48, -0.011), (49, -0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97883934 <a title="134-lsi-1" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>Author: Yejin Choi ; Claire Cardie</p><p>Abstract: Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes polarity and intensity. Our experimental results show that our proposed approach improves the performance over a baseline that does not — exploit hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate components.</p><p>2 0.90674305 <a title="134-lsi-2" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>3 0.85226434 <a title="134-lsi-3" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>Author: Binyang Li ; Lanjun Zhou ; Shi Feng ; Kam-Fai Wong</p><p>Abstract: There is a growing research interest in opinion retrieval as on-line users’ opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user’s query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representa- , tion, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 1</p><p>4 0.84216362 <a title="134-lsi-4" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>5 0.65568125 <a title="134-lsi-5" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>6 0.39286363 <a title="134-lsi-6" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>7 0.34815258 <a title="134-lsi-7" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>8 0.33780134 <a title="134-lsi-8" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>9 0.32429248 <a title="134-lsi-9" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>10 0.31594497 <a title="134-lsi-10" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>11 0.26724559 <a title="134-lsi-11" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>12 0.25752982 <a title="134-lsi-12" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>13 0.25440711 <a title="134-lsi-13" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>14 0.2540063 <a title="134-lsi-14" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>15 0.25031945 <a title="134-lsi-15" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>16 0.24263796 <a title="134-lsi-16" href="./acl-2010-Extracting_Sequences_from_the_Web.html">111 acl-2010-Extracting Sequences from the Web</a></p>
<p>17 0.23863141 <a title="134-lsi-17" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>18 0.22703807 <a title="134-lsi-18" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>19 0.22377056 <a title="134-lsi-19" href="./acl-2010-Learning_5000_Relational_Extractors.html">159 acl-2010-Learning 5000 Relational Extractors</a></p>
<p>20 0.22254467 <a title="134-lsi-20" href="./acl-2010-Open_Information_Extraction_Using_Wikipedia.html">185 acl-2010-Open Information Extraction Using Wikipedia</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.011), (19, 0.246), (25, 0.064), (42, 0.121), (59, 0.077), (72, 0.035), (73, 0.105), (78, 0.025), (83, 0.104), (84, 0.023), (98, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78729403 <a title="134-lda-1" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>Author: Yejin Choi ; Claire Cardie</p><p>Abstract: Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes polarity and intensity. Our experimental results show that our proposed approach improves the performance over a baseline that does not — exploit hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate components.</p><p>2 0.6895237 <a title="134-lda-2" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: In this paper we present a joint content selection and compression model for single-document summarization. The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs. Using an integer linear programming formulation, the model learns to select and combine phrases subject to length, coverage and grammar constraints. We evaluate the approach on the task of generating “story highlights”—a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories. Experimental results show that the model’s output is comparable to human-written highlights in terms of both grammaticality and content.</p><p>3 0.64019114 <a title="134-lda-3" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>Author: Ryu Iida ; Syumpei Kobayashi ; Takenobu Tokunaga</p><p>Abstract: This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001 ; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on refer- ence resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.</p><p>4 0.62589979 <a title="134-lda-4" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>5 0.61955202 <a title="134-lda-5" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>6 0.61776102 <a title="134-lda-6" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>7 0.60802478 <a title="134-lda-7" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>8 0.60409492 <a title="134-lda-8" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>9 0.59482318 <a title="134-lda-9" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>10 0.59409493 <a title="134-lda-10" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>11 0.58236986 <a title="134-lda-11" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>12 0.57615292 <a title="134-lda-12" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>13 0.56564337 <a title="134-lda-13" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>14 0.56306493 <a title="134-lda-14" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>15 0.56178135 <a title="134-lda-15" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>16 0.56162059 <a title="134-lda-16" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>17 0.56015468 <a title="134-lda-17" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>18 0.55744433 <a title="134-lda-18" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>19 0.55700129 <a title="134-lda-19" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>20 0.55698067 <a title="134-lda-20" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
