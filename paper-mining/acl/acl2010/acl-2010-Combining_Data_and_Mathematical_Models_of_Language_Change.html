<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 acl-2010-Combining Data and Mathematical Models of Language Change</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-61" href="#">acl2010-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 acl-2010-Combining Data and Mathematical Models of Language Change</h1>
<br/><p>Source: <a title="acl-2010-61-pdf" href="http://aclweb.org/anthology//P/P10/P10-1104.pdf">pdf</a></p><p>Author: Morgan Sonderegger ; Partha Niyogi</p><p>Abstract: English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. We describe a longitudinal dataset of N/V pair pronunciations, leading to a set of properties to be accounted for by any computational model. We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. We compare each model’s dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect population-level dynamics.</p><p>Reference: <a title="acl-2010-61-reference" href="../acl2010_reference/acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Combining data and mathematical models of language change  Morgan Sonderegger University of Chicago Chicago, IL, USA. [sent-1, score-0.283]
</p><p>2 edu Abstract English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. [sent-4, score-1.058]
</p><p>3 We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. [sent-6, score-0.441]
</p><p>4 We compare each model’s dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect  population-level dynamics. [sent-7, score-0.411]
</p><p>5 1 Introduction The fascinating phenomena of language evolution and language change have inspired much work from computational perspectives in recent years. [sent-8, score-0.378]
</p><p>6 Research in this field considers populations of linguistic agents, and asks how the population dynamics are related to the behavior of individual agents. [sent-9, score-0.422]
</p><p>7 1 As pointed out by Choudhury (2007), most computational work on language change deals with data from cases of change either not at all, or at a relatively high level. [sent-11, score-0.51]
</p><p>8 edu spirit, we use data from an ongoing stress shift in English noun/verb (N/V) pairs. [sent-24, score-0.601]
</p><p>9 Because stress has been listed in dictionaries for several centuries, we are able to trace stress longitudinally and at the level of individual words, and observe dynamics significantly more complicated than in changes previously considered in the computational literature. [sent-25, score-1.546]
</p><p>10 In §2, we summarize aspects of the dynamics ttuo rbe. [sent-26, score-0.297]
</p><p>11 e Iancc §o2u,n wteed s ufomr by any computational dmynoadmeli cofs the stress shift. [sent-27, score-0.601]
</p><p>12 We also discuss proposed sources of these dynamics from the literature, based on experimental work by psychologists and linguists. [sent-28, score-0.297]
</p><p>13 We interpret 6 aspects of the N/V stress dynamics in DS terms; this gives a set of 6 desired properties to which any DS model’s dynamics can be compared. [sent-31, score-1.272]
</p><p>14 We consider 5 models of language learning by individuals, based on the experimental findings relevant to the N/V stress shift, and evaluate the population-level dynamics of the dynamical system model resulting from each against the set of desired properties. [sent-32, score-1.084]
</p><p>15 We are thus able to reason about which theories of the source of language change considered as hypotheses about how individuals learn lead to the populationlevel patterns observed in change. [sent-33, score-0.392]
</p><p>16 —  —  2  Data: English N/V pairs  The data considered here are the stress patterns of English homographic, disyllabic noun/verb pairs  (Table 1); we refer to these throughout as “N/V pairs”. [sent-34, score-0.759]
</p><p>17 Of the four logically possible stress patterns, all current N/V pairs follow one of the 3 patterns shown in Table 1: {1,1}, {1,2}, {2,2}. [sent-44, score-0.711]
</p><p>18 N/V pairs have been undergoing variation and change between these 3 patterns since Middle English (ME, c. [sent-46, score-0.476]
</p><p>19 The vast majority of stress shifts occurred  a{f1t,e2r} 1. [sent-48, score-0.601]
</p><p>20 Many dictionaries from the 17th century on list word stresses, making it possible to trace change in the stress of individual N/V pairs in considerable detail. [sent-50, score-0.904]
</p><p>21 Variation and change in N/V pair stress can be visualized by plotting stress trajectories: the moving average of N and V stress vs. [sent-53, score-2.097]
</p><p>22 4 Change Four types of clear-cut change between the three stress patterns are observed: {2,2}→{1,2} (Fig. [sent-58, score-0.918]
</p><p>23 1(b)) {1,2}→{2,2} However, change to {1,2} is {m1,u2c}h→ more common wthevane change tforo {m1 {1,2}; cinh particular, {2,2}→{ 1,2} aisn gthee m froosmt common change. [sent-60, score-0.51]
</p><p>24 When {2,2}→{1,2}  3However, as variation and change in N/V pair stress is ongoing, a few pairs (e. [sent-61, score-1.054]
</p><p>25 Finally, change never occurs directly between {1,1} and {2,2}. [sent-73, score-0.285]
</p><p>26 Stability Previous work on stress in N/V pairs (Sherman, 1975; Phillips, 1984) has emphasized change, in particular {2,2}→{ 1,2} (the most common change). [sent-74, score-0.649]
</p><p>27 However, an important aspect mofthe diachronic dynamics of N/V pairs is stability: most N/V pairs do not show variation or change. [sent-75, score-0.599]
</p><p>28 The 149 N/V pairs, used both in our corpus and in previous work, were chosen by Sherman (1975) as those most likely to have undergone change,  and thus are not suitable for studying how stable the three attested stress patterns are. [sent-76, score-0.946]
</p><p>29 In a random sample of N/V pairs (not the set of 149) in use over a fixed time period (1700–2007), we find that only 12% have shown variation or change in stress (Sonderegger and Niyogi, 2010). [sent-77, score-1.089]
</p><p>30 Most pairs maintain the {1,1}, {2,2}, or {1,2} stress pattern mfora nhutanidnre thdes {of1 years. [sent-78, score-0.702]
</p><p>31 ,2A} m, oorde {1l o,2f} }th setr desiasc phartotnerinc dynamics of N/V pair stress must explain how it can be the case both that some pairs show variation and change, and that many do not. [sent-79, score-1.096]
</p><p>32 Variation N/V pair stress patterns show both synchronic and diachronic variation. [sent-80, score-0.797]
</p><p>33 Synchronically, there is variation at the population level in the stress of some N/V pairs at any given time; this is reflected by the inclusion of more than one pronunciation for some N/V pairs in many dictionaries. [sent-81, score-0.927]
</p><p>34 For several N/V pairs which have currently variable pronunciation, 1/3 of speakers show variation in the stress ofthe N form. [sent-84, score-0.76]
</p><p>35 Metrical evidence from poetry suggests that individual variation also existed in the past; the best evidence is for Shakespeare, who shows variation in the stress of over 20 N/V pairs (K ¨okeritz, 1953). [sent-85, score-0.927]
</p><p>36 Diachronically, a relevant question for modeling is whether all variation is short-lived, or whether stable variation is possible. [sent-86, score-0.475]
</p><p>37 A particular type of stable variation is in fact observed relatively often in the corpus: either the N or V form stably vary (Fig. [sent-87, score-0.425]
</p><p>38 286418509 Yea1r9502 (a) concert  (b) combat  Figure 1: Example N/V pair stress trajectories. [sent-95, score-0.668]
</p><p>39 (c) exile  (d) rampage  Moving averages (60-year window) of stress placement  (1= σ´σ, 2=σ σ´). [sent-96, score-0.653]
</p><p>40 esizes that N/V pairs with lower frequencies (summed N+V word frequencies) are more likely to change to {1,2}. [sent-98, score-0.303]
</p><p>41 for the most common change, {2,2}→{ 1,2}: among N/V pairs which were {2,2} →in{ 117,200}: and are either {2,2} or {1,2} today, t{h2o,s2e} iwnh 1i7ch0 0h aavnde undergone change {h1a,v2e} significantly lower frequencies, on average, than those which have not. [sent-100, score-0.333]
</p><p>42 In (Sonderegger and Niyogi, 2010), we give preliminary evidence from realtime frequency trajectories (for < 10 N/V pairs) that it is not lower frequency per se which triggers change to {1,2}, but falling frequency. [sent-101, score-0.44]
</p><p>43 2 Sources of change The most salient facts about English N/V pair stress are that (a) change is most often to {1,2} (b) sthse a {2,1} pattern never occurs. [sent-105, score-1.233]
</p><p>44 We assume that these biases have been active over the course of the N/V stress shift, and can thus be seen as possible sources of the diachronic dynamics of N/V  pairs. [sent-109, score-1.023]
</p><p>45 related forms more similar to each other in their phonetic (and morphological) structure” (Hock, Proposed causes for analogical change thus often involve a speaker’s production and perception of a form being influenced by similar forms in their  1991). [sent-117, score-0.345]
</p><p>46 English listeners strongly prefer the typical stress pattern (N= σ´σ or V=σ σ´) in novel English disyllables (Guion et al. [sent-122, score-0.682]
</p><p>47 sounds, words, or paradigms, such as an N/V pair’s stress pattern. [sent-128, score-0.601]
</p><p>48 1021  als) is argued to be a necessary condition for the change α→β at the population level, and an explanation →foβr why tehe p change α→β ,is a common, wplahnileat othne change β→α aisn rarely (or never) oonb-, swehrivleed. [sent-131, score-0.856]
</p><p>49 Kelly and collaborators have shown mistransmission effects which they propose are responsible for the directionality of the most common type of N/V pair stress shifts  ({ 1,1}, {2,2}→{ 1,2}), based on “rhythmic cont(e{x1t,”1 (Kelly, 1988; Kelly aasnedd Bock, 1988; Kelly, 1989). [sent-134, score-0.847]
</p><p>50 Word stress is misperceived more often as initial in “trochaic-biasing” contexts, where the preceding syllable is weak or the following syllable is heavy; and more often as final in analogously “iambic-biasing” contexts. [sent-135, score-0.643]
</p><p>51 Because of the evidence for within-speaker variation in N/V pair stress (§2. [sent-138, score-0.779]
</p><p>52 The infinite-population assumption in particular makes the dynamics fully deterministic; this rules out the possibility of change due to drift (or sample variation), where a form disappears from the population because no examples of it are encountered by learners in Gt in the input from Gt−1 . [sent-146, score-0.726]
</p><p>53 The Ni are fixed (each learner hears the same number of examples), while the kit are random variables (over  learners in Gt). [sent-152, score-0.257]
</p><p>54 1 Dynamical systems We develop and analyze models of populations of language learners in the mathematical framework of (discrete) dynamical systems (DS) (Niyogi and Berwick, 1995; Niyogi, 2006). [sent-159, score-0.261]
</p><p>55 Because it is in general impossible to solve a given iterated map as a function of t, the dynamical systems viewpoint is to understand its longterm behavior by finding its fixed points and bifurcations: changes in the number and stability of fixed points as system parameters vary. [sent-161, score-0.346]
</p><p>56 Briefly, α∗ is a fixed point (FP) of f if f(α∗) = α∗; it is stable if limαt = α∗ for α0 sufficiently near α∗, and unstt→a∞ble otherwise; these are also called stable states and unstable states. [sent-162, score-0.592]
</p><p>57 Intuitively, α∗ is stable iffthe system is stable under small perturbations from α∗. [sent-163, score-0.506]
</p><p>58 8 In the context of a linguistic population, change from state α (100% of the population uses {1,1}) to msta steta (100% %of o fth teh population uses {1,2}) corresponds 1t0o0 a bifurcation, uwlahetiroen some system parameter (N) passes a critical value (N0). [sent-164, score-0.437]
</p><p>59 2 DS interpretation of observed dynamics Below, we describe 5 DS models of linguistic populations. [sent-169, score-0.362]
</p><p>60 To interpret whether each model has properties consistent with the N/V dataset, we translate the observations about the dynamics of N/V stress made above (§2. [sent-170, score-0.933]
</p><p>61 Stability of {1,1}, {1,2}, {2,2}: These stress patterns correspond 1t,o2 }sta, b{l2e, 2s}ta:te Tsh (for some system parameter values). [sent-177, score-0.663]
</p><p>62 Observed stable variation: Stable states are possible (for some system parameter values) corresponding to variation in the N or V form, but not both. [sent-179, score-0.364]
</p><p>63 Sudden change: Change from one stress pattern to another corresponds to a bifurcation, where the fixed point corresponding to the old stress pattern becomes unstable. [sent-181, score-1.354]
</p><p>64 Each model’s dynamics are evaluated with respect to the set of desired properties corresponding to patterns observed in the N/V data. [sent-191, score-0.473]
</p><p>65 In Model 1 there is no coupling ( αˆt and βˆt learned independently), in Models 2–3 coupling takes the form of a hard constraint corresponding to Ross’ generalization, and in Models 4–5 different stress patterns have different prior probabilities. [sent-194, score-0.868]
</p><p>66 1 Model 1: Mistransmission Motivated by the evidence for asymmetric misperception of N/V pair stress (§2. [sent-196, score-0.702]
</p><p>67 2), suppose the stress otifo Nn= oσf σ´ N /aVnd p Vai=r´ σ sσtr examples may boes em tihseperceived (as N=´ σσ and V=σ σ´), with mistransmission probabilities p and q. [sent-197, score-0.821]
</p><p>68 Dynamics There is a single, stable fixed point of evolution equations (3–4): (α∗ , β∗) = (0, 1), corresponding to the stress pattern {1,2}. [sent-207, score-1.143]
</p><p>69 2 t)h,e w wleea rcnoenrs attempts ctoon probability mmaodtcehl as above, but the ( αˆt, learned must satisfy the constraint that stress be more probable in the V form than in the N form. [sent-215, score-0.628]
</p><p>70 3  Fofig Muroed 2el: 2 Dynamics  Model 3: Coupling by constraint, with mistransmission  We now assume that each example is subject to mistransmission, as in Model 1; the learner then applies A2 to the heard examples. [sent-229, score-0.318]
</p><p>71 Dynamics There is a single, stable fixed point, corresponding to stable variation in both N and V. [sent-232, score-0.663]
</p><p>72 This model thus shows none of the desired properties, except that {2,1} is not a stable FP (by construction). [sent-233, score-0.295]
</p><p>73 We now assume that learners compute the probabilities of each possible N/V pair stress pattern, rather than separate probabilities for the N and V forms. [sent-237, score-0.799]
</p><p>74 Prior probabilities: = (λ11, λ12, λ21, λ22), based on the support for each stress pattern in the lexicon. [sent-240, score-0.654]
</p><p>75 Learners’ production of an N/V pair is thus influenced by both their learning experience (for the particular N/V pair) and by how much support exists in their lexicon for the different stress patterns. [sent-247, score-0.64]
</p><p>76 We leave the exact interpretation of the λij ambiguous; they could be the percentage of N/V pairs already learned which follow each stress pattern, for example. [sent-248, score-0.649]
</p><p>77 Tohsseisneg hyperplanes do not change as N1 and N2 vary, so −  −  Figure 3: Example phase diagram in (λ11 , λ22) for Model 4, with N1 = 5, N2 = 10. [sent-273, score-0.316]
</p><p>78 Thus, although othme{re1 ,i2s frequency dcye-pendence in this model, it is not as observed in the diachronic data, where change to {1,2} is  frequency-dependent. [sent-279, score-0.42]
</p><p>79 Finally, no stable variation is possible: in every stable state, all members of the population categorically use a single stress pattern. [sent-280, score-1.309]
</p><p>80 5  Model 5: Coupling by priors, with mistransmission We now suppose that each example from a learner’s data is possibly mistransmitted, as in Model 1; the learner then applies the algorithm from Model 4 to the heard examples (instead of using k1t, k2t) . [sent-283, score-0.318]
</p><p>81 Depfinoien Rt o0 = (1 − p) (1 − q)R, λ012 = λ12, a0,nd1  λ011= λ11(1−qNN2−2 1),  λ022= λ22(1−pNN1−1 1)  There are 6 regions of parameter space corre-  sponding to different stable FPs, identical to the 6 regions in Model 4, with the following substitu1025  olctnoiaint 100. [sent-288, score-0.341]
</p><p>82 204 24N16810 Figure 4: Example of falling N1 triggering change from (1, 1) to (0, 1) for Model 5. [sent-291, score-0.304]
</p><p>83 Dashed line = stable FP of the form (γ, 1), solid line = stable FP (0, 1). [sent-292, score-0.506]
</p><p>84 However, change to {1,2} entails crossing tFhPes hyperplanes λ011=λ012 oan {d1 λ02=λ012, lasnd c oiss tshinugs now frequency dependent. [sent-302, score-0.349]
</p><p>85 By a similar argument, falling frequency can lead to change from (0, κ) to (0, 1). [sent-306, score-0.337]
</p><p>86 Falling frequency can thus cause change to { 1,2} Fina lthli nsg model, as seen nin t htuhes Nca/Vus data; Fig. [sent-307, score-0.288]
</p><p>87 λλ212  Unlike in Model 4, stable variation of the type seen in the N/V stress trajectories one of N or V stably varying, but not both is possible for some —  —  parameter values. [sent-309, score-1.031]
</p><p>88 Models including asymmetric mistransmission (1, 3, 5) generally do not lead to stable states in which the entire population uses {1,1} or {2,2}. [sent-318, score-0.56]
</p><p>89 ) However, {1,1} a {n1d, {2,2} are }di iasc phorosnsii-cally very wsteavbelre, s {t1re,s1s} patterns, suggesting thhraotn aitleast for this model set, assuming mistransmission in the learner is problematic. [sent-320, score-0.268]
</p><p>90 Models 4–5, where analogy is implemented as prior probabilities over N/V stress patterns, show crucial aspects of the observed dynamics: bifurcations corresponding to the changes observed in the stress data. [sent-322, score-1.403]
</p><p>91 We have only considered some schematic properties of the dynamics observed in our dataset, and used these to qualitatively compare each model’s predictions to the dynamics. [sent-336, score-0.369]
</p><p>92 Future work should consider the dynamics in more detail, develop more complex models (for example, by relaxing the infinitepopulation assumption, allowing for stochastic dynamics), and quantitatively compare model predictions and observed dynamics. [sent-337, score-0.362]
</p><p>93 We were able to reason about how assumptions about individual learning affect population dynamics by analyzing a range of simple, related models. [sent-338, score-0.43]
</p><p>94 Although the type of individual learning assumed in each model was chosen with the same patterns of change in mind, and despite the simplicity of the models used, the resulting population-level dynamics dif-  fer greatly. [sent-348, score-0.642]
</p><p>95 This is an important point given that proposed explanations for change (e. [sent-349, score-0.288]
</p><p>96 Factors affecting stress placement for English nonwords include syllabic structure, lexical class, and stress patterns of phonologically similar words. [sent-441, score-1.264]
</p><p>97 Noun-verb stress alternation: An example of the lexical diffusion of sound change in English. [sent-642, score-0.895]
</p><p>98 Variation and change in English noun/verb pair stress: Data, dynamical systems models, and their interaction. [sent-648, score-0.41]
</p><p>99 Dynamical systems models of language variation and change: An application to an English stress shift. [sent-658, score-0.74]
</p><p>100 Testing for frequency and structural effects in an English stress shift. [sent-663, score-0.659]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stress', 0.601), ('dynamics', 0.297), ('change', 0.255), ('stable', 0.253), ('niyogi', 0.238), ('mistransmission', 0.182), ('gt', 0.142), ('sonderegger', 0.14), ('evolution', 0.123), ('dynamical', 0.116), ('variation', 0.111), ('diachronic', 0.095), ('population', 0.091), ('coupling', 0.089), ('fp', 0.088), ('learner', 0.086), ('bifurcation', 0.084), ('choudhury', 0.084), ('learners', 0.083), ('kelly', 0.073), ('equations', 0.067), ('ds', 0.066), ('patterns', 0.062), ('ross', 0.061), ('hyperplanes', 0.061), ('pattern', 0.053), ('heard', 0.05), ('falling', 0.049), ('pairs', 0.048), ('changes', 0.047), ('fixed', 0.046), ('iterated', 0.046), ('stability', 0.045), ('regions', 0.044), ('trajectories', 0.042), ('berwick', 0.042), ('bifurcations', 0.042), ('fps', 0.042), ('hears', 0.042), ('ioanvte', 0.042), ('kalish', 0.042), ('kirby', 0.042), ('misperceived', 0.042), ('rgalfsoen', 0.042), ('sherman', 0.042), ('desired', 0.042), ('assumptions', 0.042), ('unstable', 0.04), ('pair', 0.039), ('sound', 0.039), ('evolutionary', 0.038), ('probabilities', 0.038), ('individuals', 0.038), ('observed', 0.037), ('pearl', 0.037), ('pronunciations', 0.037), ('rhythmic', 0.037), ('properties', 0.035), ('chicago', 0.034), ('asymmetric', 0.034), ('populations', 0.034), ('cago', 0.034), ('frequency', 0.033), ('explanations', 0.033), ('english', 0.032), ('generalization', 0.032), ('forms', 0.03), ('biases', 0.03), ('undergone', 0.03), ('analogical', 0.03), ('never', 0.03), ('simulation', 0.029), ('pronunciation', 0.028), ('evidence', 0.028), ('models', 0.028), ('period', 0.028), ('arciuli', 0.028), ('boer', 0.028), ('concert', 0.028), ('daland', 0.028), ('disyllables', 0.028), ('guion', 0.028), ('hirsch', 0.028), ('komarova', 0.028), ('landsbergen', 0.028), ('longitudinal', 0.028), ('minett', 0.028), ('mishear', 0.028), ('mitchener', 0.028), ('obstruents', 0.028), ('rampage', 0.028), ('stresses', 0.028), ('historical', 0.027), ('constraint', 0.027), ('effects', 0.025), ('exile', 0.024), ('stably', 0.024), ('phillips', 0.024), ('unvoiced', 0.024), ('weinberg', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="61-tfidf-1" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>Author: Morgan Sonderegger ; Partha Niyogi</p><p>Abstract: English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. We describe a longitudinal dataset of N/V pair pronunciations, leading to a set of properties to be accounted for by any computational model. We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. We compare each model’s dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect population-level dynamics.</p><p>2 0.080341972 <a title="61-tfidf-2" href="./acl-2010-Correcting_Errors_in_Speech_Recognition_with_Articulatory_Dynamics.html">74 acl-2010-Correcting Errors in Speech Recognition with Articulatory Dynamics</a></p>
<p>Author: Frank Rudzicz</p><p>Abstract: We introduce a novel mechanism for incorporating articulatory dynamics into speech recognition with the theory of task dynamics. This system reranks sentencelevel hypotheses by the likelihoods of their hypothetical articulatory realizations which are derived from relationships learned with aligned acoustic/articulatory data. Experiments compare this with two baseline systems, namely an acoustic hidden Markov model and a dynamic Bayes network augmented with discretized representations of the vocal tract. Our system based on task dynamics reduces worderror rates significantly by 10.2% relative to the best baseline models.</p><p>3 0.063550234 <a title="61-tfidf-3" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>Author: Pierre Lison</p><p>Abstract: Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions ofthe state and action spaces.</p><p>4 0.055416949 <a title="61-tfidf-4" href="./acl-2010-String_Extension_Learning.html">217 acl-2010-String Extension Learning</a></p>
<p>Author: Jeffrey Heinz</p><p>Abstract: This paper provides a unified, learningtheoretic analysis of several learnable classes of languages discussed previously in the literature. The analysis shows that for these classes an incremental, globally consistent, locally conservative, set-driven learner always exists. Additionally, the analysis provides a recipe for constructing new learnable classes. Potential applications include learnable models for aspects of natural language and cognition.</p><p>5 0.052543603 <a title="61-tfidf-5" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>Author: Costanza Navarretta ; Patrizia Paggio</p><p>Abstract: This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback.</p><p>6 0.049813401 <a title="61-tfidf-6" href="./acl-2010-A_Game-Theoretic_Model_of_Metaphorical_Bargaining.html">6 acl-2010-A Game-Theoretic Model of Metaphorical Bargaining</a></p>
<p>7 0.04656982 <a title="61-tfidf-7" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>8 0.039224405 <a title="61-tfidf-8" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>9 0.038684253 <a title="61-tfidf-9" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>10 0.038414765 <a title="61-tfidf-10" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>11 0.037814438 <a title="61-tfidf-11" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>12 0.037249085 <a title="61-tfidf-12" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>13 0.036747489 <a title="61-tfidf-13" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>14 0.0362994 <a title="61-tfidf-14" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>15 0.036181159 <a title="61-tfidf-15" href="./acl-2010-Finding_Cognate_Groups_Using_Phylogenies.html">116 acl-2010-Finding Cognate Groups Using Phylogenies</a></p>
<p>16 0.034875765 <a title="61-tfidf-16" href="./acl-2010-Hierarchical_A%2A_Parsing_with_Bridge_Outside_Scores.html">131 acl-2010-Hierarchical A* Parsing with Bridge Outside Scores</a></p>
<p>17 0.034273278 <a title="61-tfidf-17" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>18 0.034262583 <a title="61-tfidf-18" href="./acl-2010-Automatic_Collocation_Suggestion_in_Academic_Writing.html">36 acl-2010-Automatic Collocation Suggestion in Academic Writing</a></p>
<p>19 0.032671981 <a title="61-tfidf-19" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>20 0.032490987 <a title="61-tfidf-20" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.1), (1, 0.023), (2, -0.016), (3, -0.045), (4, 0.013), (5, -0.046), (6, -0.0), (7, 0.001), (8, 0.041), (9, -0.02), (10, -0.025), (11, 0.029), (12, 0.056), (13, -0.01), (14, 0.018), (15, 0.003), (16, 0.015), (17, 0.045), (18, -0.014), (19, 0.048), (20, -0.048), (21, -0.017), (22, -0.004), (23, -0.078), (24, -0.024), (25, 0.014), (26, -0.06), (27, -0.025), (28, 0.035), (29, -0.008), (30, -0.019), (31, 0.016), (32, 0.06), (33, -0.028), (34, 0.013), (35, 0.012), (36, -0.038), (37, 0.012), (38, -0.041), (39, 0.106), (40, 0.006), (41, -0.032), (42, 0.032), (43, 0.027), (44, -0.023), (45, 0.064), (46, -0.002), (47, -0.007), (48, 0.057), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9041639 <a title="61-lsi-1" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>Author: Morgan Sonderegger ; Partha Niyogi</p><p>Abstract: English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. We describe a longitudinal dataset of N/V pair pronunciations, leading to a set of properties to be accounted for by any computational model. We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. We compare each model’s dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect population-level dynamics.</p><p>2 0.72639269 <a title="61-lsi-2" href="./acl-2010-Correcting_Errors_in_Speech_Recognition_with_Articulatory_Dynamics.html">74 acl-2010-Correcting Errors in Speech Recognition with Articulatory Dynamics</a></p>
<p>Author: Frank Rudzicz</p><p>Abstract: We introduce a novel mechanism for incorporating articulatory dynamics into speech recognition with the theory of task dynamics. This system reranks sentencelevel hypotheses by the likelihoods of their hypothetical articulatory realizations which are derived from relationships learned with aligned acoustic/articulatory data. Experiments compare this with two baseline systems, namely an acoustic hidden Markov model and a dynamic Bayes network augmented with discretized representations of the vocal tract. Our system based on task dynamics reduces worderror rates significantly by 10.2% relative to the best baseline models.</p><p>3 0.69171679 <a title="61-lsi-3" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>Author: Daniil Umanski ; Federico Sangati</p><p>Abstract: The growing availability of spoken language corpora presents new opportunities for enriching the methodologies of speech and language therapy. In this paper, we present a novel approach for constructing speech motor exercises, based on linguistic knowledge extracted from spoken language corpora. In our study with the Dutch Spoken Corpus, syllabic inventories were obtained by means of automatic syllabification of the spoken language data. Our experimental syllabification method exhibited a reliable performance, and allowed for the acquisition of syllabic tokens from the corpus. Consequently, the syl- labic tokens were integrated in a tool for clinicians, a result which holds the potential of contributing to the current state of speech motor training methodologies.</p><p>4 0.63623178 <a title="61-lsi-4" href="./acl-2010-Modeling_Norms_of_Turn-Taking_in_Multi-Party_Conversation.html">173 acl-2010-Modeling Norms of Turn-Taking in Multi-Party Conversation</a></p>
<p>Author: Kornel Laskowski</p><p>Abstract: Substantial research effort has been invested in recent decades into the computational study and automatic processing of multi-party conversation. While most aspects of conversational speech have benefited from a wide availability of analytic, computationally tractable techniques, only qualitative assessments are available for characterizing multi-party turn-taking. The current paper attempts to address this deficiency by first proposing a framework for computing turn-taking model perplexity, and then by evaluating several multi-participant modeling approaches. Experiments show that direct multi-participant models do not generalize to held out data, and likely never will, for practical reasons. In contrast, the Extended-Degree-of-Overlap model represents a suitable candidate for future work in this area, and is shown to successfully predict the distribution of speech in time and across participants in previously unseen conversations.</p><p>5 0.57289392 <a title="61-lsi-5" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>Author: Richard Power</p><p>Abstract: We describe the strategy currently pursued for verbalising OWL ontologies by sentences in Controlled Natural Language (i.e., combining generic rules for realising logical patterns with ontology-specific lexicons for realising atomic terms for individuals, classes, and properties) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology. We then show, through analysis of a corpus of ontologies, that although these assumptions could in principle be violated, they are overwhelmingly respected in practice by ontology developers.</p><p>6 0.56345505 <a title="61-lsi-6" href="./acl-2010-Hunting_for_the_Black_Swan%3A_Risk_Mining_from_Text.html">138 acl-2010-Hunting for the Black Swan: Risk Mining from Text</a></p>
<p>7 0.50071782 <a title="61-lsi-7" href="./acl-2010-Automatically_Generating_Term_Frequency_Induced_Taxonomies.html">43 acl-2010-Automatically Generating Term Frequency Induced Taxonomies</a></p>
<p>8 0.47926807 <a title="61-lsi-8" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>9 0.46345976 <a title="61-lsi-9" href="./acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</a></p>
<p>10 0.46150586 <a title="61-lsi-10" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>11 0.45905751 <a title="61-lsi-11" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>12 0.45843443 <a title="61-lsi-12" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>13 0.45794192 <a title="61-lsi-13" href="./acl-2010-Domain_Adaptation_of_Maximum_Entropy_Language_Models.html">91 acl-2010-Domain Adaptation of Maximum Entropy Language Models</a></p>
<p>14 0.45346558 <a title="61-lsi-14" href="./acl-2010-Intelligent_Selection_of_Language_Model_Training_Data.html">151 acl-2010-Intelligent Selection of Language Model Training Data</a></p>
<p>15 0.44177869 <a title="61-lsi-15" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>16 0.434484 <a title="61-lsi-16" href="./acl-2010-Optimal_Rank_Reduction_for_Linear_Context-Free_Rewriting_Systems_with_Fan-Out_Two.html">186 acl-2010-Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two</a></p>
<p>17 0.43104047 <a title="61-lsi-17" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>18 0.42503521 <a title="61-lsi-18" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>19 0.42163676 <a title="61-lsi-19" href="./acl-2010-A_Game-Theoretic_Model_of_Metaphorical_Bargaining.html">6 acl-2010-A Game-Theoretic Model of Metaphorical Bargaining</a></p>
<p>20 0.41607395 <a title="61-lsi-20" href="./acl-2010-Tools_for_Multilingual_Grammar-Based_Translation_on_the_Web.html">235 acl-2010-Tools for Multilingual Grammar-Based Translation on the Web</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.321), (14, 0.016), (25, 0.049), (33, 0.018), (34, 0.02), (39, 0.015), (42, 0.028), (59, 0.081), (69, 0.012), (73, 0.036), (76, 0.017), (78, 0.02), (83, 0.084), (84, 0.053), (87, 0.019), (98, 0.087)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76412988 <a title="61-lda-1" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>Author: Morgan Sonderegger ; Partha Niyogi</p><p>Abstract: English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. We describe a longitudinal dataset of N/V pair pronunciations, leading to a set of properties to be accounted for by any computational model. We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. We compare each model’s dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect population-level dynamics.</p><p>2 0.73885846 <a title="61-lda-2" href="./acl-2010-Optimal_Rank_Reduction_for_Linear_Context-Free_Rewriting_Systems_with_Fan-Out_Two.html">186 acl-2010-Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two</a></p>
<p>Author: Benoit Sagot ; Giorgio Satta</p><p>Abstract: Linear Context-Free Rewriting Systems (LCFRSs) are a grammar formalism capable of modeling discontinuous phrases. Many parsing applications use LCFRSs where the fan-out (a measure of the discontinuity of phrases) does not exceed 2. We present an efficient algorithm for optimal reduction of the length of production right-hand side in LCFRSs with fan-out at most 2. This results in asymptotical running time improvement for known parsing algorithms for this class.</p><p>3 0.67464012 <a title="61-lda-3" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>4 0.67102909 <a title="61-lda-4" href="./acl-2010-A_New_Approach_to_Improving_Multilingual_Summarization_Using_a_Genetic_Algorithm.html">11 acl-2010-A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm</a></p>
<p>Author: Marina Litvak ; Mark Last ; Menahem Friedman</p><p>Abstract: Automated summarization methods can be defined as “language-independent,” if they are not based on any languagespecific knowledge. Such methods can be used for multilingual summarization defined by Mani (2001) as “processing several languages, with summary in the same language as input.” In this paper, we introduce MUSE, a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages—English and Hebrew—and evaluated its performance with ROUGE-1 Recall vs. state- of-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank1) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.</p><p>5 0.48226583 <a title="61-lda-5" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>Author: Trevor Cohn ; Phil Blunsom</p><p>Abstract: Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently con- verges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</p><p>6 0.46798369 <a title="61-lda-6" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>7 0.45291883 <a title="61-lda-7" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>8 0.45289475 <a title="61-lda-8" href="./acl-2010-Efficient_Inference_through_Cascades_of_Weighted_Tree_Transducers.html">95 acl-2010-Efficient Inference through Cascades of Weighted Tree Transducers</a></p>
<p>9 0.45170951 <a title="61-lda-9" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>10 0.45040387 <a title="61-lda-10" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>11 0.44826269 <a title="61-lda-11" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>12 0.44497424 <a title="61-lda-12" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>13 0.44482392 <a title="61-lda-13" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>14 0.44408083 <a title="61-lda-14" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>15 0.44367912 <a title="61-lda-15" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>16 0.44081843 <a title="61-lda-16" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>17 0.44030857 <a title="61-lda-17" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>18 0.44002137 <a title="61-lda-18" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>19 0.43985438 <a title="61-lda-19" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>20 0.43978596 <a title="61-lda-20" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
