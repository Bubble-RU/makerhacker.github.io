<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-219" href="#">acl2010-219</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</h1>
<br/><p>Source: <a title="acl-2010-219-pdf" href="http://aclweb.org/anthology//P/P10/P10-1142.pdf">pdf</a></p><p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>Reference: <a title="acl-2010-219-reference" href="../acl2010_reference/acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago. [sent-2, score-0.707]
</p><p>2 1 Introduction Noun phrase (NP) coreference resolution, the task of determining which NPs in a text or dialogue refer to the same real-world entity, has been at the core of natural language processing (NLP) since the 1960s. [sent-3, score-0.659]
</p><p>3 NP coreference is related to the task of anaphora resolution, whose goal is to identify an antecedent for an anaphoric NP (i. [sent-4, score-1.038]
</p><p>4 Despite its simple task definition, coreference is generally considered a difficult NLP task, typically involving the use of sophisticated knowledge sources and inference procedures (Charniak, 1972). [sent-7, score-0.659]
</p><p>5 (1983; 1995)), have heavily influenced coreference research in the 1970s and 1980s, leading to the development of numerous centering algorithms (see Walker et al. [sent-9, score-0.702]
</p><p>6 The focus of coreference research underwent a gradual shift from heuristic approaches to machine learning approaches in the 1990s. [sent-11, score-0.659]
</p><p>7 This shift can be attributed in part to the advent of the statistical NLP era, and in part to the public availability of annotated coreference corpora produced as part of the MUC-6 (1995) and MUC-7 (1998) conferences. [sent-12, score-0.659]
</p><p>8 Learning-based coreference research has remained vibrant since then, with results regularly  . [sent-13, score-0.659]
</p><p>9 Being inherently a clustering task, coreference has also received a lot of attention in the machine learning community. [sent-20, score-0.744]
</p><p>10 Fifteen years have passed since the first paper on learning-based coreference resolution was published (Connolly et al. [sent-21, score-0.873]
</p><p>11 Our goal in this paper is to provide NLP researchers with a survey of the major milestones in supervised coreference research, focusing on the computational models, the linguistic features, the annotated corpora, and the evaluation metrics that were developed in the past fifteen years. [sent-23, score-0.765]
</p><p>12 Note that several leading coreference researchers have published books (e. [sent-24, score-0.691]
</p><p>13 , Strube (2002), Ponzetto and Poesio (2009)) that provide a broad overview of  coreference research. [sent-30, score-0.659]
</p><p>14 In particular, while existing survey papers discuss learning-based coreference research primarily in the context of the influential mention-pair model, we additionally survey recently proposed learning-based coreference models, which attempt to address the weaknesses of the mention-pair model. [sent-32, score-1.37]
</p><p>15 Due to space limitations, however, we will restrict our discussion to the most commonly investigated kind of coreference relation: the identity relation for NPs, excluding coreference among clauses and bridging references (e. [sent-33, score-1.318]
</p><p>16 2  Annotated Corpora  The widespread popularity of machine learning approaches to coreference resolution can be attributed in part to the public availability of an1396  Proce dingUsp opfs thaela 4, 8Stwhe Adnen u,a 1l1- M16e Jtiunlgy o 2f0 t1h0e. [sent-36, score-0.873]
</p><p>17 c s 2o0c1ia0ti Aosnso focria Ctio nm fpourta Ctoiomnpault Laitniognuaislt Licisn,g puaigsetisc 1s396–141 , notated coreference  corpora. [sent-38, score-0.659]
</p><p>18 document type (newswire articles only), have been extensively used for training and evaluating coreference models. [sent-42, score-0.659]
</p><p>19 , 1993), which is labeled with coreference links as part of the OntoNotes project (Hovy et al. [sent-50, score-0.659]
</p><p>20 Other publicly available coreference corpora of interest include two annotated by Ruslan Mitkov’s research group: (1) a 55,000-word corpus in the domain of security/terrorism (Hasler et al. [sent-56, score-0.659]
</p><p>21 Additional coreference data will be available in the near future. [sent-61, score-0.659]
</p><p>22 , 2009) has promised to release coreference data in six languages. [sent-63, score-0.659]
</p><p>23 In addition, Massimo Poesio and his colleagues are leading an annotation project that aims to collect large  amounts of coreference data for English via a Web Collaboration game called Phrase Detectives2. [sent-64, score-0.659]
</p><p>24 org 3  Learning-Based Coreference Models  In this section, we examine three important classes of coreference models that were developed in the past fifteen years, namely, the mention-pair model, the entity-mention model, and ranking models. [sent-70, score-0.749]
</p><p>25 It was first proposed by Aone and Bennett (1995) and McCarthy and Lehnert (1995), and is one of the most influential learning-based coreference models. [sent-73, score-0.659]
</p><p>26 Hence, a separate clustering mechanism is needed to coordinate the pairwise classification decisions made by the model and construct a coreference partition. [sent-75, score-0.842]
</p><p>27 With an eye towards improving the precision of a coreference resolver, Ng and Cardie (2002c) propose an instance creation method that involves a single modification to Soon et al. [sent-91, score-0.693]
</p><p>28 For example, motivated by the fact that some coreference relations are harder to identify than the others (see Harabagiu et al. [sent-100, score-0.659]
</p><p>29 (2001)), Ng and Cardie (2002a) present a method for mining easy positive instances, in an attempt to avoid the inclusion ofhard training instances that may complicate the acquisition of an accurate coreference model. [sent-101, score-0.659]
</p><p>30 2 Training a Coreference Classifier Once a training set is created, we can train a coreference model using an off-the-shelf learning algorithm. [sent-104, score-0.659]
</p><p>31 , C5 (Quinlan, 1993)) are the first and one of the most widely used learning algorithms by coreference researchers, although rule learners (e. [sent-107, score-0.659]
</p><p>32 , TiMBL (Daelemans and Van den Bosch, 2005)) are also popular choices, especially in early applications of machine learning to coreference resolution. [sent-111, score-0.659]
</p><p>33 , 1996), voted perceptrons (Freund and Schapire, 1999), 3In this paper, we use the term anaphoric to describe any NP that is part of a coreference chain but is not the head of the chain. [sent-113, score-0.77]
</p><p>34 , in the form of a probability) associated with a classifica-  tion, and in part due to the fact that they can be easily adapted to train recently proposed rankingbased coreference models (see Section 3. [sent-117, score-0.659]
</p><p>35 Below we describe some commonly used coreference clustering algorithms. [sent-122, score-0.744]
</p><p>36 , 2001) and best-first clustering (Ng and Cardie, 2002c) are arguably the most widely used coreference clustering algorithms. [sent-124, score-0.829]
</p><p>37 Several algorithms that address one or both of these problems have been used for coreference clustering. [sent-134, score-0.659]
</p><p>38 Graph partitioning algorithms are applied on a weighted, undirected graph where a vertex corresponds to an NP and an edge is weighted by the pairwise coreference scores between two NPs (e. [sent-138, score-0.703]
</p><p>39 Some clustering algorithms bear a closer resemblance to the way a human creates coreference clusters. [sent-143, score-0.744]
</p><p>40 In these algorithms, not only are the NPs in a text processed in a left-to-right manner, the later coreference decisions are dependent on the earlier ones (Cardie and Wagstaff, 1999; Klenner and Ailloud, 2008). [sent-144, score-0.713]
</p><p>41 If NPk and NPj are likely to be coreferent, the algorithm imposes an additional check that NPk does not violate any constraint on coreference (e. [sent-146, score-0.659]
</p><p>42 ’s (2004) Bell-tree-based algorithm is  another clustering algorithm where the later coreference decisions are dependent on the earlier ones. [sent-150, score-0.798]
</p><p>43 Specifically, based on the coreference decisions it has made in the first i−1 levels of tehnec tree, tshieo algorithm ddeet einrm thinee fisr satt i −the1 ilethv elelsv oefl whether the ith NP should start a new cluster, or to which preceding cluster it should be assigned. [sent-157, score-0.799]
</p><p>44 While many coreference clustering algorithms have been developed, there have only been a few attempts to compare their effectiveness. [sent-158, score-0.744]
</p><p>45 4 Determining NP Anaphoricity While coreference clustering algorithms attempt to resolve each NP encountered in a document, only a subset of the NPs are anaphoric and therefore need to be resolved. [sent-165, score-0.855]
</p><p>46 Hence, knowledge of the anaphoricity of an NP can potentially improve the precision of a coreference resolver. [sent-166, score-0.78]
</p><p>47 Traditionally, the task of anaphoricity determination has been tackled independently of coreference resolution using a variety of techniques. [sent-167, score-1.03]
</p><p>48 Recently, anaphoricity determination has been evaluated in the context of coreference resolution, with results showing that training an anaphoricity classifier to identify and filter non-anaphoric NPs prior to coreference resolution can improve a learning-based resolver (e. [sent-181, score-1.885]
</p><p>49 5 Combining Classification & Clustering From a learning perspective, a two-step approach to coreference classification and clustering is undesirable. [sent-189, score-0.744]
</p><p>50 That is, overall performance on the coreference task might not improve. [sent-191, score-0.659]
</p><p>51 6 Weaknesses of the Mention-Pair Model While many of the aforementioned algorithms  for clustering and anaphoricity determination have been shown to improve coreference performance, the underlying model with which they are used in combination the mention-pair model remains fundamentally weak. [sent-196, score-0.901]
</p><p>52 First, since each candidate antecedent for an anaphoric NP to be resolved is considered independently of the others, the model only determines how good a candidate antecedent is relative to the anaphoric NP, but not how good a candidate antecedent is relative to other candidates. [sent-198, score-0.8]
</p><p>53 This kind of error arises in part because the later coreference decisions are not dependent on the earlier ones. [sent-212, score-0.713]
</p><p>54 Daum e´ III and Marcu (2005) propose an online learning model for constructing coreference chains in an incremental fashion, allowing later coreference decisions to be made by exploiting cluster-level features that are computed over the coreference chains created thus far. [sent-237, score-2.031]
</p><p>55 Ranking is arguably a more natural reformulation of coreference resolution than classification, as a ranker allows all candidate antecedents to be considered simultaneously and therefore directly captures the competition among them. [sent-241, score-1.013]
</p><p>56 Ranking is first applied to learning-based coreference resolution by Connolly et al. [sent-246, score-0.873]
</p><p>57 To address this problem, Denis and Baldridge (2008) apply an independently trained anaphoricity classifier to identify non-anaphoric NPs prior to ranking, and Rahman and Ng (2009) propose a model that jointly learns coreference and anaphoricity. [sent-261, score-0.78]
</p><p>58 4  Knowledge Sources  Another thread of supervised coreference research concerns the development of linguistic features. [sent-262, score-0.659]
</p><p>59 String-matching features can be computed robustly and typically contribute a lot to the performance of a coreference system. [sent-264, score-0.659]
</p><p>60 There has been an increasing amount ofwork on investigating semantic features for coreference resolution. [sent-294, score-0.659]
</p><p>61 One of the earliest kinds of semantic knowledge employed for coreference resolution is perhaps selectional preference (Dagan and Itai, 1990; Kehler et al. [sent-295, score-0.873]
</p><p>62 (2007) for other uses of encyclopedic knowledge for coreference resolution). [sent-305, score-0.659]
</p><p>63 , 2009), and animacy (Or a˘san and Evans, 2007) have also  been exploited to improve coreference resolution. [sent-308, score-0.659]
</p><p>64 Finally, the outputs of rule-based pronoun and coreference resolvers have also been used as features for learning-based coreference resolution (Ng and Cardie, 2002c). [sent-330, score-1.698]
</p><p>65 5  Evaluation Issues  Two important issues surround the evaluation of a coreference resolver. [sent-332, score-0.659]
</p><p>66 , MUC-6 and MUC-7), the NPs that are not part of any coreference chain are not annotated. [sent-345, score-0.659]
</p><p>67 While researchers who evaluate their resolvers on gold NPs point out that the results can more accurately reflect the performance of their coreference algorithm, Stoyanov et al. [sent-350, score-0.736]
</p><p>68 Whichever NP extraction method is employed, it is clear that the use of gold NPs can considerably simplify the coreference task, and hence resolvers employing different extraction methods should not be compared against each other. [sent-352, score-0.704]
</p><p>69 , 1995) is the first  program developed for scoring coreference partitions. [sent-355, score-0.659]
</p><p>70 As a linkbased measure, it does not reward correctly identified singleton clusters since there is no coreference link in these clusters. [sent-357, score-0.659]
</p><p>71 To address these problems, two coreference scoring programs have been developed: B3 (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). [sent-359, score-0.659]
</p><p>72 Since coreference is a clustering task, any general-purpose method for evaluating a response partition against a key partition (e. [sent-363, score-0.86]
</p><p>73 , Kappa (Carletta, 1996)) can be used for coreference scor1403  ing (see Popescu-Belis et al. [sent-365, score-0.659]
</p><p>74 In practice, these general-purpose methods are typically used  to provide scores that complement those obtained via the three coreference scorers discussed above. [sent-367, score-0.699]
</p><p>75 Interestingly, recent results indicate that unsupervised approaches to coreference resolution (e. [sent-373, score-0.873]
</p><p>76 While many of the techniques discussed in this paper were originally developed for English, they have been applied to learn coreference models for other languages, such as Chinese (e. [sent-377, score-0.659]
</p><p>77 As Mitkov (2001) puts it, coreference resolution is a “difficult, but not intractable problem,” and we have been making “slow, but steady progress” on improving machine learning approaches to the problem in the past fifteen years. [sent-398, score-0.921]
</p><p>78 Publicly available coreference systems currently include JavaRAP (Qiu et al. [sent-404, score-0.659]
</p><p>79 We conclude with a discussion of two questions regarding supervised coreference research. [sent-408, score-0.659]
</p><p>80 Second, what lessons can we learn from fifteen years of learning-based coreference research? [sent-418, score-0.707]
</p><p>81 The mention-pair model is weak because it makes coreference decisions based on local information (i. [sent-419, score-0.713]
</p><p>82 Global coreference models may refer to any kind of models that can exploit non-local information, including models that can consider multiple candidate antecedents simultaneously (e. [sent-425, score-0.77]
</p><p>83 ,  ranking models), models that allow joint learning for coreference resolution and related tasks (e. [sent-427, score-0.915]
</p><p>84 8These results by no means suggest that no progress has been made since 2003: most of the recently proposed coreference models were evaluated on the ACE data sets. [sent-430, score-0.659]
</p><p>85 Unsupervised learning of contextual role knowledge for coreference resolution. [sent-456, score-0.659]
</p><p>86 Global, joint determination of anaphoricity and coreference resolution using integer programming. [sent-561, score-1.03]
</p><p>87 Simple coreference resolution with rich syntactic and semantic features. [sent-625, score-0.873]
</p><p>88 Incorporating contextual cues in trainable models for coreference resolution. [sent-681, score-0.659]
</p><p>89 Annotating a Japanese text corpus with predicate-argument and coreference relations. [sent-693, score-0.659]
</p><p>90 Optimization in coreference resolution is not needed: A nearly-optimal algorithm with intensional constraints. [sent-736, score-0.873]
</p><p>91 A mentionsynchronous coreference resolution algorithm based on the Bell tree. [sent-756, score-0.873]
</p><p>92 Coreference or not: A twin model for coreference resolution. [sent-764, score-0.659]
</p><p>93 Combining sample selection and error-driven pruning for machine learning of coreference rules. [sent-822, score-0.659]
</p><p>94 Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. [sent-826, score-0.812]
</p><p>95 State-of-the-art NLP approaches to coreference resolution: Theory and practical recipes. [sent-903, score-0.659]
</p><p>96 A machine learning ap-  proach to coreference resolution of noun phrases. [sent-959, score-0.915]
</p><p>97 Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. [sent-963, score-0.701]
</p><p>98 A constraint-based approach to noun phrase coreference resolution in German newspaper text. [sent-1015, score-0.915]
</p><p>99 An entity-mention model for coreference resolution with inductive logic programming. [sent-1068, score-0.873]
</p><p>100 Global learning of noun phrase anaphoricity in coreference resolution via label propagation. [sent-1084, score-1.036]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.659), ('nps', 0.284), ('resolution', 0.214), ('npk', 0.191), ('anaphora', 0.143), ('antecedent', 0.125), ('pronoun', 0.121), ('anaphoricity', 0.121), ('np', 0.115), ('anaphoric', 0.111), ('coreferent', 0.101), ('xiaofeng', 0.089), ('iida', 0.086), ('clustering', 0.085), ('cardie', 0.076), ('ng', 0.075), ('resolver', 0.075), ('poesio', 0.072), ('nicolae', 0.07), ('bean', 0.07), ('npj', 0.07), ('clinton', 0.068), ('ponzetto', 0.063), ('yang', 0.059), ('luo', 0.059), ('partition', 0.058), ('versley', 0.056), ('computationallinguistics', 0.056), ('pages', 0.056), ('antecedents', 0.056), ('candidate', 0.055), ('strube', 0.055), ('preceding', 0.054), ('decisions', 0.054), ('massimo', 0.053), ('conference', 0.051), ('vincent', 0.051), ('jian', 0.05), ('proceedings', 0.048), ('denis', 0.048), ('fifteen', 0.048), ('chew', 0.048), ('resolvers', 0.045), ('grosz', 0.045), ('pairwise', 0.044), ('centering', 0.043), ('ryu', 0.043), ('ranking', 0.042), ('noun', 0.042), ('ruslan', 0.041), ('mitkov', 0.041), ('connolly', 0.04), ('soon', 0.04), ('rahman', 0.04), ('scorers', 0.04), ('associationfor', 0.04), ('lim', 0.039), ('definite', 0.038), ('resolved', 0.038), ('kehler', 0.038), ('yannick', 0.038), ('klenner', 0.038), ('simone', 0.037), ('determination', 0.036), ('expressiveness', 0.036), ('claire', 0.035), ('recasens', 0.035), ('jason', 0.034), ('creation', 0.034), ('bergsma', 0.034), ('stoyanov', 0.034), ('inui', 0.034), ('gender', 0.033), ('rankers', 0.032), ('bengtson', 0.032), ('researchers', 0.032), ('su', 0.032), ('cluster', 0.032), ('empirical', 0.03), ('wellner', 0.03), ('entitymention', 0.03), ('outnumber', 0.03), ('vieira', 0.03), ('wagstaff', 0.03), ('daum', 0.03), ('riloff', 0.03), ('kentaro', 0.029), ('paolo', 0.029), ('yuji', 0.029), ('ranker', 0.029), ('xiaoqiang', 0.029), ('ace', 0.028), ('pronominal', 0.028), ('guodong', 0.027), ('chapter', 0.027), ('haghighi', 0.027), ('survey', 0.026), ('uryupina', 0.026), ('lappin', 0.026), ('muc', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999821 <a title="219-tfidf-1" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>2 0.53422922 <a title="219-tfidf-2" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>3 0.4648225 <a title="219-tfidf-3" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>4 0.42356139 <a title="219-tfidf-4" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>5 0.28313285 <a title="219-tfidf-5" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>6 0.2651526 <a title="219-tfidf-6" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>7 0.21559237 <a title="219-tfidf-7" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>8 0.20426847 <a title="219-tfidf-8" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>9 0.14232129 <a title="219-tfidf-9" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>10 0.13749973 <a title="219-tfidf-10" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>11 0.12047432 <a title="219-tfidf-11" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>12 0.11658666 <a title="219-tfidf-12" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>13 0.11227199 <a title="219-tfidf-13" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>14 0.09316469 <a title="219-tfidf-14" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>15 0.084593676 <a title="219-tfidf-15" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>16 0.065839641 <a title="219-tfidf-16" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>17 0.065018684 <a title="219-tfidf-17" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>18 0.064680561 <a title="219-tfidf-18" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>19 0.061571084 <a title="219-tfidf-19" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>20 0.058612417 <a title="219-tfidf-20" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.228), (1, 0.147), (2, 0.024), (3, -0.36), (4, -0.219), (5, 0.532), (6, 0.018), (7, 0.038), (8, 0.069), (9, 0.186), (10, 0.096), (11, -0.11), (12, 0.004), (13, -0.095), (14, 0.065), (15, -0.017), (16, -0.015), (17, -0.095), (18, -0.118), (19, 0.015), (20, -0.028), (21, 0.006), (22, 0.003), (23, -0.078), (24, -0.004), (25, 0.006), (26, -0.038), (27, -0.025), (28, -0.039), (29, 0.024), (30, -0.06), (31, -0.066), (32, -0.015), (33, 0.015), (34, -0.026), (35, -0.038), (36, -0.04), (37, -0.036), (38, -0.018), (39, 0.053), (40, 0.009), (41, -0.051), (42, 0.07), (43, 0.013), (44, -0.012), (45, -0.034), (46, -0.0), (47, -0.031), (48, -0.056), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98588884 <a title="219-lsi-1" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>same-paper 2 0.96513575 <a title="219-lsi-2" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>3 0.92118514 <a title="219-lsi-3" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>4 0.87349075 <a title="219-lsi-4" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>5 0.57378638 <a title="219-lsi-5" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>Author: Ryu Iida ; Syumpei Kobayashi ; Takenobu Tokunaga</p><p>Abstract: This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001 ; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on refer- ence resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.</p><p>6 0.50809211 <a title="219-lsi-6" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>7 0.49621314 <a title="219-lsi-7" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>8 0.47613451 <a title="219-lsi-8" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>9 0.46656078 <a title="219-lsi-9" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>10 0.46588024 <a title="219-lsi-10" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>11 0.38771418 <a title="219-lsi-11" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>12 0.3194479 <a title="219-lsi-12" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>13 0.24339123 <a title="219-lsi-13" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>14 0.23747578 <a title="219-lsi-14" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>15 0.21996465 <a title="219-lsi-15" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>16 0.20725755 <a title="219-lsi-16" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>17 0.19107071 <a title="219-lsi-17" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>18 0.19001012 <a title="219-lsi-18" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>19 0.18988903 <a title="219-lsi-19" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>20 0.18981779 <a title="219-lsi-20" href="./acl-2010-Plot_Induction_and_Evolutionary_Search_for_Story_Generation.html">196 acl-2010-Plot Induction and Evolutionary Search for Story Generation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.013), (16, 0.017), (25, 0.062), (42, 0.077), (44, 0.016), (57, 0.01), (59, 0.091), (73, 0.037), (76, 0.01), (78, 0.04), (79, 0.124), (80, 0.023), (83, 0.193), (84, 0.032), (88, 0.034), (98, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88031185 <a title="219-lda-1" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>2 0.84966344 <a title="219-lda-2" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>3 0.84541595 <a title="219-lda-3" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>Author: Mark Sammons ; V.G.Vinod Vydiswaran ; Dan Roth</p><p>Abstract: We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment. We argue that the single global label with which RTE examples are annotated is insufficient to effectively evaluate RTE system performance; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will benefit not just RTE researchers, but the NLP community as a whole. We use insights from successful RTE systems to propose a model for identifying and annotating textual infer- ence phenomena in textual entailment examples, and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful.</p><p>4 0.83982646 <a title="219-lda-4" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>Author: David Elson ; Nicholas Dames ; Kathleen McKeown</p><p>Abstract: We present a method for extracting social networks from literature, namely, nineteenth-century British novels and serials. We derive the networks from dialogue interactions, and thus our method depends on the ability to determine when two characters are in conversation. Our approach involves character name chunking, quoted speech attribution and conversation detection given the set of quotes. We extract features from the social networks and examine their correlation with one another, as well as with metadata such as the novel’s setting. Our results provide evidence that the majority of novels in this time period do not fit two characterizations provided by literacy scholars. Instead, our results suggest an alternative explanation for differences in social networks.</p><p>5 0.8258816 <a title="219-lda-5" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>6 0.82482928 <a title="219-lda-6" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>7 0.81986332 <a title="219-lda-7" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>8 0.81576777 <a title="219-lda-8" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>9 0.81552088 <a title="219-lda-9" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>10 0.81500083 <a title="219-lda-10" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>11 0.81212449 <a title="219-lda-11" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>12 0.80409002 <a title="219-lda-12" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>13 0.80364728 <a title="219-lda-13" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>14 0.80182809 <a title="219-lda-14" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>15 0.79810232 <a title="219-lda-15" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>16 0.79568577 <a title="219-lda-16" href="./acl-2010-Collocation_Extraction_beyond_the_Independence_Assumption.html">60 acl-2010-Collocation Extraction beyond the Independence Assumption</a></p>
<p>17 0.79212654 <a title="219-lda-17" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>18 0.79198146 <a title="219-lda-18" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>19 0.79173607 <a title="219-lda-19" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>20 0.79018861 <a title="219-lda-20" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
