<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-142" href="#">acl2010-142</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</h1>
<br/><p>Source: <a title="acl-2010-142-pdf" href="http://aclweb.org/anthology//P/P10/P10-1019.pdf">pdf</a></p><p>Author: Ethan Selfridge ; Peter Heeman</p><p>Abstract: Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dia- logues, and supports the improvement of mixed-initiative interaction.</p><p>Reference: <a title="acl-2010-142-reference" href="../acl2010_reference/acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. [sent-6, score-0.468]
</p><p>2 Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. [sent-9, score-0.373]
</p><p>3 For example, systems using the Keep-Or-Release approach will not attempt to take the turn unless it is sure the user has released it. [sent-21, score-0.406]
</p><p>4 Given this, any dialogue system hoping to interact with  humans efficiently and naturally should have a negotiative and importance-driven quality to its turn-taking protocol. [sent-30, score-0.403]
</p><p>5 We propose the Importance-Driven Turn-Bidding (IDTB) model where conversants bid for the turn based on the importance of their utterance. [sent-32, score-0.591]
</p><p>6 By allowing conversants to bid for the turn, the IDTB model enables negotiative turntaking and supports true mixed-initiative interaction, and with it, greater dialogue efficiency. [sent-34, score-0.837]
</p><p>7 Using an artificial collaborative dialogue task, we show that the IDTB model enables the system and user to complete 177  Proce dinUgsp osfa tlhae, 4S8wthed Aen n,u 1a1l-1 M6e Jeutilnyg 2 o0f1 t0h. [sent-36, score-0.537]
</p><p>8 2  Current Turn-Taking Approaches  Current dialogue systems focus on the release-turn as the most important aspect of turn-taking, in which a listener will only take the turn after the speaker has released it. [sent-41, score-0.504]
</p><p>9 The simplest of these approaches only allows a single utterance per turn, after which the turn necessarily transitions to the next speaker. [sent-42, score-0.353]
</p><p>10 First, the speaker-centricity leads to inefficient dialogues  since the speaker may continue to hold the turn even when the listener has vital information to give. [sent-57, score-0.362]
</p><p>11 In addition, the lack of negotiation forces the turn to necessarily transition to the listener after the speaker releases it. [sent-58, score-0.309]
</p><p>12 The possibility that the dialogue may be better served if the listener does not get the turn is not addressed by current approaches. [sent-59, score-0.399]
</p><p>13 (2002) used decision trees to determine whether the system should take the turn or not when the user pauses. [sent-64, score-0.427]
</p><p>14 The IDTB model has two foundational components: (1) The importance of speaking is the primary motivation behind turn-taking behavior, and (2) conversants use turncue strength to bid for the turn based on this importance. [sent-68, score-0.649]
</p><p>15 The IDTB model uses turn-cue strength to bid for the turn based on the importance of the utterance. [sent-76, score-0.564]
</p><p>16 In the prototype described in Section 5, both the system and user agents bid for the turn after every utterance and the bids are conceptualized here  as utterance onset: conversants should be quick to speak important utterances but slow with less important ones. [sent-78, score-1.435]
</p><p>17 2 By using RL to learn both the utterance and bid behavior, the system can find an optimal pairing between them, and choose the best combination for a given conversational situation. [sent-84, score-0.544]
</p><p>18 4  Information State Update and Reinforcement Learning  We build our dialogue system using the Information State Update approach (Larsson and Traum,  2000) and use Reinforcement Learning for action selection (Sutton and Barto, 1998). [sent-85, score-0.333]
</p><p>19 It has been shown that using RL to learn dialogue polices is generally more effective than “hand crafted” di—  —  1Our work (present and future) is distinct from some recent work on user pauses (Sato et al. [sent-89, score-0.448]
</p><p>20 Reinforcement Learning learns an optimal policy, a mapping between a state s and action a, where performing a in s leads to the lowest ex-  pected cost for the dialogue (we use minimum cost instead of maximum reward). [sent-95, score-0.42]
</p><p>21 The user’s role is to specify its desired filler for each slot, though that specific filler may not be available. [sent-116, score-0.558]
</p><p>22 3 A user’s top choice is either available, in which case we say that the user has adequate filler knowledge, or their second choice will be available, in which we say it has inadequate filler knowledge. [sent-121, score-0.867]
</p><p>23 Whether a user has adequate or inadequate filler knowledge is probabilistically determined based on user type, which will be described in Section 5. [sent-123, score-0.823]
</p><p>24 vlsaoelitrma, bvf[ialy etasrb/nl,eo]by  We model conversations at the speech act level, shown in Table 1, and so do not model the actual words that the user and system might say. [sent-126, score-0.305]
</p><p>25 The major user specific IS variables are three desiredSlotFiller variables that hold an ordered list  of fillers, and unvisitedSlots, a list of slots that the user believes are unfilled. [sent-131, score-0.569]
</p><p>26 three inform available slot fillers actions, which lists the available fillers for that slot and is proposed if that specific slot is unfilled or filled with an unavailable filler; and bye, which is always proposed. [sent-134, score-0.997]
</p><p>27 They can inform the system of a desired slot filler, inform slot filler, or query the availability of a slot’s top filler, query filler availability. [sent-136, score-1.05]
</p><p>28 A user will always respond with the same slot as a system query, but may change slots entirely for all other situations. [sent-137, score-0.55]
</p><p>29 Additional  details on user action selection are given in Section 5. [sent-138, score-0.31]
</p><p>30 For example, the speech action inform slot filler results in the utterance of ”inform drink d1. [sent-141, score-0.909]
</p><p>31 Notice that in Line 3 the system informs the user that their first filler, d1, is unavailable. [sent-143, score-0.337]
</p><p>32 The user then asks asks about the availability of its second drink choice, d2 (Line 4), and upon receiving an affirmative response (Line 5), informs the system of that filler preference (Line 6). [sent-144, score-0.642]
</p><p>33 In this domain we use a cost function based on dialogue length and the number of slots filled with an available filler: C = Number of Utterances + 25 · unavailablyFilledSlots. [sent-146, score-0.342]
</p><p>34 The system chooses the action that minimizes the expected cost of the entire dialogue from the current state. [sent-148, score-0.385]
</p><p>35 User types differ probabilistically on two dimensions: slot knowledge, and slot belief strength. [sent-155, score-0.544]
</p><p>36 We define experts to have a 90 percent chance of having adequate filler knowledge, intermediates a 50 percent chance, and novices a 10 percent chance. [sent-156, score-0.595]
</p><p>37 Slot belief strength represents the user’s confidence that it has —  adequate domain knowledge for the slot (i. [sent-158, score-0.471]
</p><p>38 Initial slot belief strength is dependent on user type and whether their filler knowledge is adequate (their initial top choice is available). [sent-163, score-0.957]
</p><p>39 Experts with adequate filler knowledge have a 70, 20, and 10 percent chance of having Strong, Warranted, and Weak beliefs respectfully. [sent-164, score-0.444]
</p><p>40 When these user types have inadequate filler knowledge the probabilities are reversed to determine belief strength (e. [sent-166, score-0.758]
</p><p>41 Experts with inadequate domain knowledge for a slot have a 70% chance of having a weak belief). [sent-168, score-0.335]
</p><p>42 The user choses whether to use the query or  inform speech action based on the slot’s belief strength. [sent-170, score-0.651]
</p><p>43 A strong belief will always result in an inform, a warranted belief resulting in an inform with p = 0. [sent-171, score-0.473]
</p><p>44 5, and weak belief will result in an inform with p = 0. [sent-172, score-0.322]
</p><p>45 If the user is informed of the correct fillers by the system’s inform, that slot’s belief strength is set to strong. [sent-174, score-0.542]
</p><p>46 If the user is informed that a filler is not available, than that filler is removed from the desired filler list and the belief remains the same. [sent-175, score-1.222]
</p><p>47 The system chooses its turn action based on the RL state and we add a boolean variable turn-action to the RL State to indicate when the system is performing a turn action or a speech action. [sent-178, score-0.587]
</p><p>48 Turn-Bidding: Agents bid for the turn at the end of each utterance to determine who will speak next. [sent-180, score-0.646]
</p><p>49 Each bid is represented as a value between 0  µ  and 1, and the agent with the lower value (stronger bid) wins the turn. [sent-181, score-0.382]
</p><p>50 The system uses RL to choose a bid and a random number (uniform distribution) is generated from that bid’s range. [sent-184, score-0.371]
</p><p>51 The users’ bids are determined by their belief strength, which specifies the mean of a Gaussian distribution, as shown in Figure 1 (e. [sent-185, score-0.326]
</p><p>52 Computing bids in this fashion leads to, on average, users with strong beliefs bidding highest, warranted beliefs bidding in the middle, and weak beliefs bidding lowest. [sent-188, score-0.664]
</p><p>53 Figure 1: Bid Value Probability Distribution Single-Utterance: The Single-Utterance (SU) approach, as described in Section 2, has a rigid 5In this simple domain the next filler is guaranteed to be available if the first is not. [sent-190, score-0.307]
</p><p>54 After a speaker makes a single utterance the turn transitions to the listener. [sent-193, score-0.434]
</p><p>55 Since the turn transitions after every utterance the system must only choose appropriate utterances, not turn-taking behavior. [sent-194, score-0.398]
</p><p>56 Similarly, user agents do not have any turn-taking behavior and slot beliefs are only used to choose between a query and an inform. [sent-195, score-0.571]
</p><p>57 Keep-Or-Release Model: The Keep-OrRelease (KR) model, as described in Section 2, allows the speaker to either keep the turn to make multiple utterances or release it. [sent-196, score-0.327]
</p><p>58 Taking the same approach as English and Heeman (2005), the system learns to keep or release the turn after each utterance that it makes. [sent-197, score-0.392]
</p><p>59 IDTB allows the conversants to negotiate the turn using turn-bids motivated by importance, whereas in KR only the speaker determines when the turn can transition. [sent-201, score-0.46]
</p><p>60 Users in the KR environment choose whether to keep or release the turn similarly to bid decisions. [sent-202, score-0.522]
</p><p>61 6 After a user performs an utterance, it chooses the slot that would be in the next utterance. [sent-203, score-0.432]
</p><p>62 A number, k, is generated from a Gaussian distribution using belief strength in the same manner as the IDTB  users’ bids are chosen. [sent-204, score-0.353]
</p><p>63 Experts always bid high and had complete domain knowledge, and the novices always bid low and had incomplete domain knowledge. [sent-213, score-0.75]
</p><p>64 The system, using all five bid types, was always able to out bid and under bid the simulated users. [sent-214, score-0.978]
</p><p>65 = 0), and the mean dialogue cost for that policy is determined. [sent-228, score-0.376]
</p><p>66 The mean policy cost between the turn-taking approaches and user conditions are shown in Table 3. [sent-230, score-0.427]
</p><p>67 o05 m521bined Single User Conditions: Single user conditions show how well each turn-taking approach can optimize its behavior for specific user populations and handle slight differences found in those populations. [sent-237, score-0.525]
</p><p>68 Since the SU system must respond to every user utterance and cannot learn a turn-taking strategy to utilize user knowledge, the dialogues are necessarily longer. [sent-239, score-0.78]
</p><p>69 For example, in the expert condition the best possible dialogue for a SU interaction will have a cost of five (three user utterances for each slot, two system utterances in response). [sent-240, score-0.798]
</p><p>70 This cost is in contrast to the best expert dialogue cost of three (three user utterances) for KR and IDTB interactions. [sent-241, score-0.584]
</p><p>71 The IDTB turn-taking approach outperforms  the KR design in all single user conditions ex7SD between policies ≤ 0. [sent-242, score-0.314]
</p><p>72 In this condition, the KR system takes the turn first, informs the available fillers for each slot, and then releases the turn. [sent-247, score-0.39]
</p><p>73 The IDTB system attempts a similar dialogue strategy by using highest bids but sometimes loses the turn when users also bid highest. [sent-249, score-0.922]
</p><p>74 If the user uses the turn to query or inform an unavailable filler the dialogue grows longer. [sent-250, score-1.087]
</p><p>75 Exploiting User bidding differences: It follows that IDTB’s performance stems from its negotiative turn transitions. [sent-268, score-0.377]
</p><p>76 A user that has a stronger belief strength is more likely to be have a higher bid and inform an available filler. [sent-270, score-0.903]
</p><p>77 Policy analysis shows that the IDTB system takes advantage of this information by using moderate bids —neither highest nor lowest bids— to filter users based on their turn behavior. [sent-271, score-0.383]
</p><p>78 The initial position refers to  the first bid of the dialogue; final position, the last bid of the dialogue; and medial position, all other bids. [sent-273, score-0.652]
</p><p>79 These distributions show that the system has learned to use the entire bid range to filter the users, and is not seeking to win or lose the turn outright. [sent-276, score-0.551]
</p><p>80 However, since solution quality never affects the dialogue cost for a trained system, dialogue length is the only component influencing the mean policy cost. [sent-290, score-0.589]
</p><p>81 The primary cause of longer dialogues are un-  available filler inform and query (UFI–Q) utterances by the user, which are easily identified. [sent-291, score-0.585]
</p><p>82 These utterances lengthen the dialogue since the system must inform the user of the available fillers (the user would otherwise not know that the filler was unavailable) and then the user must then inform the system of its second choice. [sent-292, score-1.726]
</p><p>83 The mean number of UFI–Q utterance for each dialogue over the ten learned policies are shown for all user conditions in Table 5. [sent-293, score-0.731]
</p><p>84 While a KR user will release the turn if its planned Table 5: Mean number of UFI–Q utterances over policies  IKMDRoTdBel0 N. [sent-298, score-0.531]
</p><p>85 o93m48bined  utterance has a weak belief, it may select that weak utterance when first getting the turn (either after a system utterance or at the start of the dialogue). [sent-303, score-0.787]
</p><p>86 The dialogue is the same until utterance 3, where the IDTB system wins the turn with a mid bid over the user’s low bid. [sent-307, score-0.936]
</p><p>87 In the KR environment however, the user gets the turn and performs an unavailable filler inform, which the system must react to. [sent-308, score-0.775]
</p><p>88 This is an instance of the second deficiency of the KR approach, where 183  Table 6: Sample IDTB dialogue in Combined User condition; Cost=6  Table 7: Sample KR dialogue in Combined User condition; Cost=7  the speaking system should not have released the turn. [sent-309, score-0.495]
</p><p>89 The user has the same belief in both scenar-  ios, but the negotiative nature of IDTB enables a shorter dialogues. [sent-310, score-0.53]
</p><p>90 A lesser cause of longer dialogues is an instance of the first deficiency of the KR systems; the listening user cannot get the turn when it should have it. [sent-312, score-0.45]
</p><p>91 Usually, this situation presents itself when the user releases the turn, having randomly chosen the weaker of the two unfilled slots. [sent-313, score-0.337]
</p><p>92 However, the user already had a strong belief and available top filler for one of those slots, and the system has increased the dialogue length unnecessarily. [sent-315, score-0.922]
</p><p>93 In this case, the IDTB system wins the turn initially using a low bid and informs  one of the strong slots, whereas the expert user initiates the dialogue for the KR environment and unnecessary informs are rarer. [sent-326, score-1.189]
</p><p>94 In general, however, the KR approach has more unnecessary informs since the KR system can only infer that one of the user’s beliefs was probably weak, otherwise the user would not have released the turn. [sent-327, score-0.431]
</p><p>95 The IDTB system handles this situation by using a high bid, allowing the user to outbid the system as its contribution is more important. [sent-328, score-0.382]
</p><p>96 In other words, the IDTB user can win the turn when it should have it, but the KR user cannot. [sent-329, score-0.65]
</p><p>97 In the previous section, we showed that the KR approach is deficient for two reasons: the speaking system might not keep the turn when it should have, and might release the turn when it should not have. [sent-334, score-0.39]
</p><p>98 Our performance differences arise from situations when the system is the speaker and the user is the listener. [sent-337, score-0.361]
</p><p>99 The IDTB model also excels in the opposite situation, when the system is the listener and the user is the speaker, though our domain is not sophisticated enough for this situation to occur. [sent-338, score-0.38]
</p><p>100 Information state and dialogue managment in the trindi dialogue move engine toolkit. [sent-416, score-0.454]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('idtb', 0.483), ('bid', 0.326), ('kr', 0.279), ('filler', 0.279), ('user', 0.235), ('dialogue', 0.213), ('slot', 0.197), ('utterance', 0.173), ('belief', 0.15), ('turn', 0.147), ('bids', 0.145), ('negotiative', 0.145), ('inform', 0.134), ('rl', 0.11), ('fillers', 0.099), ('reinforcement', 0.085), ('bidding', 0.085), ('conversants', 0.085), ('ufi', 0.085), ('speaker', 0.081), ('policy', 0.08), ('heeman', 0.078), ('action', 0.075), ('utterances', 0.072), ('dialogues', 0.068), ('turntaking', 0.068), ('strength', 0.058), ('informs', 0.057), ('novice', 0.053), ('cost', 0.052), ('condition', 0.052), ('policies', 0.05), ('slots', 0.049), ('sacks', 0.049), ('conversant', 0.048), ('beliefs', 0.047), ('unavailable', 0.047), ('users', 0.046), ('system', 0.045), ('percent', 0.044), ('experts', 0.044), ('collaborative', 0.044), ('duncan', 0.042), ('novices', 0.042), ('releases', 0.042), ('listener', 0.039), ('warranted', 0.039), ('weak', 0.038), ('cues', 0.038), ('adequate', 0.038), ('walker', 0.037), ('inadequate', 0.036), ('selfridge', 0.036), ('singleutterance', 0.036), ('chance', 0.036), ('sutton', 0.035), ('agents', 0.034), ('importance', 0.033), ('situation', 0.033), ('transitions', 0.033), ('win', 0.033), ('expert', 0.032), ('query', 0.032), ('wins', 0.032), ('mean', 0.031), ('su', 0.03), ('gravano', 0.029), ('sato', 0.029), ('conditions', 0.029), ('domain', 0.028), ('state', 0.028), ('release', 0.027), ('inefficient', 0.027), ('unfilled', 0.027), ('spoken', 0.027), ('behavior', 0.026), ('initiative', 0.026), ('drink', 0.026), ('interaction', 0.025), ('variables', 0.025), ('speech', 0.025), ('reward', 0.025), ('argminaq', 0.024), ('deficient', 0.024), ('devault', 0.024), ('guinn', 0.024), ('intermediates', 0.024), ('jonsdottir', 0.024), ('mixedinitiative', 0.024), ('niederehe', 0.024), ('outbid', 0.024), ('skantze', 0.024), ('thorisson', 0.024), ('agent', 0.024), ('released', 0.024), ('allen', 0.024), ('respond', 0.024), ('actions', 0.024), ('unnecessary', 0.023), ('environment', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="142-tfidf-1" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>Author: Ethan Selfridge ; Peter Heeman</p><p>Abstract: Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dia- logues, and supports the improvement of mixed-initiative interaction.</p><p>2 0.25222787 <a title="142-tfidf-2" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>3 0.20547915 <a title="142-tfidf-3" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>Author: Verena Rieser ; Oliver Lemon ; Xingkun Liu</p><p>Abstract: We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive “lower level” features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS.</p><p>4 0.16364905 <a title="142-tfidf-4" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>Author: Pierre Lison</p><p>Abstract: Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions ofthe state and action spaces.</p><p>5 0.12350047 <a title="142-tfidf-5" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>Author: Yorick Wilks ; Roberta Catizone ; Alexiei Dingli ; Weiwei Cheng</p><p>Abstract: This paper describes an initial prototype demonstrator of a Companion, designed as a platform for novel approaches to the following: 1) The use of Information Extraction (IE) techniques to extract the content of incoming dialogue utterances after an Automatic Speech Recognition (ASR) phase, 2) The conversion of the input to Resource Descriptor Format (RDF) to allow the generation of new facts from existing ones, under the control of a Dialogue Manger (DM), that also has access to stored knowledge and to open knowledge accessed in real time from the web, all in RDF form, 3) A DM implemented as a stack and network virtual machine that models mixed initiative in dialogue control, and 4) A tuned dialogue act detector based on corpus evidence. The prototype platform was evaluated, and we describe this briefly; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning.</p><p>6 0.11894352 <a title="142-tfidf-6" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>7 0.10538461 <a title="142-tfidf-7" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>8 0.10311456 <a title="142-tfidf-8" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>9 0.10015237 <a title="142-tfidf-9" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>10 0.084898941 <a title="142-tfidf-10" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>11 0.082792237 <a title="142-tfidf-11" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>12 0.082719371 <a title="142-tfidf-12" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>13 0.082045034 <a title="142-tfidf-13" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>14 0.075049408 <a title="142-tfidf-14" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>15 0.074592836 <a title="142-tfidf-15" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>16 0.072196208 <a title="142-tfidf-16" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>17 0.057702791 <a title="142-tfidf-17" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>18 0.057009228 <a title="142-tfidf-18" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<p>19 0.056354295 <a title="142-tfidf-19" href="./acl-2010-Modeling_Norms_of_Turn-Taking_in_Multi-Party_Conversation.html">173 acl-2010-Modeling Norms of Turn-Taking in Multi-Party Conversation</a></p>
<p>20 0.056290038 <a title="142-tfidf-20" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.115), (1, 0.09), (2, -0.071), (3, -0.203), (4, -0.033), (5, -0.247), (6, -0.202), (7, 0.089), (8, -0.052), (9, 0.001), (10, 0.049), (11, -0.054), (12, 0.005), (13, -0.012), (14, 0.052), (15, -0.116), (16, 0.085), (17, 0.005), (18, -0.066), (19, 0.002), (20, -0.03), (21, -0.047), (22, -0.018), (23, 0.084), (24, 0.021), (25, 0.078), (26, 0.03), (27, -0.025), (28, 0.029), (29, 0.016), (30, 0.046), (31, 0.029), (32, 0.036), (33, 0.045), (34, -0.009), (35, -0.1), (36, -0.142), (37, -0.048), (38, 0.046), (39, 0.014), (40, -0.012), (41, -0.064), (42, -0.037), (43, 0.025), (44, -0.011), (45, -0.048), (46, 0.034), (47, -0.015), (48, -0.042), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97021836 <a title="142-lsi-1" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>Author: Ethan Selfridge ; Peter Heeman</p><p>Abstract: Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dia- logues, and supports the improvement of mixed-initiative interaction.</p><p>2 0.88453227 <a title="142-lsi-2" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>Author: Verena Rieser ; Oliver Lemon ; Xingkun Liu</p><p>Abstract: We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive “lower level” features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS.</p><p>3 0.84609562 <a title="142-lsi-3" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>Author: Pierre Lison</p><p>Abstract: Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions ofthe state and action spaces.</p><p>4 0.84297651 <a title="142-lsi-4" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>5 0.79473102 <a title="142-lsi-5" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>Author: Yorick Wilks ; Roberta Catizone ; Alexiei Dingli ; Weiwei Cheng</p><p>Abstract: This paper describes an initial prototype demonstrator of a Companion, designed as a platform for novel approaches to the following: 1) The use of Information Extraction (IE) techniques to extract the content of incoming dialogue utterances after an Automatic Speech Recognition (ASR) phase, 2) The conversion of the input to Resource Descriptor Format (RDF) to allow the generation of new facts from existing ones, under the control of a Dialogue Manger (DM), that also has access to stored knowledge and to open knowledge accessed in real time from the web, all in RDF form, 3) A DM implemented as a stack and network virtual machine that models mixed initiative in dialogue control, and 4) A tuned dialogue act detector based on corpus evidence. The prototype platform was evaluated, and we describe this briefly; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning.</p><p>6 0.75052124 <a title="142-lsi-6" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>7 0.68263626 <a title="142-lsi-7" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>8 0.61872762 <a title="142-lsi-8" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>9 0.57739031 <a title="142-lsi-9" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<p>10 0.53720891 <a title="142-lsi-10" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>11 0.44942075 <a title="142-lsi-11" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>12 0.41194329 <a title="142-lsi-12" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>13 0.40805191 <a title="142-lsi-13" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>14 0.39312807 <a title="142-lsi-14" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>15 0.38832489 <a title="142-lsi-15" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>16 0.38784117 <a title="142-lsi-16" href="./acl-2010-Modeling_Norms_of_Turn-Taking_in_Multi-Party_Conversation.html">173 acl-2010-Modeling Norms of Turn-Taking in Multi-Party Conversation</a></p>
<p>17 0.36431661 <a title="142-lsi-17" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>18 0.36261868 <a title="142-lsi-18" href="./acl-2010-Automated_Planning_for_Situated_Natural_Language_Generation.html">35 acl-2010-Automated Planning for Situated Natural Language Generation</a></p>
<p>19 0.35304368 <a title="142-lsi-19" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>20 0.33178762 <a title="142-lsi-20" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.031), (22, 0.328), (25, 0.042), (39, 0.019), (41, 0.024), (42, 0.065), (45, 0.011), (59, 0.064), (73, 0.031), (76, 0.013), (78, 0.024), (80, 0.016), (83, 0.101), (84, 0.018), (98, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74454218 <a title="142-lda-1" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>Author: Ethan Selfridge ; Peter Heeman</p><p>Abstract: Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dia- logues, and supports the improvement of mixed-initiative interaction.</p><p>2 0.50170773 <a title="142-lda-2" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>3 0.4934876 <a title="142-lda-3" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>4 0.49271426 <a title="142-lda-4" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>5 0.49070036 <a title="142-lda-5" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>6 0.48370111 <a title="142-lda-6" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>7 0.47895402 <a title="142-lda-7" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>8 0.47859764 <a title="142-lda-8" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>9 0.47649044 <a title="142-lda-9" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>10 0.47558635 <a title="142-lda-10" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>11 0.47554708 <a title="142-lda-11" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>12 0.47547162 <a title="142-lda-12" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>13 0.47473687 <a title="142-lda-13" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>14 0.47447872 <a title="142-lda-14" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>15 0.47404507 <a title="142-lda-15" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>16 0.473903 <a title="142-lda-16" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>17 0.47379768 <a title="142-lda-17" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>18 0.47347349 <a title="142-lda-18" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>19 0.47345567 <a title="142-lda-19" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>20 0.47282141 <a title="142-lda-20" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
