<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-212" href="#">acl2010-212</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</h1>
<br/><p>Source: <a title="acl-2010-212-pdf" href="http://aclweb.org/anthology//P/P10/P10-2038.pdf">pdf</a></p><p>Author: Anders Sogaard</p><p>Abstract: Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004).</p><p>Reference: <a title="acl-2010-212-reference" href="../acl2010_reference/acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Simple semi-supervised training of part-of-speech taggers Anders Søgaard Center for Language Technology University of Copenhagen soegaard@hum. [sent-1, score-0.165]
</p><p>2 dk  Abstract Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. [sent-3, score-0.451]
</p><p>3 In this work stacked learning is used to reduce tagging to a classification task. [sent-4, score-0.238]
</p><p>4 Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. [sent-6, score-0.036]
</p><p>5 On the Wall Street Journal, we obtain an error reduction of 4. [sent-7, score-0.089]
</p><p>6 1 Introduction Semi-supervised part-of-speech (POS) tagging is relatively rare, and the main reason seems to be that results have mostly been negative. [sent-9, score-0.169]
</p><p>7 Meri-  aldo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data. [sent-10, score-0.362]
</p><p>8 (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al. [sent-12, score-0.279]
</p><p>9 (2009) use a new pool of unlabeled data tagged by an ensemble of state-of-the-art taggers in every training step of an averaged perceptron POS tagger with 4–5% error reduction. [sent-19, score-0.686]
</p><p>10 Finally,  Søgaard (2009) stacks a POS tagger on an unsupervised clustering algorithm trained on large amounts of unlabeled data with mixed results. [sent-20, score-0.448]
</p><p>11 This work combines a new semi-supervised learning method to POS tagging, namely tritraining (Li and Zhou, 2005), with stacking on unsupervised clustering. [sent-21, score-0.545]
</p><p>12 Finally, we introduce a variant of tri-training called tri-training with disagreement, which seems to perform equally well, but which imports much less unlabeled data and is therefore more efficient. [sent-23, score-0.327]
</p><p>13 2  Tagging as classification  This section describes our dataset and our input tagger. [sent-24, score-0.05]
</p><p>14 We also describe how stacking is used to reduce POS tagging to a classification task. [sent-25, score-0.412]
</p><p>15 Finally, we introduce the supervised learning algorithms used in our experiments. [sent-26, score-0.038]
</p><p>16 Since we need to train our classifiers on material distinct from the training material for our input POS tagger, we save Sect. [sent-33, score-0.251]
</p><p>17 Finally, we use the (untagged) Brown corpus as our unlabeled data. [sent-35, score-0.208]
</p><p>18 The number of tokens we use for training, developing and testing the classifiers, and the amount of unlabeled data available to it, are thus:  tduren asvltienabloepl mdent1, 17402t349o0,k 8426e18 7n1621s 205  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-36, score-0.243]
</p><p>19 c C2o0n1f0er Aenscseoc Sihatoirotn P faopre Crso,m papguetsat 2io0n5a–l2 L0i8n,guistics The amount of unlabeled data available to our classifiers is thus a bit more than 25 times the amount of labeled data. [sent-38, score-0.529]
</p><p>20 2 Input tagger In our experiments we use SVMTool (Gimenez and Marquez, 2004) with model type 4 run incrementally in both directions. [sent-40, score-0.17]
</p><p>21 3 Classifier input The way classifiers are constructed in our experiments is very simple. [sent-47, score-0.183]
</p><p>22 We train SVMTool and an unsupervised tagger, Unsupos (Biemann, 2006), on our training sections and apply them to the development, test and unlabeled sections. [sent-48, score-0.314]
</p><p>23 Here is an excerpt:1 Gold standard SVMTool Unsupos DTDT17 NNP NNP NNP VBD  NNP NNS NNP VBD  27 17* 17 26  Each row represents a word and lists the gold standard POS tag, the predicted POS tag and the word cluster selected by Unsupos. [sent-50, score-0.066]
</p><p>24 For example, the first word is labeled ’DT’, which SVMTool correctly predicts, and it belongs to cluster 17 of about 500 word clusters. [sent-51, score-0.105]
</p><p>25 The first column is blank in the table for the unlabeled section. [sent-52, score-0.208]
</p><p>26 Generally, the idea is that a classifier will learn to trust SVMTool in some cases, but that it may also learn that if SVMTool predicts a certain tag for some word cluster the correct label is another tag. [sent-53, score-0.198]
</p><p>27 This way of combining taggers into a single end classifier can be seen as a form of stacking (Wolpert, 1992). [sent-54, score-0.486]
</p><p>28 It has the advantage that it reduces POS tagging to a classification task. [sent-55, score-0.169]
</p><p>29 4 Learning algorithms We assume some knowledge of supervised learning algorithms. [sent-58, score-0.038]
</p><p>30 Most of our experiments are implementations of wrapper methods that call off1The numbers provided by Unsupos refer to clusters; ”*” marks out-of-vocabulary words. [sent-59, score-0.069]
</p><p>31 Specifically we have experimented with support vector machines (SVMs), decision trees, bagging and random forests. [sent-61, score-0.085]
</p><p>32 On the development section, decisions trees performed better than bagging and random forests. [sent-64, score-0.085]
</p><p>33 Let L denote the labeled data and U the unlabeled data. [sent-69, score-0.276]
</p><p>34 Assume that three classifiers c1, c2, c3  (same learning algorithm) have been trained on three bootstrap samples of L. [sent-70, score-0.238]
</p><p>35 In tri-training, an unlabeled datapoint in U is now labeled for a classifier, say c1, if the other two classifiers agree on its label, i. [sent-71, score-0.575]
</p><p>36 If the two classifiers agree on a labeling, there is a good chance that they are right. [sent-75, score-0.23]
</p><p>37 The algorithm stops when the classifiers no longer change. [sent-76, score-0.183]
</p><p>38 Li and Zhou (2005) show that under certain conditions the increase in classification noise rate is compensated by the amount of newly labeled data points. [sent-78, score-0.188]
</p><p>39 The most important condition is that the three classifiers are diverse. [sent-79, score-0.183]
</p><p>40 If the three classifiers are identical, tri-training degenerates to self-training. [sent-80, score-0.262]
</p><p>41 Diversity is obtained in Li and Zhou (2005) by training classifiers on bootstrap samples. [sent-81, score-0.267]
</p><p>42 In their experiments, they consider classifiers based on the C4. [sent-82, score-0.183]
</p><p>43 The algorithm is sketched in a simplified form in Figure 1; see Li and Zhou (2005) for all the details. [sent-84, score-0.05]
</p><p>44 Tri-training has to the best of our knowledge not been applied to POS tagging before, but it has been applied to other NLP classification tasks, incl. [sent-85, score-0.169]
</p><p>45 3} do Si ← bootstrap sample(L) ci ← train classifier(Si) end f←or repeat for i∈ {1. [sent-91, score-0.218]
</p><p>46 U3 }d od Li ← U∅ if cj (x) = ck (x) (j, k i) then Li ← Li ∪( {(x, cj(x)} end if← end for ci ← train classifier(L ∪ Li) end f←or until none of ci changes apply majority vote over ci  =  Figure 1: Tri-training (Li and Zhou, 2005). [sent-94, score-0.554]
</p><p>47 The intuition is that we only want to strengthen a classifier in its weak points, and we want to avoid skewing our labeled data by easy data points. [sent-97, score-0.168]
</p><p>48 Finally, since tritraining with disagreement imports less unlabeled data, it is much more efficient than tri-training. [sent-98, score-0.658]
</p><p>49 No one has to the best of our knowledge applied tritraining with disagreement to real-life classification tasks before. [sent-99, score-0.431]
</p><p>50 The stacking result was obtained by training a SVM on top of the predictions of SVMTool and the word clusters of Unsupos. [sent-101, score-0.3]
</p><p>51 SVMs performed better than decision trees, bagging and random forests on our development section, but improvements on test data were modest. [sent-102, score-0.085]
</p><p>52 Tri-training refers to the original algorithm sketched in Figure 1 with C4. [sent-103, score-0.05]
</p><p>53 Since tri-training degenerates to self-training if the three classifiers are trained on the same sample, we used our implementation of tri-training to obtain self-training results and validated our results by a simpler implementation. [sent-105, score-0.262]
</p><p>54 Finally, we list results for a technique called co-forests (Li and Zhou, 2007), which is a recent alternative to tri-training presented by the same authors, and for tri-training with disagreement (tri-disagr). [sent-107, score-0.216]
</p><p>55 Tri-training and tri-training with disagreement gave the best results. [sent-109, score-0.183]
</p><p>56 Note that since tri-training leads to much better results than stacking alone,  it is unlabeled data that gives us most of the improvement, not the stacking itself. [sent-110, score-0.694]
</p><p>57 It seems that tri-training with disagreement is a competitive technique in terms of accuracy. [sent-113, score-0.296]
</p><p>58 The main advantage of tritraining with disagreement compared to ordinary tri-training, however, is that it is very efficient. [sent-114, score-0.381]
</p><p>59 Self-training was, again, much slower than tri-training with disagreement since we had to train on a large pool of unlabeled data (but only once). [sent-117, score-0.473]
</p><p>60 Of course this is not a standard self-training set-up, but self-training informed by unsupervised word clusters. [sent-118, score-0.038]
</p><p>61 1 Follow-up experiments  SVMTool is one of the most accurate POS taggers available. [sent-120, score-0.136]
</p><p>62 This means that the predictions that are added to the labeled data are of very high quality. [sent-121, score-0.096]
</p><p>63 Note that error reduction is much lower in this case. [sent-124, score-0.089]
</p><p>64 5  Conclusion  This paper first shows how stacking can be used to reduce POS tagging to a classification task. [sent-137, score-0.412]
</p><p>65 The technique was used to improve the  accuracy of a state-of-the-art POS tagger, namely SVMTool. [sent-139, score-0.063]
</p><p>66 SVMTool: a  general POS tagger generator based on support vector machines. [sent-164, score-0.17]
</p><p>67 Improving a simple bigram HMM part-of-speech tagger by latent annotation and selftraining. [sent-168, score-0.17]
</p><p>68 Improve computeraided diagnosis with machine learning techniques using undiagnosed samples. [sent-176, score-0.032]
</p><p>69 Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data. [sent-211, score-0.208]
</p><p>70 Semi-supervised learning for part-of-speech tagging of Mandarin transcribed speech. [sent-215, score-0.119]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svmtool', 0.554), ('stacking', 0.243), ('unlabeled', 0.208), ('tritraining', 0.198), ('disagreement', 0.183), ('classifiers', 0.183), ('pos', 0.176), ('gimenez', 0.173), ('tagger', 0.17), ('li', 0.161), ('taggers', 0.136), ('zhou', 0.136), ('nnp', 0.124), ('marquez', 0.119), ('tagging', 0.119), ('spoustova', 0.119), ('unsupos', 0.119), ('cj', 0.096), ('ci', 0.087), ('bagging', 0.085), ('degenerates', 0.079), ('zhongqiang', 0.079), ('suzuki', 0.075), ('classifier', 0.07), ('stacked', 0.069), ('datapoint', 0.069), ('imports', 0.069), ('wall', 0.068), ('labeled', 0.068), ('nguyen', 0.068), ('gaard', 0.064), ('isozaki', 0.06), ('street', 0.059), ('svms', 0.057), ('reduction', 0.055), ('bootstrap', 0.055), ('mary', 0.052), ('anders', 0.052), ('wsj', 0.051), ('classification', 0.05), ('seems', 0.05), ('sketched', 0.05), ('vbd', 0.05), ('agree', 0.047), ('ck', 0.047), ('ming', 0.045), ('pool', 0.043), ('huang', 0.041), ('train', 0.039), ('supervised', 0.038), ('unsupervised', 0.038), ('end', 0.037), ('chunking', 0.037), ('perceptron', 0.037), ('cluster', 0.037), ('combines', 0.036), ('jan', 0.035), ('amount', 0.035), ('wrapper', 0.035), ('pvalues', 0.035), ('emilia', 0.035), ('biemann', 0.035), ('stratified', 0.035), ('aldo', 0.035), ('compensated', 0.035), ('modelsfor', 0.035), ('quinlan', 0.035), ('error', 0.034), ('implementations', 0.034), ('technique', 0.033), ('predicts', 0.032), ('akira', 0.032), ('diagnosis', 0.032), ('wenliang', 0.032), ('stacks', 0.032), ('hideki', 0.032), ('yujie', 0.032), ('wen', 0.032), ('hajic', 0.032), ('miroslav', 0.032), ('entropy', 0.031), ('namely', 0.03), ('label', 0.03), ('marcus', 0.03), ('competitive', 0.03), ('networks', 0.03), ('cybernetics', 0.03), ('mandarin', 0.03), ('untagged', 0.03), ('strengthen', 0.03), ('hmm', 0.03), ('tag', 0.029), ('training', 0.029), ('averaged', 0.029), ('predictions', 0.028), ('python', 0.028), ('oriental', 0.028), ('rfo', 0.028), ('lluis', 0.028), ('copenhagen', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="212-tfidf-1" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>Author: Anders Sogaard</p><p>Abstract: Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004).</p><p>2 0.14171959 <a title="212-tfidf-2" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>Author: Rasoul Samad Zadeh Kaljahi</p><p>Abstract: Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1</p><p>3 0.12185521 <a title="212-tfidf-3" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynam- ically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.</p><p>4 0.10109994 <a title="212-tfidf-4" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>5 0.10054084 <a title="212-tfidf-5" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>6 0.094123505 <a title="212-tfidf-6" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>7 0.086776823 <a title="212-tfidf-7" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>8 0.083172381 <a title="212-tfidf-8" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>9 0.080207981 <a title="212-tfidf-9" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>10 0.072999924 <a title="212-tfidf-10" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>11 0.068181396 <a title="212-tfidf-11" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>12 0.067903683 <a title="212-tfidf-12" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>13 0.065184005 <a title="212-tfidf-13" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>14 0.057457287 <a title="212-tfidf-14" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>15 0.055053137 <a title="212-tfidf-15" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>16 0.054491453 <a title="212-tfidf-16" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>17 0.054040186 <a title="212-tfidf-17" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>18 0.053861897 <a title="212-tfidf-18" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>19 0.053501684 <a title="212-tfidf-19" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>20 0.052029233 <a title="212-tfidf-20" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.151), (1, 0.018), (2, 0.051), (3, 0.038), (4, 0.013), (5, -0.033), (6, 0.005), (7, 0.006), (8, 0.056), (9, 0.127), (10, -0.067), (11, 0.077), (12, -0.024), (13, -0.116), (14, -0.06), (15, -0.009), (16, -0.01), (17, -0.073), (18, 0.062), (19, -0.019), (20, 0.081), (21, 0.146), (22, 0.019), (23, 0.039), (24, -0.115), (25, 0.005), (26, 0.081), (27, -0.008), (28, 0.008), (29, 0.072), (30, 0.048), (31, 0.005), (32, 0.02), (33, 0.136), (34, -0.018), (35, -0.023), (36, 0.005), (37, -0.009), (38, 0.069), (39, 0.069), (40, 0.122), (41, -0.087), (42, 0.023), (43, -0.119), (44, 0.042), (45, -0.061), (46, 0.017), (47, -0.05), (48, -0.052), (49, -0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96341842 <a title="212-lsi-1" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>Author: Anders Sogaard</p><p>Abstract: Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004).</p><p>2 0.69316494 <a title="212-lsi-2" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>Author: Rasoul Samad Zadeh Kaljahi</p><p>Abstract: Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1</p><p>3 0.62523341 <a title="212-lsi-3" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>Author: Paramveer S. Dhillon ; Partha Pratim Talukdar ; Koby Crammer</p><p>Abstract: We initiate a study comparing effectiveness of the transformed spaces learned by recently proposed supervised, and semisupervised metric learning algorithms to those generated by previously proposed unsupervised dimensionality reduction methods (e.g., PCA). Through a variety of experiments on different realworld datasets, we find IDML-IT, a semisupervised metric learning algorithm to be the most effective.</p><p>4 0.57590097 <a title="212-lsi-4" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>5 0.56185555 <a title="212-lsi-5" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>6 0.5484097 <a title="212-lsi-6" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>7 0.54629368 <a title="212-lsi-7" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>8 0.51401496 <a title="212-lsi-8" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>9 0.50428635 <a title="212-lsi-9" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>10 0.4978134 <a title="212-lsi-10" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>11 0.47581986 <a title="212-lsi-11" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>12 0.45937631 <a title="212-lsi-12" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>13 0.45491603 <a title="212-lsi-13" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>14 0.44541231 <a title="212-lsi-14" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>15 0.42236957 <a title="212-lsi-15" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>16 0.41500348 <a title="212-lsi-16" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>17 0.41030461 <a title="212-lsi-17" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>18 0.40313655 <a title="212-lsi-18" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>19 0.39463687 <a title="212-lsi-19" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>20 0.38542822 <a title="212-lsi-20" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(13, 0.332), (14, 0.027), (25, 0.077), (42, 0.01), (59, 0.133), (71, 0.014), (73, 0.054), (76, 0.023), (78, 0.033), (83, 0.065), (84, 0.016), (98, 0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76491326 <a title="212-lda-1" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>Author: Anders Sogaard</p><p>Abstract: Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004).</p><p>2 0.75657052 <a title="212-lda-2" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>Author: Federico Sangati</p><p>Abstract: We present a probabilistic model extension to the Tesni `ere Dependency Structure (TDS) framework formulated in (Sangati and Mazza, 2009). This representation incorporates aspects from both constituency and dependency theory. In addition, it makes use of junction structures to handle coordination constructions. We test our model on parsing the English Penn WSJ treebank using a re-ranking framework. This technique allows us to efficiently test our model without needing a specialized parser, and to use the standard evaluation metric on the original Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking.</p><p>3 0.70957905 <a title="212-lda-3" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>Author: Francois Mairesse ; Milica Gasic ; Filip Jurcicek ; Simon Keizer ; Blaise Thomson ; Kai Yu ; Steve Young</p><p>Abstract: Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation perfor- mance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.</p><p>4 0.68005925 <a title="212-lda-4" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>Author: Ivan Titov ; Mikhail Kozhevnikov</p><p>Abstract: We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations. A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts. We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts.</p><p>5 0.54244107 <a title="212-lda-5" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>6 0.54087275 <a title="212-lda-6" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>7 0.5405184 <a title="212-lda-7" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>8 0.53851092 <a title="212-lda-8" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>9 0.53846991 <a title="212-lda-9" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>10 0.53515846 <a title="212-lda-10" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>11 0.53464687 <a title="212-lda-11" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>12 0.53430235 <a title="212-lda-12" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>13 0.53345835 <a title="212-lda-13" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>14 0.53344357 <a title="212-lda-14" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>15 0.53268933 <a title="212-lda-15" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>16 0.53261459 <a title="212-lda-16" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>17 0.53114969 <a title="212-lda-17" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>18 0.53056192 <a title="212-lda-18" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>19 0.52928048 <a title="212-lda-19" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>20 0.52868927 <a title="212-lda-20" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
