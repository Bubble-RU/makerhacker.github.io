<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-136" href="#">acl2010-136</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</h1>
<br/><p>Source: <a title="acl-2010-136-pdf" href="http://aclweb.org/anthology//P/P10/P10-1126.pdf">pdf</a></p><p>Author: Yansong Feng ; Mirella Lapata</p><p>Abstract: In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.</p><p>Reference: <a title="acl-2010-136-reference" href="../acl2010_reference/acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract In this paper we tackle the problem of automatic caption generation for news images. [sent-6, score-0.619]
</p><p>2 Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. [sent-8, score-0.95]
</p><p>3 They both operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. [sent-9, score-0.812]
</p><p>4 Experimental results show that an abstractive model defined over phrases is superior to extractive methods. [sent-10, score-0.408]
</p><p>5 The literature is littered with various attempts to learn the associations between image features and words using supervised classification (Vailaya et al. [sent-28, score-0.495]
</p><p>6 In this paper we go one step further and generate captions for images rather than individual keywords. [sent-36, score-0.516]
</p><p>7 Although image indexing techniques based on keywords are popular and the method of choice for image retrieval engines, there are good reasons for using more linguistically meaningful descriptions. [sent-37, score-1.155]
</p><p>8 An image annotated with the words blue, sky, car could depict a blue car or a blue sky,  whereas the caption “car running under the blue sky” would make the relations between the words explicit. [sent-39, score-1.081]
</p><p>9 Automatic caption generation could improve image retrieval by supporting longer and more targeted queries. [sent-40, score-1.068]
</p><p>10 Beyond image retrieval, it could increase the accessibility of the web for visually impaired (blind and partially sighted) users who cannot access the content of many sites in the same ways as sighted users can (Ferres et al. [sent-42, score-0.561]
</p><p>11 We explore the feasibility of automatic caption generation in the news domain, and create descriptions for images associated with on-line articles. [sent-44, score-0.877]
</p><p>12 Inspired by recent work in summarization, we propose extractive and abstractive caption gen1239  Proce dingUsp opfs thaela 4, 8Stwhe Adnen u,a 1l1- M16e Jtiunlgy o 2f0 t1h0e. [sent-46, score-0.87]
</p><p>13 The backbone for both approaches  is a probabilistic image annotation model that suggests keywords for an image. [sent-49, score-0.767]
</p><p>14 We can then simply identify (and rank) the sentences in the documents that share these keywords or create a new caption that is potentially more concise but also informative and fluent. [sent-50, score-0.706]
</p><p>15 Our abstractive model operates over image description keywords and document phrases. [sent-51, score-1.058]
</p><p>16 Their combination gives rise to many caption realizations which we select probabilistically by taking into account dependency and word order constraints. [sent-52, score-0.493]
</p><p>17 Experimental results show that the model’s output compares favorably to handwritten captions and is often superior to extractive methods. [sent-53, score-0.466]
</p><p>18 2  Related Work  Although image understanding is a popular topic within computer vision, relatively little work has focused on the interplay between visual and linguistic information. [sent-54, score-0.637]
</p><p>19 A handful of approaches generate image descriptions automatically following a two-stage architecture. [sent-55, score-0.541]
</p><p>20 The picture is first analyzed using image processing techniques into an  abstract representation, which is then rendered into a natural language description with a text generation engine. [sent-56, score-0.68]
</p><p>21 Their system relies on a manually created database of objects indexed by an image signature (e. [sent-60, score-0.495]
</p><p>22 (2009) present a general framework for generating text descriptions of image and video content based on image parsing. [sent-68, score-1.101]
</p><p>23 The image parser is trained on a corpus, manually annotated with graphs representing image structure. [sent-70, score-0.99]
</p><p>24 Within natural language processing most previous efforts have focused on generating captions to accompany complex graphical presentations (Mittal et al. [sent-72, score-0.328]
</p><p>25 , 1998; Corio and Lapalme, 1999; Fasciano and Lapalme, 2000; Feiner and McKeown, 1990) or on using the captions accompanying information graphics to infer their intended message, e. [sent-73, score-0.397]
</p><p>26 Little emphasis is placed on image processing; it is assumed that the data used to create the graphics are available, and the goal is to enable users understand the information expressed in them. [sent-77, score-0.561]
</p><p>27 The task of generating captions for news images is novel to our knowledge. [sent-78, score-0.562]
</p><p>28 Using an image annotation model, we first describe the picture with keywords which are subsequently realized into a human readable sentence. [sent-82, score-0.814]
</p><p>29 The caption generation task bears some resemblance to headline generation (Dorr et al. [sent-83, score-0.753]
</p><p>30 Importantly, we aim to create a caption that not only summarizes the document but is also a faithful to the image’s content (i. [sent-86, score-0.649]
</p><p>31 , the caption should also mention some of the objects or individuals depicted in the image). [sent-88, score-0.493]
</p><p>32 We therefore explore extractive and abstractive models that rely on visual information to drive the generation process. [sent-89, score-0.55]
</p><p>33 3  Problem Formulation  We formulate image caption generation as follows. [sent-91, score-1.068]
</p><p>34 Given an image I, and a related knowledge database κ, create a natural language description C which captures the main content of the image under κ. [sent-92, score-1.086]
</p><p>35 Specifically, in the news story scenario, we will generate a caption C for an image I and its accompanying document D. [sent-93, score-1.155]
</p><p>36 During testing, we are given a document and an associated image for which we must generate a caption. [sent-96, score-0.589]
</p><p>37 The caption vocabulary is 6,180 words and the document vocabulary is 26,795. [sent-104, score-0.587]
</p><p>38 The captions tend to use half as many words as the document sentences, and more than 50% of the time contain words that are not attested in the document (even though they may be attested in the collection). [sent-106, score-0.564]
</p><p>39 Generating image captions is a challenging task even for humans, let alone computers. [sent-107, score-0.823]
</p><p>40 A good caption must be succinct and informative, clearly identify the subject of the picture, establish the picture’s relevance to the article, provide context for the picture, and ultimately draw the reader into the article. [sent-123, score-0.525]
</p><p>41 It is also worth noting that journalists often write their own captions rather than simply extract sentences from the document. [sent-124, score-0.373]
</p><p>42 4  Image Annotation  As mentioned earlier, our approach relies on an image annotation model to provide description keywords for the picture. [sent-126, score-0.775]
</p><p>43 The model is based on the assumption that images and their surrounding text are generated by mixtures of latent topics which are inferred from a concatenated representation of words and visual features. [sent-129, score-0.394]
</p><p>44 Local image descriptors are computed using the Scale Invariant Feature Transform (SIFT) algorithm (Lowe, 1999). [sent-131, score-0.495]
</p><p>45 The general idea behind the algorithm is to first sample an image with the difference-of-Gaussians point detector at different 1241  scales and locations. [sent-132, score-0.495]
</p><p>46 The image annotation model takes the topic distributions into account when finding the most likely keywords for an image and its associated document. [sent-141, score-1.285]
</p><p>47 The model delivers a ranked list of textual words wt, the n-best of which are used as annotations for image I. [sent-145, score-0.549]
</p><p>48 It is important to note that the caption generation models we propose are not especially tied to the above annotation model. [sent-146, score-0.623]
</p><p>49 For our caption generation task, we need only extract a single sentence. [sent-153, score-0.573]
</p><p>50 Word Overlap Perhaps the simplest way of measuring the similarity between image keywords and document sentences is word overlap:  Overlap(WI,Sd) =||WWII∪∩SSdd||  (2)  where WI is the set of keywords and Sd a sentence in the document. [sent-156, score-0.946]
</p><p>51 The caption is then the sentence that has the highest overlap with the keywords. [sent-157, score-0.526]
</p><p>52 For example, we may use the KL divergence to measure the difference between the distributions p and q:  D(p,q) =j∑=K1pjlog2qpjj  (4)  where p and q are shorthand for the image topic distribution PdMix and sentence topic distribution PSd, respectively. [sent-164, score-0.631]
</p><p>53 6  (5)  Abstractive Caption Generation  Although extractive methods yield grammatical captions and require relatively little linguistic analysis, there are a few caveats to consider. [sent-169, score-0.466]
</p><p>54 Secondly, the selected sentences make for long captions (sometimes longer than the average document sentence), are not concise and overall not as catchy as human-written captions. [sent-172, score-0.474]
</p><p>55 For these reasons we turn to abstractive caption generation and present models based on single words but also phrases. [sent-173, score-0.812]
</p><p>56 Word-based Model Our first abstractive model builds on and extends a well-known probabilistic model of headline generation (Banko et al. [sent-174, score-0.507]
</p><p>57 The task is related to caption generation, the aim is  to create a short, title-like headline for a given document, without however taking visual information into account. [sent-176, score-0.71]
</p><p>58 ,wn) =  i∏=n1P(wi∈ H|wi∈ D)  (6)  ·P(len(H) = n)  ·i∏=n2P(wi|wi−1) where wi is a word that may appear in headline H, D the document being summarized,  and P(len(H) = n) a headline length distribution model. [sent-186, score-0.475]
</p><p>59 The above model can be easily adapted to the caption generation task. [sent-187, score-0.604]
</p><p>60 Content selection is now the probability of a word appearing in the caption given the image and its associated document which we obtain from the output of our image annotation model (see Section 4). [sent-188, score-1.699]
</p><p>61 ,wn) =i=∏n1P(wi∈C|I,D)  (7)  ·P(len(C) = n)  ·i∏=n3P(wi|wi−1,wi−2) where C is the caption, I image, D the accomthe panying document, and P(wi ∈ C|I, D) the image annotation probability. [sent-192, score-0.545]
</p><p>62 Despite its simplicity, the caption generation model in (7) has a major drawback. [sent-193, score-0.604]
</p><p>63 One way to remedy this is to revert to a content selection model that ignores the image and simply estimates the probability of a word appearing in the caption given the same word appearing in the document. [sent-196, score-1.139]
</p><p>64 At the same time we modify our surface realization component so that it takes note of the image annotation probabilities. [sent-197, score-0.569]
</p><p>65 , t hwe original trigram model), Padap(w) the probability of w according to the image annotation model, Pback(w) the probability of w according to the original model, and β a scaling parameter. [sent-204, score-0.576]
</p><p>66 Phrase-based Model The model outlined in equation (8) will generate captions with function words. [sent-205, score-0.388]
</p><p>67 However, there is no guarantee that these will be compatible with their surrounding context or that the caption will be globally coherent beyond the trigram horizon. [sent-206, score-0.548]
</p><p>68 We only consider dependencies whose heads are nouns, verbs, and prepositions, as these constitute 80% of all dependencies attested in our caption data. [sent-211, score-0.517]
</p><p>69 We define a bag-of-phrases model for caption generation by modifying the content selection and caption length components in equation (8) as follows: P(ρ1,ρ2,. [sent-212, score-1.189]
</p><p>70 5w After integrating the attachment probabilities into equation (12), the caption generation model becomes:  P(ρ1,ρ2,. [sent-220, score-0.666]
</p><p>71 On the other hand, it has a primitive notion of caption length estimated by P(len(C) = ∑mj=1 len(ρj)) and will therefore generate captions of the same (phrase) length. [sent-225, score-0.846]
</p><p>72 Search To generate a caption it is necessary to find the sequence of words that maximizes P(w1 , w2, . [sent-228, score-0.493]
</p><p>73 Using one of the models from Section 5, we may rank its sentences in terms of  their relevance to the image keywords and consider only the n-best ones. [sent-237, score-0.692]
</p><p>74 7  Experimental Setup  In this section we discuss our experimental design for assessing the performance of the caption generation models presented above. [sent-239, score-0.573]
</p><p>75 Documents and captions were parsed with the Stanford parser (Klein and Manning, 2003) in order to obtain dependencies for the phrase-based abstractive model. [sent-242, score-0.567]
</p><p>76 Model Parameters For the image annotation model we extracted 150 (on average) SIFT features which were quantized into 750 visual terms. [sent-243, score-0.669]
</p><p>77 We tuned the caption length parameter on the development set using a range of [5, 14] tokens for the word-based model and [2, 5] phrases for the phrase-based model. [sent-252, score-0.549]
</p><p>78 In our case, the original captions written by the BBC journalists were used as reference:  TER(E,Er) =Ins+Del+NrSub+Shft  (16)  where E is the hypothetical system output, Er the reference caption, and Nr the reference length. [sent-267, score-0.373]
</p><p>79 We used TER to compare the output of our extractive and abstractive models and also for parameter tuning (see the discussion above). [sent-276, score-0.377]
</p><p>80 ) and relevance (does it describe succinctly the content of the image and document? [sent-278, score-0.565]
</p><p>81 We used a 1–7 rating scale, participants were encouraged to give high ratings to captions that were grammatical and appropriate descriptions of the image given the accompanying document. [sent-280, score-0.925]
</p><p>82 We randomly selected 12 document-image pairs from the test set and generated captions for them using the best extractive system, and two abstractive systems (word-based and phrase-based). [sent-281, score-0.705]
</p><p>83 We also included the original human-authored caption as an upper bound. [sent-282, score-0.493]
</p><p>84 We compare four extractive models based on word overlap, cosine similarity, and two probabilistic similarity measures, namely KL and JS divergence and two abstractive models based on words (see equation (8)) and phrases (see equation (15)). [sent-286, score-0.552]
</p><p>85 We also include a simple baseline that selects the first document sentence as a caption and show the average caption length (AvgLen) for each model. [sent-287, score-1.105]
</p><p>86 6 They make use of the same topic model as the image annotation model, and are thus able to select sentences that cover common content. [sent-296, score-0.625]
</p><p>87 7 This is an encouraging result as it highlights the importance of the visual information for the caption generation task. [sent-299, score-0.666]
</p><p>88 The abstractive models obtain the best TER scores overall, however they generate shorter captions in comparison to the other models (closer to the length ofthe gold standard) and as a result TER treats them favorably, simply because the number of edits is less. [sent-302, score-0.622]
</p><p>89 Table 3 reports mean ratings for the output of the extractive system (based on the KL divergence), the two abstractive systems, and the human-authored gold standard caption. [sent-305, score-0.406]
</p><p>90 1246  erated by extractive (KL), word-based abstractive (AW), and phrase-based extractive (AP systems). [sent-310, score-0.515]
</p><p>91 Overall, the captions generated by the phrase-based system, capture the same content as the human-authored captions, even though they tend to be less grammatical. [sent-319, score-0.366]
</p><p>92 9  Conclusions  We have presented extractive and abstractive models that generate image captions for news articles. [sent-321, score-1.246]
</p><p>93 This is achieved through an image annotation model that characterizes pictures in terms of description keywords that are subsequently used to guide the caption generation process. [sent-323, score-1.426]
</p><p>94 It generates  captions that are more grammatical than a closely related word-based system and manages to capture the gist of the image (and document) as well as the captions written by journalists. [sent-327, score-1.151]
</p><p>95 Rather than adopting a two-stage approach, where the image processing and caption generation are carried out sequentially, a more general model should integrate the two steps in a unified framework. [sent-329, score-1.099]
</p><p>96 Indeed, an avenue for future work would be to define a phrase-based model for both image annotation and caption generation. [sent-330, score-1.069]
</p><p>97 Object recognition as machine translation: Learning a lexicon for a fixed image vocabulary. [sent-367, score-0.495]
</p><p>98 Multiple Bernoulli relevance models for image and video annotation. [sent-386, score-0.554]
</p><p>99 Describing complex charts in natural language: A caption generation system. [sent-452, score-0.573]
</p><p>100 Content-based image retrieval at the end of the early years. [sent-471, score-0.495]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('image', 0.495), ('caption', 0.493), ('captions', 0.328), ('abstractive', 0.239), ('images', 0.188), ('keywords', 0.165), ('wi', 0.156), ('extractive', 0.138), ('len', 0.111), ('headline', 0.1), ('document', 0.094), ('visual', 0.093), ('generation', 0.08), ('picture', 0.071), ('dmix', 0.069), ('feng', 0.069), ('wj', 0.061), ('kl', 0.056), ('bbc', 0.056), ('padap', 0.055), ('pback', 0.055), ('blei', 0.052), ('annotation', 0.05), ('js', 0.049), ('topic', 0.049), ('ter', 0.049), ('sift', 0.049), ('news', 0.046), ('descriptions', 0.046), ('pictures', 0.045), ('journalists', 0.045), ('grammaticality', 0.042), ('graphics', 0.042), ('zk', 0.042), ('kojima', 0.042), ('yansong', 0.042), ('appearing', 0.041), ('vision', 0.039), ('divergence', 0.038), ('content', 0.038), ('barnard', 0.036), ('duygulu', 0.036), ('banko', 0.035), ('description', 0.034), ('attachment', 0.033), ('subsequently', 0.033), ('sky', 0.033), ('overlap', 0.033), ('wt', 0.032), ('relevance', 0.032), ('trigram', 0.031), ('blue', 0.031), ('model', 0.031), ('topics', 0.03), ('edits', 0.03), ('mittal', 0.03), ('equation', 0.029), ('multimodal', 0.029), ('ratings', 0.029), ('latent', 0.028), ('argmwatxwt', 0.028), ('atsuhiro', 0.028), ('captioned', 0.028), ('catchy', 0.028), ('corio', 0.028), ('elzer', 0.028), ('fasciano', 0.028), ('feiner', 0.028), ('ferres', 0.028), ('headlines', 0.028), ('kobus', 0.028), ('kunio', 0.028), ('monay', 0.028), ('nando', 0.028), ('pixels', 0.028), ('shft', 0.028), ('sighted', 0.028), ('vailaya', 0.028), ('wwii', 0.028), ('accompanying', 0.027), ('lavrenko', 0.027), ('video', 0.027), ('similarity', 0.027), ('probabilistic', 0.026), ('cosine', 0.026), ('latter', 0.025), ('length', 0.025), ('create', 0.024), ('attested', 0.024), ('proportions', 0.024), ('activities', 0.024), ('concise', 0.024), ('pinar', 0.024), ('smeulders', 0.024), ('lapalme', 0.024), ('freitas', 0.024), ('surface', 0.024), ('surrounding', 0.024), ('textual', 0.023), ('dorr', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000026 <a title="136-tfidf-1" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>Author: Yansong Feng ; Mirella Lapata</p><p>Abstract: In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.</p><p>2 0.27514613 <a title="136-tfidf-2" href="./acl-2010-Generating_Image_Descriptions_Using_Dependency_Relational_Patterns.html">124 acl-2010-Generating Image Descriptions Using Dependency Relational Patterns</a></p>
<p>Author: Ahmet Aker ; Robert Gaizauskas</p><p>Abstract: This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple webdocuments that contain information related to an image’s location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns.</p><p>3 0.11185275 <a title="136-tfidf-3" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<p>Author: Shih-Hsiang Lin ; Berlin Chen</p><p>Abstract: In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. 1</p><p>4 0.09707807 <a title="136-tfidf-4" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>5 0.087747365 <a title="136-tfidf-5" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>Author: Kristian Woodsend ; Mirella Lapata</p><p>Abstract: In this paper we present a joint content selection and compression model for single-document summarization. The model operates over a phrase-based representation of the source document which we obtain by merging information from PCFG parse trees and dependency graphs. Using an integer linear programming formulation, the model learns to select and combine phrases subject to length, coverage and grammar constraints. We evaluate the approach on the task of generating “story highlights”—a small number of brief, self-contained sentences that allow readers to quickly gather information on news stories. Experimental results show that the model’s output is comparable to human-written highlights in terms of both grammaticality and content.</p><p>6 0.085871108 <a title="136-tfidf-6" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>7 0.079990931 <a title="136-tfidf-7" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>8 0.076450504 <a title="136-tfidf-8" href="./acl-2010-A_New_Approach_to_Improving_Multilingual_Summarization_Using_a_Genetic_Algorithm.html">11 acl-2010-A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm</a></p>
<p>9 0.067177996 <a title="136-tfidf-9" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>10 0.066713735 <a title="136-tfidf-10" href="./acl-2010-A_Bayesian_Method_for_Robust_Estimation_of_Distributional_Similarities.html">3 acl-2010-A Bayesian Method for Robust Estimation of Distributional Similarities</a></p>
<p>11 0.063480213 <a title="136-tfidf-11" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>12 0.063098282 <a title="136-tfidf-12" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>13 0.06181879 <a title="136-tfidf-13" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>14 0.060454063 <a title="136-tfidf-14" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>15 0.060173787 <a title="136-tfidf-15" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>16 0.057025537 <a title="136-tfidf-16" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>17 0.056687396 <a title="136-tfidf-17" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>18 0.056463614 <a title="136-tfidf-18" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>19 0.054010447 <a title="136-tfidf-19" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>20 0.053324439 <a title="136-tfidf-20" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.16), (1, 0.049), (2, -0.08), (3, -0.003), (4, 0.009), (5, -0.031), (6, 0.016), (7, -0.19), (8, -0.005), (9, -0.018), (10, -0.064), (11, -0.038), (12, -0.019), (13, 0.082), (14, 0.046), (15, -0.044), (16, -0.016), (17, 0.012), (18, -0.049), (19, 0.032), (20, -0.006), (21, -0.05), (22, 0.034), (23, 0.064), (24, 0.025), (25, -0.015), (26, -0.035), (27, -0.05), (28, 0.016), (29, -0.079), (30, -0.057), (31, 0.058), (32, -0.014), (33, -0.033), (34, -0.057), (35, -0.001), (36, 0.15), (37, 0.023), (38, 0.076), (39, 0.089), (40, 0.028), (41, 0.241), (42, 0.094), (43, 0.06), (44, -0.197), (45, -0.034), (46, -0.064), (47, -0.121), (48, 0.027), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91190296 <a title="136-lsi-1" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>Author: Yansong Feng ; Mirella Lapata</p><p>Abstract: In this paper we tackle the problem of automatic caption generation for news images. Our approach leverages the vast resource of pictures available on the web and the fact that many of them are captioned. Inspired by recent work in summarization, we propose extractive and abstractive caption generation models. They both operate over the output of a probabilistic image annotation model that preprocesses the pictures and suggests keywords to describe their content. Experimental results show that an abstractive model defined over phrases is superior to extractive methods.</p><p>2 0.68497866 <a title="136-lsi-2" href="./acl-2010-Generating_Image_Descriptions_Using_Dependency_Relational_Patterns.html">124 acl-2010-Generating Image Descriptions Using Dependency Relational Patterns</a></p>
<p>Author: Ahmet Aker ; Robert Gaizauskas</p><p>Abstract: This paper presents a novel approach to automatic captioning of geo-tagged images by summarizing multiple webdocuments that contain information related to an image’s location. The summarizer is biased by dependency pattern models towards sentences which contain features typically provided for different scene types such as those of churches, bridges, etc. Our results show that summaries biased by dependency pattern models lead to significantly higher ROUGE scores than both n-gram language models reported in previous work and also Wikipedia baseline summaries. Summaries generated using dependency patterns also lead to more readable summaries than those generated without dependency patterns.</p><p>3 0.62172389 <a title="136-lsi-3" href="./acl-2010-A_Bayesian_Method_for_Robust_Estimation_of_Distributional_Similarities.html">3 acl-2010-A Bayesian Method for Robust Estimation of Distributional Similarities</a></p>
<p>Author: Jun'ichi Kazama ; Stijn De Saeger ; Kow Kuroda ; Masaki Murata ; Kentaro Torisawa</p><p>Abstract: Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words’ context profiles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is . the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures.</p><p>4 0.53235698 <a title="136-lsi-4" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>5 0.44132206 <a title="136-lsi-5" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>Author: Swati Tata ; Barbara Di Eugenio</p><p>Abstract: Music Recommendation Systems often recommend individual songs, as opposed to entire albums. The challenge is to generate reviews for each song, since only full album reviews are available on-line. We developed a summarizer that combines information extraction and generation techniques to produce summaries of reviews of individual songs. We present an intrinsic evaluation of the extraction components, and of the informativeness of the summaries; and a user study of the impact of the song review summaries on users’ decision making processes. Users were able to make quicker and more informed decisions when presented with the summary as compared to the full album review.</p><p>6 0.43918607 <a title="136-lsi-6" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>7 0.40914237 <a title="136-lsi-7" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>8 0.39200318 <a title="136-lsi-8" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>9 0.37426355 <a title="136-lsi-9" href="./acl-2010-Wrapping_up_a_Summary%3A_From_Representation_to_Generation.html">264 acl-2010-Wrapping up a Summary: From Representation to Generation</a></p>
<p>10 0.37249455 <a title="136-lsi-10" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>11 0.35162991 <a title="136-lsi-11" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<p>12 0.35015577 <a title="136-lsi-12" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>13 0.34632659 <a title="136-lsi-13" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>14 0.34378758 <a title="136-lsi-14" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>15 0.34346613 <a title="136-lsi-15" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>16 0.34068212 <a title="136-lsi-16" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>17 0.33930504 <a title="136-lsi-17" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>18 0.3385976 <a title="136-lsi-18" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>19 0.33431143 <a title="136-lsi-19" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>20 0.32936552 <a title="136-lsi-20" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.066), (14, 0.011), (25, 0.05), (33, 0.025), (39, 0.014), (42, 0.019), (44, 0.016), (59, 0.084), (61, 0.01), (72, 0.02), (73, 0.043), (76, 0.012), (78, 0.031), (80, 0.011), (83, 0.083), (84, 0.243), (97, 0.012), (98, 0.143)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96429831 <a title="136-lda-1" href="./acl-2010-Estimating_Strictly_Piecewise_Distributions.html">103 acl-2010-Estimating Strictly Piecewise Distributions</a></p>
<p>Author: Jeffrey Heinz ; James Rogers</p><p>Abstract: Strictly Piecewise (SP) languages are a subclass of regular languages which encode certain kinds of long-distance dependencies that are found in natural languages. Like the classes in the Chomsky and Subregular hierarchies, there are many independently converging characterizations of the SP class (Rogers et al., to appear). Here we define SP distributions and show that they can be efficiently estimated from positive data.</p><p>2 0.96324843 <a title="136-lda-2" href="./acl-2010-GernEdiT_-_The_GermaNet_Editing_Tool.html">126 acl-2010-GernEdiT - The GermaNet Editing Tool</a></p>
<p>Author: Verena Henrich ; Erhard Hinrichs</p><p>Abstract: GernEdiT (short for: GermaNet Editing Tool) offers a graphical interface for the lexicographers and developers of GermaNet to access and modify the underlying GermaNet resource. GermaNet is a lexical-semantic wordnet that is modeled after the Princeton WordNet for English. The traditional lexicographic development of GermaNet was error prone and time-consuming, mainly due to a complex underlying data format and no opportunity of automatic consistency checks. GernEdiT replaces the earlier development by a more userfriendly tool, which facilitates automatic checking of internal consistency and correctness of the linguistic resource. This paper pre- sents all these core functionalities of GernEdiT along with details about its usage and usability. 1</p><p>3 0.92673641 <a title="136-lda-3" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>4 0.9223882 <a title="136-lda-4" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>Author: Michael Connor ; Yael Gertner ; Cynthia Fisher ; Dan Roth</p><p>Abstract: A fundamental step in sentence comprehension involves assigning semantic roles to sentence constituents. To accomplish this, the listener must parse the sentence, find constituents that are candidate arguments, and assign semantic roles to those constituents. Each step depends on prior lexical and syntactic knowledge. Where do children learning their first languages begin in solving this problem? In this paper we focus on the parsing and argumentidentification steps that precede Semantic Role Labeling (SRL) training. We combine a simplified SRL with an unsupervised HMM part of speech tagger, and experiment with psycholinguisticallymotivated ways to label clusters resulting from the HMM so that they can be used to parse input for the SRL system. The results show that proposed shallow representations of sentence structure are robust to reductions in parsing accuracy, and that the contribution of alternative representations of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argumentidentification stages.</p><p>5 0.86956197 <a title="136-lda-5" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>Author: Georgios Paltoglou ; Mike Thelwall</p><p>Abstract: Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.</p><p>same-paper 6 0.86300254 <a title="136-lda-6" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>7 0.80222666 <a title="136-lda-7" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>8 0.77109516 <a title="136-lda-8" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>9 0.74180841 <a title="136-lda-9" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>10 0.71337616 <a title="136-lda-10" href="./acl-2010-String_Extension_Learning.html">217 acl-2010-String Extension Learning</a></p>
<p>11 0.7130931 <a title="136-lda-11" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>12 0.7014401 <a title="136-lda-12" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>13 0.69914258 <a title="136-lda-13" href="./acl-2010-Multilingual_Pseudo-Relevance_Feedback%3A_Performance_Study_of_Assisting_Languages.html">177 acl-2010-Multilingual Pseudo-Relevance Feedback: Performance Study of Assisting Languages</a></p>
<p>14 0.69538122 <a title="136-lda-14" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>15 0.69344163 <a title="136-lda-15" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>16 0.69255555 <a title="136-lda-16" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>17 0.69136292 <a title="136-lda-17" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>18 0.68563348 <a title="136-lda-18" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>19 0.68394291 <a title="136-lda-19" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>20 0.68383116 <a title="136-lda-20" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
