<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-249" href="#">acl2010-249</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2010-249-pdf" href="http://aclweb.org/anthology//P/P10/P10-3006.pdf">pdf</a></p><p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>Reference: <a title="acl-2010-249-reference" href="../acl2010_reference/acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 t r  Abstract We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. [sent-4, score-1.486]
</p><p>2 We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. [sent-5, score-0.128]
</p><p>3 We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. [sent-6, score-0.755]
</p><p>4 Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to  English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. [sent-7, score-0.86]
</p><p>5 A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU. [sent-8, score-0.375]
</p><p>6 1 Introduction In statistical machine translation (SMT), words are normally considered as the building blocks of translation models. [sent-9, score-0.546]
</p><p>7 , it has been shown that using sub-lexical units obtained after morphological preprocessing can improve the machine translation performance over a word-based system (Habash and Sadat, 2006; Oflazer and Durgar El-Kahlout, 2007; Bisazza and Federico, 2009). [sent-11, score-0.503]
</p><p>8 However, the effect of segmentation on translation performance is indirect and difficult to isolate (Lopez and Resnik, 2006). [sent-12, score-0.627]
</p><p>9 The challenge in designing a sub-lexical SMT system is the decision ofwhat segmentation to use. [sent-13, score-0.394]
</p><p>10 Linguistic morphological analysis is intuitive, but it is language-dependent and could be highly ambiguous. [sent-14, score-0.124]
</p><p>11 Furthermore, it is not necessarily optimal in that (i) manually engineered segmentation schemes can outperform a straightforward linguistic morphological segmentation, e. [sent-15, score-0.636]
</p><p>12 A SMT system designer has to decide what segmentation is optimal for the translation task at hand. [sent-20, score-0.684]
</p><p>13 Another point to consider is that the optimal degree of segmentation might decrease as the amount of training data increases (Lee, 2004; Habash and Sadat, 2006). [sent-22, score-0.451]
</p><p>14 Therefore, it is desirable to learn the optimal segmentation in an unsupervised manner. [sent-24, score-0.497]
</p><p>15 In this work, we extend the method of Creutz and Lagus (2007) so as to maximize the translation posterior in unsupervised segmentation. [sent-25, score-0.338]
</p><p>16 The learning process is tailored to the particular SMT task via the same parallel corpus that is used in training the statistical translation models. [sent-26, score-0.338]
</p><p>17 2  Related Work  Most works in SMT-oriented segmentation are supervised in that they consist of manual experimentation to choose the best among a set of segmentation schemes, and are language(pair)-dependent. [sent-27, score-0.788]
</p><p>18 For Arabic, Sadat and Habash (2006) present several morphological preprocessing schemes that entail varying degrees of decomposition and com31  UppsaPlra,o Scewe didnegn,s o 1f3 t Jhuely AC 20L10 20. [sent-28, score-0.261]
</p><p>19 0c S20tu1d0e Ants Roecsiea tirconh f Woror Cksomhop u,t pa tgioensa 3l1 L–in3g6u,istics  pare the resulting translation performances in an Arabic-to-English task. [sent-30, score-0.233]
</p><p>20 (2007) use a subset of the morphology and apply only a few simple rules in segmenting words. [sent-32, score-0.066]
</p><p>21 They use a morphological analyzer and disambiguation to arrive at morphemes as tokens. [sent-34, score-0.211]
</p><p>22 However, training the translation models with morphemes actually degrades the translation performance. [sent-35, score-0.558]
</p><p>23 Bisazza and Federico (2009) adopt an approach similar to the Arabic segmentation studies above, this time in a Turkishto-English translation setting. [sent-37, score-0.627]
</p><p>24 Unsupervised segmentation by itself has garnered considerable attention in the computational linguistics literature (Poon et al. [sent-38, score-0.394]
</p><p>25 However, few works report their performance in a translation task. [sent-40, score-0.233]
</p><p>26 (2007) used Morfessor (Creutz  and Lagus, 2007) to segment both sides of the parallel training corpora in translation between Danish, Finnish, and Swedish, but without a consistent improvement in results. [sent-42, score-0.382]
</p><p>27 , 2009), uses only monolingual information in its objective function. [sent-44, score-0.098]
</p><p>28 It is conceivable that we can achieve a better segmentation for translation by considering not one but both sides of the parallel corpus. [sent-45, score-0.776]
</p><p>29 (2009) show that optimizing segmentation with respect to alignment error rate (AER) does not improve and even degrades machine translation performance. [sent-48, score-0.765]
</p><p>30 Snyder and Barzilay (2008) use bilingual information but the segmentation is learned independently from translation modeling. [sent-49, score-0.709]
</p><p>31 One motivation in morphological preprocessing before translation modeling is “morphology matching” as in Lee (2004) and in the scheme “EN” of Habash and Sadat (2006). [sent-52, score-0.433]
</p><p>32 In Lee (2004), the goal is to match the lexical granularities of the two languages by starting with a fine-grained segmentation of the Arabic side of the corpus and then merging or deleting Arabic morphemes using alignments with a part-of-speech tagged English corpus. [sent-53, score-0.513]
</p><p>33 But this method is not completely unsupervised since it requires external linguistic resources in initializing the segmentation with the output of a morphological analyzer and disambiguator. [sent-54, score-0.597]
</p><p>34 Talbot and Osborne (2006) tackle a special case of morphology matching by identifying redundant distinctions in the morphology of one language compared to another. [sent-55, score-0.171]
</p><p>35 3  Method  Maximizing  translation  performance  directly  would require SMT training and decoding for each segmentation hypothesis considered, which is computationally infeasible. [sent-56, score-0.627]
</p><p>36 So we make some conditional independence assumptions using a generative model and decompose the posterior probability P(Mf |e, f). [sent-57, score-0.184]
</p><p>37 In this notation e and f denote the two sid|ees, fof) a parallel corpus aen dan Mf denotes the segmentation model hypothesized for f. [sent-58, score-0.51]
</p><p>38 Our approach is an extension of Morfessor (Creutz and Lagus, 2007) so as to include the translation model probability in its cost calculation. [sent-59, score-0.333]
</p><p>39 Specifically, the segmentation model takes into account the likelihood of both sides of the parallel corpus while searching for the optimal segmentation. [sent-60, score-0.691]
</p><p>40 The joint likelihood is decomposed into a prior, a monolingual likelihood, and a translation likelihood, as shown in Eq. [sent-61, score-0.422]
</p><p>41 P(e, f,Mf) = P(Mf)P(f|Mf)P(e|f, Mf) (1) Assuming conditional independence between e and Mf given f, the maximum a posteriori (MAP) objective can be written as:  Mˆf= argMmfaxP(Mf)P(f|Mf)P(e|f)  (2)  The role of the bilingual component P(e|f) in Eq. [sent-63, score-0.138]
</p><p>42 Consider an occurrence of two phrase pairs in a Turkish-English parallel corpus and the two hypothesized sets of segmentations for the Turkish phrases as in Table 1. [sent-65, score-0.19]
</p><p>43 Without access to the English side of the corpus, a monolingual segmenter can quite possibly score Seg. [sent-66, score-0.173]
</p><p>44 , due to the high frequency of the observed morph “+m”). [sent-71, score-0.065]
</p><p>45 On the other hand, a bilingual segmenter is expected to  assign a higher alignment probability P(e|f) to Seg. [sent-72, score-0.191]
</p><p>46 The monolingual likelihood P(f|Mf) dise computed as t mheo product aolf l morph probabilities estimated from their frequencies in the corpus. [sent-80, score-0.254]
</p><p>47 To compute the bilingual (translation) likelihood P(e|f), we use IBM Model 1 (Brown et al. [sent-81, score-0.173]
</p><p>48 )L,et w an aligned se Mnotednecle pair o bew represented by (se, sf), which consists of word sequences se = e1, . [sent-83, score-0.064]
</p><p>49 1 Incremental computation of Model-1 likelihood During search, the translation likelihood P(e|f) nDeuerdisn to beea rccahl,cu thlaete tdra according to Eq. [sent-92, score-0.415]
</p><p>50 3, we need to have at hand the individual morph translation probabilities t(fj |ei). [sent-95, score-0.298]
</p><p>51 These can be estimated using the EM algorithm given by (Brown, 1993), which is guaranteed to converge to a global maximum of the likelihood for Model 1. [sent-96, score-0.091]
</p><p>52 However, running the EM algorithm to optimization for each considered segmentation model can be computationally expensive, and can result in overtraining. [sent-97, score-0.394]
</p><p>53 Therefore, in this work we used the likelihood computed after the first EM iteration, which also has the nice property that  P(f|e) can be computed incrementally from one segmentation hypothesis dto in tchree mneexntt. [sent-98, score-0.522]
</p><p>54 a The incremental updates are derived from the equations for the count collection and probability estimation steps of the EM algorithm as follows. [sent-99, score-0.097]
</p><p>55 In the count collection step, in the first iteration, we need to compute the fractional counts c(fj |ei) (Brown et al. [sent-100, score-0.068]
</p><p>56 , 1993):  c(fj|ei) =l +1 1(#fj)(#ei),  (4)  where (#fj) and (#ei) denote the number of occurrences of fj in sf and ei in se, respectively. [sent-101, score-0.417]
</p><p>57 Let fk denote the word hypothesized to be segmented. [sent-102, score-0.152]
</p><p>58 (4), as a result of the segmentation no update is needed for c(fj |ei) for j = 1. [sent-105, score-0.394]
</p><p>59 M (note th)at fo fk no longer exists); ap,ndq ,t ihe = necessary updates ∆c(fj |ei) for c(fj |ei), where j = p, q; i= 1. [sent-111, score-0.122]
</p><p>60 Then in the probability estimation step, the normalization is performed including the newly added fractional counts. [sent-118, score-0.073]
</p><p>61 2 Parallelization of search In an iteration of the algorithm, all words are processed in random order, computing for each word the posterior probability of the generative model after each possible binary segmentation (splitting) of the word. [sent-120, score-0.627]
</p><p>62 If the highest-scoring split increases the posterior probability compared to not splitting, that split is accepted (for all occurrences of the word) and the resulting sub-words are explored recursively for further segmentations. [sent-121, score-0.096]
</p><p>63 The process is repeated until an iteration no more results in a significant increase in the posterior probability. [sent-122, score-0.127]
</p><p>64 The search algorithm of Morfessor is a greedy algorithm where the costs of the next search points 33  53. [sent-123, score-0.074]
</p><p>65 4 Word-based  Morfessor Morfessor-p Segmentatoin method  Morfessor-bi  Figure 1: BLEU scores obtained with different segmentation methods. [sent-132, score-0.394]
</p><p>66 This leads to a sequential search and does not lend  itself to parallelization. [sent-135, score-0.068]
</p><p>67 We propose a slightly modified search procedure, where the segmentation decisions are stored but not applied until the end of an iteration. [sent-136, score-0.431]
</p><p>68 Since the model is not updated at every decision, the search path can differ from that in the sequential greedy search and hence result in different segmentations. [sent-138, score-0.105]
</p><p>69 4  Results  We performed in vivo testing of the segmentation algorithm on the Turkish side of a Turkishto-English task. [sent-139, score-0.43]
</p><p>70 We compared the segmentations produced by Morfessor, Morfessor modified for parallel search (Morfessor-p), and Morfessor with bilingual cost (Morfessor-bi) against the word-based performance. [sent-140, score-0.314]
</p><p>71 , 2007) with default parameters for phrase-based translation model generation and decoding. [sent-148, score-0.233]
</p><p>72 Target language models were  Morfessor cost  x  106  Morfessor-bi cost  x  106  Figure 2: Cost-BLEU plots of Morfessor and Morfessor-bi. [sent-149, score-0.126]
</p><p>73 Figure 1 compares the translation performance obtained using the described segmentation methods. [sent-156, score-0.627]
</p><p>74 All segmentation methods generally improve the translation performance (Morfessor and Morfessor-p) compared to the word-based models. [sent-157, score-0.627]
</p><p>75 However, Morfessor-bi, which utilizes both sides of the parallel corpus in segmenting, does not convincingly outperform the monolingual methods. [sent-158, score-0.247]
</p><p>76 In order to investigate whether the proposed bilingual segmentation cost correlates any better than the monolingual segmentation cost of Morfessor, we show several cost-BLEU pairs obtained from the final and intermediate segmentations of Morfessor and Morfessor-bi in Fig. [sent-159, score-1.196]
</p><p>77 The correlation coefficients show that the proposed bilingual metric is somewhat predictive of the translation performance as measured by BLEU, while  the monolingual Morfessor cost metric has almost no correlation. [sent-161, score-0.56]
</p><p>78 Indeed, in our experiments even though the total cost kept decreasing at each iteration of the search algorithm, the BLEU scores obtained by those intermediate segmentations fluctuated without any consistent improvement. [sent-165, score-0.27]
</p><p>79 Table 2 displays sample segmentations produced by both the monolingual and bilingual segmentation algorithms. [sent-166, score-0.648]
</p><p>80 We can observe that utilizing the English side of the corpus enabled 34  Morfessor-bi: (i) to consistently identify the root  word “anahtar” (top portion), and (ii) to match the English plural word form “games” with the Turkish plural word form “oyunlar” (bottom portion). [sent-167, score-0.102]
</p><p>81 Monolingual Morfessor is unaware of the target segmentation, and hence it is up to the subsequent translation model training to learn that “oyun” is sometimes translated as “game” and sometimes as “games” in the segmented training corpus. [sent-168, score-0.233]
</p><p>82 5  Conclusion  We have presented a method for determining optimal sub-word translation units automatically from a parallel corpus. [sent-169, score-0.385]
</p><p>83 We have also showed a method of incrementally computing the first iteration parameters of IBM Model-1 between segmentation hypotheses. [sent-170, score-0.499]
</p><p>84 Being language-independent, the proposed algorithm can be added as a one-time preprocessing step prior to training in a SMT system without requiring any additional data/linguistic resources. [sent-171, score-0.076]
</p><p>85 The initial experiments presented here show that the translation units learned by the proposed algorithm improves on the word-based baseline in both translation directions. [sent-172, score-0.503]
</p><p>86 One avenue for future work is to relax some of the several independence assumptions made in the generative model. [sent-173, score-0.088]
</p><p>87 For example, independence of consecutive morphs could be relaxed by an HMM model for transitions between morphs (Creutz and Lagus, 2007). [sent-174, score-0.286]
</p><p>88 Other future work includes optimizing the segmentation of both sides of the corpus and experimenting with other language pairs. [sent-175, score-0.519]
</p><p>89 It is also possible that the probability distributions are not discriminative enough to outweigh the model prior tendencies since the translation probabilities are estimated only crudely (single iteration of Model-1 EM algorithm). [sent-176, score-0.338]
</p><p>90 A possible candidate solution would be to weigh the translation likelihood more in calculating the overall cost. [sent-177, score-0.324]
</p><p>91 , 2009)) of the various components of the joint corpus likelihood and possibly other features. [sent-181, score-0.091]
</p><p>92 Finally, integration of sub-word segmentation with the phrasal lexicon learning process in SMT is desireable (e. [sent-182, score-0.394]
</p><p>93 Hierarchical models (Chiang,  2007) could cover this gap and provide a means to seamlessly integrate sub-word segmentation with statistical machine translation. [sent-185, score-0.474]
</p><p>94 An efficient, probabilistically sound algorithm for segmentation and word discovery. [sent-197, score-0.394]
</p><p>95 Initial explorations in English to Turkish statistical machine translation. [sent-232, score-0.08]
</p><p>96 Combination of statistical word alignments based on multiple preprocessing schemes. [sent-237, score-0.152]
</p><p>97 Exploring different representational units in English-to-Turkish statistical machine translation. [sent-282, score-0.117]
</p><p>98 Combination of Arabic preprocessing schemes for statistical machine translation. [sent-294, score-0.217]
</p><p>99 Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner. [sent-322, score-0.474]
</p><p>100 Stochastic inversion transduction grammars and bilingual parsing ofparallel corpora. [sent-327, score-0.082]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('segmentation', 0.394), ('morfessor', 0.347), ('translation', 0.233), ('mf', 0.205), ('fj', 0.199), ('creutz', 0.196), ('lagus', 0.173), ('ei', 0.167), ('durgar', 0.144), ('turkish', 0.144), ('habash', 0.144), ('smt', 0.142), ('sadat', 0.139), ('morphological', 0.124), ('morphs', 0.115), ('monolingual', 0.098), ('fk', 0.094), ('likelihood', 0.091), ('sides', 0.091), ('arabic', 0.087), ('bisazza', 0.086), ('bilingual', 0.082), ('oflazer', 0.082), ('bleu', 0.081), ('preprocessing', 0.076), ('turkey', 0.076), ('segmentations', 0.074), ('virpioja', 0.069), ('iteration', 0.068), ('morphology', 0.066), ('morph', 0.065), ('nizar', 0.065), ('cost', 0.063), ('schemes', 0.061), ('posterior', 0.059), ('parallel', 0.058), ('hypothesized', 0.058), ('anahtar', 0.058), ('elming', 0.058), ('lknur', 0.058), ('optimal', 0.057), ('independence', 0.056), ('morphemes', 0.054), ('snyder', 0.052), ('sf', 0.051), ('poon', 0.051), ('murat', 0.051), ('fq', 0.051), ('sara', 0.051), ('ahmet', 0.051), ('dasgupta', 0.051), ('fatiha', 0.051), ('istanbul', 0.051), ('clar', 0.051), ('kikui', 0.051), ('kemal', 0.051), ('statistical', 0.047), ('unsupervised', 0.046), ('kun', 0.046), ('parallelization', 0.046), ('companion', 0.045), ('em', 0.044), ('kurimo', 0.043), ('talbot', 0.039), ('segmenter', 0.039), ('brown', 0.039), ('tackle', 0.039), ('degrades', 0.038), ('morpheme', 0.038), ('lopez', 0.038), ('search', 0.037), ('incrementally', 0.037), ('units', 0.037), ('probability', 0.037), ('federico', 0.037), ('fractional', 0.036), ('ibm', 0.036), ('side', 0.036), ('optimizing', 0.034), ('finnish', 0.034), ('fp', 0.033), ('plural', 0.033), ('analyzer', 0.033), ('alignment', 0.033), ('machine', 0.033), ('count', 0.032), ('se', 0.032), ('games', 0.032), ('hltnaacl', 0.032), ('ak', 0.032), ('shen', 0.032), ('generative', 0.032), ('sequential', 0.031), ('travel', 0.03), ('alignments', 0.029), ('metric', 0.028), ('measured', 0.028), ('intermediate', 0.028), ('updates', 0.028), ('marcello', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="249-tfidf-1" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>2 0.19485503 <a title="249-tfidf-2" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>3 0.18847702 <a title="249-tfidf-3" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>4 0.1834874 <a title="249-tfidf-4" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>Author: Joern Wuebker ; Arne Mauser ; Hermann Ney</p><p>Abstract: Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overfitting. We describe a novel leavingone-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering mod- els in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.</p><p>5 0.17896809 <a title="249-tfidf-5" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>6 0.15386452 <a title="249-tfidf-6" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>7 0.1435917 <a title="249-tfidf-7" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>8 0.14253423 <a title="249-tfidf-8" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>9 0.13797095 <a title="249-tfidf-9" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>10 0.13494623 <a title="249-tfidf-10" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>11 0.1326839 <a title="249-tfidf-11" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>12 0.12753192 <a title="249-tfidf-12" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>13 0.11508211 <a title="249-tfidf-13" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>14 0.11350217 <a title="249-tfidf-14" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>15 0.11259951 <a title="249-tfidf-15" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>16 0.10661997 <a title="249-tfidf-16" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>17 0.10642552 <a title="249-tfidf-17" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>18 0.10015054 <a title="249-tfidf-18" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>19 0.096689232 <a title="249-tfidf-19" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>20 0.094987243 <a title="249-tfidf-20" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.237), (1, -0.249), (2, -0.071), (3, -0.009), (4, 0.065), (5, 0.035), (6, -0.062), (7, -0.016), (8, 0.022), (9, 0.052), (10, 0.118), (11, 0.152), (12, 0.132), (13, -0.051), (14, -0.002), (15, -0.086), (16, -0.071), (17, 0.065), (18, 0.081), (19, -0.056), (20, -0.079), (21, -0.076), (22, -0.035), (23, 0.032), (24, 0.036), (25, 0.063), (26, -0.038), (27, -0.031), (28, -0.041), (29, 0.035), (30, 0.069), (31, 0.022), (32, -0.074), (33, -0.037), (34, -0.038), (35, -0.041), (36, 0.149), (37, 0.041), (38, -0.044), (39, 0.149), (40, -0.158), (41, 0.019), (42, -0.055), (43, -0.053), (44, 0.069), (45, 0.011), (46, -0.022), (47, 0.131), (48, -0.014), (49, -0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94998407 <a title="249-lsi-1" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>2 0.73350763 <a title="249-lsi-2" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>3 0.69223845 <a title="249-lsi-3" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>4 0.65102613 <a title="249-lsi-4" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>5 0.64696819 <a title="249-lsi-5" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>Author: Sebastian Spiegler ; Peter A. Flach</p><p>Abstract: This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can significantly improve the performance of machine learning methods for morphological word decomposition. We employ two algorithms which come from a family of generative probabilistic models. The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. The advantage of this model family is that it can learn from small datasets and easily gen- eralises to larger datasets. The first algorithm PROMODES, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm PROMODES-H is a novel development of the first using a higher order model. We present the mathematical description for both algorithms, conduct experiments on the morphologically rich language Zulu and compare characteristics of both algorithms based on the experimental results.</p><p>6 0.61730123 <a title="249-lsi-6" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>7 0.59912783 <a title="249-lsi-7" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>8 0.5968352 <a title="249-lsi-8" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>9 0.58001506 <a title="249-lsi-9" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>10 0.5755586 <a title="249-lsi-10" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>11 0.57045543 <a title="249-lsi-11" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>12 0.52309895 <a title="249-lsi-12" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>13 0.51520032 <a title="249-lsi-13" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>14 0.50992566 <a title="249-lsi-14" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>15 0.49262187 <a title="249-lsi-15" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>16 0.4708409 <a title="249-lsi-16" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>17 0.46139953 <a title="249-lsi-17" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>18 0.46059698 <a title="249-lsi-18" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>19 0.45201269 <a title="249-lsi-19" href="./acl-2010-The_Use_of_Formal_Language_Models_in_the_Typology_of_the_Morphology_of_Amerindian_Languages.html">234 acl-2010-The Use of Formal Language Models in the Typology of the Morphology of Amerindian Languages</a></p>
<p>20 0.45170096 <a title="249-lsi-20" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.013), (16, 0.308), (25, 0.055), (42, 0.023), (44, 0.011), (59, 0.155), (73, 0.032), (76, 0.012), (78, 0.017), (83, 0.087), (84, 0.021), (98, 0.186)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92837846 <a title="249-lda-1" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>Author: Ondrej Bojar ; Kamil Kos ; David Marecek</p><p>Abstract: We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.</p><p>2 0.89115751 <a title="249-lda-2" href="./acl-2010-Untangling_the_Cross-Lingual_Link_Structure_of_Wikipedia.html">250 acl-2010-Untangling the Cross-Lingual Link Structure of Wikipedia</a></p>
<p>Author: Gerard de Melo ; Gerhard Weikum</p><p>Abstract: Wikipedia articles in different languages are connected by interwiki links that are increasingly being recognized as a valuable source of cross-lingual information. Unfortunately, large numbers of links are imprecise or simply wrong. In this paper, techniques to detect such problems are identified. We formalize their removal as an optimization task based on graph repair operations. We then present an algorithm with provable properties that uses linear programming and a region growing technique to tackle this challenge. This allows us to transform Wikipedia into a much more consistent multilingual register of the world’s entities and concepts.</p><p>same-paper 3 0.83546001 <a title="249-lda-3" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>4 0.79100347 <a title="249-lda-4" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>Author: Jun Sun ; Min Zhang ; Chew Lim Tan</p><p>Abstract: We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1</p><p>5 0.68029559 <a title="249-lda-5" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>Author: Jinsong Su ; Yang Liu ; Yajuan Lv ; Haitao Mi ; Qun Liu</p><p>Abstract: Lexicalized reordering models play a crucial role in phrase-based translation systems. They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1</p><p>6 0.67573071 <a title="249-lda-6" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>7 0.67424369 <a title="249-lda-7" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>8 0.67378992 <a title="249-lda-8" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>9 0.66055167 <a title="249-lda-9" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>10 0.65613222 <a title="249-lda-10" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>11 0.65101594 <a title="249-lda-11" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>12 0.647542 <a title="249-lda-12" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>13 0.64701319 <a title="249-lda-13" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>14 0.64246726 <a title="249-lda-14" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>15 0.64166158 <a title="249-lda-15" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>16 0.64076149 <a title="249-lda-16" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>17 0.64034247 <a title="249-lda-17" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>18 0.63926524 <a title="249-lda-18" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>19 0.63852 <a title="249-lda-19" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>20 0.63837087 <a title="249-lda-20" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
