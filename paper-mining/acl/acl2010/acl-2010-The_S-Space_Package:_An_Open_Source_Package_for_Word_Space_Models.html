<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-232" href="#">acl2010-232</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</h1>
<br/><p>Source: <a title="acl-2010-232-pdf" href="http://aclweb.org/anthology//P/P10/P10-4006.pdf">pdf</a></p><p>Author: David Jurgens ; Keith Stevens</p><p>Abstract: We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</p><p>Reference: <a title="acl-2010-232-reference" href="../acl2010_reference/acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. [sent-3, score-0.267]
</p><p>2 The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. [sent-4, score-0.857]
</p><p>3 The package also includes word space benchmarks for evaluation. [sent-5, score-0.755]
</p><p>4 Both algorithms and libraries are designed for high concurrency and scalability. [sent-6, score-0.212]
</p><p>5 We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks. [sent-7, score-0.318]
</p><p>6 1 Introduction Word similarity is an essential part of understanding natural language. [sent-8, score-0.144]
</p><p>7 Similarity enables meaningful comparisons, entailments, and is a bridge to building and extending rich ontologies for evaluating word semantics. [sent-9, score-0.065]
</p><p>8 Word space algorithms have been proposed as an automated approach for developing meaningfully comparable semantic representations based on word distributions in text. [sent-10, score-0.446]
</p><p>9 In addition, these models have provided insight in fields outside of linguistics, such as information retrieval, natu-  ral language processing and cognitive psychology. [sent-15, score-0.094]
</p><p>10 For a recent survey of word space approaches and applications, see (Turney and Pantel, 2010). [sent-16, score-0.267]
</p><p>11 edu The parallel development of word space models in different fields has often resulted in duplicated work. [sent-19, score-0.306]
</p><p>12 Furthermore, given the frequent similarity of approaches, we argue that the research community would greatly benefit from a common library and evaluation utilities for word spaces. [sent-21, score-0.422]
</p><p>13 a comprehensive, highly concurrent library of  tools for building new models 3. [sent-24, score-0.19]
</p><p>14 an evaluation framework for testing models on standard benchmarks, e. [sent-25, score-0.039]
</p><p>15 a standardized interface for interacting with all word space models, which facilitates word space based applications. [sent-29, score-0.631]
</p><p>16 The package is written in Java and defines a standardized Java interface for word space algorithms. [sent-30, score-0.668]
</p><p>17 (Widdows and Ferraro, 2008), the focus of this framework is to ease the development of new algorithms and the comparison against existing models. [sent-33, score-0.115]
</p><p>18 We hope that the release of this framework will greatly facilitate other researchers in their efforts to de-  velop and validate new word space models. [sent-35, score-0.267]
</p><p>19 com/ p/airhead-research/, which includes a wiki 30  UppsalaP,r Sowce ed enin,g 1s3 o Jfu tlhye 2 A0C1L0. [sent-38, score-0.082]
</p><p>20 2  Word Space Models  Word space models are based on the contextual distribution in which a word occurs. [sent-41, score-0.306]
</p><p>21 This approach has a long history in linguistics, starting with Firth (1957) and Harris (1968), the latter of whom defined this approach as the Distributional Hypothesis: for two words, their similarity in meaning is predicted by the similarity of the distributions of their co-occurring words. [sent-42, score-0.288]
</p><p>22 Later models have expanded the notion ofco-occurrence but retain the premise that distributional similarity can be used to extract meaningful relationships between words. [sent-43, score-0.225]
</p><p>23 Word space algorithms consist of the same core  algorithmic steps: word features are extracted from a corpus and the distribution of these features is used as a basis for semantic similarity. [sent-44, score-0.502]
</p><p>24 Figure 1 illustrates the shared algorithmic structure of all the approaches, which is divided into four components: corpus processing, context selection, feature extraction and global vector space operations. [sent-45, score-0.351]
</p><p>25 Corpus processing techniques frequently include stemming and filtering of stop words or low-frequency words. [sent-47, score-0.038]
</p><p>26 For web-gathered corpora, these steps also include removal of non linguistic tokens, such as html markup, or restricting documents to a single language. [sent-48, score-0.043]
</p><p>27 Feature extraction determines the dimensions of  the vector space by selecting which tokens in the context will count as features. [sent-52, score-0.29]
</p><p>28 Features are commonly word co-occurrences, but more advanced models may perform a statistical analysis to select only those features that best distinguish word meanings. [sent-53, score-0.169]
</p><p>29 Other models approximate the full set of features to enable better scalability. [sent-54, score-0.039]
</p><p>30 Global vector space operations are applied to the entire space once the initial word features have been computed. [sent-55, score-0.582]
</p><p>31 Common operations include altering feature weights and dimensionality reducDocument-Based Models LSA (Landauer and Dumais, 1997) ESA (Gabrilovich and Markovitch, 2007) Vector Space Model (Salton et al. [sent-56, score-0.129]
</p><p>32 These operations are designed to improve word similarity by changing the feature space itself. [sent-63, score-0.509]
</p><p>33 3  The S-Space Framework  The S-Space framework is designed to be extensible, simple to use, and scalable. [sent-64, score-0.038]
</p><p>34 We achieve these goals through the use of Java interfaces, reusable word space related data structures, and support for multi-threading. [sent-65, score-0.353]
</p><p>35 Each word space algorithm is designed to run as a stand alone program and also to be used as a library class. [sent-66, score-0.407]
</p><p>36 1 Reference Algorithms The package provides reference implementations for twelve word space algorithms, which are listed in Table 1. [sent-68, score-0.817]
</p><p>37 Each algorithm is implemented in its  own Java package, and all commonalities have been factored out into reusable library classes. [sent-69, score-0.188]
</p><p>38 The algorithms implement the same Java interface, which provides a consistent abstraction of the four processing stages. [sent-70, score-0.158]
</p><p>39 We divide the algorithms into four categories based on their structural similarity: documentbased, co-occurrence, approximation, and Word Sense Induction (WSI) models. [sent-71, score-0.115]
</p><p>40 Document-based models divide a corpus into discrete documents and construct the vector space from word frequencies in the documents. [sent-72, score-0.402]
</p><p>41 The documents are defined independently of the words that appear in them. [sent-73, score-0.043]
</p><p>42 Co-occurrence models build the vector space using the distribution of co-occurring words in a context, which is typically defined as a region around a word or paths rooted in a parse tree. [sent-74, score-0.359]
</p><p>43 WSI models also use co-occurrence but also attempt to discover distinct word senses while building the vector space. [sent-76, score-0.157]
</p><p>44 For example, these algorithms might represent “earth” with two vectors based on its meanings “planet” and “dirt. [sent-77, score-0.151]
</p><p>45 2  Data Structures and Utilities  The S-Space Package provides efficient implementations for matrices, vectors, and specialized data structures such as multi-maps and tries. [sent-79, score-0.248]
</p><p>46 util library and offer concurrent implementations when multi-threading is required. [sent-81, score-0.305]
</p><p>47 In addition, the libraries provide support for converting between multiple matrix formats, enabling interaction with external matrix-based programs. [sent-82, score-0.099]
</p><p>48 The package also provides support for parsing different corpora formats, such as XML or email threads. [sent-83, score-0.347]
</p><p>49 3 Global Operation Utilities Many algorithms incorporate dimensionality reduction to smooth their feature data, e. [sent-85, score-0.184]
</p><p>50 The S-Space Package supports two common techniques: the Singular Value Decomposition (SVD) and randomized projections. [sent-93, score-0.125]
</p><p>51 All matrix data structures are designed to seamlessly integrate with six SVD implementations for maximum portability, including SVDLIBJ1 , a Java port of SVDLIBC2, a scalable sparse SVD library. [sent-94, score-0.385]
</p><p>52 The package also provides a comprehensive library for randomized projections, which project high-dimensional feature data into a lower dimensional space. [sent-95, score-0.528]
</p><p>53 The library supports both integer-based projections (Kanerva et al. [sent-96, score-0.23]
</p><p>54 The package supports common matrix transformations that have been applied to word spaces: point wise mutual information (Dekang, 1http://bender. [sent-99, score-0.496]
</p><p>55 4  Measurements  The choice of similarity function for the vector space is the least standardized across approaches. [sent-105, score-0.501]
</p><p>56 5  Clustering  Clustering serves as a tool for building and refining word spaces. [sent-109, score-0.065]
</p><p>57 (Purandare and Pedersen, 2004), use clustering to discover the different meanings of a word in a corpus. [sent-112, score-0.154]
</p><p>58 The S-Space Package provides bindings for using the CLUTO clustering package3. [sent-113, score-0.132]
</p><p>59 In addition, the package provides Java implementations  of Hierarchical Agglomerative Clustering, Spectral Clustering (Kannan et al. [sent-114, score-0.501]
</p><p>60 4  Benchmarks  Word space benchmarks assess the semantic content of the space through analyzing the geometric properties of the space itself. [sent-117, score-0.889]
</p><p>61 Currently used benchmarks assess the semantics by inspecting the representational similarity of word pairs. [sent-118, score-0.428]
</p><p>62 Two types of benchmarks are commonly used: word choice tests and association tests. [sent-119, score-0.365]
</p><p>63 The S-Space Package supports six tests, and has an easily extensible model for adding new tests. [sent-120, score-0.147]
</p><p>64 , 2008) Table 2: A comparison of the implemented algorithms on common evaluation benchmarks  4. [sent-209, score-0.299]
</p><p>65 1 Word Choice Word choice tests provide a target word and a list of options, one of which has the desired relation to the target. [sent-210, score-0.181]
</p><p>66 Word space models solve these tests by selecting the option whose representation is most similar. [sent-211, score-0.316]
</p><p>67 Three word choice benchmarks that measure synonymy are supported. [sent-212, score-0.325]
</p><p>68 2 Word Association Word association tests measure the semantic relatedness of two words by comparing word space similarity with human judgements. [sent-218, score-0.594]
</p><p>69 Frequently, these tests measure synonymy; however, other types of word relations such as antonymy (“hot” and “cold”) or functional relatedness (“doctor” and “hospital”) are also possible. [sent-219, score-0.233]
</p><p>70 The first test uses data gathered by Rubenstein and Goodneough (1965). [sent-221, score-0.036]
</p><p>71 To measure word similarity, word similarity scores of 5 1human reviewers were gathered a set of 65 noun pairs, scored on a scale of 0 to 4. [sent-222, score-0.31]
</p><p>72 The ratings are then correlated with word space similarity scores. [sent-223, score-0.411]
</p><p>73 353 word pairs were rated by either 13 or 16 subjects on a 0 to 10 scale for how related the words are. [sent-226, score-0.065]
</p><p>74 This test is notably more challenging for word space models because human ratings are not tied  to a specific semantic relation. [sent-227, score-0.37]
</p><p>75 Deese (1964) introduced 39 antonym pairs that Greffenstette (1992) used to assess whether a word space modeled the antonymy relationship. [sent-229, score-0.449]
</p><p>76 We quantify this relationship by measuring the similarity rank of each word in an antonym pair, w1, w2, i. [sent-230, score-0.307]
</p><p>77 w2 is the kth most-similar word to w1 in the vector space. [sent-232, score-0.118]
</p><p>78 The score  ranges from [0, 1] , where 1indicates that the most similar neighbors in the space are antonyms. [sent-234, score-0.202]
</p><p>79 5  Algorithm Analysis  The content of a word space is fundamentally dependent upon the corpus used to construct it. [sent-236, score-0.267]
</p><p>80 Moreover, algorithms which use operations such as the SVD have a limit to the corpora sizes they 33  Tokens in Documents (in millions) 63. [sent-237, score-0.175]
</p><p>81 5M  125M  173M  228M  267M  296M  Number of documents Figure 2: Processing time across different corpus sizes for a word space with the 100,000 most frequent words  Number of threads Figure 3: Run time improvement as a factor of increasing the number of threads can process. [sent-238, score-0.408]
</p><p>82 The resulting corpus consists of 387,082 documents and 917 million tokens. [sent-243, score-0.043]
</p><p>83 Table 2 reports the scores of reference algorithms on the six benchmarks using cosine similarity. [sent-244, score-0.408]
</p><p>84 The variation in scoring illustrates that different algorithms are more effective at capturing certain semantic relations. [sent-245, score-0.179]
</p><p>85 As a second analysis, we report the efficiency of reference implementations by varying the corpus size and number of threads. [sent-249, score-0.258]
</p><p>86 Algorithm efficiency is determined by three factors: contention on global statistics, contention on disk I/O, and memory limitations. [sent-253, score-0.225]
</p><p>87 Memory limitations account for the largest efficiency constraint, espe-  cially as the corpus size and number of features grow. [sent-255, score-0.055]
</p><p>88 Several algorithms lack data points for larger corpora and show a sharp increase in running time in Figure 2, reflecting the point at which the models no longer fit into 8GB of memory. [sent-256, score-0.154]
</p><p>89 6  Future Work and Conclusion  We have described a framework for developing and evaluating word space algorithms. [sent-257, score-0.267]
</p><p>90 Many well known algorithms are already provided as part of the framework as reference implementations for researches in distributional semantics. [sent-258, score-0.36]
</p><p>91 We have shown that the provided algorithms and libraries scale appropriately. [sent-259, score-0.174]
</p><p>92 Last, we motivate further research by illustrating the significant performance differences of the algorithms on six benchmarks. [sent-260, score-0.175]
</p><p>93 Isa meets lara: A fully incremental word space model for cognitively plausible simulations of semantic learning. [sent-264, score-0.331]
</p><p>94 Reflective random indexing and indirect inference: A scalable method for discovery of implicit connections. [sent-272, score-0.118]
</p><p>95 Finding semantic similarity in raw text: The Deese antonyms. [sent-313, score-0.208]
</p><p>96 Random indexing of text samples for latent semantic analysis. [sent-345, score-0.178]
</p><p>97 Word sense discrimination by clustering contexts in vector and similarity spaces. [sent-376, score-0.286]
</p><p>98 An improved model of semantic similarity based on lexical co-occurrence. [sent-385, score-0.208]
</p><p>99 Permu-  tations as a means to encode order in word space. [sent-400, score-0.065]
</p><p>100 Semantic vectors: a scalable open source package and online technology management application. [sent-436, score-0.346]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tasa', 0.392), ('package', 0.304), ('space', 0.202), ('benchmarks', 0.184), ('landauer', 0.181), ('jurgens', 0.163), ('implementations', 0.154), ('similarity', 0.144), ('algorithms', 0.115), ('rohde', 0.114), ('wsi', 0.114), ('utilities', 0.111), ('java', 0.108), ('sahlgren', 0.105), ('library', 0.102), ('antonym', 0.098), ('purandare', 0.098), ('deese', 0.098), ('clustering', 0.089), ('lsa', 0.088), ('supports', 0.087), ('stevens', 0.086), ('firth', 0.086), ('reusable', 0.086), ('jones', 0.082), ('salton', 0.082), ('svd', 0.082), ('wiki', 0.082), ('angeles', 0.082), ('toefl', 0.079), ('dumais', 0.077), ('indexing', 0.076), ('tests', 0.075), ('los', 0.071), ('gabrilovich', 0.07), ('burgess', 0.07), ('dimensionality', 0.069), ('beagle', 0.065), ('boelter', 0.065), ('coals', 0.065), ('contention', 0.065), ('finkelstein', 0.065), ('hermit', 0.065), ('jarmasz', 0.065), ('kanerva', 0.065), ('kannan', 0.065), ('rdwp', 0.065), ('reflective', 0.065), ('word', 0.065), ('semantic', 0.064), ('baroni', 0.064), ('standardized', 0.061), ('operations', 0.06), ('six', 0.06), ('libraries', 0.059), ('rubenstein', 0.057), ('algorithmic', 0.056), ('efficiency', 0.055), ('cognitive', 0.055), ('keith', 0.054), ('vector', 0.053), ('pedersen', 0.053), ('turney', 0.053), ('tibshirani', 0.052), ('widdows', 0.052), ('isa', 0.052), ('structures', 0.051), ('antonymy', 0.049), ('dominic', 0.049), ('concurrent', 0.049), ('threads', 0.049), ('reference', 0.049), ('weeds', 0.046), ('esl', 0.046), ('lund', 0.046), ('hal', 0.046), ('relatedness', 0.044), ('provides', 0.043), ('documents', 0.043), ('formats', 0.042), ('scalable', 0.042), ('distributional', 0.042), ('choice', 0.041), ('projections', 0.041), ('comprehensive', 0.041), ('matrix', 0.04), ('global', 0.04), ('models', 0.039), ('randomized', 0.038), ('filtering', 0.038), ('designed', 0.038), ('latent', 0.038), ('ri', 0.037), ('gathered', 0.036), ('vectors', 0.036), ('interface', 0.036), ('synonymy', 0.035), ('tokens', 0.035), ('assess', 0.035), ('matrices', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="232-tfidf-1" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>Author: David Jurgens ; Keith Stevens</p><p>Abstract: We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</p><p>2 0.14412409 <a title="232-tfidf-2" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>Author: Boxing Chen ; George Foster ; Roland Kuhn</p><p>Abstract: This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1</p><p>3 0.12557639 <a title="232-tfidf-3" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>Author: Sebastian Rudolph ; Eugenie Giesbrecht</p><p>Abstract: We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.</p><p>4 0.11913411 <a title="232-tfidf-4" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>5 0.098000154 <a title="232-tfidf-5" href="./acl-2010-Event-Based_Hyperspace_Analogue_to_Language_for_Query_Expansion.html">106 acl-2010-Event-Based Hyperspace Analogue to Language for Query Expansion</a></p>
<p>Author: Tingxu Yan ; Tamsin Maxwell ; Dawei Song ; Yuexian Hou ; Peng Zhang</p><p>Abstract: p . zhang1 @ rgu .ac .uk Bag-of-words approaches to information retrieval (IR) are effective but assume independence between words. The Hyperspace Analogue to Language (HAL) is a cognitively motivated and validated semantic space model that captures statistical dependencies between words by considering their co-occurrences in a surrounding window of text. HAL has been successfully applied to query expansion in IR, but has several limitations, including high processing cost and use of distributional statistics that do not exploit syntax. In this paper, we pursue two methods for incorporating syntactic-semantic information from textual ‘events’ into HAL. We build the HAL space directly from events to investigate whether processing costs can be reduced through more careful definition of word co-occurrence, and improve the quality of the pseudo-relevance feedback by applying event information as a constraint during HAL construction. Both methods significantly improve performance results in comparison with original HAL, and interpolation of HAL and relevance model expansion outperforms either method alone.</p><p>6 0.096812002 <a title="232-tfidf-6" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>7 0.090714514 <a title="232-tfidf-7" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>8 0.089872085 <a title="232-tfidf-8" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>9 0.078429706 <a title="232-tfidf-9" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>10 0.077069722 <a title="232-tfidf-10" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>11 0.072309867 <a title="232-tfidf-11" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>12 0.072026581 <a title="232-tfidf-12" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>13 0.06956327 <a title="232-tfidf-13" href="./acl-2010-An_Open-Source_Package_for_Recognizing_Textual_Entailment.html">30 acl-2010-An Open-Source Package for Recognizing Textual Entailment</a></p>
<p>14 0.068172373 <a title="232-tfidf-14" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>15 0.064185522 <a title="232-tfidf-15" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>16 0.062345259 <a title="232-tfidf-16" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>17 0.060925592 <a title="232-tfidf-17" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>18 0.058325097 <a title="232-tfidf-18" href="./acl-2010-Online_Generation_of_Locality_Sensitive_Hash_Signatures.html">183 acl-2010-Online Generation of Locality Sensitive Hash Signatures</a></p>
<p>19 0.058149189 <a title="232-tfidf-19" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>20 0.057856768 <a title="232-tfidf-20" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.192), (1, 0.07), (2, -0.029), (3, -0.016), (4, 0.086), (5, -0.009), (6, 0.034), (7, -0.02), (8, 0.023), (9, 0.012), (10, -0.057), (11, 0.066), (12, 0.093), (13, 0.013), (14, -0.078), (15, 0.092), (16, 0.029), (17, -0.071), (18, -0.067), (19, -0.027), (20, 0.115), (21, 0.005), (22, 0.01), (23, 0.078), (24, 0.033), (25, 0.065), (26, -0.084), (27, -0.023), (28, 0.097), (29, -0.139), (30, -0.03), (31, -0.073), (32, 0.036), (33, 0.007), (34, 0.015), (35, 0.055), (36, 0.019), (37, -0.16), (38, -0.077), (39, 0.003), (40, -0.088), (41, 0.074), (42, 0.081), (43, -0.029), (44, 0.031), (45, 0.045), (46, 0.045), (47, -0.05), (48, -0.137), (49, -0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94309437 <a title="232-lsi-1" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>Author: David Jurgens ; Keith Stevens</p><p>Abstract: We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</p><p>2 0.83239371 <a title="232-lsi-2" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>Author: Sebastian Rudolph ; Eugenie Giesbrecht</p><p>Abstract: We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.</p><p>3 0.64160323 <a title="232-lsi-3" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>Author: Joseph Turian ; Lev-Arie Ratinov ; Yoshua Bengio</p><p>Abstract: If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http ://metaoptimize com/proj ects/wordreprs/ .</p><p>4 0.61650556 <a title="232-lsi-4" href="./acl-2010-Online_Generation_of_Locality_Sensitive_Hash_Signatures.html">183 acl-2010-Online Generation of Locality Sensitive Hash Signatures</a></p>
<p>Author: Benjamin Van Durme ; Ashwin Lall</p><p>Abstract: Motivated by the recent interest in streaming algorithms for processing large text collections, we revisit the work of Ravichandran et al. (2005) on using the Locality Sensitive Hash (LSH) method of Charikar (2002) to enable fast, approximate comparisons of vector cosine similarity. For the common case of feature updates being additive over a data stream, we show that LSH signatures can be maintained online, without additional approximation error, and with lower memory requirements than when using the standard offline technique.</p><p>5 0.61425138 <a title="232-lsi-5" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>6 0.60652602 <a title="232-lsi-6" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>7 0.56884128 <a title="232-lsi-7" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>8 0.54160237 <a title="232-lsi-8" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>9 0.52873361 <a title="232-lsi-9" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>10 0.51727742 <a title="232-lsi-10" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>11 0.50989437 <a title="232-lsi-11" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<p>12 0.49593991 <a title="232-lsi-12" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>13 0.48070481 <a title="232-lsi-13" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>14 0.46047586 <a title="232-lsi-14" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>15 0.46010074 <a title="232-lsi-15" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>16 0.45505619 <a title="232-lsi-16" href="./acl-2010-Event-Based_Hyperspace_Analogue_to_Language_for_Query_Expansion.html">106 acl-2010-Event-Based Hyperspace Analogue to Language for Query Expansion</a></p>
<p>17 0.42586848 <a title="232-lsi-17" href="./acl-2010-Combining_Orthogonal_Monolingual_and_Multilingual_Sources_of_Evidence_for_All_Words_WSD.html">62 acl-2010-Combining Orthogonal Monolingual and Multilingual Sources of Evidence for All Words WSD</a></p>
<p>18 0.4162941 <a title="232-lsi-18" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>19 0.40986928 <a title="232-lsi-19" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>20 0.40850827 <a title="232-lsi-20" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.01), (25, 0.038), (42, 0.026), (44, 0.016), (45, 0.113), (59, 0.097), (65, 0.015), (71, 0.012), (73, 0.038), (78, 0.039), (83, 0.078), (84, 0.043), (98, 0.369)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96610576 <a title="232-lda-1" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>Author: David Jurgens ; Keith Stevens</p><p>Abstract: We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</p><p>2 0.95247459 <a title="232-lda-2" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>3 0.95160943 <a title="232-lda-3" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>Author: David Vickrey ; Oscar Kipersztok ; Daphne Koller</p><p>Abstract: We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</p><p>4 0.94970602 <a title="232-lda-4" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>Author: Xiangyu Duan ; Min Zhang ; Haizhou Li</p><p>Abstract: The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus. But word appears to be too fine-grained in some cases such as non-compositional phrasal equivalences, where no clear word alignments exist. Using words as inputs to PBSMT pipeline has inborn deficiency. This paper proposes pseudo-word as a new start point for PB-SMT pipeline. Pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation. By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in a monolingual way and a bilingual synchronous way. Experiments show that pseudo-word significantly outperforms word for PB-SMT model in both travel translation domain and news translation domain. 1</p><p>5 0.94771063 <a title="232-lda-5" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>6 0.94760674 <a title="232-lda-6" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>7 0.94449109 <a title="232-lda-7" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>8 0.94298244 <a title="232-lda-8" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>9 0.93950403 <a title="232-lda-9" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>10 0.93898249 <a title="232-lda-10" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>11 0.9310829 <a title="232-lda-11" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>12 0.9297713 <a title="232-lda-12" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>13 0.91640735 <a title="232-lda-13" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>14 0.91527909 <a title="232-lda-14" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>15 0.91426694 <a title="232-lda-15" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>16 0.90829378 <a title="232-lda-16" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>17 0.89926302 <a title="232-lda-17" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>18 0.89833379 <a title="232-lda-18" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>19 0.89721882 <a title="232-lda-19" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>20 0.89532983 <a title="232-lda-20" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
