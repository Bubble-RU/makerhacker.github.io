<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-118" href="#">acl2010-118</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</h1>
<br/><p>Source: <a title="acl-2010-118-pdf" href="http://aclweb.org/anthology//P/P10/P10-1034.pdf">pdf</a></p><p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>Reference: <a title="acl-2010-118-reference" href="../acl2010_reference/acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp s  t su j i  Abstract Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. [sent-5, score-0.512]
</p><p>2 In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. [sent-6, score-0.318]
</p><p>3 1 Introduction Tree-to-string translation rules are generic and applicable to numerous linguistically syntax-based Statistical Machine Translation (SMT) systems, such as string-to-tree translation (Galley et al. [sent-10, score-0.545]
</p><p>4 (2004; 2006) are frequently used for extracting minimal and composed rules from aligned 1-best tree-string pairs. [sent-18, score-0.304]
</p><p>5 Dealing with the parse error problem and rule sparseness problem, Mi and Huang (2008) replaced the 1-best parse tree with a packed forest which compactly encodes exponentially many parses for treeto-string rule extraction. [sent-19, score-0.817]
</p><p>6 As will be testified by our experiments, we argue that the simple POS/phrasal tags are too coarse to reflect the accurate translation probabilities of the translation rules. [sent-22, score-0.46]
</p><p>7 For example, as shown in Table 1, suppose a simple tree fragment “VBN(killed)” appears 6 times with “koroshita”, which is a Japanese translation of an active form of “killed”, and 4 times with “korosareta”, which is a Japanese translation of a passive form of “killed”. [sent-23, score-0.713]
</p><p>8 Now, by attaching the voice information to “killed”, we are gaining a rule set that is more appropriate to reflect the real translation situations. [sent-28, score-0.391]
</p><p>9 This motivates our proposal of using deep syntactic information to obtain a fine-grained translation rule set. [sent-29, score-0.472]
</p><p>10 We name the information such as the voice of a verb in a tree fragment as deep syntactic information. [sent-30, score-0.384]
</p><p>11 We use a head-driven phrase structure grammar (HPSG) parser to obtain the 1For example, “John has killed Mary. [sent-31, score-0.364]
</p><p>12 We extract fine-grained translation rules from aligned HPSG tree/forest-string pairs. [sent-36, score-0.366]
</p><p>13 We localize an HPSG tree/forest to make it segmentable at any nodes to fit the extraction algorithms described in (Galley et al. [sent-37, score-0.223]
</p><p>14 We also propose a linear-time algorithm for extracting composed rules guided by predicate-argument structures. [sent-39, score-0.213]
</p><p>15 The effectiveness of the rules are testified in our tree-to-string and string-to-tree systems, taking bidirectional Japanese-English translations as our test cases. [sent-40, score-0.254]
</p><p>16 In Section 2, we briefly review the tree-to-string and string-totree translation frameworks, tree-to-string rule extraction algorithms, and rich syntactic information previously used for SMT. [sent-42, score-0.437]
</p><p>17 The HPSG grammar and our proposal of fine-grained rule extraction algorithms are described in Section 3. [sent-43, score-0.235]
</p><p>18 Section 4 gives the experiments for applying fine-grained translation rules to large-scale Japanese-English translation tasks. [sent-44, score-0.512]
</p><p>19 1 Tree-to-string and string-to-tree translations Tree-to-string translation (Liu et al. [sent-47, score-0.197]
</p><p>20 , 2006) first uses a parser to parse a source sentence into a 1-best tree and then searches for the best derivation that segments and converts the tree into a target string. [sent-49, score-0.343]
</p><p>21 That is, giving a (bilingual) translation grammar and a source sentence, we are trying to construct a parse forest in the target language. [sent-54, score-0.446]
</p><p>22 Consequently, the translation results can be collected from the leaves of the parse forest. [sent-55, score-0.246]
</p><p>23 The English sentence is “John killed Mary” and the Japanese sentence is “jyon ha mari wo koroshita”, in which the function words “ha” and “wo” are not aligned with any English word. [sent-57, score-0.381]
</p><p>24 f1J is a sentence of a foreign language other than English, Et is a 1-best parse tree of an English sentence E = e1I, and A = {(j, i)} is an alignment between the wo, ardnsd di nA AF = =an {d( jE,i. [sent-61, score-0.242]
</p><p>25 The basic idea of GHKM algorithm is to decompose Et into a series of tree fragments, each of which will form a rule with its corresponding translation in the foreign language. [sent-62, score-0.544]
</p><p>26 A is used as a constraint to guide the segmentation procedure, so that the root node of every tree fragment of Et exactly corresponds to a contiguous span on the foreign language side. [sent-63, score-0.46]
</p><p>27 Based on this consideration, a frontier set (fs) is defined to be a set of nodes n in Et that satisfies the following constraint: fs = {n|span(n) ∩ comp span(n) = ϕ}. [sent-64, score-0.229]
</p><p>28 With fs computed, rules are extracted through a depthfirst traversal of Et: we cut Et at all nodes in fs to form tree fragments and extract a rule for each fragment. [sent-78, score-0.762]
</p><p>29 These extracted rules are called minimal rules (Galley et al. [sent-79, score-0.276]
</p><p>30 For example, the 1best tree (with gray nodes) in Figure 2 is cut into 7 pieces, each of which corresponds to the tree fragment in a rule (bottom-left corner of the figure). [sent-81, score-0.582]
</p><p>31 In order to include richer context information and account for multiple interpretations of unaligned words of foreign language, minimal rules which share adjacent tree fragments are connected together to form composed rules (Galley et al. [sent-82, score-0.595]
</p><p>32 (2006) constructed a derivation-forest, in which composed rules were generated, unaligned words of foreign language were consistently attached, and the translation probabilities  of rules were estimated by using ExpectationMaximization (EM) (Dempster et al. [sent-85, score-0.562]
</p><p>33 For example, by combining the minimal rules of 1, 4, and 5, we obtain a composed rule, as shown in the bottom-right corner of Figure 2. [sent-87, score-0.267]
</p><p>34 Considering the parse error problem in the 1-best or k-best parse trees, Mi and Huang (2008) extracted tree-to-string translation rules from aligned packed forest-string pairs. [sent-88, score-0.577]
</p><p>35 A forest compactly encodes exponentially many trees 327  rather than the 1-best tree used by Galley et al. [sent-89, score-0.298]
</p><p>36 Two problems were managed to be tackled during extracting rules from an aligned forest-string pair: where to cut and how to cut. [sent-91, score-0.228]
</p><p>37 Equation 1 was used again to compute a frontier node set to determine where to cut the packed forest into a number of tree-fragments. [sent-92, score-0.427]
</p><p>38 The difference with tree-based rule extraction is that the nodes in a packed forest (which is a hypergraph) now are hypernodes, which can take a set of incoming hyperedges. [sent-93, score-0.541]
</p><p>39 Then, by limiting each frag-  ment to be a tree and whose root/leaf hypernodes all appearing in the frontier set, the packed forest can be segmented properly into a set of tree fragments, each of which can be used to generate a tree-to-string translation rule. [sent-94, score-0.806]
</p><p>40 3  Rich syntactic information for SMT  Before describing our approaches of applying deep syntactic information yielded by an HPSG parser for fine-grained rule extraction, we would like to briefly review what kinds of deep syntactic information have been employed for SMT. [sent-96, score-0.461]
</p><p>41 (2007) also reported a significant improvement for Dutch-English translation by applying  CCG supertags at a word level to a factorized SMT system (Koehn et al. [sent-104, score-0.279]
</p><p>42 The major differences are that, we use a larger feature set (Table 2) including the supertags for fine-grained tree-to-string rule extraction, rather than string-to-string translation (Hassan et al. [sent-115, score-0.433]
</p><p>43 In contrast, the fine-grained tree-to-string translation rule extraction approaches in this paper are totally data-driven, and easily applicable to numerous language pairs by taking English as the source or target language. [sent-121, score-0.431]
</p><p>44 3  Fine-grained rule extraction  We now introduce the deep syntactic information generated by an HPSG parser and then describe our approaches for fine-grained tree-tostring rule extraction. [sent-122, score-0.476]
</p><p>45 Also, we propose a linear-time composed rule extraction algorithm by making use of predicate-argument structures. [sent-125, score-0.269]
</p><p>46 First, we can carefully control the condition of the application of a translation rule by exploiting the fine-grained syntactic 2http://www. [sent-136, score-0.39]
</p><p>47 description in the English parse tree/forest, as well as those in the translation rules. [sent-142, score-0.246]
</p><p>48 We expect that extraction of translation rules based on such semantically-connected subtrees will give a compact and effective set of translation rules. [sent-144, score-0.559]
</p><p>49 html  kilA R G 21 JMoahrny ignAoRrAeGRG21factShewARAanGRAtG2R2G1dIAisRpGut1e Figure 3: Predicate argument structures for the sentences of “John killed Mary” and “She ignored the fact that I wanted to dispute”. [sent-162, score-0.33]
</p><p>50 Figure 3 shows the PAS of the example sentence in Figure 2, “John killed Mary”, and a more complex PAS for another sentence, “She ignored the fact that I wanted to dispute”, which is adopted from (Miyao et al. [sent-165, score-0.33]
</p><p>51 2  Localize HPSG forest  Our fine-grained translation rule extraction algorithm is sketched in Algorithm 1. [sent-170, score-0.564]
</p><p>52 Considering that a parse tree is a trivial packed forest, we only use the term forest to expand our discussion, hereafter. [sent-171, score-0.46]
</p><p>53 However, the three nodes are not included in one (minimal) translation rule. [sent-175, score-0.258]
</p><p>54 , 2006) 6: else if Ef is an HPSG forest then ′′  ′  7: 8:  Ef′ = localize Forest(Ef); R2 = forest based rule extraction(E′f, F, A) ◃ AlgorRithm 1in (Mi and Huang, 2008) 9: end if  the identifier of the daughter node as the values. [sent-183, score-0.782]
</p><p>55 A pure syntactic-based HPSG forest without any pointer-valued features can be yielded through this  pre-processing for the consequent execution of the extraction algorithms (Galley et al. [sent-186, score-0.213]
</p><p>56 3 Predicate-argument structures In order to extract translation rules from PASs, we want to localize a predicate word and its arguments into one tree fragment. [sent-189, score-0.605]
</p><p>57 For example, in Figure 2, we can use a tree fragment which takes c0 as its root node and c1, t1, and c5 on its yield (= leaf nodes of a tree fragment) to cover “killed” and its subject and direct object arguments. [sent-190, score-0.583]
</p><p>58 We define this kind of tree fragment to be a minimum covering tree. [sent-191, score-0.349]
</p><p>59 For example, the minimum covering tree of {t1, c1, c5} is shown in the bottom-right corner o {ft Figure 2}. [sent-192, score-0.299]
</p><p>60 Tsh seh dwefnin iinti tohne supplies us a linear-time algorithm to directly find the tree fragment that covers a PAS during both rule extracting and rule matching when decoding an HPSG tree. [sent-193, score-0.639]
</p><p>61 Algorithm 2 PASR extraction Input: HPSG tree Et, foreign sentence F, and alignment A Output: a PAS-based rule set R 1: R = {}  12:: fRor = =n {od}e n ∈ Leaves(Et) do 23:: i fn Oodpeen n( ∈n. [sent-194, score-0.394]
</p><p>62 ARGs) 5: if root and leaf nodes of Tc are in fs then 6: generate a rule r using fragment Tc 7: R. [sent-196, score-0.476]
</p><p>63 Taking a minimum covering tree as the tree fragment, we can easily build a tree-to-string translation rule that reflects the semantic dependency of a PAS. [sent-199, score-0.741]
</p><p>64 The algorithm of PAS-based rule (PASR) extraction is sketched in Algorithm 2. [sent-200, score-0.201]
</p><p>65 We extract PAS-based rules through one-time traversal of the leaf nodes in Et (line 2). [sent-204, score-0.275]
</p><p>66 For each leaf node n, we extract a minimum covering tree Tc if n contains at least one argument. [sent-205, score-0.39]
</p><p>67 Based on Tc, we can easily build a tree-to-string translation rule by further completing the right-hand-side string by sorting the spans of Tc’s leaf nodes, lexicalizing the terminal node’s span(s), and assigning a variable to each non-terminal node’s span. [sent-208, score-0.432]
</p><p>68 Maximum likelihood estimation is used to calculate the translation probabilities of each rule. [sent-209, score-0.197]
</p><p>69 Both models use a phrase translation table (PTT), an HPSG tree-based rule set (TRS), and a PAS-based rule set (PRS). [sent-214, score-0.505]
</p><p>70 Since the three rule  sets are independently extracted and estimated, we 330  use Minimum Error Rate Training (MERT) (Och, 2003) to tune the weights of the features from the three rule sets on the development set. [sent-215, score-0.308]
</p><p>71 ∏r∈d  This equation reflects that the translation rules in one d come from three sets. [sent-218, score-0.315]
</p><p>72 , 2009b), it is appealing to combine these rule sets together in one decoder because PTT provides excellent rule coverages while TRS and PRS offer linguistically motivated phrase selections and nonlocal reorderings. [sent-220, score-0.337]
</p><p>73 , 2006) and inversely binarize all translation rules into Chomsky Normal Forms that contain at most two variables and can be incrementally scored by LM. [sent-225, score-0.315]
</p><p>74 The string-to-tree decoder searches for the optimal derivation d∗ that parses a Japanese string F into a packed forest of the set of all possible derivations D: d∗ =argmax{λ1logpLM(τ(d)) + λ3g(d) + log s(d|F)}. [sent-229, score-0.338]
</p><p>75 2  Decoding algorithms  In our translation models, we have made use of three kinds of translation rule sets which are trained separately. [sent-233, score-0.574]
</p><p>76 , 2009b) for mixing different types of translation rules within one derivation. [sent-235, score-0.315]
</p><p>77 Recall the definition of minimum covering tree, which supports a faster way to retrieve available rules from PRS without generating all the subtrees. [sent-239, score-0.244]
</p><p>78 That is, when node n fortunately to be the root of some minimum covering tree(s), we use the tree(s) to seek available PAS-based rules in PRS. [sent-240, score-0.358]
</p><p>79 For example, suppose we are  decoding an HPSG tree (with gray nodes) shown in Figure 2. [sent-244, score-0.213]
</p><p>80 At t1, we can extract its minimum covering tree with the root node to be c0, then take this tree fragment as the key to retrieve PRS, and consequently put c0 and the available rules in the hash-table. [sent-245, score-0.713]
</p><p>81 We modified this parser to output a packed forest for each English sentence. [sent-270, score-0.279]
</p><p>82 , 2007) on the training set to obtain a phrase-  aligned parallel corpus, from which bidirectional phrase translation tables were estimated. [sent-272, score-0.318]
</p><p>83 We evaluated the translation quality using the case-insensitive BLEU-4 metric (Papineni et al. [sent-274, score-0.197]
</p><p>84 3M translation rules from the training set for the 4K English and Japanese sentences in the development and test sets. [sent-281, score-0.315]
</p><p>85 The corpus can be conditionally obtained from NTCIR-7 patent translation workshop homepage: http://research. [sent-286, score-0.197]
</p><p>86 200 for English-to-Japanese translation and 500 for Japanese-to-English translation. [sent-309, score-0.197]
</p><p>87 Table 5 reports the BLEU-4 scores achieved by decoding the test set making use of Joshua and our systems (t2s = tree-to-string and s2t = string-totree) under numerous rule sets. [sent-314, score-0.268]
</p><p>88 We take C3S and FS as approximations of CFG-based translation rules. [sent-317, score-0.197]
</p><p>89 The decoding time (seconds per sentence) of tree-to-string translation is listed as well. [sent-352, score-0.278]
</p><p>90 Furthermore, in Table 5, the decoding time (sec-  onds per sentence) of tree-to-string translation by using PTT+PRS is more than 86 times faster than using the other tree-to-string rule sets. [sent-356, score-0.432]
</p><p>91 This suggests that the direct generation of minimum covering trees for rule matching is extremely faster than generating all subtrees of a tree node. [sent-357, score-0.412]
</p><p>92 5  Conclusion  We have proposed approaches of using deep syntactic information for extracting fine-grained treeto-string translation rules from aligned HPSG forest-string pairs. [sent-368, score-0.514]
</p><p>93 , 2006; Mi and Huang, 2008) to HPSG forests and a linear-time algorithm for extracting composed rules from predicate-argument structures. [sent-370, score-0.213]
</p><p>94 We applied our fine-grained translation rules to a tree-to-string system and an Hiero-style string-totree system. [sent-371, score-0.315]
</p><p>95 We argue the fine-grained translation rules are generic and applicable to many syntax-based SMT frameworks such as the forest-to-string model (Mi et al. [sent-373, score-0.315]
</p><p>96 Furthermore, it will be interesting to extract fine-grained tree-to-tree translation rules  by integrating deep syntactic information in the source and/or target language side(s). [sent-375, score-0.436]
</p><p>97 These treeto-tree rules are applicable for forest-to-tree translation models (Liu et al. [sent-376, score-0.315]
</p><p>98 Scalable inference and training of context-rich syntactic translation models. [sent-416, score-0.236]
</p><p>99 Towards hybrid quality-oriented machine translation - on linguistics and probabilities in mt. [sent-479, score-0.197]
</p><p>100 Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. [sent-508, score-0.255]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hpsg', 0.403), ('ptt', 0.394), ('killed', 0.33), ('prs', 0.244), ('translation', 0.197), ('forest', 0.166), ('rule', 0.154), ('galley', 0.147), ('tree', 0.132), ('vbn', 0.132), ('rules', 0.118), ('localize', 0.115), ('packed', 0.113), ('huang', 0.096), ('ef', 0.094), ('fragment', 0.091), ('mi', 0.09), ('koroshita', 0.082), ('deep', 0.082), ('fs', 0.082), ('supertags', 0.082), ('decoding', 0.081), ('node', 0.079), ('trs', 0.072), ('xm', 0.07), ('daughter', 0.07), ('bidirectional', 0.07), ('covering', 0.068), ('composed', 0.068), ('lexentry', 0.066), ('testified', 0.066), ('tfs', 0.066), ('tfss', 0.066), ('tc', 0.064), ('span', 0.062), ('sem', 0.062), ('nodes', 0.061), ('foreign', 0.061), ('passive', 0.06), ('arg', 0.059), ('minimum', 0.058), ('pas', 0.058), ('fragments', 0.058), ('pointer', 0.058), ('japanese', 0.057), ('leaf', 0.053), ('miyao', 0.051), ('aligned', 0.051), ('comp', 0.049), ('korosareta', 0.049), ('pasr', 0.049), ('liu', 0.049), ('parse', 0.049), ('extraction', 0.047), ('head', 0.045), ('ghkm', 0.043), ('predicate', 0.043), ('traversal', 0.043), ('joshua', 0.041), ('corner', 0.041), ('voice', 0.04), ('minimal', 0.04), ('chiang', 0.04), ('syntactic', 0.039), ('np', 0.038), ('frontier', 0.037), ('smt', 0.037), ('signs', 0.037), ('mary', 0.036), ('active', 0.036), ('root', 0.035), ('glue', 0.035), ('matsuzaki', 0.035), ('grammar', 0.034), ('haitao', 0.033), ('numerous', 0.033), ('qun', 0.033), ('dispute', 0.033), ('enju', 0.033), ('jst', 0.033), ('mext', 0.033), ('xianchao', 0.033), ('identifier', 0.032), ('cut', 0.032), ('hypergraph', 0.031), ('hassan', 0.03), ('pred', 0.03), ('yusuke', 0.03), ('derivation', 0.03), ('birch', 0.029), ('decoder', 0.029), ('hypernodes', 0.029), ('phrasal', 0.028), ('terminal', 0.028), ('liang', 0.028), ('kevin', 0.027), ('ccg', 0.027), ('extracting', 0.027), ('appending', 0.026), ('kinds', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="118-tfidf-1" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>2 0.28456387 <a title="118-tfidf-2" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>3 0.26038891 <a title="118-tfidf-3" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>Author: Yang Liu ; Liang Huang</p><p>Abstract: unkown-abstract</p><p>4 0.19871958 <a title="118-tfidf-4" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>5 0.18360817 <a title="118-tfidf-5" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>6 0.16933836 <a title="118-tfidf-6" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>7 0.16214965 <a title="118-tfidf-7" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>8 0.15696515 <a title="118-tfidf-8" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>9 0.15116253 <a title="118-tfidf-9" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>10 0.1505944 <a title="118-tfidf-10" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>11 0.13824473 <a title="118-tfidf-11" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>12 0.13179165 <a title="118-tfidf-12" href="./acl-2010-Wide-Coverage_NLP_with_Linguistically_Expressive_Grammars.html">260 acl-2010-Wide-Coverage NLP with Linguistically Expressive Grammars</a></p>
<p>13 0.11794844 <a title="118-tfidf-13" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>14 0.11652544 <a title="118-tfidf-14" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>15 0.11155129 <a title="118-tfidf-15" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>16 0.10886765 <a title="118-tfidf-16" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>17 0.10633658 <a title="118-tfidf-17" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>18 0.10411645 <a title="118-tfidf-18" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>19 0.09919408 <a title="118-tfidf-19" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>20 0.097855322 <a title="118-tfidf-20" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.254), (1, -0.253), (2, 0.078), (3, 0.023), (4, -0.118), (5, -0.04), (6, 0.12), (7, 0.038), (8, -0.194), (9, -0.063), (10, 0.149), (11, -0.05), (12, 0.061), (13, -0.041), (14, 0.04), (15, 0.074), (16, -0.013), (17, 0.007), (18, -0.098), (19, 0.038), (20, 0.085), (21, 0.007), (22, -0.092), (23, -0.045), (24, -0.086), (25, 0.112), (26, 0.057), (27, -0.018), (28, 0.102), (29, -0.04), (30, -0.053), (31, -0.076), (32, -0.04), (33, 0.002), (34, -0.021), (35, -0.103), (36, 0.023), (37, 0.109), (38, 0.048), (39, 0.011), (40, -0.024), (41, 0.019), (42, 0.027), (43, -0.094), (44, 0.037), (45, 0.003), (46, -0.013), (47, 0.023), (48, 0.084), (49, -0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94677675 <a title="118-lsi-1" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>2 0.89530718 <a title="118-lsi-2" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>Author: Yang Liu ; Liang Huang</p><p>Abstract: unkown-abstract</p><p>3 0.82150972 <a title="118-lsi-3" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>4 0.77089316 <a title="118-lsi-4" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>Author: Chris Dyer ; Adam Lopez ; Juri Ganitkevitch ; Jonathan Weese ; Ferhan Ture ; Phil Blunsom ; Hendra Setiawan ; Vladimir Eidelman ; Philip Resnik</p><p>Abstract: Adam Lopez University of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</p><p>5 0.67931342 <a title="118-lsi-5" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>6 0.66673493 <a title="118-lsi-6" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>7 0.61970407 <a title="118-lsi-7" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>8 0.57403195 <a title="118-lsi-8" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>9 0.56304467 <a title="118-lsi-9" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>10 0.55581641 <a title="118-lsi-10" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>11 0.5469169 <a title="118-lsi-11" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>12 0.49398321 <a title="118-lsi-12" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>13 0.47549042 <a title="118-lsi-13" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>14 0.45048106 <a title="118-lsi-14" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>15 0.44360843 <a title="118-lsi-15" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>16 0.42402959 <a title="118-lsi-16" href="./acl-2010-SystemT%3A_An_Algebraic_Approach_to_Declarative_Information_Extraction.html">222 acl-2010-SystemT: An Algebraic Approach to Declarative Information Extraction</a></p>
<p>17 0.41544586 <a title="118-lsi-17" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>18 0.41499674 <a title="118-lsi-18" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>19 0.40959579 <a title="118-lsi-19" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>20 0.40571705 <a title="118-lsi-20" href="./acl-2010-Tools_for_Multilingual_Grammar-Based_Translation_on_the_Web.html">235 acl-2010-Tools for Multilingual Grammar-Based Translation on the Web</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.012), (16, 0.018), (25, 0.139), (42, 0.013), (44, 0.022), (59, 0.102), (73, 0.35), (76, 0.01), (78, 0.045), (83, 0.045), (84, 0.024), (98, 0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95325339 <a title="118-lda-1" href="./acl-2010-WebLicht%3A_Web-Based_LRT_Services_for_German.html">259 acl-2010-WebLicht: Web-Based LRT Services for German</a></p>
<p>Author: Erhard Hinrichs ; Marie Hinrichs ; Thomas Zastrow</p><p>Abstract: This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a webbased service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project1. WebLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services. WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.) Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service. 1</p><p>same-paper 2 0.90215939 <a title="118-lda-2" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>3 0.90013862 <a title="118-lda-3" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<p>Author: Sindhu Raghavan ; Adriana Kovashka ; Raymond Mooney</p><p>Abstract: In this paper, we present a novel approach for authorship attribution, the task of identifying the author of a document, using probabilistic context-free grammars. Our approach involves building a probabilistic context-free grammar for each author and using this grammar as a language model for classification. We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy.</p><p>4 0.89241594 <a title="118-lda-4" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>Author: Nikolaos Trogkanis ; Charles Elkan</p><p>Abstract: Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random fields. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages.</p><p>5 0.8880713 <a title="118-lda-5" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>Author: Jesus Gonzalez Rubio ; Daniel Ortiz Martinez ; Francisco Casacuberta</p><p>Abstract: This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.</p><p>6 0.87961322 <a title="118-lda-6" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>7 0.87506121 <a title="118-lda-7" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>8 0.71621305 <a title="118-lda-8" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>9 0.66749001 <a title="118-lda-9" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>10 0.66250229 <a title="118-lda-10" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>11 0.65300405 <a title="118-lda-11" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>12 0.64889264 <a title="118-lda-12" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>13 0.64858603 <a title="118-lda-13" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>14 0.64427376 <a title="118-lda-14" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>15 0.6365127 <a title="118-lda-15" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>16 0.63395679 <a title="118-lda-16" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>17 0.63310301 <a title="118-lda-17" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>18 0.62913227 <a title="118-lda-18" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>19 0.62659907 <a title="118-lda-19" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>20 0.62588102 <a title="118-lda-20" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
