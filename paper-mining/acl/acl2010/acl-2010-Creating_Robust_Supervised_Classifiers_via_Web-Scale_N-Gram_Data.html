<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-76" href="#">acl2010-76</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</h1>
<br/><p>Source: <a title="acl-2010-76-pdf" href="http://aclweb.org/anthology//P/P10/P10-1089.pdf">pdf</a></p><p>Author: Shane Bergsma ; Emily Pitler ; Dekang Lin</p><p>Abstract: In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.</p><p>Reference: <a title="acl-2010-76-reference" href="../acl2010_reference/acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. [sent-7, score-0.424]
</p><p>2 We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. [sent-8, score-0.669]
</p><p>3 More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance. [sent-9, score-0.216]
</p><p>4 1 Introduction  Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al. [sent-10, score-0.124]
</p><p>5 They show web counts are superior to counts from a large corpus. [sent-13, score-0.338]
</p><p>6 (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). [sent-15, score-0.244]
</p><p>7 Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised approaches? [sent-18, score-0.27]
</p><p>8 While previous work has combined web-scale features with other features in specific classification problems (Modjeska et al. [sent-22, score-0.178]
</p><p>9 For example, for the task of prenominal adjective ordering (Section 3), a system that needs to describe a ball that is both big and red can simply check that big red is more common on the web than red big, and order the adjectives accordingly. [sent-27, score-0.779]
</p><p>10 For example, ordering adjectives by direct web evidence performs 7% worse than our best supervised system (Section 3. [sent-29, score-0.424]
</p><p>11 For example,  there are currently no pages indexed by Google with the preferred adjective ordering for bedraggled 56-year-old [professor]. [sent-32, score-0.426]
</p><p>12 Systems trained on labeled data can learn the domain usage and leverage other regularities, such as suffixes and transitivity for adjective ordering. [sent-34, score-0.378]
</p><p>13 How well do supervised and unsupervised NLP systems perform when used uncustomized, out-of-the-box on new domains, and how can we best design our systems for robust open-domain performance? [sent-45, score-0.125]
</p><p>14 For our supervised approaches, we represent the examples as feature vectors, and learn a classifier on the training vectors. [sent-56, score-0.228]
</p><p>15 N-GM features are real-valued features giving the log-count of a particular N-gram in the auxiliary web corpus. [sent-58, score-0.309]
</p><p>16 LEX features are binary features that indicate the presence or absence of a particular string at a given position in the input. [sent-59, score-0.178]
</p><p>17 We plot learning curves to measure the accuracy of the classifier when the num-  ber of labeled training examples varies. [sent-65, score-0.222]
</p><p>18 The size of the N-gram data and its counts remain constant. [sent-66, score-0.124]
</p><p>19 2  Tasks and Labeled Data  We study two generation tasks: prenominal adjective ordering (Section 3) and context-sensitive spelling correction (Section 4), followed by two analysis tasks: noun compound bracketing (Section 5) and verb part-of-speech disambiguation (Section 6). [sent-70, score-1.107]
</p><p>20 We describe how labeled adjective and spelling examples are created from these corpora in the corresponding sections. [sent-76, score-0.448]
</p><p>21 The third enhancement is especially relevant here, as we can use the POS distribution to collect counts for N-grams of mixed words and tags. [sent-95, score-0.124]
</p><p>22 For example, we have developed an N-gram search engine that can count how often the adjective unprecedented precedes another adjective in our web corpus (113K times) and how often it follows one (11K times). [sent-96, score-0.624]
</p><p>23 Thus, even if we haven’t seen a particular adjective pair directly, we can use the positional preferences of each adjective to order them. [sent-97, score-0.454]
</p><p>24 3  Prenominal Adjective Ordering  Prenominal adjective ordering strongly affects text readability. [sent-100, score-0.426]
</p><p>25 For example, while the unprecedented statistical revolution is fluent, the statistical unprecedented revolution is not. [sent-101, score-0.168]
</p><p>26 Many NLP systems need to handle adjective ordering robustly. [sent-102, score-0.426]
</p><p>27 In machine translation, if a noun has two adjective modifiers, they must be ordered correctly in the target language. [sent-103, score-0.361]
</p><p>28 Adjective ordering is also needed in Natural Language Generation systems that produce information from databases; for example, to convey information (in sentences) about medical patients (Shaw and Hatzivassiloglou, 1999). [sent-104, score-0.199]
</p><p>29 We focus on the task of ordering a pair of adjectives independently of the noun they modify and achieve good performance in this setting. [sent-105, score-0.382]
</p><p>30 Following the set-up of Malouf (2000), we experiment on the 263K adjective pairs Malouf extracted from the British National Corpus (BNC). [sent-106, score-0.258]
</p><p>31 We create examples from all sequences of two adjectives followed by a noun. [sent-111, score-0.182]
</p><p>32 Since the N-gram data includes case, we merge counts from the upper and lower case combinations. [sent-117, score-0.124]
</p><p>33 1 LEX features Our adjective ordering model with LEX features is a novel contribution of this paper. [sent-124, score-0.604]
</p><p>34 We begin with two features for each pair: an indicator feature for a1, which gets a feature value of +1, and an indicator feature for a2, which gets a feature value of −1. [sent-125, score-0.155]
</p><p>35 If the alphabetic ordering is correct, the weight on a1 should be higher than the weight on a2, so that the classifier returns a positive score. [sent-128, score-0.251]
</p><p>36 If the reverse ordering is preferred, a2 should receive a higher weight. [sent-129, score-0.199]
</p><p>37 Training the model in this setting is a matter of assigning weights to all the observed adjectives such that the training pairs are maximally ordered correctly. [sent-130, score-0.181]
</p><p>38 The feature weights thus implicitly produce a linear ordering of all observed adjectives. [sent-131, score-0.199]
</p><p>39 wn to improve adjective ordering, there are many  conflicting pairs that make a strict linear ordering of adjectives impossible (Malouf, 2000). [sent-135, score-0.535]
</p><p>40 Finally, we also have features for all suffixes of length 1-to-4 letters, as these encode useful information about adjective class (Malouf, 2000). [sent-138, score-0.316]
</p><p>41 Like the adjective features, the suffix features receive a value of +1 for adjectives in the first position and −1 for those in the second. [sent-139, score-0.394]
</p><p>42 2 N-GM features Lapata and Keller (2005) propose a web-based approach to adjective ordering: take the most867  TSwaMbyVealsMbtoe1cmuw(: faiA1(t2,hd0ajLN2e0)Ec-XGtviMsef. [sent-142, score-0.316]
</p><p>43 We merge the counts for the adjectives occurring contiguously and separated by a comma. [sent-150, score-0.202]
</p><p>44 These are indubitably the most important N-GM features; we include them but also other, tag-based counts from Google V2. [sent-151, score-0.124]
</p><p>45 Raw counts include cases where one of the adjectives is not used as a modifier: “the special present was” vs. [sent-152, score-0.202]
</p><p>46 We also include features for the log-counts of each adjective preceded or followed by a word matching an adjective-tag: c(a1 J. [sent-159, score-0.344]
</p><p>47 The more frequent adjective occurs first 57% of the time. [sent-166, score-0.227]
</p><p>48 As in all tasks, the counts are features in a classifier, so the importance of the different patterns is weighted discriminatively during training. [sent-167, score-0.246]
</p><p>49 With fewer training examples, the systems with N-GM features strongly outperform the LEX-only system. [sent-178, score-0.132]
</p><p>50 Number of training examples Figure 1: In-domain learning curve of adjective ordering classifiers on BNC. [sent-180, score-0.658]
</p><p>51 Number of training examples Figure 2: Out-of-domain learning curve of adjective ordering classifiers on Gutenberg. [sent-181, score-0.658]
</p><p>52 While other ordering models have also achieved “very poor results” out-of-domain (Mitchell, 2009), we expected our expanded set of LEX features to provide good generalization on new data. [sent-187, score-0.288]
</p><p>53 N-GM features do not rely on specific pairs in training data, and thus remain fairly robust crossdomain. [sent-189, score-0.196]
</p><p>54 Across the three test sets, 84-89% of examples had the correct ordering appear at least once on the web. [sent-190, score-0.275]
</p><p>55 The system disregards the robust N-gram counts as it is more and more confident in the LEX features, and it suffers the consequences. [sent-199, score-0.157]
</p><p>56 4  Context-Sensitive Spelling Correction  We now turn to the generation problem of contextsensitive spelling correction. [sent-200, score-0.122]
</p><p>57 There are 100K training, 10K development, and 10K test examples for each confusion set. [sent-207, score-0.155]
</p><p>58 3) The baseline predicts the most frequent member of each confusion set, based on frequencies in the NYT training data. [sent-217, score-0.122]
</p><p>59 Number of training examples Figure 3: In-domain learning curve of spelling correction classifiers on NYT. [sent-219, score-0.39]
</p><p>60 1 Supervised Spelling Correction Our LEX features are typical disambiguation features that flag specific aspects of the context. [sent-221, score-0.216]
</p><p>61 We have features for the words at all positions in a 9-word window (called collocation features by Golding and Roth (1999)), plus indicators for a particular word preceding or following the confusable word. [sent-222, score-0.247]
</p><p>62 We include the log-counts of all N-grams that span the confusable word, with each word in the confusion set filling the N-gram pattern. [sent-226, score-0.148]
</p><p>63 (2009), we get N-gram counts using the original Google N-gram Corpus. [sent-229, score-0.124]
</p><p>64 Note the learning curves for N-GM+LEX on Gutenberg and Medline (not shown) do not display the decrease that we observed in adjective ordering (Figure 2). [sent-244, score-0.426]
</p><p>65 5  Noun Compound Bracketing  About 70% of web queries are noun phrases (Barr et al. [sent-249, score-0.195]
</p><p>66 For example, a web query for zebra hair straightener should be bracketed as (zebra (hair straightener)), a stylish hair straightener with zebra print, rather than ((zebra hair) straightener), a useless product since the fur of zebras is already quite straight. [sent-251, score-0.532]
</p><p>67 The noun compound (NC) bracketing task is usually cast as a decision whether a 3-word NC has a left or right bracketing. [sent-252, score-0.271]
</p><p>68 Number of labeled examples Figure 4: In-domain NC-bracketer learning curve from sections 0-22 of the Treebank as training, 72 from section 24 for development and 95 from section 23 as a test set. [sent-263, score-0.194]
</p><p>69 1 Supervised Noun Bracketing Our LEX features indicate the specific noun at each position in the compound, plus the three pairs of nouns and the full noun triple. [sent-267, score-0.33]
</p><p>70 Following Nakov and Hearst (2005), we also include counts of noun pairs collapsed into a single token; if a pair occurs often on the web as a single unit, it strongly indicates the pair is a constituent. [sent-271, score-0.35]
</p><p>71 The absence of a sufficient amount of labeled data explains why NC-bracketing is generally regarded as a task where corpus counts are crucial. [sent-285, score-0.203]
</p><p>72 With little training data and crossdomain usage, N-gram features are essential. [sent-290, score-0.171]
</p><p>73 For example, in the troops stationed in Iraq, the verb stationed is a VBN; troops is the head of the phrase. [sent-293, score-0.389]
</p><p>74 On the other hand, for the troops vacationed in Iraq, the verb vacationed is a VBD and also the head. [sent-294, score-0.285]
</p><p>75 , the global lexical relation between the noun and verb (E. [sent-300, score-0.168]
</p><p>76 , troops tends to be the object of stationed but the subject of vacationed). [sent-302, score-0.163]
</p><p>77 For out-of-domain data, we get 21K  6HMM-style taggers, like the fast TnT tagger used on our web corpus, do not use bilexical features, and so perform especially poorly on these cases. [sent-306, score-0.138]
</p><p>78 examples from the Brown portion of the Treebank and 6296 examples from tagged Medline abstracts in the PennBioIE corpus (Kulick et al. [sent-308, score-0.18]
</p><p>79 1 LEX features For 1), we use indicators for the noun and verb, the noun-verb pair, whether the verb is on an inhouse list of said-verb (like warned, announced, etc. [sent-316, score-0.257]
</p><p>80 ), whether the noun is capitalized and whether it’s upper-case. [sent-317, score-0.133]
</p><p>81 For 2), we provide indicator features for the words before the noun and after the verb. [sent-321, score-0.227]
</p><p>82 2 N-GM features For 1), we characterize a noun-verb relation via features for the pair’s distribution in Google V2. [sent-324, score-0.178]
</p><p>83 We extract the 20 most-frequent N-grams that contain both the noun and the verb in the pair. [sent-326, score-0.168]
</p><p>84 We mask the noun of interest as N and the verb of interest as V. [sent-328, score-0.168]
</p><p>85 For 2), we use counts for the verb’s context cooccurring with a VBD or VBN tag. [sent-333, score-0.124]
</p><p>86 , we see whether VBD cases like troops ate or VBN cases like troops eaten are more frequent. [sent-336, score-0.277]
</p><p>87 Although our corpus contains many VBN/VBD errors, we hope the errors are random enough for aggregate counts to be useful. [sent-337, score-0.152]
</p><p>88 Number of training examples Figure 5: Out-of-domain learning curve of verb disambiguation classifiers on Medline. [sent-344, score-0.333]
</p><p>89 We include separate count features for contexts matching the specific noun and for when the noun token can match any word tagged as a noun. [sent-346, score-0.299]
</p><p>90 ContextSum: We use these context counts in an unsupervised system, ContextSum. [sent-347, score-0.159]
</p><p>91 With two views of an example, LEX is more likely to have domain-neutral features to draw on. [sent-363, score-0.119]
</p><p>92 Also, the Treebank provides an atypical num-  ber of labeled examples for analysis tasks. [sent-365, score-0.127]
</p><p>93 In other tasks we only had a handful of N-GM features; here there are 21K features for the distributional patterns of N,V pairs. [sent-371, score-0.153]
</p><p>94 Counts from any large auxiliary corpus may also help, but web counts should help more (Lapata and Keller, 2005). [sent-377, score-0.283]
</p><p>95 Our results suggest better features, such as web pattern counts, may help more than expanding training data. [sent-382, score-0.133]
</p><p>96 In some sense, using web counts as features is a form of domain adaptation: adapting a web model to the training domain. [sent-384, score-0.473]
</p><p>97 How do we ensure these features are adapted well and not used in  domain-specific ways (especially with many features to adapt, as in Section 6)? [sent-385, score-0.178]
</p><p>98 When less training data is used, or when the system is used on a  different domain, N-gram features greatly improve performance. [sent-395, score-0.132]
</p><p>99 The order of prenominal adjectives in natural language generation. [sent-496, score-0.173]
</p><p>100 Search engine statistics beyond the n-gram: Application to noun compound bracketing. [sent-523, score-0.196]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lex', 0.514), ('gutenberg', 0.317), ('medline', 0.312), ('adjective', 0.227), ('ordering', 0.199), ('malouf', 0.14), ('bergsma', 0.134), ('counts', 0.124), ('noun', 0.105), ('troops', 0.104), ('vbd', 0.099), ('prenominal', 0.095), ('vbn', 0.095), ('spelling', 0.094), ('compound', 0.091), ('web', 0.09), ('nyt', 0.089), ('vadas', 0.089), ('features', 0.089), ('google', 0.084), ('confusion', 0.079), ('straightener', 0.079), ('zebra', 0.079), ('adjectives', 0.078), ('keller', 0.077), ('examples', 0.076), ('bracketing', 0.075), ('nakov', 0.074), ('confusable', 0.069), ('curve', 0.067), ('correction', 0.064), ('hair', 0.063), ('verb', 0.063), ('wsj', 0.059), ('grolier', 0.059), ('stationed', 0.059), ('vacationed', 0.059), ('bnc', 0.058), ('supervised', 0.057), ('brants', 0.057), ('classifier', 0.052), ('lauer', 0.052), ('golding', 0.052), ('unprecedented', 0.052), ('lapata', 0.051), ('labeled', 0.051), ('poorly', 0.048), ('classifiers', 0.046), ('tnt', 0.045), ('training', 0.043), ('domains', 0.042), ('auxiliary', 0.041), ('shane', 0.04), ('ailon', 0.039), ('crftagger', 0.039), ('crossdomain', 0.039), ('eaten', 0.039), ('ncs', 0.039), ('plentiful', 0.039), ('regularize', 0.039), ('curran', 0.039), ('disambiguation', 0.038), ('domain', 0.037), ('thorsten', 0.037), ('unsupervised', 0.035), ('iraq', 0.035), ('rimell', 0.035), ('transitivity', 0.034), ('robust', 0.033), ('patterns', 0.033), ('indicator', 0.033), ('regularization', 0.033), ('biomedical', 0.032), ('emily', 0.032), ('pitler', 0.032), ('kulick', 0.032), ('modjeska', 0.032), ('preslav', 0.032), ('revolution', 0.032), ('tasks', 0.031), ('pairs', 0.031), ('red', 0.03), ('nlp', 0.03), ('views', 0.03), ('ate', 0.03), ('church', 0.03), ('shaw', 0.03), ('barr', 0.03), ('kilgarriff', 0.03), ('treebank', 0.03), ('usage', 0.029), ('ordered', 0.029), ('hearst', 0.029), ('generation', 0.028), ('sight', 0.028), ('liblinear', 0.028), ('capitalized', 0.028), ('tsuruoka', 0.028), ('corpus', 0.028), ('followed', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="76-tfidf-1" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>Author: Shane Bergsma ; Emily Pitler ; Dekang Lin</p><p>Abstract: In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.</p><p>2 0.11431456 <a title="76-tfidf-2" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>Author: Nathanael Chambers ; Dan Jurafsky</p><p>Abstract: This paper improves the use of pseudowords as an evaluation framework for selectional preferences. While pseudowords originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences. A selectional preference model ranks a set of possible arguments for a verb by their semantic fit to the verb. Pseudo-words serve as a proxy evaluation for these decisions. The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original. This paper studies two main aspects of pseudoword creation that affect performance results. (1) Pseudo-word evaluations often evaluate only a subset of the words. We show that selectional preferences should instead be evaluated on the data in its entirety. (2) Different approaches to selecting partner words can produce overly optimistic evaluations. We offer suggestions to address these factors and present a simple baseline that outperforms the state-ofthe-art by 13% absolute on a newspaper domain.</p><p>3 0.10739067 <a title="76-tfidf-3" href="./acl-2010-Learning_Phrase-Based_Spelling_Error_Models_from_Clickthrough_Data.html">164 acl-2010-Learning Phrase-Based Spelling Error Models from Clickthrough Data</a></p>
<p>Author: Xu Sun ; Jianfeng Gao ; Daniel Micol ; Chris Quirk</p><p>Abstract: This paper explores the use of clickthrough data for query spelling correction. First, large amounts of query-correction pairs are derived by analyzing users' query reformulation behavior encoded in the clickthrough data. Then, a phrase-based error model that accounts for the transformation probability between multi-term phrases is trained and integrated into a query speller system. Experiments are carried out on a human-labeled data set. Results show that the system using the phrase-based error model outperforms cantly its baseline systems. 1 signifi-</p><p>4 0.1024526 <a title="76-tfidf-4" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>Author: Valentin I. Spitkovsky ; Daniel Jurafsky ; Hiyan Alshawi</p><p>Abstract: We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning’s Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the- art by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP.</p><p>5 0.097065084 <a title="76-tfidf-5" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>6 0.090972096 <a title="76-tfidf-6" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>7 0.089899614 <a title="76-tfidf-7" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>8 0.08553873 <a title="76-tfidf-8" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>9 0.081384443 <a title="76-tfidf-9" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>10 0.074277073 <a title="76-tfidf-10" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>11 0.072998658 <a title="76-tfidf-11" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>12 0.072947413 <a title="76-tfidf-12" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>13 0.064437665 <a title="76-tfidf-13" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>14 0.063488103 <a title="76-tfidf-14" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>15 0.062397379 <a title="76-tfidf-15" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>16 0.061711229 <a title="76-tfidf-16" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>17 0.061686493 <a title="76-tfidf-17" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>18 0.05944027 <a title="76-tfidf-18" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>19 0.05843813 <a title="76-tfidf-19" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>20 0.056989357 <a title="76-tfidf-20" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.194), (1, 0.051), (2, 0.007), (3, -0.011), (4, 0.026), (5, -0.007), (6, 0.025), (7, 0.019), (8, 0.06), (9, 0.032), (10, -0.024), (11, 0.061), (12, -0.021), (13, -0.048), (14, 0.041), (15, 0.09), (16, 0.069), (17, 0.009), (18, 0.127), (19, 0.004), (20, 0.039), (21, 0.037), (22, 0.077), (23, 0.008), (24, -0.035), (25, -0.007), (26, 0.091), (27, 0.08), (28, -0.097), (29, 0.039), (30, -0.093), (31, -0.044), (32, 0.044), (33, -0.016), (34, 0.082), (35, -0.039), (36, -0.079), (37, -0.005), (38, -0.108), (39, 0.074), (40, 0.126), (41, 0.212), (42, 0.047), (43, -0.083), (44, -0.066), (45, -0.028), (46, 0.006), (47, 0.01), (48, 0.005), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93031448 <a title="76-lsi-1" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>Author: Shane Bergsma ; Emily Pitler ; Dekang Lin</p><p>Abstract: In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.</p><p>2 0.708368 <a title="76-lsi-2" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>3 0.6630497 <a title="76-lsi-3" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>Author: Nathanael Chambers ; Dan Jurafsky</p><p>Abstract: This paper improves the use of pseudowords as an evaluation framework for selectional preferences. While pseudowords originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences. A selectional preference model ranks a set of possible arguments for a verb by their semantic fit to the verb. Pseudo-words serve as a proxy evaluation for these decisions. The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original. This paper studies two main aspects of pseudoword creation that affect performance results. (1) Pseudo-word evaluations often evaluate only a subset of the words. We show that selectional preferences should instead be evaluated on the data in its entirety. (2) Different approaches to selecting partner words can produce overly optimistic evaluations. We offer suggestions to address these factors and present a simple baseline that outperforms the state-ofthe-art by 13% absolute on a newspaper domain.</p><p>4 0.65518385 <a title="76-lsi-4" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>Author: Valentin I. Spitkovsky ; Daniel Jurafsky ; Hiyan Alshawi</p><p>Abstract: We show how web mark-up can be used to improve unsupervised dependency parsing. Starting from raw bracketings of four common HTML tags (anchors, bold, italics and underlines), we refine approximate partial phrase boundaries to yield accurate parsing constraints. Conversion procedures fall out of our linguistic analysis of a newly available million-word hyper-text corpus. We demonstrate that derived constraints aid grammar induction by training Klein and Manning’s Dependency Model with Valence (DMV) on this data set: parsing accuracy on Section 23 (all sentences) of the Wall Street Journal corpus jumps to 50.4%, beating previous state-of-the- art by more than 5%. Web-scale experiments show that the DMV, perhaps because it is unlexicalized, does not benefit from orders of magnitude more annotated but noisier data. Our model, trained on a single blog, generalizes to 53.3% accuracy out-of-domain, against the Brown corpus nearly 10% higher than the previous published best. The fact that web mark-up strongly correlates with syntactic structure may have broad applicability in NLP.</p><p>5 0.65361005 <a title="76-lsi-5" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>Author: Joel Tetreault ; Jennifer Foster ; Martin Chodorow</p><p>Abstract: Jennifer Foster NCLT Dublin City University Ireland j fo st er@ comput ing . dcu . ie Martin Chodorow Hunter College of CUNY New York, NY, USA martin . chodorow @hunter . cuny . edu We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceWe evaluate the effect of adding parse features to a leading model of preposition us- age. Results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an ESL error detection task. Analysis of the parser output indicates that it is robust enough in the face of noisy non-native writing to extract useful information.</p><p>6 0.57257503 <a title="76-lsi-6" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>7 0.52319485 <a title="76-lsi-7" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>8 0.51785266 <a title="76-lsi-8" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>9 0.50603515 <a title="76-lsi-9" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>10 0.50410426 <a title="76-lsi-10" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>11 0.49544275 <a title="76-lsi-11" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>12 0.49180022 <a title="76-lsi-12" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>13 0.47131422 <a title="76-lsi-13" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>14 0.46647573 <a title="76-lsi-14" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>15 0.46447718 <a title="76-lsi-15" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<p>16 0.45703477 <a title="76-lsi-16" href="./acl-2010-Learning_Phrase-Based_Spelling_Error_Models_from_Clickthrough_Data.html">164 acl-2010-Learning Phrase-Based Spelling Error Models from Clickthrough Data</a></p>
<p>17 0.45013908 <a title="76-lsi-17" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<p>18 0.44064957 <a title="76-lsi-18" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>19 0.43302685 <a title="76-lsi-19" href="./acl-2010-Extracting_Sequences_from_the_Web.html">111 acl-2010-Extracting Sequences from the Web</a></p>
<p>20 0.43086064 <a title="76-lsi-20" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.02), (25, 0.07), (39, 0.01), (42, 0.033), (44, 0.013), (59, 0.112), (71, 0.013), (72, 0.011), (73, 0.052), (76, 0.014), (78, 0.061), (80, 0.036), (83, 0.093), (84, 0.03), (92, 0.226), (98, 0.119)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79163742 <a title="76-lda-1" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>Author: Shane Bergsma ; Emily Pitler ; Dekang Lin</p><p>Abstract: In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.</p><p>2 0.67902404 <a title="76-lda-2" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>Author: Diarmuid O Seaghdha</p><p>Abstract: This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.</p><p>3 0.67609221 <a title="76-lda-3" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>Author: Omri Abend ; Ari Rappoport</p><p>Abstract: The core-adjunct argument distinction is a basic one in the theory of argument structure. The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategorization acquisition. This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms. This is the first work to tackle this task in a fully unsupervised scenario.</p><p>4 0.67455024 <a title="76-lda-4" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>5 0.67423362 <a title="76-lda-5" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and treesubstitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.</p><p>6 0.67134798 <a title="76-lda-6" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>7 0.67116416 <a title="76-lda-7" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>8 0.67074752 <a title="76-lda-8" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>9 0.67051351 <a title="76-lda-9" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>10 0.6704632 <a title="76-lda-10" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>11 0.66981637 <a title="76-lda-11" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>12 0.66937596 <a title="76-lda-12" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>13 0.66805756 <a title="76-lda-13" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>14 0.66665941 <a title="76-lda-14" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>15 0.66632366 <a title="76-lda-15" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>16 0.66618657 <a title="76-lda-16" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>17 0.66603798 <a title="76-lda-17" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>18 0.66575754 <a title="76-lda-18" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>19 0.6630137 <a title="76-lda-19" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>20 0.6626904 <a title="76-lda-20" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
