<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-29" href="#">acl2010-29</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</h1>
<br/><p>Source: <a title="acl-2010-29-pdf" href="http://aclweb.org/anthology//P/P10/P10-1106.pdf">pdf</a></p><p>Author: Eric Corlett ; Gerald Penn</p><p>Abstract: Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment prob- lems.</p><p>Reference: <a title="acl-2010-29-reference" href="../acl2010_reference/acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. [sent-3, score-0.69]
</p><p>2 It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. [sent-4, score-0.184]
</p><p>3 We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment prob-  lems. [sent-7, score-0.622]
</p><p>4 1 Introduction Letter-substitution ciphers encode a document from a known language into an unknown writing system or an unknown encoding of a known writing system. [sent-8, score-0.69]
</p><p>5 While this is not a problem in languages like English and Chinese, which have a small set of well known standard encodings such as ASCII, Big5 and Unicode, there are other languages such as Hindi in which there is no dominant encoding standard for the writing system. [sent-10, score-0.143]
</p><p>6 In these languages, we would like to be able to automatically retrieve and display the information in electronic documents which use unknown encodings when we find them. [sent-11, score-0.19]
</p><p>7 The purpose of this paper, then, is to simplify the problem of reading documents in unknown encodings by presenting a new algorithm to be used in their decipherment. [sent-14, score-0.196]
</p><p>8 Our algorithm operates by running a search over the n-gram probabilities ofpossible solutions to the cipher, using a generalization of the Viterbi algorithm that is wrapped in an A* search, which determines at each step which partial solutions to expand. [sent-15, score-0.694]
</p><p>9 We specifically consider the problem of finding decodings of electronic documents drawn from the internet, and we test our algorithm on ciphers drawn from randomly selected pages of Wikipedia. [sent-17, score-0.584]
</p><p>10 It may seem at first that automatically decoding (as opposed to deciphering) a document is a sim-  ple matter, but studies have shown that simple algorithms such as letter frequency counting do not always produce optimal solutions (Bauer, 2007). [sent-19, score-0.311]
</p><p>11 If the text from which a language model is trained is of a different genre than the plaintext of a cipher, the unigraph letter frequencies may differ substantially from those of the language model, and so frequency counting will be misleading. [sent-20, score-0.514]
</p><p>12 Because of the perceived simplicity of the problem, however, little work was performed to understand its computational properties until Peleg and Rosenfeld (1979), who developed a method that repeatedly swaps letters in a cipher to find a maximum probability solution. [sent-21, score-0.463]
</p><p>13 Since then, several different approaches to this problem have been suggested, some of which use word counts in the language to arrive at a solution (Hart, 1994), and some of 1040  Proce dingUsp opfs thaela 4, 8Stwhe Adnen u,a 1l1- M16e Jtiunlgy o 2f0 t1h0e. [sent-22, score-0.138]
</p><p>14 Unlike the present method, however, Ravi and Knight (2008) treat the decipherment of letter-substitution ciphers as an integer programming problem. [sent-28, score-0.575]
</p><p>15 Clever though this constraint-based encoding is, their paper does not quantify the massive running times required to decode even very short documents with this sort of approach. [sent-29, score-0.187]
</p><p>16 In any case, an exact method is available with a much more efficient A* search that is linear-time in the length of the cipher (though still horribly exponential in the size of the cipher and plain text alphabets), and has the additional advantage of being  massively parallelizable. [sent-31, score-0.78]
</p><p>17 (Ravi and Knight, 2008) also seem to believe that short cipher texts are somehow inherently more difficult to solve than long cipher texts. [sent-32, score-0.731]
</p><p>18 Uniform character models equivocate regardless of the length of the cipher, and sharp character models with many zeroes can quickly converge even on short ciphers of only a few characters. [sent-34, score-0.853]
</p><p>19 In fact, we must use add-one smoothing to decipher texts of even modest lengths because even one unseen plain-text letter sequence is enough to knock out the correct solution. [sent-36, score-0.201]
</p><p>20 Applications of decipherment are also explored by (Nagy et al. [sent-38, score-0.117]
</p><p>21 , 1987), who uses it in the context of optical character recognition (OCR). [sent-39, score-0.182]
</p><p>22 2  Terminology  Substitution ciphers are ciphers that are defined by some permutation of a plaintext alphabet. [sent-42, score-1.298]
</p><p>23 Every character of a plaintext string is consistently mapped to a single character of an output string using this permutation. [sent-43, score-0.872]
</p><p>24 For example, if we took the string ” hello world” to be the plaintext, then  the string ” ifmmp xpsme” would be a cipher that maps e to f, l to m, and so on. [sent-44, score-0.489]
</p><p>25 It is easy to extend this kind of cipher so that the plaintext alphabet is different from the ciphertext alphabet, but still stands in a one to one correspondence to it. [sent-45, score-1.124]
</p><p>26 Given a ciphertext C, we say that the set of characters used in C is the ciphertext alphabet ΣC, and that its size is nC. [sent-46, score-0.858]
</p><p>27 Similarly, the entire possible plaintext alphabet is ΣP, and its size is is nP. [sent-47, score-0.495]
</p><p>28 Since nC is the number of letters actually used in the cipher, rather than the entire alphabet it is sampled from, we may find that nC < nP even when the two alphabets are the same. [sent-48, score-0.187]
</p><p>29 We refer to the length of the cipher string C as clen. [sent-49, score-0.432]
</p><p>30 Given the ciphertext C, we say that a partial solution of size k is a map σ = {p1 : c1, . [sent-54, score-0.708]
</p><p>31 If for a partial sdol aurteio dni σtin0, we hnadv we htheraet σ ⊂ σ0, then we say that σ0 extends σ. [sent-64, score-0.174]
</p><p>32 wIfet hheav vseiz teh aoft σσ0 ⊂is  ≤  k+1 and σ is size k, we say that σ0 is an immediate extension of σ. [sent-65, score-0.113]
</p><p>33 A full solution is a partial solution of size nC. [sent-66, score-0.497]
</p><p>34 In the above example, σ1 = { : d : e} would be a partial solution of size 2, a {nd : σ2 = {: ,d : e, g : m} swolouutilod n be o a partial nsodl σution ld o{f size, d3 t:h ea,t immediately dex bteen ads p σ1. [sent-67, score-0.533]
</p><p>35 lA s partial solution σT{: , : e, e : f,h : i,l : m, o : d :  ,  1041  p, r : s, w : x} would be both a full solution and tph,er correct one. [sent-68, score-0.45]
</p><p>36 Every possible full solution to a cipher C will produce a plaintext string with some associated language model probability, and we will consider the best possible solution to be the one that gives the highest probability. [sent-70, score-1.059]
</p><p>37 This plaintext can be found by treating all of the length clen strings S as being the output of different character mappings from C. [sent-72, score-0.698]
</p><p>38 A string S that results from such a mapping is consistent with a partial solu-  tion σ iff, for every pi : ci ∈ σ, the character positions of C that map to pi are exactly tahrea cchtearra poctsei-r positions with ci in C. [sent-73, score-0.675]
</p><p>39 In our above example, we had C = ” ifmmp xpsme” , in which case we had clen = 11. [sent-74, score-0.114]
</p><p>40 So mappings from C to ” hhhhh hhhhh” or ” hhhhhhhhhh” would be consistent with a partial solution of size 0, while ” hhhhh hhhhn” would be consistent with the size 2 partial solution σ = { : n : e}. [sent-75, score-0.906]
</p><p>41 If we start with an empty solution and iteratively choose the most likely remaining partial solution in this way, storing the extensions obtained in a priority heap as we go, we will eventually reach a solution of size nC. [sent-78, score-0.701]
</p><p>42 Every extension of σ has a probability that is, at best, equal to that of σ, and every partial solution receives, at worst, a score equal to its best extension, because the score is potentially based on an inconsistent mapping that does not qualify as an extension. [sent-79, score-0.411]
</p><p>43 Thus the first solution of size nC will be the best solution of size nC. [sent-81, score-0.37]
</p><p>44 The order by which we add the letters c to partial solutions is the order of the distinct cipher-  text letters in right-to-left order of their final occurrence in C. [sent-82, score-0.489]
</p><p>45 Create a priority queue Q for partial solutions, ordered by highest probability. [sent-91, score-0.27]
</p><p>46 while Q is not empty do Pop the best partial solution σ from Q. [sent-93, score-0.312]
</p><p>47 isf s = nC then return σ else For all p not in the range of σ, push the immediate extension σp onto Q with the score assigned to table cell G(rs+1 ,p, p) by GVit(σ, cs+1 , rs+1) if it is non-zero. [sent-95, score-0.156]
</p><p>48 Unlike the real Viterbi algorithm, we must also observe the constraints of the input partial solution’s mapping. [sent-98, score-0.2]
</p><p>49 A least frequent first regimen has the opposite problem, in which their rare occurrence in the ciphertext provides too few opportunities to potentially reduce the score of a candidate. [sent-101, score-0.358]
</p><p>50 1042  A typical decipherment  involves multiple runs of  this algorithm, each of which scores all of the immediate extensions, both tightening and lowering their scores relative to the score of the input partial solution. [sent-102, score-0.353]
</p><p>51 t The real Viterbi algorithm lacks these final two constraints, and would only store a single cell at G(i, l). [sent-104, score-0.137]
</p><p>52 The table is completed by filling in the columns from i = 1 to clen in order. [sent-107, score-0.134]
</p><p>53 Because we are using a trigram character model, the cells in the first and second columns must be primed with unigram and bigram probabilities. [sent-109, score-0.435]
</p><p>54 The remaining probabilities are calculated by searching through the cells from the previous two columns, using the entry at the earlier column to indicate the probability of the best string up to that point, and searching through the trigram probabilities over two additional letters. [sent-110, score-0.371]
</p><p>55 In order to decrease the search space, we add the further restriction that the solutions of every three character sequence must be consistent: if the ciphertext indicates that two adjacent letters are the same, then only the plaintext strings that map the same letter to each will be considered. [sent-113, score-1.337]
</p><p>56 The number of letters that are forced to be consistent is three because consistency is enforced by removing inconsistent strings from consideration during  ×  trigram model evaluation. [sent-114, score-0.262]
</p><p>57 Because every partial solution is only obtained by extending a solution of size one less, and extensions are only made in a predetermined order of cipher alphabet letters, every partial solution is only considered / extended once. [sent-115, score-1.341]
</p><p>58 The nP nP cells of every column ido not depend on each× onther only on the cells of the previous two columns i−1 aonnldy i−2, as cwelellsl as tthhee language tmwood ceoll. [sent-117, score-0.289]
</p><p>59 4  —  Experiment  The above algorithm is designed for application to the transliteration of electronic documents, specifically, the transliteration of websites, and it has been tested with this in mind. [sent-120, score-0.136]
</p><p>60 A rough search over internet articles has shown that a length of 1000 to 11000 characters is a realistic length for many articles, although this can vary according to the genre of the page. [sent-124, score-0.157]
</p><p>61 In the first set of tests, we chose the mean of the above lengths to be our sample size, and we created and decoded 10 ciphers of this size (i. [sent-127, score-0.54]
</p><p>62 We made these cipher texts by appending the contents of randomly chosen Wikipedia pages until they contained at least 6000 characters, and then using the first 6000 characters of the resulting files as the plaintexts of the cipher. [sent-130, score-0.475]
</p><p>63 The plaintext for this set of tests was developed in the same way as the first set, and the input ciphertext lengths considered were 1000, 3500, 6000, 8500, 11000, and 13500 characters. [sent-135, score-0.792]
</p><p>64 Each cell in the greenhouse is indexed by a plaintext letter and a character from the cipher. [sent-137, score-0.842]
</p><p>65 The cells in the array give the best probabilities of any path passing through the greenhouse cell, given that the index character of the array maps to the character in column c, where c is the next ciphertext character to be fixed in the solution. [sent-139, score-1.191]
</p><p>66 The cell at (d) is filled using the trigram probabilities and the probability of the path at starting at (a). [sent-142, score-0.24]
</p><p>67 In all of the data considered, the frequency of spaces was far higher than that of any other character, and so in any real application the character corresponding to the space can likely be guessed  without difficulty. [sent-143, score-0.28]
</p><p>68 The ciphers we have considered have therefore been simplified by allowing the knowledge of which character corresponds to the space. [sent-144, score-0.64]
</p><p>69 In the event that a trigram or bigram would be found in the plaintext that was not counted in the language model, add one smoothing was used. [sent-147, score-0.495]
</p><p>70 The characters used in the language model were the upper and lower case letters, spaces, and full stops; other characters were skipped when counting the frequencies. [sent-150, score-0.223]
</p><p>71 As discussed in the previous paragraph, the space character is assumed to be known. [sent-152, score-0.182]
</p><p>72 These latter numbers give us insight into the quality of trigram probabilities as a heuristic for the A* search. [sent-154, score-0.149]
</p><p>73 We judged the quality of the decoding by measuring the percentage of characters in the cipher alphabet that were correctly guessed, and also the word error rate of the plaintext generated by our solution. [sent-155, score-0.928]
</p><p>74 The second metric is useful because a low probability character in the ciphertext may be guessed wrong without changing as much of the actual plaintext. [sent-156, score-0.58]
</p><p>75 Counting the actual number of word errors is meant as an estimate of how useful or readable the plaintext will be. [sent-157, score-0.382]
</p><p>76 We would have liked to compare our results with those of Ravi and Knight (2008), but the method presented there was simply not feasible  1044  ×  Algorithm 2 Generalized Viterbi Algorithm  GVit(σ,c,r) Input: partial solution σ, ciphertext character c, and index r into C. [sent-159, score-0.819]
</p><p>77 end for end for for i= 4 to r do for (l, k) such that σ ∪ {k : c, l : Ci} is consfiosrte (nlt, kd)o for j1 ,j2 such that σ ∪ {k : c, j2 : C[i−2] ,j1 : C[i−1] , tl σ : Ci} is consistent Cdo[ G(i, l, k) = max(G(i, l, k) ,  G(i−2,j2, k)×P(j1|j2j2(back)) P(l|j2j1)). [sent-164, score-0.14]
</p><p>78 end for end for end for  on texts and (case-sensitive) alphabets of this size  ×  with the computing hardware at our disposal. [sent-165, score-0.211]
</p><p>79 5  Results  In our first set of tests, we measured the time consumption and accuracy of our algorithm over 10 ciphers taken from random texts that were 6000 characters long. [sent-166, score-0.686]
</p><p>80 All running times reported in this section were obtained on a computer running Ubuntu Linux 8. [sent-171, score-0.15]
</p><p>81 Cwoitlhum 4n G-lBev oefl subcomputations 5in G Hthez greenhouse were dispatched to an NVIDIA Quadro FX 1700 GPU card that is attached through a 16-lane PCI Express adapter. [sent-174, score-0.138]
</p><p>82 In our second set of tests, we measured the time consumption and accuracy of our algorithm over several prefixes of different lengths of a single 13500-character ciphertext. [sent-176, score-0.192]
</p><p>83 This is a Zipf’s Law effect misclassified characters come from poorly attested character trigrams, which are in turn found only in longer, rarer words. [sent-182, score-0.277]
</p><p>84 The overall high accuracy is probably due to the large size of the texts relative to the uniticity distance of an English letter-substitution cipher (Bauer, 2007). [sent-183, score-0.427]
</p><p>85 The results do show, however, that character trigram probabilities are an effective indicator of the most likely solution, even when the language model and test data are from very different genres (here, the  —  Wall Street Journal and Wikipedia, respectively). [sent-184, score-0.331]
</p><p>86 As far as the running time of the algorithm goes, we see a substantial variance: from a few minutes to several hours for most of the longer ciphers, and that there are some that take longer than the threshold we gave in the experiment. [sent-191, score-0.155]
</p><p>87 Desiring to reduce the variance of the running time, we look at the second set of tests for possible causes. [sent-193, score-0.125]
</p><p>88 In the second test set, there is a general decrease in both the running time and the number of solutions expanded as the length of the ciphers increases. [sent-194, score-0.737]
</p><p>89 In particular, the length 8500 cipher generates more solutions than the length 3500 or 6000 ones. [sent-198, score-0.558]
</p><p>90 Recall that the ciphers in this section are all prefixes of the same string. [sent-199, score-0.511]
</p><p>91 Because the algorithm fixes characters starting from the end of the cipher, these prefixes have very different character orderings, c1, . [sent-200, score-0.41]
</p><p>92 , cnC , and thus a very different order of partial solutions. [sent-203, score-0.174]
</p><p>93 The running time of our algorithm depends very crucially on these initial conditions. [sent-204, score-0.122]
</p><p>94 Perhaps most interestingly, we note that the number of enqueued partial solutions is in every case identical or nearly identical to the number of partial solutions expanded. [sent-205, score-0.728]
</p><p>95 A more complex heuristic that can additionally rank non-zero probability solutions with more prescience would likely make a very great difference to the running time of this method. [sent-209, score-0.247]
</p><p>96 1046  6  Conclusions  In the above paper, we have presented an algorithm for solving letter-substitution ciphers, with an eye towards discovering unknown encoding standards in electronic documents on the fly. [sent-210, score-0.228]
</p><p>97 In a test of our algorithm over ciphers drawn from Wikipedia, we found its accuracy to be 100% on the ciphers that it solved within a threshold of 12 hours, this being 80% of the total attempted. [sent-211, score-0.963]
</p><p>98 There is, however, a great deal of room for improvement in the trigram model’s ability to rank partial solutions that are not eliminated outright. [sent-213, score-0.432]
</p><p>99 Our approach makes no explicit attempt to account for noisy ciphers, in which characters are erroneously mapped, nor any attempt to account for more general substitution ciphers in which a single plaintext (resp. [sent-217, score-0.977]
</p><p>100 plaintext) letters, nor for ciphers in which ciphertext units corresponds to larger units of plaintext such syllables or words. [sent-219, score-1.165]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ciphers', 0.458), ('plaintext', 0.382), ('cipher', 0.351), ('ciphertext', 0.325), ('character', 0.182), ('partial', 0.174), ('solutions', 0.145), ('solution', 0.138), ('decipherment', 0.117), ('greenhouse', 0.115), ('trigram', 0.113), ('cells', 0.109), ('letter', 0.099), ('characters', 0.095), ('letters', 0.085), ('ci', 0.084), ('ravi', 0.079), ('clen', 0.076), ('gvit', 0.076), ('running', 0.075), ('alphabet', 0.066), ('viterbi', 0.065), ('queue', 0.065), ('cell', 0.064), ('knight', 0.062), ('consumption', 0.057), ('encodings', 0.057), ('hhhhh', 0.057), ('nc', 0.056), ('unknown', 0.054), ('prefixes', 0.053), ('trigrams', 0.052), ('tests', 0.05), ('string', 0.05), ('enqueued', 0.05), ('encoding', 0.048), ('size', 0.047), ('algorithm', 0.047), ('deciphering', 0.046), ('guessed', 0.046), ('substitution', 0.042), ('wikipedia', 0.042), ('electronic', 0.041), ('every', 0.04), ('writing', 0.038), ('aisltlen', 0.038), ('corlett', 0.038), ('dko', 0.038), ('fcoorns', 0.038), ('ifmmp', 0.038), ('knock', 0.038), ('mhz', 0.038), ('nagy', 0.038), ('nvidia', 0.038), ('peleg', 0.038), ('xpsme', 0.038), ('documents', 0.038), ('tl', 0.037), ('consistent', 0.037), ('alphabets', 0.036), ('probabilities', 0.036), ('lengths', 0.035), ('extensions', 0.035), ('immediate', 0.034), ('decoding', 0.034), ('bauer', 0.033), ('regimen', 0.033), ('end', 0.033), ('counting', 0.033), ('hours', 0.033), ('extension', 0.032), ('length', 0.031), ('priority', 0.031), ('columns', 0.031), ('clever', 0.031), ('restarts', 0.031), ('array', 0.03), ('texts', 0.029), ('zeros', 0.029), ('decrease', 0.028), ('runs', 0.028), ('badly', 0.027), ('ocr', 0.027), ('filling', 0.027), ('strings', 0.027), ('probability', 0.027), ('spaces', 0.026), ('decode', 0.026), ('mapped', 0.026), ('real', 0.026), ('onto', 0.026), ('generalization', 0.025), ('orderings', 0.025), ('pk', 0.025), ('transliteration', 0.024), ('bosch', 0.024), ('map', 0.024), ('relaxation', 0.023), ('card', 0.023), ('cutoff', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="29-tfidf-1" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>Author: Eric Corlett ; Gerald Penn</p><p>Abstract: Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment prob- lems.</p><p>2 0.12028427 <a title="29-tfidf-2" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>Author: Benjamin Snyder ; Regina Barzilay ; Kevin Knight</p><p>Abstract: In this paper we propose a method for the automatic decipherment of lost languages. Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates. We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and highlevel morphemic correspondences. This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers. When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew.</p><p>3 0.082859822 <a title="29-tfidf-3" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>Author: David Elson ; Nicholas Dames ; Kathleen McKeown</p><p>Abstract: We present a method for extracting social networks from literature, namely, nineteenth-century British novels and serials. We derive the networks from dialogue interactions, and thus our method depends on the ability to determine when two characters are in conversation. Our approach involves character name chunking, quoted speech attribution and conversation detection given the set of quotes. We extract features from the social networks and examine their correlation with one another, as well as with metadata such as the novel’s setting. Our results provide evidence that the majority of novels in this time period do not fit two characterizations provided by literacy scholars. Instead, our results suggest an alternative explanation for differences in social networks.</p><p>4 0.081446446 <a title="29-tfidf-4" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>Author: Matthew Skala ; Victoria Krakovna ; Janos Kramar ; Gerald Penn</p><p>Abstract: Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector techniques.</p><p>5 0.077482417 <a title="29-tfidf-5" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>6 0.069197856 <a title="29-tfidf-6" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>7 0.065399177 <a title="29-tfidf-7" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>8 0.055442542 <a title="29-tfidf-8" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>9 0.052580524 <a title="29-tfidf-9" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>10 0.049538888 <a title="29-tfidf-10" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>11 0.049168121 <a title="29-tfidf-11" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>12 0.048692472 <a title="29-tfidf-12" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>13 0.047299746 <a title="29-tfidf-13" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>14 0.045716528 <a title="29-tfidf-14" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>15 0.044840228 <a title="29-tfidf-15" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>16 0.044325609 <a title="29-tfidf-16" href="./acl-2010-String_Extension_Learning.html">217 acl-2010-String Extension Learning</a></p>
<p>17 0.044044428 <a title="29-tfidf-17" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>18 0.0416415 <a title="29-tfidf-18" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>19 0.040096298 <a title="29-tfidf-19" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>20 0.040055778 <a title="29-tfidf-20" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.129), (1, -0.01), (2, -0.015), (3, -0.035), (4, 0.015), (5, -0.038), (6, 0.035), (7, -0.003), (8, 0.078), (9, -0.014), (10, -0.07), (11, 0.015), (12, 0.026), (13, -0.047), (14, -0.122), (15, -0.01), (16, -0.037), (17, 0.038), (18, -0.015), (19, 0.034), (20, -0.024), (21, 0.015), (22, -0.034), (23, -0.073), (24, 0.025), (25, -0.037), (26, -0.042), (27, -0.053), (28, -0.01), (29, 0.031), (30, -0.072), (31, -0.04), (32, 0.081), (33, -0.051), (34, -0.195), (35, -0.063), (36, 0.003), (37, -0.044), (38, -0.062), (39, -0.074), (40, 0.069), (41, 0.034), (42, -0.04), (43, -0.061), (44, -0.04), (45, -0.091), (46, 0.016), (47, 0.022), (48, 0.037), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92729712 <a title="29-lsi-1" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>Author: Eric Corlett ; Gerald Penn</p><p>Abstract: Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment prob- lems.</p><p>2 0.70362836 <a title="29-lsi-2" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>Author: Benjamin Snyder ; Regina Barzilay ; Kevin Knight</p><p>Abstract: In this paper we propose a method for the automatic decipherment of lost languages. Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates. We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and highlevel morphemic correspondences. This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers. When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew.</p><p>3 0.59268576 <a title="29-lsi-3" href="./acl-2010-Finding_Cognate_Groups_Using_Phylogenies.html">116 acl-2010-Finding Cognate Groups Using Phylogenies</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline ap- proach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.</p><p>4 0.57313174 <a title="29-lsi-4" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>Author: Dong Yang ; Paul Dixon ; Sadaoki Furui</p><p>Abstract: This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outper- forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</p><p>5 0.5517413 <a title="29-lsi-5" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>Author: David Elson ; Nicholas Dames ; Kathleen McKeown</p><p>Abstract: We present a method for extracting social networks from literature, namely, nineteenth-century British novels and serials. We derive the networks from dialogue interactions, and thus our method depends on the ability to determine when two characters are in conversation. Our approach involves character name chunking, quoted speech attribution and conversation detection given the set of quotes. We extract features from the social networks and examine their correlation with one another, as well as with metadata such as the novel’s setting. Our results provide evidence that the majority of novels in this time period do not fit two characterizations provided by literacy scholars. Instead, our results suggest an alternative explanation for differences in social networks.</p><p>6 0.54811394 <a title="29-lsi-6" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>7 0.49442118 <a title="29-lsi-7" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>8 0.48188627 <a title="29-lsi-8" href="./acl-2010-Untangling_the_Cross-Lingual_Link_Structure_of_Wikipedia.html">250 acl-2010-Untangling the Cross-Lingual Link Structure of Wikipedia</a></p>
<p>9 0.46729425 <a title="29-lsi-9" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>10 0.45969084 <a title="29-lsi-10" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>11 0.45538014 <a title="29-lsi-11" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>12 0.43216524 <a title="29-lsi-12" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>13 0.39987665 <a title="29-lsi-13" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>14 0.39184082 <a title="29-lsi-14" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>15 0.38524824 <a title="29-lsi-15" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>16 0.37209249 <a title="29-lsi-16" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>17 0.36681637 <a title="29-lsi-17" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>18 0.35567576 <a title="29-lsi-18" href="./acl-2010-Efficient_Inference_through_Cascades_of_Weighted_Tree_Transducers.html">95 acl-2010-Efficient Inference through Cascades of Weighted Tree Transducers</a></p>
<p>19 0.33462322 <a title="29-lsi-19" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>20 0.32759842 <a title="29-lsi-20" href="./acl-2010-Estimating_Strictly_Piecewise_Distributions.html">103 acl-2010-Estimating Strictly Piecewise Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.016), (25, 0.042), (39, 0.035), (42, 0.025), (52, 0.011), (59, 0.105), (62, 0.301), (72, 0.011), (73, 0.062), (78, 0.035), (83, 0.094), (84, 0.038), (98, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74079251 <a title="29-lda-1" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>Author: Eric Corlett ; Gerald Penn</p><p>Abstract: Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment prob- lems.</p><p>2 0.54975414 <a title="29-lda-2" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>3 0.54731429 <a title="29-lda-3" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>Author: Diarmuid O Seaghdha</p><p>Abstract: This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.</p><p>4 0.54716009 <a title="29-lda-4" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>Author: Stephen Wu ; Asaf Bachrach ; Carlos Cardenas ; William Schuler</p><p>Abstract: Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.</p><p>5 0.54608071 <a title="29-lda-5" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>Author: Ivan Titov ; Mikhail Kozhevnikov</p><p>Abstract: We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations. A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts. We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts.</p><p>6 0.54490852 <a title="29-lda-6" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>7 0.54358256 <a title="29-lda-7" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>8 0.54263079 <a title="29-lda-8" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>9 0.54231036 <a title="29-lda-9" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>10 0.54223496 <a title="29-lda-10" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>11 0.54124856 <a title="29-lda-11" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>12 0.54091823 <a title="29-lda-12" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>13 0.54065919 <a title="29-lda-13" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>14 0.54065001 <a title="29-lda-14" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>15 0.54036427 <a title="29-lda-15" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>16 0.54004997 <a title="29-lda-16" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>17 0.53946537 <a title="29-lda-17" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>18 0.53905666 <a title="29-lda-18" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>19 0.53844345 <a title="29-lda-19" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>20 0.53806412 <a title="29-lda-20" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
