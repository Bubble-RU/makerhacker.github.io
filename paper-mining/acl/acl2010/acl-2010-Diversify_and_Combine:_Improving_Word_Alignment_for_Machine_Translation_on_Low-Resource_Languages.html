<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-90" href="#">acl2010-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</h1>
<br/><p>Source: <a title="acl-2010-90-pdf" href="http://aclweb.org/anthology//P/P10/P10-2005.pdf">pdf</a></p><p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>Reference: <a title="acl-2010-90-reference" href="../acl2010_reference/acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. [sent-5, score-0.94]
</p><p>2 Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. [sent-6, score-0.644]
</p><p>3 We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. [sent-7, score-0.524]
</p><p>4 The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla-  tion performance. [sent-8, score-0.511]
</p><p>5 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. [sent-9, score-0.423]
</p><p>6 They all focused on the improvement of word alignment models. [sent-12, score-0.411]
</p><p>7 In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. [sent-13, score-0.99]
</p><p>8 The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. [sent-14, score-0.369]
</p><p>9 Most of the research on alignment combination in the past has focused on how to combine the alignments from two different directions, source-  to-target and target-to-source. [sent-16, score-0.877]
</p><p>10 Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (Koehn et al. [sent-17, score-0.166]
</p><p>11 , 2003), to achieve a better balance compared to using either intersection (high precision) or union (high recall). [sent-18, score-0.048]
</p><p>12 In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. [sent-19, score-0.868]
</p><p>13 A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. [sent-20, score-0.12]
</p><p>14 It tries to maximize the number of phrases that can be extracted in the combined alignments. [sent-21, score-0.095]
</p><p>15 A greedy search method was utilized and it achieved higher translation performance than the baseline. [sent-22, score-0.102]
</p><p>16 More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. [sent-23, score-1.14]
</p><p>17 The alignments used in that work were generated from different align-  ers (HMM, block model, and maximum entropy model). [sent-24, score-0.438]
</p><p>18 In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. [sent-25, score-0.222]
</p><p>19 Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners. [sent-27, score-0.416]
</p><p>20 Our strategy is to diversify and then combine in order to catch any complementary information captured in the word alignments for low-resource languages. [sent-28, score-0.606]
</p><p>21 0c 2 C0o1n0fe Aresnsoceci Sathio rnt f Poarp Ceorsm,p paugteastio 2n2a–l2 L6i,nguistics We present three different sets of alignments i n Section 2 for an English-to-Pashto MT task. [sent-32, score-0.427]
</p><p>22 I n Section 3, we propose the alignment combinatio n algorithm. [sent-33, score-0.369]
</p><p>23 2  Diversified Word Alignments  We take an English-to-Pashto MT task as an exam -  ple and create three sets of additional alignments on top of the baseline alignment. [sent-36, score-0.482]
</p><p>24 In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse. [sent-42, score-0.215]
</p><p>25 The VP-based reordering rule we apply in the work is: •  V P(V B∗, ∗)  →  V P(∗, V B∗)  where V B∗ represents V B, V BD, V BG, V BN, Vw hBePre aVn Bd ∗V r BepZre. [sent-43, score-0.215]
</p><p>26 In Figure 1, we show the reference alignment between an English sentence and the corresponding Pashto translation, where E is the original English sentence, P is the Pashto sentence (in romanized text), and E′ is the English sentence after  reordering. [sent-44, score-0.369]
</p><p>27 As we can see, after the VP-based reordering, the alignment between the two sentences becomes monotone, which makes it easier for the aligner to get the alignment correct. [sent-45, score-0.792]
</p><p>28 During the reordering of English sentences, we store the index changes for the English words. [sent-46, score-0.215]
</p><p>29 After getting the alignment trained on the reordered English and original Pashto sentence pairs, we map the English words back to the original order, along with the learned alignment links. [sent-47, score-0.772]
</p><p>30 In this way, the alignment is ready to be combined with the baseline alignment and any other alternatives. [sent-48, score-0.832]
</p><p>31 In addition to the linguistic knowledge applied in the syntactic reordering described above, we also utilize morphological analysis by applying stemming on both the English and Pashto sides. [sent-51, score-0.456]
</p><p>32 For English, we use Porter stemming (Porter, S  EP’: htQNveyPR soVtBauArPOe Rmpkyo$AluSrvVeNPsmplodaryeNSsCaAnvdNtPyosuOVBkthPnQoeSvwym tVshPeNRlm pwkOeRnZAlBoDVP Figure 1: Alignment before/after ordering. [sent-52, score-0.15]
</p><p>33 VP-based re-  1980), a widely applied algorithm to remove the common morphological and inflexional endings from words in English. [sent-53, score-0.052]
</p><p>34 For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et al. [sent-54, score-0.091]
</p><p>35 We start from a fixed set of affixes with 8 prefixes and 21 suffixes. [sent-56, score-0.101]
</p><p>36 The prefixes and suffixes are stripped off from the Pashto words under the two constraints:(1) Longest matched affixes first; (2) Remaining stem must be at least two characters long. [sent-57, score-0.101]
</p><p>37 , 2009), which keeps partial English and Urdu words in the training data for alignment training. [sent-61, score-0.462]
</p><p>38 This is similar to the stemming method, but is more heuristicsbased, and does not rely on a set of available affixes. [sent-62, score-0.15]
</p><p>39 With the same motivation, we keep the first 4 characters of each English and Pashto word to generate one more alternative for the word alignment. [sent-63, score-0.084]
</p><p>40 3  Confidence-Based Alignment Combination  Now we describe the algorithm to combine multiple sets of word alignments based on weighted confidence scores. [sent-64, score-0.652]
</p><p>41 Suppose aijk is an alignment link in the i-th set of alignments between the j-th source word and k-th target word in sentence pair (S,T). [sent-65, score-1.247]
</p><p>42 tween source word sj and target word tk in the i-th set of alignments. [sent-68, score-0.326]
</p><p>43 Each candidate link ajk gets soft votes from N sets of alignments via weighted confidence scores: XN  v(ajk|S,T) =iX=1wi∗ c(aijk|S,T),  (3)  where the weight wi for each set of alignment can be optimized under various criteria. [sent-71, score-1.213]
</p><p>44 In this work, we tune it on a hand-aligned development set to maximize the alignment Fscore. [sent-72, score-0.459]
</p><p>45 All candidates are sorted by soft votes in descending order and evaluated sequentially. [sent-74, score-0.117]
</p><p>46 A candidate link ajk is included if one of the following is true: • Neither sj nor tk is aligned so far; • sj is not aligned and its left or right neighboring word is aligned to tk so far; • tk is not aligned and its left or right neighboring word is aligned to sj so far. [sent-75, score-1.352]
</p><p>47 Repeat scanning all candidate links until no more links can be added. [sent-77, score-0.136]
</p><p>48 In this way, those alignment links with higher confidence scores have higher priority to be included in the combined alignment. [sent-78, score-0.754]
</p><p>49 The baseline is a phrase-based MT system similar to (Koehn et al. [sent-81, score-0.055]
</p><p>50 We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then apply grow-diagonal-final (gdf). [sent-83, score-0.424]
</p><p>51 The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al. [sent-84, score-0.163]
</p><p>52 There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. [sent-86, score-0.046]
</p><p>53 We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. [sent-87, score-0.411]
</p><p>54 The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. [sent-88, score-0.641]
</p><p>55 The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). [sent-89, score-0.276]
</p><p>56 2  Improvement in Word Alignment  In Table 1 we show the precision, recall and Fscore of each set of word alignments for the 150sentence set. [sent-91, score-0.419]
</p><p>57 Using partial word provides the highest F-score among all individual alignments. [sent-92, score-0.135]
</p><p>58 The F-score is 5% higher than for the baseline alignment. [sent-93, score-0.103]
</p><p>59 The VP-based reordering itself does not improve the F-score, which could be due to the parse errors on the conversational training data. [sent-94, score-0.215]
</p><p>60 We experiment with three options (c0, c1, c2) when combining the baseline and reordering-based alignments. [sent-95, score-0.055]
</p><p>61 In c0, the weights wi and confidence scores c(aijk |S, T) in Eq. [sent-96, score-0.229]
</p><p>62 In c1, we set| c,oTnf)id inen cEeq scores to 1, while tuning the weights with hill climbing to maximize the Fscore on a hand-aligned tuning set. [sent-98, score-0.322]
</p><p>63 In c2, we compute the confidence scores as in Eq. [sent-99, score-0.182]
</p><p>64 The numbers in Table 1 show the effectiveness of having both weights and confidence scores during the combination. [sent-101, score-0.229]
</p><p>65 Similarly, we combine the baseline with each of the other sets of alignments using c2. [sent-102, score-0.543]
</p><p>66 We also generate alignments on VP-reordered partial words (X in Table 1) and compared B + X and B V P. [sent-104, score-0.47]
</p><p>67 The better results with B V P show the benefit of keeping the alignments as diversified as possible before the combination. [sent-105, score-0.516]
</p><p>68 Finally, we compare the proposed alignment combination c2 with the heuristics-based method (gdf), where the latter starts from the intersection of all 4 sets of alignments and then applies grow-diagonalfinal (Koehn et al. [sent-106, score-0.914]
</p><p>69 The proposed combination approach on B + V + S + P results in close to 7% higher Fscores than the baseline and also 2% higher than  + +  + +  24  gdf. [sent-108, score-0.221]
</p><p>70 We also notice that its higher F-score is mainly due to the higher precision, which should result from the consideration of confidence scores. [sent-109, score-0.218]
</p><p>71 AlignmentCombPRF  (B: baseline; V: VP-based reordering; S: stemming; P: partial word; X: VP-reordered partial word). [sent-110, score-0.186]
</p><p>72 3 Improvement in MT Performance In Table 2, we show the corresponding BLEU scores on the test set for the systems built on each set of word alignment in Table 1. [sent-112, score-0.471]
</p><p>73 We also ran one experiment in which we concatenated all 4 sets of alignments into one big set (shown as cat). [sent-114, score-0.427]
</p><p>74 Overall, the BLEU score with confidence-based combination was increased by 1point compared to the baseline, 0. [sent-115, score-0.07]
</p><p>75 5  Conclusions  In this work, we have presented a word alignment combination method that improves both the alignment quality and the translation performance. [sent-121, score-0.904]
</p><p>76 We generated multiple sets of diversified alignments based on linguistics, morphology, and heuristics, and demonstrated the effectiveness of combination on the English-to-Pashto translation task. [sent-122, score-0.69]
</p><p>77 We showed that the combined alignment significantly outperforms the baseline alignment with  AlignmentCombLinksPhraseBLEU  line; V: VP-based reordering; S: stemming; P: partial word; X: VP-reordered partial word). [sent-123, score-1.018]
</p><p>78 The combination approach itself is not limited to any specific alignment. [sent-125, score-0.07]
</p><p>79 It provides a general framework that can take advantage of as many align-  ments as possible, which could differ in preprocessing, alignment modeling, or any other aspect. [sent-126, score-0.369]
</p><p>80 Improving bitext word alignments via syntax-based reordering of english. [sent-155, score-0.634]
</p><p>81 A maximum entropy word aligner for arabic-english machine translation. [sent-175, score-0.157]
</p><p>82 A linear observed time statistical parser based on maximum entropy models. [sent-204, score-0.061]
</p><p>83 Using a dependency parser to improve smt for subject-object-verb languages. [sent-219, score-0.045]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alignments', 0.377), ('alignment', 0.369), ('aijk', 0.358), ('pashto', 0.313), ('reordering', 0.215), ('gdf', 0.159), ('stemming', 0.15), ('diversified', 0.139), ('sj', 0.132), ('confidence', 0.122), ('ajk', 0.119), ('xiang', 0.119), ('tk', 0.11), ('partial', 0.093), ('deng', 0.072), ('bleu', 0.07), ('ayan', 0.07), ('bowen', 0.07), ('diversify', 0.07), ('transtac', 0.07), ('combination', 0.07), ('links', 0.068), ('koehn', 0.066), ('sov', 0.064), ('yonggang', 0.064), ('votes', 0.064), ('entropy', 0.061), ('combine', 0.061), ('scores', 0.06), ('fscore', 0.06), ('bd', 0.06), ('mt', 0.059), ('link', 0.059), ('affixes', 0.057), ('ittycheriah', 0.057), ('complementary', 0.056), ('maximize', 0.056), ('aligned', 0.056), ('baseline', 0.055), ('aligner', 0.054), ('translation', 0.054), ('soft', 0.053), ('franz', 0.052), ('morphological', 0.052), ('sets', 0.05), ('och', 0.05), ('porter', 0.05), ('dr', 0.048), ('intersection', 0.048), ('higher', 0.048), ('english', 0.047), ('weights', 0.047), ('tuning', 0.046), ('nguyen', 0.045), ('smt', 0.045), ('prefixes', 0.044), ('fraser', 0.043), ('philipp', 0.042), ('word', 0.042), ('neighboring', 0.042), ('bing', 0.042), ('huang', 0.041), ('collins', 0.04), ('utilize', 0.039), ('combined', 0.039), ('emnlp', 0.038), ('xu', 0.036), ('morphology', 0.036), ('della', 0.035), ('salim', 0.035), ('maskey', 0.035), ('samad', 0.035), ('fscores', 0.035), ('ringgaard', 0.035), ('aligners', 0.035), ('fazil', 0.035), ('inen', 0.035), ('necip', 0.035), ('abraham', 0.035), ('bg', 0.035), ('elliott', 0.035), ('franco', 0.035), ('prunes', 0.035), ('weaver', 0.035), ('tune', 0.034), ('getting', 0.034), ('zhou', 0.034), ('arabic', 0.034), ('papineni', 0.034), ('pietra', 0.034), ('ibm', 0.033), ('roukos', 0.033), ('darpa', 0.033), ('heuristics', 0.033), ('giza', 0.033), ('chiang', 0.032), ('avn', 0.032), ('climbing', 0.032), ('heights', 0.032), ('lexica', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="90-tfidf-1" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>2 0.40577587 <a title="90-tfidf-2" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>3 0.35356748 <a title="90-tfidf-3" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>4 0.28399491 <a title="90-tfidf-4" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>5 0.28319296 <a title="90-tfidf-5" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>6 0.23757531 <a title="90-tfidf-6" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>7 0.23218116 <a title="90-tfidf-7" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>8 0.20522971 <a title="90-tfidf-8" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>9 0.20428413 <a title="90-tfidf-9" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>10 0.16493978 <a title="90-tfidf-10" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>11 0.15638012 <a title="90-tfidf-11" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>12 0.14774141 <a title="90-tfidf-12" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>13 0.14374593 <a title="90-tfidf-13" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>14 0.13501769 <a title="90-tfidf-14" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>15 0.12594441 <a title="90-tfidf-15" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>16 0.1097034 <a title="90-tfidf-16" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>17 0.10893469 <a title="90-tfidf-17" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>18 0.10192777 <a title="90-tfidf-18" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>19 0.092689708 <a title="90-tfidf-19" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>20 0.086382315 <a title="90-tfidf-20" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.248), (1, -0.39), (2, -0.088), (3, -0.004), (4, 0.122), (5, 0.13), (6, -0.235), (7, 0.074), (8, 0.115), (9, -0.085), (10, -0.048), (11, -0.04), (12, -0.18), (13, 0.08), (14, -0.052), (15, 0.001), (16, 0.02), (17, 0.053), (18, 0.01), (19, -0.057), (20, 0.013), (21, 0.039), (22, 0.047), (23, -0.017), (24, 0.065), (25, -0.037), (26, -0.1), (27, 0.006), (28, 0.006), (29, 0.074), (30, 0.021), (31, -0.001), (32, -0.041), (33, 0.056), (34, -0.044), (35, -0.026), (36, -0.031), (37, -0.026), (38, 0.058), (39, -0.02), (40, 0.008), (41, -0.042), (42, 0.009), (43, 0.028), (44, -0.028), (45, -0.005), (46, 0.027), (47, -0.059), (48, 0.0), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9712491 <a title="90-lsi-1" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>2 0.85847032 <a title="90-lsi-2" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>3 0.85064209 <a title="90-lsi-3" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>4 0.84435946 <a title="90-lsi-4" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>5 0.82150108 <a title="90-lsi-5" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>6 0.78822976 <a title="90-lsi-6" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>7 0.72844797 <a title="90-lsi-7" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>8 0.67460364 <a title="90-lsi-8" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>9 0.65503287 <a title="90-lsi-9" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>10 0.59842527 <a title="90-lsi-10" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>11 0.57865721 <a title="90-lsi-11" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>12 0.53573322 <a title="90-lsi-12" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>13 0.50252813 <a title="90-lsi-13" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>14 0.43926722 <a title="90-lsi-14" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>15 0.43902853 <a title="90-lsi-15" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>16 0.417779 <a title="90-lsi-16" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>17 0.41283411 <a title="90-lsi-17" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>18 0.40974289 <a title="90-lsi-18" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>19 0.40589839 <a title="90-lsi-19" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>20 0.39755771 <a title="90-lsi-20" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.012), (18, 0.011), (25, 0.04), (33, 0.018), (42, 0.02), (55, 0.234), (59, 0.113), (73, 0.046), (78, 0.018), (80, 0.023), (83, 0.096), (98, 0.264)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86666495 <a title="90-lda-1" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>2 0.78778112 <a title="90-lda-2" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>Author: David Jurgens ; Keith Stevens</p><p>Abstract: We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</p><p>3 0.78365552 <a title="90-lda-3" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>Author: Duo Zhang ; Qiaozhu Mei ; ChengXiang Zhai</p><p>Abstract: Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way. One common deficiency of existing topic models, though, is that they would not work well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other. In this paper, we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages. Specifically, we propose a new topic model called Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA) which extends the Proba- bilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary. Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data.</p><p>4 0.78238529 <a title="90-lda-4" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>Author: Wenliang Chen ; Jun'ichi Kazama ; Kentaro Torisawa</p><p>Abstract: This paper proposes a dependency parsing method that uses bilingual constraints to improve the accuracy of parsing bilingual texts (bitexts). In our method, a targetside tree fragment that corresponds to a source-side tree fragment is identified via word alignment and mapping rules that are automatically learned. Then it is verified by checking the subtree list that is collected from large scale automatically parsed data on the target side. Our method, thus, requires gold standard trees only on the source side of a bilingual corpus in the training phase, unlike the joint parsing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because ofricher bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English.</p><p>5 0.77880293 <a title="90-lda-5" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>6 0.77789211 <a title="90-lda-6" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>7 0.77785122 <a title="90-lda-7" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>8 0.77599329 <a title="90-lda-8" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>9 0.77485621 <a title="90-lda-9" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>10 0.77115184 <a title="90-lda-10" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>11 0.77062881 <a title="90-lda-11" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>12 0.77048695 <a title="90-lda-12" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>13 0.76879877 <a title="90-lda-13" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>14 0.76796234 <a title="90-lda-14" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>15 0.76746082 <a title="90-lda-15" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>16 0.76549768 <a title="90-lda-16" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>17 0.76545084 <a title="90-lda-17" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>18 0.76538324 <a title="90-lda-18" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>19 0.76446527 <a title="90-lda-19" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>20 0.76347667 <a title="90-lda-20" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
