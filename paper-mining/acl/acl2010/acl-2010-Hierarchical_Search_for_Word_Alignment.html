<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>133 acl-2010-Hierarchical Search for Word Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-133" href="#">acl2010-133</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>133 acl-2010-Hierarchical Search for Word Alignment</h1>
<br/><p>Source: <a title="acl-2010-133-pdf" href="http://aclweb.org/anthology//P/P10/P10-1017.pdf">pdf</a></p><p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>Reference: <a title="acl-2010-133-reference" href="../acl2010_reference/acl-2010-Hierarchical_Search_for_Word_Alignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. [sent-2, score-0.5]
</p><p>2 We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. [sent-3, score-0.572]
</p><p>3 We report results on Arabic-English word alignment and translation tasks. [sent-4, score-0.425]
</p><p>4 1 Introduction Automatic word alignment is generally accepted as a first step in training any statistical machine translation system. [sent-8, score-0.425]
</p><p>5 Generative alignment models like IBM Model-4 (Brown et al. [sent-10, score-0.352]
</p><p>6 , 1993) have been in wide use for over 15 years, and while not perfect (see Figure 1), they are completely unsupervised, requiring no annotated training data to learn alignments that have powered many current state-of-the-art translation system. [sent-11, score-0.503]
</p><p>7 Today, there exist human-annotated alignments and an abundance of other information for many language pairs potentially useful for inducing accurate alignments. [sent-12, score-0.43]
</p><p>8 Circles represent links in a human-annotated alignment, and black boxes represent links in the Model-4 alignment. [sent-32, score-0.286]
</p><p>9 has motivated much recent work in discriminative modeling for word alignment (Moore, 2005; Ittycheriah and Roukos, 2005; Liu et al. [sent-34, score-0.449]
</p><p>10 We present in this paper a discriminative alignment model trained on relatively little data, with a simple, yet powerful hierarchical search procedure. [sent-39, score-0.56]
</p><p>11 Using a foreign string and an English parse tree as input, we formulate a bottom-up search on the parse tree, with the structure of the tree as a backbone for building a hypergraph of possible alignments. [sent-41, score-0.656]
</p><p>12 c As2s0o1c0ia Atisosnoc foiart Cionom fopru Ctaotmiopnuatla Lti on gaulis Lti cnsg,u piasgtiecs 157–16 ,  ‫ﺍﻟﺮﺍﳋﻛﺟﺒ ﻞﺰ ﺍﻟﺮﺍﳋﻛﺟﺒ ﻞﺰ‬  ‫ﺍﻟﺮﺍﳋﻛﺟﺒ ﻞﺰ‬  Figure 2: Example of approximate search through a hypergraph with beam size = 5. [sent-44, score-0.325]
</p><p>13 Each partial alignment at each node is ranked according to its model score. [sent-46, score-0.637]
</p><p>14 In this figure, we see that the partial alignment implied by the 1-best hypothesis at the leftmost NP node is constructed by composing the best hypothesis at the terminal node labeled “the” and the 2ndbest hypothesis at the terminal node labeled “man”. [sent-47, score-0.951]
</p><p>15 ) Hypotheses at the root node imply full alignment structures. [sent-49, score-0.472]
</p><p>16 We handle an arbitrary number of features, compute them efficiently, and score alignments using a linear model. [sent-51, score-0.539]
</p><p>17 Our model can generate arbitrary alignments and learn from arbitrary gold alignments. [sent-54, score-0.603]
</p><p>18 2  Word Alignment as a Hypergraph  Algorithm input The input to our alignment algorithm is a sentence-pair (en1, f1m) and a parse tree over one of the input sentences. [sent-55, score-0.498]
</p><p>19 Word alignments are built bottom-up on the parse tree. [sent-61, score-0.494]
</p><p>20 Each node v in the tree holds partial alignments sorted by score. [sent-62, score-0.797]
</p><p>21 Figure 3: Cube pruning with alignment hypotheses to select the top-k alignments at node v with children hu1,u2i. [sent-88, score-0.958]
</p><p>22 Each box represents  the combination of two partial alignments to create  a larger one. [sent-90, score-0.595]
</p><p>23 bE oaxc his b tohxe sum osefn nthtse t scores obifn nthateio cnh oildf alignments plus a cnotsm tboin caretiaoten  cost. [sent-92, score-0.43]
</p><p>24 Each partial alignment comprises the columns of the alignment matrix for the e-words spanned by v, and each is scored by a linear combination of feature functions. [sent-93, score-1.011]
</p><p>25 Initial partial alignments are enumerated and scored at preterminal nodes, each spanning a single column of the word alignment matrix. [sent-95, score-1.11]
</p><p>26 From here, we traverse the tree nodes bottomup, combining partial alignments from child nodes until we have constructed a single full alignment at  the root node of the tree. [sent-98, score-1.266]
</p><p>27 1 We use one set of feature functions for preterminal nodes, and another set for nonterminal nodes. [sent-100, score-0.246]
</p><p>28 This is analogous to local and nonlocal feature functions for parse-reranking used by Huang (2008). [sent-101, score-0.292]
</p><p>29 Using nonlocal features at a nonterminal node emits a combination cost for composing a set of child partial alignments. [sent-102, score-0.589]
</p><p>30 Because combination costs come into play, we use cube pruning (Chiang, 2007) to approximate the k-best combinations at some nonterminal node v. [sent-103, score-0.361]
</p><p>31 We find the oracle for a given (T,e,f) triple by proceeding through our search algorithm, forcing ourselves to always select correct links with respect to the gold alignment when possible, breaking ties arbitrarily. [sent-109, score-0.667]
</p><p>32 1 Hierarchical search Initial alignments We can construct a word alignment hierarchically, bottom-up, by making use of the structure inherent in syntactic parse trees. [sent-113, score-0.905]
</p><p>33 We can think of building a word alignment as filling in an M×N matrix (Figure 1), and we begin by visiting Me×achN preterminal nroe 1de), i ann tdh we tree. [sent-114, score-0.464]
</p><p>34 At this level of the tree the span size is 1, and the par-  tial alignment we have made spans a single column ofthe matrix. [sent-118, score-0.493]
</p><p>35 We can make many such partial alignments depending on the links selected. [sent-119, score-0.71]
</p><p>36 Each partial alignment is scored and stored in a sorted heap (Lines 9 and 13). [sent-121, score-0.652]
</p><p>37 We limit the number of total partial alignments αv kept at each node to k. [sent-123, score-0.715]
</p><p>38 If at any time we wish to push onto the heap a new partial alignment when the heap is full, we pop the current worst off the heap and replace it with our new partial alignment if its score is better than the current worst. [sent-124, score-1.385]
</p><p>39 Building the hypergraph We now visit internal nodes (Line 16) in the tree in bottom-up order. [sent-125, score-0.319]
</p><p>40 At each nonterminal node v we wish to combine the partial alignments of its children u1, . [sent-126, score-0.82]
</p><p>41 We use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to select the k-best combinations of the partial alignments of u1,. [sent-130, score-0.768]
</p><p>42 H() * Figure 4: Correct version of Figure 1 after hypergraph alignment. [sent-146, score-0.199]
</p><p>43 In the general case, cube pruning will operate on a d-dimensional hypercube, where d is the branching factor of node v. [sent-149, score-0.293]
</p><p>44 We cannot enumerate and score every possibility; without the cube pruning approximation, we will have kc possible combinations at each node, exploding the search space exponentially. [sent-150, score-0.321]
</p><p>45 Figure 3 depicts how we select the top-k alignments at a node v from its children h u1, u2 i. [sent-151, score-0.55]
</p><p>46 3  Discriminative training  We incorporate all our new features into a linear model and learn weights for each using the online averaged perceptron algorithm (Collins, 2002) with a few modifications for structured outputs inspired by Chiang et al. [sent-152, score-0.209]
</p><p>47 We define: 2We find empirically that using binarized trees reduces search errors in cube pruning. [sent-154, score-0.176]
</p><p>48 Figure 5: A common problem with GIZA++ Model 4 alignments is a weak distortion model. [sent-165, score-0.482]
</p><p>49 We select the oracle alignment according to:  y+ =ay ∈rg m(ixn)γ(y)  (2)  where (x) is a set of hypothesis alignments generated from input x. [sent-170, score-0.862]
</p><p>50 Our hierarchical search framework allows us to compute these features when needed, and affords us extra useful syntactic information. [sent-178, score-0.234]
</p><p>51 Huang (2008) defines a feature h to be local if and only if it can be factored among the local productions in a tree, and non-local otherwise. [sent-180, score-0.192]
</p><p>52 Analogously for alignments, our class of local features are those that can be factored among the local  partial alignments competing to comprise a larger span of the matrix, and non-local otherwise. [sent-181, score-0.86]
</p><p>53 These features score a set of links and the words connected by them. [sent-182, score-0.24]
</p><p>54 Feature development Our features are inspired by analysis of patterns contained among our gold alignment data and automatically generated parse trees. [sent-183, score-0.549]
</p><p>55 We use both local lexical and nonlocal structural features as described below. [sent-184, score-0.262]
</p><p>56 Negative weights essentially penalize alignments with links never seen before in the Model 4 alignment, and positive weights encourage such links. [sent-191, score-0.545]
</p><p>57 Critically, this feature tells us how much to trust alignments involving nouns, verbs, adjectives, function words, punctuation, etc. [sent-194, score-0.488]
</p><p>58 from the Model 4 alignments from which our p(e | f) and p(f | e) tables are built. [sent-195, score-0.43]
</p><p>59 TIan-tuitively, alignments involving English partsof-speech more likely to be content words (e. [sent-197, score-0.43]
</p><p>60 ehead  f 6: Features PP-NP-head, NP-DT-head, and VP-VP-head fire on these tree-alignment patterns. [sent-208, score-0.239]
</p><p>61 For example, PP-NP-head fires exactly when the head of the PP is aligned to exactly the same f words as the Figure  head of it’s sister NP. [sent-209, score-0.38]
</p><p>62 Negative weights penalize links never seen before in a baseline alignment used to initialize lexical p(e | f) and p(f | e) tables. [sent-239, score-0.467]
</p><p>63 TWheis aflesaotu irnec returns t mhee adsiustraenc oef to tshteo diagonal of the matrix for any link in a partial alignment. [sent-245, score-0.252]
</p><p>64 Although local features do not know the partial alignments at other spans, they do have access to the entire English sentence at every step because our input is constant. [sent-250, score-0.734]
</p><p>65 If some e exists more than once in e1n we fire this feature on all links containing word e, returning again the distance to the diagonal for that link. [sent-251, score-0.369]
</p><p>66 Punctuation-mismatch fires on any link that causes nonpunctuation to be aligned to punctuation. [sent-255, score-0.227]
</p><p>67 These fire for for each link (e, f) and partof-speech tag. [sent-257, score-0.203]
</p><p>68 Given the tag of e, this affords the model the ability to pay more or less attention to the features described above depending on the tag given to e. [sent-259, score-0.207]
</p><p>69 Any aligned words in the span of the sister NP are aligned to words following Æ? [sent-273, score-0.335]
</p><p>70 2 Nonlocal features These features comprise the combination cost component of a partial alignment score and may fire when concatenating two partial alignments  to create a larger span. [sent-285, score-1.461]
</p><p>71 Because these features can look into any two arbitrary subtrees, they are considered nonlocal features as defined by Huang (2008). [sent-286, score-0.323]
</p><p>72 Likewise, we observe the head of a VP to align to the head of an immediate sister VP. [sent-289, score-0.298]
</p><p>73 ,  163 In Figure 4, when the search arrives at the left-most NPB node, the NP-DT-head  fea-  ture will fire given this structure and links over the span [the  . [sent-309, score-0.428]
</p><p>74 When  search arrives at the second NPB node, it will fire given the structure and links over the  span [the . [sent-313, score-0.428]
</p><p>75 However, we also introduce nonlocal lexicalized features for the most common types of English and foreign prepositions to also compete with these general headword features. [sent-318, score-0.381]
</p><p>76 PP features PP-of-prep, PP-from-prep, PPto-prep, PP-on-prep, and PP-in-prep fire at any PP whose left child is a preposition and right child is an NP. [sent-319, score-0.347]
</p><p>77 The head of the PP is one ofthe enumerated English prepositions and is aligned to any of the three most common foreign words to which it has also been observed aligned in the gold alignments. [sent-320, score-0.476]
</p><p>78 The last constraint on this pattern is that all words under the span of the sister NP, if aligned, must align to words following the foreign preposition. [sent-321, score-0.357]
</p><p>79 For any pair of links (ei, f) and (ej, f) in which the e words differ but the f word is the same token in each, return the tree height of first common ancestor of ei and ej. [sent-325, score-0.237]
</p><p>80 This feature captures the intuition that it is much worse to align two English words at different ends of the tree to the same foreign word, than it is to align two English words under the same NP to the same foreign word. [sent-326, score-0.54]
</p><p>81 To see why a string distance feature that counts only the flat horizontal distance from ei to ej is not the best strategy, consider the following. [sent-327, score-0.182]
</p><p>82 We wish to align a determiner to the same f word as its sister head noun under the same NP. [sent-328, score-0.329]
</p><p>83 A string distance metric, with no knowledge of the relationship between determiner and noun will levy a much heavier penalty than its tree distance analog. [sent-330, score-0.217]
</p><p>84 Very recent work in word alignment has also started to report downstream effects on BLEU score. [sent-332, score-0.352]
</p><p>85 DeNero and Klein (2007) refine the distortion model of an HMM aligner to reflect tree distance instead of string distance. [sent-338, score-0.217]
</p><p>86 (2008) start with the output from GIZA++ Model-4 union, and focus on increasing precision by deleting links based on a linear discriminative model exposed to syntactic and lexical information. [sent-340, score-0.212]
</p><p>87 6 Experiments We evaluate our model and and resulting alignments on Arabic-English data against those induced by IBM Model-4 using GIZA++ (Och and Ney, 2003) with both the union and grow-diagfinal heuristics. [sent-343, score-0.485]
</p><p>88 We use 1,000 sentence pairs and gold alignments from LDC2006E86 to train model  parameters: 800 sentences for training, 100 for testing, and 100 as a second held-out development set to decide when to stop perceptron training. [sent-344, score-0.586]
</p><p>89 Training epoch  Figure 8: Learning curves for 10 random restarts  over time for parallel averaged perceptron training. [sent-347, score-0.273]
</p><p>90 7 6 76 392175408Model1HM odel4 Initial alignments  Figure 9: Model robustness to the initial alignments from which the p(e | f) and p(f | e) features are ndtserfirvoemd. [sent-353, score-0.932]
</p><p>91 The first three columns of Table 2 show the balanced F-measure, Precision, and Recall of our alignments versus the two GIZA++ Model-4 baselines. [sent-359, score-0.43]
</p><p>92 164 F  P  R  Arabic/English  # Unknown  BLEU M4 (union) M4 (grow-diag-final) Hypergraph alignment Table 2: F-measure,  45. [sent-363, score-0.352]
</p><p>93 Figure 8 shows the stability of the search procedure over ten random restarts of parallel averaged perceptron training with 40 CPUs. [sent-381, score-0.281]
</p><p>94 Figure 9 shows the robustness of the model to initial alignments used to derive lexical features p(e | f) and p(f | e). [sent-383, score-0.502]
</p><p>95 In addition to IBM Model 4, we experiment wf |it eh) alignments nfr toom IB BMMo dMelo 1d aln 4d, the HMM model. [sent-384, score-0.43]
</p><p>96 In each case, we significantly outperform the baseline GIZA++ Model 4 alignments on a heldout test set. [sent-385, score-0.473]
</p><p>97 We align the same core subset with our trained hypergraph alignment model, and extract a second set of translation rules. [sent-389, score-0.718]
</p><p>98 4 BLEU increase over a system trained with alignments from Model-4 union. [sent-404, score-0.43]
</p><p>99 7  Conclusion  We have opened up the word alignment task to advances in hypergraph algorithms currently used in parsing and machine translation decoding. [sent-405, score-0.624]
</p><p>100 We treat word alignment as a parsing problem, and by taking advantage of English syntax and the hypergraph structure of our search algorithm, we report significant increases in both F-measure and BLEU score over standard baselines in use by most state-of-the-art MT systems today. [sent-406, score-0.663]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alignments', 0.43), ('alignment', 0.352), ('hypergraph', 0.199), ('partial', 0.165), ('fire', 0.152), ('nonlocal', 0.123), ('node', 0.12), ('giza', 0.118), ('cube', 0.117), ('links', 0.115), ('foreign', 0.106), ('bleu', 0.102), ('sister', 0.098), ('discriminative', 0.097), ('perceptron', 0.095), ('align', 0.094), ('chiang', 0.093), ('aligned', 0.089), ('huang', 0.089), ('heap', 0.087), ('ehead', 0.087), ('fires', 0.087), ('tree', 0.082), ('oracle', 0.08), ('npb', 0.076), ('preterminal', 0.076), ('itg', 0.075), ('arabic', 0.074), ('translation', 0.073), ('features', 0.072), ('forest', 0.07), ('ay', 0.069), ('nonterminal', 0.068), ('beam', 0.067), ('local', 0.067), ('parse', 0.064), ('fertility', 0.062), ('gold', 0.061), ('np', 0.06), ('span', 0.059), ('search', 0.059), ('sydney', 0.059), ('feature', 0.058), ('riesa', 0.058), ('boxes', 0.056), ('pruning', 0.056), ('arbitrary', 0.056), ('vancouver', 0.056), ('union', 0.055), ('galley', 0.054), ('taskar', 0.054), ('score', 0.053), ('pp', 0.053), ('head', 0.053), ('denero', 0.052), ('distortion', 0.052), ('hierarchical', 0.052), ('moore', 0.051), ('link', 0.051), ('randomized', 0.051), ('affords', 0.051), ('epoch', 0.051), ('collins', 0.049), ('ibm', 0.048), ('scored', 0.048), ('klein', 0.048), ('determiner', 0.047), ('restarts', 0.046), ('marcu', 0.044), ('distance', 0.044), ('english', 0.044), ('functions', 0.044), ('circles', 0.043), ('arrives', 0.043), ('heldout', 0.043), ('fossum', 0.043), ('mt', 0.043), ('tag', 0.042), ('averaged', 0.042), ('headword', 0.041), ('ittycheriah', 0.041), ('preposition', 0.041), ('child', 0.041), ('return', 0.04), ('prepositions', 0.039), ('talbot', 0.039), ('aligner', 0.039), ('enumerated', 0.039), ('nns', 0.039), ('parallel', 0.039), ('iterations', 0.038), ('nodes', 0.038), ('wish', 0.037), ('terminal', 0.037), ('ej', 0.036), ('enumerate', 0.036), ('isi', 0.036), ('meeting', 0.036), ('matrix', 0.036), ('annual', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="133-tfidf-1" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>2 0.40577587 <a title="133-tfidf-2" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>3 0.38016853 <a title="133-tfidf-3" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>4 0.36561239 <a title="133-tfidf-4" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>5 0.32423636 <a title="133-tfidf-5" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>6 0.27734733 <a title="133-tfidf-6" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>7 0.25565025 <a title="133-tfidf-7" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>8 0.24875855 <a title="133-tfidf-8" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>9 0.20440255 <a title="133-tfidf-9" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>10 0.20328681 <a title="133-tfidf-10" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>11 0.19635646 <a title="133-tfidf-11" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>12 0.19334736 <a title="133-tfidf-12" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>13 0.1891803 <a title="133-tfidf-13" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>14 0.17310935 <a title="133-tfidf-14" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>15 0.15696515 <a title="133-tfidf-15" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>16 0.15100689 <a title="133-tfidf-16" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>17 0.1369759 <a title="133-tfidf-17" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>18 0.13426889 <a title="133-tfidf-18" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>19 0.13389474 <a title="133-tfidf-19" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>20 0.11943664 <a title="133-tfidf-20" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.366), (1, -0.43), (2, -0.012), (3, -0.005), (4, 0.037), (5, 0.106), (6, -0.151), (7, 0.122), (8, 0.103), (9, -0.128), (10, -0.127), (11, -0.161), (12, -0.161), (13, 0.058), (14, -0.075), (15, 0.012), (16, 0.042), (17, 0.006), (18, 0.004), (19, -0.026), (20, 0.052), (21, 0.074), (22, 0.035), (23, -0.026), (24, -0.007), (25, 0.02), (26, 0.001), (27, 0.021), (28, 0.03), (29, 0.045), (30, -0.016), (31, -0.052), (32, -0.005), (33, 0.062), (34, -0.042), (35, -0.067), (36, -0.056), (37, 0.01), (38, -0.007), (39, 0.003), (40, -0.057), (41, 0.049), (42, -0.018), (43, 0.013), (44, 0.006), (45, 0.002), (46, -0.013), (47, -0.002), (48, 0.029), (49, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97064424 <a title="133-lsi-1" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>2 0.91702807 <a title="133-lsi-2" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>3 0.90122378 <a title="133-lsi-3" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>4 0.89380544 <a title="133-lsi-4" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>Author: Shujie Liu ; Chi-Ho Li ; Ming Zhou</p><p>Abstract: While Inversion Transduction Grammar (ITG) has regained more and more attention in recent years, it still suffers from the major obstacle of speed. We propose a discriminative ITG pruning framework using Minimum Error Rate Training and various features from previous work on ITG alignment. Experiment results show that it is superior to all existing heuristics in ITG pruning. On top of the pruning framework, we also propose a discriminative ITG alignment model using hierarchical phrase pairs, which improves both F-score and Bleu score over the baseline alignment system of GIZA++. 1</p><p>5 0.88013548 <a title="133-lsi-5" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>6 0.86515468 <a title="133-lsi-6" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>7 0.69749546 <a title="133-lsi-7" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>8 0.69737297 <a title="133-lsi-8" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>9 0.64445955 <a title="133-lsi-9" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>10 0.64132464 <a title="133-lsi-10" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>11 0.59659529 <a title="133-lsi-11" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>12 0.5937106 <a title="133-lsi-12" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>13 0.52166754 <a title="133-lsi-13" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>14 0.47676373 <a title="133-lsi-14" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>15 0.46314347 <a title="133-lsi-15" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>16 0.44962507 <a title="133-lsi-16" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>17 0.44769073 <a title="133-lsi-17" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>18 0.44735181 <a title="133-lsi-18" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>19 0.44481146 <a title="133-lsi-19" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>20 0.42895526 <a title="133-lsi-20" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.134), (14, 0.034), (18, 0.012), (25, 0.089), (33, 0.018), (39, 0.015), (42, 0.017), (44, 0.011), (59, 0.141), (73, 0.051), (76, 0.025), (78, 0.038), (83, 0.104), (84, 0.024), (98, 0.219)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9651981 <a title="133-lda-1" href="./acl-2010-Wrapping_up_a_Summary%3A_From_Representation_to_Generation.html">264 acl-2010-Wrapping up a Summary: From Representation to Generation</a></p>
<p>Author: Josef Steinberger ; Marco Turchi ; Mijail Kabadjov ; Ralf Steinberger ; Nello Cristianini</p><p>Abstract: The main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries. This is motivated by empirical evidence from TAC 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries. We report encouraging preliminary results comparable to those attained by participating systems at TAC 2009.</p><p>same-paper 2 0.9337101 <a title="133-lda-2" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>3 0.9020828 <a title="133-lda-3" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>Author: Trung H. Bui ; Stanley Peters</p><p>Abstract: We investigate hierarchical graphical models (HGMs) for automatically detecting decisions in multi-party discussions. Several types of dialogue act (DA) are distinguished on the basis of their roles in formulating decisions. HGMs enable us to model dependencies between observed features of discussions, decision DAs, and subdialogues that result in a decision. For the task of detecting decision regions, an HGM classifier was found to outperform non-hierarchical graphical models and support vector machines, raising the F1-score to 0.80 from 0.55.</p><p>4 0.89774823 <a title="133-lda-4" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>Author: Xianpei Han ; Jun Zhao</p><p>Abstract: Name ambiguity problem has raised urgent demands for efficient, high-quality named entity disambiguation methods. In recent years, the increasing availability of large-scale, rich semantic knowledge sources (such as Wikipedia and WordNet) creates new opportunities to enhance the named entity disambiguation by developing algorithms which can exploit these knowledge sources at best. The problem is that these knowledge sources are heterogeneous and most of the semantic knowledge within them is embedded in complex structures, such as graphs and networks. This paper proposes a knowledge-based method, called Structural Semantic Relatedness (SSR), which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge sources. Empirical results show that, in comparison with the classical BOW based methods and social network based methods, our method can significantly improve the disambiguation performance by respectively 8.7% and 14.7%. 1</p><p>5 0.89522612 <a title="133-lda-5" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and treesubstitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.</p><p>6 0.89298856 <a title="133-lda-6" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>7 0.89056444 <a title="133-lda-7" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>8 0.89050484 <a title="133-lda-8" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>9 0.8868649 <a title="133-lda-9" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>10 0.88651133 <a title="133-lda-10" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>11 0.88651031 <a title="133-lda-11" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>12 0.88631499 <a title="133-lda-12" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>13 0.88468313 <a title="133-lda-13" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>14 0.88461792 <a title="133-lda-14" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>15 0.88386858 <a title="133-lda-15" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>16 0.88353968 <a title="133-lda-16" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>17 0.88342559 <a title="133-lda-17" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>18 0.88340735 <a title="133-lda-18" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>19 0.88331729 <a title="133-lda-19" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>20 0.88242483 <a title="133-lda-20" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
