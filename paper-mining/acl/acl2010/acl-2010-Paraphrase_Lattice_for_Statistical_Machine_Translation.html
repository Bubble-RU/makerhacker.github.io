<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-192" href="#">acl2010-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2010-192-pdf" href="http://aclweb.org/anthology//P/P10/P10-2001.pdf">pdf</a></p><p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.</p><p>Reference: <a title="acl-2010-192-reference" href="../acl2010_reference/acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jp chi  Abstract Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. [sent-5, score-0.727]
</p><p>2 We show that lattice decoding is also useful for handling input variations. [sent-6, score-0.704]
</p><p>3 Given an input sentence, we build a lattice which represents paraphrases of the input sentence. [sent-7, score-0.844]
</p><p>4 Then, we give the paraphrase lattice as an input to the lattice decoder. [sent-9, score-1.658]
</p><p>5 Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets. [sent-11, score-0.764]
</p><p>6 1 Introduction Lattice decoding in SMT is useful in speech translation and in the translation of German (Bertoldi et al. [sent-12, score-0.378]
</p><p>7 In speech translation, by using lattices that represent not only 1-best result but also other possibilities of speech recognition, we can take into account the ambiguities of speech recognition. [sent-14, score-0.312]
</p><p>8 Thus, the translation quality for lattice inputs is better than the quality for 1best inputs. [sent-15, score-0.623]
</p><p>9 In this paper, we show that lattice decoding is also useful for handling input variations. [sent-16, score-0.704]
</p><p>10 “Input variations” refers to the differences of input texts with the same meaning. [sent-17, score-0.042]
</p><p>11 ” have the same meaning with variations in “beauty salon ” and “beauty parlor”. [sent-20, score-0.243]
</p><p>12 Since these variations are frequently found in natural language texts, a mismatch of the expressions in source sentences and the expressions in training corpus leads to a decrease in translation quality. [sent-21, score-0.212]
</p><p>13 Therefore, 1  we propose a novel method that can handle input variations using paraphrases and lattice decoding. [sent-22, score-0.839]
</p><p>14 In the proposed method, we regard a given source sentence as one ofmany variations (1-best). [sent-23, score-0.143]
</p><p>15 Given an input sentence, we build a paraphrase lattice which represents paraphrases of the input sentence. [sent-24, score-1.444]
</p><p>16 Then, we give the paraphrase lattice as an input to the Moses decoder (Koehn et al. [sent-25, score-1.199]
</p><p>17 By using paraphrases of source sentences, we can translate expressions which are not found in a training corpus on the condition that paraphrases of them are found in the training corpus. [sent-28, score-0.493]
</p><p>18 Moreover, by using lattice decoding, we can employ the source-side language model as a decoding feature. [sent-29, score-0.643]
</p><p>19 Since this feature is affected by the source-side context, the decoder can choose a proper paraphrase and translate correctly. [sent-30, score-0.697]
</p><p>20 This paper is organized as follows: Related works on lattice decoding and paraphrasing are presented in Section 2. [sent-31, score-0.778]
</p><p>21 2  Related Work  Lattice decoding has been used to handle ambiguities of preprocessing. [sent-35, score-0.212]
</p><p>22 (2007) employed a confusion network, which is a kind of lattice and represents speech recognition hypotheses in speech translation. [sent-37, score-0.647]
</p><p>23 Dyer (2009) also employed a segmentation lattice, which represents ambiguities of compound word segmentation in German, Hungarian and Turkish translation. [sent-38, score-0.152]
</p><p>24 However, to the best of our knowledge, there is no work which employed a lattice representing paraphrases of an input sentence. [sent-39, score-0.772]
</p><p>25 On the other hand, paraphrasing has been used to enrich the SMT model. [sent-40, score-0.135]
</p><p>26 1c 02 C01o0n Afesresonc ieat Siho nr fto Pra Cpoemrsp,u ptagateison 1a–l5 L,inguistics Input  sentence  (Pfoar apaleral Cphor apsues)ParaLpishtraseParaphrasing  Paraphrase Lattice  Pa(froarl teral Cinoinrpgu)sSMT modelLattice Decoding Output sentence Figure 1: Overview of the proposed method. [sent-43, score-0.094]
</p><p>27 (2009) augmented the translation phrase table with paraphrases to translate unknown phrases. [sent-46, score-0.443]
</p><p>28 (2008) and Nakov (2008) augmented the training data by paraphrasing. [sent-48, score-0.036]
</p><p>29 However, there is no work which augments input sentences by paraphrasing and represents them in lattices. [sent-49, score-0.202]
</p><p>30 3  Paraphrase Lattice for SMT  Overview of the proposed method is shown in Figure 1. [sent-50, score-0.052]
</p><p>31 In advance, we automatically acquire a paraphrase list from a parallel corpus. [sent-51, score-0.707]
</p><p>32 In order to  acquire paraphrases of unknown phrases, this parallel corpus is different from the parallel corpus for training. [sent-52, score-0.38]
</p><p>33 Given an input sentence, we build a lattice which represents paraphrases of the input sentence using the paraphrase list. [sent-53, score-1.478]
</p><p>34 Then, we give the paraphrase lattice to the lattice decoder. [sent-55, score-1.616]
</p><p>35 1 Acquiring the paraphrase list We acquire a paraphrase list using Bannard and Callison-Burch (2005)’s method. [sent-57, score-1.289]
</p><p>36 Their idea is, if two different phrases e1, e2 in one language are aligned to the same phrase c in another language, they are hypothesized to be paraphrases of each other. [sent-58, score-0.289]
</p><p>37 Build a phrase table from parallel corpus using standard SMT techniques. [sent-62, score-0.12]
</p><p>38 The phrase table built in 1has many inappropriate phrase pairs. [sent-65, score-0.126]
</p><p>39 Therefore, we filter the  2 phrase table and keep only appropriate phrase pairs using the sigtest-filter (Johnson et al. [sent-66, score-0.162]
</p><p>40 Calculate the paraphrase probability p(e2 |e1) if e2 is hypothesized to be a paraphrase of| e1. [sent-70, score-1.223]
</p><p>41 p(e2|e1)  = ∑P(c|e1)P(e2|c) ∑c  where P(· | ·) is phrase translation probability. [sent-71, score-0.157]
</p><p>42 Acquire (e1, e2) as a paraphrase pair if p(e2 |e1) > p(e1 |e1). [sent-74, score-0.6]
</p><p>43 The purpose of this thres|heold is to keep highly-accurate paraphrase pairs. [sent-75, score-0.6]
</p><p>44 In experiments, more than 80% of paraphrase pairs were eliminated by this threshold. [sent-76, score-0.617]
</p><p>45 2 Building paraphrase lattice An input sentence is paraphrased using the paraphrase list and transformed into a paraphrase lat-  tice. [sent-78, score-2.524]
</p><p>46 The paraphrase lattice is a lattice which represents paraphrases of the input sentence. [sent-79, score-1.886]
</p><p>47 An example of a paraphrase lattice is shown in Figure 2. [sent-80, score-1.108]
</p><p>48 In this example, an input sentence is “is there a beauty salon ? [sent-81, score-0.476]
</p><p>49 This paraphrase lattice contains two paraphrase pairs “beauty salon ” = “beauty parlor” and “beauty salon ” = “salon ”, and represents following three sentences. [sent-83, score-2.154]
</p><p>50 In the paraphrase lattice, each node consists of a token, the distance to the next node and features for lattice decoding. [sent-87, score-1.182]
</p><p>51 •  Paraphrase probability (p) A paraphrase probability p(e2 |e1) calculated when acquiring the paraphrase. [sent-89, score-0.624]
</p><p>52 hp  •  = p(e2|e1)  Language model score (l)  A ratio between the language model probability of the paraphrased sentence (para) and that of the original sentence (orig). [sent-90, score-0.214]
</p><p>53 hl  =  0 -- (" i s"  llmm((oprairga))  , 1,, 1,, 1,, 1)  "t 1 -- ("there"  "a  , 1, 1,, 1, 1)  6534 - ( " bp? [sent-91, score-0.03]
</p><p>54 )36ed7n,gct3oh)d(in)g Figure  •  2: An example  of a paraphrase lattice, which contains three features of (p, l, d). [sent-97, score-0.624]
</p><p>55 Normalized language model score (L) A language model score where the language model probability is normalized by the sentence length. [sent-98, score-0.09]
</p><p>56 The sentence length is calculated as the number of tokens. [sent-99, score-0.061]
</p><p>57 hL= LLMM((oprairga)),  1  where LM(sent) = lm(sent) length(sent) • Paraphrase length (d) The difference between the original sentence length and the paraphrased sentence length. [sent-100, score-0.24]
</p><p>58 hd = exp(length(para) −length(orig)) The values of these features are calculated only  if the node is the first node of the paraphrase, for example the second “beauty” and “salon ” in line 3 of Figure 2. [sent-101, score-0.074]
</p><p>59 The features related to the language model, such as (l) and (L), are affected by the context of source sentences even if the same paraphrase pair is applied. [sent-103, score-0.665]
</p><p>60 As these features can penalize paraphrases which are not appropriate to the context, appropriate paraphrases are chosen and appropriate translations are output in lattice decoding. [sent-104, score-1.02]
</p><p>61 The features related to the sentence length, such as (L) and (d), are added to penalize the language model score in case the paraphrased sentence length is shorter than the original sentence length and the language model score is unreasonably low. [sent-105, score-0.379]
</p><p>62 In experiments, we use four combinations of these features, (p), (p, l), (p, L) and (p, l, d). [sent-106, score-0.021]
</p><p>63 Moses is an open source 3  SMT system which allows lattice decoding. [sent-110, score-0.53]
</p><p>64 In lattice decoding, Moses selects the best path and the best translation according to features added in each node and other SMT features. [sent-111, score-0.694]
</p><p>65 4 Experiments In order to evaluate the proposed method, we conducted English-to-Japanese and English-toChinese translation experiments using IWSLT 2007 (Fordyce, 2007) dataset. [sent-113, score-0.146]
</p><p>66 This dataset contains EJ and EC parallel corpus for the travel domain and consists of 40k sentences for training and about 500 sentences sets (dev1, dev2 and dev3) for development and testing. [sent-114, score-0.076]
</p><p>67 We used the dev1 set for parameter tuning, the dev2 set for choosing the setting of the proposed method, which is described below, and the dev3 set for testing. [sent-115, score-0.026]
</p><p>68 The English-English paraphrase list was acquired from the EC corpus for EJ translation and 53K pairs were acquired. [sent-116, score-0.805]
</p><p>69 Similarly, 47K pairs  were acquired from the EJ corpus for EC translation. [sent-117, score-0.089]
</p><p>70 In CCB, we paraphrased the phrase table using the automatically acquired paraphrase list. [sent-122, score-0.836]
</p><p>71 Then, we augmented the phrase table with paraphrased phrases which were not found in the original phrase table. [sent-123, score-0.28]
</p><p>72 Moreover, we used an additional feature whose value was the paraphrase probability (p) if the entry was generated by paraphrasing and Moses(w/o Paraphrases)CCBProposed Method EJ38. [sent-124, score-0.735]
</p><p>73 Weights of the feature and other features in SMT were optimized using MERT. [sent-136, score-0.024]
</p><p>74 2 Proposed method In the proposed method, we conducted experiments with various settings for paraphrasing and lattice decoding. [sent-138, score-0.739]
</p><p>75 1 Limitation of paraphrasing As the paraphrase list was automatically acquired, there were many erroneous paraphrase pairs. [sent-142, score-1.383]
</p><p>76 Building paraphrase lattices with all erroneous paraphrase pairs and decoding these paraphrase lattices caused high computational complexity. [sent-143, score-2.27]
</p><p>77 Therefore, we limited the number of paraphrasing per phrase and per sentence. [sent-144, score-0.198]
</p><p>78 The number ofparaphrasing per phrase was limited to three and the number of paraphrasing per sentence was limited to twice the size of the sentence length. [sent-145, score-0.266]
</p><p>79 As a criterion for limiting the number of paraphrasing, we use three features (p), (l) and (L), which are same as the features described in Subsection 3. [sent-146, score-0.076]
</p><p>80 When building paraphrase lattices, we apply paraphrases in descending order of the value of the criterion. [sent-148, score-0.822]
</p><p>81 2 Finding optimal settings As previously mentioned, we have three choices for the criterion for building paraphrase lattices and four combinations of features for lattice decoding. [sent-151, score-1.364]
</p><p>82 Thus, there are 3 4 = 12 combinations  cofo dthinegse. [sent-152, score-0.021]
</p><p>83 rWee a rceon 3d ×uc 4te =d parameter tuning with the dev1 set for each setting and used as best the setting which got the highest BLEU score for the dev2 set. [sent-154, score-0.045]
</p><p>84 In EJ translation, the proposed method obtained the highest score of 40. [sent-158, score-0.08]
</p><p>85 In EC translation, the proposed method also obtained the highest score of 27. [sent-162, score-0.08]
</p><p>86 As the relation of three systems is Moses < CCB < Proposed Method, paraphrasing is useful for SMT and using paraphrase lattices and lattice decoding is especially more useful than augmenting the phrase table. [sent-166, score-1.65]
</p><p>87 In Proposed Method, the criterion for  building paraphrase lattices and the combination of features for lattice decoding were (p) and (p, L) in EJ translation and (L) and (p, l) in EC translation. [sent-167, score-1.554]
</p><p>88 Since features related to the source-side language model were chosen in each direction, using the source-side language model is useful for decoding paraphrase lattices. [sent-168, score-0.778]
</p><p>89 We also tried a combination of Proposed Method and CCB, which is a method of decoding paraphrase lattices with an augmented phrase table. [sent-169, score-1.006]
</p><p>90 This is because the proposed method includes the effect of augmenting the phrase table. [sent-171, score-0.14]
</p><p>91 Moreover, we conducted German-English translation using the Europarl corpus (Koehn, 2005). [sent-172, score-0.137]
</p><p>92 3M pairs of German-German paraphrases from a 1M German-Spanish parallel corpus. [sent-175, score-0.26]
</p><p>93 We conducted experiments with various sizes of training corpus, using 10K, 20K, 40K, 80K, 160K and 1M. [sent-176, score-0.026]
</p><p>94 Figure 3 shows the proposed method consistently  get higher score than Moses and CCB. [sent-177, score-0.08]
</p><p>95 5  Conclusion  This paper has proposed a novel method for transforming a source sentence into a paraphrase lattice and applying lattice decoding. [sent-178, score-1.724]
</p><p>96 Since our method can employ source-side language models as a decoding feature, the decoder can choose proper paraphrases and translate properly. [sent-179, score-0.442]
</p><p>97 36 BLEU points over Moses in EJ translation and 1. [sent-182, score-0.125]
</p><p>98 In Europarl dataset, the proposed  method consistently get higher score than baselines. [sent-187, score-0.08]
</p><p>99 In future work, we plan to apply this method with paraphrases derived from a massive corpus such as the Web corpus and apply this method to a hierarchical phrase based SMT. [sent-188, score-0.352]
</p><p>100 Using a maximum entropy model to build segmentation lattices for MT. [sent-207, score-0.195]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('paraphrase', 0.6), ('lattice', 0.508), ('paraphrases', 0.203), ('salon', 0.202), ('beauty', 0.198), ('lattices', 0.146), ('moses', 0.142), ('iwslt', 0.137), ('paraphrasing', 0.135), ('decoding', 0.135), ('paraphrased', 0.118), ('parlor', 0.112), ('translation', 0.094), ('ej', 0.084), ('bleu', 0.079), ('europarl', 0.079), ('smt', 0.076), ('ccb', 0.072), ('phrase', 0.063), ('ambiguities', 0.058), ('acquired', 0.055), ('ec', 0.052), ('decoder', 0.049), ('acquire', 0.045), ('llmm', 0.045), ('oprairga', 0.045), ('orig', 0.045), ('input', 0.042), ('bertoldi', 0.041), ('variations', 0.041), ('parallel', 0.04), ('chris', 0.04), ('bond', 0.039), ('bannard', 0.039), ('koehn', 0.037), ('para', 0.036), ('speech', 0.036), ('augmented', 0.036), ('sent', 0.035), ('sentence', 0.034), ('points', 0.031), ('hl', 0.03), ('translate', 0.029), ('criterion', 0.028), ('dyer', 0.028), ('score', 0.028), ('length', 0.027), ('marton', 0.027), ('method', 0.026), ('german', 0.026), ('proposed', 0.026), ('erroneous', 0.026), ('conducted', 0.026), ('represents', 0.025), ('segmentation', 0.025), ('augmenting', 0.025), ('penalize', 0.025), ('node', 0.025), ('build', 0.024), ('acquiring', 0.024), ('features', 0.024), ('philipp', 0.024), ('selects', 0.024), ('hypothesized', 0.023), ('confusion', 0.023), ('source', 0.022), ('marcello', 0.022), ('list', 0.022), ('lm', 0.021), ('combinations', 0.021), ('inputs', 0.021), ('summit', 0.021), ('ofmany', 0.02), ('sumita', 0.02), ('darren', 0.02), ('dra', 0.02), ('ecai', 0.02), ('onishi', 0.02), ('handle', 0.019), ('useful', 0.019), ('nicola', 0.019), ('zens', 0.019), ('dataset', 0.019), ('expressions', 0.019), ('path', 0.019), ('employed', 0.019), ('appropriate', 0.019), ('affected', 0.019), ('building', 0.019), ('overview', 0.018), ('unknown', 0.018), ('gains', 0.018), ('preslav', 0.018), ('eiichiro', 0.018), ('settings', 0.018), ('pairs', 0.017), ('tuning', 0.017), ('statistical', 0.017), ('johnson', 0.017), ('corpus', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="192-tfidf-1" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.</p><p>2 0.2164295 <a title="192-tfidf-2" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>Author: Stefan Thater ; Hagen Furstenau ; Manfred Pinkal</p><p>Abstract: We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.</p><p>3 0.18649986 <a title="192-tfidf-3" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>Author: Graeme Blackwood ; Adria de Gispert ; William Byrne</p><p>Abstract: This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers. We introduce transducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices.</p><p>4 0.15759201 <a title="192-tfidf-4" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>Author: Linlin Li ; Benjamin Roth ; Caroline Sporleder</p><p>Abstract: This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. nonliteral usages of potentially idiomatic expressions. In all three cases, we outper- form state-of-the-art systems either quantitatively or statistically significantly.</p><p>5 0.14977874 <a title="192-tfidf-5" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<p>Author: Katrin Erk ; Sebastian Pado</p><p>Abstract: This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.</p><p>6 0.13965391 <a title="192-tfidf-6" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>7 0.12345012 <a title="192-tfidf-7" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>8 0.12139196 <a title="192-tfidf-8" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>9 0.10678686 <a title="192-tfidf-9" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>10 0.10080064 <a title="192-tfidf-10" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>11 0.083995603 <a title="192-tfidf-11" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>12 0.083801478 <a title="192-tfidf-12" href="./acl-2010-Learning_Script_Knowledge_with_Web_Experiments.html">165 acl-2010-Learning Script Knowledge with Web Experiments</a></p>
<p>13 0.081218958 <a title="192-tfidf-13" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>14 0.080168441 <a title="192-tfidf-14" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>15 0.078384787 <a title="192-tfidf-15" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>16 0.076160237 <a title="192-tfidf-16" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>17 0.07440953 <a title="192-tfidf-17" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>18 0.070113733 <a title="192-tfidf-18" href="./acl-2010-Wrapping_up_a_Summary%3A_From_Representation_to_Generation.html">264 acl-2010-Wrapping up a Summary: From Representation to Generation</a></p>
<p>19 0.069550827 <a title="192-tfidf-19" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>20 0.063508205 <a title="192-tfidf-20" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.14), (1, -0.117), (2, -0.056), (3, -0.009), (4, 0.091), (5, 0.007), (6, 0.006), (7, -0.028), (8, -0.059), (9, 0.013), (10, 0.096), (11, 0.079), (12, 0.151), (13, -0.008), (14, 0.006), (15, 0.003), (16, -0.103), (17, 0.05), (18, -0.118), (19, 0.088), (20, 0.24), (21, 0.133), (22, -0.113), (23, -0.093), (24, 0.102), (25, -0.229), (26, -0.014), (27, 0.032), (28, -0.272), (29, -0.139), (30, -0.054), (31, 0.187), (32, -0.088), (33, 0.032), (34, 0.059), (35, -0.054), (36, -0.029), (37, -0.033), (38, 0.019), (39, 0.012), (40, -0.014), (41, -0.105), (42, -0.018), (43, 0.11), (44, -0.079), (45, 0.035), (46, 0.002), (47, 0.037), (48, 0.083), (49, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94158536 <a title="192-lsi-1" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.</p><p>2 0.64986163 <a title="192-lsi-2" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<p>Author: Katrin Erk ; Sebastian Pado</p><p>Abstract: This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.</p><p>3 0.58414072 <a title="192-lsi-3" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>Author: Graeme Blackwood ; Adria de Gispert ; William Byrne</p><p>Abstract: This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers. We introduce transducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices.</p><p>4 0.5392617 <a title="192-lsi-4" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>Author: Stefan Thater ; Hagen Furstenau ; Manfred Pinkal</p><p>Abstract: We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.</p><p>5 0.43412367 <a title="192-lsi-5" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>Author: Roberto Navigli ; Paola Velardi</p><p>Abstract: Definition extraction is the task of automatically identifying definitional sentences within texts. The task has proven useful in many research areas including ontology learning, relation extraction and question answering. However, current approaches mostly focused on lexicosyntactic patterns suffer from both low recall and precision, as definitional sentences occur in highly variable syntactic structures. In this paper, we propose WordClass Lattices (WCLs), a generalization of word lattices that we use to model textual definitions. Lattices are learned from a dataset of definitions from Wikipedia. Our method is applied to the task of definition and hypernym extraction and compares favorably to other pattern general– – ization methods proposed in the literature.</p><p>6 0.39203501 <a title="192-lsi-6" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>7 0.3471815 <a title="192-lsi-7" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>8 0.34681684 <a title="192-lsi-8" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>9 0.34672374 <a title="192-lsi-9" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>10 0.329005 <a title="192-lsi-10" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>11 0.32686028 <a title="192-lsi-11" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>12 0.32172376 <a title="192-lsi-12" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>13 0.31072149 <a title="192-lsi-13" href="./acl-2010-Learning_Script_Knowledge_with_Web_Experiments.html">165 acl-2010-Learning Script Knowledge with Web Experiments</a></p>
<p>14 0.29704344 <a title="192-lsi-14" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>15 0.27669284 <a title="192-lsi-15" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>16 0.27418846 <a title="192-lsi-16" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>17 0.27084056 <a title="192-lsi-17" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>18 0.26880509 <a title="192-lsi-18" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>19 0.2686404 <a title="192-lsi-19" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>20 0.26854834 <a title="192-lsi-20" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.048), (18, 0.018), (25, 0.054), (33, 0.026), (42, 0.01), (44, 0.012), (58, 0.277), (59, 0.185), (73, 0.036), (78, 0.016), (80, 0.011), (83, 0.078), (98, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84395993 <a title="192-lda-1" href="./acl-2010-Temporal_Information_Processing_of_a_New_Language%3A_Fast_Porting_with_Minimal_Resources.html">225 acl-2010-Temporal Information Processing of a New Language: Fast Porting with Minimal Resources</a></p>
<p>Author: Francisco Costa ; Antonio Branco</p><p>Abstract: We describe the semi-automatic adaptation of a TimeML annotated corpus from English to Portuguese, a language for which TimeML annotated data was not available yet. In order to validate this adaptation, we use the obtained data to replicate some results in the literature that used the original English data. The fact that comparable results are obtained indicates that our approach can be used successfully to rapidly create semantically annotated resources for new languages.</p><p>same-paper 2 0.79772359 <a title="192-lda-2" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.</p><p>3 0.70829999 <a title="192-lda-3" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>Author: Myroslava O. Dzikovska ; Johanna D. Moore ; Natalie Steinhauser ; Gwendolyn Campbell</p><p>Abstract: Supporting natural language input may improve learning in intelligent tutoring systems. However, interpretation errors are unavoidable and require an effective recovery policy. We describe an evaluation of an error recovery policy in the BEETLE II tutorial dialogue system and discuss how different types of interpretation problems affect learning gain and user satisfaction. In particular, the problems arising from student use of non-standard terminology appear to have negative consequences. We argue that existing strategies for dealing with terminology problems are insufficient and that improving such strategies is important in future ITS research.</p><p>4 0.64260638 <a title="192-lda-4" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>Author: Simone Paolo Ponzetto ; Roberto Navigli</p><p>Abstract: One of the main obstacles to highperformance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.</p><p>5 0.64064336 <a title="192-lda-5" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>Author: Yun-Cheng Ju ; Tim Paek</p><p>Abstract: Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search. 1</p><p>6 0.6380524 <a title="192-lda-6" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>7 0.62836683 <a title="192-lda-7" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>8 0.62469947 <a title="192-lda-8" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>9 0.62469649 <a title="192-lda-9" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>10 0.62407666 <a title="192-lda-10" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>11 0.62354982 <a title="192-lda-11" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>12 0.61919302 <a title="192-lda-12" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>13 0.6182974 <a title="192-lda-13" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>14 0.61697316 <a title="192-lda-14" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>15 0.61600053 <a title="192-lda-15" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>16 0.61454016 <a title="192-lda-16" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>17 0.61377668 <a title="192-lda-17" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>18 0.61368901 <a title="192-lda-18" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>19 0.6123414 <a title="192-lda-19" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>20 0.6121853 <a title="192-lda-20" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
