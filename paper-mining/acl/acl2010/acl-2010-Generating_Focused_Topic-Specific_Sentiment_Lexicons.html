<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-123" href="#">acl2010-123</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</h1>
<br/><p>Source: <a title="acl-2010-123-pdf" href="http://aclweb.org/anthology//P/P10/P10-1060.pdf">pdf</a></p><p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>Reference: <a title="acl-2010-123-reference" href="../acl2010_reference/acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 nl j  Abstract We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. [sent-3, score-0.931]
</p><p>2 Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin-  ion retrieval system. [sent-5, score-0.451]
</p><p>3 Specifically, media analysis concerns the following system task: given a topic and list of documents (discussing the topic), find all instances of attitudes toward the topic (e. [sent-7, score-0.637]
</p><p>4 How can technology developed for sentiment analysis be applied to media analysis? [sent-18, score-0.487]
</p><p>5 In order to use a sentiment extraction system for a media analysis problem, a system would have to be able to determine which of the extracted sentiments are actually relevant, i. [sent-19, score-0.654]
</p><p>6 , it would not only have to identify specific targets of all extracted sentiments, but also decide which of the targets are relevant for the topic at hand. [sent-21, score-1.018]
</p><p>7 In sentiment retrieval, on the other hand, the topic is initially present in the task definition, but it is left to the user to identify sources and targets of sentiments, as systems typically return a list of documents ranked by relevance and opinionatedness. [sent-30, score-1.054]
</p><p>8 To use a traditional sentiment retrieval system in media analysis, one would still have to manually go through ranked lists of documents returned by the system. [sent-31, score-0.715]
</p><p>9 c As2s0o1c0ia Atisosnoc foiart Cionom fopru Ctaotmiopnuatla Lti on gaulis Lti cnsg,u piasgtiecs 585–594, To be able to support media analysis, we need to combine the specificity of (phrase- or word-level) sentiment analysis with the topicality provided by sentiment retrieval. [sent-34, score-0.834]
</p><p>10 Moreover, we should be able  to identify sources and specific targets of opinions. [sent-35, score-0.429]
</p><p>11 In order to move towards the requirements of media analysis, in this paper we focus on two of the problems identified above: (1) pinpointing evidence for a system’s decisions about the presence of sentiment in text, and (2) identifying specific targets of sentiment. [sent-41, score-0.837]
</p><p>12 We address these problems by introducing a special type of lexical resource: a topic-specific subjectivity lexicon that indicates specific relevant targets for which sentiments may be expressed; for a given topic, such a lexicon consists of pairs (syntactic clue, target). [sent-42, score-1.136]
</p><p>13 We evaluate the quality of the lexicon both manually and in the setting of an opinionated blog post retrieval task. [sent-44, score-0.907]
</p><p>14 We demonstrate that such a lexicon is highly focused, allowing one to effectively pinpoint evidence for sentiment, while being competetive with traditional subjectivity lexicons consisting of (a large number of) clue words. [sent-45, score-0.839]
</p><p>15 Instead, we make an existing lexicon more focused, so that it can be used to actually pin-point subjectivity in documents relevant to a given topic. [sent-47, score-0.53]
</p><p>16 We discuss related work in four parts: sentiment analysis in general, domain- and targetspecific sentiment analysis, product review mining and sentiment retrieval. [sent-49, score-1.105]
</p><p>17 Kim and Hovy (2004) select candidate sentiment sentences and use word-based sentiment classifiers to classify unseen words into a negative or positive class. [sent-53, score-0.694]
</p><p>18 (2005): a classifier is learnt that distinguishes between polar and neutral sentences, based on a prior polarity lexicon and an annotated corpus. [sent-58, score-0.39]
</p><p>19 After this initial step, the sentiment sentences are classified as negative or positive; again, a prior polarity lexicon and syntactic features are used. [sent-60, score-0.744]
</p><p>20 2  Domain- and target-specific sentiment  The way authors express their attitudes varies with the domain: An unpredictable movie can be positive, but unpredictable politicians are usually something negative. [sent-66, score-0.488]
</p><p>21 (2007) aim at measuring overall subjectivity or polarity towards a certain entity;  they identify sentiments using domain-specific lexicons. [sent-69, score-0.464]
</p><p>22 All named entites in a sentence containing a clue from a lexicon are 586  considered targets of sentiment for counting. [sent-71, score-1.15]
</p><p>23 They identify “sentiment topics,” noun phrases assumed to be linked to a sentiment clue in the same expression. [sent-75, score-0.634]
</p><p>24 Next, the sentiment topics are identified, and based on these sentiment topics and the current list of clues, new potential clues are extracted. [sent-78, score-0.898]
</p><p>25 Fahrni and Klenner (2008) identify potential targets in a given domain, and create a targetspecific polarity adjective lexicon. [sent-80, score-0.58]
</p><p>26 In contrast to much of the work in the literature, we need to specialize subjectivity lexicons not for a domain and target, but for “topics. [sent-86, score-0.391]
</p><p>27 However, topics in our setting go beyond concrete products, and the diversity and generality of possible topics makes it difficult to apply such supervised or thesaurus-based methods to identify opinion targets. [sent-94, score-0.386]
</p><p>28 Moreover, in our method we directly use associations between targets and opinions to extract both. [sent-95, score-0.417]
</p><p>29 4  Sentiment retrieval  At TREC, the Text REtrieval Conference, there has been interest in a specific type of sentiment  analysis: opinion retrieval. [sent-97, score-0.753]
</p><p>30 Finding blog posts that are not just about a topic, but also contain an opinion on the topic, proves to be a difficult task. [sent-100, score-0.647]
</p><p>31 In stage (2) one commonly uses either a binary classifier to distinguish between opinionated and non-opinionated documents or applies reranking of the initial result list using some opinion score. [sent-103, score-0.603]
</p><p>32 The authors use SentiWordNet and a corpus-derived lexicon to construct an opinion score for each post in an initial ranking of blog posts. [sent-107, score-0.813]
</p><p>33 This opinion score  is combined with the relevance score, and posts are reranked according to this new score. [sent-108, score-0.449]
</p><p>34 This domain-specific lexicon is constructed using feedback-style learning: retrieve an initial list of documents and use the top documents as training data to learn an opinion lexicon. [sent-112, score-0.687]
</p><p>35 3  Generating Topic-Specific Lexicons  In this section we describe how we generate a lexicon of subjectivity clues and targets for a given topic and a list of relevant documents (e. [sent-115, score-1.125]
</p><p>36 As the background corpus, we used the set of documents from the assessment pools of TREC 2006–2008 opinion retrieval tasks (described in detail in section 4). [sent-120, score-0.538]
</p><p>37 1 Step 1: Extracting syntactic contexts We start with a general domain-independent prior  polarity lexicon of 8,821 clue words (Wilson et al. [sent-126, score-0.686]
</p><p>38 First, we identify syntactic contexts in which specific clue words can be used to express 1http : / / nlp . [sent-128, score-0.406]
</p><p>39 shtml attitude: we try to find how a clue word can be syntactically linked to targets of sentiments. [sent-131, score-0.595]
</p><p>40 Our entropy-driven selection of syntactic contexts of a clue word is based on the following assumption: Assumption 1: In text, targets of sentiments are more diverse than sources of sentiments or other accompanying attributes such as location, time, manner, etc. [sent-147, score-0.959]
</p><p>41 To summarize, at the end of Step 1 of our method, we have extracted a list of pairs (clue word, syntactic context) such that for occurrences of the clue word, the words at the endpoint of the  syntactic dependency are likely to be targets of sentiments. [sent-150, score-0.744]
</p><p>42 2 Step 2: Selecting potential targets Here, we use the extracted syntantic clues to identify words that are likely to serve as specific targets for opinions about the topic in the relevant documents. [sent-153, score-1.184]
</p><p>43 Table 1: Examples of subjective syntactic contexts of clue words (based on Stanford dependencies). [sent-160, score-0.419]
</p><p>44 Assumption 2: The list of relevant documents contains a substantial number of documents on the topic which, moreover, contain sentiments about the topic. [sent-161, score-0.579]
</p><p>45 To identify potential attitude targets in the relevant documents, we compare their frequency in the relevant documents to the frequency in the background corpus using the standard χ2 statistics. [sent-163, score-0.743]
</p><p>46 This technique is based on the following assumption: Assumption 3: Sentiment targets related to the topic occur more often in subjective context in the set of relevant documents, than in the background corpus. [sent-164, score-0.684]
</p><p>47 In other words, while the background corpus contains sentiments towards very diverse subjects, the relevant documents tend to express attitudes related to the topic. [sent-165, score-0.442]
</p><p>48 As the result of Steps 1 and 2, as candidate targets for a given topic, we only select words that occur in subjective contexts, and that do so more often than we would normally expect. [sent-167, score-0.442]
</p><p>49 Table 2 shows  examples of extracted targets for three TREC topics (see below for a description of our experimental data). [sent-168, score-0.436]
</p><p>50 For extrinsic evaluation we apply our lexicon generation method to a collection of documents containing opinionated utterances: blog posts. [sent-178, score-0.768]
</p><p>51 The Blogs06 collection (Macdonald and Ounis, 2006) is a crawl of blog posts from 100,649 blogs over a period of 11 weeks (06/12/2005– 21/02/2006), with 3,215,171 posts in total. [sent-179, score-0.569]
</p><p>52 TREC 2006–2008 came with the task of opinionated blog post retrieval (Ounis et al. [sent-184, score-0.699]
</p><p>53 Every topic comes with a set of relevance judgments: Given a topic, a blog post can be either (i) nonrelevant, (ii) relevant, but not opinionated, or (iii) relevant and opinionated. [sent-192, score-0.618]
</p><p>54 Note that for the opinion retrieval task a document is considered relevant if it is on topic and contains opinions or sentiments towards the topic. [sent-196, score-0.806]
</p><p>55 For the quantative experiments in Section 6 we need a topical baseline: a set of blog posts potentially relevant to each topic. [sent-200, score-0.549]
</p><p>56 Because our topic-specific lexicons consist of triples (clue word, syntactic context, target), they actually contain more words than topic-independent lexicons of the same size, but topic-specific entries are more selective, which makes the lexicon more focused. [sent-205, score-0.831]
</p><p>57 Table 3 compares the application of topic-independent and topic-specific lexicons to on-topic blog text. [sent-206, score-0.503]
</p><p>58 Table 3: Posts with highlighted targets (bold) and subjectivity  clues (blue) using topic-independent  (left) and topic-specific (right) lexicons. [sent-219, score-0.594]
</p><p>59 ”  We assigned 186  matches of lexicon entries in 30 documents  into  four classes: • REL: sentiment towards a relevant target; • CREOLN:T seEnXtiTm:  esnentt tiomwearndt sto aw rearledvsa a target ;that  iCs OirNreTleEvXaTnt: t soe tnhtiem topic odwuea tdos c aon ttaerxgte (e. [sent-221, score-0.954]
</p><p>60 , opinion about a target “film”, but refering to  •  •  a film different from the topic); IRREL: sentiment towards irrelevant target (e. [sent-223, score-0.586]
</p><p>61 , “game” fmoern a topic ardbsou itrr a movie); NOSENT: no sentiment at all  In total only 8% of matches were manually classified as REL, with 62% classified as NOSENT, 23% as CONTEXT, and 6% as IRREL. [sent-225, score-0.524]
</p><p>62 On the other hand, among documents assessed as opionionated by TREC assessors, only 13% did not contain matches of the lexicon entries, compared to 27% of non-opinionated documents, which does indicate that our lexicon does attempt to separate non-opinionated documents from opinionated. [sent-226, score-0.697]
</p><p>63 To this end we deploy our lexicons to the task of opinionated blog post retrieval (Ounis et al. [sent-228, score-0.955]
</p><p>64 A commonly used approach to this task works in two stages: (1) identify topically rel-  evant blog posts, and (2) classify these posts as being opinionated or not. [sent-230, score-0.648]
</p><p>65 Our experiments have two goals: to compare the use of topic-independent and topic-specific lexicons for the opinionated post retrieval task, and to examine how different settings for the parameters of the lexicon generation affect the empirical quality. [sent-239, score-0.916]
</p><p>66 1 Reranking using a lexicon To rerank a list of posts retrieved for a given topic, we opt to use the method that showed best performance at TREC 2008. [sent-241, score-0.423]
</p><p>67 For the opinion score, terms from a (topic-independent) lexicon are matched against the post content, and weighted with the probability of term’s subjectivity. [sent-244, score-0.566]
</p><p>68 2: Opinion(D) =  X P(sub|w)  · n(w,D),  (2)  wX∈O  where O is the set of terms in the sentiment lexicon, P(sub|w) indicates the probability of term w being subjective, iacnadt n(w, D) ibs tbhiel ntyum ofbe terr omf  times term w occurs in document D. [sent-251, score-0.373]
</p><p>69 The opinion scoring can weigh lexicon terms differently, using P(sub|w); it normalizes scores to cancel out the Pef(fsecutb o|fw varying mdoacluizmesen stc soirzeess. [sent-252, score-0.505]
</p><p>70 edu /mpqa / topic-independent lexicon to the topic-dependent lexicons our method generates, which are also plugged into the reranking of Lee et al. [sent-257, score-0.515]
</p><p>71 1 Results and observations There are several parameters that we can vary when generating a topic-specific lexicon and when using it for reranking: D: the number of syntactic contexts per clue T: the number of extracted targets Sop(D) : the opinion scoring function. [sent-263, score-1.195]
</p><p>72 Note that α does not affect the lexicon creation, but only how the lexicon is used in reranking. [sent-265, score-0.416]
</p><p>73 Since we want to assess the quality of lexicons, not in the opinionated retrieval performance as such, we factor out α by selecting the best setting for each lexicon (including the topic-independent) and each evaluation measure. [sent-266, score-0.541]
</p><p>74 In Table 4 we present the results of evaluation of several lexicons in the context of opinionated blog post retrieval. [sent-267, score-0.815]
</p><p>75 When comparing topic-specific lexicons to the topic-independent one, most of the differences are not statistically significant, which is surprising given the fact that most topic-specific lexicons we evaluated are substantially smaller (see the two rightmost columns in the table). [sent-269, score-0.541]
</p><p>76 The smallest lexicon in Table 4 is seven times more selective than the general one, in terms of the number of lexicon matches per document. [sent-270, score-0.512]
</p><p>77 The only evaluation measure where the topicindependent lexicon consistently outperforms topic-specific ones, is Mean Reciprocal Rank that depends on a single relevant opinionated document high in a ranking. [sent-271, score-0.526]
</p><p>78 317 42683  Table 4: Evaluation of topic-specific lexicons applied to the opinion retrieval task, compared to the topicindependent lexicon. [sent-284, score-0.667]
</p><p>79 The two rightmost columns show the number of lexicon entries (average per topic) and the number of matches of lexicon entries in blog posts (average for top 1,000 posts). [sent-285, score-0.964]
</p><p>80 is that the large general lexicon easily finds a few “obviously subjective” posts (those with heavily used subjective words), but is not better at detecting less obvious ones, as indicated by the recalloriented MAP and R-precision. [sent-286, score-0.456]
</p><p>81 Interestingly, increasing the number of syntactic contexts considered for a clue word (parameter D) and the number of selected targets (parameter T) leads to substantially larger lexicons, but only gives marginal improvements when lexicons are used for opinion retrieval. [sent-287, score-1.182]
</p><p>82 This shows that our bootstrapping method is effective at filtering out non-relevant sentiment targets and syntactic clues. [sent-288, score-0.745]
</p><p>83 The evaluation results also show that the choice of opinion scoring function (Okapi or raw counts) depends on the lexicon size: for smaller, more focused lexicons unnormalized counts are more effective. [sent-289, score-0.733]
</p><p>84 This also confirms our intuition that for small, focused lexicons simple presence of a sentiment clue in text is a good indication of subjectivity, while for larger lexicons an overall subjectivity scoring of texts has to be used, which can be hard to interpret for (media analysis) users. [sent-290, score-1.264]
</p><p>85 2 Query expansion with lexicons In this section we evaluate the quality of targets extracted as part of the lexicons by using them for query expansion. [sent-292, score-1.074]
</p><p>86 Query expansion is a commonly used technique in information retrieval, aimed at getting a better representation of the user’s information need by adding terms to the original retrieval query; for user-generated content, selective query expansion has proved very beneficial (Weerkamp et al. [sent-293, score-0.466]
</p><p>87 We hypothesize that if our method manages to identify targets that correspond to issues, subtopics or features associated Run  MAP  P@ 10  Topical blog post retrieval Baseline 0. [sent-295, score-0.95]
</p><p>88 with the topic, the extracted targets should be good candidates for query expansion. [sent-319, score-0.467]
</p><p>89 For every test topic, we select the 20 top-scoring targets as expansion terms, and use Indri to return 1,000 most relevant documents for the expanded query. [sent-321, score-0.637]
</p><p>90 We evaluate the resulting ranking using both topical retrieval and opinionated retrieval measures. [sent-322, score-0.547]
</p><p>91 The results show that on topical retrieval query expansion using targets significantly improves retrieval performance, while using relevance models actually hurts all evaluation measures. [sent-325, score-0.934]
</p><p>92 The  failure of the latter expansion method can be attributed to the relatively large amount of noise in user-generated content, such as boilerplate 592  material, timestamps of blog posts, comments etc. [sent-326, score-0.374]
</p><p>93 The consistent improvement for topical retrieval suggests that a topic-specific lexicon can be used both for query expansion (as described in this section) and for opinion reranking (as described in Section 6. [sent-330, score-0.888]
</p><p>94 We have evaluated the quality of generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. [sent-334, score-0.841]
</p><p>95 Although the generated lexicons can be an order of magnitude more selective, they maintain, or even improve, the performance of an opinion retrieval system. [sent-335, score-0.635]
</p><p>96 As to future work, we intend to combine our method with known methods for topic-specific lexicon expansion (our method is rather concerned with lexicon “restriction”). [sent-336, score-0.511]
</p><p>97 Existing sentenceor phrase-level (trained) sentiment classifiers can also be used easily: when collecting/counting targets we can weigh them by “prior” score provided by such classifiers. [sent-337, score-0.73]
</p><p>98 We would also like to extend potential opinion targets to include multi-word phrases (NPs and VPs), in addition to individual words. [sent-340, score-0.594]
</p><p>99 Finally, we do not identify polarity yet: this can  be partially inherited from the initial lexicon and refined automatically via bootstrapping. [sent-341, score-0.401]
</p><p>100 A generative blog post retrieval model that uses query expansion based on external collections. [sent-458, score-0.682]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('targets', 0.355), ('sentiment', 0.347), ('lexicons', 0.256), ('blog', 0.247), ('clue', 0.24), ('opinion', 0.239), ('lexicon', 0.208), ('opinionated', 0.193), ('posts', 0.161), ('trec', 0.152), ('polarity', 0.146), ('retrieval', 0.14), ('topic', 0.136), ('sentiments', 0.136), ('subjectivity', 0.135), ('documents', 0.12), ('post', 0.119), ('media', 0.108), ('clues', 0.104), ('ounis', 0.096), ('expansion', 0.095), ('subjective', 0.087), ('query', 0.081), ('sop', 0.08), ('attitudes', 0.08), ('topical', 0.074), ('weerkamp', 0.07), ('relevant', 0.067), ('opinions', 0.062), ('movie', 0.061), ('selective', 0.055), ('rijke', 0.052), ('okapi', 0.052), ('reranking', 0.051), ('topics', 0.05), ('relevance', 0.049), ('contexts', 0.049), ('attitude', 0.048), ('indri', 0.048), ('identify', 0.047), ('wilson', 0.045), ('syntactic', 0.043), ('subtopics', 0.042), ('matches', 0.041), ('kim', 0.04), ('background', 0.039), ('polar', 0.036), ('macdonald', 0.036), ('croft', 0.036), ('entries', 0.035), ('atoms', 0.034), ('wiebe', 0.033), ('triples', 0.033), ('choi', 0.033), ('abramoff', 0.032), ('altheide', 0.032), ('boilerplate', 0.032), ('endpoint', 0.032), ('freezing', 0.032), ('godbole', 0.032), ('nosent', 0.032), ('penguins', 0.032), ('saturday', 0.032), ('snatching', 0.032), ('stepmum', 0.032), ('targetspecific', 0.032), ('topicindependent', 0.032), ('topicspecific', 0.032), ('tragic', 0.032), ('analysis', 0.032), ('extracted', 0.031), ('mrr', 0.031), ('march', 0.031), ('subjects', 0.031), ('lee', 0.031), ('scoring', 0.03), ('rightmost', 0.029), ('notebook', 0.028), ('predators', 0.028), ('rerank', 0.028), ('macbook', 0.028), ('dad', 0.028), ('awesome', 0.028), ('bowl', 0.028), ('christmas', 0.028), ('eggs', 0.028), ('errands', 0.028), ('fahrni', 0.028), ('kanayama', 0.028), ('weigh', 0.028), ('specific', 0.027), ('sub', 0.027), ('track', 0.026), ('programme', 0.026), ('retrieved', 0.026), ('evening', 0.026), ('nasukawa', 0.026), ('document', 0.026), ('sigir', 0.025), ('toward', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="123-tfidf-1" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>2 0.453908 <a title="123-tfidf-2" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>Author: Binyang Li ; Lanjun Zhou ; Shi Feng ; Kam-Fai Wong</p><p>Abstract: There is a growing research interest in opinion retrieval as on-line users’ opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user’s query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representa- , tion, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 1</p><p>3 0.3317543 <a title="123-tfidf-3" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>4 0.31944892 <a title="123-tfidf-4" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>5 0.24795395 <a title="123-tfidf-5" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>Author: Wei Wei ; Jon Atle Gulla</p><p>Abstract: Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products.</p><p>6 0.24319355 <a title="123-tfidf-6" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>7 0.23421392 <a title="123-tfidf-7" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>8 0.22277404 <a title="123-tfidf-8" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>9 0.2159576 <a title="123-tfidf-9" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>10 0.18789279 <a title="123-tfidf-10" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>11 0.14858997 <a title="123-tfidf-11" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>12 0.13341105 <a title="123-tfidf-12" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>13 0.13255887 <a title="123-tfidf-13" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>14 0.11859902 <a title="123-tfidf-14" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>15 0.11785912 <a title="123-tfidf-15" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>16 0.11247898 <a title="123-tfidf-16" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>17 0.10358789 <a title="123-tfidf-17" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>18 0.09732569 <a title="123-tfidf-18" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>19 0.094693467 <a title="123-tfidf-19" href="./acl-2010-Mood_Patterns_and_Affective_Lexicon_Access_in_Weblogs.html">176 acl-2010-Mood Patterns and Affective Lexicon Access in Weblogs</a></p>
<p>20 0.092293836 <a title="123-tfidf-20" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.215), (1, 0.197), (2, -0.35), (3, 0.38), (4, -0.24), (5, 0.062), (6, -0.032), (7, 0.188), (8, 0.051), (9, -0.04), (10, 0.045), (11, -0.002), (12, 0.01), (13, 0.036), (14, -0.0), (15, 0.0), (16, -0.021), (17, -0.03), (18, -0.024), (19, -0.04), (20, -0.033), (21, -0.032), (22, 0.003), (23, 0.01), (24, 0.044), (25, -0.021), (26, -0.025), (27, -0.018), (28, -0.02), (29, -0.009), (30, 0.078), (31, -0.023), (32, 0.026), (33, -0.026), (34, -0.006), (35, -0.104), (36, -0.018), (37, -0.009), (38, -0.031), (39, 0.014), (40, 0.003), (41, -0.012), (42, 0.008), (43, 0.021), (44, 0.031), (45, 0.075), (46, -0.051), (47, 0.013), (48, 0.025), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9687655 <a title="123-lsi-1" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>2 0.885436 <a title="123-lsi-2" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>Author: Binyang Li ; Lanjun Zhou ; Shi Feng ; Kam-Fai Wong</p><p>Abstract: There is a growing research interest in opinion retrieval as on-line users’ opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user’s query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representa- , tion, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 1</p><p>3 0.7586543 <a title="123-lsi-3" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>4 0.71410173 <a title="123-lsi-4" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>Author: Ainur Yessenalina ; Yejin Choi ; Claire Cardie</p><p>Abstract: One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</p><p>5 0.70423764 <a title="123-lsi-5" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>Author: Georgios Paltoglou ; Mike Thelwall</p><p>Abstract: Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.</p><p>6 0.69627166 <a title="123-lsi-6" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>7 0.68599731 <a title="123-lsi-7" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>8 0.67733783 <a title="123-lsi-8" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>9 0.60674572 <a title="123-lsi-9" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>10 0.55891079 <a title="123-lsi-10" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>11 0.5223195 <a title="123-lsi-11" href="./acl-2010-Mood_Patterns_and_Affective_Lexicon_Access_in_Weblogs.html">176 acl-2010-Mood Patterns and Affective Lexicon Access in Weblogs</a></p>
<p>12 0.50212902 <a title="123-lsi-12" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>13 0.44246763 <a title="123-lsi-13" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>14 0.40867361 <a title="123-lsi-14" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>15 0.35596606 <a title="123-lsi-15" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>16 0.35386804 <a title="123-lsi-16" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>17 0.33543625 <a title="123-lsi-17" href="./acl-2010-Multilingual_Pseudo-Relevance_Feedback%3A_Performance_Study_of_Assisting_Languages.html">177 acl-2010-Multilingual Pseudo-Relevance Feedback: Performance Study of Assisting Languages</a></p>
<p>18 0.33291289 <a title="123-lsi-18" href="./acl-2010-Cross_Lingual_Adaptation%3A_An_Experiment_on_Sentiment_Classifications.html">80 acl-2010-Cross Lingual Adaptation: An Experiment on Sentiment Classifications</a></p>
<p>19 0.32650542 <a title="123-lsi-19" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>20 0.31776607 <a title="123-lsi-20" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.015), (25, 0.042), (39, 0.012), (42, 0.409), (59, 0.062), (72, 0.017), (73, 0.058), (76, 0.014), (78, 0.023), (83, 0.074), (84, 0.024), (98, 0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96163505 <a title="123-lda-1" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>Author: Brian Pluss</p><p>Abstract: This paper presents ongoing research on computational models for non-cooperative dialogue. We start by analysing different levels of cooperation in conversation. Then, inspired by findings from an empirical study, we propose a technique for measuring non-cooperation in political interviews. Finally, we describe a research programme towards obtaining a suitable model and discuss previous accounts for conflictive dialogue, identifying the differences with our work.</p><p>2 0.90345436 <a title="123-lda-2" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>Author: Ryu Iida ; Syumpei Kobayashi ; Takenobu Tokunaga</p><p>Abstract: This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001 ; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on refer- ence resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.</p><p>same-paper 3 0.86368108 <a title="123-lda-3" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>4 0.76451123 <a title="123-lda-4" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynam- ically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.</p><p>5 0.69026864 <a title="123-lda-5" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>6 0.68503183 <a title="123-lda-6" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>7 0.66690463 <a title="123-lda-7" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>8 0.66171515 <a title="123-lda-8" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>9 0.64705336 <a title="123-lda-9" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>10 0.61180264 <a title="123-lda-10" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>11 0.59853601 <a title="123-lda-11" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>12 0.53067756 <a title="123-lda-12" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>13 0.5145129 <a title="123-lda-13" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>14 0.51040798 <a title="123-lda-14" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>15 0.50742829 <a title="123-lda-15" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>16 0.50494254 <a title="123-lda-16" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>17 0.50257766 <a title="123-lda-17" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>18 0.50086021 <a title="123-lda-18" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>19 0.49826491 <a title="123-lda-19" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>20 0.49556673 <a title="123-lda-20" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
