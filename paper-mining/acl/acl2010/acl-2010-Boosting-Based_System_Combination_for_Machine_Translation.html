<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>54 acl-2010-Boosting-Based System Combination for Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-54" href="#">acl2010-54</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>54 acl-2010-Boosting-Based System Combination for Machine Translation</h1>
<br/><p>Source: <a title="acl-2010-54-pdf" href="http://aclweb.org/anthology//P/P10/P10-1076.pdf">pdf</a></p><p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>Reference: <a title="acl-2010-54-reference" href="../acl2010_reference/acl-2010-Boosting-Based_System_Combination_for_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  ,  ,  Abstract In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. [sent-6, score-0.784]
</p><p>2 First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. [sent-8, score-0.712]
</p><p>3 Then, a strong translation system is built from the ensemble of these weak translation systems. [sent-9, score-0.931]
</p><p>4 To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. [sent-10, score-0.571]
</p><p>5 We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. [sent-11, score-0.274]
</p><p>6 The experimental results on three NIST evaluation  test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. [sent-12, score-0.482]
</p><p>7 With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. [sent-19, score-0.679]
</p><p>8 The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. [sent-20, score-0.946]
</p><p>9 Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e. [sent-21, score-0.45]
</p><p>10 sentence-level combination (Hildebrand and Vogel, 2008) simply  selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al. [sent-23, score-0.252]
</p><p>11 One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). [sent-26, score-0.912]
</p><p>12 To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. [sent-27, score-1.012]
</p><p>13 To reduce the burden of system development, it might be a nice way to combine a set of translation systems built from a single translation engine. [sent-29, score-0.793]
</p><p>14 A key issue here is how to generate an ensemble of diversified translation systems from a single translation engine in a principled way. [sent-30, score-1.069]
</p><p>15 Addressing this issue, we propose a boostingbased system combination method to learn a combined translation system from a single SMT  engine. [sent-31, score-0.801]
</p><p>16 In this method, a sequence of weak translation systems is generated from a baseline system in an iterative manner. [sent-32, score-0.712]
</p><p>17 In each iteration, a new weak translation system is learned, focusing more on the sentences that are relatively poorly translated by the previous weak translation system. [sent-33, score-0.944]
</p><p>18 Finally, a strong translation system is built from the ensemble of the weak translation systems. [sent-34, score-0.931]
</p><p>19 Our experiments are conducted on Chinese-toEnglish translation in three state-of-the-art SMT systems, including a phrase-based system, a hierarchical phrase-based system and a syntax-based 739  Proce dinUgsp osfa tlhae, 4S8wthed Aen n,u 1a1l-1 M6e Jeutilnyg 2 o0f1 t0h. [sent-35, score-0.465]
</p><p>20 Experimental re-  sults show that our method leads to significant improvements in translation accuracy over the baseline systems. [sent-39, score-0.482]
</p><p>21 e* = argmax(Pr(e e| f))  (1)  e  where Pr(e e| f) is the probability that e is the translation of the given source string f. [sent-41, score-0.341]
</p><p>22 , uT(λ*T)} , the task of system combination is to build a new translation system v(u1(λ*1), . [sent-58, score-0.658]
</p><p>23 , uT(λ*T)) denotes the combination system which combines translations from the ensemble of the output of each ui(λ*i). [sent-68, score-0.41]
</p><p>24 As discussed in Section 1, the diversity among the outputs of member systems is an important factor to the success of system combination. [sent-73, score-0.834]
</p><p>25 To obtain diversified member systems, traditional  methods concentrate more on using structurally different member systems, that is u1 ≠ u2 ≠ . [sent-74, score-0.922]
</p><p>26 However, this constraint condition cannot be satisfied when multiple translation engines are not available. [sent-78, score-0.304]
</p><p>27 In this paper, we argue that the diversified member systems can also be generated from a single engine u(λ*) by adjusting the weight vector λ* in a principled way. [sent-79, score-0.781]
</p><p>28 However, since most of the boosting algorithms are designed for the classification problem that is very different from the translation problem in natural language processing, several key components have to be redesigned when boosting is adapted to SMT system combination. [sent-89, score-0.875]
</p><p>29 As the weighted BLEU is used to measure the translation accuracy on the training set, the error rate is defined to be: εt  3. [sent-111, score-0.304]
</p><p>30 On each round, we increase the weights of the samples that are relatively poorly translated by the current weak system so that the MERT-based trainer can focus on the hard samples in next round. [sent-114, score-0.348]
</p><p>31 αt can be regarded as a measure of the importance that the t-th weak system gains in boosting. [sent-116, score-0.263]
</p><p>32 , ein} be the n-best translation candidates produced by the system. [sent-125, score-0.366]
</p><p>33 , 2006) of the translation e with respect to the reference translations ri, and ei* is the oracle translation which is selected from {ei1, . [sent-127, score-0.734]
</p><p>34 li can be viewed as a measure of the average cost that we guess the top-k translation candidates instead of the oracle translation. [sent-131, score-0.492]
</p><p>35 The value of li counts for the magnitude of weight update, that is, a larger li means a larger weight update on Dt(i). [sent-132, score-0.261]
</p><p>36 3  System Combination Scheme  In the last step of our method, a strong translation system v(u(λ*1), . [sent-138, score-0.418]
</p><p>37 In this work, a sentence-level combination method is used to select the best translation from the pool of the n-best outputs of all the member systems. [sent-147, score-0.913]
</p><p>38 Let H(u(λ*t)) (or Ht for short) be the set of the n-best translation candidates produced by the t-th member system u(λ*t), and H(v) be the union set of all Ht (i. [sent-148, score-0.859]
</p><p>39 The final translation is generated from H(v) based on the following scoring function:  e*=areg∈ Hm( va )x∑tT=1βt⋅ φt(e )+ψ(e , H( v ))  (8)  where φt (e ) is the log-scaled model score of e in the t-th member system, and βt is the corre-  sponding feature weight. [sent-151, score-0.723]
</p><p>40 In this case, we can still calculate the model score of e in any other member systems, since all the member systems are based on the same model and share the same feature space. [sent-153, score-0.829]
</p><p>41 ψ(e , H( v )) is a consensusbased scoring function which has been successfully adopted in SMT system combination (Duan et al. [sent-154, score-0.24]
</p><p>42 ≠  ψ(e , H( v )) = ∑θn+ ⋅hn+(e , H( v ))+ n  ∑θn−⋅h−n(e , H( v ))  (9)  n  For each order of n-gram, hn+ (e , H( v )) and h−n (e , H( v )) are defined to measure the n-gram agreement and disagreement between e and other translation candidates in H(v), respectively. [sent-158, score-0.366]
</p><p>43 If p orders of n-gram are used in computing ψ(e , H( v )) , the total number of features in the system combination will be T + 2 p (T modelscore-based features defined in Equation 8 and 2 p consensus-based features defined in Equation 9). [sent-163, score-0.24]
</p><p>44 4 Optimization If implemented naively, the translation speed of the final translation system will be very slow. [sent-165, score-0.785]
</p><p>45 For a given input sentence, each member system has to encode it individually, and the translation speed is inversely proportional to the number of member systems generated by our method. [sent-166, score-1.35]
</p><p>46 A simple solution is to run member systems in parallel when translating a new sentence. [sent-168, score-0.45]
</p><p>47 Since all the member systems share the same data re-  sources, such as language model and translation table, we only need to keep one copy of the required resources in memory. [sent-169, score-0.754]
</p><p>48 The translation speed just depends on the computing power of parallel computation environment, such as the number of CPUs. [sent-170, score-0.409]
</p><p>49 Furthermore, we can use joint decoding techniques to save the computation of the equivalent translation hypotheses among member systems. [sent-171, score-0.81]
</p><p>50 In joint decoding of member systems, the search space is structured as a translation hypergraph where the member systems can share their translation hypotheses. [sent-172, score-1.486]
</p><p>51 If more than one member systems share the same translation hypothesis, we just need to compute the corresponding feature values only once, instead of repeating the computation in individual decoders. [sent-173, score-0.796]
</p><p>52 In our experiments, we find that over 60% translation hypotheses can be shared among member systems when the number of member systems is over 4. [sent-174, score-1.24]
</p><p>53 Another method to speed up the system is to accelerate n-gram language model with n-gram  caching techniques. [sent-176, score-0.256]
</p><p>54 As the translation speed of SMT system depends heavily on the computation of n-gram language model, the acceleration of n-gram language model generally leads to substantial speed-up of SMT system. [sent-180, score-0.523]
</p><p>55 742  5  Experiments  Our experiments are conducted on Chinese-toEnglish translation in three SMT systems. [sent-182, score-0.304]
</p><p>56 1  Baseline Systems  The first SMT system is a phrase-based system with two reordering models including the maxi-  mum entropy-based lexicalized reordering model proposed by Xiong et al. [sent-184, score-0.344]
</p><p>57 The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). [sent-187, score-0.275]
</p><p>58 The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al. [sent-188, score-0.228]
</p><p>59 , 2009) is performed on each translation rule for the CKYstyle decoding. [sent-193, score-0.304]
</p><p>60 In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i. [sent-194, score-0.624]
</p><p>61 To obtain satisfactory baseline per-  formance, we train each SMT system for 5 times using MERT with different initial values of feature weights to generate a group of baseline candidates, and then select the best-performing one from this group as the final baseline system (i. [sent-197, score-0.554]
</p><p>62 A 5gram language model is trained on the target-side 4 Our in-house experimental results show that this system performs slightly better than Moses on Chinese-to-English translation tasks. [sent-204, score-0.418]
</p><p>63 The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. [sent-207, score-0.352]
</p><p>64 The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. [sent-210, score-0.304]
</p><p>65 The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. [sent-215, score-0.24]
</p><p>66 3  Evaluation of Translations  First we investigate the effectiveness of the boosting-based system combination on the three systems. [sent-217, score-0.24]
</p><p>67 Figures 2-5 show the BLEU curves on the development and test sets, where the X-axis is the iteration number, and the Y-axis is the BLEU score of the system generated by the boostingbased system combination. [sent-218, score-0.473]
</p><p>68 After 5, 7 and 8 iterations, relatively stable improvements are achieved by the phrase-based system, the Hiero system and the syntax-based system, respectively. [sent-222, score-0.238]
</p><p>69 Figures 2-5 also show that the boosting-based system combination seems to be more helpful to the phrase-based system than to the Hiero system and the syntax-based system. [sent-224, score-0.468]
</p><p>70 For the comparison, we show the performance of the baseline systems with the n-best list size of 600 (Baseline+600best in Table 1) which equals to the maximum number of translation candidates accessed in the final combination system (combine 30 member systems, i. [sent-249, score-1.169]
</p><p>71 These results indicate that the SMT systems can  benefit more from the diversified outputs of member systems rather than from larger n-best lists produced by a single system. [sent-256, score-0.748]
</p><p>72 4 Diversity among Member Systems We also study the change of diversity among the outputs of member systems during iterations. [sent-258, score-0.72]
</p><p>73 In this work, the TER score for a given group of member systems is calculated by averaging the TER scores between the outputs of each pair of member systems in this group. [sent-262, score-0.997]
</p><p>74 Figures 6-9 show the curves of diversity on the development and test sets, where the X-axis  is the iteration number, and the Y-axis is the diversity. [sent-263, score-0.342]
</p><p>75 The points at iteration 1 stand for the diversities of baseline systems. [sent-264, score-0.277]
</p><p>76 In this work, the baseline’s diversity is the TER score of the group of baseline candidates that are generated in advance (Section 5. [sent-265, score-0.415]
</p><p>77 It indicates that our method is very effective to generate diversified member systems. [sent-268, score-0.584]
</p><p>78 In addition, the diversities of baseline systems (iteration 1) are much lower 745  than those of the systems generated by boosting (iterations 2-30). [sent-269, score-0.529]
</p><p>79 Together with the results shown in Figures 2-5, it confirms our motivation that the diversified translation outputs can lead to performance improvements over the baseline systems. [sent-270, score-0.668]
</p><p>80 Also as shown in Figures 6-9, the diversity of the Hiero system is much lower than that of the phrase-based and syntax-based systems at each individual setting of iteration number. [sent-271, score-0.527]
</p><p>81 This interesting finding supports the observation that the  performance of the Hiero system is relatively more stable than the other two systems as shown in Figures 2-5. [sent-272, score-0.244]
</p><p>82 The relative lack of diversity in the Hiero system might be due to the spurious ambiguity in Hiero derivations which generally results in very few different translations in translation outputs (Chiang, 2007). [sent-273, score-0.76]
</p><p>83 5  Evaluation of Oracle Translations  In this set of experiments, we evaluate the oracle performance on the n-best lists of the baseline systems and the combined systems generated by boosting-based system combination. [sent-275, score-0.454]
</p><p>84 Table 2 shows the results, where Baseline+600best stands for the top-600 translation candidates generated by the baseline systems, and Boosting-30iterations stands for the ensemble of 30 member systems’ top-20 translation candidates. [sent-277, score-1.259]
</p><p>85 This result indicates that our method can provide much “better” translation candidates for system combination  than enlarging the size of n-best list naively. [sent-279, score-0.647]
</p><p>86 However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. [sent-287, score-0.295]
</p><p>87 To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. [sent-288, score-0.248]
</p><p>88 There are also some other studies on building diverse translation systems from a single translation engine for system combination. [sent-291, score-0.84]
</p><p>89 They empirically showed that diverse translation systems could be generated by changing parameters at early-stages of the training procedure. [sent-293, score-0.415]
</p><p>90 (2009) proposed a feature subspace method to build a group of translation systems from various different sub-models of an existing SMT system. [sent-295, score-0.491]
</p><p>91 In this work, we use a sentence-level system combination method to generate final translations. [sent-306, score-0.281]
</p><p>92 Another issue is how to determine an appropriate number of iterations for boosting-based system combination. [sent-308, score-0.241]
</p><p>93 Our empirical study shows that the stable and satisfactory improvements can be achieved after 6-8 iterations, while the largest improvements can be achieved after 20 iterations. [sent-310, score-0.231]
</p><p>94 In our future work, we will study in-depth principled ways to determine the appropriate number of iterations for boosting-based system combination. [sent-311, score-0.236]
</p><p>95 8  Conclusions  We have proposed a boosting-based system combination method to address the issue of building a strong translation system from a group of weak translation systems generated from a single SMT engine. [sent-312, score-1.302]
</p><p>96 The experimental results show that our method is very effective to improve the translation accuracy of the SMT systems. [sent-314, score-0.345]
</p><p>97 Scalable inferences and training of context-rich syntax translation models. [sent-370, score-0.304]
</p><p>98 Combination of machine translation systems via hypothesis selection from combined n-best lists. [sent-381, score-0.407]
</p><p>99 SPMT: Statistical machine translation with syntactified target language phrases. [sent-426, score-0.304]
</p><p>100 Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. [sent-431, score-0.755]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('member', 0.379), ('smt', 0.354), ('translation', 0.304), ('bleu', 0.256), ('diversity', 0.207), ('boosting', 0.205), ('diversified', 0.164), ('iteration', 0.135), ('schapire', 0.132), ('combination', 0.126), ('wbleu', 0.117), ('system', 0.114), ('weak', 0.111), ('duan', 0.111), ('hiero', 0.106), ('ensemble', 0.098), ('hildebrand', 0.093), ('iterations', 0.084), ('lagarda', 0.075), ('ut', 0.074), ('li', 0.072), ('baseline', 0.072), ('translations', 0.072), ('freund', 0.071), ('hm', 0.071), ('systems', 0.071), ('adaboost', 0.07), ('boostingbased', 0.07), ('diversities', 0.07), ('macherey', 0.066), ('mert', 0.065), ('improvements', 0.065), ('speed', 0.063), ('outputs', 0.063), ('candidates', 0.062), ('stable', 0.059), ('xiao', 0.059), ('figures', 0.058), ('reordering', 0.058), ('nist', 0.058), ('chiang', 0.056), ('spmt', 0.056), ('galley', 0.055), ('oracle', 0.054), ('ri', 0.052), ('och', 0.051), ('ter', 0.051), ('casacuberta', 0.05), ('decoding', 0.049), ('dt', 0.049), ('cache', 0.048), ('binarization', 0.048), ('hierarchical', 0.047), ('engine', 0.047), ('elb', 0.047), ('jingbo', 0.047), ('matusov', 0.047), ('redesigned', 0.047), ('rosti', 0.047), ('rudin', 0.047), ('emnlp', 0.045), ('mu', 0.044), ('issue', 0.043), ('tong', 0.042), ('satisfactory', 0.042), ('pr', 0.042), ('weight', 0.042), ('computation', 0.042), ('yoram', 0.041), ('accessed', 0.041), ('vogel', 0.041), ('hn', 0.041), ('synchronous', 0.041), ('dongdong', 0.041), ('ghkm', 0.041), ('subspace', 0.041), ('trainer', 0.041), ('method', 0.041), ('samples', 0.041), ('consensus', 0.04), ('ming', 0.04), ('generated', 0.04), ('zhu', 0.038), ('principled', 0.038), ('pages', 0.038), ('gains', 0.038), ('eij', 0.038), ('caching', 0.038), ('mbr', 0.038), ('string', 0.037), ('hypotheses', 0.036), ('robert', 0.035), ('liu', 0.035), ('group', 0.034), ('zhang', 0.033), ('update', 0.033), ('combined', 0.032), ('nan', 0.032), ('mh', 0.032), ('liang', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="54-tfidf-1" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>2 0.20679621 <a title="54-tfidf-2" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>Author: Joern Wuebker ; Arne Mauser ; Hermann Ney</p><p>Abstract: Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overfitting. We describe a novel leavingone-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering mod- els in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.</p><p>3 0.20621015 <a title="54-tfidf-3" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>4 0.20276614 <a title="54-tfidf-4" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from Nbest lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.</p><p>5 0.19677271 <a title="54-tfidf-5" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>6 0.19485503 <a title="54-tfidf-6" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>7 0.1832197 <a title="54-tfidf-7" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>8 0.17982808 <a title="54-tfidf-8" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>9 0.17358588 <a title="54-tfidf-9" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>10 0.16472766 <a title="54-tfidf-10" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>11 0.15638012 <a title="54-tfidf-11" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>12 0.15620758 <a title="54-tfidf-12" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>13 0.15377918 <a title="54-tfidf-13" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>14 0.15364327 <a title="54-tfidf-14" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>15 0.15320382 <a title="54-tfidf-15" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>16 0.15313166 <a title="54-tfidf-16" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>17 0.15100689 <a title="54-tfidf-17" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>18 0.15080123 <a title="54-tfidf-18" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>19 0.14984843 <a title="54-tfidf-19" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>20 0.13824473 <a title="54-tfidf-20" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.27), (1, -0.306), (2, -0.089), (3, 0.015), (4, 0.067), (5, 0.025), (6, -0.076), (7, -0.018), (8, -0.105), (9, 0.078), (10, 0.204), (11, 0.17), (12, 0.114), (13, -0.082), (14, 0.064), (15, 0.035), (16, -0.059), (17, 0.072), (18, -0.128), (19, 0.023), (20, -0.028), (21, -0.021), (22, -0.017), (23, -0.016), (24, -0.082), (25, 0.004), (26, 0.06), (27, 0.036), (28, 0.019), (29, 0.059), (30, 0.096), (31, -0.007), (32, 0.019), (33, 0.09), (34, 0.002), (35, -0.002), (36, 0.08), (37, 0.001), (38, 0.042), (39, 0.01), (40, 0.0), (41, 0.004), (42, -0.003), (43, -0.004), (44, -0.023), (45, 0.029), (46, -0.018), (47, -0.016), (48, -0.045), (49, -0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9758845 <a title="54-lsi-1" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>2 0.88212907 <a title="54-lsi-2" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>Author: Jesus Gonzalez Rubio ; Daniel Ortiz Martinez ; Francisco Casacuberta</p><p>Abstract: This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.</p><p>3 0.82058179 <a title="54-lsi-3" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>4 0.77874058 <a title="54-lsi-4" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>Author: Yifan He ; Yanjun Ma ; Josef van Genabith ; Andy Way</p><p>Abstract: We propose a translation recommendation framework to integrate Statistical Machine Translation (SMT) output with Translation Memory (TM) systems. The framework recommends SMT outputs to a TM user when it predicts that SMT outputs are more suitable for post-editing than the hits provided by the TM. We describe an implementation of this framework using an SVM binary classifier. We exploit methods to fine-tune the classifier and investigate a variety of features of different types. We rely on automatic MT evaluation metrics to approximate human judgements in our experiments. Experimental results show that our system can achieve 0.85 precision at 0.89 recall, excluding ex- act matches. Furthermore, it is possible for the end-user to achieve a desired balance between precision and recall by adjusting confidence levels.</p><p>5 0.77285701 <a title="54-lsi-5" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>Author: Ondrej Bojar ; Kamil Kos ; David Marecek</p><p>Abstract: We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.</p><p>6 0.7692517 <a title="54-lsi-6" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>7 0.76310503 <a title="54-lsi-7" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>8 0.71848017 <a title="54-lsi-8" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>9 0.70179564 <a title="54-lsi-9" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>10 0.66394174 <a title="54-lsi-10" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>11 0.65620315 <a title="54-lsi-11" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>12 0.63912725 <a title="54-lsi-12" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>13 0.63572651 <a title="54-lsi-13" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>14 0.63541901 <a title="54-lsi-14" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>15 0.63469106 <a title="54-lsi-15" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>16 0.63372558 <a title="54-lsi-16" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>17 0.60375261 <a title="54-lsi-17" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>18 0.59974009 <a title="54-lsi-18" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>19 0.57755333 <a title="54-lsi-19" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>20 0.5579325 <a title="54-lsi-20" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.014), (25, 0.055), (39, 0.011), (42, 0.017), (44, 0.013), (59, 0.181), (73, 0.06), (78, 0.023), (80, 0.021), (83, 0.118), (84, 0.019), (90, 0.174), (95, 0.026), (98, 0.167)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94273508 <a title="54-lda-1" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>Author: Marcus Dobrinkat ; Tero Tapiovaara ; Jaakko Vayrynen ; Kimmo Kettunen</p><p>Abstract: This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English.</p><p>2 0.91243517 <a title="54-lda-2" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>Author: Bharat Ram Ambati</p><p>Abstract: Statistical systems with high accuracy are very useful in real-world applications. If these systems can capture basic linguistic information, then the usefulness of these statistical systems improve a lot. This paper is an attempt at incorporating linguistic constraints in statistical dependency parsing. We consider a simple linguistic constraint that a verb should not have multiple subjects/objects as its children in the dependency tree. We first describe the importance of this constraint considering Machine Translation systems which use dependency parser output, as an example application. We then show how the current state-ofthe-art dependency parsers violate this constraint. We present two new methods to handle this constraint. We evaluate our methods on the state-of-the-art dependency parsers for Hindi and Czech. 1</p><p>same-paper 3 0.89859056 <a title="54-lda-3" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>4 0.83995926 <a title="54-lda-4" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>Author: Nathanael Chambers ; Dan Jurafsky</p><p>Abstract: This paper improves the use of pseudowords as an evaluation framework for selectional preferences. While pseudowords originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences. A selectional preference model ranks a set of possible arguments for a verb by their semantic fit to the verb. Pseudo-words serve as a proxy evaluation for these decisions. The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original. This paper studies two main aspects of pseudoword creation that affect performance results. (1) Pseudo-word evaluations often evaluate only a subset of the words. We show that selectional preferences should instead be evaluated on the data in its entirety. (2) Different approaches to selecting partner words can produce overly optimistic evaluations. We offer suggestions to address these factors and present a simple baseline that outperforms the state-ofthe-art by 13% absolute on a newspaper domain.</p><p>5 0.83923626 <a title="54-lda-5" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>6 0.83699965 <a title="54-lda-6" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>7 0.83682656 <a title="54-lda-7" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>8 0.83412009 <a title="54-lda-8" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>9 0.83290339 <a title="54-lda-9" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>10 0.83244419 <a title="54-lda-10" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>11 0.83164954 <a title="54-lda-11" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>12 0.83114946 <a title="54-lda-12" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>13 0.83077431 <a title="54-lda-13" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>14 0.8301782 <a title="54-lda-14" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>15 0.83011806 <a title="54-lda-15" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>16 0.82782513 <a title="54-lda-16" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>17 0.82732236 <a title="54-lda-17" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>18 0.82688624 <a title="54-lda-18" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>19 0.82489252 <a title="54-lda-19" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>20 0.82453936 <a title="54-lda-20" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
