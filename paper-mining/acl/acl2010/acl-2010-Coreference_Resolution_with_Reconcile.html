<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 acl-2010-Coreference Resolution with Reconcile</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-73" href="#">acl2010-73</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 acl-2010-Coreference Resolution with Reconcile</h1>
<br/><p>Source: <a title="acl-2010-73-pdf" href="http://aclweb.org/anthology//P/P10/P10-2029.pdf">pdf</a></p><p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>Reference: <a title="acl-2010-73-reference" href="../acl2010_reference/acl-2010-Coreference_Resolution_with_Reconcile_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. [sent-6, score-1.801]
</p><p>2 This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. [sent-7, score-0.939]
</p><p>3 Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. [sent-8, score-0.022]
</p><p>4 With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. [sent-9, score-1.46]
</p><p>5 Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res-  olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. [sent-10, score-2.245]
</p><p>6 We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets. [sent-11, score-0.683]
</p><p>7 1 Introduction Noun phrase coreference resolution (or simply coreference resolution) is the problem of identifying all noun phrases (NPs) that refer to the same entity in a text. [sent-12, score-1.407]
</p><p>8 The problem of coreference resolution is fundamental in the field of natural language processing (NLP) because of its usefulness for other NLP tasks, as well as the theoretical interest in understanding the computational mechanisms involved in government, binding and linguistic reference. [sent-13, score-0.847]
</p><p>9 Several formal evaluations have been conducted for the coreference resolution task (e. [sent-14, score-0.847]
</p><p>10 , MUC-6 (1995), ACE NIST (2004)), and the data sets created for these evaluations have become standard benchmarks in the field (e. [sent-16, score-0.056]
</p><p>11 However, it is still frustratingly difficult to compare results across different coreference resolution systems. [sent-19, score-0.874]
</p><p>12 Reported coreference resolu-  tion scores vary wildly across data sets, evaluation metrics, and system configurations. [sent-20, score-0.583]
</p><p>13 gov We believe that one root cause of these disparities is the high cost of implementing an end-toend coreference resolution system. [sent-27, score-0.893]
</p><p>14 Coreference resolution is a complex problem, and successful systems must tackle a variety of non-trivial subproblems that are central to the coreference task e. [sent-28, score-0.852]
</p><p>15 , mention/markable detection, anaphor identification and that require substantial implementation efforts. [sent-30, score-0.028]
</p><p>16 As a result, many researchers exploit gold-standard annotations, when available, as a substitute for component technologies to solve these subproblems. [sent-31, score-0.023]
</p><p>17 Unfortunately, the use of gold standard annotations for key/critical component technologies leads to an unrealistic evaluation setting, and makes it impossible to directly compare results against coreference resolvers that solve all of these subproblems from scratch. [sent-33, score-0.677]
</p><p>18 Comparison of coreference resolvers is further hindered by the use of several competing (and non-trivial) evaluation measures, and data sets that have substantially different task definitions and annotation formats. [sent-34, score-0.651]
</p><p>19 Additionally, coreference resolution is a pervasive problem in NLP and many NLP applications could benefit from an effective coreference resolver that can be easily configured and customized. [sent-35, score-1.507]
</p><p>20 To address these issues, we have created a platform for coreference resolution, called Reconcile, that can serve as a software infrastructure to sup-  port the creation of, experimentation with, and evaluation of coreference resolvers. [sent-36, score-1.176]
</p><p>21 Reconcile was designed with the following seven desiderata in mind: •  implement the basic underlying software ar156  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-37, score-0.11]
</p><p>22 While several other coreference resolution systems are publicly available (e. [sent-42, score-0.842]
</p><p>23 (2008)), none meets all seven of these desiderata (see Related Work). [sent-46, score-0.073]
</p><p>24 Reconcile is a modular software platform that abstracts the basic architecture of most contemporary supervised learningbased coreference resolution systems (e. [sent-47, score-1.014]
</p><p>25 (2001), Ng and Cardie (2002), Bengtson and Roth (2008)) and achieves performance comparable to the state-of-the-art on several benchmark data sets. [sent-50, score-0.058]
</p><p>26 Additionally, Reconcile can be easily reconfigured to use different algorithms, features, preprocessing elements, evaluation settings and metrics. [sent-51, score-0.066]
</p><p>27 In the rest of this paper, we review related work  (Section 2), describe Reconcile’s organization and components (Section 3) and show experimental results for Reconcile on six data sets and two evaluation metrics (Section 4). [sent-52, score-0.103]
</p><p>28 2  Related Work  Several coreference resolution systems are currently publicly available. [sent-53, score-0.823]
</p><p>29 , 2004) is an implementation of the Lappin and Leass’ (1994) Resolution of Anaphora Procedure (RAP). [sent-55, score-0.028]
</p><p>30 JavaRap resolves only pronouns and, thus, it is not directly comparable to Reconcile. [sent-56, score-0.027]
</p><p>31 , 2008) (which can be considered a successor of GuiTaR) are both modular systems that target the full coreference resolution task. [sent-58, score-0.857]
</p><p>32 As such, both systems come close to meeting the majority of the desiderata set forth in Section 1. [sent-59, score-0.054]
</p><p>33 In addition, the architecture and system components of  Reconcile (including a comprehensive set of features that draw on the expertise of state-of-the-art supervised learning approaches, such as Bengtson and Roth (2008)) result in performance closer to the state-of-the-art. [sent-61, score-0.118]
</p><p>34 Coreference resolution has received much research attention, resulting in an array of approaches, algorithms and features. [sent-62, score-0.304]
</p><p>35 Reconcile is modeled after typical supervised learning approaches to coreference resolution (e. [sent-63, score-0.865]
</p><p>36 However, there have been other approaches to coreference resolution, including unsupervised and semi-supervised approaches (e. [sent-67, score-0.583]
</p><p>37 McCallum and Wellner (2004) and Finley and Joachims (2005)), competition approaches (e. [sent-71, score-0.046]
</p><p>38 Most of these approaches rely on some notion of pairwise feature-based similarity and can be directly implemented in Reconcile. [sent-76, score-0.022]
</p><p>39 3 System Description Reconcile was designed to be a research testbed capable of implementing most current approaches to coreference resolution. [sent-77, score-0.617]
</p><p>40 Reconcile is written in Java, to be portable across platforms, and was designed to be easily reconfigurable with respect to subcomponents, feature sets, parameter settings, etc. [sent-78, score-0.068]
</p><p>41 The basic architecture of the system includes five major steps. [sent-83, score-0.057]
</p><p>42 Starting with a corpus of documents together with a manually annotated coreference resolution answer key1, Reconcile performs 1Only required during training. [sent-84, score-0.823]
</p><p>43 All of the extractors utilize a syntactic parse of the text and the output of a Named Entity (NE) extractor, but extract different constructs as specialized in the corresponding definition. [sent-95, score-0.028]
</p><p>44 The NP extractors successfully recognize about 95% of the NPs in the MUC and ACE gold standards. [sent-96, score-0.028]
</p><p>45 Using annotations produced during preprocessing, Reconcile produces feature vectors for pairs of NPs. [sent-99, score-0.025]
</p><p>46 Reconcile includes over 80 features, inspired by other successful coreference resolution systems such as Soon et al. [sent-101, score-0.823]
</p><p>47 Reconcile learns a classifier that operates on feature vectors representing  Table 1: Preprocessing components available in Reconcile. [sent-105, score-0.078]
</p><p>48 A clustering algorithm consolidates the predictions output by the classifier and forms the final set of coreference clusters (chains). [sent-109, score-0.585]
</p><p>49 Finally, during testing Reconcile runs scoring algorithms that compare the chains produced by the system to the goldstandard chains in the answer key. [sent-112, score-0.131]
</p><p>50 Each of the five steps above can invoke different components. [sent-113, score-0.018]
</p><p>51 Reconcile’s modularity makes it 2Some structured coreference resolution algorithms (e. [sent-114, score-0.843]
</p><p>52 , McCallum and Wellner (2004) and Finley and Joachims (2005)) combine the classification and clustering steps above. [sent-116, score-0.046]
</p><p>53 ),dk21w90it5o)lk8  Table 2: Available implementations for different modules available in Reconcile. [sent-119, score-0.04]
</p><p>54 easy for new components to be implemented and existing ones to be removed or replaced. [sent-120, score-0.064]
</p><p>55 Reconcile’s standard distribution comes with a comprehensive set of implemented components those available for steps 2–5 are shown in Table 2. [sent-121, score-0.078]
</p><p>56 Only about 15% of the code is concerned with running existing components in the preprocessing step, while the rest deals with NP extraction, implementations of features, clustering algorithms and scorers. [sent-123, score-0.128]
</p><p>57 More details about Recon–  cile’s architecture and available components and features can be found in Stoyanov et al. [sent-124, score-0.117]
</p><p>58 1 Data Sets Reconcile incorporates the six most commonly used coreference resolution data sets, two from the MUC conferences (MUC-6, 1995; MUC-7, 1997) and four from the ACE Program (NIST, 2004). [sent-127, score-0.881]
</p><p>59 Performance is evaluated according to the B3 and MUC scoring metrics. [sent-131, score-0.051]
</p><p>60 2 The Reconcile2010 Configuration Reconcile can be easily configured with different algorithms for markable detection, anaphoricity determination, feature extraction, etc. [sent-133, score-0.14]
</p><p>61 to differentiate it from the general  Reconcile2010  is configured using the  following components: 1. [sent-136, score-0.062]
</p><p>62 For all data sets,  scores are higher than MUC scores. [sent-147, score-0.022]
</p><p>63 B3  The MUC  score is highest for the MUC6 data set, while  B3  scores are higher for the ACE data sets as compared to the MUC data sets. [sent-148, score-0.054]
</p><p>64 Due to the difficulties  outlined in Section 1,  results for Reconcile presented here are directly  comparable only to a limited number of scores reported in the literature. [sent-149, score-0.049]
</p><p>65 The bottom three rows of Table 3 list these comparable scores, which show that Reconcile2010 exhibits state-ofthe-art performance for supervised learning-based coreference resolvers. [sent-150, score-0.586]
</p><p>66 A more detailed study of Reconcile-based coreference resolution systems in different evaluation scenarios can be found in Stoyanov et al. [sent-151, score-0.823]
</p><p>67 5 Conclusions Reconcile is a general architecture for coreference resolution that can be used to easily create various coreference resolvers. [sent-153, score-1.446]
</p><p>68 Reconcile provides broad support for experimentation in coreference resolution, including implementation of the basic architecture of contemporary state-of-the-art coreference systems and a variety of individual modules employed in these systems. [sent-154, score-1.261]
</p><p>69 Additionally, Reconcile handles all of the formatting and scoring peculiarities of the most widely used coreference resolution data sets (those created as part of the MUC and ACE conferences) and, thus, allows for easy implementation and evaluation  across these data sets. [sent-155, score-0.979]
</p><p>70 We hope that Reconcile will support experimental research in coreference resolution and provide a state-of-the-art coreference resolver for both researchers and application developers. [sent-156, score-1.418]
</p><p>71 We believe that in this way Reconcile will facilitate meaningful and consistent comparisons of coreference resolution systems. [sent-157, score-0.845]
</p><p>72 The full Reconcile release is available for download at http : / /www . [sent-158, score-0.019]
</p><p>73 424– – – – – – Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems. [sent-172, score-0.65]
</p><p>74 A mention-synchronous coreference resolution algorithm based on the bell tree. [sent-256, score-0.823]
</p><p>75 A general-purpose, off-the-shelf anaphora resolution module: implementation and preliminary evaluation. [sent-297, score-0.347]
</p><p>76 A public reference implementation of the rap anaphora resolution algorithm. [sent-306, score-0.38]
</p><p>77 Conundrums in noun phrase coreference resolution: Making sense of the state-of-the-art. [sent-321, score-0.566]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reconcile', 0.666), ('coreference', 0.539), ('resolution', 0.284), ('muc', 0.121), ('ace', 0.088), ('nps', 0.086), ('cardie', 0.073), ('stoyanov', 0.07), ('bengtson', 0.066), ('resolvers', 0.062), ('configured', 0.062), ('livermore', 0.062), ('bart', 0.059), ('architecture', 0.057), ('resolver', 0.056), ('soon', 0.055), ('desiderata', 0.054), ('scoring', 0.051), ('versley', 0.05), ('opennlp', 0.047), ('finley', 0.046), ('substituting', 0.046), ('poesio', 0.046), ('qiu', 0.044), ('ut', 0.043), ('contemporary', 0.042), ('gilbert', 0.042), ('lnl', 0.041), ('components', 0.041), ('ah', 0.04), ('preprocessing', 0.039), ('implementing', 0.037), ('buttler', 0.036), ('lappin', 0.036), ('anaphora', 0.035), ('experimentation', 0.035), ('modular', 0.034), ('javarap', 0.033), ('rap', 0.033), ('gov', 0.033), ('sets', 0.032), ('benchmark', 0.031), ('anaphoricity', 0.031), ('guitar', 0.031), ('wellner', 0.031), ('ng', 0.031), ('chains', 0.03), ('six', 0.03), ('subproblems', 0.029), ('kabadjov', 0.029), ('frustratingly', 0.029), ('roth', 0.029), ('implementation', 0.028), ('clustering', 0.028), ('conferences', 0.028), ('extractors', 0.028), ('noun', 0.027), ('message', 0.027), ('cornell', 0.027), ('luo', 0.027), ('np', 0.027), ('comparable', 0.027), ('easily', 0.027), ('annotations', 0.025), ('infrastructure', 0.025), ('understanding', 0.024), ('competition', 0.024), ('evaluations', 0.024), ('easy', 0.023), ('mccallum', 0.023), ('substitute', 0.023), ('lawrence', 0.023), ('facilitate', 0.022), ('unrealistic', 0.022), ('across', 0.022), ('scores', 0.022), ('approaches', 0.022), ('modules', 0.021), ('determination', 0.021), ('edu', 0.021), ('platform', 0.02), ('riloff', 0.02), ('supervised', 0.02), ('algorithms', 0.02), ('seven', 0.019), ('java', 0.019), ('designed', 0.019), ('available', 0.019), ('yang', 0.019), ('named', 0.019), ('nips', 0.019), ('joachims', 0.019), ('software', 0.018), ('haghighi', 0.018), ('classifier', 0.018), ('entity', 0.018), ('steps', 0.018), ('hindered', 0.018), ('bree', 0.018), ('cile', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="73-tfidf-1" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>2 0.53422922 <a title="73-tfidf-2" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>3 0.35266161 <a title="73-tfidf-3" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>4 0.26336932 <a title="73-tfidf-4" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>5 0.21351071 <a title="73-tfidf-5" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>6 0.17345537 <a title="73-tfidf-6" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>7 0.16504739 <a title="73-tfidf-7" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>8 0.1556976 <a title="73-tfidf-8" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>9 0.094004601 <a title="73-tfidf-9" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>10 0.080123156 <a title="73-tfidf-10" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>11 0.079230882 <a title="73-tfidf-11" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>12 0.057168253 <a title="73-tfidf-12" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>13 0.056608003 <a title="73-tfidf-13" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>14 0.054092798 <a title="73-tfidf-14" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>15 0.051836744 <a title="73-tfidf-15" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>16 0.043577578 <a title="73-tfidf-16" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>17 0.040519234 <a title="73-tfidf-17" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>18 0.035129488 <a title="73-tfidf-18" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>19 0.034854792 <a title="73-tfidf-19" href="./acl-2010-It_Makes_Sense%3A_A_Wide-Coverage_Word_Sense_Disambiguation_System_for_Free_Text.html">152 acl-2010-It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for Free Text</a></p>
<p>20 0.03477918 <a title="73-tfidf-20" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.135), (1, 0.108), (2, 0.01), (3, -0.311), (4, -0.186), (5, 0.457), (6, 0.006), (7, 0.023), (8, 0.049), (9, 0.167), (10, 0.074), (11, -0.073), (12, 0.02), (13, -0.097), (14, 0.045), (15, -0.024), (16, -0.018), (17, -0.096), (18, -0.123), (19, 0.007), (20, -0.034), (21, -0.001), (22, 0.002), (23, -0.088), (24, -0.007), (25, 0.015), (26, -0.033), (27, -0.035), (28, -0.038), (29, 0.01), (30, -0.06), (31, -0.06), (32, -0.002), (33, 0.025), (34, -0.037), (35, -0.024), (36, -0.056), (37, -0.037), (38, 0.028), (39, 0.062), (40, 0.007), (41, -0.028), (42, 0.078), (43, 0.045), (44, 0.015), (45, -0.03), (46, 0.01), (47, -0.014), (48, -0.079), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9832359 <a title="73-lsi-1" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>2 0.92941099 <a title="73-lsi-2" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>3 0.89270413 <a title="73-lsi-3" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>4 0.81881607 <a title="73-lsi-4" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>5 0.50526172 <a title="73-lsi-5" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>Author: Ryu Iida ; Syumpei Kobayashi ; Takenobu Tokunaga</p><p>Abstract: This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001 ; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on refer- ence resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.</p><p>6 0.45506343 <a title="73-lsi-6" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>7 0.44447577 <a title="73-lsi-7" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>8 0.43632805 <a title="73-lsi-8" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>9 0.41363838 <a title="73-lsi-9" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>10 0.35678571 <a title="73-lsi-10" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>11 0.30579311 <a title="73-lsi-11" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>12 0.25167432 <a title="73-lsi-12" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>13 0.19267718 <a title="73-lsi-13" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>14 0.17161615 <a title="73-lsi-14" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>15 0.16713773 <a title="73-lsi-15" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>16 0.16427648 <a title="73-lsi-16" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>17 0.13564566 <a title="73-lsi-17" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>18 0.13514952 <a title="73-lsi-18" href="./acl-2010-It_Makes_Sense%3A_A_Wide-Coverage_Word_Sense_Disambiguation_System_for_Free_Text.html">152 acl-2010-It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for Free Text</a></p>
<p>19 0.12818676 <a title="73-lsi-19" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>20 0.12417803 <a title="73-lsi-20" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.061), (39, 0.016), (42, 0.066), (57, 0.183), (59, 0.073), (73, 0.049), (78, 0.023), (79, 0.02), (80, 0.024), (83, 0.216), (84, 0.017), (88, 0.03), (98, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88469851 <a title="73-lda-1" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>2 0.80153054 <a title="73-lda-2" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>Author: Mark Sammons ; V.G.Vinod Vydiswaran ; Dan Roth</p><p>Abstract: We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment. We argue that the single global label with which RTE examples are annotated is insufficient to effectively evaluate RTE system performance; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will benefit not just RTE researchers, but the NLP community as a whole. We use insights from successful RTE systems to propose a model for identifying and annotating textual infer- ence phenomena in textual entailment examples, and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful.</p><p>3 0.78923875 <a title="73-lda-3" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>4 0.78826547 <a title="73-lda-4" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>Author: Emily Pitler ; Annie Louis ; Ani Nenkova</p><p>Abstract: To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference informa- tion, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.</p><p>5 0.78155279 <a title="73-lda-5" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>Author: Jenny Rose Finkel ; Christopher D. Manning</p><p>Abstract: One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. Experiments on joint parsing and named entity recog- nition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.</p><p>6 0.78081089 <a title="73-lda-6" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>7 0.78006983 <a title="73-lda-7" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>8 0.77140689 <a title="73-lda-8" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>9 0.77040344 <a title="73-lda-9" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>10 0.76699239 <a title="73-lda-10" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>11 0.76675534 <a title="73-lda-11" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>12 0.75896811 <a title="73-lda-12" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>13 0.75764453 <a title="73-lda-13" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>14 0.75612557 <a title="73-lda-14" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>15 0.75352818 <a title="73-lda-15" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>16 0.74194968 <a title="73-lda-16" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>17 0.74131364 <a title="73-lda-17" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>18 0.74102265 <a title="73-lda-18" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>19 0.74091023 <a title="73-lda-19" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>20 0.73735082 <a title="73-lda-20" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
