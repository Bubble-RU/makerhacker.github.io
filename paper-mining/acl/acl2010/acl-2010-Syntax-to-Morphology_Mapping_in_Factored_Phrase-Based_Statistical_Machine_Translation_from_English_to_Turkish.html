<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-221" href="#">acl2010-221</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</h1>
<br/><p>Source: <a title="acl-2010-221-pdf" href="http://aclweb.org/anthology//P/P10/P10-1047.pdf">pdf</a></p><p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>Reference: <a title="acl-2010-221-reference" href="../acl2010_reference/acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish  Reyyan Yeniterzi Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA reyyan @ c s . [sent-1, score-0.093]
</p><p>2 edu Abstract We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. [sent-3, score-0.356]
</p><p>3 Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. [sent-4, score-0.992]
</p><p>4 On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of  separating morphemes. [sent-5, score-0.983]
</p><p>5 We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. [sent-6, score-0.463]
</p><p>6 Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17. [sent-7, score-0.388]
</p><p>7 78 BLEU, all averaged over 10 training and test sets. [sent-9, score-0.05]
</p><p>8 1 Introduction Statistical machine translation into a morphologically complex language such as Turkish, Finnish or Arabic, involves the generation of target words with the proper morphology, in addition to prop-  erly ordering the target words. [sent-11, score-0.499]
</p><p>9 Earlier work on translation from English to Turkish (Oflazer and Kemal Oflazer Computer Science Carnegie Mellon University-Qatar PO Box 24866, Doha, Qatar ko @ c s . [sent-12, score-0.049]
</p><p>10 The main reason given for these problems was that the same statistical translation, reordering and language modeling mechanisms were being employed to both determine the morphological structure of the words and, at the same time, get the global order of the words correct. [sent-18, score-0.434]
</p><p>11 Even though a significant improvement of a standard word-based baseline was achieved, further analysis hinted at a direction where morphology and syntax on the Turkish side had to be dealt with using separate mechanisms. [sent-19, score-0.419]
</p><p>12 On the English side, we rely on a full syntactic analysis using a dependency parser. [sent-21, score-0.173]
</p><p>13 This analysis then lets us abstract and encode many local and some nonlocal syntactic structures as complex tags (dynamically, as opposed to the static complex tags as proposed by Birch et al. [sent-22, score-1.184]
</p><p>14 c As2s0o1c0ia Atisosnoc foiart Cionom fopru Ctaotmiopnuatla Lti on gaulis Lti cnsg,u piasgtiecs 454–464, we can bring the representation of English syntax closer to the Turkish morphosyntax. [sent-27, score-0.148]
</p><p>15 (ii) Continuous and discontinuous variants of certain (syntactic) phrases can be conflated during the SMT phrase extraction process. [sent-29, score-0.159]
</p><p>16 (iii) The length of the English sentences can be dramatically reduced, as most function words encoding syntax are now abstracted into complex tags. [sent-30, score-0.349]
</p><p>17 (iv) The representation ofboth the source and the target sides of the parallel corpus can now be mostly normalized. [sent-31, score-0.197]
</p><p>18 This facilitates the use of factored phrase-based translation that was not previously applicable due to the morphological complexity on the target side and mismatch between source and target morphologies. [sent-32, score-0.827]
</p><p>19 We find that with the full set of syntax-tomorphology transformations and some additional  techniques we can get about 39% relative improvement in BLEU scores over a word-based baseline and about 28% improvement of a factored baseline, all experiments being done over 10 training and test sets. [sent-33, score-0.417]
</p><p>20 We also find that further constituent reordering taking advantage of the syntactic analysis of the source side, does not provide tangible improvements when averaged over the 10 data sets. [sent-34, score-0.446]
</p><p>21 Section 4 summarizes our constituent reordering experiments and their results. [sent-37, score-0.172]
</p><p>22 We assume that the reader is familiar with the basics of phrase-based statistical machine translation (Koehn et al. [sent-39, score-0.084]
</p><p>23 , 2003) and factored statistical machine translation (Koehn and Hoang, 2007). [sent-40, score-0.156]
</p><p>24 2  Syntax-to-Morphology Mapping  In this section, we describe how we map between certain source language syntactic structures and target words with complex morphological structures. [sent-41, score-0.739]
</p><p>25 At the top of Figure 1, we see a pair of (syntactic) phrases, where we have (positionally) aligned the words that should be translated to each other. [sent-42, score-0.081]
</p><p>26 We can note that the function words on and  Figure 1: Transformation of an English prepositional phrase their are not really aligned to any of the Turkish words as they really correspond to two of the morphemes of the last Turkish word. [sent-43, score-0.515]
</p><p>27 1 The basic idea in our approach is to take various function words on the English side, whose syntactic relationships are identified by the parser, and then package them as complex tags on the related content words. [sent-45, score-0.545]
</p><p>28 So, in this example, if we move the first two function words from the English side and attach as syntactic tags to the word they are in dependency relation with, we get the aligned representation at the bottom of Figure 1. [sent-46, score-1.044]
</p><p>29 2,3 Here we can note that all root words and tags that correspond to each other are nicely structured and are in the same relative order. [sent-47, score-0.392]
</p><p>30 In fact, we can treat each token as being composed of two factors: the roots and the accompanying tags. [sent-48, score-0.079]
</p><p>31 The tags on the Turkish side encode morphosyntactic information encoded in the morphology of the words, while the 1The meanings of various tags are as follows: Dependency Labels: PMOD - Preposition Modifier; POS - Pos-  sessive. [sent-49, score-0.866]
</p><p>32 Note that we mark an English plural noun as +NN NNS to indicate that the root is a noun and there is a plural morpheme on it. [sent-52, score-0.371]
</p><p>33 Note also that economic is also related to relations but we are not interested in such content words and their relations. [sent-53, score-0.192]
</p><p>34 2We use to prefix such syntactic tags on the English side. [sent-54, score-0.3]
</p><p>35 3The order is important in that we would like to attach the same sequence of function words in the same order so that the resulting tags on the English side are the same. [sent-55, score-0.791]
</p><p>36 e  455  (complex) tags on the English side encode local (and sometimes, non-local) syntactic information. [sent-56, score-0.683]
</p><p>37 Furthermore,  we can see that before the transfor-  mations, the English side has terwards it has only  2 words. [sent-57, score-0.266]
</p><p>38 4  words, while af-  We find (and elab-  orate later) that this reduction in the English side  of the training corpus, in general, is about 30%, and is correlated with improved BLEU scores. [sent-58, score-0.266]
</p><p>39 We believe the removal of many function words and their folding into complex tags (which do not get involved in GIZA++ alignment  –  we only align the  root words) seems to improve alignment are  less number of “words”  to worry  as there  about during  that process. [sent-59, score-0.707]
</p><p>40 4 Another interesting side effect of this representation is the following. [sent-60, score-0.304]
</p><p>41 As the complex syntactic tags on the English side are based on syntactic relations and not necessarily positional proximity, the tag for relations in a phrase like in their cultural, historical and economic relations would be exactly the same as above. [sent-61, score-1.183]
</p><p>42 Thus phrase extraction algorithms can conflate all constructs like in their . [sent-62, score-0.055]
</p><p>43 economic relations as one phrase, regardless of the intervening modifiers, assuming that parser does its job properly. [sent-65, score-0.15]
</p><p>44 Not all cases can be captured as cleanly as the example above, but most transformations capture local and nonlocal syntax involving many function words and then encode syntax with complex tags resembling full morphological tags on the Turkish side. [sent-66, score-1.609]
</p><p>45 These transformations, however, are not meant to perform sentence level constituent reordering on the English side. [sent-67, score-0.172]
</p><p>46 We developed set of about 20 linguisticallymotivated syntax-to-morphology transformations which had variants parameterized depending on what, for instance, the preposition or the adverbial  was, and how they map to morphological structure on the Turkish side. [sent-69, score-0.723]
</p><p>47 It is also possible that multiple transformations can apply to generate a single English complex tag: a portion of the tag can come from a verb complex transformation, and another from an adverbial phrase transformation involving a marked such as while. [sent-78, score-0.889]
</p><p>48 Our transformations handle the following cases: • •  Prepositions attach to the head-word of their  Prepositi  4Fraser (2009) uses the first four letters of German words after morphological stripping and compound decomposition to help with alignment in German to English and reverse translation. [sent-79, score-0.763]
</p><p>49 •  •  complement noun phrase as a component in its complex tag. [sent-80, score-0.262]
</p><p>50 Possessive pronouns attach to the head-word tPhoesys espsseicveify p. [sent-81, score-0.233]
</p><p>51 The possessive markers following a noun (Tshepear paotsesde s bsyiv tehe m taorkkeenrsize fro)l aotwtaicnhged a ato n otuhen noun. [sent-82, score-0.285]
</p><p>52 •  • •  •  •  Auxiliary verbs and negation markers attach tAou uthxiel laerxyic vaelr vbesr ban tdha nte etghaetyio ofnor mma rak kveersrb a cttoamchplex with. [sent-83, score-0.468]
</p><p>53 Forms of be used as predicates with adjectivFoalr mors n oofm biena usl edde pasen pdreendtisc aatettasc wh ttoh athdjee cdtei-pendent. [sent-85, score-0.078]
</p><p>54 Forms of be or have used to form passive vFooricme sw oitfh past r pa hrativceip lues evder btos, faonrdm fo pramsssi voef be used with -ing verbs to form present continuous verbs, attach to the verb. [sent-86, score-0.361]
</p><p>55 Various adverbial clauses formed with if, wVharilieo,u sw ahdevne, ebtiacl. [sent-87, score-0.129]
</p><p>56 , alareu eresor fgoarnmizeedd wsiot hth iaf,t these markers attach to the head verb of the clause. [sent-88, score-0.386]
</p><p>57 As stated earlier, these rules are linguistically motivated and are based on the morphological structure of the target language words. [sent-89, score-0.283]
</p><p>58 Hence for different target languages these rules will be different. [sent-90, score-0.069]
</p><p>59 The rules recognize various local and nonlocal syntactic structures in the source side parse tree that correspond to complex morphological of target words and then remove source function words folding them into complex tags. [sent-91, score-1.553]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('turkish', 0.514), ('side', 0.266), ('transformations', 0.233), ('attach', 0.233), ('morphological', 0.214), ('tags', 0.21), ('morphemes', 0.175), ('complex', 0.163), ('oflazer', 0.132), ('nonlocal', 0.132), ('possessive', 0.121), ('english', 0.11), ('factored', 0.107), ('plural', 0.106), ('economic', 0.099), ('reordering', 0.096), ('reyyan', 0.093), ('syntactic', 0.09), ('adverbial', 0.087), ('morphology', 0.084), ('tangible', 0.081), ('roots', 0.079), ('markers', 0.079), ('constituent', 0.076), ('folding', 0.074), ('root', 0.071), ('syntax', 0.069), ('target', 0.069), ('morphologically', 0.066), ('preposition', 0.066), ('bleu', 0.065), ('nns', 0.063), ('structures', 0.062), ('carnegie', 0.06), ('mellon', 0.06), ('encode', 0.059), ('local', 0.058), ('transformation', 0.058), ('tag', 0.057), ('phrase', 0.055), ('cmu', 0.054), ('source', 0.053), ('relations', 0.051), ('averaged', 0.05), ('translation', 0.049), ('koehn', 0.047), ('map', 0.046), ('dependency', 0.046), ('continuous', 0.045), ('really', 0.045), ('noun', 0.044), ('sw', 0.042), ('words', 0.042), ('bring', 0.041), ('verb', 0.041), ('ttoh', 0.041), ('mma', 0.041), ('erly', 0.041), ('faonrdm', 0.041), ('resembling', 0.041), ('linguisticallymotivated', 0.041), ('rak', 0.041), ('ato', 0.041), ('kemal', 0.041), ('stripping', 0.041), ('function', 0.04), ('get', 0.04), ('aligned', 0.039), ('representation', 0.038), ('nicely', 0.037), ('overt', 0.037), ('oofm', 0.037), ('morphosyntactic', 0.037), ('ofboth', 0.037), ('lets', 0.037), ('tdha', 0.037), ('ban', 0.037), ('full', 0.037), ('variants', 0.036), ('smt', 0.035), ('nn', 0.035), ('locative', 0.035), ('malt', 0.035), ('abstracted', 0.035), ('basics', 0.035), ('radically', 0.035), ('discontinuous', 0.035), ('disparate', 0.035), ('align', 0.034), ('essentially', 0.034), ('conflated', 0.033), ('hth', 0.033), ('wherever', 0.033), ('cultural', 0.033), ('modals', 0.033), ('worry', 0.033), ('correspond', 0.032), ('involving', 0.032), ('analyze', 0.032), ('mapping', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="221-tfidf-1" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>2 0.18847702 <a title="221-tfidf-2" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>3 0.18327378 <a title="221-tfidf-3" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>4 0.11977988 <a title="221-tfidf-4" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>5 0.11228514 <a title="221-tfidf-5" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>6 0.11148409 <a title="221-tfidf-6" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>7 0.10092653 <a title="221-tfidf-7" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>8 0.09961921 <a title="221-tfidf-8" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>9 0.096042402 <a title="221-tfidf-9" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>10 0.094487354 <a title="221-tfidf-10" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>11 0.092741564 <a title="221-tfidf-11" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>12 0.090745367 <a title="221-tfidf-12" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>13 0.087946258 <a title="221-tfidf-13" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>14 0.086382315 <a title="221-tfidf-14" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>15 0.081600793 <a title="221-tfidf-15" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>16 0.080445558 <a title="221-tfidf-16" href="./acl-2010-Discourse_Structure%3A_Theory%2C_Practice_and_Use.html">86 acl-2010-Discourse Structure: Theory, Practice and Use</a></p>
<p>17 0.079114795 <a title="221-tfidf-17" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>18 0.077681214 <a title="221-tfidf-18" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>19 0.074012682 <a title="221-tfidf-19" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>20 0.071279049 <a title="221-tfidf-20" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.2), (1, -0.131), (2, 0.013), (3, -0.014), (4, 0.003), (5, 0.009), (6, 0.014), (7, 0.022), (8, 0.009), (9, 0.075), (10, 0.078), (11, 0.144), (12, 0.025), (13, 0.028), (14, -0.002), (15, -0.038), (16, -0.009), (17, 0.131), (18, 0.234), (19, -0.079), (20, -0.048), (21, -0.017), (22, 0.012), (23, 0.069), (24, 0.161), (25, 0.101), (26, -0.109), (27, -0.057), (28, -0.072), (29, 0.005), (30, 0.046), (31, -0.022), (32, -0.052), (33, -0.054), (34, 0.013), (35, -0.084), (36, 0.071), (37, -0.036), (38, -0.086), (39, 0.172), (40, -0.094), (41, -0.016), (42, 0.025), (43, -0.049), (44, -0.007), (45, -0.044), (46, 0.129), (47, 0.071), (48, -0.019), (49, -0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95906311 <a title="221-lsi-1" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>2 0.62646884 <a title="221-lsi-2" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>3 0.62626863 <a title="221-lsi-3" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>4 0.6249184 <a title="221-lsi-4" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>Author: Sebastian Spiegler ; Peter A. Flach</p><p>Abstract: This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can significantly improve the performance of machine learning methods for morphological word decomposition. We employ two algorithms which come from a family of generative probabilistic models. The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. The advantage of this model family is that it can learn from small datasets and easily gen- eralises to larger datasets. The first algorithm PROMODES, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm PROMODES-H is a novel development of the first using a higher order model. We present the mathematical description for both algorithms, conduct experiments on the morphologically rich language Zulu and compare characteristics of both algorithms based on the experimental results.</p><p>5 0.61206466 <a title="221-lsi-5" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>Author: Seth Kulick</p><p>Abstract: We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.</p><p>6 0.59955949 <a title="221-lsi-6" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>7 0.56596333 <a title="221-lsi-7" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>8 0.51634145 <a title="221-lsi-8" href="./acl-2010-The_Use_of_Formal_Language_Models_in_the_Typology_of_the_Morphology_of_Amerindian_Languages.html">234 acl-2010-The Use of Formal Language Models in the Typology of the Morphology of Amerindian Languages</a></p>
<p>9 0.45631507 <a title="221-lsi-9" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>10 0.45147401 <a title="221-lsi-10" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>11 0.44826204 <a title="221-lsi-11" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>12 0.42063224 <a title="221-lsi-12" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>13 0.3885828 <a title="221-lsi-13" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>14 0.38835061 <a title="221-lsi-14" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>15 0.37986985 <a title="221-lsi-15" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>16 0.37790224 <a title="221-lsi-16" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>17 0.37455431 <a title="221-lsi-17" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>18 0.37372881 <a title="221-lsi-18" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>19 0.36334017 <a title="221-lsi-19" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>20 0.35793859 <a title="221-lsi-20" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.034), (59, 0.074), (73, 0.022), (78, 0.02), (83, 0.046), (84, 0.024), (98, 0.669)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99505424 <a title="221-lda-1" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>Author: Yabin Zheng ; Zhiyuan Liu ; Lixing Xie</p><p>Abstract: Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance. 1</p><p>same-paper 2 0.99376488 <a title="221-lda-2" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>3 0.99329144 <a title="221-lda-3" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>Author: Kotaro Kitagawa ; Kumiko Tanaka-Ishii</p><p>Abstract: Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average pars- ing time not much slower than O(n).</p><p>4 0.98795646 <a title="221-lda-4" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>Author: David Vickrey ; Oscar Kipersztok ; Daphne Koller</p><p>Abstract: We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</p><p>5 0.98488224 <a title="221-lda-5" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>Author: Xiangyu Duan ; Min Zhang ; Haizhou Li</p><p>Abstract: The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus. But word appears to be too fine-grained in some cases such as non-compositional phrasal equivalences, where no clear word alignments exist. Using words as inputs to PBSMT pipeline has inborn deficiency. This paper proposes pseudo-word as a new start point for PB-SMT pipeline. Pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation. By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in a monolingual way and a bilingual synchronous way. Experiments show that pseudo-word significantly outperforms word for PB-SMT model in both travel translation domain and news translation domain. 1</p><p>6 0.97992557 <a title="221-lda-6" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>7 0.97373897 <a title="221-lda-7" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>8 0.94897878 <a title="221-lda-8" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>9 0.94351667 <a title="221-lda-9" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>10 0.93087858 <a title="221-lda-10" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>11 0.91860491 <a title="221-lda-11" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>12 0.88235366 <a title="221-lda-12" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>13 0.88206583 <a title="221-lda-13" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>14 0.87391835 <a title="221-lda-14" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>15 0.87339526 <a title="221-lda-15" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>16 0.85966784 <a title="221-lda-16" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>17 0.85695583 <a title="221-lda-17" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>18 0.85090226 <a title="221-lda-18" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>19 0.84714305 <a title="221-lda-19" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>20 0.84665322 <a title="221-lda-20" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
