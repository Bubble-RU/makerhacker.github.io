<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-57" href="#">acl2010-57</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2010-57-pdf" href="http://aclweb.org/anthology//P/P10/P10-1088.pdf">pdf</a></p><p>Author: Michael Bloodgood ; Chris Callison-Burch</p><p>Abstract: We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.</p><p>Reference: <a title="acl-2010-57-reference" href="../acl2010_reference/acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. [sent-2, score-0.233]
</p><p>2 The main challenge is how to buck the trend of diminishing returns that is commonly encountered. [sent-3, score-0.468]
</p><p>3 We present an active learning-style data solicitation algorithm to meet this challenge. [sent-4, score-0.043]
</p><p>4 We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement. [sent-5, score-0.112]
</p><p>5 1 Introduction Figure 1shows the learning curves for two state of the art statistical machine translation (SMT) systems for Urdu-English translation. [sent-6, score-0.208]
</p><p>6 Observe how the learning curves rise rapidly at first but then a trend of diminishing returns occurs: put simply, the curves flatten. [sent-7, score-0.591]
</p><p>7 This paper investigates whether we can buck the trend of diminishing returns, and if so, how we can do it effectively. [sent-8, score-0.392]
</p><p>8 , 2009; Haffari and Sarkar, 2009) but they were interested in starting with a tiny seed set of data, and they stopped their investigations after only adding a relatively tiny amount of data as depicted in Figure 1. [sent-10, score-0.275]
</p><p>9 We develop an AL algorithm that focuses on keeping annotation costs (measured by time in seconds) low. [sent-12, score-0.295]
</p><p>10 It succeeds in doing this by only soliciting translations for parts of sentences. [sent-13, score-0.23]
</p><p>11 We show that this  gets a savings in human annotation time above and beyond what the reduction in # words annotated would have indicated by a factor of about three and speculate as to why. [sent-14, score-0.137]
</p><p>12 edu JSyntax and JHier Learning Curves on the LDC Urdu−English Language Pack (BLEU vs Sentences)  Figure 1: Syntax-based and Hierarchical PhraseBased MT systems’ learning curves on the LDC Urdu-English language pack. [sent-17, score-0.341]
</p><p>13 The x-axis measures the number of sentence pairs in the training data. [sent-18, score-0.092]
</p><p>14 Note the diminishing returns as more data is added. [sent-20, score-0.264]
</p><p>15 Also note how relatively early on in the process previous studies were terminated. [sent-21, score-0.059]
</p><p>16 In contrast, the focus of our main experiments doesn’t even be-  gin until much higher performance has already been achieved with a period of diminishing returns firmly established. [sent-22, score-0.294]
</p><p>17 We conduct experiments for Urdu-English translation, gathering annotations via Amazon Mechanical Turk (MTurk) and show that we can indeed buck the trend of diminishing returns, achieving an order of magnitude increase in the rate of improvement in performance. [sent-23, score-0.538]
</p><p>18 Section 2 discusses related work; Section 3 discusses preliminary experiments that show the guiding principles behind the algorithm we use; Section 4 explains our method for soliciting new translation data; Section 5 presents our main results; and Section 6 concludes. [sent-24, score-0.33]
</p><p>19 c As2s0o1c0ia Atisosnoc foiart Cionom fopru Ctaotmiopnuatla Lti on gaulis Lti cnsg,u piasgtiecs 854–864, 2  Related Work  Active learning has been shown to be effective for improving NLP systems and reducing annotation burdens for a number of NLP tasks (see,  e. [sent-27, score-0.197]
</p><p>20 The vast majority of AL research has not focused on accurate cost accounting and a typical assumption is that each annotatable has equal annotation cost. [sent-36, score-0.263]
</p><p>21 An early exception in the AL for NLP field was the work of Hwa (2000), which makes a point of  using # of brackets to measure cost for a syntactic analysis task instead of using # of sentences. [sent-37, score-0.14]
</p><p>22 Another relatively early work in our field along these lines was the work of Ngai and Yarowsky (2000), which measured actual times of annotation to compare the efficacy of rule writing versus annotation with AL for the task of BaseNP chunking. [sent-38, score-0.279]
</p><p>23 Osborne and Baldridge (2004) argued for the use of discriminant cost over unit cost for the task of Head Phrase Structure Grammar parse selection. [sent-39, score-0.226]
</p><p>24 The robot chooses which experiments to conduct by using AL and takes monetary costs (in pounds sterling) into account during AL selection and evaluation. [sent-42, score-0.279]
</p><p>25 Unlike our situation for SMT, their costs are all known beforehand because they are simply the cost of materials to conduct the experiments, which are already known to the robot. [sent-43, score-0.299]
</p><p>26 This work is related to ours because it shows that how examples are se-  lected can impact the cost of annotation, an idea we turn around to use for our advantage when developing our data selection algorithm. [sent-46, score-0.198]
</p><p>27 (2008) emphasize measuring costs carefully for AL for POS tagging. [sent-48, score-0.199]
</p><p>28 They develop a model based on a user study that can estimate the time required for POS annotating. [sent-49, score-0.063]
</p><p>29 (2007) assign costs for AL based on message length for a voicemail classification task. [sent-51, score-0.122]
</p><p>30 In contrast, we show for SMT that annotation times do not scale according to length in words and we show our method can achieve a speedup in annotation time above and beyond what the reduction in words would indicate. [sent-52, score-0.247]
</p><p>31 Tomanek and Hahn (2009) measure cost by # of tokens for an NER task. [sent-53, score-0.113]
</p><p>32 Their AL method only solicits labels for parts of sentences in the interest of reducing annotation effort. [sent-54, score-0.241]
</p><p>33 Along these lines, our method is similar in the respect that we also will only solicit annotation for parts of sentences, though we prefer to measure cost with time and we show that time doesn’t track with token length for SMT. [sent-55, score-0.304]
</p><p>34 Also, by SMT standards, they only add a very tiny amount of data during AL. [sent-62, score-0.063]
</p><p>35 All their simulations top out at 10,000 sentences of labeled data and the models learned have relatively low translation quality compared to the state of the art. [sent-63, score-0.209]
</p><p>36 On the other hand, in the current paper, we demonstrate how to apply AL in situations where we already have large corpora. [sent-64, score-0.057]
</p><p>37 Our goal is to buck the trend of diminishing returns and use AL to add data to build some of the highest-performing MT systems in the world while keeping annotation costs low. [sent-65, score-0.7]
</p><p>38 , 2009; Haffari and Sarkar, 2009) stop their investigations with where we begin our studies. [sent-67, score-0.141]
</p><p>39 , 2009; Haffari and Sarkar, 2009) measure annotation cost by # of sentences. [sent-69, score-0.223]
</p><p>40 In contrast, we bring to light some potential drawbacks of this practice, showing it can lead to different conclusions than if other annotation cost metrics are used, such as time and money, which are the metrics that we use. [sent-70, score-0.25]
</p><p>41 855  3  Simulation Experiments  Here we report on results of simulation experiments that help to illustrate and motivate the design decisions of the algorithm we present in Section 4. [sent-71, score-0.045]
</p><p>42 In addition, the language pack contains an Urdu-English dictionary consisting of ≈ 114000 entries. [sent-75, score-0.086]
</p><p>43 In all the ex-  periments, we use t≈he 1 dictionary at every ilte thraet eioxnof training. [sent-76, score-0.039]
</p><p>44 This will make it harder for us to show our methods providing substantial gains since the dictionary will provide a higher base performance to begin with. [sent-77, score-0.069]
</p><p>45 However, it would be artificial to ignore dictionary resources when they exist. [sent-78, score-0.039]
</p><p>46 We experiment with two translation models: hierarchical phrase-based translation (Chiang, 2007) and syntax augmented translation (Zollmann and Venugopal, 2006), both of which are implemented in the Joshua decoder (Li et al. [sent-79, score-0.264]
</p><p>47 how to automatically detect when to stop soliciting annotations from a pool of data. [sent-86, score-0.274]
</p><p>48 1 Annotation Costs We begin our cost investigations with four simple methods for growing MT training data: random, shortest, longest, and VocabGrowth sentence selection. [sent-88, score-0.217]
</p><p>49 VocabGrowth (hereafter VG) selection is modeled after the best methods from previous work (Haffari et al. [sent-90, score-0.085]
</p><p>50 , 2009; Haffari and Sarkar, 2009), which are based on preferring sentences that contain phrases that occur frequently in unlabeled data and infrequently in the so-far labeled data. [sent-91, score-0.174]
</p><p>51 Our VG method selects sentences for translation that contain n-grams (for n in {1,2,3,4}) that tion  that  conta  1LDC Catalog No. [sent-92, score-0.132]
</p><p>52 d tr rfiigndg ethre ← ←fir Gsto n-gram sthoratt eisdnN’tG Gcrovaemresd l i snt the so far labeled training data. [sent-99, score-0.045]
</p><p>53 Remove selectedSentence from unlabeled data and add it to labeled training data. [sent-104, score-0.079]
</p><p>54 End Loop Figure 2: The VG sentence selection algorithm do not occur at all in our so-far labeled data. [sent-105, score-0.13]
</p><p>55 We call an n-gram “covered” if it occurs at least once in our so-far labeled data. [sent-106, score-0.045]
</p><p>56 Figure 3 shows the learning curves for both jHier and jSyntax for VG selection and random selection. [sent-109, score-0.253]
</p><p>57 , 2002),which is a fast automatic way of measuring translation quality that has been shown  to correlate with human judgments and is perhaps the most widely used metric in the MT community. [sent-111, score-0.135]
</p><p>58 The x-axis measures the number of sentence translation pairs in the training data. [sent-112, score-0.18]
</p><p>59 The VG curves are cut off at the point at which the stopping criterion in Section 3. [sent-113, score-0.344]
</p><p>60 From Figure 3 it might appear that VG selection is better than random selection, achieving higher-performing systems with fewer translations in the labeled data. [sent-115, score-0.232]
</p><p>61 However, it is important to take care when measuring annotation costs (especially for relatively complicated tasks such as translation). [sent-116, score-0.346]
</p><p>62 Figure 4 shows the learning curves for the same systems and selection methods as in Figure 3 but now the x-axis measures the number of foreign words in the training data. [sent-117, score-0.374]
</p><p>63 The difference between VG and random selection now appears smaller. [sent-118, score-0.133]
</p><p>64 For an extreme case, to illustrate the ramifica856  jHier and jSyntax:  VG vs  Random selection (BLEU  vs Sents)  Figure 3: Random vs VG selection. [sent-119, score-0.748]
</p><p>65 The x-axis measures the number of sentence pairs in the training data. [sent-120, score-0.092]
</p><p>66 tions of measuring translation annotation cost by # of sentences versus # of words, consider Figures 5 and 6. [sent-122, score-0.402]
</p><p>67 They both show the same three selection methods but Figure 5 measures the x-axis by # of  sentences and Figure 6 measures by # of words. [sent-123, score-0.313]
</p><p>68 In Figure 5, one would conclude that shortest is a far inferior selection method to longest but in Figure 6 one would conclude the opposite. [sent-124, score-0.265]
</p><p>69 Measuring annotation time and cost in dollars are probably the most important measures of annotation cost. [sent-125, score-0.499]
</p><p>70 We can’t measure these for the simulated experiments but we will use time (in seconds) and money (in US dollars) as cost measures in Section 5, which discusses our nonsimulated AL experiments. [sent-126, score-0.328]
</p><p>71 If # sentences or # words track these other more relevant costs in predictable known relationships, then it would suffice to measure # sentences or # words instead. [sent-127, score-0.21]
</p><p>72 But it’s clear that different sentences can have very different annotation time requirements according to how long and complicated they are so we will not use # sentences as an annotation cost any more. [sent-128, score-0.483]
</p><p>73 It is not as clear how # words tracks with annotation time. [sent-129, score-0.11]
</p><p>74 In Section 5 we will present evidence showing that time per word can vary considerably and also show a method for soliciting annotations that reduces time per word by nearly a factor of three. [sent-130, score-0.259]
</p><p>75 As it is prudent to evaluate using accurate cost accounting, so it is also prudent to develop new AL algorithms that take costs carefully into account. [sent-131, score-0.419]
</p><p>76 Hence, reducing annotation time burdens jHier and jSyntax: VG vs Random selection (BLEU vs FWords)  Figure 4: Random vs VG selection. [sent-132, score-0.972]
</p><p>77 The x-axis measures the number of foreign words in the train-  ing data. [sent-133, score-0.169]
</p><p>78 instead of the # of sentences translated (which might be quite a different thing) will be a cornerstone of the algorithm we describe in Section 4. [sent-135, score-0.083]
</p><p>79 2 Managing Uncertainty One of the most successful of all AL methods developed to date is uncertainty sampling and it has been applied successfully many times (e. [sent-137, score-0.071]
</p><p>80 However, with MT being a relatively complicated task (compared with binary classification, for example), it might be the case that the uncertainty approach has to be re-considered. [sent-141, score-0.138]
</p><p>81 If words have never occurred in the training data, then uncertainty can be expected to be high. [sent-142, score-0.071]
</p><p>82 The x-axis measures the number of sentence pairs in the training data. [sent-146, score-0.092]
</p><p>83 3 Automatic Stopping The problem of automatically detecting when to stop AL is a substantial one, discussed at length in the literature (e. [sent-154, score-0.037]
</p><p>84 In our simulation, we stop VG once all n-grams (n in {1,2,3,4}) have been covered. [sent-157, score-0.037]
</p><p>85 Though simple, nth {is1 stopping carvieter bieoenn seems dto. [sent-158, score-0.134]
</p><p>86 3% of the annotation (in terms of words) and actually achieves slightly higher BLEU scores than if all the data were used. [sent-167, score-0.11]
</p><p>87 52 Number of Foreign Words in Training Datax  106  Figure 6: Random vs Shortest vs Longest selection. [sent-170, score-0.442]
</p><p>88 The x-axis measures the number of foreign  words in the training data. [sent-171, score-0.169]
</p><p>89 4  Highlighted N-Gram Method  In this section we describe a method for soliciting human translations that we have applied successfully to improving translation quality in real (not simulated) conditions. [sent-176, score-0.318]
</p><p>90 HNG solicits translations only for trigger n-grams and not for entire sentences. [sent-178, score-0.189]
</p><p>91 We provide sentential context, highlight the trigger n-gram that we want translated, and ask for a translation of just the highlighted trigger n-gram. [sent-179, score-0.309]
</p><p>92 HNG asks for translations for triggers in the same order that the triggers are encountered by the algorithm in Figure 2. [sent-180, score-0.13]
</p><p>93 A screenshot of our interface is depicted in Figure 8. [sent-181, score-0.043]
</p><p>94 The same stopping criterion is used as was used in the last section. [sent-182, score-0.196]
</p><p>95 When the stopping criterion be-  comes true, it is time to tap a new unlabeled pool of foreign text, if available. [sent-183, score-0.366]
</p><p>96 Our motivations for soliciting translations for only parts of sentences are twofold, corresponding to two possible cases. [sent-184, score-0.274]
</p><p>97 Case one is that a translation model learned from the so-far labeled data will be able to translate most of the non-trigger words in the sentence correctly. [sent-185, score-0.133]
</p><p>98 Thus, by asking a human to translate only the trigger words, we avoid wasting human translation effort. [sent-186, score-0.172]
</p><p>99 (We will show in 858  jHiero:  VG vs mostNew vs  moderateNew  Figure 7: VG vs MostNew vs ModerateNew selection. [sent-187, score-0.884]
</p><p>100 The x-axis measures the number of sentence pairs in the training data. [sent-188, score-0.092]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vg', 0.448), ('haffari', 0.257), ('al', 0.231), ('vs', 0.221), ('jhier', 0.205), ('jsyntax', 0.205), ('diminishing', 0.188), ('mostnew', 0.176), ('soliciting', 0.176), ('bloodgood', 0.154), ('bleu', 0.145), ('stopping', 0.134), ('costs', 0.122), ('curves', 0.12), ('buck', 0.117), ('moderatenew', 0.117), ('cost', 0.113), ('annotation', 0.11), ('longest', 0.093), ('measures', 0.092), ('hng', 0.088), ('jhiero', 0.088), ('translation', 0.088), ('shortest', 0.087), ('trend', 0.087), ('sarkar', 0.086), ('selection', 0.085), ('trigger', 0.084), ('foreign', 0.077), ('returns', 0.076), ('investigations', 0.074), ('uncertainty', 0.071), ('tiny', 0.063), ('criterion', 0.062), ('mt', 0.061), ('prudent', 0.059), ('selectedsentence', 0.059), ('sortedngrams', 0.059), ('vocabgrowth', 0.059), ('smt', 0.056), ('translations', 0.054), ('highlighted', 0.053), ('burdens', 0.051), ('preferring', 0.051), ('schohn', 0.051), ('solicits', 0.051), ('doesn', 0.051), ('ldc', 0.05), ('random', 0.048), ('dollars', 0.047), ('pack', 0.047), ('measuring', 0.047), ('simulation', 0.045), ('labeled', 0.045), ('vijayshanker', 0.044), ('sentences', 0.044), ('depicted', 0.043), ('active', 0.043), ('managing', 0.042), ('gathering', 0.042), ('baltimore', 0.042), ('magnitude', 0.041), ('accounting', 0.04), ('dictionary', 0.039), ('translated', 0.039), ('robot', 0.038), ('triggers', 0.038), ('stop', 0.037), ('loop', 0.037), ('hwa', 0.037), ('reducing', 0.036), ('develop', 0.036), ('complicated', 0.035), ('hereafter', 0.034), ('unlabeled', 0.034), ('conduct', 0.034), ('money', 0.033), ('johns', 0.033), ('discusses', 0.033), ('turk', 0.033), ('relatively', 0.032), ('pool', 0.032), ('already', 0.03), ('begin', 0.03), ('carefully', 0.03), ('mechanical', 0.03), ('md', 0.03), ('simulated', 0.03), ('annotations', 0.029), ('ner', 0.028), ('amazon', 0.028), ('seconds', 0.028), ('cut', 0.028), ('hopkins', 0.028), ('prefer', 0.027), ('early', 0.027), ('time', 0.027), ('hu', 0.027), ('cohn', 0.027), ('situations', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="57-tfidf-1" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>Author: Michael Bloodgood ; Chris Callison-Burch</p><p>Abstract: We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.</p><p>2 0.16387464 <a title="57-tfidf-2" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>3 0.12655458 <a title="57-tfidf-3" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>Author: Katrin Tomanek ; Udo Hahn ; Steffen Lohmann ; Jurgen Ziegler</p><p>Abstract: We report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eyetracking device. As experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics. Our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and, more interestingly, indicates that fullscale context is mostly negligible with – the exception of semantic high-complexity cases. We then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models. Our data reveals that the cognitively founded model explains annotation costs (expressed in annotation time) more adequately than non-cognitive ones.</p><p>4 0.11565881 <a title="57-tfidf-4" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>5 0.11425639 <a title="57-tfidf-5" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>6 0.10040075 <a title="57-tfidf-6" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>7 0.08289738 <a title="57-tfidf-7" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>8 0.081394084 <a title="57-tfidf-8" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>9 0.073319294 <a title="57-tfidf-9" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>10 0.073236234 <a title="57-tfidf-10" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>11 0.070833765 <a title="57-tfidf-11" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>12 0.069830522 <a title="57-tfidf-12" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>13 0.069464192 <a title="57-tfidf-13" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>14 0.067555398 <a title="57-tfidf-14" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>15 0.062737286 <a title="57-tfidf-15" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>16 0.062247273 <a title="57-tfidf-16" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>17 0.060600307 <a title="57-tfidf-17" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>18 0.058995586 <a title="57-tfidf-18" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>19 0.058739383 <a title="57-tfidf-19" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>20 0.05775084 <a title="57-tfidf-20" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.161), (1, -0.086), (2, -0.041), (3, -0.011), (4, 0.025), (5, -0.003), (6, -0.053), (7, -0.008), (8, -0.031), (9, 0.059), (10, 0.047), (11, 0.15), (12, 0.041), (13, 0.002), (14, -0.036), (15, 0.047), (16, 0.018), (17, 0.056), (18, 0.008), (19, 0.049), (20, -0.05), (21, 0.065), (22, 0.147), (23, -0.001), (24, -0.053), (25, -0.027), (26, 0.097), (27, 0.153), (28, 0.073), (29, 0.0), (30, 0.042), (31, -0.067), (32, -0.026), (33, 0.038), (34, -0.105), (35, 0.041), (36, 0.106), (37, -0.005), (38, 0.108), (39, 0.022), (40, 0.047), (41, -0.073), (42, -0.105), (43, 0.13), (44, -0.049), (45, 0.009), (46, -0.003), (47, -0.046), (48, 0.054), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9404512 <a title="57-lsi-1" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>Author: Michael Bloodgood ; Chris Callison-Burch</p><p>Abstract: We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.</p><p>2 0.64442003 <a title="57-lsi-2" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>Author: Katrin Tomanek ; Udo Hahn ; Steffen Lohmann ; Jurgen Ziegler</p><p>Abstract: We report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eyetracking device. As experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics. Our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and, more interestingly, indicates that fullscale context is mostly negligible with – the exception of semantic high-complexity cases. We then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models. Our data reveals that the cognitively founded model explains annotation costs (expressed in annotation time) more adequately than non-cognitive ones.</p><p>3 0.61898577 <a title="57-lsi-3" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>Author: Eduard Hovy</p><p>Abstract: unkown-abstract</p><p>4 0.57354617 <a title="57-lsi-4" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>5 0.54902905 <a title="57-lsi-5" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>Author: Marcus Dobrinkat ; Tero Tapiovaara ; Jaakko Vayrynen ; Kimmo Kettunen</p><p>Abstract: This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English.</p><p>6 0.54449761 <a title="57-lsi-6" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>7 0.52190971 <a title="57-lsi-7" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>8 0.52166474 <a title="57-lsi-8" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>9 0.50479281 <a title="57-lsi-9" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>10 0.47097382 <a title="57-lsi-10" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>11 0.45287478 <a title="57-lsi-11" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>12 0.44654164 <a title="57-lsi-12" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>13 0.42976281 <a title="57-lsi-13" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>14 0.40824285 <a title="57-lsi-14" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>15 0.39509863 <a title="57-lsi-15" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>16 0.38354278 <a title="57-lsi-16" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>17 0.36151168 <a title="57-lsi-17" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>18 0.35346749 <a title="57-lsi-18" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>19 0.33885512 <a title="57-lsi-19" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>20 0.33644274 <a title="57-lsi-20" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.024), (39, 0.434), (42, 0.018), (59, 0.097), (73, 0.04), (78, 0.037), (80, 0.012), (83, 0.113), (84, 0.023), (98, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8727178 <a title="57-lda-1" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>Author: Matthew Skala ; Victoria Krakovna ; Janos Kramar ; Gerald Penn</p><p>Abstract: Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector techniques.</p><p>2 0.83678907 <a title="57-lda-2" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>Author: Marie-Catherine de Marneffe ; Christopher D. Manning ; Christopher Potts</p><p>Abstract: Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.</p><p>3 0.78897649 <a title="57-lda-3" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>Author: Yufeng Chen ; Chengqing Zong ; Keh-Yih Su</p><p>Abstract: We observe that (1) how a given named entity (NE) is translated (i.e., either semantically or phonetically) depends greatly on its associated entity type, and (2) entities within an aligned pair should share the same type. Also, (3) those initially detected NEs are anchors, whose information should be used to give certainty scores when selecting candidates. From this basis, an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between Chinese and English. It adopts a new mapping type ratio feature (which is the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional monolingual candidate certainty factors (based on those NE anchors). The experi- ments show that this novel approach has substantially raised the type-sensitive F-score of identified NE-pairs from 68.4% to 81.7% (42.1% F-score imperfection reduction) in our Chinese-English NE alignment task.</p><p>same-paper 4 0.77964157 <a title="57-lda-4" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>Author: Michael Bloodgood ; Chris Callison-Burch</p><p>Abstract: We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.</p><p>5 0.4811959 <a title="57-lda-5" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>Author: Eric Corlett ; Gerald Penn</p><p>Abstract: Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment prob- lems.</p><p>6 0.47240049 <a title="57-lda-6" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>7 0.46437299 <a title="57-lda-7" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>8 0.45198026 <a title="57-lda-8" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>9 0.45056129 <a title="57-lda-9" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>10 0.44718874 <a title="57-lda-10" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>11 0.44259474 <a title="57-lda-11" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>12 0.43876803 <a title="57-lda-12" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>13 0.43794018 <a title="57-lda-13" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>14 0.4370456 <a title="57-lda-14" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>15 0.4368766 <a title="57-lda-15" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>16 0.43625271 <a title="57-lda-16" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>17 0.4336876 <a title="57-lda-17" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>18 0.43363398 <a title="57-lda-18" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>19 0.43356299 <a title="57-lda-19" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>20 0.4329524 <a title="57-lda-20" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
