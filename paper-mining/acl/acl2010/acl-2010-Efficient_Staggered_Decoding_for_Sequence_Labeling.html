<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-98" href="#">acl2010-98</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</h1>
<br/><p>Source: <a title="acl-2010-98-pdf" href="http://aclweb.org/anthology//P/P10/P10-1050.pdf">pdf</a></p><p>Author: Nobuhiro Kaji ; Yasuhiro Fujiwara ; Naoki Yoshinaga ; Masaru Kitsuregawa</p><p>Abstract: The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algo- rithm, CARPEDIEM (Esposito and Radicioni, 2009).</p><p>Reference: <a title="acl-2010-98-reference" href="../acl2010_reference/acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 j p , i  Abstract The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. [sent-5, score-0.349]
</p><p>2 Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. [sent-6, score-0.35]
</p><p>3 This paper proposes an exact decoding algorithm that overcomes this problem. [sent-7, score-0.276]
</p><p>4 A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. [sent-8, score-0.194]
</p><p>5 One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. [sent-13, score-0.273]
</p><p>6 As we shall see later, we need over 300 labels to reduce joint POS tagging and chunking into the single sequence labeling problem. [sent-24, score-0.282]
</p><p>7 In this paper, we present a new decoding algorithm that overcomes this problem. [sent-26, score-0.223]
</p><p>8 The results demonstrate that our algorithm is up to several orders of magnitude faster than the basic Viterbi algorithm and a state-of-the-art algorithm (Esposito and Radicioni, 2009); it makes exact decoding practical even in labeling problems with a large label set. [sent-31, score-0.547]
</p><p>9 1 Models Sequence labeling is the problem of predicting label sequence y = {yn}nN=1 for given token se-  quence x = { yxn =}nN= {1y . [sent-41, score-0.245]
</p><p>10 2 Viterbi decoding Given the score function f(x, y), we have to locate the best label sequence. [sent-76, score-0.323]
</p><p>11 Let ω(yn) be the best score of the partial label sequence ending with yn. [sent-78, score-0.207]
</p><p>12 yn −1 Using this recursive definition, we can evaluate ω(yn) for all yn. [sent-81, score-0.652]
</p><p>13 3 Related work To the best of our knowledge, the Viterbi algorithm is the only algorithm widely adopted in the NLP field that offers exact decoding. [sent-87, score-0.197]
</p><p>14 In other communities, several exact algorithms have already been proposed for handling large label sets. [sent-88, score-0.198]
</p><p>15 It accelerates decoding by assuming that the adjacent labels are not strongly correlated. [sent-98, score-0.274]
</p><p>16 Our algorithm locates the best label sequence by iteratively solving labeling problems with a reduced label set. [sent-121, score-0.401]
</p><p>17 This is because the algorithm allows us to check the optimality of the solution achieved by using only the reduced label set. [sent-124, score-0.2]
</p><p>18 1 Degenerate lattice We begin by introducing the degenerate lattice, which plays a central role in our algorithm. [sent-130, score-0.571]
</p><p>19 Following convention, we regard each path on the lattice as a label sequence. [sent-132, score-0.381]
</p><p>20 n Boyf tahgeg lattice, we can ltr naondsefosrm in tthhee original lattice into a simpler form, which we call the degenerate lattice (Figure 1(b)). [sent-135, score-0.749]
</p><p>21 Let us examine the intuition behind the degenerate lattice. [sent-136, score-0.393]
</p><p>22 Here, a label is referred to as an active label if it is not aggregated (e. [sent-138, score-0.318]
</p><p>23 , A, B, C, and D in the first column of Figure 1(b)), and otherwise as an inactive label (i. [sent-140, score-0.21]
</p><p>24 The new label, which is made by grouping the inactive labels, is referred to as a degenerate label (i. [sent-143, score-0.582]
</p><p>25 Two degenerate labels can be seen  as equivalent if their corresponding inactive label sets are the same (e. [sent-146, score-0.686]
</p><p>26 , degenerate labels in the first and the last column). [sent-148, score-0.497]
</p><p>27 In this approach, each path of the degenerate lattice can also be interpreted as a label sequence. [sent-149, score-0.774]
</p><p>28 In this case, however, the label to be assigned is either an active label or a degenerate label. [sent-150, score-0.711]
</p><p>29 We then define the parameters associated with degenerate label z. [sent-151, score-0.503]
</p><p>30 (b) Tathhe path z ,o fE t,h Ge degenerate lattice that corresponds to y. [sent-167, score-0.664]
</p><p>31 are degenerate labels, and I(z) denotes one-to-one mapping from z to its corresponding inactive label set. [sent-169, score-0.582]
</p><p>32 The degenerate lattice has an important property which is the key to our algorithm: Lemma 1. [sent-170, score-0.571]
</p><p>33 If the best path of the degenerate lattice does not include any degenerate label, it is equivalent to the best path of the original lattice. [sent-171, score-1.188]
</p><p>34 Let zmax be the best path of the degenerate lattice. [sent-173, score-0.591]
</p><p>35 Our goal is to prove that if zmax does not include any degenerate label, then  ∀y  ∈  Y,  logp(x, y) ≤ logp(x, zmax)  (5)  where Y is the set of all paths on the original lattice. [sent-174, score-0.479]
</p><p>36 We prove this by partitioning Y into two disjoint sets: Y0 and Y1, where Y0 is the subset of Y appearing in the degenerate lattice. [sent-175, score-0.393]
</p><p>37 Since zmax is the best path of the degenerate lattice, we have ∀y  ∈  Y0,  logp(x, y) ≤ logp(x, zmax). [sent-177, score-0.591]
</p><p>38 For eaamcihn path y ∈ Y1, qthueernec eex yis stsu a unique path z on tehaec degenerate Ylattice that corresponds to y (Figure 2). [sent-180, score-0.579]
</p><p>39 Therefore, we have ∀y  ∈  Y1, ∃z  ∈  Z, logp(x, y) ≤ logp(x, z) < logp(x, zmax)  where Z is the set of all paths of the degenerate lattice. [sent-181, score-0.393]
</p><p>40 A (a)A BABA(b)BABA DCABDCA(c)BACD BA Figure 3: (a) The best path of the initial degenerate lattice, which is denoted by the line, is located. [sent-185, score-0.505]
</p><p>41 (b) The active labels are expanded and the best path is searched again. [sent-186, score-0.314]
</p><p>42 (c) The best path without degenerate labels is obtained. [sent-187, score-0.609]
</p><p>43 2 Staggered decoding Now we can describe our algorithm, which we call staggered decoding. [sent-189, score-0.392]
</p><p>44 The algorithm successively constructs degenerate lattices and checks whether the best path includes degenerate labels. [sent-190, score-0.951]
</p><p>45 In building each degenerate lattice, labels with high probability p(y), estimated from training data, are pref-  erentially selected as the active label; the expectation is that such labels are likely to belong to the best path. [sent-191, score-0.718]
</p><p>46 The algorithm is detailed as follows: Initialization step The algorithm starts by building a degenerate lattice in which there is only one active label in each column. [sent-192, score-0.907]
</p><p>47 We select label y with the highest p(y) as the active label. [sent-193, score-0.208]
</p><p>48 Search step The best path of the degenerate lattice is located (Figure 3(a)). [sent-194, score-0.705]
</p><p>49 If the best path does not include any degenerate label, we can terminate the algorithm since it is identical with the best path of the original lattice according to Lemma 1. [sent-197, score-0.848]
</p><p>50 Expansion step We double the number of the active labels in the degenerate lattice. [sent-199, score-0.617]
</p><p>51 The new active labels are selected from the current inactive label set in descending order of p(y). [sent-200, score-0.391]
</p><p>52 If the inactive label set becomes empty, we simply reconstructed the original lattice. [sent-201, score-0.213]
</p><p>53 , the best path has no degenerate label (Figure 3(c)). [sent-205, score-0.615]
</p><p>54 Compared to the Viterbi algorithm, staggered decoding requires two additional computations for 488  training. [sent-206, score-0.412]
</p><p>55 First, we have to estimate p(y) so as to select active labels in the initialization and expansion step. [sent-207, score-0.273]
</p><p>56 Second, we have to compute the parameters regarding degenerate labels according to Equations (1)-(4). [sent-208, score-0.517]
</p><p>57 3 Pruning To achieve speed-up, it is crucial that staggered decoding efficiently performs the search step. [sent-211, score-0.42]
</p><p>58 In earlier iterations, the Viterbi algorithm is indeed efficient because the label set to be handled is much smaller than the original one. [sent-213, score-0.185]
</p><p>59 In later iterations, however, our algorithm drastically in-  creases the number of labels, making Viterbi decoding quite expensive. [sent-214, score-0.223]
</p><p>60 To handle this problem, we propose a method of pruning the lattice nodes. [sent-215, score-0.221]
</p><p>61 This technique is motivated by the observation that the degenerate lattice shares many active labels with the previous iteration. [sent-216, score-0.804]
</p><p>62 node corresponding to yn can be removed from consideration. [sent-232, score-0.652]
</p><p>63 This can be done by retaining the best path among those consisting of only active labels. [sent-236, score-0.21]
</p><p>64 As the iteration proceeds, the degenerate lattice becomes closer to the original one, so the lower bound becomes tighter. [sent-239, score-0.663]
</p><p>65 Let ω(yn) be the best score of the partial label sequence ending with yn. [sent-243, score-0.207]
</p><p>66 Presuming that we traverse the lattice from left to right, ω(yn) can be defined as myn−ax1{ω(yn−1)  + logp(yn|yn−1)} + logp(xn|yn). [sent-244, score-0.178]
</p><p>67 If we traverse the lattice from right to left, an analogous score ¯ω (yn) can be defined as  logp(xn|yn) +y mn+ax1{ ω¯(yn+1) + logp(yn|yn+1)}. [sent-245, score-0.202]
</p><p>68 Let λ(yn) and λ¯(yn) be scores analogous to ω(yn) and ¯ω (yn) that are computed using the degenerate lattice. [sent-249, score-0.393]
</p><p>69 + λ¯(yn)  −  −  logp(xn |yn) logp(xn|yn)  For the sake of simplicity, we assume that yn is an active label. [sent-252, score-0.75]
</p><p>70 We just point out that, if yn is an inactive label, then there exists a degenerate label zn in the n-th column such that yn ∈ I(zn), and we can use λ(zn) and λ¯(zn) instead o∈f λ(yn) and λ¯(yn). [sent-254, score-1.938]
</p><p>71 3 Pruning procedure We make use of the bounds in pruning the lattice nodes. [sent-262, score-0.246]
</p><p>72 4 Analysis We provide here a theoretical analysis of staggered decoding. [sent-268, score-0.222]
</p><p>73 Staggered decoding requires at most (log2 L + 1) iterations to terminate. [sent-274, score-0.218]
</p><p>74 We have 2m−1 active labels in the m-th search step (m = 1, 2 . [sent-276, score-0.252]
</p><p>75 ), which means we have L active labels and no degenerate labels in the (log2 L + 1)-th search step. [sent-279, score-0.727]
</p><p>76 Since we create one new degenerate label in all but the last expansion step, we have log2 L degenerate labels. [sent-284, score-0.945]
</p><p>77 The time complexity of the Viterbi algorithm is O(NL2) since there are NL nodes in the lattice and it takes O(L) time to evaluate the score of each node. [sent-289, score-0.279]
</p><p>78 Staggered decoding requires O(L2 + LV ) memory space. [sent-291, score-0.19]
</p><p>79 Notice that the active label in the first column is not expanded. [sent-295, score-0.229]
</p><p>80 memory space to perform Viterbi decoding in the search step. [sent-297, score-0.198]
</p><p>81 Staggered decoding has O(N) best case time complexity and O(NL2) worst case time complexity. [sent-299, score-0.233]
</p><p>82 To perform the m-th search step, staggered decoding requires the order of O(N4m−1) time because we have 2m−1 active labels. [sent-301, score-0.538]
</p><p>83 Theorem 1 shows that staggered decoding asymptotically requires the same order of memory space as the Viterbi algorithm. [sent-308, score-0.412]
</p><p>84 Theorem 2 reveals that staggered decoding has the same order of time complexity as the Viterbi algorithm even in the worst case. [sent-309, score-0.489]
</p><p>85 First, we can initialize the value of lower bound l by selecting a path from the original lattice in some way, and then computing the score of that  path. [sent-312, score-0.339]
</p><p>86 In our experiments, we use the path located by the left-to-right deterministic decoding (i. [sent-313, score-0.263]
</p><p>87 2, we can expand the active labels in a heuristic manner to keep the number of active labels small: Column-wise expansion step We double the number of the active labels in the column only if the best path of the degenerate lattice passes through the degenerate label of that column (Figure 4). [sent-320, score-1.905]
</p><p>88 Finally, it is worth noting that we can use staggered decoding in training perceptrons as well, although such application lies outside the scope of this paper. [sent-380, score-0.422]
</p><p>89 To reduce joint tagging into a single sequence labeling problem, we produced the labels by concatenating the POS tag and the chunk tag (BIO format), e. [sent-384, score-0.253]
</p><p>90 virtually avoided performing extra iterations, while heuristically restricting active label expansion. [sent-447, score-0.208]
</p><p>91 ) with beam search, which is the approximate algorithm widely adopted for sequence labeling in NLP. [sent-454, score-0.257]
</p><p>92 6  Relation to coarse-to-fine approach  Before concluding remarks, we briefly examine the relationship between staggered decoding and coarse-to-fine PCFG parsing (2006). [sent-471, score-0.392]
</p><p>93 Since the degenerate label can be interpreted as a coarse-level label, one may consider that staggered decoding is an instance of coarse-to-fine approach. [sent-473, score-0.895]
</p><p>94 Second, our algorithm does not always perform decoding at the fine-grained level. [sent-476, score-0.223]
</p><p>95 7  Conclusions  The sequence labeling algorithm is indispensable to modern statistical NLP. [sent-478, score-0.182]
</p><p>96 However, the Viterbi algorithm, which is the standard decoding algorithm in NLP, is not efficient when we have to deal with a large number of labels. [sent-479, score-0.245]
</p><p>97 In this paper we presented staggered decoding, which provides a principled way of resolving this problem. [sent-480, score-0.222]
</p><p>98 We hope this work opens a new perspective on decoding algorithms for a wide range of NLP problems, not just sequence labeling. [sent-484, score-0.259]
</p><p>99 Speeding up HMM decoding and training by exploiting sequence repetitions. [sent-535, score-0.224]
</p><p>100 Error bounds for convolutional codes and an asymeptotically optimum decoding algorithm. [sent-588, score-0.195]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('yn', 0.652), ('degenerate', 0.393), ('logp', 0.246), ('staggered', 0.222), ('viterbi', 0.192), ('lattice', 0.178), ('decoding', 0.17), ('label', 0.11), ('labels', 0.104), ('active', 0.098), ('path', 0.093), ('carpediem', 0.086), ('esposito', 0.086), ('zmax', 0.086), ('inactive', 0.079), ('radicioni', 0.074), ('sd', 0.068), ('xn', 0.064), ('hmms', 0.063), ('labeling', 0.055), ('sequence', 0.054), ('exact', 0.053), ('algorithm', 0.053), ('perceptron', 0.052), ('supertagging', 0.05), ('lv', 0.05), ('taggingjoint', 0.049), ('taggingsupertagging', 0.049), ('expansion', 0.049), ('beam', 0.046), ('tsuruoka', 0.044), ('bound', 0.044), ('pruning', 0.043), ('pos', 0.043), ('tagging', 0.04), ('max', 0.037), ('optimality', 0.037), ('baba', 0.037), ('lemma', 0.036), ('algorithms', 0.035), ('theorem', 0.035), ('speeding', 0.035), ('matsuzaki', 0.035), ('zn', 0.031), ('technique', 0.031), ('perceptrons', 0.03), ('approximate', 0.03), ('chunking', 0.029), ('iterations', 0.028), ('hpsg', 0.028), ('search', 0.028), ('xz', 0.028), ('nlp', 0.027), ('token', 0.026), ('collins', 0.025), ('bounds', 0.025), ('badcba', 0.025), ('deafghbced', 0.025), ('fedcbhgedfba', 0.025), ('lifshits', 0.025), ('siddiqi', 0.025), ('tasker', 0.025), ('score', 0.024), ('complexity', 0.024), ('sha', 0.024), ('tsujii', 0.024), ('prohibitively', 0.024), ('offline', 0.024), ('becomes', 0.024), ('width', 0.023), ('ptb', 0.023), ('miyao', 0.023), ('yusuke', 0.022), ('initialization', 0.022), ('step', 0.022), ('efficient', 0.022), ('markov', 0.022), ('crfs', 0.022), ('quadratic', 0.022), ('tokyo', 0.022), ('yoshimasa', 0.022), ('felzenszwalb', 0.022), ('maxy', 0.022), ('semimarkov', 0.022), ('column', 0.021), ('worst', 0.02), ('nl', 0.02), ('compute', 0.02), ('requires', 0.02), ('termination', 0.02), ('hga', 0.02), ('aggregating', 0.02), ('indispensable', 0.02), ('sarawagi', 0.02), ('tsochantaridis', 0.02), ('tasks', 0.02), ('best', 0.019), ('adopted', 0.019), ('discriminative', 0.019), ('ichi', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="98-tfidf-1" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>Author: Nobuhiro Kaji ; Yasuhiro Fujiwara ; Naoki Yoshinaga ; Masaru Kitsuregawa</p><p>Abstract: The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algo- rithm, CARPEDIEM (Esposito and Radicioni, 2009).</p><p>2 0.16285986 <a title="98-tfidf-2" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><p>3 0.12388106 <a title="98-tfidf-3" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>Author: Graeme Blackwood ; Adria de Gispert ; William Byrne</p><p>Abstract: This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers. We introduce transducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices.</p><p>4 0.12139196 <a title="98-tfidf-4" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.</p><p>5 0.077143259 <a title="98-tfidf-5" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>Author: Yang Liu ; Liang Huang</p><p>Abstract: unkown-abstract</p><p>6 0.077105984 <a title="98-tfidf-6" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>7 0.067338705 <a title="98-tfidf-7" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>8 0.067055896 <a title="98-tfidf-8" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>9 0.05892875 <a title="98-tfidf-9" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>10 0.05780841 <a title="98-tfidf-10" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>11 0.055495858 <a title="98-tfidf-11" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>12 0.053232223 <a title="98-tfidf-12" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>13 0.050905552 <a title="98-tfidf-13" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>14 0.050029896 <a title="98-tfidf-14" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>15 0.048217118 <a title="98-tfidf-15" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>16 0.04791002 <a title="98-tfidf-16" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>17 0.047846373 <a title="98-tfidf-17" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>18 0.04743829 <a title="98-tfidf-18" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>19 0.046338592 <a title="98-tfidf-19" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>20 0.045260206 <a title="98-tfidf-20" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.131), (1, -0.036), (2, 0.031), (3, 0.012), (4, -0.02), (5, -0.052), (6, 0.031), (7, 0.009), (8, 0.035), (9, 0.026), (10, -0.073), (11, 0.001), (12, 0.04), (13, -0.095), (14, -0.082), (15, -0.016), (16, -0.093), (17, 0.079), (18, -0.062), (19, 0.073), (20, 0.065), (21, 0.059), (22, -0.108), (23, -0.098), (24, -0.062), (25, -0.046), (26, 0.094), (27, 0.001), (28, -0.085), (29, 0.022), (30, -0.016), (31, 0.07), (32, -0.147), (33, -0.114), (34, -0.064), (35, -0.085), (36, -0.036), (37, 0.02), (38, 0.026), (39, -0.003), (40, 0.089), (41, -0.042), (42, 0.058), (43, 0.032), (44, 0.004), (45, -0.093), (46, -0.184), (47, -0.006), (48, -0.009), (49, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95327574 <a title="98-lsi-1" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>Author: Nobuhiro Kaji ; Yasuhiro Fujiwara ; Naoki Yoshinaga ; Masaru Kitsuregawa</p><p>Abstract: The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algo- rithm, CARPEDIEM (Esposito and Radicioni, 2009).</p><p>2 0.6495896 <a title="98-lsi-2" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>Author: Graeme Blackwood ; Adria de Gispert ; William Byrne</p><p>Abstract: This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers. We introduce transducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices.</p><p>3 0.6373027 <a title="98-lsi-3" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><p>4 0.57626569 <a title="98-lsi-4" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>Author: Dong Yang ; Paul Dixon ; Sadaoki Furui</p><p>Abstract: This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outper- forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</p><p>5 0.53416681 <a title="98-lsi-5" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>6 0.48611239 <a title="98-lsi-6" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>7 0.445568 <a title="98-lsi-7" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>8 0.42844915 <a title="98-lsi-8" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>9 0.41043186 <a title="98-lsi-9" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>10 0.36041558 <a title="98-lsi-10" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>11 0.36001909 <a title="98-lsi-11" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>12 0.35488945 <a title="98-lsi-12" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>13 0.35378692 <a title="98-lsi-13" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>14 0.3500855 <a title="98-lsi-14" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>15 0.33504051 <a title="98-lsi-15" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>16 0.32860851 <a title="98-lsi-16" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>17 0.32347032 <a title="98-lsi-17" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>18 0.29278323 <a title="98-lsi-18" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>19 0.27763906 <a title="98-lsi-19" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>20 0.27510211 <a title="98-lsi-20" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.011), (14, 0.027), (25, 0.04), (26, 0.038), (33, 0.018), (37, 0.184), (39, 0.011), (42, 0.014), (44, 0.017), (59, 0.14), (73, 0.054), (76, 0.012), (78, 0.048), (80, 0.013), (83, 0.077), (84, 0.021), (98, 0.13)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86514115 <a title="98-lda-1" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>Author: Nobuhiro Kaji ; Yasuhiro Fujiwara ; Naoki Yoshinaga ; Masaru Kitsuregawa</p><p>Abstract: The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algo- rithm, CARPEDIEM (Esposito and Radicioni, 2009).</p><p>2 0.85865951 <a title="98-lda-2" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>Author: Frank Keller</p><p>Abstract: We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, specifically selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed.</p><p>3 0.75057673 <a title="98-lda-3" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>4 0.7494117 <a title="98-lda-4" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>Author: Stephen Wu ; Asaf Bachrach ; Carlos Cardenas ; William Schuler</p><p>Abstract: Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.</p><p>5 0.7480582 <a title="98-lda-5" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>Author: Nathanael Chambers ; Dan Jurafsky</p><p>Abstract: This paper improves the use of pseudowords as an evaluation framework for selectional preferences. While pseudowords originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences. A selectional preference model ranks a set of possible arguments for a verb by their semantic fit to the verb. Pseudo-words serve as a proxy evaluation for these decisions. The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original. This paper studies two main aspects of pseudoword creation that affect performance results. (1) Pseudo-word evaluations often evaluate only a subset of the words. We show that selectional preferences should instead be evaluated on the data in its entirety. (2) Different approaches to selecting partner words can produce overly optimistic evaluations. We offer suggestions to address these factors and present a simple baseline that outperforms the state-ofthe-art by 13% absolute on a newspaper domain.</p><p>6 0.74609017 <a title="98-lda-6" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>7 0.74537146 <a title="98-lda-7" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>8 0.74049807 <a title="98-lda-8" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>9 0.74019271 <a title="98-lda-9" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>10 0.73903966 <a title="98-lda-10" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>11 0.73832452 <a title="98-lda-11" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>12 0.73772776 <a title="98-lda-12" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>13 0.73769236 <a title="98-lda-13" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>14 0.73666745 <a title="98-lda-14" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>15 0.7363441 <a title="98-lda-15" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>16 0.73621768 <a title="98-lda-16" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>17 0.73569649 <a title="98-lda-17" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>18 0.73552907 <a title="98-lda-18" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>19 0.733024 <a title="98-lda-19" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>20 0.73267102 <a title="98-lda-20" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
