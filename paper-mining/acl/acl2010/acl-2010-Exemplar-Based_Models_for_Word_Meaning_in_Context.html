<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>107 acl-2010-Exemplar-Based Models for Word Meaning in Context</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-107" href="#">acl2010-107</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>107 acl-2010-Exemplar-Based Models for Word Meaning in Context</h1>
<br/><p>Source: <a title="acl-2010-107-pdf" href="http://aclweb.org/anthology//P/P10/P10-2017.pdf">pdf</a></p><p>Author: Katrin Erk ; Sebastian Pado</p><p>Abstract: This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.</p><p>Reference: <a title="acl-2010-107-reference" href="../acl2010_reference/acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract This paper describes ongoing work on distributional models for word meaning in context. [sent-4, score-0.153]
</p><p>2 We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. [sent-5, score-0.567]
</p><p>3 On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models. [sent-6, score-0.502]
</p><p>4 They describe a lemma through a high-dimensional vector that records co-occurrence with context features over a large corpus. [sent-8, score-0.174]
</p><p>5 Distributional models are also attractive as a model of word meaning in context, since they do not have to rely on fixed sets of dictionary sense with their well-known problems (Kilgarriff, 1997; McCarthy and Navigli, 2009). [sent-13, score-0.131]
</p><p>6 Also, they can be used directly for testing paraphrase applicability (Szpektor et al. [sent-14, score-0.18]
</p><p>7 , 2008), a task that has recently become prominent in the context of textual entailment (Bar-Haim et al. [sent-15, score-0.084]
</p><p>8 However, polysemy is a fundamental problem for distributional models. [sent-17, score-0.131]
</p><p>9 Typically, distributional models compute a single “type” vector for a target word, which contains cooccurrence counts for all the occurrences of the target in a large corpus. [sent-18, score-0.316]
</p><p>10 If the target is polysemous, this vector mixes contextual features for all the senses of the target. [sent-19, score-0.117]
</p><p>11 This problem has typically been approached by modifying the type vector for a target to better match a given context (Mitchell and Lapata, 2008; Erk and Pad o´, 2008; Thater et al. [sent-23, score-0.184]
</p><p>12 In the terms of research on human concept representation, which often employs feature vector representations, the use of type vectors can be understood as aprototype-based approach, which uses a single vector per category. [sent-25, score-0.159]
</p><p>13 From this angle, computing prototypes throws away much interesting distributional information. [sent-26, score-0.105]
</p><p>14 A rival class of models is that of exemplar models, which memorize each seen instance of a category and perform categorization by comparing a new stimulus to each remembered exemplar vector. [sent-27, score-1.019]
</p><p>15 We can address the polysemy issue through an exemplar model by simply removing all exem-  plars that are “not relevant” for the present context, or conversely activating only the relevant ones. [sent-28, score-0.6]
</p><p>16 For the coach example, in the context of a text about motorways, presumably an instance like “The coach drove a steady 45 mph” would be activated, while “The team lost all games since the new coach arrived” would not. [sent-29, score-0.291]
</p><p>17 In this paper, we present an exemplar-based distributional model for modeling word meaning in context, applying the model to the task of deciding paraphrase applicability. [sent-30, score-0.334]
</p><p>18 With a very simple vector representation and just using activation, we outperform the state-of-the-art prototype models. [sent-31, score-0.13]
</p><p>19 2  Related Work  Among distributional models of word, there are some approaches that address polysemy, either by inducing a fixed clustering of contexts into senses (Sch u¨tze, 1998) or by dynamically modi92  Uppsala,P Srwoce de dni,n 1g1s- 1of6 t Jhuely AC 20L1 20 . [sent-33, score-0.151]
</p><p>20 0c 2 C0o1n0fe Aresnsoceci Sathio rnt f Poarp Ceorsm,p paugteastio 9n2a–l9 L7i,nguistics  fying a word’s type vector according to each given sentence context (Landauer and Dumais, 1997; Mitchell and Lapata, 2008; Erk and Pad o´, 2008; Thater et al. [sent-35, score-0.112]
</p><p>21 Some use a bag-of-words representation of words in the current sentence (Sch u¨tze, 1998; Landauer and Dumais, 1997), some make use of syntactic context (Mitchell and Lapata, 2008; Erk and Pad o´, 2008; Thater et al. [sent-38, score-0.072]
</p><p>22 The approach that we present in the current paper computes a representation dynamically for each sentence context, using a simple bag-of-words representation of context. [sent-40, score-0.066]
</p><p>23 In cognitive science, prototype models predict degree of category membership through similarity to a single prototype, while exemplar theory represents a concept as a collection of all previously seen exemplars (Murphy, 2002). [sent-41, score-0.915]
</p><p>24 (2007) found that the benefit of exemplars over prototypes grows with the number of available exemplars. [sent-43, score-0.309]
</p><p>25 The problem of representing meaning in context, which we consider in this paper, is closely related to the problem of concept combination in  cognitive science, i. [sent-44, score-0.089]
</p><p>26 , the derivation of representations for complex concepts (such as “metal spoon”) given the representations of base concepts (“metal” and “spoon”). [sent-46, score-0.092]
</p><p>27 While most approaches to concept combination are based on prototype models, Voorspoels et al. [sent-47, score-0.063]
</p><p>28 (2009) show superior results for an exemplar model based on exemplar activation. [sent-48, score-0.987]
</p><p>29 In the current paper, we use an exemplar model for computing distributional representations for word meaning in context, using the context to activate relevant exemplars. [sent-51, score-0.756]
</p><p>30 Comparing representations of context, bag-of-words (BOW) representations are more informative and noisier, while syntax-based representations deliver sparser and less noisy information. [sent-52, score-0.138]
</p><p>31 Following the hypothesis that richer, topical information is more suitable for exemplar activation, we use BOW representations of sentential context in the current paper. [sent-53, score-0.61]
</p><p>32 3  Exemplar Activation Models  We now present an exemplar-based model for  meaning in context. [sent-54, score-0.06]
</p><p>33 It lemma is represented by an exemplar is a sentence represented as a vector. [sent-55, score-0.565]
</p><p>34 for individual exemplars  assumes that each target a set of exemplars, where in which the target occurs, We use lowercase letters (vectors), and uppercase  AaSriWtleswfnduatrseiyngutrifbeachnlorteWdxnutrihegn. [sent-56, score-0.425]
</p><p>35 We model polysemy by activating relevant exemplars of a lemma E in a given sentence context s. [sent-60, score-0.525]
</p><p>36 (Note that we use E to refer to both a lemma and its exemplar set, and that s can be viewed as just another exemplar vector. [sent-61, score-1.05]
</p><p>37 ) In general, we define activation of a set E by exemplar s as  act(E, s) = {e ∈ E | sim(e, s) > θ(E, s) } where E is an exemplar set, s is the “point of comparison”, sim is some similarity measure such as Cosine or Jaccard, and θ(E, s) is a threshold. [sent-62, score-1.524]
</p><p>38 Exemplars belong to the activated set iftheir similarity to s exceeds θ(E, s). [sent-63, score-0.133]
</p><p>39 In kNN activation, the k most similar exemplars to s are activated by setting θ to the similarity of the k-th most similar exemplar. [sent-65, score-0.414]
</p><p>40 Note that, while in the kNN activation scheme the number of activated exemplars is the same for every lemma, this is not the case for percentage activation: There, a more frequent lemma (i. [sent-67, score-0.962]
</p><p>41 , a lemma with more exemplars) will have more exemplars activated. [sent-69, score-0.361]
</p><p>42 A paraphrases is typically only applicable to a particular sense of a target word. [sent-71, score-0.276]
</p><p>43 Table 1illustrates this on two examples from the Lexical Substitution (LexSub) dataset (McCarthy and Navigli, 2009), both  featuring the target return. [sent-72, score-0.115]
</p><p>44 The right column lists appropriate paraphrases of return in each context (given by human annotators). [sent-73, score-0.216]
</p><p>45 2 We apply the exemplar activation model to the task of predicting paraphrase felicity: Given a target lemma T in a particular sentential context s, and given a list of 1In principle, activation could be treated not just as binary inclusion/exclusion, but also as a graded weighting scheme. [sent-74, score-1.857]
</p><p>46 2Each annotator was allowed to give up to three paraphrases per target in context. [sent-76, score-0.239]
</p><p>47 As a consequence, the number of gold paraphrases per target sentence varies. [sent-77, score-0.263]
</p><p>48 93  potential paraphrases of T, the task is to predict which of the paraphrases are applicable in s. [sent-78, score-0.352]
</p><p>49 , 2009) have performed this task by modifying the type vector for T to the context s and then comparing the resulting vector T0 to the type vector of a paraphrase candidate P. [sent-80, score-0.4]
</p><p>50 In our exemplar setting, we select a contextually adequate subset of contexts in which T has been observed, using  T0 = act(T, s) as a generalized representation of meaning of target T in the context of s. [sent-81, score-0.693]
</p><p>51 Previous approaches used all of P as a representation for a paraphrase candidate P. [sent-82, score-0.203]
</p><p>52 However, P includes also irrelevant exemplars, while for a paraphrase to be judged as good, it is sufficient that one plausible reading exists. [sent-83, score-0.197]
</p><p>53 We evaluate our model on predicting paraphrases from the Lexical Substitution (LexSub) dataset (McCarthy and Navigli, 2009). [sent-86, score-0.227]
</p><p>54 This dataset consists of 2000 instances of 200 target words in sentential contexts, with paraphrases for each target word instance generated by up to 6 participants. [sent-87, score-0.384]
</p><p>55 Following Erk and Pad o´ (2008), we take the list of paraphrase candidates for a target as given (computed by pooling all paraphrases that LexSub annotators proposed for the target) and use the models to rank them for any given sentence context. [sent-90, score-0.486]
</p><p>56 These vectors represent instances of a target word by the other words in the same sentence, lemmatized and POStagged, minus stop words. [sent-92, score-0.102]
</p><p>57 , if the lemma gnurge occurs twice in the BNC, once in the sentence “The dog will gnurge the other dog”, and once in “The old windows gnurged”, the exemplar set for gnurge contains the vectors [dog-n: 2, othera:1] and [old-a: 1, window-n: 1]. [sent-95, score-0.783]
</p><p>58 For exemplar similarity, we use the standard Cosine similarity, and for the similarity of two exemplar sets, the Cosine of their centroids. [sent-96, score-0.998]
</p><p>59 The model’s prediction for an item is a list of paraphrases ranked by their predicted goodness of fit. [sent-98, score-0.167]
</p><p>60 , qmi be the list of gold paraphrases with gold weights hy1, . [sent-104, score-0.215]
</p><p>61 , xni be the gold weights  amssoodceli,at eadnd dw lietth htxhem (assume xi = g0o lifd pi ∈ G), where G ⊆ P. [sent-114, score-0.077]
</p><p>62 Lwertite I xi = i1 xk f,o arn tdhe z average gold weight of the first Pi model predictions, and analogously yi. [sent-117, score-0.093]
</p><p>63 Then  Pik=1∈  GAP(P,G) =Pjm=11I(yj)yjXi=n1I(xi)xi Since the model mayP Prank multiple paraphrases the same, we average over 10 random permutations of equally ranked paraphrases. [sent-118, score-0.184]
</p><p>64 We first computed two models that activate either the paraphrase or the target, but not both. [sent-121, score-0.252]
</p><p>65 Model 1, actT, activates only the target, using the complete P as paraphrase, and ranking paraphrases by sim(P, act(T, s)). [sent-122, score-0.253]
</p><p>66 Model 2, actP, activates only the paraphrase, using s as the target word, ranking by sim(act(P, s) , s). [sent-123, score-0.158]
</p><p>67 The results for these models are shown in Table 2, with both kNN and percentage activation: kNN activation with a parameter of 10 means that  the 10 closest neighbors were activated, while percentage with a parameter of 10 means that the closest 10% of the exemplars were used. [sent-124, score-0.854]
</p><p>68 6) corresponds to a prototype-based model that ranks paraphrase candidates by the distance between their type vectors and the target’s type vector. [sent-129, score-0.279]
</p><p>69 Note also that both actT and actP show the best results for small values of the activation parameter. [sent-131, score-0.491]
</p><p>70 This indicates paraphrases can be judged on the basis of a rather small number of exemplars. [sent-132, score-0.184]
</p><p>71 For actT, a small absolute number of activated exemplars (here, 20) works best , while actP yields the best results for a small percentage of paraphrase exemplars. [sent-134, score-0.628]
</p><p>72 Section 3): Activation of the paraphrase must allow a guess about whether there is reasonable interpretation of P in the context s. [sent-136, score-0.229]
</p><p>73 In contrast, target activation merely has to counteract the sparsity of s, and activation of too many exemplars from T leads to oversmoothing. [sent-138, score-1.318]
</p><p>74 With the exception of actT/perc, all activation methods significantly outperform the best baseline (actP, no activation). [sent-151, score-0.511]
</p><p>75 Based on these observations, we computed a third model, actTP, that activates both T (by kNN) and P (by percentage), ranking paraphrases by sim(act(P, s) , act(T, s)). [sent-152, score-0.253]
</p><p>76 Table 2), namely by setting the activation parameters to small values. [sent-155, score-0.493]
</p><p>77 e137ful  LexSub dataset (GAP evaluation) we fix the actP activation level, we find comparatively large performance differences between the T activation settings k=5 and k=10 (highly significant for 10% actP, and significant for 20% and 30% actP). [sent-160, score-1.007]
</p><p>78 On the other hand, when we fix the actT activation level, changes in actP activation generally have an insignificant impact. [sent-161, score-0.964]
</p><p>79 This indicates that at least in the current vector space the sparsity of s is less of a problem than the “dilution” of s that we face when we representing the target word by exemplars of T close to s. [sent-163, score-0.419]
</p><p>80 An analysis of the results by target part-of-speech showed that the globally optimal parameters also yield the best results for individual POS, even though there are substantial differences among POS. [sent-166, score-0.112]
</p><p>81 For actT, the  best results emerge for all POS with kNN activation with k between 10 and 30. [sent-167, score-0.491]
</p><p>82 For actP, the best parameter for all POS was activation of 10%, with GAPs of 36. [sent-172, score-0.491]
</p><p>83 9) are better than actP for verbs, but worse for nouns and adjectives, which indicates that the sparsity problem might be more prominent than for the other POS. [sent-179, score-0.061]
</p><p>84 In all three models, we found a clear effect of target and paraphrase frequency, with deteriorating performance for the highest-frequency targets as well as for the lemmas with the highest average paraphrase frequency. [sent-180, score-0.432]
</p><p>85 We have re-evaluated our exemplar models on the subsets we used in Erk and Pad o´ (2008, EP08, 367 95  Models EP08 EP09 TDP09 EP08 dataset27. [sent-183, score-0.518]
</p><p>86 The results in Table 4 compare these models against our best previous exemplar models and show that our models outperform these models across the board. [sent-198, score-0.656]
</p><p>87 5  Conclusions and Outlook  This paper reports on work in progress on an exemplar activation model as an alternative to onevector-per-word approaches to word meaning in context. [sent-205, score-1.017]
</p><p>88 Exemplar activation is very effective in handling polysemy, even with a very simple (and sparse) bag-of-words vector representation. [sent-206, score-0.517]
</p><p>89 On both the EP08 and EP09 datasets, our models surpass more complex prototype-based approaches (Tab. [sent-207, score-0.065]
</p><p>90 It is also noteworthy that the exemplar activation models work best when few exemplars are used, which bodes well for their efficiency. [sent-209, score-1.29]
</p><p>91 We found that the best target representations re3Since  our  models had the advantage of being tuned  on  the dataset, we also report the range of results across the parameters we tested. [sent-210, score-0.191]
</p><p>92 Paraphrase representations are best activated with a percentage-based threshold. [sent-227, score-0.17]
</p><p>93 Overall, we found that paraphrase activation had a much larger impact on performance than target activation, and that drawing on target exemplars other than s to represent the target meaning in context improved over using s itself only for verbs (Tab. [sent-228, score-1.266]
</p><p>94 This suggests the possibility of considering T’s activated paraphrase candidates as the representation of T in the context s, rather than some vector of T itself, in the spirit of Kintsch (2001). [sent-230, score-0.418]
</p><p>95 While it is encouraging that the best parameter settings involved the activation of only few exemplars, computation with exemplar models still requires the management of large numbers of vectors. [sent-231, score-1.009]
</p><p>96 The computational overhead can be reduced by us-  ing data structures that cut down on the number of vector comparisons, or by decreasing vector dimensionality (Gorman and Curran, 2006). [sent-232, score-0.09]
</p><p>97 , 2008), and we hope that they can be integrated in a more sophisticated exemplar model. [sent-236, score-0.485]
</p><p>98 A structured vector space 96  model for word meaning in context. [sent-268, score-0.105]
</p><p>99 Paraphrase assessment in structured vector space: Exploring parameters and datasets. [sent-274, score-0.066]
</p><p>100 Testing the distributional hypothesis: The influence of context on judgements of semantic similarity. [sent-333, score-0.126]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('exemplar', 0.485), ('activation', 0.472), ('actp', 0.35), ('exemplars', 0.281), ('actt', 0.221), ('paraphrase', 0.18), ('paraphrases', 0.167), ('lexsub', 0.148), ('erk', 0.119), ('activated', 0.105), ('pad', 0.104), ('thater', 0.097), ('knn', 0.097), ('lemma', 0.08), ('distributional', 0.077), ('acttp', 0.074), ('coach', 0.074), ('target', 0.072), ('mccarthy', 0.067), ('activates', 0.065), ('gap', 0.059), ('szpektor', 0.055), ('gnurge', 0.055), ('sim', 0.054), ('polysemy', 0.054), ('act', 0.054), ('context', 0.049), ('representations', 0.046), ('vector', 0.045), ('activating', 0.044), ('dataset', 0.043), ('meaning', 0.043), ('prototype', 0.042), ('landauer', 0.041), ('substitution', 0.04), ('activate', 0.039), ('datapoints', 0.037), ('spoon', 0.037), ('voorspoels', 0.037), ('navigli', 0.036), ('salton', 0.035), ('models', 0.033), ('surpass', 0.032), ('geometrical', 0.032), ('gorman', 0.032), ('metal', 0.032), ('mitchell', 0.032), ('vectors', 0.03), ('adjectives', 0.03), ('sentential', 0.03), ('bow', 0.03), ('xi', 0.029), ('similarity', 0.028), ('cogsci', 0.028), ('prototypes', 0.028), ('lapata', 0.027), ('dumais', 0.026), ('verbs', 0.025), ('daelemans', 0.025), ('cognitive', 0.025), ('pi', 0.024), ('baroni', 0.024), ('percentage', 0.024), ('gold', 0.024), ('sch', 0.023), ('dog', 0.023), ('tdhe', 0.023), ('florian', 0.023), ('representation', 0.023), ('tze', 0.022), ('concept', 0.021), ('parameters', 0.021), ('ranking', 0.021), ('cosine', 0.021), ('sparsity', 0.021), ('nouns', 0.021), ('contexts', 0.021), ('team', 0.02), ('neighbors', 0.02), ('dagan', 0.02), ('fix', 0.02), ('dynamically', 0.02), ('outperform', 0.02), ('predictions', 0.02), ('attractive', 0.019), ('prominent', 0.019), ('athens', 0.019), ('sense', 0.019), ('best', 0.019), ('applicable', 0.018), ('type', 0.018), ('datasets', 0.018), ('annotators', 0.018), ('model', 0.017), ('cooccurrence', 0.017), ('significance', 0.017), ('judged', 0.017), ('entailment', 0.016), ('categorization', 0.016), ('candidates', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="107-tfidf-1" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<p>Author: Katrin Erk ; Sebastian Pado</p><p>Abstract: This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.</p><p>2 0.25045249 <a title="107-tfidf-2" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>Author: Stefan Thater ; Hagen Furstenau ; Manfred Pinkal</p><p>Abstract: We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.</p><p>3 0.14977874 <a title="107-tfidf-3" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.</p><p>4 0.12410437 <a title="107-tfidf-4" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>Author: Linlin Li ; Benjamin Roth ; Caroline Sporleder</p><p>Abstract: This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. nonliteral usages of potentially idiomatic expressions. In all three cases, we outper- form state-of-the-art systems either quantitatively or statistically significantly.</p><p>5 0.075015724 <a title="107-tfidf-5" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>Author: Boxing Chen ; George Foster ; Roland Kuhn</p><p>Abstract: This paper proposes new algorithms to compute the sense similarity between two units (words, phrases, rules, etc.) from parallel corpora. The sense similarity scores are computed by using the vector space model. We then apply the algorithms to statistical machine translation by computing the sense similarity between the source and target side of translation rule pairs. Similarity scores are used as additional features of the translation model to improve translation performance. Significant improvements are obtained over a state-of-the-art hierarchical phrase-based machine translation system. 1</p><p>6 0.060120948 <a title="107-tfidf-6" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>7 0.054835286 <a title="107-tfidf-7" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>8 0.053450249 <a title="107-tfidf-8" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>9 0.051790912 <a title="107-tfidf-9" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>10 0.047781356 <a title="107-tfidf-10" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>11 0.044843365 <a title="107-tfidf-11" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>12 0.044314027 <a title="107-tfidf-12" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>13 0.04292395 <a title="107-tfidf-13" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>14 0.042355515 <a title="107-tfidf-14" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>15 0.041926712 <a title="107-tfidf-15" href="./acl-2010-Learning_Script_Knowledge_with_Web_Experiments.html">165 acl-2010-Learning Script Knowledge with Web Experiments</a></p>
<p>16 0.040316645 <a title="107-tfidf-16" href="./acl-2010-Global_Learning_of_Focused_Entailment_Graphs.html">127 acl-2010-Global Learning of Focused Entailment Graphs</a></p>
<p>17 0.039342597 <a title="107-tfidf-17" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>18 0.039079584 <a title="107-tfidf-18" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>19 0.036873635 <a title="107-tfidf-19" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>20 0.036667246 <a title="107-tfidf-20" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.113), (1, 0.047), (2, -0.017), (3, -0.0), (4, 0.093), (5, -0.002), (6, 0.056), (7, 0.017), (8, 0.014), (9, -0.028), (10, 0.016), (11, 0.047), (12, 0.14), (13, 0.048), (14, 0.008), (15, -0.016), (16, -0.024), (17, -0.085), (18, -0.087), (19, -0.004), (20, 0.24), (21, 0.166), (22, -0.019), (23, 0.029), (24, 0.111), (25, -0.151), (26, -0.064), (27, 0.026), (28, -0.103), (29, -0.161), (30, -0.085), (31, 0.026), (32, -0.046), (33, 0.028), (34, 0.059), (35, 0.056), (36, -0.102), (37, -0.088), (38, -0.015), (39, 0.041), (40, -0.063), (41, -0.093), (42, -0.119), (43, 0.121), (44, -0.046), (45, 0.058), (46, 0.141), (47, 0.097), (48, 0.07), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9359414 <a title="107-lsi-1" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<p>Author: Katrin Erk ; Sebastian Pado</p><p>Abstract: This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.</p><p>2 0.77847767 <a title="107-lsi-2" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>Author: Stefan Thater ; Hagen Furstenau ; Manfred Pinkal</p><p>Abstract: We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.</p><p>3 0.63250393 <a title="107-lsi-3" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>Author: Takashi Onishi ; Masao Utiyama ; Eiichiro Sumita</p><p>Abstract: Lattice decoding in statistical machine translation (SMT) is useful in speech translation and in the translation of German because it can handle input ambiguities such as speech recognition ambiguities and German word segmentation ambiguities. We show that lattice decoding is also useful for handling input variations. Given an input sentence, we build a lattice which represents paraphrases of the input sentence. We call this a paraphrase lattice. Then, we give the paraphrase lattice as an input to the lattice decoder. The decoder selects the best path for decoding. Using these paraphrase lattices as inputs, we obtained significant gains in BLEU scores for IWSLT and Europarl datasets.</p><p>4 0.46855238 <a title="107-lsi-4" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>Author: Linlin Li ; Benjamin Roth ; Caroline Sporleder</p><p>Abstract: This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. nonliteral usages of potentially idiomatic expressions. In all three cases, we outper- form state-of-the-art systems either quantitatively or statistically significantly.</p><p>5 0.41925028 <a title="107-lsi-5" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>Author: Sebastian Rudolph ; Eugenie Giesbrecht</p><p>Abstract: We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.</p><p>6 0.38361838 <a title="107-lsi-6" href="./acl-2010-Online_Generation_of_Locality_Sensitive_Hash_Signatures.html">183 acl-2010-Online Generation of Locality Sensitive Hash Signatures</a></p>
<p>7 0.36476412 <a title="107-lsi-7" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>8 0.35248876 <a title="107-lsi-8" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>9 0.3405754 <a title="107-lsi-9" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>10 0.32198128 <a title="107-lsi-10" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>11 0.29549539 <a title="107-lsi-11" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>12 0.28504857 <a title="107-lsi-12" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>13 0.28257912 <a title="107-lsi-13" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>14 0.28045571 <a title="107-lsi-14" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>15 0.26301336 <a title="107-lsi-15" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>16 0.25423634 <a title="107-lsi-16" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>17 0.24612837 <a title="107-lsi-17" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>18 0.24461465 <a title="107-lsi-18" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>19 0.23397614 <a title="107-lsi-19" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>20 0.23152515 <a title="107-lsi-20" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.015), (23, 0.272), (25, 0.065), (35, 0.011), (42, 0.021), (44, 0.018), (59, 0.078), (64, 0.011), (71, 0.01), (72, 0.016), (73, 0.042), (78, 0.114), (83, 0.082), (84, 0.038), (98, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78530079 <a title="107-lda-1" href="./acl-2010-Comparable_Entity_Mining_from_Comparative_Questions.html">63 acl-2010-Comparable Entity Mining from Comparative Questions</a></p>
<p>Author: Shasha Li ; Chin-Yew Lin ; Young-In Song ; Zhoujun Li</p><p>Abstract: Comparing one thing with another is a typical part of human decision making process. However, it is not always easy to know what to compare and what are the alternatives. To address this difficulty, we present a novel way to automatically mine comparable entities from comparative questions that users posted online. To ensure high precision and high recall, we develop a weakly-supervised bootstrapping method for comparative question identification and comparable entity extraction by leveraging a large online question archive. The experimental results show our method achieves F1measure of 82.5% in comparative question identification and 83.3% in comparable entity extraction. Both significantly outperform an existing state-of-the-art method. 1</p><p>same-paper 2 0.76019788 <a title="107-lda-2" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<p>Author: Katrin Erk ; Sebastian Pado</p><p>Abstract: This paper describes ongoing work on distributional models for word meaning in context. We abandon the usual one-vectorper-word paradigm in favor of an exemplar model that activates only relevant occurrences. On a paraphrasing task, we find that a simple exemplar model outperforms more complex state-of-the-art models.</p><p>3 0.71876121 <a title="107-lda-3" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>Author: Swati Tata ; Barbara Di Eugenio</p><p>Abstract: Music Recommendation Systems often recommend individual songs, as opposed to entire albums. The challenge is to generate reviews for each song, since only full album reviews are available on-line. We developed a summarizer that combines information extraction and generation techniques to produce summaries of reviews of individual songs. We present an intrinsic evaluation of the extraction components, and of the informativeness of the summaries; and a user study of the impact of the song review summaries on users’ decision making processes. Users were able to make quicker and more informed decisions when presented with the summary as compared to the full album review.</p><p>4 0.594028 <a title="107-lda-4" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>Author: Alan Ritter ; Mausam Mausam ; Oren Etzioni</p><p>Abstract: The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al. ’s system (Pantel et al., 2007).</p><p>5 0.59232634 <a title="107-lda-5" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>Author: Stefan Thater ; Hagen Furstenau ; Manfred Pinkal</p><p>Abstract: We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.</p><p>6 0.58216596 <a title="107-lda-6" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>7 0.57945186 <a title="107-lda-7" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>8 0.57271564 <a title="107-lda-8" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>9 0.56768578 <a title="107-lda-9" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>10 0.56580603 <a title="107-lda-10" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>11 0.56329918 <a title="107-lda-11" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>12 0.55990928 <a title="107-lda-12" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>13 0.55847979 <a title="107-lda-13" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>14 0.55798101 <a title="107-lda-14" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>15 0.55425799 <a title="107-lda-15" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>16 0.55095929 <a title="107-lda-16" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>17 0.54774475 <a title="107-lda-17" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>18 0.54754478 <a title="107-lda-18" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>19 0.54732352 <a title="107-lda-19" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>20 0.54701716 <a title="107-lda-20" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
