<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-46" href="#">acl2010-46</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</h1>
<br/><p>Source: <a title="acl-2010-46-pdf" href="http://aclweb.org/anthology//P/P10/P10-1096.pdf">pdf</a></p><p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>Reference: <a title="acl-2010-46-reference" href="../acl2010_reference/acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stsg', 0.506), ('gc', 0.257), ('compress', 0.238), ('synchron', 0.233), ('align', 0.178), ('tn', 0.171), ('rul', 0.17), ('fronty', 0.167), ('gramm', 0.144), ('facelift', 0.142), ('nontermin', 0.129), ('delet', 0.128), ('dp', 0.125), ('tre', 0.12), ('nonparamet', 0.119), ('isomorph', 0.114), ('el', 0.111), ('np', 0.099), ('narrow', 0.098), ('atm', 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="46-tfidf-1" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>2 0.45197132 <a title="46-tfidf-2" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>Author: Yoshihide Kato ; Shigeki Matsubara</p><p>Abstract: This paper proposes a method of correcting annotation errors in a treebank. By using a synchronous grammar, the method transforms parse trees containing annotation errors into the ones whose errors are corrected. The synchronous grammar is automatically induced from the treebank. We report an experimental result of applying our method to the Penn Treebank. The result demonstrates that our method corrects syntactic annotation errors with high precision.</p><p>3 0.28283912 <a title="46-tfidf-3" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>Author: Trevor Cohn ; Phil Blunsom</p><p>Abstract: Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous methodâ€™s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently con- verges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</p><p>4 0.25856712 <a title="46-tfidf-4" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>5 0.24474427 <a title="46-tfidf-5" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>Author: Andreas Maletti</p><p>Abstract: A characterization of the expressive power of synchronous tree-adjoining grammars (STAGs) in terms of tree transducers (or equivalently, synchronous tree substitution grammars) is developed. Essentially, a STAG corresponds to an extended tree transducer that uses explicit substitution in both the input and output. This characterization allows the easy integration of STAG into toolkits for extended tree transducers. Moreover, the applicability of the characterization to several representational and algorithmic problems is demonstrated.</p><p>6 0.20741618 <a title="46-tfidf-6" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>7 0.16740881 <a title="46-tfidf-7" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>8 0.16617651 <a title="46-tfidf-8" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>9 0.16098696 <a title="46-tfidf-9" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>10 0.15225676 <a title="46-tfidf-10" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>11 0.14684893 <a title="46-tfidf-11" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>12 0.14125678 <a title="46-tfidf-12" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>13 0.14037186 <a title="46-tfidf-13" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>14 0.13991095 <a title="46-tfidf-14" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>15 0.13679412 <a title="46-tfidf-15" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>16 0.12618256 <a title="46-tfidf-16" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>17 0.12423637 <a title="46-tfidf-17" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>18 0.12389103 <a title="46-tfidf-18" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>19 0.12234929 <a title="46-tfidf-19" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>20 0.11717016 <a title="46-tfidf-20" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.278), (1, 0.237), (2, 0.061), (3, 0.021), (4, -0.106), (5, 0.108), (6, 0.159), (7, -0.146), (8, 0.057), (9, 0.173), (10, -0.101), (11, -0.08), (12, 0.182), (13, 0.107), (14, -0.186), (15, 0.035), (16, 0.026), (17, -0.031), (18, -0.055), (19, 0.09), (20, -0.114), (21, -0.252), (22, -0.02), (23, 0.015), (24, 0.132), (25, 0.032), (26, 0.142), (27, 0.036), (28, -0.022), (29, 0.09), (30, 0.017), (31, 0.045), (32, 0.043), (33, -0.018), (34, 0.038), (35, 0.036), (36, 0.095), (37, 0.009), (38, -0.091), (39, 0.14), (40, 0.055), (41, 0.017), (42, -0.009), (43, -0.072), (44, -0.022), (45, 0.042), (46, -0.074), (47, -0.017), (48, -0.038), (49, -0.12)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93655735 <a title="46-lsi-1" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>2 0.85356587 <a title="46-lsi-2" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>Author: Yoshihide Kato ; Shigeki Matsubara</p><p>Abstract: This paper proposes a method of correcting annotation errors in a treebank. By using a synchronous grammar, the method transforms parse trees containing annotation errors into the ones whose errors are corrected. The synchronous grammar is automatically induced from the treebank. We report an experimental result of applying our method to the Penn Treebank. The result demonstrates that our method corrects syntactic annotation errors with high precision.</p><p>3 0.81029081 <a title="46-lsi-3" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>Author: Trevor Cohn ; Phil Blunsom</p><p>Abstract: Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous methodâ€™s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently con- verges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</p><p>4 0.76930636 <a title="46-lsi-4" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>Author: Andreas Maletti</p><p>Abstract: A characterization of the expressive power of synchronous tree-adjoining grammars (STAGs) in terms of tree transducers (or equivalently, synchronous tree substitution grammars) is developed. Essentially, a STAG corresponds to an extended tree transducer that uses explicit substitution in both the input and output. This characterization allows the easy integration of STAG into toolkits for extended tree transducers. Moreover, the applicability of the characterization to several representational and algorithmic problems is demonstrated.</p><p>5 0.68831629 <a title="46-lsi-5" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>6 0.53199852 <a title="46-lsi-6" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>7 0.51657778 <a title="46-lsi-7" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>8 0.49183774 <a title="46-lsi-8" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>9 0.48928568 <a title="46-lsi-9" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>10 0.4882769 <a title="46-lsi-10" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>11 0.48084015 <a title="46-lsi-11" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>12 0.46659952 <a title="46-lsi-12" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>13 0.46396378 <a title="46-lsi-13" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>14 0.4570348 <a title="46-lsi-14" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>15 0.43798709 <a title="46-lsi-15" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>16 0.43601146 <a title="46-lsi-16" href="./acl-2010-Efficient_Inference_through_Cascades_of_Weighted_Tree_Transducers.html">95 acl-2010-Efficient Inference through Cascades of Weighted Tree Transducers</a></p>
<p>17 0.43138832 <a title="46-lsi-17" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>18 0.39432955 <a title="46-lsi-18" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>19 0.38965631 <a title="46-lsi-19" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>20 0.37462455 <a title="46-lsi-20" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.046), (14, 0.03), (29, 0.042), (40, 0.054), (54, 0.063), (56, 0.042), (68, 0.064), (71, 0.073), (84, 0.172), (96, 0.06), (98, 0.272)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83346796 <a title="46-lda-1" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>same-paper 2 0.81574923 <a title="46-lda-2" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>3 0.77333879 <a title="46-lda-3" href="./acl-2010-Open_Information_Extraction_Using_Wikipedia.html">185 acl-2010-Open Information Extraction Using Wikipedia</a></p>
<p>Author: Fei Wu ; Daniel S. Weld</p><p>Abstract: Information-extraction (IE) systems seek to distill semantic relations from naturallanguage text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? This paper presents WOE, an open IE system which improves dramatically on TextRunnerâ€™s precision and recall. The key to WOEâ€™s performance is a novel form of self-supervised learning for open extractors using heurisâ€” tic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOEâ€™s extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.</p><p>4 0.76183397 <a title="46-lda-4" href="./acl-2010-An_Open-Source_Package_for_Recognizing_Textual_Entailment.html">30 acl-2010-An Open-Source Package for Recognizing Textual Entailment</a></p>
<p>Author: Milen Kouylekov ; Matteo Negri</p><p>Abstract: This paper presents a general-purpose open source package for recognizing Textual Entailment. The system implements a collection of algorithms, providing a configurable framework to quickly set up a working environment to experiment with the RTE task. Fast prototyping of new solutions is also allowed by the possibility to extend its modular architecture. We present the tool as a useful resource to approach the Textual Entailment problem, as an instrument for didactic purposes, and as an opportunity to create a collaborative environment to promote research in the field.</p><p>5 0.70106852 <a title="46-lda-5" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>6 0.69398034 <a title="46-lda-6" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>7 0.6910187 <a title="46-lda-7" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>8 0.68699008 <a title="46-lda-8" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>9 0.6815443 <a title="46-lda-9" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>10 0.67972517 <a title="46-lda-10" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>11 0.67448407 <a title="46-lda-11" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>12 0.66981184 <a title="46-lda-12" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>13 0.66390258 <a title="46-lda-13" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>14 0.66230267 <a title="46-lda-14" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>15 0.66167641 <a title="46-lda-15" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>16 0.66007292 <a title="46-lda-16" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>17 0.65763044 <a title="46-lda-17" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>18 0.64716893 <a title="46-lda-18" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>19 0.64607465 <a title="46-lda-19" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>20 0.6454435 <a title="46-lda-20" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
