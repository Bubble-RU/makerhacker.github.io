<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-46" href="#">acl2010-46</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</h1>
<br/><p>Source: <a title="acl-2010-46-pdf" href="http://aclweb.org/anthology//P/P10/P10-1096.pdf">pdf</a></p><p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>Reference: <a title="acl-2010-46-reference" href="../acl2010_reference/acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu i  Abstract We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. [sent-4, score-0.509]
</p><p>2 These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. [sent-5, score-0.122]
</p><p>3 We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. [sent-6, score-0.577]
</p><p>4 We achieve improvements against a number of baselines, including  expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar. [sent-7, score-0.564]
</p><p>5 1 Introduction Given an aligned corpus of tree pairs, we might want to learn a mapping between the paired trees. [sent-8, score-0.18]
</p><p>6 Such induction of tree mappings has application in a variety of natural-language-processing tasks including machine translation, paraphrase, and sentence compression. [sent-9, score-0.232]
</p><p>7 The induced tree mappings can be expressed by synchronous grammars. [sent-10, score-0.32]
</p><p>8 Where the tree pairs are isomorphic, synchronous context-free grammars (SCFG) may suffice, but in general, non-isomorphism can make the problem of rule extraction difficult (Galley and McKeown, 2007). [sent-11, score-0.549]
</p><p>9 More expressive formalisms such as synchronous tree-substitution (Eisner, 2003) or treeadjoining grammars may better capture the pairings. [sent-12, score-0.299]
</p><p>10 In this work, we explore techniques for inducing synchronous tree-substitution grammars (STSG) using as a testbed application extractive sentence compression. [sent-13, score-0.542]
</p><p>11 1 These elementary tree pairs serve as the rules of the extracted grammar. [sent-15, score-0.671]
</p><p>12 For SCFG, segmentation is trivial each parent with its immediate children is an elementary tree but the formalism then restricts us to deriving isomorphic tree pairs. [sent-16, score-0.905]
</p><p>13 STSG is much more expressive, especially if we allow some elementary trees on the source or target side to be unsynchronized, so that insertions and deletions can be modeled, but the segmentation and alignment problems become nontrivial. [sent-17, score-0.711]
</p><p>14 Previous approaches to this problem have treated the two steps grammar extraction and weight estimation with a variety of methods. [sent-18, score-0.067]
</p><p>15 —  —  —  —  One approach is to use word alignments (where these can be reliably estimated, as in our testbed application) to align subtrees and extract rules (Och and Ney, 2004; Galley et al. [sent-19, score-0.288]
</p><p>16 Once a given set of rules is extracted, weights can be imputed using a discriminative approach to maximize the (joint or conditional) likelihood or the classification margin in the training data (taking or not taking into account the derivational ambiguity). [sent-24, score-0.116]
</p><p>17 First, EM search over the space of all possible rules is computationally impractical. [sent-29, score-0.116]
</p><p>18 Second, even if such a search were practical, the method is degenerate, pushing the probability mass towards larger rules in order to better approximate the empirical distribution of the data (Goldwater et al. [sent-30, score-0.171]
</p><p>19 Indeed, the optimal grammar would be one in which each tree pair in the training data is its own rule. [sent-33, score-0.213]
</p><p>20 Therefore, proposals for using EM for this task start with a precomputed subset of rules, and with EM used just to assign weights within this grammar. [sent-34, score-0.035]
</p><p>21 In summary, previous methods suffer from problems of narrowness of search, having to restrict the space of possible rules, and overfitting in preferring overly specific grammars. [sent-35, score-0.307]
</p><p>22 We pursue the use of hierarchical probabilistic models incorporating sparse priors to simultaneously solve both the narrowness and overfitting problems. [sent-36, score-0.336]
</p><p>23 Such models have been used as generative solutions to several other segmentation problems, ranging from word segmentation (Goldwater et al. [sent-37, score-0.233]
</p><p>24 Interestingly, samplingbased nonparametric inference further allows the possibility of searching over the infinite space of grammars (and, in machine translation, possible word alignments), thus side-stepping the narrowness problem outlined above as well. [sent-43, score-0.408]
</p><p>25 In this work, we use an extension of the aforementioned models of generative segmentation for STSG induction, and describe an algorithm for posterior inference under this model that is tailored to the task of extractive sentence compression. [sent-44, score-0.54]
</p><p>26 This task is characterized by the availability of word alignments, providing a clean testbed for investigating the effects of grammar extraction. [sent-45, score-0.176]
</p><p>27 We achieve substantial improvements against a number of baselines including EM, support vector machine (SVM) based discriminative training, and variational Bayes (VB). [sent-46, score-0.047]
</p><p>28 Our results are thus not only encouraging for grammar estimation using sparse priors but also illustrate the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar. [sent-48, score-0.643]
</p><p>29 In the following, we define the task of extractive sentence compression and the Bayesian STSG  model, and algorithms we used for inference and prediction. [sent-49, score-0.481]
</p><p>30 We then describe the experiments in extractive sentence compression and present our results in contrast with alternative algorithms. [sent-50, score-0.422]
</p><p>31 We conclude by giving examples of compression patterns learned by the Bayesian method. [sent-51, score-0.217]
</p><p>32 2  Sentence compression  Sentence compression is the task of summarizing a sentence while retaining most of the informational content and remaining grammatical (Jing, 2000). [sent-52, score-0.485]
</p><p>33 In extractive sentence compression, which we focus on in this paper, an order-preserving subset of the words in the sentence are selected to form the summary, that is, we summarize by deleting words (Knight and Marcu, 2002). [sent-53, score-0.256]
</p><p>34 An example sentence pair, which we use as a running example, is the following: •  Like FaceLift, much of ATM’s screen performance depends on t ohfe A underlying application. [sent-54, score-0.099]
</p><p>35 Like  FaceLift,  much  of  •  ATM’s screen performance depends on the underlying application. [sent-55, score-0.048]
</p><p>36 938  Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression. [sent-56, score-0.306]
</p><p>37 In supervised sentence compression, the goal is to generalize from a parallel training corpus of sentences (source) and their compressions (target) to unseen sentences in a test set to predict their compressions. [sent-58, score-0.051]
</p><p>38 3  The STSG Model  Synchronous tree-substitution grammar is a formalism for synchronously generating a pair of non-isomorphic source and target trees (Eisner, 2003). [sent-61, score-0.381]
</p><p>39 We use square bracketed indices to represent the align→  ment γ of frontier nodes NP[1] aligns with NP[1], VP[2] aligns with VP[2], NP[? [sent-65, score-0.395]
</p><p>40 -aligned target nodes are used to represent insertions into the target tree. [sent-69, score-0.185]
</p><p>41 can be used to continue deriving the deleted subtree. [sent-72, score-0.091]
</p><p>42 See Figure 1for an example of how an STSG with these rules would operate in synchronously generating our example sentence pair. [sent-73, score-0.284]
</p><p>43 STSG is a convenient choice of formalism for a number of reasons. [sent-74, score-0.07]
</p><p>44 Second, the ability to have rules deeper than one level provides a principled way of modeling lexicalization, whose importance has been emphasized (Galley and McKeown, 2007; Yamangil and Nelken, 2008). [sent-76, score-0.116]
</p><p>45 Third, we may have our STSG operate on trees instead of sentences, which allows for efficient parsing algorithms, as well as providing syntactic analyses for our predictions, which is  desirable for automatic evaluation purposes. [sent-77, score-0.126]
</p><p>46 A straightforward extension of the popular EM algorithm for probabilistic context free grammars (PCFG), the inside-outside algorithm (Lari and Young, 1990), can be used to estimate the rule weights of a given unweighted STSG based on a corpus ofparallel parse trees t = t1, . [sent-78, score-0.297]
</p><p>47 Similarly, an 939  Figure 2: Gibbs sampling updates. [sent-85, score-0.058]
</p><p>48 We illustrate a sampler move to align/unalign a source node with a target node (top row in blue), and split/merge a deletion rule via aligning with ? [sent-86, score-0.228]
</p><p>49 extension of the Viterbi algorithm is available for finding the maximum probability derivation, useful for predicting the target analysis tN+1,t for a test instance tN+1,s. [sent-88, score-0.076]
</p><p>50 (Eisner, 2003) However, as noted earlier, EM is subject to the narrowness and overfitting problems. [sent-89, score-0.237]
</p><p>51 1 The Bayesian generative process Both of these issues can be addressed by taking a nonparametric Bayesian approach, namely, assuming that the elementary tree pairs are sampled from an independent collection of Dirichlet process (DP) priors. [sent-91, score-0.712]
</p><p>52 We describe such a process for sampling a corpus of tree pairs t. [sent-92, score-0.257]
</p><p>53 For all pairs of root labels c = cs/ct that we consider, where up to one of cs or ct can be ? [sent-93, score-0.224]
</p><p>54 We then sample a sequence of elementary tree pairs to serve as a derivation for each observed derived tree pair. [sent-98, score-0.842]
</p><p>55 , N, we sample elementary tree pairs en = en,1 , . [sent-102, score-0.635]
</p><p>56 , en,dn in a derivation sequence (where dn is the number of rules used in the derivation), consulting Gc whenever an elementary tree pair with root c is to be sampled. [sent-105, score-0.792]
</p><p>57 e  ∼iid  Gc,  for all e whose root label is c  Given the derivation sequence en, a tree pair tn is determined, that is,  p(tn| en) =? [sent-106, score-0.463]
</p><p>58 n,dnderives tn  (2) The hyperparameters αc can be incorporated into the generative model as random variables; however, we opt to fix these at various constants to investigate different levels of sparsity. [sent-111, score-0.238]
</p><p>59 For the base distribution P0(· | c) there are a variety of choices; we used the( following simple scenario. [sent-112, score-0.121]
</p><p>60 ) Synchronous rules For the case where neither cs nor ct are the special symbol ? [sent-114, score-0.214]
</p><p>61 , the base distribution first generates es and et independently, and then samples an alignment between the frontier nodes. [sent-115, score-0.419]
</p><p>62 Given a nonterminal, an elementary tree is generated by first making a decision to expand the nonterminal (with probability βc) or to leave it as a frontier node (1 βc). [sent-116, score-0.801]
</p><p>63 If the decision to expand was made, we sample an appropriate −  rule from a PCFG which we estimate ahead 940  of time from the training corpus. [sent-117, score-0.174]
</p><p>64 We expand the nonterminal using this rule, and then repeat the same procedure for every child generated that is a nonterminal until there are no generated nonterminal children left. [sent-118, score-0.328]
</p><p>65 Finally, we sample an alignment between the frontier nodes uniformly at random out of all possible alingments. [sent-120, score-0.305]
</p><p>66 , that is, we have a deletion rule, we need to generate e = es/? [sent-122, score-0.063]
</p><p>67 ) The base distribution generates es using the same process described for synchronous rules above. [sent-125, score-0.496]
</p><p>68 Then with probability 1we align all frontier nodes in es with ? [sent-126, score-0.339]
</p><p>69 In essence, this process generates TSG rules, rather than STSG rules, which are used to cover deleted (or inserted) subtrees. [sent-128, score-0.057]
</p><p>70 This simple base distribution does nothing to enforce an alignment between the internal nodes of es and et. [sent-129, score-0.312]
</p><p>71 One may come up with more sophisticated base distributions. [sent-130, score-0.066]
</p><p>72 However the main point of the base distribution is to encode a controllable preference towards simpler rules; we therefore make the simplest possible assumption. [sent-131, score-0.156]
</p><p>73 2 Posterior inference via Gibbs sampling Assuming fixed hyperparameters α = {αc} and β = {βc}, our hinyfpeererpnacrea problem i s= =to { αfin}d atnhed posterior }di,st oruibru itniofner eonfc tehe p rdoebrlivemati oisn sequences e = e1, . [sent-133, score-0.211]
</p><p>74 Applying Bayes’ rule, we have p(e | t)  ∝  p(t | e)p(e)  (3)  where p(t | e) is a 0/1 distribution (2) which does nwohte depend on Gc, a/n1d d p(e) can b (e2 )o wbthaiicnhed d by collapsing Gc for all c. [sent-140, score-0.055]
</p><p>75 The conditional prior of the i-th elementary  tree pair given previously generated ones e1, . [sent-146, score-0.502]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stsg', 0.424), ('elementary', 0.356), ('compression', 0.217), ('gc', 0.216), ('synchronous', 0.174), ('narrowness', 0.159), ('frontier', 0.159), ('extractive', 0.154), ('tree', 0.146), ('tn', 0.143), ('facelift', 0.119), ('rules', 0.116), ('dp', 0.105), ('nonparametric', 0.102), ('derivation', 0.101), ('nonterminal', 0.094), ('segmentation', 0.089), ('trees', 0.088), ('rule', 0.088), ('grammars', 0.088), ('em', 0.085), ('es', 0.085), ('np', 0.083), ('atm', 0.079), ('iid', 0.079), ('synchronously', 0.079), ('yamangil', 0.079), ('overfitting', 0.078), ('bayesian', 0.077), ('testbed', 0.075), ('root', 0.073), ('aligns', 0.07), ('formalism', 0.07), ('preferring', 0.07), ('grammar', 0.067), ('base', 0.066), ('isomorphic', 0.064), ('sparse', 0.063), ('deletion', 0.063), ('inference', 0.059), ('sampling', 0.058), ('ct', 0.058), ('ei', 0.058), ('deleted', 0.057), ('galley', 0.056), ('gibbs', 0.056), ('distribution', 0.055), ('generative', 0.055), ('harvard', 0.054), ('merits', 0.054), ('alignment', 0.054), ('posterior', 0.054), ('alignments', 0.054), ('pairs', 0.053), ('scfg', 0.052), ('lexicalization', 0.052), ('parametric', 0.052), ('nodes', 0.052), ('sentence', 0.051), ('eisner', 0.05), ('screen', 0.048), ('denero', 0.048), ('insertions', 0.047), ('variational', 0.047), ('expand', 0.046), ('tailored', 0.045), ('goldwater', 0.045), ('bayes', 0.045), ('vp', 0.045), ('indices', 0.044), ('align', 0.043), ('target', 0.043), ('en', 0.04), ('hyperparameters', 0.04), ('maximization', 0.04), ('generality', 0.04), ('cs', 0.04), ('sample', 0.04), ('mckeown', 0.038), ('operate', 0.038), ('expressive', 0.037), ('priors', 0.036), ('cohn', 0.036), ('simplest', 0.035), ('induction', 0.035), ('minimality', 0.035), ('pnp', 0.035), ('oev', 0.035), ('evidenced', 0.035), ('exemplar', 0.035), ('mitigating', 0.035), ('precomputed', 0.035), ('turner', 0.035), ('characterized', 0.034), ('gildea', 0.034), ('pcfg', 0.034), ('source', 0.034), ('deriving', 0.034), ('aligned', 0.034), ('extension', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="46-tfidf-1" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>2 0.41292146 <a title="46-tfidf-2" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>Author: Yoshihide Kato ; Shigeki Matsubara</p><p>Abstract: This paper proposes a method of correcting annotation errors in a treebank. By using a synchronous grammar, the method transforms parse trees containing annotation errors into the ones whose errors are corrected. The synchronous grammar is automatically induced from the treebank. We report an experimental result of applying our method to the Penn Treebank. The result demonstrates that our method corrects syntactic annotation errors with high precision.</p><p>3 0.27086404 <a title="46-tfidf-3" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>Author: Trevor Cohn ; Phil Blunsom</p><p>Abstract: Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently con- verges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</p><p>4 0.25580779 <a title="46-tfidf-4" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>5 0.18027888 <a title="46-tfidf-5" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>Author: Andreas Maletti</p><p>Abstract: A characterization of the expressive power of synchronous tree-adjoining grammars (STAGs) in terms of tree transducers (or equivalently, synchronous tree substitution grammars) is developed. Essentially, a STAG corresponds to an extended tree transducer that uses explicit substitution in both the input and output. This characterization allows the easy integration of STAG into toolkits for extended tree transducers. Moreover, the applicability of the characterization to several representational and algorithmic problems is demonstrated.</p><p>6 0.14910918 <a title="46-tfidf-6" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>7 0.1369759 <a title="46-tfidf-7" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>8 0.13134341 <a title="46-tfidf-8" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>9 0.1304376 <a title="46-tfidf-9" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>10 0.12877254 <a title="46-tfidf-10" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>11 0.12239082 <a title="46-tfidf-11" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>12 0.12171952 <a title="46-tfidf-12" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>13 0.11155129 <a title="46-tfidf-13" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>14 0.10668432 <a title="46-tfidf-14" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>15 0.097710416 <a title="46-tfidf-15" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>16 0.093054652 <a title="46-tfidf-16" href="./acl-2010-Top-Down_K-Best_A%2A_Parsing.html">236 acl-2010-Top-Down K-Best A* Parsing</a></p>
<p>17 0.092486389 <a title="46-tfidf-17" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>18 0.091228314 <a title="46-tfidf-18" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>19 0.090816624 <a title="46-tfidf-19" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>20 0.090399094 <a title="46-tfidf-20" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.247), (1, -0.196), (2, 0.067), (3, -0.027), (4, -0.145), (5, -0.058), (6, 0.197), (7, -0.011), (8, -0.026), (9, -0.233), (10, -0.004), (11, -0.223), (12, 0.071), (13, -0.108), (14, -0.011), (15, -0.104), (16, 0.067), (17, -0.12), (18, 0.078), (19, 0.057), (20, -0.101), (21, 0.091), (22, 0.142), (23, 0.107), (24, 0.086), (25, -0.175), (26, 0.073), (27, -0.068), (28, 0.007), (29, -0.03), (30, 0.037), (31, -0.019), (32, -0.022), (33, -0.007), (34, 0.02), (35, 0.085), (36, 0.029), (37, 0.053), (38, 0.006), (39, 0.147), (40, -0.14), (41, -0.049), (42, -0.032), (43, -0.003), (44, -0.006), (45, -0.063), (46, -0.048), (47, 0.085), (48, -0.059), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95722884 <a title="46-lsi-1" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>2 0.86910498 <a title="46-lsi-2" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>Author: Trevor Cohn ; Phil Blunsom</p><p>Abstract: Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently con- verges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</p><p>3 0.85199261 <a title="46-lsi-3" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>Author: Yoshihide Kato ; Shigeki Matsubara</p><p>Abstract: This paper proposes a method of correcting annotation errors in a treebank. By using a synchronous grammar, the method transforms parse trees containing annotation errors into the ones whose errors are corrected. The synchronous grammar is automatically induced from the treebank. We report an experimental result of applying our method to the Penn Treebank. The result demonstrates that our method corrects syntactic annotation errors with high precision.</p><p>4 0.70717216 <a title="46-lsi-4" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>Author: Andreas Maletti</p><p>Abstract: A characterization of the expressive power of synchronous tree-adjoining grammars (STAGs) in terms of tree transducers (or equivalently, synchronous tree substitution grammars) is developed. Essentially, a STAG corresponds to an extended tree transducer that uses explicit substitution in both the input and output. This characterization allows the easy integration of STAG into toolkits for extended tree transducers. Moreover, the applicability of the characterization to several representational and algorithmic problems is demonstrated.</p><p>5 0.67040771 <a title="46-lsi-5" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>6 0.50996524 <a title="46-lsi-6" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>7 0.50454593 <a title="46-lsi-7" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>8 0.49052146 <a title="46-lsi-8" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>9 0.47198406 <a title="46-lsi-9" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>10 0.46884501 <a title="46-lsi-10" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>11 0.43664244 <a title="46-lsi-11" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>12 0.42999911 <a title="46-lsi-12" href="./acl-2010-Efficient_Inference_through_Cascades_of_Weighted_Tree_Transducers.html">95 acl-2010-Efficient Inference through Cascades of Weighted Tree Transducers</a></p>
<p>13 0.42022637 <a title="46-lsi-13" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>14 0.38233581 <a title="46-lsi-14" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>15 0.37693608 <a title="46-lsi-15" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>16 0.37021455 <a title="46-lsi-16" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>17 0.36735162 <a title="46-lsi-17" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>18 0.35770178 <a title="46-lsi-18" href="./acl-2010-Optimal_Rank_Reduction_for_Linear_Context-Free_Rewriting_Systems_with_Fan-Out_Two.html">186 acl-2010-Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two</a></p>
<p>19 0.33413598 <a title="46-lsi-19" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>20 0.32272807 <a title="46-lsi-20" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.289), (14, 0.021), (25, 0.108), (33, 0.022), (42, 0.018), (44, 0.018), (59, 0.093), (73, 0.036), (78, 0.056), (83, 0.076), (84, 0.017), (98, 0.155)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86689067 <a title="46-lda-1" href="./acl-2010-Optimal_Rank_Reduction_for_Linear_Context-Free_Rewriting_Systems_with_Fan-Out_Two.html">186 acl-2010-Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two</a></p>
<p>Author: Benoit Sagot ; Giorgio Satta</p><p>Abstract: Linear Context-Free Rewriting Systems (LCFRSs) are a grammar formalism capable of modeling discontinuous phrases. Many parsing applications use LCFRSs where the fan-out (a measure of the discontinuity of phrases) does not exceed 2. We present an efficient algorithm for optimal reduction of the length of production right-hand side in LCFRSs with fan-out at most 2. This results in asymptotical running time improvement for known parsing algorithms for this class.</p><p>same-paper 2 0.83471215 <a title="46-lda-2" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar.</p><p>3 0.82967395 <a title="46-lda-3" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>Author: Morgan Sonderegger ; Partha Niyogi</p><p>Abstract: English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. We describe a longitudinal dataset of N/V pair pronunciations, leading to a set of properties to be accounted for by any computational model. We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. We compare each model’s dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect population-level dynamics.</p><p>4 0.77019274 <a title="46-lda-4" href="./acl-2010-A_New_Approach_to_Improving_Multilingual_Summarization_Using_a_Genetic_Algorithm.html">11 acl-2010-A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm</a></p>
<p>Author: Marina Litvak ; Mark Last ; Menahem Friedman</p><p>Abstract: Automated summarization methods can be defined as “language-independent,” if they are not based on any languagespecific knowledge. Such methods can be used for multilingual summarization defined by Mani (2001) as “processing several languages, with summary in the same language as input.” In this paper, we introduce MUSE, a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages—English and Hebrew—and evaluated its performance with ROUGE-1 Recall vs. state- of-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank1) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.</p><p>5 0.65802622 <a title="46-lda-5" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>Author: Trevor Cohn ; Phil Blunsom</p><p>Abstract: Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently con- verges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</p><p>6 0.6192376 <a title="46-lda-6" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>7 0.6165275 <a title="46-lda-7" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>8 0.61558437 <a title="46-lda-8" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>9 0.60576648 <a title="46-lda-9" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>10 0.60510671 <a title="46-lda-10" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>11 0.60501415 <a title="46-lda-11" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>12 0.60148031 <a title="46-lda-12" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>13 0.60144609 <a title="46-lda-13" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>14 0.60009646 <a title="46-lda-14" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>15 0.59924829 <a title="46-lda-15" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>16 0.59785938 <a title="46-lda-16" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>17 0.59779382 <a title="46-lda-17" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>18 0.59724295 <a title="46-lda-18" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>19 0.59612763 <a title="46-lda-19" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>20 0.59553117 <a title="46-lda-20" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
