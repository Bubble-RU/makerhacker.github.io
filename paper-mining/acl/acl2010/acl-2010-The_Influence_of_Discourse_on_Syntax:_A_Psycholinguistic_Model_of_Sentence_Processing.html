<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-229" href="#">acl2010-229</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</h1>
<br/><p>Source: <a title="acl-2010-229-pdf" href="http://aclweb.org/anthology//P/P10/P10-1120.pdf">pdf</a></p><p>Author: Amit Dubey</p><p>Abstract: Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. Our simulations show that a Weakly Interactive cognitive architecture can explain data which had been provided as evidence for the Strongly Interactive hypothesis.</p><p>Reference: <a title="acl-2010-229-reference" href="../acl2010_reference/acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. [sent-3, score-0.726]
</p><p>2 This paper addresses this gap by building a computational model which simulates the influence of discourse on syntax. [sent-11, score-0.225]
</p><p>3 This is the first model we know of which introduces a broad-coverage sentence processing model which takes the effect of coreference and discourse into account. [sent-20, score-0.485]
</p><p>4 A major question concerning discourse-syntax interactions involves the strength of communication between discourse and syntactic information. [sent-21, score-0.228]
</p><p>5 The Weakly Interactive (Altmann and Steedman, 1988) hypothesis states that a discourse context can reactively prune syntactic choices that have been proposed by the parser, whereas the Strongly Interactive hypothesis posits that context can proactively suggest choices to the syntactic processor. [sent-22, score-0.477]
</p><p>6 Support for Weak Interaction comes from  experiments in which there are temporary ambiguities, or garden paths, which cause processing difficulty. [sent-23, score-0.297]
</p><p>7 The general finding is that supportive contexts can reduce the effect of the garden path. [sent-24, score-0.515]
</p><p>8 (2005) found that supportive contexts even facilitate the processing of unambiguous sentences. [sent-26, score-0.295]
</p><p>9 There are three main parts of the model: a syntactic processor, a coreference resolution system, and a simple pragmatics processor which computes certain limited forms of discourse coherence. [sent-30, score-0.827]
</p><p>10 Following Hale (2001) and Levy (2008), among others, the syntactic processor uses an incremental probabilistic Earley parser to compute a metric which  correlates with increased reading difficulty. [sent-31, score-0.534]
</p><p>11 The coreference resolution system is implemented 1179  Proce dingUsp opfs thaela 4, 8Stwhe Adnen u,a 1l1- M16e Jtiunlgy o 2f0 t1h0e. [sent-32, score-0.292]
</p><p>12 Finally, the pragmatics processing system contains a small set of probabilistic constraints which convey some intuitive facts about discourse processing. [sent-35, score-0.384]
</p><p>13 ’s result on unambiguous syntactic structures and we present a new experiment on involving a garden path which was designed to be similar to the Grodner et al. [sent-40, score-0.65]
</p><p>14 1 Discourse and Ambiguity Resolution There is a fairly large literature on garden path experiments involving context (Crain and Steedman, 1985; Mitchell et al. [sent-45, score-0.474]
</p><p>15 Spivey and Tanenhaus, 1998) have used reduced relative clause attachment ambiguity. [sent-50, score-0.284]
</p><p>16 1 The experimental items all had a target sentence containing a relative clause, and one of two possible context sentences, one of which supports the relative clause reading and the other which does not. [sent-53, score-0.662]
</p><p>17 (1998), were either the reduced or unreduced sentences similar to: (2)  The postman who was carried by the paramedics was having trouble breathing. [sent-61, score-0.284]
</p><p>18 We measured reading times in the underlined region, which is the first point at which there is evidence for the relative clause interpretation. [sent-63, score-0.443]
</p><p>19 The relative clauses in the target sentence act as restrictive relative clauses, selecting one referent from a larger set. [sent-65, score-0.395]
</p><p>20 This makes the context in Example (1-a) supportive of a reduced relative reading, and the context in Example (1-b) unsupportive of a reduced relative clause. [sent-67, score-0.665]
</p><p>21 Other experiments, for instance Spivey and Tanenhaus (1998), used an unsupportive context where only one postman was mentioned. [sent-68, score-0.313]
</p><p>22 Results An ANOVA revealed that all conditions with a supportive context were read faster than one  with a neutral context (i. [sent-73, score-0.334]
</p><p>23 a main effect of context), and all conditions with unambiguous syntax were read faster than those with a garden path (i. [sent-75, score-0.713]
</p><p>24 Finally, there was a statisically significant interaction between syntax and discourse whereby context decreases reading times much more when a garden path is present compared to an unambiguous structure. [sent-78, score-1.099]
</p><p>25 In other words, a supportive context helped reduce the effect of a garden path. [sent-79, score-0.581]
</p><p>26 (2005) proposed an experiment with a supportive or unsupportive discourse followed by an unambiguous target sentence. [sent-84, score-0.655]
</p><p>27 The director that the critics at a banquet announced that retiring to make room for talent in the industry. [sent-87, score-0.259]
</p><p>28 praised he was young praised he was young  They also manipulated the context, which was either supportive of the target, or a null context. [sent-89, score-0.302]
</p><p>29 A group of film critics praised a director at a banquet and another director at a film premiere. [sent-92, score-0.443]
</p><p>30 A group of film critics praised a director and a producer for lifetime achievement. [sent-93, score-0.268]
</p><p>31 The target sentence in (3-a) is a restrictive relative clause, as in the garden path experiments. [sent-94, score-0.705]
</p><p>32 Therefore, the context (4-a) is only used with the restrictive relative clause, and the context (4-b), where only one director is mentioned, is used as the context for the non-restrictive relative clause. [sent-96, score-0.636]
</p><p>33 They found that the supportive contexts decreased reading time, and that this effect was stronger for restrictive relatives compared to nonrestricted relatives. [sent-102, score-0.536]
</p><p>34 As there was no garden path, and hence no incorrect structure for the discourse processor to prune, the authors conclude that this must be evidence for the Strongly Interactive hypothesis. [sent-103, score-0.663]
</p><p>35 Unlike the garden path experiment above, these results do not appear to be consistent with a Weakly Interactive model. [sent-104, score-0.484]
</p><p>36 NP  NP  VP  VP  The postman  VBDPP  carried INNP-LGS by The paramedics (a) Standard WSJ Tree  . [sent-106, score-0.247]
</p><p>37 S  NP  NPbase  VP  VP-LGS  The postman VBD1  carried  PP:by  IN:by  NPbase-LGS  by  The paramedics  (b) Minimally Modified Tree Figure 1: A schematic representation of the smallest set of grammar transformations which we found were required to accurately parse the experimental items. [sent-107, score-0.247]
</p><p>38 Because these results are computed as regressions against a baseline, a reading time of 0ms indicates average difficulty, with negative numbers showing some facilitation has occured, and positive number indicating reading difficulty. [sent-109, score-0.352]
</p><p>39 3 Model The model comprises three parts: a parser, a coreference resolution system, and a pragmatics subsystem. [sent-110, score-0.419]
</p><p>40 1 Parser The parser is an incremental unlexicalized probabilistic Earley parser, which is capable of computing prefix probabilities. [sent-113, score-0.238]
</p><p>41 Using the prefix proPbability, we can compute the word-by-word Surprisal (Hale, 2001), by taking the log ratio of the previous word’s prefix  probability against this word’s prefix probability:  log? [sent-122, score-0.262]
</p><p>42 Higher  Surprisal  scores  are interpreted  (1)  as  1181  being correlated with more reading difficulty, and likewise lower scores with greater reading ease. [sent-130, score-0.314]
</p><p>43 However, as the coreference processor takes trees as input, we  must therefore unpack parses before resolving referential ambiguity. [sent-137, score-0.388]
</p><p>44 the agent), get the -LGS label; (iv) non-recursive NPs are renamed NPbase (the coreference system treats each NPbase as a markable). [sent-148, score-0.208]
</p><p>45 2 Discourse Processor The primary function of the discourse processing  module is to perform coreference resolution for each mention in an incrementally processed text. [sent-150, score-0.481]
</p><p>46 Because each mention in a coreference chains is transitive, we cannot use a simple classifier, as they cannot enforce global transitivity constraints. [sent-151, score-0.208]
</p><p>47 Two of these predicates, Coref and First, are the output of the MLN they provide a labelling of coreference mentions into entity classes. [sent-168, score-0.265]
</p><p>48 The remaining predicates in Table 1 are a subset of features used by other coreference resolution systems (cf. [sent-171, score-0.355]
</p><p>49 ) As our main focus is not to produce a state-of-the-art coreference system, we do not include predicates which are irrevelant for our simulations even if they have been shown to be effective for coreference resolution. [sent-177, score-0.521]
</p><p>50 For example, we do not have predicates if two mentions are in an apposition relationship, or if two mentions are synonyms for each other. [sent-178, score-0.177]
</p><p>51 To test that the coreference resolution system was producing meaningful results, we evaluated our system on the test section of the ACE-2 dataset. [sent-182, score-0.292]
</p><p>52 The discourse model is run iteratively at each word. [sent-186, score-0.189]
</p><p>53 At each step, we compute both the maximum a posteriori (MAP) assignment of coreference relationships as well  as the probability that each individual coreference assignment is true. [sent-191, score-0.453]
</p><p>54 Taken together, they allow us to calculate, for a coreference assignment c, Pcoref(c|w, t) where w is the text input (of the entire( cd|owcu,mt)e wnth huenrteil wthis is point), axntd i tn pisu tth (eo parse of each tree in the document up to and including the current incremental parse. [sent-192, score-0.253]
</p><p>55 Overall, we have:  P(w)  = =  XXP(c,w,t) XXc  Xt  XXPcoref(c|w,t)Pparser(w,t) Xc  Xt  Note that we only consider one possible assignment of NPs to coreference entities per parse, as we only retrieve the probabilities of the MAP solution. [sent-194, score-0.252]
</p><p>56 3 Pragmatics Processor The effect of context in the experiments described  in Section 2 cannot be fully explained using a coreference resolution system alone. [sent-196, score-0.408]
</p><p>57 In the case of restrictive relative clauses, the referential ‘mismatch’ in the unsupported conditions is caused by an expectation elicited by a restrictive relative clause which is inconsistent with the previous discourse when there is no salient restricted subset of a larger set. [sent-197, score-0.966]
</p><p>58 When the larger set is not found in the discourse, the relative clause becomes incoherent given the context, causing reading difficulty. [sent-198, score-0.404]
</p><p>59 Modeling this coherence constraint is essentially a pragmatics problem, and is under the purview of the pragmatics processor in our system. [sent-199, score-0.392]
</p><p>60 The pragmatics processor is quite specialised and, although the information it encapsulates is quite intuitive, it nonetheless relies on hand-coded expert knowledge. [sent-200, score-0.265]
</p><p>61 The pragmatics processor takes as input an incremental pragmatics configuration p and computes the probability Pprag(p|w, t, c). [sent-201, score-0.437]
</p><p>62 The first two elements of the 3-tuple depend on the identity of the determiner as recovered by the parser, and on whether the coreference system adduces the predicate First for the current NP. [sent-206, score-0.208]
</p><p>63 As the coreference system wasn’t designed to find anaphoric contrast sets, these sets were found using a simple post-processing check. [sent-207, score-0.208]
</p><p>64 The distribution Pprag(p|w, t, c) applies a processing penalty for an unsupported restrictive relative clause whenever a restrictive relative clause is in the n best list. [sent-210, score-0.892]
</p><p>65 Because Surprisal computes a ratio of probabilities, this in effect means we only pay this penality when an unsupported restrictive relative clause first appears in the n best list (otherwise the effect is cancelled out). [sent-211, score-0.584]
</p><p>66 The penalty for discourse new entities is applied on the first word (ignoring punctuation) following the end of the NP. [sent-212, score-0.189]
</p><p>67 This spillover processing effect is simply a matter of modeling convenience: without it, we would have to compute Surprisal probabilities over regions rather than individual words. [sent-213, score-0.169]
</p><p>68 1 Method  When modeling the garden path experiment we presented in Section 2. [sent-216, score-0.484]
</p><p>69 1, we compute Surprisal values on the word ‘by’, which is the earliest point at which there is evidence for a relative clause interpretation. [sent-217, score-0.323]
</p><p>70 Again, this is the earliest point at which there is evidence for a relative clause, and depending upon the presence or absence of a preceding comma, it will be known to be restrictive or nonrestrictive clause. [sent-220, score-0.298]
</p><p>71 In addition to the overall Surprisal values, we also compute syntactic Surprisal scores, to test if there is any benefit from the discourse and pragmatics subsystems. [sent-221, score-0.392]
</p><p>72 For the garden path experiment, the simulation was run on each of the 28 experimental items in each of the 4 conditions, resulting in a total of 112 runs. [sent-223, score-0.552]
</p><p>73 For each run, the model was reset, purging all discourse information gained while reading earlier items. [sent-226, score-0.346]
</p><p>74 First, the simulated results suggested a much larger reading difficulty due to ambiguity than the experimental results. [sent-235, score-0.33]
</p><p>75 Also, in the unambiguous case, the model predicted a null cost of an unsupportive context on the word ‘by’, because the model bears the cost of an unsupportive context earlier in the sentence, and assumes no spillover to the word ‘by’ . [sent-236, score-0.487]
</p><p>76 experiment  (c) Syntax-only simulation Figure 3: The simulated results predict the outcome of the Grodner et al. [sent-240, score-0.241]
</p><p>77 In this experiment, the pattern of simulated results in Figure 3b showed a much closer resemblance to the experimental results in Figure 3a than the garden path experiment. [sent-246, score-0.485]
</p><p>78 There is a main effect of context, which is much stronger in the restrictive relative case compared to nonrestrictive relatives. [sent-247, score-0.351]
</p><p>79 As with the garden path experiment, the ANOVA reported that all effects were significant at the p < 0. [sent-248, score-0.447]
</p><p>80 3  Discussion  We have shown that our incremental sentence processor augmented with discourse processing can successfully simulate syntax-discourse interaction effects which have been shown in the literature. [sent-255, score-0.571]
</p><p>81 In a weaker sense, even a pipeline architecture where the discourse can influence syntactic probabilities could be claimed to be a Strongly Interactive model. [sent-257, score-0.384]
</p><p>82 Unlike Altmann and Steedman, who posited that the discourse processor actually removes parsing hypotheses, we were able to simulate this pruning behaviour by simply re-weighting parses in our coreference and pragmatics modules. [sent-259, score-0.828]
</p><p>83 The fact that a Weakly Interactive system can simulate the result of an experiment proposed in  ××  support of the Strongly Interactive hypothesis is initially counter-intuitive. [sent-260, score-0.202]
</p><p>84 model: a lower probability, even in an unambiguous structure, is associated with increased reading difficulty. [sent-281, score-0.284]
</p><p>85 In the restrictive relative clause condition, even though there was not any competition between a relative and main clause reading, our n best list was at all times filled with analyses. [sent-284, score-0.697]
</p><p>86 For example, on the word ‘who’ in the restricted  relative clause condition, the parser is already predicting both the subject-relative (‘the postman who was bit by the dog’) and object-relative (‘the postman who the dog bit’) readings. [sent-285, score-0.601]
</p><p>87 Overall, these results are supportive of the growing importance of probabilistic reasoning as a model of human cognitive behaviour. [sent-286, score-0.287]
</p><p>88 We note that Surprisal does indeed show processing difficulty on the word ‘by’ in the garden path experiment. [sent-288, score-0.462]
</p><p>89 However, Figure 4 (which shows the top three parses on the word ‘by’) indicates that not only are there still main clause interpretations present, but in fact, the top two parses are main clause interpretations. [sent-289, score-0.466]
</p><p>90 This suggests that neither Jurafsky (1996)’s notion of pruning as processing difficulty nor Crocker and Brants (2000) notion of attention shifts would correctly predict higher reading times  on a region containing the word ‘by’ . [sent-291, score-0.282]
</p><p>91 In fact, the main clause interpretation remains the highestranked interpretation until it is finally pruned at an auxiliary of the main verb of the sentence (‘The postman carried by the paramedics was having’). [sent-292, score-0.518]
</p><p>92 5 Conclusions The main result of this paper is that it is possible to produce a Surprisal-based sentence processing model which can simulate the influence of discourse on syntax in both garden path and unambiguous sentences. [sent-297, score-0.979]
</p><p>93 Computationally, the  inclusion of Markov Logic allowed the discourse module to compute well-formed coreference chains, and opens two avenues of future research. [sent-298, score-0.434]
</p><p>94 Our primary cognitive finding that our model, which assumes the Weakly Interactive hypothesis (whereby discourse is influenced by syntax in a reactive manner), is nonetheless able to simulate the experimental results of Grodner et al. [sent-301, score-0.418]
</p><p>95 Finally, we found that the attention shift  (Crocker and Brants, 2000) and pruning (Jurafsky, 1996) linking theories are unable to correctly simulate the results of the garden path experiment. [sent-304, score-0.606]
</p><p>96 On not being led down the garden path: the use of context by the psychological syntax processor. [sent-322, score-0.415]
</p><p>97 A computational model of prediction in human parsing: Unifying  locality and surprisal effects. [sent-332, score-0.214]
</p><p>98 The effect of discourse inferences on syntactic ambiguity resolution. [sent-338, score-0.32]
</p><p>99 A machine learning approach to coreference resolution of noun phrases. [sent-415, score-0.292]
</p><p>100 Syntactic ambiguity resolution in discourse: Modeling the effects of referential context and lexical frequency. [sent-422, score-0.231]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('garden', 0.297), ('grodner', 0.283), ('coref', 0.283), ('surprisal', 0.214), ('coreference', 0.208), ('discourse', 0.189), ('supportive', 0.168), ('interactive', 0.164), ('restrictive', 0.161), ('reading', 0.157), ('postman', 0.152), ('clause', 0.149), ('processor', 0.138), ('unambiguous', 0.127), ('pragmatics', 0.127), ('crocker', 0.114), ('path', 0.111), ('weakly', 0.103), ('relative', 0.098), ('paramedics', 0.095), ('pparser', 0.095), ('unsupportive', 0.095), ('logic', 0.088), ('simulation', 0.088), ('simulate', 0.087), ('resolution', 0.084), ('critics', 0.083), ('director', 0.081), ('simulated', 0.077), ('experiment', 0.076), ('dubey', 0.076), ('npbase', 0.076), ('unsupported', 0.076), ('prefix', 0.075), ('strongly', 0.069), ('probabilistic', 0.068), ('praised', 0.067), ('context', 0.066), ('whereby', 0.065), ('brants', 0.064), ('predicates', 0.063), ('earley', 0.062), ('hale', 0.058), ('markov', 0.057), ('mentions', 0.057), ('cognition', 0.057), ('pronoun', 0.057), ('altmann', 0.057), ('banquet', 0.057), ('pprag', 0.057), ('spivey', 0.057), ('levy', 0.056), ('items', 0.056), ('difficulty', 0.054), ('syntax', 0.052), ('cognitive', 0.051), ('parser', 0.05), ('effect', 0.05), ('np', 0.05), ('steedman', 0.049), ('amit', 0.046), ('mln', 0.046), ('incremental', 0.045), ('psycholinguistic', 0.045), ('probabilities', 0.044), ('richardson', 0.043), ('parses', 0.042), ('ambiguity', 0.042), ('main', 0.042), ('claimed', 0.041), ('frank', 0.04), ('shift', 0.04), ('effects', 0.039), ('syntactic', 0.039), ('hypothesis', 0.039), ('evidence', 0.039), ('ccoorreeff', 0.038), ('crain', 0.038), ('dependants', 0.038), ('facilitation', 0.038), ('haviland', 0.038), ('pcoref', 0.038), ('postmen', 0.038), ('samenumber', 0.038), ('sameperson', 0.038), ('spillover', 0.038), ('talent', 0.038), ('thenpp', 0.038), ('sentence', 0.038), ('film', 0.037), ('pruning', 0.037), ('reduced', 0.037), ('compute', 0.037), ('influence', 0.036), ('pipeline', 0.035), ('interaction', 0.035), ('conditions', 0.034), ('attention', 0.034), ('jurafsky', 0.034), ('domingos', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="229-tfidf-1" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>Author: Amit Dubey</p><p>Abstract: Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. Our simulations show that a Weakly Interactive cognitive architecture can explain data which had been provided as evidence for the Strongly Interactive hypothesis.</p><p>2 0.29390207 <a title="229-tfidf-2" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>Author: Frank Keller</p><p>Abstract: We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, specifically selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed.</p><p>3 0.25396737 <a title="229-tfidf-3" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>4 0.21559237 <a title="229-tfidf-4" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>5 0.17265211 <a title="229-tfidf-5" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>6 0.17058277 <a title="229-tfidf-6" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>7 0.1556976 <a title="229-tfidf-7" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>8 0.15311755 <a title="229-tfidf-8" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>9 0.15161325 <a title="229-tfidf-9" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>10 0.12718035 <a title="229-tfidf-10" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>11 0.12159716 <a title="229-tfidf-11" href="./acl-2010-Discourse_Structure%3A_Theory%2C_Practice_and_Use.html">86 acl-2010-Discourse Structure: Theory, Practice and Use</a></p>
<p>12 0.11270616 <a title="229-tfidf-12" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>13 0.097453684 <a title="229-tfidf-13" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>14 0.095763773 <a title="229-tfidf-14" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>15 0.093053363 <a title="229-tfidf-15" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>16 0.086308591 <a title="229-tfidf-16" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>17 0.085131928 <a title="229-tfidf-17" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>18 0.07892888 <a title="229-tfidf-18" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>19 0.073243774 <a title="229-tfidf-19" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>20 0.072508395 <a title="229-tfidf-20" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.209), (1, 0.094), (2, 0.05), (3, -0.232), (4, -0.149), (5, 0.185), (6, 0.0), (7, -0.012), (8, 0.072), (9, 0.044), (10, -0.025), (11, 0.019), (12, 0.171), (13, 0.211), (14, -0.145), (15, 0.236), (16, 0.032), (17, -0.009), (18, -0.086), (19, 0.029), (20, -0.119), (21, 0.034), (22, 0.014), (23, 0.064), (24, 0.014), (25, 0.06), (26, -0.034), (27, -0.122), (28, -0.061), (29, 0.116), (30, 0.032), (31, 0.017), (32, -0.007), (33, -0.04), (34, 0.097), (35, -0.043), (36, -0.006), (37, 0.06), (38, 0.028), (39, -0.012), (40, -0.026), (41, -0.051), (42, -0.01), (43, 0.045), (44, 0.035), (45, -0.078), (46, -0.024), (47, 0.018), (48, -0.003), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95722944 <a title="229-lsi-1" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>Author: Amit Dubey</p><p>Abstract: Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. Our simulations show that a Weakly Interactive cognitive architecture can explain data which had been provided as evidence for the Strongly Interactive hypothesis.</p><p>2 0.82507449 <a title="229-lsi-2" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>Author: Frank Keller</p><p>Abstract: We pose the development of cognitively plausible models of human language processing as a challenge for computational linguistics. Existing models can only deal with isolated phenomena (e.g., garden paths) on small, specifically selected data sets. The challenge is to build models that integrate multiple aspects of human language processing at the syntactic, semantic, and discourse level. Like human language processing, these models should be incremental, predictive, broad coverage, and robust to noise. This challenge can only be met if standardized data sets and evaluation measures are developed.</p><p>3 0.73836231 <a title="229-lsi-3" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>Author: Stephen Wu ; Asaf Bachrach ; Carlos Cardenas ; William Schuler</p><p>Abstract: Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.</p><p>4 0.68332976 <a title="229-lsi-4" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>Author: Jeff Mitchell ; Mirella Lapata ; Vera Demberg ; Frank Keller</p><p>Abstract: The analysis of reading times can provide insights into the processes that underlie language comprehension, with longer reading times indicating greater cognitive load. There is evidence that the language processor is highly predictive, such that prior context allows upcoming linguistic material to be anticipated. Previous work has investigated the contributions of semantic and syntactic contexts in isolation, essentially treating them as independent factors. In this paper we analyze reading times in terms of a single predictive measure which integrates a model of semantic composition with an incremental parser and a language model.</p><p>5 0.61468214 <a title="229-lsi-5" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>Author: Klinton Bicknell ; Roger Levy</p><p>Abstract: A number of results in the study of realtime sentence comprehension have been explained by computational models as resulting from the rational use of probabilistic linguistic information. Many times, these hypotheses have been tested in reading by linking predictions about relative word difficulty to word-aggregated eye tracking measures such as go-past time. In this paper, we extend these results by asking to what extent reading is well-modeled as rational behavior at a finer level of analysis, predicting not aggregate measures, but the duration and location of each fixation. We present a new rational model of eye movement control in reading, the central assumption of which is that eye move- ment decisions are made to obtain noisy visual information as the reader performs Bayesian inference on the identities of the words in the sentence. As a case study, we present two simulations demonstrating that the model gives a rational explanation for between-word regressions.</p><p>6 0.54719073 <a title="229-lsi-6" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>7 0.50936669 <a title="229-lsi-7" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>8 0.47451779 <a title="229-lsi-8" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>9 0.46788427 <a title="229-lsi-9" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>10 0.4399375 <a title="229-lsi-10" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>11 0.41451085 <a title="229-lsi-11" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>12 0.40401059 <a title="229-lsi-12" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>13 0.3936339 <a title="229-lsi-13" href="./acl-2010-Discourse_Structure%3A_Theory%2C_Practice_and_Use.html">86 acl-2010-Discourse Structure: Theory, Practice and Use</a></p>
<p>14 0.3804591 <a title="229-lsi-14" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>15 0.34549516 <a title="229-lsi-15" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>16 0.33934724 <a title="229-lsi-16" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>17 0.31340989 <a title="229-lsi-17" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>18 0.29224095 <a title="229-lsi-18" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>19 0.29020813 <a title="229-lsi-19" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>20 0.28344125 <a title="229-lsi-20" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.017), (25, 0.039), (37, 0.033), (42, 0.019), (59, 0.052), (73, 0.031), (78, 0.417), (83, 0.132), (84, 0.065), (98, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96782404 <a title="229-lda-1" href="./acl-2010-The_Importance_of_Rule_Restrictions_in_CCG.html">228 acl-2010-The Importance of Rule Restrictions in CCG</a></p>
<p>Author: Marco Kuhlmann ; Alexander Koller ; Giorgio Satta</p><p>Abstract: Combinatory Categorial Grammar (CCG) is generally construed as a fully lexicalized formalism, where all grammars use one and the same universal set of rules, and crosslinguistic variation is isolated in the lexicon. In this paper, we show that the weak generative capacity of this ‘pure’ form of CCG is strictly smaller than that of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG.</p><p>2 0.90388048 <a title="229-lda-2" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>Author: Hector-Hugo Franco-Penya</p><p>Abstract: ―Tree SRL system‖ is a Semantic Role Labelling supervised system based on a tree-distance algorithm and a simple k-NN implementation. The novelty of the system lies in comparing the sentences as tree structures with multiple relations instead of extracting vectors of features for each relation and classifying them. The system was tested with the English CoNLL-2009 shared task data set where 79% accuracy was obtained. 1</p><p>same-paper 3 0.88022405 <a title="229-lda-3" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>Author: Amit Dubey</p><p>Abstract: Probabilistic models of sentence comprehension are increasingly relevant to questions concerning human language processing. However, such models are often limited to syntactic factors. This paper introduces a novel sentence processing model that consists of a parser augmented with a probabilistic logic-based model of coreference resolution, which allows us to simulate how context interacts with syntax in a reading task. Our simulations show that a Weakly Interactive cognitive architecture can explain data which had been provided as evidence for the Strongly Interactive hypothesis.</p><p>4 0.78536773 <a title="229-lda-4" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>Author: Alan Ritter ; Mausam Mausam ; Oren Etzioni</p><p>Abstract: The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al. ’s system (Pantel et al., 2007).</p><p>5 0.71471453 <a title="229-lda-5" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>Author: Stefan Thater ; Hagen Furstenau ; Manfred Pinkal</p><p>Abstract: We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.</p><p>6 0.64873815 <a title="229-lda-6" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>7 0.64826232 <a title="229-lda-7" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>8 0.62673247 <a title="229-lda-8" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>9 0.59974283 <a title="229-lda-9" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>10 0.58876753 <a title="229-lda-10" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>11 0.5783782 <a title="229-lda-11" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>12 0.5761289 <a title="229-lda-12" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>13 0.56882054 <a title="229-lda-13" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>14 0.56664211 <a title="229-lda-14" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<p>15 0.56586403 <a title="229-lda-15" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>16 0.56546575 <a title="229-lda-16" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>17 0.56363773 <a title="229-lda-17" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>18 0.55503654 <a title="229-lda-18" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>19 0.55258316 <a title="229-lda-19" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>20 0.54370219 <a title="229-lda-20" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
