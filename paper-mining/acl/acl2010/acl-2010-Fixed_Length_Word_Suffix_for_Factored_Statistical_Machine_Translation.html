<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-119" href="#">acl2010-119</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2010-119-pdf" href="http://aclweb.org/anthology//P/P10/P10-2027.pdf">pdf</a></p><p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>Reference: <a title="acl-2010-119-reference" href="../acl2010_reference/acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. [sent-3, score-0.078]
</p><p>2 Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. [sent-4, score-0.128]
</p><p>3 However, high quality part of speech taggers are not available in open domain for many languages. [sent-5, score-0.252]
</p><p>4 In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC  English to Iraqi system. [sent-6, score-0.803]
</p><p>5 This approach is a purely lexical approach, using surface forms of the words in the parallel corpus to generate the translations and estimate probabilities. [sent-9, score-0.194]
</p><p>6 It is possible to incorporate syntactical information into this framework through different ways. [sent-10, score-0.033]
</p><p>7 Source side syntax based re-ordering as preprocessing step, dependency based reordering models, cohesive decoding features are among many available successful attempts for the integration of syntax into the translation model. [sent-11, score-0.33]
</p><p>8 Factored translation modeling is another way to achieve this goal. [sent-12, score-0.16]
</p><p>9 These models allow each word to be represented as a vector of factors rather than a single surface form. [sent-13, score-0.441]
</p><p>10 Any factors such as word stems, gender, part of speech, tense, etc. [sent-15, score-0.299]
</p><p>11 edu Previous work in factored translation modeling have reported consistent improvements from Part of Speech(POS) tags, morphology, gender, and case factors (Koehn et. [sent-20, score-0.903]
</p><p>12 Creating the factors is done as a preprocessing step, and so far, most of the experiments have assumed existence of external tools for the creation of these factors (i. [sent-26, score-0.507]
</p><p>13 POS tags, CCG supertags, etc) have been very frequently used as factors in many applications including MT, simpler representations have  also been effective in achieving the same result in other application areas. [sent-33, score-0.29]
</p><p>14 2008, were able to use fixed length suffixes as features for training a POS tagging. [sent-36, score-0.274]
</p><p>15 In another work Saberi and Perrot 1999 showed that reversing middle chunks of the words while keeping the first and last part intact, does not decrease listeners’ recognition ability. [sent-37, score-0.128]
</p><p>16 This result is very relevant to Machine Translation, suggesting that inaccurate context which is usually modeled with n-gram language models, can still be as effective as accurate surface forms. [sent-38, score-0.229]
</p><p>17 Another research (Rawlinson 1997) confirms this finding; this time in textual domain, observing that randomization of letters in the middle of words has little or no effect on the ability of skilled readers to understand the text. [sent-39, score-0.156]
</p><p>18 These results suggest that the inexpensive representational factors which do not need unavailable tools might also be worth investigating. [sent-40, score-0.339]
</p><p>19 These results encouraged us to introduce language independent simple factors for machine translation. [sent-41, score-0.208]
</p><p>20 we used fixed length suf147  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-44, score-0.142]
</p><p>21 c C2o0n1f0er Aenscseoc Sihatoirotn P faopre Crso,m papguetsat 1io4n7a–l1 L5i0n,guistics fix as word factor, to lower the perplexity of the language model, and have the factors roughly function as part of speech tags, thus increasing the grammaticality of the translation results. [sent-46, score-0.654]
</p><p>22 We were able to obtain consistent, significant improvements over our baseline in 3 different experiments, large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. [sent-47, score-0.202]
</p><p>23 2  Factored Translation Model  Statistical Machine Translation uses the log linear combination of a number of features, to compute the highest probable hypothesis as the translation. [sent-51, score-0.078]
</p><p>24 The model can then extend the definition of each of the features from a uni-dimensional value to an arbitrary joint and conditional combination of features. [sent-56, score-0.039]
</p><p>25 The factored features are defined as an extension of phrase translation features. [sent-58, score-0.708]
</p><p>26 The function τ(fj,ej), which was defined for a phrase pair before, can now be extended as a log linear combination Σf τf(fjf,ejf). [sent-59, score-0.137]
</p><p>27 The model also allows for a generation feature, defining the relationship between final surface form and target factors. [sent-60, score-0.343]
</p><p>28 Other features include additional language model fea-  tures over individual factors, and factored reordering features. [sent-61, score-0.626]
</p><p>29 Figure 1 shows an example of a possible factored model. [sent-62, score-0.452]
</p><p>30 Figure1:AnexampleofaFactoredTranslationand Generation Model  In this particular model, words on both source and target side are represented as a vector of four factors: surface form, lemma, part of speech (POS) and the morphology. [sent-63, score-0.466]
</p><p>31 The target phrase is generated as follows: Source word lemma generates target word lemma. [sent-64, score-0.367]
</p><p>32 Source word's Part of speech and morphology together generate the target word's part of speech and morphology, and from its lemma, part of speech and morphology the surface form of the target word is finally gen-  erated. [sent-65, score-0.964]
</p><p>33 This model has been able to result in higher translation BLEU score as well as grammatical coherency for English to German, English to Spanish, English to Czech, English to Chinese, Chinese to English and German to English. [sent-66, score-0.234]
</p><p>34 3  Fixed Length Suffix Factors for Factored Translation Modeling  Part of speech tagging, constituent and dependency parsing, combinatory categorical grammar super tagging are used extensively in most applications when syntactic representations are needed. [sent-67, score-0.158]
</p><p>35 However training these tools require medium size treebanks and tagged data, which for most languages will not be available for a while. [sent-68, score-0.173]
</p><p>36 On the other hand, many simple words features, such as their character n-grams, have in fact proven to be comparably as effective in many applications. [sent-69, score-0.144]
</p><p>37 2008) did an experiment on text classification on noisy data, and compared several word representations. [sent-72, score-0.039]
</p><p>38 They compared surface form, stemmed words, character n-grams, and  semantic relationships, and found that for noisy and open domain text, character-ngrams outperform other representations when used for text classification. [sent-73, score-0.358]
</p><p>39 In another work (Dincer et al 2009) showed that using fixed length word ending outperforms whole word representation for training a part of speech tagger for Turkish language. [sent-74, score-0.416]
</p><p>40 148  Based on this result, we proposed a suffix factored model for translation, which is shown in Figure 2. [sent-75, score-0.842]
</p><p>41 Source  Target  LM  Wor Fdigure2:SufixFSWauocfrditoxredmoSWeulof:rdSixLoaunrgcueagweoMrdo del -  termines factor vectors (target word, target word suffix) and each factor will be associated with its language model. [sent-76, score-0.324]
</p><p>42 Based on this model, the final probability of the translation hypothesis will be the log linear combination of phrase probabilities, reordering model probabilities, and each of the language models’ probabilities. [sent-77, score-0.503]
</p><p>43 P(e|f) ~ plm-word(eword)* plm-suffix(esuffix) * Σi=1n p(eword-j & esuffix-j|fj) * Σi=1n p(fj | eword-j & esuffix-j) Where plm-word is the n-gram language model probability over the word surface sequence, with the language model built from the surface forms. [sent-78, score-0.597]
</p><p>44 Similarly, plm-suffix(esuffix) is the language model probability over suffix sequences. [sent-79, score-0.422]
</p><p>45 p(eword-j & esuffix-j|fj) and p(fj | eword-j & esuffix-j) are translation probabilities for each phrase pair i , used in by the decoder. [sent-80, score-0.304]
</p><p>46 This probability is estimated after the phrase extraction step which is based on grow-diag heuristic at this stage. [sent-81, score-0.128]
</p><p>47 4  Experiments and Results  We used Moses implementation of the factored model for training the feature weights, and SRI  toolkit for building n-gram language models. [sent-82, score-0.491]
</p><p>48 The baseline for all systems included the moses system with lexicalized re-ordering, SRI 5-gram language models. [sent-83, score-0.163]
</p><p>49 1  Small System from Dialog Domain: English to Iraqi  This system was TRANSTAC system, which was built on about 650K sentence pairs with the average sentence length of 5. [sent-85, score-0.178]
</p><p>50 After choosing length 3 for suffixes, we built a new parallel corpus, and SRI 5-gram language models for each factor. [sent-87, score-0.13]
</p><p>51 Vocabulary size for the surface form was 110K whereas the word suffixes had about 8K distinct words. [sent-88, score-0.365]
</p><p>52 Table 1 shows the result (BLEU Score) of the system compared to the baseline. [sent-89, score-0.048]
</p><p>53 As you can see, this improvement is consistent over multiple unseen datasets. [sent-91, score-0.04]
</p><p>54 Arabic cases and numbers show up as the word suffix. [sent-92, score-0.039]
</p><p>55 Also, verb numbers usually appear partly as word suffix and in some cases as word prefix. [sent-93, score-0.429]
</p><p>56 Defining a language model over the word endings increases the probability of sequences which have this case and number agreement, favoring correct agreements over the incorrect ones. [sent-94, score-0.277]
</p><p>57 2  Medium System on Travel Domain: Spanish to English  This system is the WMT08 system, on a corpus  of 1. [sent-96, score-0.048]
</p><p>58 Like the previous experiment, we defined the 3 character suffix of the words as the second factor, and built the language model and reordering model on the joint event of (surface, suffix) pairs. [sent-99, score-0.698]
</p><p>59 The system had about 97K distinct vocabulary in the surface language model, which was reduced to 8K using the suffix corpus. [sent-101, score-0.625]
</p><p>60 Having defined the baseline, the system results are as follows. [sent-102, score-0.048]
</p><p>61 Here, we see improvement with the suffix factors compared to the baseline system. [sent-108, score-0.601]
</p><p>62 Word endings in English language are major indicators of  word’s part of speech in the sentence. [sent-109, score-0.262]
</p><p>63 Having a language model on these suffixes pushes the common patterns of these suffixes to the top, making the more grammatically coherent sentences to achieve a better probability. [sent-111, score-0.303]
</p><p>64 3  Large NIST 2009 System: Arabic to English  We used NIST2009 system as our baseline in this experiment. [sent-113, score-0.09]
</p><p>65 8 Million sentence pairs, with average sentence length of 33. [sent-115, score-0.07]
</p><p>66 As before we defined 3 character long word endings, and built 5-gram SRI language models for each factor. [sent-118, score-0.173]
</p><p>67 This result confirms the positive effect of the suffix factors even on large systems. [sent-121, score-0.604]
</p><p>68 As mentioned before we believe that this result is due to the ability of the suffix to reduce the word into a very simple but rough grammatical representation. [sent-122, score-0.45]
</p><p>69 Defining language models for this factor forces the decoder to prefer sentences with more probable suffix sequences, which is believed to increase the grammaticality of the result. [sent-123, score-0.583]
</p><p>70 Future error analysis will show us more insight of the exact effect of this factor on the outcome. [sent-124, score-0.111]
</p><p>71 5  Conclusion  In this paper we introduced a simple yet very effective factor: fixed length word suffix, to use in Factored Translation Models. [sent-125, score-0.216]
</p><p>72 This simple factor has been shown to be effective as a rough replacement for part of speech. [sent-126, score-0.258]
</p><p>73 We tested our factors in three experiments in a small, English to Iraqi system, a medium sized system of Spanish to English, and a large system, NIST09 Arabic to English. [sent-127, score-0.41]
</p><p>74 We observed consistent and significant improvements over the baseline. [sent-128, score-0.083]
</p><p>75 This result, obtained from the language independent and inexpensive factor, shows promising new opportunities for all language pairs. [sent-129, score-0.11]
</p><p>76 A machine learning experiment to determine part of speech from wordendings, Lecture Notes in Computer Science, Communications Session 6B Learning and Discovery Systems, 1997. [sent-144, score-0.163]
</p><p>77 , The significance of letter position in word recognition, PhD Thesis, Psychology Department, University of Nottingham, Nottingham UK, 1976. [sent-154, score-0.039]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('factored', 0.452), ('suffix', 0.351), ('factors', 0.208), ('surface', 0.194), ('iraqi', 0.163), ('translation', 0.16), ('dincer', 0.139), ('reordering', 0.135), ('suffixes', 0.132), ('transtac', 0.122), ('medium', 0.117), ('fj', 0.116), ('speech', 0.111), ('factor', 0.111), ('smt', 0.106), ('ccg', 0.103), ('sri', 0.103), ('spanish', 0.102), ('arabic', 0.1), ('endings', 0.099), ('phrase', 0.096), ('argmaxe', 0.093), ('esuffix', 0.093), ('keikha', 0.093), ('nottingham', 0.093), ('perrot', 0.093), ('rawlinson', 0.093), ('razavian', 0.093), ('saberi', 0.093), ('sharif', 0.093), ('universiy', 0.093), ('grammaticality', 0.084), ('morphology', 0.084), ('inexpensive', 0.075), ('character', 0.074), ('fixed', 0.072), ('wmt', 0.07), ('length', 0.07), ('lemma', 0.067), ('english', 0.067), ('supertags', 0.066), ('target', 0.063), ('koehn', 0.062), ('rough', 0.06), ('carnegie', 0.06), ('mellon', 0.06), ('built', 0.06), ('tools', 0.056), ('tem', 0.053), ('part', 0.052), ('gender', 0.052), ('turkish', 0.052), ('nist', 0.049), ('probabilities', 0.048), ('system', 0.048), ('representations', 0.047), ('defining', 0.047), ('source', 0.046), ('taggers', 0.046), ('confirms', 0.045), ('tags', 0.044), ('domain', 0.043), ('pittsburgh', 0.043), ('improvements', 0.043), ('baseline', 0.042), ('birch', 0.041), ('reversing', 0.041), ('skilled', 0.041), ('eword', 0.041), ('wor', 0.041), ('log', 0.041), ('consistent', 0.04), ('word', 0.039), ('old', 0.039), ('moses', 0.039), ('model', 0.039), ('london', 0.039), ('intact', 0.037), ('plm', 0.037), ('sized', 0.037), ('probable', 0.037), ('pos', 0.036), ('preprocessing', 0.035), ('effective', 0.035), ('identifiable', 0.035), ('favoring', 0.035), ('opportunities', 0.035), ('randomization', 0.035), ('comparably', 0.035), ('coherency', 0.035), ('middle', 0.035), ('lexicalized', 0.034), ('tagger', 0.033), ('stemmer', 0.033), ('syntactical', 0.033), ('agreements', 0.033), ('bleu', 0.033), ('vocabulary', 0.032), ('probability', 0.032), ('listeners', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="119-tfidf-1" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>2 0.18327378 <a title="119-tfidf-2" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>3 0.17032297 <a title="119-tfidf-3" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>4 0.16287293 <a title="119-tfidf-4" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>Author: Joern Wuebker ; Arne Mauser ; Hermann Ney</p><p>Abstract: Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overfitting. We describe a novel leavingone-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering mod- els in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.</p><p>5 0.15080123 <a title="119-tfidf-5" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>6 0.13830726 <a title="119-tfidf-6" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>7 0.13797095 <a title="119-tfidf-7" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>8 0.1097034 <a title="119-tfidf-8" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>9 0.10893629 <a title="119-tfidf-9" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>10 0.10362418 <a title="119-tfidf-10" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>11 0.10111413 <a title="119-tfidf-11" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>12 0.09577591 <a title="119-tfidf-12" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>13 0.092163049 <a title="119-tfidf-13" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>14 0.090638749 <a title="119-tfidf-14" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>15 0.088639677 <a title="119-tfidf-15" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>16 0.085516214 <a title="119-tfidf-16" href="./acl-2010-The_Importance_of_Rule_Restrictions_in_CCG.html">228 acl-2010-The Importance of Rule Restrictions in CCG</a></p>
<p>17 0.085432515 <a title="119-tfidf-17" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>18 0.083663985 <a title="119-tfidf-18" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>19 0.08177682 <a title="119-tfidf-19" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>20 0.079405218 <a title="119-tfidf-20" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.221), (1, -0.167), (2, -0.037), (3, -0.009), (4, 0.026), (5, -0.011), (6, -0.026), (7, -0.037), (8, 0.033), (9, 0.097), (10, 0.2), (11, 0.198), (12, 0.072), (13, 0.034), (14, -0.056), (15, -0.005), (16, -0.033), (17, 0.123), (18, 0.024), (19, -0.058), (20, -0.048), (21, -0.03), (22, -0.023), (23, 0.025), (24, 0.08), (25, 0.077), (26, -0.111), (27, -0.064), (28, 0.007), (29, -0.021), (30, 0.015), (31, 0.069), (32, -0.059), (33, -0.008), (34, 0.026), (35, 0.042), (36, -0.019), (37, 0.025), (38, 0.001), (39, 0.088), (40, 0.046), (41, -0.046), (42, 0.063), (43, -0.018), (44, -0.078), (45, -0.052), (46, 0.044), (47, 0.04), (48, -0.037), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93244672 <a title="119-lsi-1" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>2 0.76863915 <a title="119-lsi-2" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>3 0.74311209 <a title="119-lsi-3" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>Author: Jinsong Su ; Yang Liu ; Yajuan Lv ; Haitao Mi ; Qun Liu</p><p>Abstract: Lexicalized reordering models play a crucial role in phrase-based translation systems. They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1</p><p>4 0.73043096 <a title="119-lsi-4" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>5 0.6830219 <a title="119-lsi-5" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>6 0.61776221 <a title="119-lsi-6" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>7 0.59402674 <a title="119-lsi-7" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>8 0.50130314 <a title="119-lsi-8" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>9 0.48943216 <a title="119-lsi-9" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>10 0.48661575 <a title="119-lsi-10" href="./acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</a></p>
<p>11 0.48376563 <a title="119-lsi-11" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>12 0.4824585 <a title="119-lsi-12" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>13 0.47924232 <a title="119-lsi-13" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>14 0.47441038 <a title="119-lsi-14" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>15 0.46901536 <a title="119-lsi-15" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>16 0.46749583 <a title="119-lsi-16" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>17 0.46431527 <a title="119-lsi-17" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>18 0.46189579 <a title="119-lsi-18" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>19 0.4512929 <a title="119-lsi-19" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>20 0.45069304 <a title="119-lsi-20" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.012), (16, 0.015), (25, 0.041), (59, 0.098), (73, 0.042), (78, 0.035), (80, 0.335), (83, 0.096), (84, 0.039), (98, 0.188)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89418066 <a title="119-lda-1" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>2 0.81295347 <a title="119-lda-2" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>Author: Rasoul Samad Zadeh Kaljahi</p><p>Abstract: Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1</p><p>same-paper 3 0.80751532 <a title="119-lda-3" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>4 0.79249674 <a title="119-lda-4" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>5 0.64770544 <a title="119-lda-5" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>Author: Roni Ben Aharon ; Idan Szpektor ; Ido Dagan</p><p>Abstract: Idan Szpektor Ido Dagan Yahoo! Research Department of Computer Science Haifa, Israel Bar-Ilan University idan @ yahoo- inc .com Ramat Gan, Israel dagan @ c s .biu . ac . i l FrameNet is a manually constructed database based on Frame Semantics. It models the semantic Many NLP tasks need accurate knowledge for semantic inference. To this end, mostly WordNet is utilized. Yet WordNet is limited, especially for inference be- tween predicates. To help filling this gap, we present an algorithm that generates inference rules between predicates from FrameNet. Our experiment shows that the novel resource is effective and complements WordNet in terms of rule coverage.</p><p>6 0.6281054 <a title="119-lda-6" href="./acl-2010-Global_Learning_of_Focused_Entailment_Graphs.html">127 acl-2010-Global Learning of Focused Entailment Graphs</a></p>
<p>7 0.6127938 <a title="119-lda-7" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>8 0.60589206 <a title="119-lda-8" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>9 0.60527968 <a title="119-lda-9" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>10 0.60516012 <a title="119-lda-10" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>11 0.60415578 <a title="119-lda-11" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>12 0.60222936 <a title="119-lda-12" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>13 0.59981936 <a title="119-lda-13" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>14 0.59802747 <a title="119-lda-14" href="./acl-2010-Finding_Cognate_Groups_Using_Phylogenies.html">116 acl-2010-Finding Cognate Groups Using Phylogenies</a></p>
<p>15 0.59795785 <a title="119-lda-15" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>16 0.59765959 <a title="119-lda-16" href="./acl-2010-An_Open-Source_Package_for_Recognizing_Textual_Entailment.html">30 acl-2010-An Open-Source Package for Recognizing Textual Entailment</a></p>
<p>17 0.59557712 <a title="119-lda-17" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>18 0.59342527 <a title="119-lda-18" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>19 0.59283686 <a title="119-lda-19" href="./acl-2010-A_Bayesian_Method_for_Robust_Estimation_of_Distributional_Similarities.html">3 acl-2010-A Bayesian Method for Robust Estimation of Distributional Similarities</a></p>
<p>20 0.59154326 <a title="119-lda-20" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
