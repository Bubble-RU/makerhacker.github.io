<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 acl-2010-Models of Metaphor in NLP</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-175" href="#">acl2010-175</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 acl-2010-Models of Metaphor in NLP</h1>
<br/><p>Source: <a title="acl-2010-175-pdf" href="http://aclweb.org/anthology//P/P10/P10-1071.pdf">pdf</a></p><p>Author: Ekaterina Shutova</p><p>Abstract: Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). Both of them have been repeatedly addressed in NLP. This paper is the first comprehensive and systematic review of the existing computational models of metaphor, the issues of metaphor annotation in corpora and the available resources.</p><p>Reference: <a title="acl-2010-175-reference" href="../acl2010_reference/acl-2010-Models_of_Metaphor_in_NLP_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). [sent-6, score-3.349]
</p><p>2 This paper is the first comprehensive and systematic review of the existing computational models of metaphor, the issues of metaphor annotation in corpora and the available resources. [sent-8, score-0.804]
</p><p>3 However, recent work on lexical semantics and lexical acquisition techniques opens many new avenues for creation of fully automated models for recognition and interpretation of figurative language. [sent-20, score-0.125]
</p><p>4 In this paper Iwill focus on the phenomenon of metaphor and describe the most prominent computational approaches to metaphor, as well the issues of resource creation and metaphor annotation. [sent-21, score-1.559]
</p><p>5 1 In metaphorical expressions seemingly unrelated features of one concept are associated with another concept. [sent-30, score-0.443]
</p><p>6 The use of metaphor is ubiquitous in natural language text and it is a serious bottleneck in automatic text understanding. [sent-35, score-0.763]
</p><p>7 They manually annotated metaphorical expressions in this data and found that 241 out of 761 sentences contained a metaphor. [sent-40, score-0.443]
</p><p>8 Due to such a high frequency of their use, a system capable of recognizing and interpreting metaphorical expressions in unrestricted text would become an invaluable component of any semantics-oriented NLP application. [sent-41, score-0.443]
</p><p>9 Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). [sent-42, score-3.349]
</p><p>10 All of these approaches share the idea of an interconceptual mapping that underlies the production of metaphorical expressions. [sent-45, score-0.41]
</p><p>11 In other words, metaphor always involves two concepts or conceptual domains: the target (also called topic or tenor in the linguistics literature) and the source (or vehicle). [sent-46, score-0.902]
</p><p>12 can  However, Lakoff and Johnson do not discuss how metaphors can be recognized in the linguistic data, which is the primary task in the automatic processing of metaphor. [sent-54, score-0.252]
</p><p>13 Although humans are highly capable of producing and comprehending metaphorical expressions, the task of distinguishing between literal and non-literal meanings and, therefore, identifying metaphor in text appears to be challenging. [sent-55, score-1.252]
</p><p>14 Gibbs (1984) suggests that literal and figurative meanings are situated at the ends of a single continuum, along which metaphoricity and idiomaticity are spread. [sent-57, score-0.166]
</p><p>15 This makes demarcation of metaphorical and literal language fuzzy. [sent-58, score-0.489]
</p><p>16 So far, the most influential account of metaphor recognition is that of Wilks (1978). [sent-59, score-0.783]
</p><p>17 According to Wilks, metaphors represent a violation of selectional restrictions in a given context. [sent-60, score-0.394]
</p><p>18 Therefore, drink taking a car as a subject is an anomaly, which may in turn indicate the metaphorical use of drink. [sent-65, score-0.387]
</p><p>19 3  Automatic Metaphor Recognition  One of the first attempts to identify and interpret metaphorical expressions in text automatically is the approach of Fass (1991). [sent-66, score-0.443]
</p><p>20 Fass (1991) developed a system called met*, capable of discriminating between literalness, metonymy, metaphor and anomaly. [sent-68, score-0.763]
</p><p>21 First, literalness is distinguished from non-literalness using selectional preference violation as an indicator. [sent-70, score-0.201]
</p><p>22 If the system fails to recognize metonymy, it proceeds to search the knowledge base for a relevant analogy in order to discriminate metaphorical relations from anomalous ones. [sent-72, score-0.41]
</p><p>23 met* then searches its knowledge base for a triple containing a hypernym of both the actual argument and the desired argument and finds (thing,use,energy source), which represents the metaphorical interpretation. [sent-76, score-0.407]
</p><p>24 However, Fass himself indicated a problem with the selectional preference violation approach applied to metaphor recognition. [sent-77, score-0.942]
</p><p>25 Another problem with this approach arises from the high conventionality of metaphor in language. [sent-82, score-0.763]
</p><p>26 This means that some metaphorical senses are very common. [sent-83, score-0.419]
</p><p>27 As a result the system would extract selectional preference distributions skewed towards such conventional metaphorical senses of the verb or one of its arguments. [sent-84, score-0.592]
</p><p>28 Therefore, although some expressions may be fully metaphorical in nature, no selectional preference violation can be detected in their use. [sent-85, score-0.622]
</p><p>29 the phrase all men are animals can be used metaphorically, however, without any violation of selectional restrictions. [sent-88, score-0.142]
</p><p>30 Goatly (1997) addresses the phenomenon of metaphor by identifying a set of linguistic cues indicating it. [sent-89, score-0.796]
</p><p>31 He gives examples of lexical patterns indicating the presence of a metaphorical expression, such as metaphorically speaking, utterly, completely, so to speak and, surprisingly, literally. [sent-90, score-0.459]
</p><p>32 Such cues would probably not be enough for  metaphor extraction on their own, but could contribute to a more complex system. [sent-91, score-0.763]
</p><p>33 They mine WordNet (Fellbaum, 1998) for the examples of systematic polysemy, which allows to capture metonymic and metaphorical relations. [sent-93, score-0.473]
</p><p>34 The CorMet system discussed in (Mason, 2004) is the first attempt to discover source-target domain mappings automatically. [sent-100, score-0.113]
</p><p>35 In the LAB –  –  domain pour has a strong selectional preference for objects of type liquid, whereas in the FINANCE domain it selects for money. [sent-103, score-0.214]
</p><p>36 (2006) focus only on metaphors expressed by a verb. [sent-128, score-0.252]
</p><p>37 They use hyponymy relation in WordNet and word bigram counts to predict metaphors at a sentence level. [sent-130, score-0.252]
</p><p>38 Along with this they consider expressions containing a verb or an adjective used metaphorically (e. [sent-134, score-0.168]
</p><p>39 This idea is a modification of the selectional preference view of Wilks. [sent-139, score-0.12]
</p><p>40 , 1991), whereby highly conventionalized metaphors (they call them dead metaphors) are taken to be negative examples. [sent-142, score-0.29]
</p><p>41 Thus they do not deal with literal examples as such: essentially, the distinction they are making is between the senses included in WordNet, even if they are conventional metaphors, and those not included in WordNet. [sent-143, score-0.167]
</p><p>42 The idea behind this is that the more specific conventional metaphors descend from the general ones. [sent-146, score-0.285]
</p><p>43 3William Shakespeare Given an example of a metaphorical expression, MIDAS searches its database for a corresponding metaphor that would explain the anomaly. [sent-147, score-1.15]
</p><p>44 If it is not able to, it calls MIDAS which detects metaphorical expressions via selectional preference violation and searches its database for a metaphor explaining the anomaly in the question. [sent-153, score-1.414]
</p><p>45 Another cohort of approaches relies on per-  forming inferences about entities and events in the source and target domains for metaphor interpretation. [sent-154, score-0.863]
</p><p>46 The results are then projected onto the target domain using the conceptual mapping representation. [sent-159, score-0.147]
</p><p>47 The ATT-Meta project concerns metaphorical and metonymic description of mental states and reasoning about mental states using first order logic. [sent-160, score-0.533]
</p><p>48 Veale and Hao (2008) derive a “fluid knowl-  edge representation for metaphor interpretation and generation”, called Talking Points. [sent-163, score-0.804]
</p><p>49 Below is an example demonstrating how slippage operates to explain the metaphor Make-up is a Western burqa. [sent-167, score-0.785]
</p><p>50 Veale and Hao (2008), however, did not evaluate to which extent their knowledge base of Talking Points and the associated reasoning framework are useful to interpret metaphorical expressions occurring in text. [sent-169, score-0.47]
</p><p>51 Shutova (2010) defines metaphor interpretation as a paraphrasing task and presents a method for deriving literal paraphrases for metaphorical expressions from the BNC. [sent-170, score-1.349]
</p><p>52 For example, for the metaphors in “All of this stirred an unfathomable excitement in her” or “a carelessly leaked report” their system produces interpretations “All of this provoked an unfathomable excitement in her” and “a carelessly disclosed report” respectively. [sent-171, score-0.38]
</p><p>53 They first apply a probabilistic model to rank all possible paraphrases for the metaphorical expression  given the context; and then use automatically induced selectional preferences to discriminate between figurative and literal paraphrases. [sent-172, score-0.681]
</p><p>54 The selectional preference distribution is defined in terms of selectional association measure introduced by Resnik (1993) over the noun classes automatically produced by Sun and Korhonen (2009). [sent-173, score-0.203]
</p><p>55 Shutova (2010) tested their system only on metaphors expressed by a verb and report a paraphrasing accuracy of 0. [sent-174, score-0.272]
</p><p>56 Hence there is a need for either an extensive manually-created knowledge-base or a robust knowledge acquisition system for interpretation of metaphorical expressions. [sent-177, score-0.428]
</p><p>57 The latter being a hard task, a great deal of metaphor research resorted to the first option. [sent-178, score-0.763]
</p><p>58 Although hand-coded knowledge proved useful for metaphor interpretation (Fass, 1991 ; Martin, 1990), it should be noted that the systems utilizing it have a very limited coverage. [sent-179, score-0.804]
</p><p>59 One of the first attempts to create a multi-  purpose knowledge base of source–target domain mappings is the Master Metaphor List (Lakoff et al. [sent-180, score-0.113]
</p><p>60 It includes a classification of metaphorical mappings (mainly those related to mind, feelings and emotions) with the corresponding examples of language use. [sent-182, score-0.453]
</p><p>61 However, both the idea of the list and its actual mappings ontology inspired the creation of other metaphor resources. [sent-186, score-0.829]
</p><p>62 The MetaBank is a knowledge-base of English metaphorical conventions, represented in the form of metaphor maps (Martin, 1988) containing detailed information about source-target con-  cept mappings backed by empirical evidence. [sent-189, score-1.236]
</p><p>63 The ATT-meta project databank contains a large number of examples of metaphors of mind classified by source–target domain mappings taken from the Master Metaphor List. [sent-190, score-0.365]
</p><p>64 Along with this it is worth mentioning metaphor resources in languages other than English. [sent-191, score-0.763]
</p><p>65 There has been a wealth of research on metaphor in Spanish, Chinese, Russian, German, French and Italian. [sent-192, score-0.763]
</p><p>66 The Hamburg Metaphor Database (L ¨onneker, 2004; Reining and L ¨onneker-Rodman, 2007) contains examples of metaphorical expressions in German and French, which are mapped to senses from EuroWordNet5 and annotated with source–target domain mappings taken from the Master Metaphor List. [sent-193, score-0.588]
</p><p>67 Alonge and Castelli (2003) discuss how metaphors can be represented in ItalWordNet for 4http://www. [sent-194, score-0.252]
</p><p>68 Encoding metaphorical information in generaldomain lexical resources for English, e. [sent-206, score-0.387]
</p><p>69 Traditional approaches to metaphor annotation include manual search for lexical items used metaphorically (Pragglejaz Group, 2007), for source and target domain vocabulary (Deignan, 2006; Koivisto-Alanko and Tissari, 2006; Martin, 2006) or for linguistic markers of metaphor (Goatly, 1997). [sent-210, score-1.745]
</p><p>70 However, a corpus annotated for conceptual mappings could provide a new starting point for both linguistic and cognitive experiments. [sent-212, score-0.105]
</p><p>71 1 Metaphor and Polysemy The theorists of metaphor distinguish between two kinds of metaphorical language: novel (or poetic) metaphors, that surprise our imagination, and conventionalized metaphors, that become a part of an ordinary discourse. [sent-214, score-1.15]
</p><p>72 Following Orwell  (1946) Nunberg calls such metaphors “dead” and claims that they are not psychologically distinct from literally-used terms. [sent-217, score-0.252]
</p><p>73 This scheme demonstrates how metaphorical associations capture some generalisations governing polysemy: over time some of the aspects of the target domain are added to the meaning of a term in a source domain, resulting in a (metaphorical) sense extension of this term. [sent-218, score-0.561]
</p><p>74 Copestake and Briscoe (1995) discuss sense extension mainly based on metonymic examples and model the phenomenon using lexical rules encoding metonymic patterns. [sent-219, score-0.195]
</p><p>75 Along with this they suggest that similar mechanisms can be used to account for metaphoric processes, and the conceptual mappings encoded in the sense extension rules would define the limits to the possible shifts in meaning. [sent-220, score-0.206]
</p><p>76 However, it is often unclear if a metaphorical instance is a case of broadening of the sense in context due to general vagueness in language, or it manifests a formation of a new distinct metaphorical sense. [sent-221, score-0.802]
</p><p>77 ”6 In (8b) this sense stretches to describe dealing with software, whereby COMPUTER PROGRAMS are viewed as PHYSICAL SPACES. [sent-233, score-0.107]
</p><p>78 These two senses are clearly linked via –  the metaphoric mapping between EMOTIONAL STATES and TEMPERATURES. [sent-236, score-0.109]
</p><p>79 A number of metaphorical senses are included in WordNet, however without any accompanying semantic annotation. [sent-237, score-0.419]
</p><p>80 1 Pragglejaz Procedure Pragglejaz Group (2007) proposes a metaphor identification procedure (MIP) within the frame6Sense definitions are taken from the Oxford English Dictionary. [sent-241, score-0.763]
</p><p>81 The procedure involves metaphor annotation at the word level as opposed to identifying metaphorical relations (between words) or source– target domain mappings (between concepts or domains). [sent-243, score-1.345]
</p><p>82 2 Source Target Domain Vocabulary Another popular method that has been used to extract metaphors is searching for sentences containing lexical items from the source domain, the target domain, or both (Stefanowitsch, 2006). [sent-252, score-0.35]
</p><p>83 This method requires exhaustive lists of source and target domain vocabulary. [sent-253, score-0.125]
</p><p>84 Martin (2006) conducted a corpus study in order to confirm that metaphorical expressions –  occur in text in contexts containing such lexical items. [sent-254, score-0.463]
</p><p>85 He performed his analysis on the data from the Wall Street Journal (WSJ) corpus and focused on four conceptual metaphors that occur with considerable regularity in the corpus. [sent-255, score-0.291]
</p><p>86 Martin manually compiled the lists of terms characteristic for each domain by examining sampled metaphors of these types and then augmented them through the use of thesaurus. [sent-257, score-0.299]
</p><p>87 He then searched the WSJ for sentences containing vocabulary from these lists and checked whether they contain metaphors of the above types. [sent-258, score-0.272]
</p><p>88 The goal of this study was to evaluate predictive ability of contexts containing vocabulary from (1) source domain and (2) target domain, as well as (3) estimating the likelihood of a metaphorical expression following another metaphorical expression described by the same mapping. [sent-259, score-0.963]
</p><p>89 He obtained the most positive results for  metaphors of the type NUMERICAL-VALUEAS-LOCATION (P(Metaphor|Source) = 0. [sent-260, score-0.252]
</p><p>90 (2003) carried out a metaphor annotation experiment in the framework of the ATTMeta project. [sent-266, score-0.785]
</p><p>91 (2003) attempted to annotate the involved source target domain mappings. [sent-271, score-0.125]
</p><p>92 Shutova and Teufel (2010) adopt a different approach to the annotation of source target domain mappings. [sent-274, score-0.147]
</p><p>93 They propose a two stage procedure, whereby the metaphorical expressions are first identified using MIP, and then the source domain (where the basic sense comes from) and the target domain (the given context) are selected from the lists of categories. [sent-276, score-0.681]
</p><p>94 However, there is still a clear need in a unified metaphor annotation procedure and creation of a large publicly available metaphor corpus. [sent-284, score-1.548]
</p><p>95 Besides making our thoughts more vivid and filling our communication with richer imagery, metaphors also play an important structural role in our cognition. [sent-286, score-0.252]
</p><p>96 Thus, one of the long term goals of metaphor research in NLP and AI would be to build a computational intelligence model accounting for the way metaphors organize our conceptual system, in terms of which we think and act. [sent-287, score-1.054]
</p><p>97 met*: A method for discriminating metonymy and metaphor by computer. [sent-348, score-0.826]
</p><p>98 Sense and sensibility: Rational thought versus emotion in metaphorical language. [sent-431, score-0.387]
</p><p>99 The hamburg metaphor database project: issues in resource creation. [sent-465, score-0.783]
</p><p>100 Metaphor corpus annotated for source - target domain mappings. [sent-554, score-0.125]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('metaphor', 0.763), ('metaphorical', 0.387), ('metaphors', 0.252), ('lakoff', 0.108), ('literal', 0.102), ('selectional', 0.083), ('shutova', 0.078), ('metaphorically', 0.072), ('metonymic', 0.067), ('worn', 0.067), ('mappings', 0.066), ('figurative', 0.064), ('metonymy', 0.063), ('master', 0.061), ('violation', 0.059), ('barnden', 0.059), ('fass', 0.059), ('expressions', 0.056), ('wilks', 0.056), ('metaphoric', 0.054), ('women', 0.05), ('pragglejaz', 0.049), ('domain', 0.047), ('gries', 0.045), ('midas', 0.045), ('stefanowitsch', 0.045), ('interpretation', 0.041), ('source', 0.04), ('conceptual', 0.039), ('poetic', 0.039), ('whereby', 0.038), ('peters', 0.038), ('target', 0.038), ('preference', 0.037), ('mason', 0.036), ('martin', 0.034), ('wordnet', 0.034), ('birke', 0.034), ('agerri', 0.034), ('analogies', 0.034), ('fluid', 0.034), ('gedigan', 0.034), ('hofstadter', 0.034), ('metabank', 0.034), ('mip', 0.034), ('onneker', 0.034), ('wallington', 0.034), ('polysemy', 0.033), ('phenomenon', 0.033), ('conventional', 0.033), ('senses', 0.032), ('mouton', 0.03), ('johnson', 0.03), ('krishnakumaran', 0.029), ('veale', 0.029), ('anomaly', 0.029), ('sense', 0.028), ('talking', 0.028), ('reasoning', 0.027), ('mental', 0.026), ('narayanan', 0.025), ('cold', 0.025), ('mapping', 0.023), ('war', 0.023), ('discriminate', 0.023), ('alonge', 0.022), ('carelessly', 0.022), ('cohort', 0.022), ('cormet', 0.022), ('excitement', 0.022), ('goatly', 0.022), ('karov', 0.022), ('liquid', 0.022), ('literalness', 0.022), ('slippage', 0.022), ('tourangeau', 0.022), ('enter', 0.022), ('finance', 0.022), ('expression', 0.022), ('annotation', 0.022), ('concepts', 0.022), ('meaning', 0.021), ('met', 0.021), ('viewed', 0.021), ('recognition', 0.02), ('containing', 0.02), ('verb', 0.02), ('sarkar', 0.02), ('creative', 0.02), ('reining', 0.02), ('stretches', 0.02), ('imagination', 0.02), ('nunberg', 0.02), ('cure', 0.02), ('karma', 0.02), ('hamburg', 0.02), ('unfathomable', 0.02), ('mechanisms', 0.019), ('systematic', 0.019), ('lab', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="175-tfidf-1" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>Author: Ekaterina Shutova</p><p>Abstract: Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). Both of them have been repeatedly addressed in NLP. This paper is the first comprehensive and systematic review of the existing computational models of metaphor, the issues of metaphor annotation in corpora and the available resources.</p><p>2 0.49938086 <a title="175-tfidf-2" href="./acl-2010-A_Game-Theoretic_Model_of_Metaphorical_Bargaining.html">6 acl-2010-A Game-Theoretic Model of Metaphorical Bargaining</a></p>
<p>Author: Beata Beigman Klebanov ; Eyal Beigman</p><p>Abstract: We present a game-theoretic model of bargaining over a metaphor in the context of political communication, find its equilibrium, and use it to rationalize observed linguistic behavior. We argue that game theory is well suited for modeling discourse as a dynamic resulting from a number of conflicting pressures, and suggest applications of interest to computational linguists.</p><p>3 0.20575197 <a title="175-tfidf-3" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>Author: Daria Bogdanova</p><p>Abstract: Various text mining algorithms require the process offeature selection. High-level semantically rich features, such as figurative language uses, speech errors etc., are very promising for such problems as e.g. writing style detection, but automatic extraction of such features is a big challenge. In this paper, we propose a framework for figurative language use detection. This framework is based on the idea of sense differentiation. We describe two algorithms illustrating the mentioned idea. We show then how these algorithms work by applying them to Russian language data.</p><p>4 0.10796118 <a title="175-tfidf-4" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>Author: Linlin Li ; Benjamin Roth ; Caroline Sporleder</p><p>Abstract: This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. nonliteral usages of potentially idiomatic expressions. In all three cases, we outper- form state-of-the-art systems either quantitatively or statistically significantly.</p><p>5 0.062323358 <a title="175-tfidf-5" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>Author: Diarmuid O Seaghdha</p><p>Abstract: This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.</p><p>6 0.061968967 <a title="175-tfidf-6" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>7 0.04959023 <a title="175-tfidf-7" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>8 0.048697133 <a title="175-tfidf-8" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>9 0.042084496 <a title="175-tfidf-9" href="./acl-2010-BabelNet%3A_Building_a_Very_Large_Multilingual_Semantic_Network.html">44 acl-2010-BabelNet: Building a Very Large Multilingual Semantic Network</a></p>
<p>10 0.041856661 <a title="175-tfidf-10" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>11 0.041567072 <a title="175-tfidf-11" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<p>12 0.041556966 <a title="175-tfidf-12" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>13 0.040052433 <a title="175-tfidf-13" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>14 0.037561517 <a title="175-tfidf-14" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>15 0.037555404 <a title="175-tfidf-15" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>16 0.036252949 <a title="175-tfidf-16" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>17 0.03335163 <a title="175-tfidf-17" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>18 0.032910619 <a title="175-tfidf-18" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>19 0.031998523 <a title="175-tfidf-19" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>20 0.031968959 <a title="175-tfidf-20" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.112), (1, 0.076), (2, -0.016), (3, -0.034), (4, 0.095), (5, -0.013), (6, 0.025), (7, 0.057), (8, 0.016), (9, -0.034), (10, 0.059), (11, -0.026), (12, 0.092), (13, 0.116), (14, 0.043), (15, -0.038), (16, 0.012), (17, 0.018), (18, 0.203), (19, 0.197), (20, 0.229), (21, -0.11), (22, 0.247), (23, -0.249), (24, -0.39), (25, -0.153), (26, -0.243), (27, -0.218), (28, 0.103), (29, 0.073), (30, 0.2), (31, -0.01), (32, -0.054), (33, -0.069), (34, 0.058), (35, -0.002), (36, -0.018), (37, -0.059), (38, 0.013), (39, -0.004), (40, -0.014), (41, -0.014), (42, 0.035), (43, 0.024), (44, 0.014), (45, -0.001), (46, 0.065), (47, 0.026), (48, -0.027), (49, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95435447 <a title="175-lsi-1" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>Author: Ekaterina Shutova</p><p>Abstract: Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). Both of them have been repeatedly addressed in NLP. This paper is the first comprehensive and systematic review of the existing computational models of metaphor, the issues of metaphor annotation in corpora and the available resources.</p><p>2 0.93065226 <a title="175-lsi-2" href="./acl-2010-A_Game-Theoretic_Model_of_Metaphorical_Bargaining.html">6 acl-2010-A Game-Theoretic Model of Metaphorical Bargaining</a></p>
<p>Author: Beata Beigman Klebanov ; Eyal Beigman</p><p>Abstract: We present a game-theoretic model of bargaining over a metaphor in the context of political communication, find its equilibrium, and use it to rationalize observed linguistic behavior. We argue that game theory is well suited for modeling discourse as a dynamic resulting from a number of conflicting pressures, and suggest applications of interest to computational linguists.</p><p>3 0.47456408 <a title="175-lsi-3" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>Author: Daria Bogdanova</p><p>Abstract: Various text mining algorithms require the process offeature selection. High-level semantically rich features, such as figurative language uses, speech errors etc., are very promising for such problems as e.g. writing style detection, but automatic extraction of such features is a big challenge. In this paper, we propose a framework for figurative language use detection. This framework is based on the idea of sense differentiation. We describe two algorithms illustrating the mentioned idea. We show then how these algorithms work by applying them to Russian language data.</p><p>4 0.22966032 <a title="175-lsi-4" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>Author: Morgan Sonderegger ; Partha Niyogi</p><p>Abstract: English noun/verb (N/V) pairs (contract, cement) have undergone complex patterns of change between 3 stress patterns for several centuries. We describe a longitudinal dataset of N/V pair pronunciations, leading to a set of properties to be accounted for by any computational model. We analyze the dynamics of 5 dynamical systems models of linguistic populations, each derived from a model of learning by individuals. We compare each model’s dynamics to a set of properties observed in the N/V data, and reason about how assumptions about individual learning affect population-level dynamics.</p><p>5 0.21417843 <a title="175-lsi-5" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>Author: Linlin Li ; Benjamin Roth ; Caroline Sporleder</p><p>Abstract: This paper presents a probabilistic model for sense disambiguation which chooses the best sense based on the conditional probability of sense paraphrases given a context. We use a topic model to decompose this conditional probability into two conditional probabilities with latent variables. We propose three different instantiations of the model for solving sense disambiguation problems with different degrees of resource availability. The proposed models are tested on three different tasks: coarse-grained word sense disambiguation, fine-grained word sense disambiguation, and detection of literal vs. nonliteral usages of potentially idiomatic expressions. In all three cases, we outper- form state-of-the-art systems either quantitatively or statistically significantly.</p><p>6 0.19457558 <a title="175-lsi-6" href="./acl-2010-Expanding_Verb_Coverage_in_Cyc_with_VerbNet.html">108 acl-2010-Expanding Verb Coverage in Cyc with VerbNet</a></p>
<p>7 0.19156775 <a title="175-lsi-7" href="./acl-2010-WSD_as_a_Distributed_Constraint_Optimization_Problem.html">257 acl-2010-WSD as a Distributed Constraint Optimization Problem</a></p>
<p>8 0.17593148 <a title="175-lsi-8" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>9 0.17454934 <a title="175-lsi-9" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>10 0.16227172 <a title="175-lsi-10" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<p>11 0.16052547 <a title="175-lsi-11" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>12 0.1425851 <a title="175-lsi-12" href="./acl-2010-Comparable_Entity_Mining_from_Comparative_Questions.html">63 acl-2010-Comparable Entity Mining from Comparative Questions</a></p>
<p>13 0.1406205 <a title="175-lsi-13" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>14 0.13816835 <a title="175-lsi-14" href="./acl-2010-Correcting_Errors_in_Speech_Recognition_with_Articulatory_Dynamics.html">74 acl-2010-Correcting Errors in Speech Recognition with Articulatory Dynamics</a></p>
<p>15 0.13812399 <a title="175-lsi-15" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<p>16 0.13719197 <a title="175-lsi-16" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>17 0.13549265 <a title="175-lsi-17" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>18 0.13383752 <a title="175-lsi-18" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>19 0.12760258 <a title="175-lsi-19" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>20 0.12592451 <a title="175-lsi-20" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.017), (14, 0.017), (25, 0.064), (39, 0.013), (42, 0.035), (44, 0.015), (59, 0.062), (69, 0.011), (71, 0.012), (73, 0.086), (75, 0.268), (76, 0.013), (78, 0.045), (80, 0.018), (83, 0.072), (84, 0.047), (97, 0.013), (98, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77896881 <a title="175-lda-1" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>Author: Ekaterina Shutova</p><p>Abstract: Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). Both of them have been repeatedly addressed in NLP. This paper is the first comprehensive and systematic review of the existing computational models of metaphor, the issues of metaphor annotation in corpora and the available resources.</p><p>2 0.68296659 <a title="175-lda-2" href="./acl-2010-Collocation_Extraction_beyond_the_Independence_Assumption.html">60 acl-2010-Collocation Extraction beyond the Independence Assumption</a></p>
<p>Author: Gerlof Bouma</p><p>Abstract: In this paper we start to explore two-part collocation extraction association measures that do not estimate expected probabilities on the basis of the independence assumption. We propose two new measures based upon the well-known measures of mutual information and pointwise mutual information. Expected probabilities are derived from automatically trained Aggregate Markov Models. On three collocation gold standards, we find the new association measures vary in their effectiveness.</p><p>3 0.54427761 <a title="175-lda-3" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from Nbest lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.</p><p>4 0.5148865 <a title="175-lda-4" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>5 0.51240295 <a title="175-lda-5" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>Author: Danilo Croce ; Cristina Giannone ; Paolo Annesi ; Roberto Basili</p><p>Abstract: Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations.</p><p>6 0.50239402 <a title="175-lda-6" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>7 0.50127804 <a title="175-lda-7" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>8 0.4976874 <a title="175-lda-8" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>9 0.49603045 <a title="175-lda-9" href="./acl-2010-WebLicht%3A_Web-Based_LRT_Services_for_German.html">259 acl-2010-WebLicht: Web-Based LRT Services for German</a></p>
<p>10 0.49546778 <a title="175-lda-10" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>11 0.49195734 <a title="175-lda-11" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>12 0.49113458 <a title="175-lda-12" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>13 0.49024311 <a title="175-lda-13" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>14 0.48985994 <a title="175-lda-14" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>15 0.48969433 <a title="175-lda-15" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>16 0.4893837 <a title="175-lda-16" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>17 0.48894393 <a title="175-lda-17" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>18 0.48837996 <a title="175-lda-18" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>19 0.48832327 <a title="175-lda-19" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>20 0.48822454 <a title="175-lda-20" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
