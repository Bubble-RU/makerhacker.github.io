<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-8" href="#">acl2010-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</h1>
<br/><p>Source: <a title="acl-2010-8-pdf" href="http://aclweb.org/anthology//P/P10/P10-1084.pdf">pdf</a></p><p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>Reference: <a title="acl-2010-8-reference" href="../acl2010_reference/acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu i Abstract Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. [sent-3, score-0.326]
</p><p>2 In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. [sent-4, score-0.338]
</p><p>3 We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. [sent-5, score-0.693]
</p><p>4 Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form  a summary. [sent-6, score-0.335]
</p><p>5 1 Introduction Extractive approach to multi-document summarization (MDS) produces a summary by selecting sentences from original documents. [sent-10, score-0.519]
</p><p>6 Document Understanding Conferences (DUC), now TAC, fosters the effort on building MDS systems, which take document clusters (documents on a same topic) and description of the desired summary focus as input and output a word length limited summary. [sent-11, score-0.465]
</p><p>7 Human summaries are provided for training summarization models and measuring the performance of machine generated summaries. [sent-12, score-0.299]
</p><p>8 Extractive summarization methods can be classified into two groups: supervised methods that rely on provided document-summary pairs, and unsupervised methods based upon properties derived from document clusters. [sent-13, score-0.284]
</p><p>9 Each candidate sentence is classified as summary or non-summary based on the features that they pose and those with highest scores are selected. [sent-21, score-0.438]
</p><p>10 Such models can yield comparable or better performance on DUC and other evaluations, since representing documents as topic distributions rather than bags of words diminishes the effect of lexical variability. [sent-28, score-0.288]
</p><p>11 In this paper, we present a novel approach that formulates MDS as a prediction problem based on a two-step hybrid model: a generative model for hierarchical topic discovery and a regression model for inference. [sent-30, score-0.516]
</p><p>12 We investigate if a hierarchical model can be adopted to discover salient characteristics of sentences organized into hierarchies utilizing human generated summary text. [sent-31, score-0.607]
</p><p>13 We present a probabilistic topic model on sentence level building on hierarchical Latent Dirichlet Allocation (hLDA) (Blei et al. [sent-32, score-0.367]
</p><p>14 We construct a hybrid learning algorithm by extracting salient features to characterize summary sentences, and implement a regression model for inference (Fig. [sent-35, score-0.504]
</p><p>15 We show in § 6 that our hybrid summarizer achWieeve ssh comparable (if tno otu better) iRdO sUumGmE score on the challenging task of extracting the summaries of multiple newswire documents. [sent-43, score-0.296]
</p><p>16 While, earlier work on summarization depend on a word score function, which is used to measure sentence rank scores based on (semi-)supervised learning methods, recent trend of purely data-driven methods, (Barzilay and Lee, 2004; Daum e´III and Marcu, 2006; Tang et al. [sent-48, score-0.319]
</p><p>17 Our objective is to discover from document clusters, the latent topics that are organized into hierarchies following (Haghighi and Vanderwende, 2009). [sent-51, score-0.348]
</p><p>18 It follows that summary topics are commonly shared by many documents, while specific topics are more likely to be  mentioned in rather a small subset of documents. [sent-57, score-0.53]
</p><p>19 Feature based learning approaches to summarization methods discover salient features by measuring similarity between candidate sentences and summary sentences (Nenkova and Vanderwende, 2005; Conroy et al. [sent-58, score-0.829]
</p><p>20 Recent studies focused on the discovery of latent topics of document sets in extracting summaries. [sent-62, score-0.307]
</p><p>21 One of the challenges of using a previously trained topic model is that the new document might have a totally new vocabulary or may include many other specific topics, which may or may not exist in the trained model. [sent-64, score-0.311]
</p><p>22 An alternative yet feasible solution, presented in this work, is building a model that can summarize new document clusters using characteristics of topic distributions of training documents. [sent-66, score-0.471]
</p><p>23 Our approach differs from the early work, in that, we combine a generative hierarchical model and regression model to score sentences in new documents, eliminating the need for building a generative model for new document clusters. [sent-67, score-0.556]
</p><p>24 3  Summary-Focused Hierarchical Model  Our MDS system, hybrid hierarchical summarizer, HybHSum, is based on an hybrid learning approach to extract sentences for generating summary. [sent-68, score-0.418]
</p><p>25 We discover hidden topic distributions of sentences in a given document cluster along with provided summary sentences based on hLDA described in (Blei et al. [sent-69, score-0.935]
</p><p>26 We build a summary-focused hierarchical probabilistic topic model, sumHLDA, for each document cluster at sentence level, because it enables capturing expected topic distributions in given sentences di-  rectly from the model. [sent-71, score-0.853]
</p><p>27 Besides, document clusters contain a relatively small number of documents, which may limit the variability of topics if they are evaluated on the document level. [sent-72, score-0.477]
</p><p>28 Let a given document cluster D be represented with sentences O={om}|mO=|1 and its corresponding human summary be represented with sentences S={sn? [sent-75, score-0.659]
</p><p>29 816  Summary hLDA (sumHLDA): The hLDA represents distribution of topics in sentences by organizing topics into a tree of a fixed depth L (Fig. [sent-89, score-0.425]
</p><p>30 Each candidate sentence om is assigned to a path com in the tree and each word wi in a given sentence is assigned to a hidden topic zom at a level lof com . [sent-92, score-1.482]
</p><p>31 The sampler method  alternates between choosing a new path for each sentence through the tree and assigning each word in each sentence to a topic along that path. [sent-94, score-0.476]
</p><p>32 The assignments of sentences to paths are sampled sequentially: The first sentence takes the initial L-level path, starting with a single branch tree. [sent-99, score-0.313]
</p><p>33 The idea is to represent each path shared by similar candidate sentences with representative summary sentence(s). [sent-105, score-0.57]
</p><p>34 •A itf aea ccahn node, we nlteet summary sentences sample a path by choosing only from the existing children of that node with a probability proportional to the number of other sentences assigned to that child. [sent-107, score-0.689]
</p><p>35 By choosing γs ≪ γo we suppress the generation of new branches for summary sentences and modify the γ of nCRP prior in Eq. [sent-110, score-0.438]
</p><p>36 (2) For each sentence d ∈ {O ∪ S}, (a) irf e da ∈ Ose,n dternawce a path cd v nCRP(γo), eifls de i∈f dO ∈ Sra,w wdr aaw pa a path cd v nCRP(γs). [sent-114, score-0.351]
</p><p>37 (c) For each word n, choose:∼ (i) lierv(eαl zd,n |θd and (ii) word wd,n | {zd,n, cd, β} Given sentence d, θd |is{ a vecto,rβ o}f topic proportions from L dimensional Dirichlet parameterized by α (distribution over levels in the tree. [sent-116, score-0.32]
</p><p>38 The aim is to obtain the following samples from the posterior of: (i) the latent tree T, (ii) the level assignment z for all words, (iii) the path assignments c for all sentences conditioned on the observed words w. [sent-123, score-0.41]
</p><p>39 4  Tree-Based Sentence Scoring  The sumHLDA constructs a hierarchical tree structure of candidate sentences (per document cluster) by positioning summary sentences on the tree. [sent-129, score-0.825]
</p><p>40 Each sentence is represented by a path in the tree, and each path can be shared by many sentences. [sent-130, score-0.323]
</p><p>41 Moreover, if a path includes a summary sentence, then candidate sentences on that path are more likely to be selected for summary text. [sent-132, score-0.959]
</p><p>42 In particular, the similarity of a candidate sentence om to a summary  sentence sn sharing the same path is a measure of strength, indicating how likely om is to be included in the generated summary (Algorithm 1): Let com be the path for a given om. [sent-133, score-2.232]
</p><p>43 We find summary sentences that share the same path with om via: M = {sn ∈ S|csn = com }. [sent-134, score-1.055]
</p><p>44 The score of eachv isae:nt Menc =e i s{ csalc∈u lSat|ecd by similarity teo stchoer bee ostf matching summary sentence in M: score(om) =  maxsn∈M  sim(om, sn)  (4)  If M=ø, then score(om)=ø. [sent-135, score-0.415]
</p><p>45 The efficiency of our similarity measure in identifying the best matching summary sentence, is tied to how expressive the extracted topics of our sumHLDA models are. [sent-136, score-0.48]
</p><p>46 Given path com , we calculate the similarity of om to each sn, n=1. [sent-137, score-0.739]
</p><p>47 sparse unigram distributions (sim1) at each topic lon com : similarity between p(wom,l |zom = l, com , vl) and p(wsn,l |zsn = l, com , vl) ? [sent-140, score-0.686]
</p><p>48 distributions of topic proportions (sim2); similarity between p(zom |com ) and p(zsn |com ). [sent-142, score-0.372]
</p><p>49 sim1: We define two sparse (discrete) un−  igram dmistributions for candidate om and summary sn at each node l on a vocabulary identified with words generated by th? [sent-143, score-0.975]
</p><p>50 at are generated from topic zom at level l on path com . [sent-153, score-0.615]
</p><p>51 The discrete unigram distribution poml = p(wom,l |zom = l, com , vl) represents the probability over zall words vl assigned to topic zom at level l, by sampling only for words in wom,l. [sent-154, score-0.595]
</p><p>52 Algorithm 1Tree-Based Sentence Scoring 1: Given tree T from sumHLDA, candidate and summary sentences: O = {o1, . [sent-160, score-0.372]
</p><p>53 ,ee|O OT| danod summary sentences 4: on path com : M = {sn ∈ S|csn = com } 5: for summary Msen =tenc {ess n∈ ← S| 1c, . [sent-171, score-0.995]
</p><p>54 (6) at each level of com by:  sim1(om, sn) = L1 PlL=1 Wcom,l(pom,l,psn,l)  ∗  l  (7) The similarity betweenpom,l andpsn,l at each level is weighted proportional to the level lbecause the similarity between sentences should be rewarded if there is a specific word overlap at child nodes. [sent-187, score-0.495]
</p><p>55 −sim2: We introduce another measure based on sentence-topic mixing proportions to calculate the concept-based similarities between om and sn. [sent-188, score-0.589]
</p><p>56 We calculate the topic proportions of om and sn, represented by pzom = p(zom |com ) and pzsn = p(zsn |com ) via Eq. [sent-189, score-0.794]
</p><p>57 Distribution of words in given two sentences, a candidate (om) and a summary (sn) using sub-vocabulary of words at each topic vzl . [sent-195, score-0.503]
</p><p>58 Discrete distributions on the left are topic mixtures for each sentence, pzom and pzsn . [sent-196, score-0.337]
</p><p>59 (6) by: sim2 (om, sn) =  10−IRcom (pzom,pzsn)  (8)  sim1 provides information about the similarity between two sentences, om and sn based on topicword distributions. [sent-198, score-0.633]
</p><p>60 They jointly effect the sentence score and are combined in one measure: sim(om, sn) = sim1 (om, sn)  ∗  sim2 (om, sn) (9)  The final score for a given om is calculated from Eq. [sent-200, score-0.57]
</p><p>61 b depicts a sample path illustrating sparse unigram distributions of om and sm at each level as well as their topic proportions, pzom , and pzsn . [sent-204, score-1.009]
</p><p>62 In experiment 3, we discuss the effect of our tree-based scoring on summarization performance in comparison to a classical scoring method presented as our baseline model. [sent-205, score-0.295]
</p><p>63 Thus, we create ngram meta-features to represent sentences instead of word n-gram frequencies: (I) nGram Meta-Features (NMF): For each document cluster D, we identify most frequent (non-stop word) unigrams, i. [sent-214, score-0.287]
</p><p>64 We measure observed unigram probabilities for each wi ∈ vfreq with pD (wi) =  nD(wi)/Pj|V= |1  nD(wj), where nD(wi) is the number oPf times wi appears in D and |V | is the total numPber of unigrams. [sent-217, score-0.327]
</p><p>65 (II) Document Word Frequency MetaFeatures (DMF): The characteristics of sentences at the document level can be important in summary generation. [sent-221, score-0.576]
</p><p>66 DMF identify whether a word in a given sentence is specific to the document in consideration or it is commonly used in the document cluster. [sent-222, score-0.339]
</p><p>67 This is important because summary sentences usually contain abstract terms rather than specific terms. [sent-223, score-0.372]
</p><p>68 Given sentence om, let d be the docum∈ent v that om belongs to, i. [sent-227, score-0.502]
</p><p>69 We measure unigram probabilities for each wi by p(wi ∈ om) = nd(wi ∈ om)/nD (wi), where nd(wi ∈ om) is the numbe∈r of times wi appears in d and nD (wi) is the number of times wi appears in D. [sent-230, score-0.403]
</p><p>70 ristics with a regression model using sentences in different document clusters. [sent-255, score-0.354]
</p><p>71 Redundancy Elimination: To eliminate redundant sentences in the generated summary, we incrementally add onto the summary the highest ranked sentence om and check if om significantly repeats the information already included in the summary until the algorithm reaches word count limit. [sent-257, score-1.61]
</p><p>72 A om is discarded if its similarity to any of the previously selected sentences is greater than a threshold identified by a greedy search on the training dataset. [sent-259, score-0.605]
</p><p>73 6  Experiments and Discussions  In this section we describe a number of experiments using our hybrid model on 100 document  clusters each containing 25 news articles from DUC2005-2006 tasks. [sent-260, score-0.312]
</p><p>74 Contrary to typical hLDA models, to efficiently represent sentences in summarization task, we set ascending values for Dirichlet hyper-parameter η as the level increases, encouraging mid to low level distributions to generate as many words as in higher levels, e. [sent-268, score-0.392]
</p><p>75 R toOU coGmEis used for performance measure (Lin and Hovy, 2003; Lin, 2004), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries. [sent-289, score-0.441]
</p><p>76 Here, we illustrate that this prior is practical in learning hierarchical topics for summarization task. [sent-292, score-0.41]
</p><p>77 We use sentences from the human generated summaries during the discovery of hierarchical topics of sentences in document clusters. [sent-293, score-0.74]
</p><p>78 Since summary sentences generally contain abstract words, they are indicative of sentences in documents and should produce minimal amount of new  sis: In sumHLDA  topics (if not none). [sent-294, score-0.672]
</p><p>79 To implement this, in nCRP prior of sumHLDA, we use dual hyper-parameters and choose a very small value for summary sentences, γs = 10e−4 ? [sent-295, score-0.296]
</p><p>80 To analyze this prior, we generate a corpus of v1300 sentences of a document cluster in DUC2005. [sent-299, score-0.287]
</p><p>81 1 8351831508 sumHhL DDAA33 5 8 2417 126 72 16570191 250272 34509808 870 5105 Table 1: Average # of topics per document cluster from sumHLDA and hLDA for different γ and γo and tree depths. [sent-303, score-0.353]
</p><p>82 Less number of topics(nodes) in sumHLDA suggests that summary sentences share pre-existing paths and no new paths or nodes are sampled for them. [sent-308, score-0.497]
</p><p>83 The score of a candidate sentence is the cosine similarity to the maximum matching summary sentence. [sent-315, score-0.484]
</p><p>84 As presented in § 5, NMF is the bundle of frequency sbeansetded m inet §a- 5fe,a NtuMresF on hdoec buumnednlet c olfus frteerlevel, DMF is a bundle of frequency based metafeatures on individual document level and OF represents sentence term frequency, location, and size features. [sent-318, score-0.418]
</p><p>85 HIERSUM : (Haghighi and Vanderwende, 2009) A generative summarization method based on topic models, which uses sentences as an additional level. [sent-328, score-0.471]
</p><p>86 Using an approximation for inference, sentences are greedily added to a summary so long as they decrease KL-divergence. [sent-329, score-0.372]
</p><p>87 We use the following  HybFSum (Hybrid Flat Summarizer): To investigate the performance of hierarchical topic model, we build another hybrid model using flat LDA (Blei et al. [sent-330, score-0.373]
</p><p>88 In LDA each sentence is a superposition of all K topics with sentence specific weights, there is no hierarchical relation between topics. [sent-332, score-0.357]
</p><p>89 Instead of the new tree-based sentence scoring (§ 4), we present a tsriemei-lbaars emdet sheondte using topics §fro 4m), L wDeA p on sentence level. [sent-335, score-0.339]
</p><p>90 Note that in LDA the topic-word distributions φ are over entire vocabulary, and topic mixing proportions for sentences θ are over all the topics discovered from sentences in a document cluster. [sent-336, score-0.854]
</p><p>91 2) and topic mixing weights θ in sentences (in place of topic proportions in Eq. [sent-338, score-0.583]
</p><p>92 Compared to the HybF Sum built on LDA, both HybHSum1&2 yield better performance indicating the effectiveness of using hierarchical topic model in summarization task. [sent-351, score-0.413]
</p><p>93 2, due to the new hierarchical tree-based sentence scoring which characterizes sentences on deeper level. [sent-353, score-0.343]
</p><p>94 This indicates that with our regression model built on training data,  summaries can be efficiently generated for test documents (suitable for online systems). [sent-358, score-0.31]
</p><p>95 Human annotators are given two sets of summary text for each document set, generated from two approaches: best hierarchical hybrid HybHSum2 and flat hybrid HybFSum models, and are asked to mark the better summary ( a ) Ref. [sent-360, score-1.002]
</p><p>96 gk$el1uotwasifnrcdgvy9ehowt7,  Figure 2: Example summary text generated by systems compared in Experiment 3. [sent-365, score-0.299]
</p><p>97 according to five criteria: non-redundancy (which summary is less redundant), coherence (which summary is more coherent), focus and readability (content and not include unnecessary details), responsiveness and overall performance. [sent-372, score-0.52]
</p><p>98 We asked 4 annotators to rate DUC2007 predicted summaries (45 summary pairs per annotator). [sent-373, score-0.373]
</p><p>99 Kz candidate sentence scores  candidate sentence scores  . [sent-403, score-0.356]
</p><p>100 We demonstrated that implementation of a summary focused hierarchical topic model to discover sentence structures as well as construction of a discriminative method for inference can benefit summarization quality on  manual and automatic evaluation metrics. [sent-407, score-0.779]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('om', 0.437), ('sumhlda', 0.36), ('summary', 0.26), ('ncrp', 0.223), ('hlda', 0.206), ('topic', 0.174), ('hybhsum', 0.171), ('summarization', 0.147), ('sn', 0.14), ('blei', 0.139), ('document', 0.137), ('topics', 0.135), ('path', 0.129), ('zom', 0.12), ('com', 0.117), ('summaries', 0.113), ('sentences', 0.112), ('wi', 0.11), ('hybrid', 0.107), ('regression', 0.105), ('rouge', 0.101), ('vanderwende', 0.093), ('hierarchical', 0.092), ('proportions', 0.081), ('mds', 0.075), ('scoring', 0.074), ('vl', 0.07), ('candidate', 0.069), ('kl', 0.069), ('dmf', 0.069), ('fmi', 0.069), ('zsn', 0.069), ('clusters', 0.068), ('lda', 0.065), ('sentence', 0.065), ('distributions', 0.061), ('similarity', 0.056), ('documents', 0.053), ('duc', 0.052), ('azerbaijan', 0.051), ('hybfsum', 0.051), ('nmf', 0.051), ('pzom', 0.051), ('pzsn', 0.051), ('dirichlet', 0.048), ('extractive', 0.048), ('nenkova', 0.047), ('paths', 0.046), ('haghighi', 0.046), ('proportional', 0.046), ('svr', 0.045), ('scores', 0.044), ('unigram', 0.044), ('tree', 0.043), ('nd', 0.043), ('summarizer', 0.042), ('mixing', 0.042), ('discover', 0.041), ('pd', 0.04), ('generated', 0.039), ('ir', 0.039), ('conroy', 0.039), ('generative', 0.038), ('cluster', 0.038), ('coherent', 0.038), ('nested', 0.037), ('prior', 0.036), ('level', 0.036), ('latent', 0.035), ('unigrams', 0.035), ('csn', 0.034), ('dilek', 0.034), ('drucker', 0.034), ('inet', 0.034), ('metafeatures', 0.034), ('otest', 0.034), ('vfreq', 0.034), ('wom', 0.034), ('discrete', 0.034), ('score', 0.034), ('mo', 0.033), ('tang', 0.033), ('sampled', 0.033), ('salient', 0.032), ('divergence', 0.032), ('branch', 0.031), ('characteristics', 0.031), ('node', 0.03), ('suppress', 0.03), ('evaluations', 0.03), ('rated', 0.029), ('measure', 0.029), ('posterior', 0.029), ('compile', 0.029), ('restaurant', 0.029), ('frequency', 0.028), ('bundle', 0.028), ('irf', 0.028), ('assignments', 0.026), ('sm', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="8-tfidf-1" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>2 0.26350412 <a title="8-tfidf-2" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>Author: Xiaojun Wan ; Huiying Li ; Jianguo Xiao</p><p>Abstract: Cross-language document summarization is a task of producing a summary in one language for a document set in a different language. Existing methods simply use machine translation for document translation or summary translation. However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in readability and content. In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process. First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process. Finally, the English sentences with high translation quality and high informativeness are selected and translated to form the Chinese summary. Experimental results demonstrate the effectiveness and usefulness of the proposed approach. 1</p><p>3 0.22408526 <a title="8-tfidf-3" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<p>Author: Shih-Hsiang Lin ; Berlin Chen</p><p>Abstract: In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. 1</p><p>4 0.18244798 <a title="8-tfidf-4" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>Author: Emily Pitler ; Annie Louis ; Ani Nenkova</p><p>Abstract: To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference informa- tion, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.</p><p>5 0.17218986 <a title="8-tfidf-5" href="./acl-2010-Wrapping_up_a_Summary%3A_From_Representation_to_Generation.html">264 acl-2010-Wrapping up a Summary: From Representation to Generation</a></p>
<p>Author: Josef Steinberger ; Marco Turchi ; Mijail Kabadjov ; Ralf Steinberger ; Nello Cristianini</p><p>Abstract: The main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries. This is motivated by empirical evidence from TAC 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries. We report encouraging preliminary results comparable to those attained by participating systems at TAC 2009.</p><p>6 0.17096226 <a title="8-tfidf-6" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>7 0.17084387 <a title="8-tfidf-7" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>8 0.15638112 <a title="8-tfidf-8" href="./acl-2010-A_New_Approach_to_Improving_Multilingual_Summarization_Using_a_Genetic_Algorithm.html">11 acl-2010-A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm</a></p>
<p>9 0.14517522 <a title="8-tfidf-9" href="./acl-2010-Generating_Image_Descriptions_Using_Dependency_Relational_Patterns.html">124 acl-2010-Generating Image Descriptions Using Dependency Relational Patterns</a></p>
<p>10 0.13546246 <a title="8-tfidf-10" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>11 0.13168736 <a title="8-tfidf-11" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>12 0.12719096 <a title="8-tfidf-12" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>13 0.12292669 <a title="8-tfidf-13" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>14 0.12095662 <a title="8-tfidf-14" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>15 0.11655281 <a title="8-tfidf-15" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>16 0.10392193 <a title="8-tfidf-16" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>17 0.09707807 <a title="8-tfidf-17" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>18 0.096289098 <a title="8-tfidf-18" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>19 0.092637718 <a title="8-tfidf-19" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>20 0.071218044 <a title="8-tfidf-20" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.221), (1, 0.073), (2, -0.144), (3, 0.052), (4, -0.025), (5, -0.001), (6, 0.017), (7, -0.396), (8, 0.021), (9, -0.115), (10, -0.004), (11, -0.114), (12, 0.0), (13, 0.059), (14, 0.071), (15, -0.092), (16, -0.077), (17, -0.028), (18, -0.041), (19, -0.056), (20, -0.001), (21, -0.033), (22, -0.009), (23, 0.133), (24, -0.05), (25, -0.064), (26, -0.035), (27, 0.04), (28, -0.041), (29, 0.053), (30, -0.015), (31, 0.036), (32, 0.051), (33, 0.034), (34, -0.038), (35, -0.02), (36, 0.006), (37, 0.006), (38, 0.125), (39, 0.006), (40, 0.055), (41, -0.007), (42, 0.03), (43, 0.0), (44, 0.0), (45, 0.003), (46, 0.019), (47, -0.046), (48, -0.029), (49, 0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96430755 <a title="8-lsi-1" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>2 0.76630616 <a title="8-lsi-2" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<p>Author: Shih-Hsiang Lin ; Berlin Chen</p><p>Abstract: In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. 1</p><p>3 0.70806444 <a title="8-lsi-3" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>Author: Xiaojun Wan ; Huiying Li ; Jianguo Xiao</p><p>Abstract: Cross-language document summarization is a task of producing a summary in one language for a document set in a different language. Existing methods simply use machine translation for document translation or summary translation. However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in readability and content. In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process. First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process. Finally, the English sentences with high translation quality and high informativeness are selected and translated to form the Chinese summary. Experimental results demonstrate the effectiveness and usefulness of the proposed approach. 1</p><p>4 0.70792955 <a title="8-lsi-4" href="./acl-2010-Wrapping_up_a_Summary%3A_From_Representation_to_Generation.html">264 acl-2010-Wrapping up a Summary: From Representation to Generation</a></p>
<p>Author: Josef Steinberger ; Marco Turchi ; Mijail Kabadjov ; Ralf Steinberger ; Nello Cristianini</p><p>Abstract: The main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries. This is motivated by empirical evidence from TAC 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries. We report encouraging preliminary results comparable to those attained by participating systems at TAC 2009.</p><p>5 0.70508385 <a title="8-lsi-5" href="./acl-2010-A_New_Approach_to_Improving_Multilingual_Summarization_Using_a_Genetic_Algorithm.html">11 acl-2010-A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm</a></p>
<p>Author: Marina Litvak ; Mark Last ; Menahem Friedman</p><p>Abstract: Automated summarization methods can be defined as “language-independent,” if they are not based on any languagespecific knowledge. Such methods can be used for multilingual summarization defined by Mani (2001) as “processing several languages, with summary in the same language as input.” In this paper, we introduce MUSE, a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages—English and Hebrew—and evaluated its performance with ROUGE-1 Recall vs. state- of-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank1) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.</p><p>6 0.67411524 <a title="8-lsi-6" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>7 0.66276145 <a title="8-lsi-7" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>8 0.6370697 <a title="8-lsi-8" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>9 0.63369161 <a title="8-lsi-9" href="./acl-2010-Generating_Image_Descriptions_Using_Dependency_Relational_Patterns.html">124 acl-2010-Generating Image Descriptions Using Dependency Relational Patterns</a></p>
<p>10 0.57605588 <a title="8-lsi-10" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>11 0.57401603 <a title="8-lsi-11" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>12 0.53910393 <a title="8-lsi-12" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>13 0.51716489 <a title="8-lsi-13" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>14 0.50872368 <a title="8-lsi-14" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>15 0.50687933 <a title="8-lsi-15" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>16 0.4721998 <a title="8-lsi-16" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>17 0.42392164 <a title="8-lsi-17" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>18 0.41839829 <a title="8-lsi-18" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>19 0.4047541 <a title="8-lsi-19" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<p>20 0.39918682 <a title="8-lsi-20" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.011), (25, 0.037), (42, 0.012), (59, 0.061), (73, 0.027), (78, 0.025), (83, 0.088), (84, 0.026), (98, 0.595)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99828267 <a title="8-lda-1" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>Author: Yabin Zheng ; Zhiyuan Liu ; Lixing Xie</p><p>Abstract: Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance. 1</p><p>2 0.9958685 <a title="8-lda-2" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>Author: Kotaro Kitagawa ; Kumiko Tanaka-Ishii</p><p>Abstract: Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average pars- ing time not much slower than O(n).</p><p>3 0.99537563 <a title="8-lda-3" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>Author: David Vickrey ; Oscar Kipersztok ; Daphne Koller</p><p>Abstract: We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</p><p>4 0.9941532 <a title="8-lda-4" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>same-paper 5 0.99084139 <a title="8-lda-5" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>6 0.98766279 <a title="8-lda-6" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>7 0.97912437 <a title="8-lda-7" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>8 0.95888984 <a title="8-lda-8" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>9 0.95355546 <a title="8-lda-9" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>10 0.94457221 <a title="8-lda-10" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>11 0.93543571 <a title="8-lda-11" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>12 0.90916616 <a title="8-lda-12" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>13 0.89929754 <a title="8-lda-13" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>14 0.89423585 <a title="8-lda-14" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>15 0.88982892 <a title="8-lda-15" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>16 0.88161474 <a title="8-lda-16" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>17 0.8768028 <a title="8-lda-17" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>18 0.8741858 <a title="8-lda-18" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>19 0.87089622 <a title="8-lda-19" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>20 0.87057555 <a title="8-lda-20" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
