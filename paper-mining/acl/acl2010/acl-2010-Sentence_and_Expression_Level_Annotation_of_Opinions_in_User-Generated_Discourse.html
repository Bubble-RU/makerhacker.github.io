<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-208" href="#">acl2010-208</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</h1>
<br/><p>Source: <a title="acl-2010-208-pdf" href="http://aclweb.org/anthology//P/P10/P10-1059.pdf">pdf</a></p><p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>Reference: <a title="acl-2010-208-reference" href="../acl2010_reference/acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). [sent-7, score-0.774]
</p><p>2 Opinion mining spans a variety of subtasks including: creating opinion word lexicons (Esuli and Sebastiani, 2006; Ding et al. [sent-10, score-0.496]
</p><p>3 , 2008), identifying opinion expressions (Riloff and Wiebe, 2003; Fahrni and Klenner, 2008), identifying polarities of opinions in context (Breck et al. [sent-11, score-0.6]
</p><p>4 , 2005), extracting opinion targets (Hu and Liu, 2004; Zhuang et al. [sent-13, score-0.542]
</p><p>5 , 2006; Cheng and Xu, 2008) and opinion holders (Kim and Hovy, 2006; Choi et al. [sent-14, score-0.483]
</p><p>6 Data-driven approaches for extracting opinion expressions, their holders and targets require reliably annotated data at the expression level. [sent-16, score-0.789]
</p><p>7 In previous research, expression level annotation of opinions was extensively investigated on newspaper articles (Wiebe et al. [sent-17, score-0.503]
</p><p>8 To the best of our knowledge, there are two corpora of user-generated discourse which are annotated for opinion related information at the expression level: The corpus of Hu & Liu (2004) consists of customer reviews about consumer electronics, and the corpus of Zhuang et al. [sent-23, score-0.87]
</p><p>9 Our goal is to create sentence and expression level annotated corpus of customer reviews which fulfills the following requirements: (1) It filters individual sentences regarding their topic rele-  vancy and the existence of an opinion or factual information which implies an evaluation. [sent-27, score-1.015]
</p><p>10 (2) It identifies opinion expressions including the respective opinion target, opinion holder, modifiers, and anaphoric expressions if applicable. [sent-28, score-1.367]
</p><p>11 (3) The semantic orientation of the opinion expression is identified while considering negation, and the opinion expression is linked to the respective holder and target in the discourse. [sent-29, score-1.291]
</p><p>12 Such a resource would (i) enable novel applications ofopinion mining such as a fine-grained identification of opinion properties, e. [sent-30, score-0.399]
</p><p>13 opinion modification detection including negation, and (ii) enhance opinion target extraction and the polarity assignment by linking the opinion expression with its target 1http : / /blog . [sent-32, score-1.666]
</p><p>14 1 Newspaper Articles and Meeting Dialogs Most prominent work concerning the expression level annotation of opinions is the MultiPerspective Question Answering (MPQA) corpus2 (Wiebe et al. [sent-43, score-0.47]
</p><p>15 It was extended several times over the last years, either by adding new documents or annotating new types of opinion related information (Wilson and Wiebe, 2005; Stoyanov and Cardie, 2008; Wilson, 2008b). [sent-45, score-0.399]
</p><p>16 Besides subjective utterances, the AMIDA scheme contains objective polar utterances which annotates evaluations without expressing explicit opinion expressions. [sent-55, score-1.239]
</p><p>17 (2008) proposes opinion frames for representing discourse level associations in meeting dialogs. [sent-57, score-0.574]
</p><p>18 The opinion frames are built based on the links between targets. [sent-61, score-0.428]
</p><p>19 (2008) show that opinion frames enable a coherent interpretation of the  2http : / /www . [sent-63, score-0.428]
</p><p>20 They propose a scheme which first identifies and assigns categories to the opinion segments as reporting, judgment, advice, or sentiment; and then links the opinion segments with each other via rhetorical relations including contrast, correction, support, result, or continuation. [sent-70, score-0.9]
</p><p>21 However, in contrast to our scheme and other schemes, instead of marking expression boundaries without any restriction they annotate an opinion segment only if it contains an opinion word from their lexicon, or if it has a rhetorical relation to another opinion segment. [sent-71, score-1.538]
</p><p>22 The corpus provides only target and polarity annotations, and do not contain opinion expression or opinion modifier annotations which lead to these polarity scores. [sent-80, score-1.43]
</p><p>23 The annotation scheme allows the annotation of implicit features (indicated with the the attribute [u]). [sent-81, score-0.545]
</p><p>24 e, it is unclear to which mention of the feature the opinion refers to. [sent-84, score-0.399]
</p><p>25 The opinion words (Oword) and their semantic orientations (Otype) are identified. [sent-90, score-0.399]
</p><p>26 1 Opinion versus Polar Facts The goal of the annotation scheme is to capture the evaluations regarding the topics being discussed in the consumer reviews. [sent-98, score-0.471]
</p><p>27 The evaluations in consumer reviews are either explicit expressions of opinions, or facts which imply evaluations as discussed below. [sent-99, score-0.572]
</p><p>28 (2005), we view opinions in terms of their functional components, as opinion holders, e. [sent-106, score-0.553]
</p><p>29 We call such objectively verifiable, but evaluative sentences polar facts. [sent-120, score-0.598]
</p><p>30 opinion words, loaded with a positive or negative connotation (e. [sent-123, score-0.438]
</p><p>31 However, evaluations in polar facts can only be inferred within the context of the review. [sent-127, score-0.691]
</p><p>32 For instance, the targets of the implied evalution in the polar facts (2), (3) and (4) are the professors. [sent-128, score-0.765]
</p><p>33 First, the sentence level scheme analyses each sentence in terms of (i) its relevancy to the overall topic of the review, and (ii) whether it contains an evaluation (an opinion or a polar fact) about the topic. [sent-131, score-1.363]
</p><p>34 Upon marking an opinion expression span, the target and holder of the opinion is marked and linked to the marked opinion expression. [sent-133, score-1.653]
</p><p>35 Furthermore, the expression level  scheme allows assigning polarities to the marked opinion expression spans and targets of the polar facts. [sent-134, score-1.735]
</p><p>36 The following attributes are used: topic relevant attribute is labeled as yes if the sentence discusses the given topic itself or its aspects, properties or features as in examples (1)(4). [sent-151, score-0.719]
</p><p>37 opinionated attribute is labeled as yes if the sentence contains any explicit expressions of opinions about the given topic. [sent-153, score-0.786]
</p><p>38 This attribute is presented if the topic relevant attribute has been labeled as none given or yes. [sent-154, score-0.642]
</p><p>39 [Topic: University of Phoenix] polar fact attribute is labeled as yes if the sentence is a polar fact. [sent-168, score-1.464]
</p><p>40 This attribute is presented if the opinionated attribute has been labeled as no. [sent-169, score-0.67]
</p><p>41 Examples (2)-(4) demonstrate sentences labeled as topic relevant=yes, opinionated=no and polar fact=yes. [sent-170, score-0.762]
</p><p>42 polar fact polarity attribute represents the polarity of the evaluation in a polar fact sentence. [sent-171, score-1.675]
</p><p>43 The value both is intended for the polar fact sentences containing more than one evaluation with contradicting polarities. [sent-173, score-0.634]
</p><p>44 At the expression level analysis, the targets of the contradicting polar fact evaluations are identified distinctly and assigned polarities of positive or negative later on. [sent-174, score-1.147]
</p><p>45 Examples (9)-(1 1) demonstrate examples of polar fact sentences with different values of the attribute polar fact polarity. [sent-175, score-1.352]
</p><p>46 , sentences labeled as topic relevant=yes, opinionated=yes or topic relevant=yes, opinionated=no, polar fact=yes. [sent-184, score-0.887]
</p><p>47 If the sentence is opinionated, then, the aim is to mark the opinion expression span, and label its polarity and strength (i. [sent-186, score-0.846]
</p><p>48 At this stage, annotators mark text spans, and are allowed to assign one of the five labels to the marked span: The polar target is used to label the targets of the evaluations implied by polar facts. [sent-190, score-1.517]
</p><p>49 The is-  Reference attribute labels polar targets which are anaphoric references. [sent-191, score-0.923]
</p><p>50 The polar target polarity 578  Figure 2: The expression level annotation scheme  attribute is used to label the polarity as positive or negative. [sent-192, score-1.605]
</p><p>51 If the isReference attribute is labeled as true, then the referent attribute appears which enables the annotator to resolve the reference to its antecedent. [sent-193, score-0.563]
</p><p>52 The polar target in (13), written bold, is labeled as isReference=true, polar target polarity=negative. [sent-195, score-1.257]
</p><p>53 To resolve the reference, annotator first creates another polar target markable for the antecedent, namely the bold text span in (12), then, links the antecedent to the referent attribute of the polar target in (13). [sent-196, score-1.61]
</p><p>54 The target annotation represents what the opinion is about. [sent-199, score-0.584]
</p><p>55 Both polar targets and targets can be the topic ofthe review or different aspects, i. [sent-200, score-0.976]
</p><p>56 Similar to the polar targets, the isReference attribute allows the identification of the targets which are anaphoric references and the  referent attribute links them to their antecedents in the discourse. [sent-203, score-1.158]
</p><p>57 The holder type represents the holder of an opinion in the discourse and is labeled in the same manner as the targets and polar targets. [sent-206, score-1.365]
</p><p>58 , which affect the strength of an opinion or shift its polarity. [sent-210, score-0.466]
</p><p>59 The opinionexpression annotation is used to label the opinion terms in the sentence. [sent-214, score-0.632]
</p><p>60 The polarity attribute assesses the semantic orientation of the attitude, where the strength attribute marks the intensity of this attitude. [sent-218, score-0.715]
</p><p>61 The polarity and strength attributes focus solely on the marked opinionexpression span, not the whole evaluation implied in the sentence. [sent-219, score-0.479]
</p><p>62 We infer the polarity of the evaluation only after considering the modifier, polarity and the strength attributes together. [sent-221, score-0.476]
</p><p>63 Finally, Figure 3 demonstrates all expression level markables created for an opinionated sentence and how they relate to each other. [sent-231, score-0.501]
</p><p>64 579  Figure 3: Expression level annotation example 4  Annotation Study  Each review has been annotated by two annotators independently according to the annotation scheme introduced above. [sent-232, score-0.55]
</p><p>65 A new attribute is presented based on the decision made for the previous attribute, for instance, opinionated attribute is only presented if the topic relevant attribute is labeled as yes or none given; polar fact attribute is only presented if the opinionated attribute is labeled as no etc. [sent-275, score-2.322]
</p><p>66 We calculate κ for each attribute considering only the markables which  were labeled the same by both annotators in the previously required step. [sent-276, score-0.41]
</p><p>67 r9756e231ment  The agreement for topic relevancy shows that it is possible to label this attribute reliably. [sent-281, score-0.486]
</p><p>68 The sentences labeled as topic relevant by both annotators correspond to 59% of all sentences, suggesting that people often drift offthe topic in consumer reviews. [sent-282, score-0.606]
</p><p>69 On the other hand, we obtain moderate agreement levels for the opinionated and polar fact attributes. [sent-284, score-0.847]
</p><p>70 62% of the topic relevant sentences were  labeled as opinionated by at least one annotator, and the rest 38% constitute the topic relevant sentences labeled as not opinionated by both annotators. [sent-285, score-0.924]
</p><p>71 Nonetheless, they still contain evaluations (polar facts), as 15% of the topic relevant sen580  tences were labeled as polar facts by both annotators. [sent-286, score-0.931]
</p><p>72 When we merge the attributes opinionated and polar fact into a single category, we obtain κ of 0. [sent-287, score-0.801]
</p><p>73 Thus, we conclude that opinion-relevant sentences, either in the form of an explicit expression of opinion or a polar fact, can be labeled reliably in consumer reviews. [sent-289, score-1.327]
</p><p>74 However, there is a thin border between polar facts and explicit expressions of opinions. [sent-290, score-0.697]
</p><p>75 To the best ofour knowledge, similar annotation efforts on consumer or movie reviews do not provide any agreement figures for direct comparison. [sent-291, score-0.499]
</p><p>76 They calculate the sentence level κ values based on the existence of a subjective expression span in the sentence. [sent-299, score-0.426]
</p><p>77 Although the task definitions, approaches and the corpora have quite disparate characteristics in both studies, we obtain comparable results when we merge opinionated and polar fact categories. [sent-300, score-0.748]
</p><p>78 3  Expression Level Agreement  At the expression level, annotators focus only on the sentences which were labeled as opinionated or polar fact by both annotators. [sent-302, score-1.116]
</p><p>79 Annotators were instructed to mark text spans, and then, assign them the annotation types such as polar target, opinionexpression etc. [sent-303, score-0.758]
</p><p>80 (2008) present a discourse level annotation study in which opinion and target spans are marked and linked with each other in a meeting transcript corpus. [sent-310, score-0.867]
</p><p>81 We obtain higher agreement values for both opinion expression and target spans. [sent-330, score-0.725]
</p><p>82 We attribute this to the fact that the annotators look for opinion expression and target spans within the opinionated sentences which they agreed upon. [sent-331, score-1.273]
</p><p>83 Compared to the high agreement on marking target spans, we obtain lower agreement values on marking polar target spans. [sent-333, score-1.003]
</p><p>84 We observe that it is easier to attribute explicit expressions of evaluations to topic relevant entities compared to  attributing evaluations implied by experiences to specific topic relevant entities in the reviews. [sent-334, score-0.882]
</p><p>85 We considered the overlapping target and polar target spans together in this calculation, and obtained an α value of 0. [sent-336, score-0.75]
</p><p>86 For instance, upon marking a text span as a polar target or an opinionexpression, one has to label the polarity and strength. [sent-356, score-0.925]
</p><p>87 e5697x4pression level  We observe that the strength of the opinionexpression and the polar target polarity cannot be labeled as reliably as the polarity of the opinionexpression. [sent-362, score-1.276]
</p><p>88 61% of the agreed upon polar targets were labeled as negative by both annotators. [sent-363, score-0.786]
</p><p>89 This indicates that reviewers tend to report negative experiences using polar facts, probably objectively describing what has happened, but report positive experiences with explicit opinion expressions. [sent-366, score-1.109]
</p><p>90 4 Discussion We analyzed the discrepancies in the annotations to gain insights about the challenges involved in various opinion related labeling tasks. [sent-370, score-0.399]
</p><p>91 However, they had difficulties distinguishing between a polar fact and an opinion. [sent-380, score-0.559]
</p><p>92 Sentence level annotation increases the reliability of the expression level annotation in terms of marking text spans. [sent-385, score-0.665]
</p><p>93 For instance, one annotator labeled the 582  opinion expression in (24) as strong, while the other one labeled it as average. [sent-387, score-0.768]
</p><p>94 5  Conclusions  We presented a corpus of consumer reviews from the rateitall and eopinions websites annotated with opinion related information. [sent-393, score-0.659]
</p><p>95 Existing opinion annotated user-generated corpora suffer from several limitations which result in difficulties for interpreting the experimental results and for performing error analysis. [sent-394, score-0.399]
</p><p>96 To name a few, they do not explicitly link the functional components of the opinions like targets, holders, or modifiers with the opinion expression; some of them do not mark  opinion expression spans, none of them resolves anaphoric references in discourse. [sent-395, score-1.198]
</p><p>97 Therefore, we introduced a two level annotation scheme consisting of the sentence and expression levels, which overcomes the limitations of the existing review corpora. [sent-396, score-0.538]
</p><p>98 The expression level annotation further investigates on-topic sentences containing evaluations for pinpointing the properties (polarity, strength), and marking the functional components of the evaluations (opinion terms, modifiers, targets and holders), and linking them within a discourse. [sent-399, score-0.846]
</p><p>99 We applied the annotation scheme to the consumer review genre and presented an extensive inter-annotator study providing insights to the challenges involved in various opinion related labeling tasks in consumer reviews. [sent-400, score-0.892]
</p><p>100 Similar to the MPQA scheme, which is successfully applied to the newspaper genre, the annotation  scheme treats opinions and evaluations as a composition of functional components  and it is eas-  ily extendable. [sent-401, score-0.508]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('polar', 0.525), ('opinion', 0.399), ('attribute', 0.201), ('opinionated', 0.189), ('polarity', 0.178), ('expression', 0.163), ('targets', 0.143), ('topic', 0.125), ('annotation', 0.121), ('consumer', 0.115), ('opinions', 0.113), ('opinionexpression', 0.112), ('scheme', 0.102), ('somasundaran', 0.101), ('agreement', 0.099), ('evaluations', 0.098), ('spans', 0.097), ('wiebe', 0.093), ('annotators', 0.093), ('reviews', 0.089), ('wilson', 0.085), ('holders', 0.084), ('capella', 0.084), ('span', 0.082), ('labeled', 0.079), ('marking', 0.076), ('movie', 0.075), ('holder', 0.073), ('level', 0.073), ('discourse', 0.073), ('subjective', 0.069), ('facts', 0.068), ('markable', 0.067), ('strength', 0.067), ('target', 0.064), ('relevancy', 0.061), ('lenient', 0.061), ('yes', 0.061), ('expressions', 0.058), ('professors', 0.056), ('private', 0.055), ('anaphoric', 0.054), ('attributes', 0.053), ('modifier', 0.049), ('annotator', 0.048), ('explicit', 0.046), ('zhuang', 0.042), ('attitude', 0.042), ('contradicting', 0.042), ('ftype', 0.042), ('fword', 0.042), ('isreference', 0.042), ('nightmare', 0.042), ('otype', 0.042), ('oword', 0.042), ('phoenix', 0.042), ('syllabus', 0.042), ('functional', 0.041), ('theresa', 0.041), ('marked', 0.04), ('review', 0.04), ('janyce', 0.04), ('objectively', 0.04), ('mpqa', 0.04), ('negative', 0.039), ('sentence', 0.039), ('hu', 0.038), ('intensity', 0.038), ('reliability', 0.038), ('markables', 0.037), ('relevant', 0.036), ('regarding', 0.035), ('sentiment', 0.035), ('referent', 0.034), ('adt', 0.034), ('fact', 0.034), ('sentences', 0.033), ('newspaper', 0.033), ('claire', 0.033), ('customer', 0.031), ('orientation', 0.03), ('polarities', 0.03), ('experiences', 0.03), ('passonneau', 0.03), ('implied', 0.029), ('modifiers', 0.029), ('frames', 0.029), ('disagreements', 0.028), ('choi', 0.028), ('amida', 0.028), ('appauled', 0.028), ('complaints', 0.028), ('devry', 0.028), ('eopinions', 0.028), ('fulfills', 0.028), ('iment', 0.028), ('insulting', 0.028), ('nonsense', 0.028), ('pheonix', 0.028), ('rateitall', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="208-tfidf-1" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>2 0.39925045 <a title="208-tfidf-2" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>Author: Binyang Li ; Lanjun Zhou ; Shi Feng ; Kam-Fai Wong</p><p>Abstract: There is a growing research interest in opinion retrieval as on-line users’ opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user’s query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representa- , tion, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 1</p><p>3 0.39082959 <a title="208-tfidf-3" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>4 0.35527474 <a title="208-tfidf-4" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>Author: Yejin Choi ; Claire Cardie</p><p>Abstract: Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes polarity and intensity. Our experimental results show that our proposed approach improves the performance over a baseline that does not — exploit hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate components.</p><p>5 0.31944892 <a title="208-tfidf-5" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>6 0.16461152 <a title="208-tfidf-6" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>7 0.13725837 <a title="208-tfidf-7" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>8 0.12669182 <a title="208-tfidf-8" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>9 0.12260086 <a title="208-tfidf-9" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>10 0.11338596 <a title="208-tfidf-10" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>11 0.10957469 <a title="208-tfidf-11" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>12 0.10868617 <a title="208-tfidf-12" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>13 0.10436736 <a title="208-tfidf-13" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>14 0.084779963 <a title="208-tfidf-14" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>15 0.083166584 <a title="208-tfidf-15" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>16 0.083152413 <a title="208-tfidf-16" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>17 0.07828334 <a title="208-tfidf-17" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>18 0.074088998 <a title="208-tfidf-18" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>19 0.073223948 <a title="208-tfidf-19" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>20 0.073022515 <a title="208-tfidf-20" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.194), (1, 0.19), (2, -0.28), (3, 0.268), (4, -0.284), (5, 0.085), (6, -0.071), (7, 0.234), (8, 0.01), (9, -0.05), (10, 0.06), (11, -0.056), (12, 0.041), (13, 0.129), (14, 0.027), (15, -0.016), (16, -0.167), (17, 0.166), (18, 0.062), (19, 0.063), (20, 0.032), (21, 0.047), (22, 0.041), (23, 0.076), (24, -0.03), (25, -0.024), (26, -0.004), (27, 0.091), (28, 0.057), (29, 0.04), (30, -0.104), (31, 0.009), (32, 0.041), (33, -0.033), (34, 0.019), (35, 0.057), (36, 0.093), (37, 0.013), (38, 0.048), (39, 0.016), (40, 0.019), (41, -0.029), (42, -0.011), (43, -0.002), (44, -0.035), (45, -0.086), (46, 0.041), (47, 0.026), (48, 0.004), (49, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96908933 <a title="208-lsi-1" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>2 0.89604223 <a title="208-lsi-2" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>3 0.88305283 <a title="208-lsi-3" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>Author: Yejin Choi ; Claire Cardie</p><p>Abstract: Automatic opinion recognition involves a number of related tasks, such as identifying the boundaries of opinion expression, determining their polarity, and determining their intensity. Although much progress has been made in this area, existing research typically treats each of the above tasks in isolation. In this paper, we apply a hierarchical parameter sharing technique using Conditional Random Fields for fine-grained opinion analysis, jointly detecting the boundaries of opinion expressions as well as determining two of their key attributes polarity and intensity. Our experimental results show that our proposed approach improves the performance over a baseline that does not — exploit hierarchical structure among the classes. In addition, we find that the joint approach outperforms a baseline that is based on cascading two separate components.</p><p>4 0.83342075 <a title="208-lsi-4" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>Author: Binyang Li ; Lanjun Zhou ; Shi Feng ; Kam-Fai Wong</p><p>Abstract: There is a growing research interest in opinion retrieval as on-line users’ opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user’s query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representa- , tion, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 1</p><p>5 0.6971283 <a title="208-lsi-5" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>6 0.42046961 <a title="208-lsi-6" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>7 0.39994889 <a title="208-lsi-7" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>8 0.38475594 <a title="208-lsi-8" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>9 0.37112018 <a title="208-lsi-9" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>10 0.36826023 <a title="208-lsi-10" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>11 0.34700361 <a title="208-lsi-11" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>12 0.32392788 <a title="208-lsi-12" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>13 0.31013229 <a title="208-lsi-13" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>14 0.30183697 <a title="208-lsi-14" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>15 0.29550919 <a title="208-lsi-15" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>16 0.29517326 <a title="208-lsi-16" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>17 0.29430601 <a title="208-lsi-17" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>18 0.29197925 <a title="208-lsi-18" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>19 0.28536481 <a title="208-lsi-19" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>20 0.2836844 <a title="208-lsi-20" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.022), (25, 0.047), (38, 0.188), (39, 0.012), (42, 0.126), (44, 0.01), (59, 0.059), (60, 0.01), (72, 0.043), (73, 0.065), (76, 0.017), (78, 0.027), (80, 0.026), (83, 0.124), (84, 0.028), (98, 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83871508 <a title="208-lda-1" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>2 0.73351514 <a title="208-lda-2" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>Author: Ryu Iida ; Syumpei Kobayashi ; Takenobu Tokunaga</p><p>Abstract: This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001 ; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on refer- ence resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.</p><p>3 0.72517037 <a title="208-lda-3" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>4 0.72423255 <a title="208-lda-4" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>5 0.7111662 <a title="208-lda-5" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>Author: Brian Pluss</p><p>Abstract: This paper presents ongoing research on computational models for non-cooperative dialogue. We start by analysing different levels of cooperation in conversation. Then, inspired by findings from an empirical study, we propose a technique for measuring non-cooperation in political interviews. Finally, we describe a research programme towards obtaining a suitable model and discuss previous accounts for conflictive dialogue, identifying the differences with our work.</p><p>6 0.70803583 <a title="208-lda-6" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>7 0.70417875 <a title="208-lda-7" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>8 0.6879614 <a title="208-lda-8" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>9 0.68325716 <a title="208-lda-9" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>10 0.68006796 <a title="208-lda-10" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>11 0.66635311 <a title="208-lda-11" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>12 0.6619544 <a title="208-lda-12" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>13 0.65632463 <a title="208-lda-13" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>14 0.65608817 <a title="208-lda-14" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>15 0.65247375 <a title="208-lda-15" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>16 0.64820957 <a title="208-lda-16" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>17 0.64425397 <a title="208-lda-17" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>18 0.64188832 <a title="208-lda-18" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>19 0.64073539 <a title="208-lda-19" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>20 0.63898993 <a title="208-lda-20" href="./acl-2010-Modeling_Semantic_Relevance_for_Question-Answer_Pairs_in_Web_Social_Communities.html">174 acl-2010-Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
