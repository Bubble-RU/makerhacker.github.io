<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-202" href="#">acl2010-202</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</h1>
<br/><p>Source: <a title="acl-2010-202-pdf" href="http://aclweb.org/anthology//P/P10/P10-1129.pdf">pdf</a></p><p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><p>Reference: <a title="acl-2010-202-reference" href="../acl2010_reference/acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Zettlemoyer, Regina Barzilay Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology  {branavan  ,  lz s  ,  Abstract In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. [sent-5, score-0.866]
</p><p>2 Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. [sent-6, score-0.554]
</p><p>3 We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. [sent-7, score-0.831]
</p><p>4 We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm  for text interpretation. [sent-8, score-0.727]
</p><p>5 1  1 Introduction In this paper, we introduce a novel method for mapping high-level instructions to commands in an external environment. [sent-10, score-0.818]
</p><p>6 These instructions specify goals to be achieved without explicitly stating all the required steps. [sent-11, score-0.528]
</p><p>7 This dependence on domain knowledge makes the automatic interpretation of high-level instructions particularly challenging. [sent-14, score-0.548]
</p><p>8 The standard approach to this task is to start with both a manually-developed model of the environment, and rules for interpreting high-level instructions in the context of this model (Agre and —  1Code, data, and annotations used in this work are available at http://groups. [sent-15, score-0.573]
</p><p>9 Our approach, in contrast, operates directly on the textual instructions in the context of the interactive environment, while requiring no additional information. [sent-23, score-0.494]
</p><p>10 By interacting with the environment and observing the resulting feedback, our method automatically learns both the mapping between the text and the commands, and the underlying model of the environment. [sent-24, score-0.63]
</p><p>11 One particularly noteworthy aspect of our solution is the interplay between the evolving mapping and the progressively acquired environment model as the system learns how to interpret the text. [sent-25, score-0.664]
</p><p>12 At the same time, the environment model enables the algorithm to consider the consequences ofcommands before they are ex-  ecuted, thereby improving the accuracy of interpretation. [sent-27, score-0.544]
</p><p>13 We apply our method to the task of mapping software troubleshooting guides to GUI actions in the Windows environment (Branavan et al. [sent-29, score-0.716]
</p><p>14 Second, we demonstrate that explicitly modeling the environment also greatly improves the accuracy of processing low-level instructions, yielding a 14% absolute increase in performance over a competitive baseline (Branavan et al. [sent-35, score-0.498]
</p><p>15 Finally, we show the importance of constructing an environment model relevant to the language interpretation task using textual —  1268  ProceedingUsp opfs thaela 4, 8Stwhe Adnennu,a 1l1- M16ee Jtiunlgy o 2f0 t1h0e. [sent-37, score-0.632]
</p><p>16 The mapping process involves segmenting the document into individual instruction word spans Wa, and translating each instruction into the sequence c of one or more commands it describes. [sent-40, score-0.864]
</p><p>17 instructions enables us to bias exploration toward transitions relevant for language learning. [sent-42, score-0.636]
</p><p>18 This approach yields superior performance compared to a policy that relies on an environment model constructed via random exploration. [sent-43, score-0.684]
</p><p>19 2  Related Work  Interpreting Instructions Our approach is most closely related to the reinforcement learning algorithm for mapping text instructions to commands developed by Branavan et al. [sent-44, score-1.019]
</p><p>20 Their method is predicated on the assumption that each command to be executed is explicitly specified in the instruction text. [sent-46, score-0.437]
</p><p>21 This assumption of a direct correspondence between the text and the environment is not unique to that pa-  per, being inherent in other work on grounded language learning (Siskind, 2001 ; Oates, 2001 ; Yu and Ballard, 2004; Fleischman and Roy, 2005; Mooney, 2008; Liang et al. [sent-47, score-0.498]
</p><p>22 (2009), which learns how an environment operates by reading text, rather than learning an explicit mapping from the text to the environment. [sent-51, score-0.654]
</p><p>23 For example, their method can learn the rules of a card game given instructions for how to play. [sent-52, score-0.471]
</p><p>24 Many instances of work on instruction interpretation are replete with examples where instructions are formulated as high-level goals, targeted at users with relevant knowledge (Winograd, 1972; Di Eugenio, 1992; Webber et al. [sent-53, score-0.768]
</p><p>25 Not surprisingly, automatic approaches for processing such instructions have relied on hand-engineered world knowledge to reason about the preconditions and effects of environment commands. [sent-56, score-0.967]
</p><p>26 The assumption of a fully specified environment model is also common in work on semantics in the linguistics lit-  erature (Lascarides approach learns to directed manner, it fication of relevant  and Asher, 2004). [sent-57, score-0.572]
</p><p>27 While our analyze instructions in a goaldoes not require manual specienvironment knowledge. [sent-58, score-0.471]
</p><p>28 The first approach, model-based learning, constructs a model of the environment in which the learner operates (e. [sent-60, score-0.52]
</p><p>29 It then computes a policy directly from the rich information represented in the induced environment model. [sent-63, score-0.656]
</p><p>30 However, if the environment cannot be accurately approximated by a compact representation, these methods perform poorly (Boyan and Moore, 1995; Jong and Stone, 2007). [sent-66, score-0.492]
</p><p>31 cLwolEiFmcrTkd_minCsLagpInCasdKnt(ar ts: art) Policy function  State Observed text and environment  IctSnyelicpltkeheice"ntdgorcpuosentmanarcfbtn. [sent-72, score-0.514]
</p><p>32 State s is comprised of the state of the external environment E, and the state of the document (d, W), where W is the list of all twheord st spans mapped by previous aecnttio En,s . [sent-75, score-0.771]
</p><p>33 a nAdn t haect siotante a sfe tlheect dso a span Wa ,oWf u)n, uwsheder we Words is f trhoem l (d, W), and maps them to an environment command c. [sent-76, score-0.733]
</p><p>34 As a consequence of a, the environment state changes to E0 ∼ p(E0 |E, c), and the list of mapped words is updated to W0 = W ∪ Wa. [sent-77, score-0.56]
</p><p>35 While pol-  icy learners can effectively operate in complex environments, they are not designed to benefit from a learned environment model. [sent-79, score-0.469]
</p><p>36 We address this limitation by expanding a policy learning algorithm to take advantage of a partial environment model estimated during learning. [sent-80, score-0.788]
</p><p>37 The approach of conditioning the policy function on future reachable states is similar in concept to the use of postdecision state information in the approximate dynamic programming framework (Powell, 2007). [sent-81, score-0.444]
</p><p>38 3  Problem Formulation  Our goal is to map instructions expressed in a natural language document d into the corresponding sequence of commands = hc1, . [sent-82, score-0.843]
</p><p>39 A =s input, we are given a set of raw instruction documents, an environment, and a reward function as described below. [sent-86, score-0.394]
</p><p>40 The environment is formalized as its states and transition function. [sent-87, score-0.62]
</p><p>41 The environment state transition function  c  p(E0|E, c) encodes how the state changes from E tpo( EE0| Ein, response tso a wcom thema sntadt ec c. [sent-90, score-0.773]
</p><p>42 3h During loemarn Eing, this function is not known, but samples from it can be collected by executing commands and ob3While in the general case the environment state transitions maybe stochastic, they are deterministic in the software GUI used in this work. [sent-91, score-1.051]
</p><p>43 A realvalued reward function measures how well a command sequence c achieves the task described in the document. [sent-93, score-0.454]
</p><p>44 4 Background Our innovation established mapping  takes place within a previously  general  framework  instructions  et al. [sent-102, score-0.557]
</p><p>45 for the task of  to commands  This framework  (Branavan  formalizes  the  mapping process as a Markov Decision Process (MDP)  (Sutton and Barto,  1998), with actions  encoding individual instruction-to-command mappings, and states representing tions of the document. [sent-104, score-0.606]
</p><p>46 1270  Figure 3: Using information derived from future states to interpret the high-level instruction “open control panel. [sent-108, score-0.386]
</p><p>47 Environment states are tsrhoolw pna as circles, with previously visited environment states colored green. [sent-110, score-0.665]
</p><p>48 All else being equal, the information that the control panel icon was observed in state E5 during previous exploration steps can help to correctly select command c3. [sent-112, score-0.554]
</p><p>49 Each action selects a word span from the document, and maps it to one environment command. [sent-114, score-0.636]
</p><p>50 To predict actions sequentially, we track the states of the environment and the document over time as shown in Figure 2. [sent-115, score-0.803]
</p><p>51 The mapping action a is a tuple (c, Wa) that represents the joint selection of a span of words Wa and an environment command c. [sent-118, score-0.915]
</p><p>52 Some of the candidate actions would correspond to the correct instruction mappings, e. [sent-119, score-0.353]
</p><p>53 The algorithm learns to interpret instructions by learning to construct sequences of actions that assign the correct com-  mands to the words. [sent-123, score-0.814]
</p><p>54 The interpretation of a document d begins at an initial mapping state s0 = (Ed, d, ∅), Ed being the starting state of the enviro=nm (Een,td f,o∅r) t,h Ee document. [sent-124, score-0.42]
</p><p>55 Given a state s = (E, d, W), the space of possibGleiv eanct aion ssta a = (c, Wa) iWs d),e ftihneed s by ee nofum poesrsait-ing sub-spans of unused words in d and candidate commands in E. [sent-125, score-0.375]
</p><p>56 6 A Log-Linear Parameterization The policy function used for action selection is defined as a log-linear distribution over actions:  p(a|s;θ) =Xeθe·φθ(·φs(,as,)a0),  (1)  Xa0 where θ ∈ Rn is a weight vector, and φ(s, a) ∈ Rn iws an enθ-d ∈im Rensiisoanawle fieghattuvree ftounr,c atinodn. [sent-132, score-0.374]
</p><p>57 The main challenge in processing these instructions is that, in contrast to their low-level counterparts, they correspond to sequences of one or more commands. [sent-140, score-0.519]
</p><p>58 However, this change significantly complicates the interpretation problem we need to be able to predict commands that are not directly described by any words, and allowing ac–  tion sequences significantly increases the space of possibilities for each instruction. [sent-146, score-0.386]
</p><p>59 To motivate the approach, consider the decision problem in Figure 3, where we need to find a command sequence for the high-level instruction “open control panel. [sent-148, score-0.506]
</p><p>60 ” The algorithm focuses on command sequences leading to environment states where the control panel icon was previously observed. [sent-149, score-1.033]
</p><p>61 The information about such states is acquired during exploration and is stored in a partial environment model q(E0 |E, c) . [sent-150, score-0.687]
</p><p>62 Our goal is to map high-level instructions to command sequences by leveraging knowledge about the long-term effects of commands. [sent-151, score-0.762]
</p><p>63 We do this by integrating the partial environment model into the policy function. [sent-152, score-0.735]
</p><p>64 Below, we first describe how we estimate the partial environment transition model and how this model is used to compute the look-ahead features. [sent-156, score-0.652]
</p><p>65 1 Partial Environment Transition Model To compute the look-ahead features, we first need to collect statistics about the environment transition function p(E0 |E, c). [sent-159, score-0.567]
</p><p>66 We collect this information through observation, and build a partial environment transition model q(E0|E, c). [sent-162, score-0.601]
</p><p>67 One possible strategy for constructing q is to observe the effects of executing random commands in the environment. [sent-163, score-0.399]
</p><p>68 During training, we execute the command sequences predicted by the policy function in the environment, caching the resulting state transitions. [sent-166, score-0.621]
</p><p>69 As learning progresses and the quality of the interpretation improves, more promising parts of the environment will be observed. [sent-168, score-0.575]
</p><p>70 Instead, we capitalize on the state transitions observed during the sampling process described above, allowing us to incrementally build  an environment model of actions and their effects. [sent-173, score-0.845]
</p><p>71 Based on this transition information, we can estimate the usefulness of actions by considering the properties of states they can reach. [sent-174, score-0.361]
</p><p>72 cTtihoins property is computed using the learned environment model, and is therefore an approximation. [sent-178, score-0.469]
</p><p>73 Because we can never encounter all states and all actions, our environment model is always incomplete and these properties can only be computed based on partial information. [sent-182, score-0.672]
</p><p>74 In particular, we select actions a based on the current state s and the partial environment model q, resulting in the following policy definition:  p(a|s;q,θ) =Xeθe·θφ·(φs(,as,,qa)0,q),  (3)  Xa0 where the feature representation φ(s, a, q) has been extended to be a function of q. [sent-185, score-1.032]
</p><p>75 3 Parameter Estimation The learning algorithm is provided with a set of documents d ∈ D, an environment in which to ex-  cn,  edcouctuem ceonmtsm da n∈d D sequences manendt a r ewwhaicrdh tfou enxc-tion r(h). [sent-187, score-0.629]
</p><p>76 The goal is to estimate two sets of parameters: 1) the parameters θ of the policy function, and 2) the partial environment transition model q(E0|E, c), which is the observed portion of mtheo terul eq Emo|Ede,lc p(E0 |E, c). [sent-188, score-0.834]
</p><p>77 An improved policy function in turn produces state samples that are more relevant to the document interpretation task. [sent-204, score-0.534]
</p><p>78 Environment States and Actions  In this appli-  cation of our model, the environment state is the set of visible user interface (UI) objects, along 7http://support. [sent-210, score-0.56]
</p><p>79 The environment commands consist of the UI commands left-click, right-click, double-click, and type-into. [sent-215, score-0.991]
</p><p>80 Since such verification is a challenging task, we rely on a noisy approximation: we assume that each sentence specifies at least one command, and that the text describing the command has words matching the label of the environment object. [sent-220, score-0.71]
</p><p>81 If a history h has at least one such command for each sen-  tence, the environment reward function r(h) returns a positive value, otherwise it returns a negative value. [sent-221, score-0.91]
</p><p>82 This environment reward function is a simplification of the one described in Branavan et al. [sent-222, score-0.671]
</p><p>83 These features are functions of both the text and environment state, modeling local properties that are useful for action selection. [sent-227, score-0.614]
</p><p>84 states with the lowest possible immediate reward, and use the induced environment model to encourage additional exploration by lowering the likelihood of actions that lead to such dead-end states. [sent-241, score-0.797]
</p><p>85 During the early stages of learning, experience gathered in the environment model is extremely  sparse, causing the look-ahead features to provide poor estimates. [sent-242, score-0.497]
</p><p>86 For example, executing an incorrect action early on, often leads to an environment state from which the remaining instructions cannot be completed. [sent-255, score-1.231]
</p><p>87 This discrepancy is explained by the fact that in this dataset, high-level instructions are often located towards the beginning of the document. [sent-270, score-0.471]
</p><p>88 If these initial challenging instructions are not processed correctly, the rest of the actions for the document cannot be interpreted. [sent-271, score-0.732]
</p><p>89 We also performed experiments to validate the intuition that the partial environment model must contain information relevant for the language interpretation task. [sent-275, score-0.653]
</p><p>90 To test this hypothesis, we replaced the learned environment model with one of the same size gathered by executing random commands. [sent-276, score-0.578]
</p><p>91 The model with randomly sampled environment transitions performs poorly: it can only process 4. [sent-277, score-0.57]
</p><p>92 This result also explains why training with full supervision hurts performance on highlevel instructions (see Table 1). [sent-281, score-0.497]
</p><p>93 Finally, to demonstrate the quality of the learned word–command alignments, we evaluate our method’s ability to paraphrase from high-level instructions to low-level instructions. [sent-285, score-0.471]
</p><p>94 We did this by finding high-level instructions where each of the commands they are associated with is also described by a low-level instruction in some other document. [sent-287, score-0.924]
</p><p>95 For example, if the text “open control panel” was mapped to the three commands in Figure 1, and each of those commands was described by a low-level instruction elsewhere, this procedure would create a paraphrase such as “click start, left click setting, and select control panel. [sent-288, score-0.908]
</p><p>96 ” Of the 60 highlevel instructions tagged in the test set, this approach found paraphrases for 33 of them. [sent-289, score-0.524]
</p><p>97 9  Conclusions and Future Work  In this paper, we demonstrate that knowledge about the environment can be learned and used effectively for the task ofmapping instructions to actions. [sent-292, score-0.94]
</p><p>98 A key feature of this approach is the synergy between language analysis and the construction of the environment model: instruction text drives the sampling of the environment transitions, while the acquired environment model facilitates language interpretation. [sent-293, score-1.651]
</p><p>99 This design enables us to learn to map high-level instructions while also improving accuracy on low-level instructions. [sent-294, score-0.494]
</p><p>100 An interest–  ing avenue of future work is to explore an alternative approach which learns these phenomena by combining linguistic information with knowledge gleaned from an automatically induced environment model. [sent-297, score-0.516]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('instructions', 0.471), ('environment', 0.469), ('commands', 0.261), ('command', 0.216), ('branavan', 0.192), ('instruction', 0.192), ('policy', 0.187), ('actions', 0.161), ('reward', 0.157), ('reinforcement', 0.148), ('action', 0.119), ('states', 0.098), ('state', 0.091), ('wa', 0.088), ('mapping', 0.086), ('panel', 0.085), ('executing', 0.081), ('interpretation', 0.077), ('document', 0.075), ('transitions', 0.073), ('gui', 0.072), ('click', 0.07), ('eugenio', 0.064), ('sutton', 0.063), ('control', 0.062), ('documents', 0.059), ('kushman', 0.054), ('transition', 0.053), ('partial', 0.051), ('sequences', 0.048), ('learns', 0.047), ('interpreting', 0.046), ('double', 0.045), ('function', 0.045), ('gradient', 0.043), ('singh', 0.043), ('exploration', 0.041), ('di', 0.039), ('barto', 0.038), ('grounding', 0.037), ('sequence', 0.036), ('agre', 0.036), ('boyan', 0.036), ('darken', 0.036), ('satinder', 0.036), ('webber', 0.035), ('interpret', 0.034), ('regina', 0.034), ('execute', 0.034), ('windows', 0.034), ('luke', 0.031), ('macmahon', 0.031), ('matuszek', 0.031), ('icon', 0.031), ('jong', 0.031), ('samples', 0.031), ('barbara', 0.031), ('dataset', 0.031), ('ui', 0.03), ('constructing', 0.03), ('explicitly', 0.029), ('learning', 0.029), ('zettlemoyer', 0.029), ('model', 0.028), ('goals', 0.028), ('relevant', 0.028), ('steps', 0.028), ('paraphrases', 0.027), ('posit', 0.027), ('effects', 0.027), ('properties', 0.026), ('lascarides', 0.026), ('eisenstein', 0.026), ('highlevel', 0.026), ('fleischman', 0.026), ('schatzmann', 0.026), ('span', 0.025), ('challenging', 0.025), ('nips', 0.024), ('synergy', 0.024), ('tso', 0.024), ('tab', 0.024), ('advanced', 0.024), ('microsoft', 0.024), ('algorithm', 0.024), ('maps', 0.023), ('reachable', 0.023), ('estimate', 0.023), ('st', 0.023), ('parameters', 0.023), ('accurately', 0.023), ('incrementally', 0.023), ('operates', 0.023), ('enables', 0.023), ('datasets', 0.023), ('stochastic', 0.023), ('history', 0.023), ('ee', 0.023), ('iws', 0.023), ('spans', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="202-tfidf-1" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><p>2 0.2985622 <a title="202-tfidf-2" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>Author: Adam Vogel ; Dan Jurafsky</p><p>Abstract: We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.</p><p>3 0.16772674 <a title="202-tfidf-3" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>4 0.15817258 <a title="202-tfidf-4" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>Author: Pierre Lison</p><p>Abstract: Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions ofthe state and action spaces.</p><p>5 0.15380006 <a title="202-tfidf-5" href="./acl-2010-Automated_Planning_for_Situated_Natural_Language_Generation.html">35 acl-2010-Automated Planning for Situated Natural Language Generation</a></p>
<p>Author: Konstantina Garoufi ; Alexander Koller</p><p>Abstract: We present a natural language generation approach which models, exploits, and manipulates the non-linguistic context in situated communication, using techniques from AI planning. We show how to generate instructions which deliberately guide the hearer to a location that is convenient for the generation of simple referring expressions, and how to generate referring expressions with context-dependent adjectives. We implement and evaluate our approach in the framework of the Challenge on Generating Instructions in Virtual Environments, finding that it performs well even under the constraints of realtime generation.</p><p>6 0.12113605 <a title="202-tfidf-6" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>7 0.087643482 <a title="202-tfidf-7" href="./acl-2010-P10-5005_k2opt.pdf.html">190 acl-2010-P10-5005 k2opt.pdf</a></p>
<p>8 0.084898941 <a title="202-tfidf-8" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>9 0.069148652 <a title="202-tfidf-9" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>10 0.068731621 <a title="202-tfidf-10" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>11 0.067086101 <a title="202-tfidf-11" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>12 0.054654229 <a title="202-tfidf-12" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>13 0.053252976 <a title="202-tfidf-13" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>14 0.0532046 <a title="202-tfidf-14" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>15 0.053133894 <a title="202-tfidf-15" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>16 0.052763514 <a title="202-tfidf-16" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>17 0.050687809 <a title="202-tfidf-17" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>18 0.048503898 <a title="202-tfidf-18" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>19 0.047745299 <a title="202-tfidf-19" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>20 0.046282243 <a title="202-tfidf-20" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.156), (1, 0.058), (2, -0.059), (3, -0.147), (4, -0.029), (5, -0.169), (6, -0.115), (7, 0.038), (8, 0.005), (9, 0.02), (10, -0.03), (11, -0.041), (12, 0.044), (13, 0.008), (14, -0.022), (15, -0.117), (16, 0.083), (17, 0.113), (18, -0.05), (19, 0.026), (20, 0.061), (21, -0.069), (22, -0.112), (23, 0.074), (24, 0.039), (25, -0.057), (26, -0.021), (27, 0.074), (28, -0.137), (29, -0.059), (30, 0.214), (31, -0.348), (32, 0.089), (33, 0.047), (34, -0.04), (35, 0.132), (36, -0.041), (37, 0.206), (38, -0.107), (39, -0.007), (40, 0.083), (41, -0.037), (42, 0.044), (43, 0.069), (44, -0.038), (45, 0.028), (46, 0.091), (47, -0.104), (48, 0.077), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96013635 <a title="202-lsi-1" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><p>2 0.94371849 <a title="202-lsi-2" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>Author: Adam Vogel ; Dan Jurafsky</p><p>Abstract: We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.</p><p>3 0.78053051 <a title="202-lsi-3" href="./acl-2010-Automated_Planning_for_Situated_Natural_Language_Generation.html">35 acl-2010-Automated Planning for Situated Natural Language Generation</a></p>
<p>Author: Konstantina Garoufi ; Alexander Koller</p><p>Abstract: We present a natural language generation approach which models, exploits, and manipulates the non-linguistic context in situated communication, using techniques from AI planning. We show how to generate instructions which deliberately guide the hearer to a location that is convenient for the generation of simple referring expressions, and how to generate referring expressions with context-dependent adjectives. We implement and evaluate our approach in the framework of the Challenge on Generating Instructions in Virtual Environments, finding that it performs well even under the constraints of realtime generation.</p><p>4 0.58774054 <a title="202-lsi-4" href="./acl-2010-P10-5005_k2opt.pdf.html">190 acl-2010-P10-5005 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: unkown-abstract</p><p>5 0.50163776 <a title="202-lsi-5" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>Author: Pierre Lison</p><p>Abstract: Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions ofthe state and action spaces.</p><p>6 0.3808094 <a title="202-lsi-6" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>7 0.37977314 <a title="202-lsi-7" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>8 0.37101358 <a title="202-lsi-8" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>9 0.34751201 <a title="202-lsi-9" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>10 0.32055673 <a title="202-lsi-10" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>11 0.2917721 <a title="202-lsi-11" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>12 0.28813437 <a title="202-lsi-12" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>13 0.28116226 <a title="202-lsi-13" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>14 0.27828336 <a title="202-lsi-14" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<p>15 0.24512252 <a title="202-lsi-15" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>16 0.23647478 <a title="202-lsi-16" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>17 0.22928284 <a title="202-lsi-17" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>18 0.22914186 <a title="202-lsi-18" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>19 0.22886869 <a title="202-lsi-19" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>20 0.22248298 <a title="202-lsi-20" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.057), (25, 0.059), (28, 0.011), (39, 0.013), (42, 0.033), (44, 0.019), (59, 0.097), (73, 0.054), (74, 0.237), (78, 0.027), (83, 0.092), (84, 0.057), (98, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90858781 <a title="202-lda-1" href="./acl-2010-SystemT%3A_An_Algebraic_Approach_to_Declarative_Information_Extraction.html">222 acl-2010-SystemT: An Algebraic Approach to Declarative Information Extraction</a></p>
<p>Author: Laura Chiticariu ; Rajasekar Krishnamurthy ; Yunyao Li ; Sriram Raghavan ; Frederick Reiss ; Shivakumar Vaithyanathan</p><p>Abstract: As information extraction (IE) becomes more central to enterprise applications, rule-based IE engines have become increasingly important. In this paper, we describe SystemT, a rule-based IE system whose basic design removes the expressivity and performance limitations of current systems based on cascading grammars. SystemT uses a declarative rule language, AQL, and an optimizer that generates high-performance algebraic execution plans for AQL rules. We compare SystemT’s approach against cascading grammars, both theoretically and with a thorough experimental evaluation. Our results show that SystemT can deliver result quality comparable to the state-of-the- art and an order of magnitude higher annotation throughput.</p><p>same-paper 2 0.81547415 <a title="202-lda-2" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><p>3 0.65464127 <a title="202-lda-3" href="./acl-2010-Combining_Orthogonal_Monolingual_and_Multilingual_Sources_of_Evidence_for_All_Words_WSD.html">62 acl-2010-Combining Orthogonal Monolingual and Multilingual Sources of Evidence for All Words WSD</a></p>
<p>Author: Weiwei Guo ; Mona Diab</p><p>Abstract: Word Sense Disambiguation remains one ofthe most complex problems facing computational linguists to date. In this paper we present a system that combines evidence from a monolingual WSD system together with that from a multilingual WSD system to yield state of the art performance on standard All-Words data sets. The monolingual system is based on a modification ofthe graph based state ofthe art algorithm In-Degree. The multilingual system is an improvement over an AllWords unsupervised approach, SALAAM. SALAAM exploits multilingual evidence as a means of disambiguation. In this paper, we present modifications to both of the original approaches and then their combination. We finally report the highest results obtained to date on the SENSEVAL 2 standard data set using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme.</p><p>4 0.65208447 <a title="202-lda-4" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>Author: Stephen Wu ; Asaf Bachrach ; Carlos Cardenas ; William Schuler</p><p>Abstract: Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.</p><p>5 0.64844835 <a title="202-lda-5" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>6 0.64691544 <a title="202-lda-6" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>7 0.6421535 <a title="202-lda-7" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>8 0.64208913 <a title="202-lda-8" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>9 0.64156455 <a title="202-lda-9" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>10 0.63988793 <a title="202-lda-10" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>11 0.63955295 <a title="202-lda-11" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>12 0.63936239 <a title="202-lda-12" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>13 0.6384905 <a title="202-lda-13" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>14 0.6378783 <a title="202-lda-14" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>15 0.63662481 <a title="202-lda-15" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>16 0.6352163 <a title="202-lda-16" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>17 0.63461804 <a title="202-lda-17" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>18 0.63430321 <a title="202-lda-18" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>19 0.63427347 <a title="202-lda-19" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>20 0.63427043 <a title="202-lda-20" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
