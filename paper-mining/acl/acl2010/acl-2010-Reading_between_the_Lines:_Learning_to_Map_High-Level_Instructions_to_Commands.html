<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-202" href="#">acl2010-202</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</h1>
<br/><p>Source: <a title="acl-2010-202-pdf" href="http://aclweb.org/anthology//P/P10/P10-1129.pdf">pdf</a></p><p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><p>Reference: <a title="acl-2010-202-reference" href="../acl2010_reference/acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('instruct', 0.529), ('environ', 0.458), ('command', 0.433), ('branav', 0.208), ('policy', 0.185), ('reward', 0.151), ('reinforc', 0.144), ('docu', 0.112), ('wa', 0.096), ('interpret', 0.095), ('transit', 0.089), ('execut', 0.089), ('panel', 0.086), ('map', 0.085), ('click', 0.08), ('gui', 0.078), ('eugenio', 0.069), ('sutton', 0.069), ('act', 0.063), ('kushm', 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="202-tfidf-1" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><p>2 0.42158234 <a title="202-tfidf-2" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>Author: Adam Vogel ; Dan Jurafsky</p><p>Abstract: We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.</p><p>3 0.24603593 <a title="202-tfidf-3" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>4 0.19497965 <a title="202-tfidf-4" href="./acl-2010-Automated_Planning_for_Situated_Natural_Language_Generation.html">35 acl-2010-Automated Planning for Situated Natural Language Generation</a></p>
<p>Author: Konstantina Garoufi ; Alexander Koller</p><p>Abstract: We present a natural language generation approach which models, exploits, and manipulates the non-linguistic context in situated communication, using techniques from AI planning. We show how to generate instructions which deliberately guide the hearer to a location that is convenient for the generation of simple referring expressions, and how to generate referring expressions with context-dependent adjectives. We implement and evaluate our approach in the framework of the Challenge on Generating Instructions in Virtual Environments, finding that it performs well even under the constraints of realtime generation.</p><p>5 0.13092251 <a title="202-tfidf-5" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>Author: Verena Rieser ; Oliver Lemon ; Xingkun Liu</p><p>Abstract: We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive “lower level” features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS.</p><p>6 0.11406258 <a title="202-tfidf-6" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>7 0.090718679 <a title="202-tfidf-7" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>8 0.089229248 <a title="202-tfidf-8" href="./acl-2010-P10-5005_k2opt.pdf.html">190 acl-2010-P10-5005 k2opt.pdf</a></p>
<p>9 0.081329554 <a title="202-tfidf-9" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>10 0.076777644 <a title="202-tfidf-10" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>11 0.066798069 <a title="202-tfidf-11" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>12 0.065752327 <a title="202-tfidf-12" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>13 0.064762332 <a title="202-tfidf-13" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>14 0.063806564 <a title="202-tfidf-14" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>15 0.063307688 <a title="202-tfidf-15" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>16 0.062841803 <a title="202-tfidf-16" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>17 0.059954219 <a title="202-tfidf-17" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>18 0.059834797 <a title="202-tfidf-18" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>19 0.054352012 <a title="202-tfidf-19" href="./acl-2010-An_Open-Source_Package_for_Recognizing_Textual_Entailment.html">30 acl-2010-An Open-Source Package for Recognizing Textual Entailment</a></p>
<p>20 0.05288187 <a title="202-tfidf-20" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.164), (1, -0.071), (2, -0.043), (3, -0.143), (4, 0.011), (5, 0.175), (6, -0.179), (7, -0.01), (8, -0.008), (9, 0.005), (10, 0.014), (11, -0.048), (12, 0.09), (13, -0.014), (14, -0.012), (15, 0.114), (16, 0.015), (17, -0.0), (18, 0.095), (19, -0.041), (20, 0.012), (21, 0.132), (22, -0.078), (23, -0.005), (24, 0.25), (25, -0.093), (26, 0.193), (27, -0.344), (28, 0.083), (29, -0.122), (30, 0.106), (31, -0.093), (32, -0.007), (33, -0.164), (34, -0.157), (35, -0.178), (36, -0.081), (37, 0.0), (38, -0.056), (39, 0.0), (40, -0.042), (41, 0.052), (42, -0.079), (43, -0.045), (44, 0.026), (45, 0.141), (46, -0.084), (47, -0.036), (48, -0.072), (49, -0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94430721 <a title="202-lsi-1" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><p>2 0.90322638 <a title="202-lsi-2" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>Author: Adam Vogel ; Dan Jurafsky</p><p>Abstract: We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.</p><p>3 0.75616622 <a title="202-lsi-3" href="./acl-2010-Automated_Planning_for_Situated_Natural_Language_Generation.html">35 acl-2010-Automated Planning for Situated Natural Language Generation</a></p>
<p>Author: Konstantina Garoufi ; Alexander Koller</p><p>Abstract: We present a natural language generation approach which models, exploits, and manipulates the non-linguistic context in situated communication, using techniques from AI planning. We show how to generate instructions which deliberately guide the hearer to a location that is convenient for the generation of simple referring expressions, and how to generate referring expressions with context-dependent adjectives. We implement and evaluate our approach in the framework of the Challenge on Generating Instructions in Virtual Environments, finding that it performs well even under the constraints of realtime generation.</p><p>4 0.55103457 <a title="202-lsi-4" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>5 0.47439811 <a title="202-lsi-5" href="./acl-2010-P10-5005_k2opt.pdf.html">190 acl-2010-P10-5005 k2opt.pdf</a></p>
<p>Author: empty-author</p><p>Abstract: unkown-abstract</p><p>6 0.41712362 <a title="202-lsi-6" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>7 0.36001086 <a title="202-lsi-7" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>8 0.35356215 <a title="202-lsi-8" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>9 0.33308455 <a title="202-lsi-9" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>10 0.32319769 <a title="202-lsi-10" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>11 0.27957928 <a title="202-lsi-11" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>12 0.26416048 <a title="202-lsi-12" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>13 0.25352523 <a title="202-lsi-13" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>14 0.21517283 <a title="202-lsi-14" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>15 0.21515696 <a title="202-lsi-15" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>16 0.21327317 <a title="202-lsi-16" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>17 0.21142405 <a title="202-lsi-17" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>18 0.20707226 <a title="202-lsi-18" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>19 0.20470186 <a title="202-lsi-19" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>20 0.2026355 <a title="202-lsi-20" href="./acl-2010-Online_Generation_of_Locality_Sensitive_Hash_Signatures.html">183 acl-2010-Online Generation of Locality Sensitive Hash Signatures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.028), (14, 0.011), (29, 0.043), (40, 0.04), (42, 0.011), (52, 0.148), (54, 0.144), (56, 0.045), (68, 0.03), (71, 0.05), (84, 0.112), (96, 0.091), (97, 0.156)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85817248 <a title="202-lda-1" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>Author: S.R.K. Branavan ; Luke Zettlemoyer ; Regina Barzilay</p><p>Abstract: In this paper, we address the task of mapping high-level instructions to sequences of commands in an external environment. Processing these instructions is challenging—they posit goals to be achieved without specifying the steps required to complete them. We describe a method that fills in missing information using an automatically derived environment model that encodes states, transitions, and commands that cause these transitions to happen. We present an efficient approximate approach for learning this environment model as part of a policygradient reinforcement learning algorithm for text interpretation. This design enables learning for mapping high-level instructions, which previous statistical methods cannot handle.1</p><p>2 0.83924687 <a title="202-lda-2" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>Author: Adam Vogel ; Dan Jurafsky</p><p>Abstract: We present a system that learns to follow navigational natural language directions. Where traditional models learn from linguistic annotation or word distributions, our approach is grounded in the world, learning by apprenticeship from routes through a map paired with English descriptions. Lacking an explicit alignment between the text and the reference path makes it difficult to determine what portions of the language describe which aspects of the route. We learn this correspondence with a reinforcement learning algorithm, using the deviation of the route we follow from the intended path as a reward signal. We demonstrate that our system successfully grounds the meaning of spatial terms like above and south into geometric properties of paths.</p><p>3 0.82344747 <a title="202-lda-3" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>Author: Myroslava O. Dzikovska ; Johanna D. Moore ; Natalie Steinhauser ; Gwendolyn Campbell</p><p>Abstract: Supporting natural language input may improve learning in intelligent tutoring systems. However, interpretation errors are unavoidable and require an effective recovery policy. We describe an evaluation of an error recovery policy in the BEETLE II tutorial dialogue system and discuss how different types of interpretation problems affect learning gain and user satisfaction. In particular, the problems arising from student use of non-standard terminology appear to have negative consequences. We argue that existing strategies for dealing with terminology problems are insufficient and that improving such strategies is important in future ITS research.</p><p>4 0.81249613 <a title="202-lda-4" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>Author: Hiroshi Echizen-ya ; Kenji Araki</p><p>Abstract: As described in this paper, we propose a new automatic evaluation method for machine translation using noun-phrase chunking. Our method correctly determines the matching words between two sentences using corresponding noun phrases. Moreover, our method determines the similarity between two sentences in terms of the noun-phrase order of appearance. Evaluation experiments were conducted to calculate the correlation among human judgments, along with the scores produced us- ing automatic evaluation methods for MT outputs obtained from the 12 machine translation systems in NTCIR7. Experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and fluency.</p><p>5 0.78984529 <a title="202-lda-5" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>Author: Weiwei Sun</p><p>Abstract: Developing features has been shown crucial to advancing the state-of-the-art in Semantic Role Labeling (SRL). To improve Chinese SRL, we propose a set of additional features, some of which are designed to better capture structural information. Our system achieves 93.49 Fmeasure, a significant improvement over the best reported performance 92.0. We are further concerned with the effect of parsing in Chinese SRL. We empirically analyze the two-fold effect, grouping words into constituents and providing syntactic information. We also give some preliminary linguistic explanations.</p><p>6 0.78866756 <a title="202-lda-6" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>7 0.78735614 <a title="202-lda-7" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>8 0.75708777 <a title="202-lda-8" href="./acl-2010-Semantics-Driven_Shallow_Parsing_for_Chinese_Semantic_Role_Labeling.html">207 acl-2010-Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling</a></p>
<p>9 0.75095737 <a title="202-lda-9" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>10 0.74111187 <a title="202-lda-10" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>11 0.73950326 <a title="202-lda-11" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>12 0.73624504 <a title="202-lda-12" href="./acl-2010-BabelNet%3A_Building_a_Very_Large_Multilingual_Semantic_Network.html">44 acl-2010-BabelNet: Building a Very Large Multilingual Semantic Network</a></p>
<p>13 0.73444784 <a title="202-lda-13" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>14 0.73094952 <a title="202-lda-14" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>15 0.73005784 <a title="202-lda-15" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>16 0.72350037 <a title="202-lda-16" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>17 0.72332609 <a title="202-lda-17" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>18 0.72300446 <a title="202-lda-18" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>19 0.7226451 <a title="202-lda-19" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>20 0.72116679 <a title="202-lda-20" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
