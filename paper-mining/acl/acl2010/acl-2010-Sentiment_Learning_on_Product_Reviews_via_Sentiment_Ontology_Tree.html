<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-209" href="#">acl2010-209</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</h1>
<br/><p>Source: <a title="acl-2010-209-pdf" href="http://aclweb.org/anthology//P/P10/P10-1042.pdf">pdf</a></p><p>Author: Wei Wei ; Jon Atle Gulla</p><p>Abstract: Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products.</p><p>Reference: <a title="acl-2010-209-reference" href="../acl2010_reference/acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 no @ Abstract Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. [sent-3, score-1.324]
</p><p>2 In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a  defined Sentiment Ontology Tree (SOT). [sent-5, score-0.64]
</p><p>3 While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products. [sent-7, score-1.019]
</p><p>4 The usergenerated opinion-rich reviews will not only help other users make better judgements but they are also useful resources for manufacturers of products to keep track and manage customer opinions. [sent-9, score-0.287]
</p><p>5 However, as the number of product reviews grows, it becomes difficult for a user to manually learn the panorama of an interesting topic from existing online information. [sent-10, score-0.403]
</p><p>6 , 2009), of sentiment analysis on product reviews were proposed and have become a popular research topic at the crossroads of information retrieval and computational linguistics. [sent-15, score-0.967]
</p><p>7 no Carrying out sentiment analysis on product reviews is not a trivial task. [sent-18, score-0.902]
</p><p>8 The product’s attributes mentioned in reviews might have some relationships between each other. [sent-26, score-0.435]
</p><p>9 However, a sentence like “40D handles noise very well up to ISO 800”, also refers to image quality of the camera 40D. [sent-28, score-0.291]
</p><p>10 We argue that the hierarchical relationship between a product’s attributes can be useful knowledge if it can be formulated and utilized in product reviews analysis. [sent-30, score-0.694]
</p><p>11 Secondly, Vocabularies used in product reviews tend to be highly overlapping. [sent-31, score-0.376]
</p><p>12 Especially, for same attribute, usually same words or synonyms are involved to refer to them and to describe sentiment on them. [sent-32, score-0.489]
</p><p>13 We believe that labeling existing product reviews with attributes and corresponding sentiment forms an effective training resource to perform sentiment analysis. [sent-33, score-1.555]
</p><p>14 Thirdly, sentiments expressed in a review or even in a  sentence might be opposite on different attributes and not every attributes mentioned are with sentiments. [sent-34, score-0.463]
</p><p>15 I am very impressed with this camera exceptfor its a bit heavy weight especially with 404  ProceedinUgspp osfa tlhae, 4S8wthed Aennn,u 1a1l-1 M6e Jeutilnyg 2 o0f1 t0h. [sent-38, score-0.247]
</p><p>16 c As2s0o1c0ia Atisosnoc foiart Cionom fopru Ctaotmiopnuatla Lti onngaulis Lti cnsg,u piasgtiecss 404–413,  Figure 1: an example of part of a SOT for digital camera  extra lenses attached. [sent-40, score-0.333]
</p><p>17 Even if the words “lenses” appears in the review, it is not fair to say the customer expresses any sentiment on lens. [sent-47, score-0.534]
</p><p>18 It’s also not feasible to try to get any sentiment from these contents. [sent-49, score-0.489]
</p><p>19 We argue that when performing sentiment analysis on reviews, such as in the Example 1, more attention is needed to distinguish between attributes that are mentioned with and without sentiment. [sent-50, score-0.697]
</p><p>20 In this paper, we study the problem of sentiment analysis on product reviews through a novel method, called the HL-SOT approach, namely Hierarchical Learning (HL) with Sentiment Ontology Tree (SOT). [sent-51, score-0.902]
</p><p>21 By sentiment analysis on product reviews we aim to fulfill two tasks, i. [sent-52, score-0.902]
</p><p>22 , labeling a target text1 with: 1) the product’s attributes (attributes identification task), and 2) their corresponding sentiments mentioned therein (sentiment annotation task). [sent-54, score-0.294]
</p><p>23 The result of this kind of labeling process is quite useful because it makes it possible for a user to search reviews on particular attributes of a product. [sent-55, score-0.399]
</p><p>24 The root node of the SOT is 1Each product review to be analyzed is called target text in the following of this paper. [sent-60, score-0.398]
</p><p>25 2Due to the space limitation, not all attributes of a digital camera are enumerated in this SOT; m+/m- means posia camera itself. [sent-61, score-0.69]
</p><p>26 All leaf nodes (gray nodes) of the SOT represent sentiment (positive/negative) nodes respectively associated with their parent nodes. [sent-63, score-0.703]
</p><p>27 With the proposed concept of SOT, we manage to formulate the two tasks of the sentiment analysis to be a hierarchical classification problem. [sent-66, score-0.777]
</p><p>28 This property makes the approach well suited for the situation where complicated sentiments on different attributes are expressed in one target text. [sent-70, score-0.264]
</p><p>29 This paper makes the following contributions: • To the best of our knowledge, with the proposed concept o ofu SOT, twhele proposed H thLe-S pOroTapproach is the first work to formulate the tasks of sentiment analysis to be a hierarchical classification problem. [sent-74, score-0.815]
</p><p>30 • •  A  A specific hierarchical learning algorithm is specifi  tive/negative sentiment associated with an attribute m. [sent-75, score-0.772]
</p><p>31 3A product itself can be treated as an overall attribute of the product. [sent-76, score-0.314]
</p><p>32 405  further proposed to achieve tasks of sentiment analysis in one hierarchical classification process. [sent-77, score-0.737]
</p><p>33 •  The proposed HL-SOT approach can be geneTrhaeliz perodp to emda kHeL i-tS possible to perform es genetni-ment analysis on target texts that are a mix of reviews of different products, whereas existing works mainly focus on analyzing reviews of only one type of product. [sent-78, score-0.599]
</p><p>34 In Section 2, we provide an overview of related work on sentiment analysis. [sent-80, score-0.489]
</p><p>35 Section 3 presents our work on sentiment analysis with HLSOT approach. [sent-81, score-0.526]
</p><p>36 2  Related Work  The task of sentiment analysis on product reviews  was originally performed to extract overall sentiment from the target texts. [sent-83, score-1.421]
</p><p>37 However, in (Turney, 2002), as the difficulty shown in the experiments, the whole sentiment of a document is not necessarily the sum of its parts. [sent-84, score-0.489]
</p><p>38 Then there came up with research works shifting focus from overall document sentiment to sentiment analysis based on product attributes (Hu and Liu, 2004; Popescu and Etzioni, 2005; Ding and Liu, 2007; Liu et al. [sent-85, score-1.393]
</p><p>39 Document overall sentiment analysis is to summarize the overall sentiment in the document. [sent-87, score-1.015]
</p><p>40 Research works related to document overall sentiment analysis mainly rely on two finer levels sentiment annotation: word-level sentiment annotation and phrase-level sentiment annotation. [sent-88, score-2.022]
</p><p>41 The phrase-level sentiment annotation focuses sentiment annotation on phrases not words with concerning that atomic units of expression is not individual words but rather appraisal groups (Whitelaw et al. [sent-91, score-1.006]
</p><p>42 This paper presented a system that is able to automatically identify the contextual polarity for a large subset of sentiment expressions. [sent-95, score-0.577]
</p><p>43 In (Turney, 2002), an unsupervised learning algorithm was proposed to classify reviews as recommended or not recommended by averaging sentiment annotation of phrases in reviews that contain adjectives or adverbs. [sent-96, score-0.958]
</p><p>44 However, the performances of these works are not good enough for sentiment analysis on product reviews, where sentiment on each attribute of a product could be so complicated that it is unable to be expressed by overall document sentiment. [sent-97, score-1.536]
</p><p>45 Attributes-based sentiment analysis is to ana-  lyze sentiment based on each attribute of a product. [sent-98, score-1.151]
</p><p>46 In (Hu and Liu, 2004), mining product features was proposed together with sentiment polarity annotation for each opinion sentence. [sent-99, score-0.892]
</p><p>47 In that work, sentiment analysis was performed on product attributes level. [sent-100, score-0.875]
</p><p>48 The system made users be able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. [sent-103, score-0.356]
</p><p>49 In (Popescu and Etzioni, 2005), Popescu and Etzioni not only analyzed polarity of opinions regarding product features but also ranked opinions based on their strength. [sent-104, score-0.372]
</p><p>50 proposed Sentiment-PLSA that analyzed blog entries and viewed them as a document generated by a number of hidden sentiment factors. [sent-107, score-0.553]
</p><p>51 These sentiment factors may also be factors based on product attributes. [sent-108, score-0.667]
</p><p>52 The work in (Titov and McDonald, 2008) presented a multi-grain topic model for extracting the ratable attributes from product reviews. [sent-111, score-0.376]
</p><p>53 All these research works concentrated on attribute-based sentiment analysis. [sent-114, score-0.518]
</p><p>54 However, the main difference with our work is that they did not sufficiently utilize the hierarchical relationships among a product attributes. [sent-115, score-0.356]
</p><p>55 In the contrast, our work solves the sentiment analysis problem as a hierarchical classification problem that fully utilizes the  hierarchy of the SOT during training and classification process. [sent-117, score-0.76]
</p><p>56 In this novel approach, tasks of sentiment analysis are to be achieved in a hierarchical classification process. [sent-120, score-0.699]
</p><p>57 1 Sentiment Ontology Tree As we discussed in Section 1, the hierarchial relationships among a product’s attributes might help improve the performance of attribute-based sentiment analysis. [sent-122, score-0.768]
</p><p>58 is a positive sentiment leaf node associated with the attribute v. [sent-129, score-0.744]
</p><p>59 v−is a negative sentiment leaf node associated with the attribute v. [sent-130, score-0.744]
</p><p>60 The SOT’s two leaf child nodes are sentiment (positive/negative) nodes associated with the root attribute. [sent-134, score-0.716]
</p><p>61 This definition successfully describes the hierarchical relationships among all the attributes of a product. [sent-136, score-0.349]
</p><p>62 1the root node ofthe SOT for a digital camera is its general overview attribute. [sent-138, score-0.407]
</p><p>63 Comments on a digital camera’s general overview attribute appearing in a review might be like “this camera is great”. [sent-139, score-0.495]
</p><p>64 The “camera” SOT has two sentiment leaf child nodes as well as three non-leaf child nodes which are respectively root nodes of sub-SOTs for  sub-attributes “design and usability”, “image quality”, and “lens”. [sent-140, score-0.798]
</p><p>65 These sub-attributes SOTs recursively repeat until each node in the SOT does not have any more non-leaf child node, which means the corresponding attributes do not have any sub-attributes, e. [sent-141, score-0.266]
</p><p>66 With the defined SOT, the problem of sentiment analysis is able to be formulated to be a hierarchial classification problem. [sent-147, score-0.664]
</p><p>67 tTinheg requirement ol fv a generated lthabaetl r evsepcetocrts y ∈ Y ensures tehmate a target teenxetr aist etod bl aeb elalb veelecdto rw yith ∈ a Yno edneonly if its parent attribute node is labeled with the target text. [sent-171, score-0.317]
</p><p>68 Therefore we propose a specific hierarchical learning algorithm, named HL-SOT algorithm, that is able to train each node classifier in a batch-learning setting and allows separately learning for the threshold of each node classifier. [sent-182, score-0.423]
</p><p>69 Then the hierarchical classification function f is parameterized by the weight matrix W = (w1, . [sent-199, score-0.268]
</p><p>70 Then the label vector ˆy rt is computed for the instance rt, before the real label vector lrt is observed. [sent-226, score-0.303]
</p><p>71 Then the current threshold vector θt is updated by: θt+1 = θt + ϵ(ˆ yrt − lrt),  (2)  where ϵ is a small positive real number that denotes a corrective step for correcting the current threshold vector θt. [sent-227, score-0.497]
</p><p>72 T−he l Formula 2 correct the current threshold θi,t for the classifier iin the following way: •  If yi′,t = 0, it means the classifier imade a proper classification for the current instance rt. [sent-230, score-0.323]
</p><p>73 •  •  If yi′,t = 1, it means the classifier imade an improper classification by mistakenly identifying the attribute i of the training instance rt that should have not been identified. [sent-232, score-0.391]
</p><p>74 This indicates the value of θi is not big enough to serve as a threshold so that the attribute iin this case can be filtered out by the classifier i. [sent-233, score-0.334]
</p><p>75 If yi′,t = −1, it means the classifier imade an improper c1la,s itsi mficeaatniosn th by failing to identify the attribute iof the training instance rt that should have been identified. [sent-235, score-0.33]
</p><p>76 This indicates the value of θi is not small enough to serve as a threshold so that the attribute iin this case 408  Algorithm  1  Algorithm 1Hierarchical Learning Algorithm HL-SOT INITIALIZATION: 1: Each vector wi,1 , i = 1, . [sent-236, score-0.294]
</p><p>77 N do 6: Update each row wi,t of weight matrix Wt by Formula 1 7: end for 8: Compute ˆy rt = f(rt) = g(Wt · rt) 9: Observe label vector lrt ∈ Y orf the instance rt 10: Update threshold vector θt by Formula 2 11: end for END can be recognized by the classifier i. [sent-248, score-0.572]
</p><p>78 (3) how does  the corrective step ϵ impact the performance of the proposed approach? [sent-258, score-0.285]
</p><p>79 1 Data Set Preparation The data set contains 1446 snippets of customer reviews on digital cameras that are collected from a customer review website4. [sent-261, score-0.469]
</p><p>80 We manually construct a SOT for the product of digital cameras. [sent-262, score-0.261]
</p><p>81 1) contains 105 nodes that include 35 non-leaf nodes representing attributes of the digital camera and 70 leaf nodes representing associated sentiments with attribute nodes. [sent-266, score-0.885]
</p><p>82 Then we label all the snippets with corresponding labels of nodes in the constructed SOT complying with the rule that a target text is to be labeled with a node only if its parent attribute node is labeled with the target text. [sent-267, score-0.505]
</p><p>83 2 Evaluation Metrics Since the proposed HL-SOT approach is a hierarchical classification process, we use three classic loss functions for measuring classification performance. [sent-272, score-0.327]
</p><p>84 Unlike the O-Loss function and the S-Loss function, the H-Loss function captures the intuition that loss should only be charged on a node when-  ever a classification mistake is made on a node of SOT but no more should be charged for any additional mistake occurring in the subtree of that node. [sent-301, score-0.43]
</p><p>85 In the training process of HL-flat, the algorithm reflexes the restriction in the  •  HL-SOT algorithm that requires the weight vector wi,t of the classifier iis only updated on the examples that are positive for its parent node. [sent-306, score-0.283]
</p><p>86 Unlike our proposed HL-SOT algorithm that enables the threshold values to be learned separately for each classifiers in the training process, the H-RLS algorithm only uses an identical threshold values for each classifiers in the classification process. [sent-309, score-0.374]
</p><p>87 4 Impact of Corrective Step ϵ The parameter ϵ in the proposed HL-SOT approach controls the corrective step of the classifiers’ thresholds when any mistake is observed in the training process. [sent-319, score-0.282]
</p><p>88 Hence, the corrective step ϵ is a factor that might impact the performance of the proposed approach. [sent-328, score-0.285]
</p><p>89 However, a fine-grained corrective step generally makes a better performance than a coarse-grained corrective step. [sent-342, score-0.389]
</p><p>90 5  Conclusions, Discussions and Future  Work In this paper, we propose a novel and effective approach to sentiment analysis on product reviews. [sent-355, score-0.704]
</p><p>91 In our proposed HL-SOT approach, we define SOT to formulate the knowledge of hierarchical relationships among a product’s attributes and tackle the problem of sentiment analysis in a hierarchical classification process with the proposed algorithm. [sent-356, score-1.164]
</p><p>92 This confirms two intuitive motivations based on which our approach is proposed: 1) separately learning threshold values for 411  each classifier improve the classification accuracy; 2) knowledge of hierarchical relationships of labels improve the approach’s performance. [sent-359, score-0.416]
</p><p>93 The experiments on analyzing the impact of parameter ϵ indicate that a fine-grained corrective step gen-  erally makes a better performance than a coarsegrained corrective step. [sent-360, score-0.468]
</p><p>94 In this generalization for sentiment analysis on multiple products reviews, a “big” SOT is constructed and the SOT for each product reviews is a sub-tree of the “big” SOT. [sent-364, score-0.946]
</p><p>95 The sentiment analysis on multiple products reviews can be performed the same way the HL-SOT approach is applied on single product reviews and can be tackled in a hierarchical classification process with the “big” SOT. [sent-365, score-1.317]
</p><p>96 This paper is motivated by the fact that the relationships among a product’s attributes could be a useful knowledge for mining product review texts. [sent-366, score-0.514]
</p><p>97 However, what  attributes to be included in a product’s SOT and how to structure these attributes in the SOT is an effort of human beings. [sent-368, score-0.342]
</p><p>98 In addition, an automatic method to learn a product’s attributes and the structure of SOT from existing product review texts will greatly benefit the efficiency of the proposed approach. [sent-371, score-0.472]
</p><p>99 Mining the peanut gallery: opinion extraction and semantic classification of product reviews. [sent-387, score-0.297]
</p><p>100 Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. [sent-470, score-0.244]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sot', 0.53), ('sentiment', 0.489), ('camera', 0.218), ('reviews', 0.198), ('corrective', 0.181), ('product', 0.178), ('attributes', 0.171), ('attribute', 0.136), ('hierarchical', 0.112), ('hlsot', 0.097), ('rt', 0.093), ('polarity', 0.088), ('digital', 0.083), ('threshold', 0.081), ('ontology', 0.078), ('lu', 0.076), ('image', 0.073), ('node', 0.067), ('relationships', 0.066), ('sentiments', 0.063), ('liu', 0.063), ('classification', 0.061), ('dimensionality', 0.059), ('opinion', 0.058), ('review', 0.058), ('lrt', 0.056), ('loss', 0.055), ('parent', 0.054), ('nodes', 0.054), ('classifier', 0.053), ('leaf', 0.052), ('vector', 0.05), ('dial', 0.048), ('imade', 0.048), ('sots', 0.048), ('esuli', 0.046), ('index', 0.045), ('customer', 0.045), ('hu', 0.044), ('products', 0.044), ('orientation', 0.043), ('separately', 0.043), ('hierarchial', 0.042), ('mining', 0.041), ('yi', 0.041), ('snippets', 0.04), ('hatzivassiloglou', 0.04), ('analyzing', 0.04), ('formulate', 0.04), ('matrix', 0.04), ('opinions', 0.04), ('root', 0.039), ('impact', 0.039), ('popescu', 0.039), ('proposed', 0.038), ('analysis', 0.037), ('big', 0.037), ('mistake', 0.036), ('algorithm', 0.035), ('formulated', 0.035), ('www', 0.035), ('bremen', 0.032), ('chaovalit', 0.032), ('idi', 0.032), ('impacted', 0.032), ('lenses', 0.032), ('ontologysupported', 0.032), ('titov', 0.031), ('vasileios', 0.031), ('etzioni', 0.03), ('target', 0.03), ('labeling', 0.03), ('mix', 0.029), ('folds', 0.029), ('works', 0.029), ('weight', 0.029), ('child', 0.028), ('zhai', 0.028), ('appraisal', 0.028), ('devitt', 0.028), ('aof', 0.028), ('humanlabeled', 0.028), ('charged', 0.028), ('norwegian', 0.028), ('andreevskaia', 0.028), ('kamps', 0.028), ('world', 0.028), ('label', 0.027), ('efficiency', 0.027), ('updated', 0.027), ('step', 0.027), ('iin', 0.027), ('topic', 0.027), ('demonstrates', 0.026), ('function', 0.026), ('turney', 0.026), ('analyzed', 0.026), ('whitelaw', 0.026), ('dave', 0.026), ('ey', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="209-tfidf-1" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>Author: Wei Wei ; Jon Atle Gulla</p><p>Abstract: Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products.</p><p>2 0.30379194 <a title="209-tfidf-2" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>Author: Christian Scheible</p><p>Abstract: The translation of sentiment information is a task from which sentiment analysis systems can benefit. We present a novel, graph-based approach using SimRank, a well-established vertex similarity algorithm to transfer sentiment information between a source language and a target language graph. We evaluate this method in comparison with SO-PMI.</p><p>3 0.24795395 <a title="209-tfidf-3" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>4 0.24680398 <a title="209-tfidf-4" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>Author: Hitoshi Nishikawa ; Takaaki Hasegawa ; Yoshihiro Matsuo ; Genichiro Kikui</p><p>Abstract: We propose a novel algorithm for sentiment summarization that takes account of informativeness and readability, simultaneously. Our algorithm generates a summary by selecting and ordering sentences taken from multiple review texts according to two scores that represent the informativeness and readability of the sentence order. The informativeness score is defined by the number of sentiment expressions and the readability score is learned from the target corpus. We evaluate our method by summarizing reviews on restaurants. Our method outperforms an existing algorithm as indicated by its ROUGE score and human readability experiments.</p><p>5 0.21288849 <a title="209-tfidf-5" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>Author: Binyang Li ; Lanjun Zhou ; Shi Feng ; Kam-Fai Wong</p><p>Abstract: There is a growing research interest in opinion retrieval as on-line users’ opinions are becoming more and more popular in business, social networks, etc. Practically speaking, the goal of opinion retrieval is to retrieve documents, which entail opinions or comments, relevant to a target subject specified by the user’s query. A fundamental challenge in opinion retrieval is information representation. Existing research focuses on document-based approaches and documents are represented by bag-of-word. However, due to loss of contextual information, this representation fails to capture the associative information between an opinion and its corresponding target. It cannot distinguish different degrees of a sentiment word when associated with different targets. This in turn seriously affects opinion retrieval performance. In this paper, we propose a sentence-based approach based on a new information representa- , tion, namely topic-sentiment word pair, to capture intra-sentence contextual information between an opinion and its target. Additionally, we consider inter-sentence information to capture the relationships among the opinions on the same topic. Finally, the two types of information are combined in a unified graph-based model, which can effectively rank the documents. Compared with existing approaches, experimental results on the COAE08 dataset showed that our graph-based model achieved significant improvement. 1</p><p>6 0.21248084 <a title="209-tfidf-6" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>7 0.16461152 <a title="209-tfidf-7" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>8 0.1502146 <a title="209-tfidf-8" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>9 0.15012529 <a title="209-tfidf-9" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>10 0.1474953 <a title="209-tfidf-10" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>11 0.144263 <a title="209-tfidf-11" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>12 0.14004947 <a title="209-tfidf-12" href="./acl-2010-Cross_Lingual_Adaptation%3A_An_Experiment_on_Sentiment_Classifications.html">80 acl-2010-Cross Lingual Adaptation: An Experiment on Sentiment Classifications</a></p>
<p>13 0.1346228 <a title="209-tfidf-13" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>14 0.12920475 <a title="209-tfidf-14" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>15 0.11778128 <a title="209-tfidf-15" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>16 0.11047599 <a title="209-tfidf-16" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>17 0.10817326 <a title="209-tfidf-17" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>18 0.068425119 <a title="209-tfidf-18" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>19 0.062594257 <a title="209-tfidf-19" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>20 0.059603889 <a title="209-tfidf-20" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.197), (1, 0.128), (2, -0.244), (3, 0.288), (4, -0.193), (5, 0.005), (6, -0.015), (7, 0.104), (8, -0.032), (9, 0.022), (10, -0.011), (11, -0.003), (12, 0.0), (13, -0.032), (14, -0.052), (15, -0.003), (16, 0.222), (17, -0.087), (18, -0.023), (19, -0.0), (20, -0.029), (21, 0.092), (22, -0.051), (23, -0.117), (24, 0.003), (25, 0.071), (26, 0.025), (27, -0.098), (28, -0.08), (29, -0.051), (30, 0.056), (31, 0.028), (32, -0.142), (33, 0.104), (34, 0.0), (35, -0.04), (36, 0.017), (37, 0.05), (38, 0.011), (39, -0.003), (40, -0.025), (41, 0.119), (42, -0.025), (43, 0.095), (44, 0.007), (45, 0.034), (46, 0.065), (47, 0.016), (48, -0.123), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96180254 <a title="209-lsi-1" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>Author: Wei Wei ; Jon Atle Gulla</p><p>Abstract: Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products.</p><p>2 0.90328997 <a title="209-lsi-2" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>Author: Ainur Yessenalina ; Yejin Choi ; Claire Cardie</p><p>Abstract: One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</p><p>3 0.80970037 <a title="209-lsi-3" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>Author: Christian Scheible</p><p>Abstract: The translation of sentiment information is a task from which sentiment analysis systems can benefit. We present a novel, graph-based approach using SimRank, a well-established vertex similarity algorithm to transfer sentiment information between a source language and a target language graph. We evaluate this method in comparison with SO-PMI.</p><p>4 0.74306244 <a title="209-lsi-4" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>Author: Georgios Paltoglou ; Mike Thelwall</p><p>Abstract: Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.</p><p>5 0.71309787 <a title="209-lsi-5" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>Author: Hitoshi Nishikawa ; Takaaki Hasegawa ; Yoshihiro Matsuo ; Genichiro Kikui</p><p>Abstract: We propose a novel algorithm for sentiment summarization that takes account of informativeness and readability, simultaneously. Our algorithm generates a summary by selecting and ordering sentences taken from multiple review texts according to two scores that represent the informativeness and readability of the sentence order. The informativeness score is defined by the number of sentiment expressions and the readability score is learned from the target corpus. We evaluate our method by summarizing reviews on restaurants. Our method outperforms an existing algorithm as indicated by its ROUGE score and human readability experiments.</p><p>6 0.65985465 <a title="209-lsi-6" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>7 0.55960339 <a title="209-lsi-7" href="./acl-2010-Mood_Patterns_and_Affective_Lexicon_Access_in_Weblogs.html">176 acl-2010-Mood Patterns and Affective Lexicon Access in Weblogs</a></p>
<p>8 0.53340262 <a title="209-lsi-8" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>9 0.51416409 <a title="209-lsi-9" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>10 0.51274294 <a title="209-lsi-10" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>11 0.44781983 <a title="209-lsi-11" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>12 0.43001646 <a title="209-lsi-12" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>13 0.41806465 <a title="209-lsi-13" href="./acl-2010-Cross_Lingual_Adaptation%3A_An_Experiment_on_Sentiment_Classifications.html">80 acl-2010-Cross Lingual Adaptation: An Experiment on Sentiment Classifications</a></p>
<p>14 0.4007408 <a title="209-lsi-14" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>15 0.39894667 <a title="209-lsi-15" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>16 0.36681297 <a title="209-lsi-16" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>17 0.32347566 <a title="209-lsi-17" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>18 0.32183194 <a title="209-lsi-18" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>19 0.30642483 <a title="209-lsi-19" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>20 0.26112768 <a title="209-lsi-20" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.013), (25, 0.061), (33, 0.019), (39, 0.01), (42, 0.039), (44, 0.011), (59, 0.062), (71, 0.015), (72, 0.3), (73, 0.098), (78, 0.03), (83, 0.083), (84, 0.045), (98, 0.13)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87725699 <a title="209-lda-1" href="./acl-2010-Learning_5000_Relational_Extractors.html">159 acl-2010-Learning 5000 Relational Extractors</a></p>
<p>Author: Raphael Hoffmann ; Congle Zhang ; Daniel S. Weld</p><p>Abstract: Many researchers are trying to use information extraction (IE) to create large-scale knowledge bases from natural language text on the Web. However, the primary approach (supervised learning of relation-specific extractors) requires manually-labeled training data for each relation and doesn’t scale to the thousands of relations encoded in Web text. This paper presents LUCHS, a self-supervised, relation-specific IE system which learns 5025 relations more than an order of magnitude greater than any previous approach with an average F1 score of 61%. Crucial to LUCHS’s performance is an automated system for dynamic lexicon learning, which allows it to learn accurately from heuristically-generated training data, which is often noisy and sparse. — —</p><p>2 0.86235392 <a title="209-lda-2" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>Author: Mattia Tomasoni ; Minlie Huang</p><p>Abstract: This paper presents a framework for automatically processing information coming from community Question Answering (cQA) portals with the purpose of generating a trustful, complete, relevant and succinct summary in response to a question. We exploit the metadata intrinsically present in User Generated Content (UGC) to bias automatic multi-document summarization techniques toward high quality information. We adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap. Experimental re- sults on data drawn from Yahoo! Answers demonstrate the effectiveness of our method in terms of ROUGE scores. We show that the information contained in the best answers voted by users of cQA portals can be successfully complemented by our method.</p><p>same-paper 3 0.82282698 <a title="209-lda-3" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>Author: Wei Wei ; Jon Atle Gulla</p><p>Abstract: Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products.</p><p>4 0.75952178 <a title="209-lda-4" href="./acl-2010-Global_Learning_of_Focused_Entailment_Graphs.html">127 acl-2010-Global Learning of Focused Entailment Graphs</a></p>
<p>Author: Jonathan Berant ; Ido Dagan ; Jacob Goldberger</p><p>Abstract: We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms.</p><p>5 0.6563679 <a title="209-lda-5" href="./acl-2010-Modeling_Semantic_Relevance_for_Question-Answer_Pairs_in_Web_Social_Communities.html">174 acl-2010-Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities</a></p>
<p>Author: Baoxun Wang ; Xiaolong Wang ; Chengjie Sun ; Bingquan Liu ; Lin Sun</p><p>Abstract: Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. In this paper, a deep belief network is proposed to model the semantic relevance for question-answer pairs. Observing the textual similarity between the community-driven questionanswering (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora.</p><p>6 0.63833308 <a title="209-lda-6" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>7 0.63172436 <a title="209-lda-7" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>8 0.60346824 <a title="209-lda-8" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>9 0.60005116 <a title="209-lda-9" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>10 0.59916842 <a title="209-lda-10" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>11 0.59042442 <a title="209-lda-11" href="./acl-2010-Open_Information_Extraction_Using_Wikipedia.html">185 acl-2010-Open Information Extraction Using Wikipedia</a></p>
<p>12 0.58597445 <a title="209-lda-12" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>13 0.58252633 <a title="209-lda-13" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>14 0.57902706 <a title="209-lda-14" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>15 0.57356668 <a title="209-lda-15" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>16 0.57157481 <a title="209-lda-16" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>17 0.5705002 <a title="209-lda-17" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>18 0.56988263 <a title="209-lda-18" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>19 0.56811726 <a title="209-lda-19" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>20 0.56566799 <a title="209-lda-20" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
