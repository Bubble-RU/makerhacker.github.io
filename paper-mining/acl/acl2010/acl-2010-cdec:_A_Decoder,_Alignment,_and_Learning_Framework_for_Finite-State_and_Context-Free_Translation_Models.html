<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-265" href="#">acl2010-265</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</h1>
<br/><p>Source: <a title="acl-2010-265-pdf" href="http://aclweb.org/anthology//P/P10/P10-4002.pdf">pdf</a></p><p>Author: Chris Dyer ; Adam Lopez ; Juri Ganitkevitch ; Jonathan Weese ; Ferhan Ture ; Phil Blunsom ; Hendra Setiawan ; Vladimir Eidelman ; Philip Resnik</p><p>Abstract: Adam Lopez University of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</p><p>Reference: <a title="acl-2010-265-reference" href="../acl2010_reference/acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. [sent-21, score-0.608]
</p><p>2 Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring,  pruning, and inference algorithms. [sent-22, score-0.735]
</p><p>3 From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. [sent-23, score-0.324]
</p><p>4 Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. [sent-24, score-0.063]
</p><p>5 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). [sent-25, score-0.523]
</p><p>6 , 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). [sent-28, score-0.126]
</p><p>7 We introduce a software package called cdec that manipulates both  7  based and hierarchical translation models have been available for several years (Koehn et al. [sent-29, score-0.878]
</p><p>8 , 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. [sent-31, score-0.118]
</p><p>9 First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. [sent-32, score-0.139]
</p><p>10 This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. [sent-33, score-0.595]
</p><p>11 In cdec, model-specific code is only required to construct a translation forest (§3). [sent-34, score-0.455]
</p><p>12 r models), pruning, winiftehr leanncgeu, agned m alignment algorithms then apply to the unified data structure (§4). [sent-36, score-0.244]
</p><p>13 ); new models can be more easily prototyped; and controlled comparison of models is made easier. [sent-39, score-0.104]
</p><p>14 Second, existing open source decoders were designed with the traditional phrase-based parame-  terization using a very small number of dense features (typically less than 10). [sent-40, score-0.215]
</p><p>15 cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al. [sent-41, score-0.656]
</p><p>16 Since the inference algorithms necessary to compute a training objective (e. [sent-44, score-0.101]
</p><p>17 conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be turnaiinfieedd using rwucithtu any 5o)f, tahney supported training 1The software is released under the Apache License, sion 2. [sent-46, score-0.257]
</p><p>18 The software package includes general function optimization utilities that can be used for discriminative training (§6). [sent-52, score-0.107]
</p><p>19 We show experimentally that cdec uses less memory and time than comparable decoders on a controlled translation task (§7). [sent-54, score-0.95]
</p><p>20 2  Decoder workflow  The decoding pipeline consists of two phases. [sent-55, score-0.27]
</p><p>21 The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al. [sent-56, score-0.121]
</p><p>22 , 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. [sent-57, score-0.799]
</p><p>23 Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. [sent-60, score-0.26]
</p><p>24 Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-specific code. [sent-61, score-0.139]
</p><p>25 Once this unscored translation forest has been generated, any non-coaccessible states (i. [sent-62, score-0.455]
</p><p>26 , states that are not reachable from the goal node) are removed and the resulting structure is rescored with  language models using a user-specified intersection/pruning strategy (§4) resulting in a rescored ttrioann/splartuionnin gfor setrsat aegndy completing phase a1. [sent-64, score-0.318]
</p><p>27 r The second phase of the decoding pipeline (depicted in Figure 2) computes a value from the rescored forest: 1- or k-best derivations, feature expectations, or intersection with a target language reference (sentence or lattice). [sent-65, score-0.423]
</p><p>28 The last option generates an alignment forest, from which a word alignment or feature expectations can be extracted. [sent-66, score-0.244]
</p><p>29 Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). [sent-67, score-0.727]
</p><p>30 1 Alignment forests and alignment Alignment is the process of determining if and how a translation model generates a hsource, targeti string pair. [sent-69, score-0.383]
</p><p>31 nT mo compute an alignment eu,n tdare-r a etrtian sstrlaintigon p mairo. [sent-70, score-0.145]
</p><p>32 de Tol, t choem phase a1n ntr aalnigsnlamtioennt hypergraph is reinterpreted as a synchronous contextfree grammar and then used to parse the target sentence. [sent-71, score-0.309]
</p><p>33 2 This results in an alignment forest, which is a compact representation of all the deriva-  tions of the sentence pair under the translation model. [sent-72, score-0.332]
</p><p>34 From this forest, the Viterbi or maximum a posteriori word alignment can be generated. [sent-73, score-0.108]
</p><p>35 This alignment algorithm is explored in depth by Dyer (2010). [sent-74, score-0.108]
</p><p>36 Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. [sent-75, score-0.459]
</p><p>37 3  Translation hypergraphs  Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). [sent-76, score-0.62]
</p><p>38 In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). [sent-77, score-0.325]
</p><p>39 Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. [sent-78, score-0.297]
</p><p>40 In cdec’s translation hypergraph, a node represents a contiguous sequence of target language  words. [sent-79, score-0.28]
</p><p>41 For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated. [sent-80, score-0.333]
</p><p>42 In a phrase-based translation hypergraph, the node will correspond to a source coverage vector (Koehn et al. [sent-81, score-0.328]
</p><p>43 In word-based models, a single node may derive multiple different source language coverages since word based models impose no requirements on covering all words in the input. [sent-83, score-0.156]
</p><p>44 Edges are associated with exactly one synchronous production in the source and target language, and alternative translation possibilities are expressed as alternative edges. [sent-85, score-0.346]
</p><p>45 Edges are further annotated with feature values, and are annotated with the source span vector the edge corresponds to. [sent-86, score-0.077]
</p><p>46 2The parser is smart enough to detect the left-branching grammars generated by lexical translation and tagging models, and use a more efficient intersection algorithm. [sent-88, score-0.363]
</p><p>47 8 Input  Transducers  FST rescoring  Output  Figure 1: Forest generation workflow (first half of decoding pipeline). [sent-89, score-0.399]
</p><p>48 The decoder’s configuration specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph. [sent-90, score-0.322]
</p><p>49 The highlighted path is the workflow used in the test reported in §7. [sent-91, score-0.08]
</p><p>50 Translation outputs  Alignment outputs  Figure 2: Output generation workflow (second half of decoding pipeline). [sent-92, score-0.179]
</p><p>51 In the case of SCFG grammars, the edges correspond simply to rules in the synchronous  gram-  mar. [sent-94, score-0.138]
</p><p>52 For non-SCFG translation models, there are two kinds of edges. [sent-95, score-0.224]
</p><p>53 , an arity of 0), and correspond phrase translation  to word or  pairs (with all translation  op-  tions existing on edges deriving the same head  node), or glue rules that glue phrases together. [sent-98, score-0.592]
</p><p>54 4  Rescoring with weighted FSTs  The design of cdec separates the creation of a translation forest from its rescoring with a language models or similar models. [sent-100, score-1.397]
</p><p>55 3 Since the structure of the unified search space is context free (§3), we use hthee logic dfo sre language em isod coeln rescoring §d3e)-, scribed by Chiang (2007), although any weighted intersection algorithm can be applied. [sent-101, score-0.489]
</p><p>56 The rescor3Other rescoring models that depend on sequential context include distance-based reordering models or Markov features in tagging models. [sent-102, score-0.424]
</p><p>57 Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. [sent-104, score-0.424]
</p><p>58 cdec therefore supports three  pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e. [sent-105, score-0.854]
</p><p>59 , Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007). [sent-107, score-0.114]
</p><p>60 5 Semiring framework Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings. [sent-108, score-0.386]
</p><p>61 A semiring is a 5-tuple (K, ⊕, ⊗, 0, 1) that indiAcat seesm mthirei nsegt ifsro am 5 w-tuhpicleh t(hKe, v⊕al,u⊗es, 0w,1ill) b the adtr ainwdni,K, a generic addition and multiplication operation, ⊕ and ⊗, and their identities 0 and 1. [sent-109, score-0.248]
</p><p>62 Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). [sent-114, score-0.617]
</p><p>63 Since semirings are such a useful abstraction, cdec has been designed to facilitate implementation of new semirings. [sent-115, score-0.728]
</p><p>64 A generic first-order expectation semiring is also provided (Li and Eisner, 2009). [sent-119, score-0.273]
</p><p>65 Ele⊗⊕K1m0entT CT (+: 1:)+o) rep e r esa etnot arti*+o=n Three standard algorithms parameterized with semirings are provided: INSIDE, OUTSIDE, and INSIDEOUTSIDE, and the semiring is specified using C++ generics (templates). [sent-122, score-0.413]
</p><p>66 Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. [sent-123, score-0.395]
</p><p>67 1 Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as INSIDE algorithms with the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). [sent-125, score-0.64]
</p><p>68 Thus, many of the semiring types define not only the el-  ements shown in Table 1but T : :operat or< as well. [sent-126, score-0.211]
</p><p>69 The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. [sent-127, score-0.155]
</p><p>70 6  Model training  Two training pipelines are provided with cdec. [sent-129, score-0.14]
</p><p>71 The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). [sent-130, score-0.327]
</p><p>72 1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al. [sent-132, score-0.325]
</p><p>73 In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication  operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the INSIDE algorithm. [sent-134, score-0.347]
</p><p>74 Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. [sent-135, score-1.055]
</p><p>75 2 Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al. [sent-140, score-1.03]
</p><p>76 In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. [sent-142, score-0.37]
</p><p>77 Since log likelihood is differentiable with respect to the feature weights in an exponential  model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features. [sent-143, score-0.197]
</p><p>78 While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec. [sent-144, score-0.259]
</p><p>79 When used with sequential tagging models, this pipeline is identical to traditional sequential CRF training (Sha and Pereira, 2003). [sent-145, score-0.267]
</p><p>80 Both the objective (conditional log likelihood) and its gradient have the form of a difference in two quantities: each has one term that is computed over the translation hypergraph which is subtracted from the result of the same computation over the alignment hypergraph (refer to Figures 1 and 2). [sent-146, score-0.85]
</p><p>81 The conditional log likelihood is the difference in the log partition of the translation and alignment hypergraph, and is computed using the INSIDE algorithm. [sent-147, score-0.517]
</p><p>82 The gradient with respect to a particular feature is the difference in this feature’s expected value in the translation and alignment hypergraphs, and can be computed using either INSIDEOUTSIDE or the expectation semiring  and INSIDE. [sent-148, score-0.664]
</p><p>83 Since a translation forest is generated as an intermediate step in generating an alignment forest (§2) this computation is straightforward. [sent-149, score-0.794]
</p><p>84 rSesintc (§e gradient-based optimization techniques may require thousands of evaluations to converge, the batch training pipeline is split into map and reduce components, facilitating distribution over very large clusters. [sent-150, score-0.169]
</p><p>85 Briefly, the cdec is run as the map function, and sentence pairs are mapped over. [sent-151, score-0.564]
</p><p>86 , 1989), RPROP (Riedmiller and Braun, 1993), and stochastic gradient descent. [sent-153, score-0.054]
</p><p>87 3 (running with 1 or 8 threads) decoding using a hierarchical phrase-based translation grammar and identical pruning settings. [sent-155, score-0.46]
</p><p>88 4 Figure 4 shows the cdec configuration and weights file used for this test. [sent-156, score-0.564]
</p><p>89 All decoders use SRI’s language model toolkit, version 1. [sent-162, score-0.127]
</p><p>90 A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). [sent-168, score-0.513]
</p><p>91 A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. [sent-169, score-0.368]
</p><p>92 Table 2: Memory usage and average per-sentence running time, in seconds, for decoding a ChineseEnglish test set. [sent-173, score-0.099]
</p><p>93 a76-ti752o340nfile(above)andfeature  weights file (below) used for the decoding test described in §7. [sent-181, score-0.099]
</p><p>94 net/projects/joshua/ 11  8  Future work  cdec continues to be under active development. [sent-183, score-0.564]
</p><p>95 We are taking advantage of its modular design to study alternative algorithms for language model integration. [sent-184, score-0.066]
</p><p>96 Further training pipelines are under development, including minimum risk training using a linearly decomposable approximation of BLEU (Li and Eisner, 2009), and MIRA training (Chiang et al. [sent-185, score-0.185]
</p><p>97 We are also improving support for parallel training using Hadoop (an open-source implementation of MapReduce). [sent-188, score-0.063]
</p><p>98 Efficient minimum error rate training and minimum B ayes-risk decoding for translation hypergraphs and lattices. [sent-321, score-0.582]
</p><p>99 First- and second-order expectation semirings with applications to minimum-risk training on translation forests. [sent-328, score-0.429]
</p><p>100 A study of translation edit rate with targeted human annotation. [sent-421, score-0.224]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cdec', 0.564), ('forest', 0.231), ('translation', 0.224), ('rescoring', 0.22), ('semiring', 0.211), ('hypergraph', 0.195), ('hypergraphs', 0.148), ('semirings', 0.136), ('decoders', 0.127), ('rescored', 0.113), ('alignment', 0.108), ('dyer', 0.106), ('decoding', 0.099), ('blunsom', 0.099), ('pruning', 0.099), ('unified', 0.098), ('pipeline', 0.091), ('chiang', 0.091), ('workflow', 0.08), ('intersection', 0.08), ('maryland', 0.078), ('synchronous', 0.074), ('decoder', 0.074), ('lopez', 0.073), ('lattice', 0.073), ('fsts', 0.068), ('mapreduce', 0.068), ('edges', 0.064), ('tagging', 0.059), ('huang', 0.058), ('cube', 0.057), ('koehn', 0.057), ('insideoutside', 0.056), ('riedmiller', 0.056), ('rprop', 0.056), ('vest', 0.056), ('node', 0.056), ('sha', 0.055), ('scfg', 0.055), ('gradient', 0.054), ('models', 0.052), ('forests', 0.051), ('logic', 0.05), ('bleu', 0.05), ('hendra', 0.049), ('source', 0.048), ('li', 0.047), ('dean', 0.045), ('quantities', 0.045), ('viterbi', 0.043), ('derivations', 0.043), ('optimization', 0.043), ('joshua', 0.043), ('pipelines', 0.042), ('log', 0.041), ('weighted', 0.041), ('inside', 0.041), ('sequential', 0.041), ('enabling', 0.041), ('phase', 0.04), ('dense', 0.04), ('juri', 0.04), ('weese', 0.04), ('glue', 0.04), ('resnik', 0.04), ('integration', 0.04), ('minimum', 0.038), ('algorithms', 0.038), ('ganitkevitch', 0.038), ('parameterization', 0.038), ('hierarchical', 0.038), ('multiplication', 0.037), ('separates', 0.037), ('mo', 0.037), ('tail', 0.037), ('snover', 0.037), ('conditional', 0.036), ('eisner', 0.036), ('double', 0.035), ('training', 0.035), ('memory', 0.035), ('likelihood', 0.034), ('kumar', 0.034), ('expectation', 0.034), ('abstraction', 0.033), ('computed', 0.033), ('operation', 0.033), ('transducers', 0.032), ('johns', 0.032), ('mert', 0.031), ('span', 0.029), ('discriminative', 0.029), ('design', 0.028), ('expectations', 0.028), ('schwartz', 0.028), ('inference', 0.028), ('implementation', 0.028), ('provided', 0.028), ('parameterized', 0.028), ('suffix', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="265-tfidf-1" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>Author: Chris Dyer ; Adam Lopez ; Juri Ganitkevitch ; Jonathan Weese ; Ferhan Ture ; Phil Blunsom ; Hendra Setiawan ; Vladimir Eidelman ; Philip Resnik</p><p>Abstract: Adam Lopez University of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</p><p>2 0.20328681 <a title="265-tfidf-2" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>3 0.20147851 <a title="265-tfidf-3" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>Author: Yang Liu ; Liang Huang</p><p>Abstract: unkown-abstract</p><p>4 0.19191895 <a title="265-tfidf-4" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>5 0.17233315 <a title="265-tfidf-5" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>6 0.16214965 <a title="265-tfidf-6" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>7 0.16140383 <a title="265-tfidf-7" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>8 0.15377918 <a title="265-tfidf-8" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>9 0.13622048 <a title="265-tfidf-9" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>10 0.13095714 <a title="265-tfidf-10" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>11 0.11658808 <a title="265-tfidf-11" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>12 0.11535604 <a title="265-tfidf-12" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>13 0.11390848 <a title="265-tfidf-13" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>14 0.11185324 <a title="265-tfidf-14" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>15 0.10923846 <a title="265-tfidf-15" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>16 0.10192777 <a title="265-tfidf-16" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>17 0.10110323 <a title="265-tfidf-17" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>18 0.10080064 <a title="265-tfidf-18" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>19 0.10015054 <a title="265-tfidf-19" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>20 0.10006681 <a title="265-tfidf-20" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.229), (1, -0.27), (2, -0.013), (3, 0.001), (4, -0.012), (5, -0.005), (6, 0.004), (7, 0.037), (8, -0.092), (9, -0.026), (10, 0.042), (11, -0.02), (12, 0.091), (13, -0.04), (14, -0.061), (15, 0.042), (16, -0.037), (17, 0.051), (18, -0.132), (19, 0.043), (20, 0.112), (21, 0.0), (22, -0.112), (23, -0.095), (24, -0.126), (25, 0.085), (26, 0.008), (27, -0.009), (28, 0.014), (29, 0.009), (30, -0.045), (31, -0.014), (32, 0.014), (33, 0.001), (34, 0.008), (35, -0.121), (36, -0.017), (37, 0.023), (38, 0.039), (39, 0.007), (40, 0.02), (41, 0.067), (42, 0.083), (43, -0.033), (44, 0.002), (45, -0.026), (46, -0.075), (47, 0.034), (48, 0.029), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93810624 <a title="265-lsi-1" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>Author: Chris Dyer ; Adam Lopez ; Juri Ganitkevitch ; Jonathan Weese ; Ferhan Ture ; Phil Blunsom ; Hendra Setiawan ; Vladimir Eidelman ; Philip Resnik</p><p>Abstract: Adam Lopez University of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</p><p>2 0.80875611 <a title="265-lsi-2" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>Author: Yang Liu ; Liang Huang</p><p>Abstract: unkown-abstract</p><p>3 0.7778694 <a title="265-lsi-3" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>4 0.66521847 <a title="265-lsi-4" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>5 0.65937525 <a title="265-lsi-5" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>6 0.56588769 <a title="265-lsi-6" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>7 0.56569314 <a title="265-lsi-7" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>8 0.55468297 <a title="265-lsi-8" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>9 0.54658413 <a title="265-lsi-9" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>10 0.54117441 <a title="265-lsi-10" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>11 0.53844094 <a title="265-lsi-11" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>12 0.53686547 <a title="265-lsi-12" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>13 0.5368433 <a title="265-lsi-13" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>14 0.53356403 <a title="265-lsi-14" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>15 0.50636971 <a title="265-lsi-15" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>16 0.50614583 <a title="265-lsi-16" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>17 0.50032127 <a title="265-lsi-17" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>18 0.49577138 <a title="265-lsi-18" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>19 0.48559922 <a title="265-lsi-19" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>20 0.48205528 <a title="265-lsi-20" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.019), (16, 0.01), (25, 0.061), (39, 0.012), (42, 0.014), (59, 0.566), (73, 0.032), (78, 0.022), (83, 0.053), (84, 0.016), (98, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98382527 <a title="265-lda-1" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>2 0.9831503 <a title="265-lda-2" href="./acl-2010-Intelligent_Selection_of_Language_Model_Training_Data.html">151 acl-2010-Intelligent Selection of Language Model Training Data</a></p>
<p>Author: Robert C. Moore ; William Lewis</p><p>Abstract: We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.</p><p>3 0.97288758 <a title="265-lda-3" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>Author: Galina Tremper</p><p>Abstract: Presupposition relations between verbs are not very well covered in existing lexical semantic resources. We propose a weakly supervised algorithm for learning presupposition relations between verbs that distinguishes five semantic relations: presupposition, entailment, temporal inclusion, antonymy and other/no relation. We start with a number of seed verb pairs selected manually for each semantic relation and classify unseen verb pairs. Our algorithm achieves an overall accuracy of 36% for type-based classification.</p><p>same-paper 4 0.97161645 <a title="265-lda-4" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>Author: Chris Dyer ; Adam Lopez ; Juri Ganitkevitch ; Jonathan Weese ; Ferhan Ture ; Phil Blunsom ; Hendra Setiawan ; Vladimir Eidelman ; Philip Resnik</p><p>Abstract: Adam Lopez University of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</p><p>5 0.96938044 <a title="265-lda-5" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>Author: Joern Wuebker ; Arne Mauser ; Hermann Ney</p><p>Abstract: Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overfitting. We describe a novel leavingone-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering mod- els in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.</p><p>6 0.86957133 <a title="265-lda-6" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>7 0.858962 <a title="265-lda-7" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>8 0.79647487 <a title="265-lda-8" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>9 0.79571539 <a title="265-lda-9" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>10 0.78384298 <a title="265-lda-10" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>11 0.77683657 <a title="265-lda-11" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>12 0.77606213 <a title="265-lda-12" href="./acl-2010-Domain_Adaptation_of_Maximum_Entropy_Language_Models.html">91 acl-2010-Domain Adaptation of Maximum Entropy Language Models</a></p>
<p>13 0.7691263 <a title="265-lda-13" href="./acl-2010-BabelNet%3A_Building_a_Very_Large_Multilingual_Semantic_Network.html">44 acl-2010-BabelNet: Building a Very Large Multilingual Semantic Network</a></p>
<p>14 0.76901126 <a title="265-lda-14" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>15 0.76796263 <a title="265-lda-15" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>16 0.75804967 <a title="265-lda-16" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>17 0.75685525 <a title="265-lda-17" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>18 0.75541443 <a title="265-lda-18" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>19 0.74486637 <a title="265-lda-19" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>20 0.74237013 <a title="265-lda-20" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
