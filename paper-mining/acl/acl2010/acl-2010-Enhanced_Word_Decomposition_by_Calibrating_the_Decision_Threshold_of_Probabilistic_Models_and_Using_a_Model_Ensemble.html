<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-100" href="#">acl2010-100</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</h1>
<br/><p>Source: <a title="acl-2010-100-pdf" href="http://aclweb.org/anthology//P/P10/P10-1039.pdf">pdf</a></p><p>Author: Sebastian Spiegler ; Peter A. Flach</p><p>Abstract: This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can significantly improve the performance of machine learning methods for morphological word decomposition. We employ two algorithms which come from a family of generative probabilistic models. The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. The advantage of this model family is that it can learn from small datasets and easily gen- eralises to larger datasets. The first algorithm PROMODES, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm PROMODES-H is a novel development of the first using a higher order model. We present the mathematical description for both algorithms, conduct experiments on the morphologically rich language Zulu and compare characteristics of both algorithms based on the experimental results.</p><p>Reference: <a title="acl-2010-100-reference" href="../acl2010_reference/acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Enhanced word decomposition by calibrating the decision threshold of probabilistic models and using a model ensemble  Sebastian Spiegler Intelligent Systems Laboratory, University of Bristol, U. [sent-1, score-0.365]
</p><p>2 uk Abstract This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can significantly improve the performance of machine learning methods for morphological word decomposition. [sent-6, score-0.374]
</p><p>3 We employ two algorithms which come from a family of generative probabilistic models. [sent-7, score-0.157]
</p><p>4 The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. [sent-8, score-0.298]
</p><p>5 The first algorithm PROMODES, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm PROMODES-H is a novel development of the first using a higher order model. [sent-10, score-0.153]
</p><p>6 It is worthwhile studying this internal structure since a language description using its morphological formation is  more compact and complete than listing all possible words. [sent-13, score-0.093]
</p><p>7 Results of morphological analysis are applied in speech synthesis (Sproat, 1996) and recognition (Hirsimaki et al. [sent-22, score-0.093]
</p><p>8 1 Background In the past years, there has been a lot of inter-  est and activity in the development of algorithms for morphological analysis. [sent-25, score-0.148]
</p><p>9 All these approaches have in common that they build a morphological model which is then applied to analyse words. [sent-26, score-0.093]
</p><p>10 Another way of classifying approaches is based on the learning aspect during the construction of the morphological model. [sent-28, score-0.093]
</p><p>11 If the data for training the model has the same structure as the desired output of the morphological analysis, in other words, if a morphological model is learnt from labelled data, the algorithm is classified under supervised learning. [sent-29, score-0.245]
</p><p>12 Unsupervised algorithms for morphological analysis are Linguistica (Goldsmith, 2001), Morfessor (Creutz,  2006) and Paramor (Monson, 2008). [sent-33, score-0.148]
</p><p>13 A comparison of different levels of supervision for morphology learning on Zulu has been carried out by Spiegler et al. [sent-39, score-0.074]
</p><p>14 Our two algorithms, PROMODES and PROMODES-H, perform word decomposition and are based on probabilistic methods by incorporating a probabilistic generative model. [sent-41, score-0.166]
</p><p>15 Furthermore, it is  investigated how the decision threshold can be calibrated and a model ensemble is tested. [sent-44, score-0.414]
</p><p>16 In Section 2 we introduce the probabilistic generative process and show in Sections 2. [sent-46, score-0.082]
</p><p>17 2, show how we find a better decision threshold for placing morpheme boundaries in 3. [sent-52, score-0.465]
</p><p>18 3 and combine both algorithms using a model ensemble to leverage individual strengths in 3. [sent-53, score-0.245]
</p><p>19 We are trying to reverse this process in order to find the underlying sequence oftosses which determine the morpheme boundaries. [sent-59, score-0.198]
</p><p>20 abilistic generative process consisting of words as observed variables X and their hidden segmentation as latent variables Y. [sent-65, score-0.082]
</p><p>21 If a generative model is fully parameterised it can be reversed to find the underlying word decomposition by forming the conditional probability distribution Pr(Y|X). [sent-66, score-0.115]
</p><p>22 A h awso mrd’ =s segmentation niss depicted as a boundary vector bj = (bj1, . [sent-69, score-0.162]
</p><p>23 ,bjm) consisting of boundary values bji ∈ {0, 1} with 1 ≤ i≤ m which disclose whether or ∈n {ot0 a }bou wnidth−  ary i is placed i cnh position ei . [sent-72, score-0.661]
</p><p>24 w Ahe tlheetterr o lj,i-1 precedes the position iin wj and a letter lji follows it. [sent-73, score-0.257]
</p><p>25 Both letters lj,i-1 and lji are part of an alphabet. [sent-74, score-0.089]
</p><p>26 Furthermore, we introduce a letter transition tji which goes from lj,i-1 to lji. [sent-75, score-0.389]
</p><p>27 1 PROMODES PROMODES is based on a zero-order model for boundaries bji and on a first-order model for letter transitions tji. [sent-77, score-0.819]
</p><p>28 It describes a word’s segmentation by its morpheme boundaries and resulting letter transitions within morphemes. [sent-78, score-0.51]
</p><p>29 A boundary vector bj is found by evaluating each position iwith  argbmjiaxPr(bji|tji) = argbmjiaxPr(bji)Pr(tji|bji)  . [sent-79, score-0.166]
</p><p>30 We assume that a boundary in i is inserted independently from other boundaries (zeroorder) and the graphemic representation of the word, however, is conditioned on the length of  the word mj which means that the probability distribution is in fact Pr(bji|mj). [sent-81, score-0.268]
</p><p>31 The second component is the letter transition probability distribution Pr(tji|bji). [sent-84, score-0.166]
</p><p>32 We suppose a first-order Markov chain consisting of transitions tji from letter lj,i-1 ∈ AB to letter lji ∈ A where A is a regular letter alphabet and AB=A ∪A {B} ein Acludes B as an abstract morpheme start symbol which can occur in lj,i-1. [sent-85, score-0.909]
</p><p>33 For instance, the suffix ‘s’ of the verb form gets, marking 3rd person singular, would be modelled as B → s whereas a morpheme ionutledrn bael mtraondseitllieodn acso Buld → → be s g → e. [sent-86, score-0.236]
</p><p>34 aWs ea 376  guarantee ∑lji∈A Pr(tji |bji)=1 with tji being a transition from a cAertain lj,i−1 ∈ AB to lji. [sent-87, score-0.262]
</p><p>35 )Pr(tji|bji=01)  (2)  The simplifying assumptions made, however, reduce the expressive power of the model by not allowing any dependencies on preceding boundaries or letters. [sent-92, score-0.13]
</p><p>36 2  PROMODES-H  In contrast to the original PROMODES model, we also consider the boundary value bj,i-1 and modify our transition assumptions for PROMODESH in such a way that the new algorithm applies a first-order boundary model and a second-order  transition model. [sent-96, score-0.276]
</p><p>37 A transition tji is now defined as a transition from an abstract symbol in lj,i-1 ∈ {N , B} to a letter in lji ∈ A. [sent-97, score-0.517]
</p><p>38 The second component, the letter transition probability distribution Pr(tji |bji, bj,i-1 ), fulfils ∑lji∈A Pr(tji|bji, tj,i-1 bj,i-1)=1 with tji being a tra∈nAsition f|brom a certain lj,i−1 ∈ AB to lji. [sent-107, score-0.414]
</p><p>39 For the experiments described below, we chose the South African language Zulu since our research work mainly aims at creating morphological resources for under-resourced indigenous languages. [sent-112, score-0.093]
</p><p>40 Zulu –  is an agglutinative language with a complex morphology where multiple prefixes and suffixes contribute to a word’s meaning. [sent-113, score-0.074]
</p><p>41 Nevertheless, it seems that segment boundaries are more likely in certain word positions. [sent-114, score-0.155]
</p><p>42 The PROMODES family harnesses this characteristic in combination with describing morphemes by letter transitions. [sent-115, score-0.194]
</p><p>43 1 Learning with increasing experience In our first experiment we applied 10-fold crossvalidation on datasets ranging from 500 to 2500 words with the goal of measuring how the learning improves with increasing experience in terms of training set size. [sent-119, score-0.126]
</p><p>44 We compared results on the test set against the ground truth by counting true positive (TP), false positive (FP), true negative (TN) and 377  false negative (FN) morpheme boundary predictions. [sent-123, score-0.33]
</p><p>45 This suggests that to some extent fewer morpheme boundaries are discovered but the ones which are found are more likely to be correct. [sent-149, score-0.305]
</p><p>46 We believe that this effect is caused by the limited memory of the model which uses order zero for the occurrence of a boundary and order one for letter transitions. [sent-150, score-0.226]
</p><p>47 Since both algorithms show different behaviour with increasing experience and PROMODES-H yields a higher f-measure across all datasets, we will investigate in the next experiments how these differences manifest themselves at the boundary level. [sent-162, score-0.205]
</p><p>48 For this reason we broke down the summary measures of precision and recall into their original components: true/false positive (TP/FP) and negative (TN/FN) counts presented in the 2 2 contingency table of Figure 1. [sent-169, score-0.081]
</p><p>49 Note that the relative frequencies of positives (TP + FN) and negatives (TN + FP) each sum to one. [sent-171, score-0.08]
</p><p>50 The goal was to find out how predictions in each word position changed when applying PROMODES-H instead of PROMODES. [sent-172, score-0.093]
</p><p>51 When applying PROMODES-H, the majority of the FP’s are turned into non-boundaries, however, a slightly higher number of previously correctly labelled non-boundaries are turned into false boundaries. [sent-181, score-0.1]
</p><p>52 On the other side, more false non378  boundaries (FN) are turned into boundaries than in the opposite direction with a net increase of 0. [sent-184, score-0.313]
</p><p>53 0819 of correct boundaries which led to the increased recall. [sent-185, score-0.155]
</p><p>54 In summary, PROMODES predicts more accurately non-boundaries whereas PROMODES-H is better at finding morpheme boundaries. [sent-187, score-0.211]
</p><p>55 So far we have based our decision for placing a boundary in  a certain word position on Equation 2 and 4 assuming that P(bji=1 | . [sent-188, score-0.236]
</p><p>56 However, )if> >thPe underlying distribution for boundaries given the evidence is skewed, it might be possible to improve results by introducing a certain decision threshold for inserting morpheme boundaries. [sent-195, score-0.484]
</p><p>57 3  Calibration of the decision threshold  For the third experiment we slightly changed our experimental setup. [sent-198, score-0.151]
</p><p>58 Instead of dividing datasets during 10-fold cross-validation into training and test subsets with the ratio of 9: 1we randomly split the data into training, validation and test sets with the ratio of 8: 1:1. [sent-199, score-0.079]
</p><p>59 ) might be skewed and an  optimal decision can b me gahcthie bveed s at a ddi afnfedre annt threshold. [sent-218, score-0.085]
</p><p>60 The optimal threshold was sought on the validation set and evaluated on the test set. [sent-219, score-0.187]
</p><p>61 We want to point out that the threshold which yields the best f-measure result on the validation set returns almost the same result on the separate test set for both algorithms which suggests the existence of a general optimal threshold. [sent-221, score-0.242]
</p><p>62 Since this experiment provided us with a set of data points where the recall varied monotonically with the threshold and the precision changed accordingly, we reverted to precision-recall curves (PR curves) from machine learning. [sent-222, score-0.185]
</p><p>63 The PR curve is plotted with recall on the x-axis and precision on the y-axis for increasing thresholds h. [sent-228, score-0.169]
</p><p>64 The PR curves for PRO-  MODES and PROMODES-H are shown in Figure 2 on the validation set from which we learnt our optimal thresholds h∗. [sent-229, score-0.201]
</p><p>65 Nevertheless, across the entire PR curve none of the algorithms dominates. [sent-235, score-0.087]
</p><p>66 Summarizing, we have shown that both algorithms commit different errors at the word position level whereas PROMODES is better in predicting non-boundaries and PROMODES-H gives better results for morpheme boundaries at the default threshold of h = 0. [sent-249, score-0.57]
</p><p>67 In this section, we demonstrated that across different decision thresholds h for P(bji=1 | . [sent-251, score-0.074]
</p><p>68 ) > h none of algorithms dominates the oth=er1 one, a>nd h ha tn tohnee optimal othrirtehsmh-s old PROMODES achieves a slightly higher performance than PROMODES-H. [sent-254, score-0.136]
</p><p>69 The question which arises is whether we can combine PROMODES and PROMODES-H in an ensemble that leverages indi-  vidual strengths of both. [sent-255, score-0.167]
</p><p>70 379  Figure 2: Precision-recall curves for algorithms on validation set. [sent-256, score-0.144]
</p><p>71 4  A model ensemble to leverage individual strengths A model ensemble is a set of individually trained classifiers whose predictions are combined when classifying new instances (Opitz and Maclin, 1999). [sent-258, score-0.348]
</p><p>72 We introduce PROMODES-E as the ensemble of PROMODES and PROMODESH. [sent-260, score-0.126]
</p><p>73 ) and simply averages them:  Pr(bji=1|tji)+Pr(2bji=1|tji,bj,i-1,tj,i-1)> h As before, we used the default threshold h = 0. [sent-264, score-0.133]
</p><p>74 In short, we have shown that by combining  PROMODES and PROMODES-H and finding the optimal threshold, the ensemble PROMODES-E gives better results than the individual models themselves and therefore manages to leverage the individual strengths of both to a certain extend. [sent-283, score-0.258]
</p><p>75 5  Analysis of calibrated algorithms and their model ensemble For the entire dataset of 2500 words, we have examined boundary predictions dependent on the relative word position. [sent-287, score-0.493]
</p><p>76 In Figure 3 and 4 we have plotted the absolute counts of correct boundaries (TP) and non-boundaries (TN) which PROMODES predicted but not PROMODES-H, and vice versa, as continuous lines. [sent-288, score-0.162]
</p><p>77 We furthermore provided the number of individual predictions which were ultimately adopted by PROMODES-E in the ensemble as dashed lines. [sent-289, score-0.158]
</p><p>78 Here, PROMODES-H outperforms PROMODES in predicting correct boundaries across the entire word length. [sent-292, score-0.13]
</p><p>79 For the boundary prediction in Figure 4b the signal disappears after calibration. [sent-294, score-0.099]
</p><p>80 Therefore, the ensemble leveraged strengths of both algorithms which led to a better overall performance with a calibrated threshold. [sent-296, score-0.404]
</p><p>81 Another generative model for morphological analysis has been described by Snover and Brent (2001) and Snover et al. [sent-298, score-0.138]
</p><p>82 (2002), however, they were interested in finding paradigms as sets of mutual exclusive operations on a word form whereas we are describing a generative process using morpheme boundaries and resulting letter transitions. [sent-299, score-0.513]
</p><p>83 The main difference is that we have dependencies between states as well as between emissions whereas in HMMs emissions only depend on the underlying state. [sent-301, score-0.103]
</p><p>84 Combining different morphological analysers has been performed, for example, by Atwell and Roberts (2006) and Spiegler et al. [sent-302, score-0.093]
</p><p>85 Their approaches, though, used majority vote to decide whether a morpheme boundary is inserted in a certain word position or not. [sent-304, score-0.34]
</p><p>86 The tagger would then return a probability for starting a new morpheme in a certain position based on the original algorithm. [sent-310, score-0.241]
</p><p>87 In contrast, our ensemble algorithm PROMODES-E directly accesses the probabilistic framework of each algorithm and combines them based on an optimal threshold learnt on a validation set. [sent-312, score-0.413]
</p><p>88 5  Conclusions  We have presented a method to learn a calibrated decision threshold from a validation set and demonstrated that ensemble methods in connection with calibrated decision thresholds can give better results than the individual models them-  selves. [sent-313, score-0.7]
</p><p>89 We introduced two algorithms for word decomposition which are based on generative probabilistic models. [sent-314, score-0.184]
</p><p>90 The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. [sent-315, score-0.298]
</p><p>91 We compared the performance on increasing training set sizes and analysed for each word position whether their boundary prediction agreed or disagreed. [sent-318, score-0.171]
</p><p>92 We found out that PROMODES was better in predicting non-boundaries and PROMODESH gave better results for morpheme boundaries at a default decision threshold. [sent-319, score-0.391]
</p><p>93 At an optimal decision threshold, however, both yielded a similar f-measure result. [sent-320, score-0.085]
</p><p>94 We then performed a further analysis based on relative word positions and found out that the calibrated PROMODES-H predicted non-boundaries better for initial word posi-  tions whereas the calibrated PROMODES for midand final word positions. [sent-321, score-0.374]
</p><p>95 For boundaries, the calibrated algorithms had a similar behaviour. [sent-322, score-0.212]
</p><p>96 Subsequently, we showed that a model ensemble of both algorithms in conjunction with finding an optimal threshold exceeded the performance of the single algorithms at their individually optimal threshold. [sent-323, score-0.411]
</p><p>97 381 Performance on non−boundaries, default threshold  Performance on boundaries, default threshold  Relative word position  Relative word position  (a) True negatives, default  (b) True positives, default  Figure 3: Analysis of results using default threshold. [sent-326, score-0.48]
</p><p>98 Performance on non−boundaries, calibrated threshold  Relative word position (a) True negatives, calibrated Performance on boundaries, calibrated threshold  Relative word position  (b) True positives, calibrated Figure 4: Analysis of results using calibrated threshold. [sent-327, score-1.045]
</p><p>99 Reductive and generative approaches to management of morphological variation of keywords in monolingual information retrieval: An overview. [sent-395, score-0.138]
</p><p>100 Bootstrapping morphological analyzers by combining human elicitation and machine learning. [sent-438, score-0.093]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('promodes', 0.566), ('bji', 0.521), ('tji', 0.223), ('morpheme', 0.175), ('spiegler', 0.164), ('calibrated', 0.157), ('pr', 0.13), ('boundaries', 0.13), ('letter', 0.127), ('ensemble', 0.126), ('zulu', 0.119), ('boundary', 0.099), ('morphological', 0.093), ('argbmjiaxpr', 0.089), ('lji', 0.089), ('threshold', 0.089), ('gol', 0.074), ('paramor', 0.074), ('morphology', 0.074), ('validation', 0.055), ('algorithms', 0.055), ('corfu', 0.052), ('monson', 0.052), ('nia', 0.051), ('decomposition', 0.047), ('morphemes', 0.047), ('generative', 0.045), ('shalonova', 0.045), ('default', 0.044), ('optimal', 0.043), ('fp', 0.042), ('decision', 0.042), ('strengths', 0.041), ('position', 0.041), ('transitions', 0.041), ('tp', 0.04), ('clef', 0.04), ('flach', 0.039), ('contingency', 0.039), ('transition', 0.039), ('mj', 0.039), ('snover', 0.039), ('tn', 0.038), ('learnt', 0.037), ('segmentation', 0.037), ('probabilistic', 0.037), ('whereas', 0.036), ('morpho', 0.036), ('curves', 0.034), ('morfessor', 0.034), ('kurimo', 0.034), ('thresholds', 0.032), ('predictions', 0.032), ('plotted', 0.032), ('curve', 0.032), ('increasing', 0.031), ('creutz', 0.03), ('negatives', 0.03), ('atwell', 0.03), ('hafer', 0.03), ('hirsimaki', 0.03), ('isometrics', 0.03), ('opitz', 0.03), ('promodesh', 0.03), ('rumelhart', 0.03), ('ukwabelana', 0.03), ('connectionist', 0.029), ('placing', 0.029), ('ab', 0.029), ('fn', 0.029), ('false', 0.028), ('bj', 0.026), ('positives', 0.026), ('accesses', 0.026), ('muggleton', 0.026), ('bristol', 0.026), ('falsely', 0.026), ('certain', 0.025), ('turned', 0.025), ('led', 0.025), ('modelled', 0.025), ('datasets', 0.024), ('unsupervised', 0.024), ('calibrating', 0.024), ('goldsmith', 0.024), ('virpioja', 0.024), ('relative', 0.024), ('underlying', 0.023), ('leverage', 0.023), ('precision', 0.023), ('labelled', 0.022), ('emissions', 0.022), ('calibration', 0.021), ('oflazer', 0.021), ('notes', 0.02), ('family', 0.02), ('experience', 0.02), ('changed', 0.02), ('recall', 0.019), ('subscript', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="100-tfidf-1" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>Author: Sebastian Spiegler ; Peter A. Flach</p><p>Abstract: This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can significantly improve the performance of machine learning methods for morphological word decomposition. We employ two algorithms which come from a family of generative probabilistic models. The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. The advantage of this model family is that it can learn from small datasets and easily gen- eralises to larger datasets. The first algorithm PROMODES, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm PROMODES-H is a novel development of the first using a higher order model. We present the mathematical description for both algorithms, conduct experiments on the morphologically rich language Zulu and compare characteristics of both algorithms based on the experimental results.</p><p>2 0.093892038 <a title="100-tfidf-2" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>3 0.062349483 <a title="100-tfidf-3" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>4 0.059289869 <a title="100-tfidf-4" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>Author: Benjamin Snyder ; Regina Barzilay ; Kevin Knight</p><p>Abstract: In this paper we propose a method for the automatic decipherment of lost languages. Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates. We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and highlevel morphemic correspondences. This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers. When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew.</p><p>5 0.056371208 <a title="100-tfidf-5" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>6 0.054736499 <a title="100-tfidf-6" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>7 0.050510358 <a title="100-tfidf-7" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>8 0.048983157 <a title="100-tfidf-8" href="./acl-2010-The_Use_of_Formal_Language_Models_in_the_Typology_of_the_Morphology_of_Amerindian_Languages.html">234 acl-2010-The Use of Formal Language Models in the Typology of the Morphology of Amerindian Languages</a></p>
<p>9 0.047844376 <a title="100-tfidf-9" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>10 0.044094119 <a title="100-tfidf-10" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>11 0.040315147 <a title="100-tfidf-11" href="./acl-2010-A_Rational_Model_of_Eye_Movement_Control_in_Reading.html">13 acl-2010-A Rational Model of Eye Movement Control in Reading</a></p>
<p>12 0.035642486 <a title="100-tfidf-12" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>13 0.03384893 <a title="100-tfidf-13" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>14 0.033775281 <a title="100-tfidf-14" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>15 0.032576121 <a title="100-tfidf-15" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>16 0.031706814 <a title="100-tfidf-16" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>17 0.031314854 <a title="100-tfidf-17" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>18 0.030437704 <a title="100-tfidf-18" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>19 0.029547285 <a title="100-tfidf-19" href="./acl-2010-A_New_Approach_to_Improving_Multilingual_Summarization_Using_a_Genetic_Algorithm.html">11 acl-2010-A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm</a></p>
<p>20 0.028926119 <a title="100-tfidf-20" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.102), (1, -0.005), (2, -0.006), (3, -0.016), (4, 0.007), (5, -0.024), (6, 0.002), (7, -0.019), (8, 0.065), (9, 0.008), (10, -0.029), (11, 0.061), (12, 0.042), (13, -0.036), (14, -0.009), (15, -0.04), (16, -0.027), (17, 0.046), (18, 0.051), (19, -0.024), (20, -0.032), (21, -0.001), (22, -0.001), (23, -0.017), (24, 0.08), (25, 0.02), (26, -0.028), (27, -0.053), (28, -0.004), (29, 0.07), (30, -0.008), (31, -0.007), (32, 0.026), (33, -0.007), (34, -0.025), (35, -0.115), (36, 0.059), (37, 0.057), (38, -0.036), (39, 0.089), (40, -0.116), (41, -0.018), (42, -0.038), (43, -0.083), (44, -0.009), (45, 0.002), (46, 0.096), (47, 0.087), (48, 0.039), (49, -0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88909119 <a title="100-lsi-1" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>Author: Sebastian Spiegler ; Peter A. Flach</p><p>Abstract: This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can significantly improve the performance of machine learning methods for morphological word decomposition. We employ two algorithms which come from a family of generative probabilistic models. The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. The advantage of this model family is that it can learn from small datasets and easily gen- eralises to larger datasets. The first algorithm PROMODES, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm PROMODES-H is a novel development of the first using a higher order model. We present the mathematical description for both algorithms, conduct experiments on the morphologically rich language Zulu and compare characteristics of both algorithms based on the experimental results.</p><p>2 0.68539476 <a title="100-lsi-2" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>Author: Benjamin Snyder ; Regina Barzilay ; Kevin Knight</p><p>Abstract: In this paper we propose a method for the automatic decipherment of lost languages. Given a non-parallel corpus in a known related language, our model produces both alphabetic mappings and translations of words into their corresponding cognates. We employ a non-parametric Bayesian framework to simultaneously capture both low-level character mappings and highlevel morphemic correspondences. This formulation enables us to encode some of the linguistic intuitions that have guided human decipherers. When applied to the ancient Semitic language Ugaritic, the model correctly maps 29 of 30 letters to their Hebrew counterparts, and deduces the correct Hebrew cognate for 60% of the Ugaritic words which have cognates in Hebrew.</p><p>3 0.67784053 <a title="100-lsi-3" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>Author: Vipul Mittal</p><p>Abstract: In this paper, we propose a novel method for automatic segmentation of a Sanskrit string into different words. The input for our segmentizer is a Sanskrit string either encoded as a Unicode string or as a Roman transliterated string and the output is a set of possible splits with weights associated with each of them. We followed two different approaches to segment a Sanskrit text using sandhi1 rules extracted from a parallel corpus of manually sandhi split text. While the first approach augments the finite state transducer used to analyze Sanskrit morphology and traverse it to segment a word, the second approach generates all possible segmentations and validates each constituent using a morph an- alyzer.</p><p>4 0.63331026 <a title="100-lsi-4" href="./acl-2010-The_Use_of_Formal_Language_Models_in_the_Typology_of_the_Morphology_of_Amerindian_Languages.html">234 acl-2010-The Use of Formal Language Models in the Typology of the Morphology of Amerindian Languages</a></p>
<p>Author: Andres Osvaldo Porta</p><p>Abstract: The aim of this work is to present some preliminary results of an investigation in course on the typology of the morphology of the native South American languages from the point of view of the formal language theory. With this object, we give two contrasting examples of descriptions of two Aboriginal languages finite verb forms morphology: Argentinean Quechua (quichua santiague n˜o) and Toba. The description of the morphology of the finite verb forms of Argentinean quechua, uses finite automata and finite transducers. In this case the construction is straightforward using two level morphology and then, describes in a very natural way the Argentinean Quechua morphology using a regular language. On the contrary, the Toba verbs morphology, with a system that simultaneously uses prefixes and suffixes, has not a natural description as regular language. Toba has a complex system of causative suffixes, whose successive applications determinate the use of prefixes belonging different person marking prefix sets. We adopt the solution of Creider et al. (1995) to naturally deal with this and other similar morphological processes which involve interactions between prefixes and suffixes and then we describe the toba morphology using linear context-free languages.1 .</p><p>5 0.59705192 <a title="100-lsi-5" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>6 0.52174836 <a title="100-lsi-6" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>7 0.51662731 <a title="100-lsi-7" href="./acl-2010-Finding_Cognate_Groups_Using_Phylogenies.html">116 acl-2010-Finding Cognate Groups Using Phylogenies</a></p>
<p>8 0.49118876 <a title="100-lsi-8" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>9 0.4718568 <a title="100-lsi-9" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>10 0.43468288 <a title="100-lsi-10" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>11 0.4202261 <a title="100-lsi-11" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>12 0.38734022 <a title="100-lsi-12" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>13 0.37139553 <a title="100-lsi-13" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>14 0.35900033 <a title="100-lsi-14" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>15 0.35836941 <a title="100-lsi-15" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>16 0.35666889 <a title="100-lsi-16" href="./acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</a></p>
<p>17 0.35595199 <a title="100-lsi-17" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>18 0.34272823 <a title="100-lsi-18" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>19 0.32766032 <a title="100-lsi-19" href="./acl-2010-Correcting_Errors_in_Speech_Recognition_with_Articulatory_Dynamics.html">74 acl-2010-Correcting Errors in Speech Recognition with Articulatory Dynamics</a></p>
<p>20 0.32544446 <a title="100-lsi-20" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.01), (14, 0.015), (16, 0.018), (25, 0.061), (33, 0.013), (39, 0.012), (42, 0.036), (44, 0.011), (45, 0.292), (59, 0.083), (73, 0.072), (78, 0.032), (80, 0.016), (83, 0.06), (84, 0.025), (98, 0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87795061 <a title="100-lda-1" href="./acl-2010-Hunting_for_the_Black_Swan%3A_Risk_Mining_from_Text.html">138 acl-2010-Hunting for the Black Swan: Risk Mining from Text</a></p>
<p>Author: Jochen Leidner ; Frank Schilder</p><p>Abstract: In the business world, analyzing and dealing with risk permeates all decisions and actions. However, to date, risk identification, the first step in the risk management cycle, has always been a manual activity with little to no intelligent software tool support. In addition, although companies are required to list risks to their business in their annual SEC filings in the USA, these descriptions are often very highlevel and vague. In this paper, we introduce Risk Mining, which is the task of identifying a set of risks pertaining to a business area or entity. We argue that by combining Web mining and Information Extraction (IE) techniques, risks can be detected automatically before they materialize, thus providing valuable business intelligence. We describe a system that induces a risk taxonomy with concrete risks (e.g., interest rate changes) at its leaves and more abstract risks (e.g., financial risks) closer to its root node. The taxonomy is induced via a bootstrapping algorithms starting with a few seeds. The risk taxonomy is used by the system as input to a risk monitor that matches risk mentions in financial documents to the abstract risk types, thus bridging a lexical gap. Our system is able to automatically generate company specific “risk maps”, which we demonstrate for a corpus of earnings report conference calls.</p><p>2 0.79402435 <a title="100-lda-2" href="./acl-2010-Modeling_Norms_of_Turn-Taking_in_Multi-Party_Conversation.html">173 acl-2010-Modeling Norms of Turn-Taking in Multi-Party Conversation</a></p>
<p>Author: Kornel Laskowski</p><p>Abstract: Substantial research effort has been invested in recent decades into the computational study and automatic processing of multi-party conversation. While most aspects of conversational speech have benefited from a wide availability of analytic, computationally tractable techniques, only qualitative assessments are available for characterizing multi-party turn-taking. The current paper attempts to address this deficiency by first proposing a framework for computing turn-taking model perplexity, and then by evaluating several multi-participant modeling approaches. Experiments show that direct multi-participant models do not generalize to held out data, and likely never will, for practical reasons. In contrast, the Extended-Degree-of-Overlap model represents a suitable candidate for future work in this area, and is shown to successfully predict the distribution of speech in time and across participants in previously unseen conversations.</p><p>same-paper 3 0.75130427 <a title="100-lda-3" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>Author: Sebastian Spiegler ; Peter A. Flach</p><p>Abstract: This paper demonstrates that the use of ensemble methods and carefully calibrating the decision threshold can significantly improve the performance of machine learning methods for morphological word decomposition. We employ two algorithms which come from a family of generative probabilistic models. The models consider segment boundaries as hidden variables and include probabilities for letter transitions within segments. The advantage of this model family is that it can learn from small datasets and easily gen- eralises to larger datasets. The first algorithm PROMODES, which participated in the Morpho Challenge 2009 (an international competition for unsupervised morphological analysis) employs a lower order model whereas the second algorithm PROMODES-H is a novel development of the first using a higher order model. We present the mathematical description for both algorithms, conduct experiments on the morphologically rich language Zulu and compare characteristics of both algorithms based on the experimental results.</p><p>4 0.57061523 <a title="100-lda-4" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>Author: David Jurgens ; Keith Stevens</p><p>Abstract: We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</p><p>5 0.53880751 <a title="100-lda-5" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>6 0.53789753 <a title="100-lda-6" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>7 0.53686202 <a title="100-lda-7" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>8 0.53661907 <a title="100-lda-8" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>9 0.53647643 <a title="100-lda-9" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>10 0.53610009 <a title="100-lda-10" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>11 0.53606534 <a title="100-lda-11" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>12 0.53541869 <a title="100-lda-12" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>13 0.53487694 <a title="100-lda-13" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>14 0.53371018 <a title="100-lda-14" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>15 0.53370273 <a title="100-lda-15" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>16 0.53342044 <a title="100-lda-16" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>17 0.53303111 <a title="100-lda-17" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>18 0.53262758 <a title="100-lda-18" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>19 0.53181505 <a title="100-lda-19" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>20 0.53154516 <a title="100-lda-20" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
