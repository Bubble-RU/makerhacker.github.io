<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-17" href="#">acl2010-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</h1>
<br/><p>Source: <a title="acl-2010-17-pdf" href="http://aclweb.org/anthology//P/P10/P10-2018.pdf">pdf</a></p><p>Author: Yotaro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: In predicate-argument structure analysis, it is important to capture non-local dependencies among arguments and interdependencies between the sense of a predicate and the semantic roles of its arguments. However, no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments. In this paper we propose a structured model that overcomes the limitation of existing approaches; the model captures both types of dependencies simultaneously by introducing four types of factors including a global factor type capturing non-local dependencies among arguments and a pairwise factor type capturing local dependencies between a predicate and an argument. In experiments the proposed model achieved competitive results compared to the stateof-the-art systems without applying any feature selection procedure.</p><p>Reference: <a title="acl-2010-17-reference" href="../acl2010_reference/acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 t Abstract In predicate-argument structure analysis, it is important to capture non-local dependencies among arguments and interdependencies between the sense of a predicate and the semantic roles of its arguments. [sent-4, score-1.119]
</p><p>2 However, no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments. [sent-5, score-0.289]
</p><p>3 In experiments the proposed model achieved competitive results compared to the stateof-the-art systems without applying any feature selection procedure. [sent-7, score-0.175]
</p><p>4 Arguments of a predicate are assigned particular semantic roles, such as Agent, Theme, Patient, etc. [sent-10, score-0.51]
</p><p>5 Lately, predicate-argument structure analysis has been regarded as a task of assigning semantic roles of arguments as well as word senses of a predicate (Surdeanu et al. [sent-11, score-0.793]
</p><p>6 Several researchers have paid much attention to predicate-argument structure analysis, and the fol-  lowing two important factors have been shown. [sent-14, score-0.13]
</p><p>7 (2009) presented importance of capturing non-local dependencies Masayuki Asahara Yuji Matsumoto Graduate School of Information Science Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {ma s ayu-a mat su} @ i . [sent-17, score-0.195]
</p><p>8 j p s  ,  of core arguments in predicate-argument structure analysis. [sent-19, score-0.17]
</p><p>9 They used argument sequences tied with a predicate sense (e. [sent-20, score-0.896]
</p><p>10 01/ActivePATIENT) as a feature for the re-ranker of the system where predicate sense and argument role candidates are generated by their pipelined architecture. [sent-23, score-1.036]
</p><p>11 They reported that incorporating this type of features provides substantial gain of the system performance. [sent-24, score-0.183]
</p><p>12 The other factor is inter-dependencies between a predicate sense and argument roles, which relate to selectional preference, and motivated us  to jointly identify a predicate sense and its argument roles. [sent-25, score-1.918]
</p><p>13 This type of dependencies has been explored by Riedel and Meza-Ruiz (2008; 2009b; 2009a), all of which use Markov Logic Networks (MLN). [sent-26, score-0.148]
</p><p>14 The work uses the global formulae that have atoms in terms of both a predicate sense and each of its argument roles, and the system identifies predicate senses and argument roles simultaneously. [sent-27, score-1.932]
</p><p>15 Ideally, we want to capture both types of dependencies simultaneously. [sent-28, score-0.165]
</p><p>16 The former approaches can not explicitly include features that capture inter-dependencies between a predicate sense and its argument roles. [sent-29, score-0.969]
</p><p>17 Though these are implicitly incorporated by re-ranking where the most plausible assignment is selected from a small subset of predicate and argument candidates, which are generated independently. [sent-30, score-0.805]
</p><p>18 On the other hand, it is difficult to deal with core argument features in MLN. [sent-31, score-0.346]
</p><p>19 Because the number of core arguments varies with the role assignments, this type of features cannot be expressed by a single formula. [sent-32, score-0.322]
</p><p>20 (2010) proposed a gener-  ative model that captures both predicate senses and its argument roles. [sent-34, score-0.878]
</p><p>21 However, the first-order markov assumption of the model eliminates ability to capture non-local dependencies among arguments. [sent-35, score-0.239]
</p><p>22 Also, generative models are in general inferior to discriminatively trained linear or log98  UppsalaP,r Sowce de dni,ng 1 s- o1f6 th Jeul AyC 2L01 20 1. [sent-36, score-0.034]
</p><p>23 Figure 1: Undirected graphical model representation of the structured model linear models. [sent-51, score-0.168]
</p><p>24 In this paper we propose a structured model that overcomes limitations of the previous approaches. [sent-52, score-0.146]
</p><p>25 For the model, we introduce several  types of features including those that capture both non-local dependencies of core arguments, and inter-dependencies between a predicate sense and its argument roles. [sent-53, score-1.139]
</p><p>26 By doing this, both tasks are mutually influenced, and the model determines the most plausible set of assignments of a predicate sense and its argument roles simultaneously. [sent-54, score-1.148]
</p><p>27 We present an exact inference algorithm for the model, and a large-margin learning algorithm that can handle both local and global features. [sent-55, score-0.145]
</p><p>28 Each node is assigned a particular predicate sense or an argument role label. [sent-61, score-0.992]
</p><p>29 The black squares are factors which provide scores of label assignments. [sent-62, score-0.174]
</p><p>30 In the model, the nodes for arguments depend on the predicate sense, and by influencing labels of a predicate sense and its argument roles, the most plausible label assignment of  the nodes is determined considering all factors. [sent-63, score-1.597]
</p><p>31 Let x be words in a sentence, p be a sense of a predicate in x, and A = {an}1N be a set of possible role label assignments {foar x. [sent-65, score-0.853]
</p><p>32 We define tihse r score nfutendct biyon a f pora predicate-argument dsterfuincetures as s(p, A) = ∑Fk∈F Fk (x, p, A). [sent-67, score-0.033]
</p><p>33 F is a set eosf aasll sth(pe, factors, ∑Fk (x∈, p, A) corresponds itos a particular factor in Figure 1, a,And) gives a score ttoo a predicate or argument label assignments. [sent-68, score-0.961]
</p><p>34 Since we use linear models, Fk (x, p, A) = w · Φk(x, p, A). [sent-69, score-0.034]
</p><p>35 1 Factors of the Model We define four types of factors for the model. [sent-71, score-0.13]
</p><p>36 Predicate Factor FP scores a sense of p, and does not depend on any arguments. [sent-72, score-0.167]
</p><p>37 The score function is defined by FP(x, p, A) = w·ΦP(x, p). [sent-73, score-0.033]
</p><p>38 Argument Factor FA scores a label assignment of a particular argument a ∈ A. [sent-74, score-0.345]
</p><p>39 The score is deteromfi nae pda independently nfrtoam ∈ a predicate sense, atnerdis given by FA(x, p, a) = w · ΦA(x, a). [sent-75, score-0.523]
</p><p>40 Predicate-Argument Pairwise Factor FPA captures inter-dependencies between a predicate sense and one of its argument roles. [sent-76, score-0.94]
</p><p>41 The score function is defined as FPA(x, p, a) = w · ΦPA(x, p, a). [sent-77, score-0.033]
</p><p>42 The difference from FA wis ·th Φat FPA influences both the predicate sense and the argument role. [sent-78, score-0.896]
</p><p>43 By introducing this factor, the role label can be influenced by the predicate sense, and vise versa. [sent-79, score-0.66]
</p><p>44 Global Factor FG is introduced to capture plausibility of the whole predicate-argument structure. [sent-80, score-0.085]
</p><p>45 Like the other factors, the score function is defined as FG(x, p, A) = w · ΦG(x, p, A). [sent-81, score-0.033]
</p><p>46 th Ais pfaosc-tor is the mutual dependencies among core arguments. [sent-83, score-0.17]
</p><p>47 For instance, if a predicate-argument structure has an agent (A0) followed by the predicate and a patient (A1), we encode the structure as a string A0-PRED-A1 and use it as a feature. [sent-84, score-0.5]
</p><p>48 This type of features provide plausibility of predicateargument structures. [sent-85, score-0.096]
</p><p>49 Even if the highest scoring predicate-argument structure with the other factors  misses some core arguments, the global feature demands the model to fill the missing arguments. [sent-86, score-0.354]
</p><p>50 The numbers of factors for each factor type are: FP and FG are 1, FA and FPA are |A| . [sent-87, score-0.284]
</p><p>51 By integrating the all factors, the score faurnect |iAo|n. [sent-88, score-0.033]
</p><p>52 2 Inference The crucial point of the model is how to deal with the global factor FG, because enumerating possible assignments is too costly. [sent-91, score-0.341]
</p><p>53 A number of methods have been proposed for the use of global features for linear models such as (Daum ´e III and Marcu, 2005; Kazama and Torisawa, 2007). [sent-92, score-0.159]
</p><p>54 Although the approach is proposed for sequence labeling tasks, it 99  can be easily extended to our structured That is, for each possible predicate sense predicate, we provide N-best argument signments using three local factors FP,  model. [sent-94, score-1.203]
</p><p>55 p of the role asFA and  FPA, and then add scores of the global factor FG, finally select the argmax from them. [sent-95, score-0.394]
</p><p>56 In this case, the argmax is selected from |Pl |N candidates. [sent-96, score-0.075]
</p><p>57 3 Learning the Model For learning of the model, we borrow a fundamental idea of Kazama and Torisawa’s perceptron learning algorithm. [sent-98, score-0.062]
</p><p>58 The margin perceptron learning proposed by Kazama and Torisawa can be seen as an optimiza-  tion with the following two constrains. [sent-104, score-0.127]
</p><p>59 (A) w·ΦL+G(x, y)−w·ΦL+G(x, ˆy L+G) ≥ ρ(y, ˆy L+G) (B) w · ΦL(x, y) − w · ΦL(x, ˆy L) ≥ ρ(y, ˆy L) (A) is the constraint that ensures a sufficient margin ρ(y, ˆy L+G) between y and ˆy L+G. [sent-105, score-0.093]
</p><p>60 (B) is the constraint that ensures a sufficient margin ρ(y, ˆy L) between y and ˆy L. [sent-106, score-0.093]
</p><p>61 The necessity of this constraint is that if we apply only (A), the algorithm does not guarantee a sufficient margin in terms of local features, and it leads to poor quality in the N-best assignments. [sent-107, score-0.113]
</p><p>62 The Kazama and Torisawa’s perceptron algorithm uses constant values for the cost function ρ(y, ˆy L+G) and ρ(y, ˆ yL). [sent-108, score-0.062]
</p><p>63 The proposed model is trained using the following optimization problem. [sent-109, score-0.033]
</p><p>64 In order to avoid this bias, we define a special sense label, senseany, that is used to calculate the score for a predicate and a roll-less argument, regardless of the predicate’s sense. [sent-111, score-0.661]
</p><p>65 We use the feature vector ΦPA(x, senseany, ak) if ak= “NONE00 and ΦPA(x, sensei, ak) other-  wise. [sent-112, score-0.044]
</p><p>66 It is a dataset for multi-lingual syntactic and semantic dependency parsing 1. [sent-116, score-0.049]
</p><p>67 Therefore the problems to be solved are predicate sense disambiguation and argument role labeling. [sent-118, score-1.028]
</p><p>68 For learning of the joint model, the loss function ρ(yt, y0) of the Passive-Aggressive Algorithm was set to the number of incorrect assignments of a predicate sense and its argument roles. [sent-121, score-1.012]
</p><p>69 Table 1 shows the features used for the structured model. [sent-123, score-0.096]
</p><p>70 The global features used for FG are based on those used in (Toutanova et al. [sent-124, score-0.125]
</p><p>71 1274G1046  Table 3: Predicate sense disambiguation and argu-  ment role labeling results (average). [sent-137, score-0.36]
</p><p>72 used for FPA are inspired by formulae used in the MLN-based SRL systems, such as (Meza-Ruiz and Riedel, 2009b). [sent-138, score-0.047]
</p><p>73 We used the same feature templates for all languages. [sent-139, score-0.086]
</p><p>74 By incorporating FPA, we achieved performance improvement for all languages. [sent-142, score-0.135]
</p><p>75 This results suggest that it is effective to capture local interdependencies between a predicate sense and one of its argument roles. [sent-143, score-1.055]
</p><p>76 Comparing the results with FP+FA and FP+FA+FG, incorporating FG also contributed performance improvements for all languages, especially the substantial F1 improvement of +1. [sent-144, score-0.196]
</p><p>77 By incorporating both FPA and FG, our joint model achieved competitive results compared to the top 2 systems (Bj o¨rkelund and Zhao), and achieved the better results than the Meza-Ruiz’s system 2. [sent-147, score-0.297]
</p><p>78 The systems by Bj ¨orkelund and Zhao applied feature selection algorithms in order to select the best set of feature templates for each language, requiring about 1 to 2 months to obtain the best feature set. [sent-148, score-0.174]
</p><p>79 On the other hand, our system achieved the competitive results with the top two systems, despite the fact that we used the same feature templates for all languages without applying any feature engineering procedure. [sent-149, score-0.228]
</p><p>80 Table 3 shows the performances of predicate sense disambiguation and argument role labeling separately. [sent-150, score-1.089]
</p><p>81 In terms of sense disambiguation results, incorporating FPA and FG worked well. [sent-151, score-0.29]
</p><p>82 Although incorporating either of FPA and FG provided improvements of +0. [sent-152, score-0.117]
</p><p>83 18 on average, adding both factors provided improvements of +0. [sent-154, score-0.16]
</p><p>84 We compared the predicate sense dis-  2The result of Meza-Ruiz for Czech is substantially worse than the other systems because of inappropriate preprocessing for predicate sense disambiguation. [sent-156, score-1.256]
</p><p>85 This result suggests that combination of these factors is effective for sense disambiguation. [sent-162, score-0.297]
</p><p>86 As for argument role labeling results, incorporating FPA and FG contributed positively for all languages. [sent-163, score-0.551]
</p><p>87 By incorporating FPA, the system achieved the F1 improvements of +0. [sent-166, score-0.165]
</p><p>88 This result shows that capturing inter-dependencies between a predicate and its arguments contributes to argument role labeling. [sent-168, score-0.991]
</p><p>89 By incorporating FG, the system achieved the substantial improvement of F1 (+1. [sent-169, score-0.175]
</p><p>90 Since both tasks improved by using all factors, we can say that the proposed joint model suc-  ceeded in joint learning of predicate senses and its argument roles. [sent-171, score-0.896]
</p><p>91 4  Conclusion  In this paper, we proposed a structured model that captures both non-local dependencies between arguments, and inter-dependencies between a predicate sense and its argument roles. [sent-172, score-1.161]
</p><p>92 We designed a linear model-based structured model, and defined four types of factors: predicate factor, argument factor, predicate-argument pairwise factor and global factor for the model. [sent-173, score-1.22]
</p><p>93 In the experiments, the proposed model achieved competitive results compared to the state-of-the-art systems without any feature engineering. [sent-174, score-0.175]
</p><p>94 Semisupervised semantic role labeling methods have been explored by (Collobert and Weston, 2008; Deschacht and Moens, 2009; F ¨urstenau and Lapata, 2009), and they have achieved successful outcomes. [sent-176, score-0.254]
</p><p>95 Learning as search optimization: Approximate large margin methods for structured prediction. [sent-192, score-0.133]
</p><p>96 Semi-supervised semantic role labeling using the latent words language model. [sent-196, score-0.206]
</p><p>97 The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. [sent-204, score-0.203]
</p><p>98 A new perceptron algorithm for sequence labeling with non-local features. [sent-212, score-0.123]
</p><p>99 Jointly identifying predicates, arguments and senses using markov logic. [sent-216, score-0.233]
</p><p>100 The  CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. [sent-228, score-0.114]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('predicate', 0.461), ('fpa', 0.395), ('fg', 0.304), ('argument', 0.268), ('sense', 0.167), ('fa', 0.139), ('kazama', 0.134), ('fp', 0.131), ('factors', 0.13), ('factor', 0.126), ('arguments', 0.12), ('dependencies', 0.12), ('torisawa', 0.117), ('orkelund', 0.099), ('global', 0.097), ('role', 0.096), ('roles', 0.091), ('incorporating', 0.087), ('fk', 0.086), ('assignments', 0.085), ('johansson', 0.083), ('bj', 0.077), ('argmax', 0.075), ('riedel', 0.074), ('ak', 0.073), ('senses', 0.072), ('structured', 0.068), ('interdependencies', 0.066), ('senseany', 0.066), ('margin', 0.065), ('sebastian', 0.062), ('perceptron', 0.062), ('labeling', 0.061), ('pa', 0.06), ('ivan', 0.058), ('nara', 0.058), ('surdeanu', 0.056), ('haji', 0.053), ('collobert', 0.053), ('urstenau', 0.053), ('core', 0.05), ('competitive', 0.05), ('nugues', 0.049), ('semantic', 0.049), ('toutanova', 0.049), ('local', 0.048), ('achieved', 0.048), ('deschacht', 0.047), ('formulae', 0.047), ('thompson', 0.047), ('capturing', 0.046), ('overcomes', 0.045), ('capture', 0.045), ('feature', 0.044), ('captures', 0.044), ('label', 0.044), ('plausible', 0.043), ('pierre', 0.043), ('templates', 0.042), ('markov', 0.041), ('mihai', 0.041), ('plausibility', 0.04), ('substantial', 0.04), ('pairwise', 0.04), ('contributed', 0.039), ('labelling', 0.039), ('meyers', 0.039), ('patient', 0.039), ('arquez', 0.036), ('disambiguation', 0.036), ('llu', 0.036), ('shared', 0.034), ('graduate', 0.034), ('linear', 0.034), ('model', 0.033), ('crammer', 0.033), ('assignment', 0.033), ('score', 0.033), ('daum', 0.032), ('joakim', 0.031), ('joint', 0.031), ('influenced', 0.03), ('improvements', 0.03), ('zhao', 0.029), ('jan', 0.029), ('asahara', 0.029), ('masayuki', 0.029), ('mat', 0.029), ('nae', 0.029), ('ofer', 0.029), ('ofonly', 0.029), ('stra', 0.029), ('ttoo', 0.029), ('vise', 0.029), ('watanabe', 0.029), ('wc', 0.029), ('czech', 0.029), ('ensures', 0.028), ('features', 0.028), ('type', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="17-tfidf-1" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>Author: Yotaro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: In predicate-argument structure analysis, it is important to capture non-local dependencies among arguments and interdependencies between the sense of a predicate and the semantic roles of its arguments. However, no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments. In this paper we propose a structured model that overcomes the limitation of existing approaches; the model captures both types of dependencies simultaneously by introducing four types of factors including a global factor type capturing non-local dependencies among arguments and a pairwise factor type capturing local dependencies between a predicate and an argument. In experiments the proposed model achieved competitive results compared to the stateof-the-art systems without applying any feature selection procedure.</p><p>2 0.27041081 <a title="17-tfidf-2" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>Author: Hector-Hugo Franco-Penya</p><p>Abstract: ―Tree SRL system‖ is a Semantic Role Labelling supervised system based on a tree-distance algorithm and a simple k-NN implementation. The novelty of the system lies in comparing the sentences as tree structures with multiple relations instead of extracting vectors of features for each relation and classifying them. The system was tested with the English CoNLL-2009 shared task data set where 79% accuracy was obtained. 1</p><p>3 0.27008668 <a title="17-tfidf-3" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>Author: Matthew Gerber ; Joyce Chai</p><p>Abstract: Despite its substantial coverage, NomBank does not account for all withinsentence arguments and ignores extrasentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.</p><p>4 0.26781395 <a title="17-tfidf-4" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>Author: Hirotoshi Taira ; Sanae Fujita ; Masaaki Nagata</p><p>Abstract: Maintaining high annotation consistency in large corpora is crucial for statistical learning; however, such work is hard, especially for tasks containing semantic elements. This paper describes predicate argument structure analysis using transformation-based learning. An advantage of transformation-based learning is the readability of learned rules. A disadvantage is that the rule extraction procedure is time-consuming. We present incremental-based, transformation-based learning for semantic processing tasks. As an example, we deal with Japanese predicate argument analysis and show some tendencies of annotators for constructing a corpus with our method.</p><p>5 0.23842832 <a title="17-tfidf-5" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>6 0.23372741 <a title="17-tfidf-6" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>7 0.22719832 <a title="17-tfidf-7" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>8 0.22027354 <a title="17-tfidf-8" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>9 0.16752699 <a title="17-tfidf-9" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>10 0.16512559 <a title="17-tfidf-10" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>11 0.11907544 <a title="17-tfidf-11" href="./acl-2010-Semantics-Driven_Shallow_Parsing_for_Chinese_Semantic_Role_Labeling.html">207 acl-2010-Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling</a></p>
<p>12 0.10069577 <a title="17-tfidf-12" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>13 0.099589683 <a title="17-tfidf-13" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>14 0.09693519 <a title="17-tfidf-14" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>15 0.095598921 <a title="17-tfidf-15" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>16 0.090972371 <a title="17-tfidf-16" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>17 0.088251717 <a title="17-tfidf-17" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>18 0.085053556 <a title="17-tfidf-18" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>19 0.084810592 <a title="17-tfidf-19" href="./acl-2010-Global_Learning_of_Focused_Entailment_Graphs.html">127 acl-2010-Global Learning of Focused Entailment Graphs</a></p>
<p>20 0.078941353 <a title="17-tfidf-20" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.22), (1, 0.149), (2, 0.307), (3, 0.147), (4, 0.116), (5, 0.032), (6, -0.147), (7, -0.029), (8, -0.014), (9, -0.077), (10, 0.107), (11, -0.04), (12, 0.049), (13, 0.051), (14, 0.035), (15, -0.081), (16, 0.001), (17, 0.038), (18, -0.099), (19, -0.029), (20, -0.001), (21, -0.013), (22, 0.002), (23, -0.157), (24, 0.119), (25, 0.136), (26, 0.1), (27, -0.059), (28, 0.119), (29, 0.063), (30, 0.03), (31, -0.005), (32, -0.125), (33, -0.033), (34, -0.119), (35, 0.095), (36, -0.007), (37, -0.008), (38, -0.052), (39, -0.03), (40, -0.076), (41, -0.008), (42, 0.006), (43, 0.018), (44, -0.052), (45, 0.038), (46, 0.027), (47, -0.016), (48, 0.057), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97457218 <a title="17-lsi-1" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>Author: Yotaro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: In predicate-argument structure analysis, it is important to capture non-local dependencies among arguments and interdependencies between the sense of a predicate and the semantic roles of its arguments. However, no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments. In this paper we propose a structured model that overcomes the limitation of existing approaches; the model captures both types of dependencies simultaneously by introducing four types of factors including a global factor type capturing non-local dependencies among arguments and a pairwise factor type capturing local dependencies between a predicate and an argument. In experiments the proposed model achieved competitive results compared to the stateof-the-art systems without applying any feature selection procedure.</p><p>2 0.85741556 <a title="17-lsi-2" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>Author: Matthew Gerber ; Joyce Chai</p><p>Abstract: Despite its substantial coverage, NomBank does not account for all withinsentence arguments and ignores extrasentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.</p><p>3 0.8133713 <a title="17-lsi-3" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>Author: Hirotoshi Taira ; Sanae Fujita ; Masaaki Nagata</p><p>Abstract: Maintaining high annotation consistency in large corpora is crucial for statistical learning; however, such work is hard, especially for tasks containing semantic elements. This paper describes predicate argument structure analysis using transformation-based learning. An advantage of transformation-based learning is the readability of learned rules. A disadvantage is that the rule extraction procedure is time-consuming. We present incremental-based, transformation-based learning for semantic processing tasks. As an example, we deal with Japanese predicate argument analysis and show some tendencies of annotators for constructing a corpus with our method.</p><p>4 0.70824838 <a title="17-lsi-4" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>Author: Hector-Hugo Franco-Penya</p><p>Abstract: ―Tree SRL system‖ is a Semantic Role Labelling supervised system based on a tree-distance algorithm and a simple k-NN implementation. The novelty of the system lies in comparing the sentences as tree structures with multiple relations instead of extracting vectors of features for each relation and classifying them. The system was tested with the English CoNLL-2009 shared task data set where 79% accuracy was obtained. 1</p><p>5 0.69903791 <a title="17-lsi-5" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>Author: Omri Abend ; Ari Rappoport</p><p>Abstract: The core-adjunct argument distinction is a basic one in the theory of argument structure. The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategorization acquisition. This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms. This is the first work to tackle this task in a fully unsupervised scenario.</p><p>6 0.61222869 <a title="17-lsi-6" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>7 0.58703905 <a title="17-lsi-7" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>8 0.57249248 <a title="17-lsi-8" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>9 0.53655285 <a title="17-lsi-9" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>10 0.47384089 <a title="17-lsi-10" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>11 0.420441 <a title="17-lsi-11" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>12 0.36659193 <a title="17-lsi-12" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>13 0.34657535 <a title="17-lsi-13" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>14 0.33742204 <a title="17-lsi-14" href="./acl-2010-Semantics-Driven_Shallow_Parsing_for_Chinese_Semantic_Role_Labeling.html">207 acl-2010-Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling</a></p>
<p>15 0.33571002 <a title="17-lsi-15" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>16 0.33297792 <a title="17-lsi-16" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>17 0.32992104 <a title="17-lsi-17" href="./acl-2010-Event-Based_Hyperspace_Analogue_to_Language_for_Query_Expansion.html">106 acl-2010-Event-Based Hyperspace Analogue to Language for Query Expansion</a></p>
<p>18 0.32551619 <a title="17-lsi-18" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>19 0.32369459 <a title="17-lsi-19" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<p>20 0.30480331 <a title="17-lsi-20" href="./acl-2010-WSD_as_a_Distributed_Constraint_Optimization_Problem.html">257 acl-2010-WSD as a Distributed Constraint Optimization Problem</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.029), (25, 0.085), (34, 0.226), (42, 0.012), (59, 0.066), (73, 0.066), (76, 0.031), (78, 0.132), (83, 0.085), (84, 0.038), (98, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84985191 <a title="17-lda-1" href="./acl-2010-Tools_for_Multilingual_Grammar-Based_Translation_on_the_Web.html">235 acl-2010-Tools for Multilingual Grammar-Based Translation on the Web</a></p>
<p>Author: Aarne Ranta ; Krasimir Angelov ; Thomas Hallgren</p><p>Abstract: This is a system demo for a set of tools for translating texts between multiple languages in real time with high quality. The translation works on restricted languages, and is based on semantic interlinguas. The underlying model is GF (Grammatical Framework), which is an open-source toolkit for multilingual grammar implementations. The demo will cover up to 20 parallel languages. Two related sets of tools are presented: grammarian’s tools helping to build translators for new domains and languages, and translator’s tools helping to translate documents. The grammarian’s tools are designed to make it easy to port the technique to new applications. The translator’s tools are essential in the restricted language context, enabling the author to remain in the fragments recognized by the system. The tools that are demonstrated will be ap- plied and developed further in the European project MOLTO (Multilingual On-Line Translation) which has started in March 2010 and runs for three years. 1 Translation Needs for the Web The best-known translation tools on the web are Google translate1 and Systran2. They are targeted to consumers of web documents: users who want to find out what a given document is about. For this purpose, browsing quality is sufficient, since the user has intelligence and good will, and understands that she uses the translation at her own risk. Since Google and Systran translations can be grammatically and semantically flawed, they don’t reach publication quality, and cannot hence be used by the producers of web documents. For instance, the provider of an e-commerce site cannot take the risk that the product descriptions or selling conditions have errors that change the original intentions. There are very few automatic translation systems actually in use for producers of information. As already 1www .google . com/t rans l e at 2www. systransoft . com noted by Bar-Hillel (1964), machine translation is one of those AI-complete tasks that involves a trade-off between coverage and precision, and the current mainstream systems opt for coverage. This is also what web users expect: they want to be able to throw just anything at the translation system and get something useful back. Precision-oriented approaches, the prime example of which is METEO (Chandioux 1977), have not been popular in recent years. However, from the producer’s point of view, large coverage is not essential: unlike the consumer’s tools, their input is predictable, and can be restricted to very specific domains, and to content that the producers themselves are creating in the first place. But even in such tasks, two severe problems remain: • • The development cost problem: a large amount oTfh ew dorekv eisl onpemedeendt f coors building tmra:n asl laatorgrse afomr new domains and new languages. The authoring problem: since the method does nTohte ew aourkth foorri nalgl input, etmhe: :asu tihnocer othfe eth me source toexest of translation may need special training to write in a way that can be translated at all. These two problems have probably been the main obstacles to making high-quality restricted language translation more wide-spread in tasks where it would otherwise be applicable. We address these problems by providing tools that help developers of translation systems on the one hand, and authors and translators—i.e. the users of the systems—on the other. In the MOLTO project (Multilingual On-Line Translation)3, we have the goal to improve both the development and use of restricted language translation by an order of magnitude, as compared with the state of the art. As for development costs, this means that a system for many languages and with adequate quality can be built in a matter of days rather than months. As for authoring, this means that content production does not require the use of manuals or involve trial and error, both of which can easily make the work ten times slower than normal writing. In the proposed system demo, we will show how some of the building blocks for MOLTO can already now be used in web-based translators, although on a 3 www.molto-project .eu 66 UppsalaP,r Sowceeeddenin,g 1s3 o Jfu tlhye 2 A0C1L0. 2 ?c 01200 S1y0s Atesmso Dcieamtioonns ftorart Cioonms,p puatagteiso 6n6a–l7 L1in,guistics Figure 1: A multilingual GF grammar with reversible mappings from a common abstract syntax to the 15 languages currently available in the GF Resource Grammar Library. smaller scale as regards languages and application domains. A running demo system is available at http : / / grammat i cal framework .org : 4 1 9 6. 2 2 Multilingual Grammars The translation tools are based on GF, Grammatical Framework4 (Ranta 2004). GF is a grammar formalism—that is, a mathematical model of natural language, equipped with a formal notation for writing grammars and a computer program implementing parsing and generation which are declaratively defined by grammars. Thus GF is comparable with formalism such as HPSG (Pollard and Sag 1994), LFG (Bresnan 1982) or TAG (Joshi 1985). The novel feature of GF is the notion of multilingual grammars, which describe several languages simultaneously by using a common representation called abstract syntax; see Figure 1. In a multilingual GF grammar, meaning-preserving translation is provided as a composition of parsing and generation via the abstract syntax, which works as an interlingua. This model of translation is different from approaches based on other comparable grammar formalisms, such as synchronous TAGs (Shieber and Schabes 1990), Pargram (Butt & al. 2002, based on LFG), LINGO Matrix (Bender and Flickinger 2005, based on HPSG), and CLE (Core Language Engine, Alshawi 1992). These approaches use transfer rules between individual languages, separate for each pair of languages. Being interlingua-based, GF translation scales up linearly to new languages without the quadratic blowup of transfer-based systems. In transfer-based sys- 4www.grammaticalframework.org tems, as many as n(n − 1) components (transfer functtieomnss), are naeneyde ads nto( cover a)l cl language pairs nisnf bero tfhu ndci-rections. In an interlingua-based system, 2n + 1components are enough: the interlingua itself, plus translations in both directions between each language and the interlingua. However, in GF, n + 1 components are sufficient, because the mappings from the abstract syntax to each language (the concrete syntaxes) are reversible, i.e. usable for both generation and parsing. Multilingual GF grammars can be seen as an implementation of Curry’s distinction between tectogrammatical and phenogrammatical structure (Curry 1961). In GF, the tectogrammatical structure is called abstract syntax, following standard computer science terminology. It is defined by using a logical framework (Harper & al. 1993), whose mathematical basis is in the type theory of Martin-L o¨f (1984). Two things can be noted about this architecture, both showing im- provements over state-of-the-art grammar-based translation methods. First, the translation interlingua (the abstract syntax) is a powerful logical formalism, able to express semantical structures such as context-dependencies and anaphora (Ranta 1994). In particular, dependent types make it more expressive than the type theory used in Montague grammar (Montague 1974) and employed in the Rosetta translation project (Rosetta 1998). Second, GF uses a framework for interlinguas, rather than one universal interlingua. This makes the interlingual approach more light-weight and feasible than in systems assuming one universal interlingua, such as Rosetta and UNL, Universal Networking Language5 . It also gives more precision to special-purpose translation: the interlingua of a GF translation system (i.e. the abstract syntax of a multilingual grammar) can encode precisely those structures and distinctions that are relevant for the task at hand. Thus an interlingua for mathematical proofs (Hallgren and Ranta 2000) is different from one for commands for operating an MP3 player (Perera and Ranta 2007). The expressive power of the logical framework is sufficient for both kinds of tasks. One important source of inspiration for GF was the WYSIWYM system (Power and Scott 1998), which used domain-specific interlinguas and produced excellent quality in multilingual generation. But the generation components were hard-coded in the program, instead of being defined declaratively as in GF, and they were not usable in the direction of parsing. 3 Grammars and Ontologies Parallel to the first development efforts of GF in the late 1990’s, another framework idea was emerging in web technology: XML, Extensible Mark-up Language, which unlike HTML is not a single mark-up language but a framework for creating custom mark-up lan5www .undl .org 67 guages. The analogy between GF and XML was seen from the beginning, and GF was designed as a formalism for multilingual rendering of semantic content (Dymetman and al. 2000). XML originated as a format for structuring documents and structured data serialization, but a couple ofits descendants, RDF(S) and OWL, developed its potential to formally express the semantics of data and content, serving as the fundaments of the emerging Semantic Web. Practically any meaning representation format can be converted into GF’s abstract syntax, which can then be mapped to different target languages. In particular the OWL language can be seen as a syntactic sugar for a subset of Martin-L o¨f’s type theory so it is trivial to embed it in GF’s abstract syntax. The translation problem defined in terms of an ontology is radically different from the problem of translating plain text from one language to another. Many of the projects in which GF has been used involve precisely this: a meaning representation formalized as GF abstract syntax. Some projects build on previously existing meaning representation and address mathematical proofs (Hallgren and Ranta 2000), software specifications (Beckert & al. 2007), and mathematical exercises (the European project WebALT6). Other projects start with semantic modelling work to build meaning representations from scratch, most notably ones for dialogue systems (Perera and Ranta 2007) in the European project TALK7. Yet another project, and one closest to web translation, is the multilingual Wiki system presented in (Meza Moreno and Bringert 2008). In this system, users can add and modify reviews of restaurants in three languages (English, Spanish, and Swedish). Any change made in any of the languages gets automatically translated to the other languages. To take an example, the OWL-to-GF mapping trans- lates OWL’s classes to GF’s categories and OWL’s properties to GF’s functions that return propositions. As a running example in this and the next section, we will use the class of integers and the two-place property of being divisible (“x is divisible by y”). The correspondences are as follows: Clas s (pp : intege r . . . ) m catm integer Ob j e ctP roperty ( pp :div domain (pp : int ege r ) range ( pp :integer ) ) m funm div : int eger -> 4 int ege r -> prop Grammar Engineer’s Tools In the GF setting, building a multilingual translation system is equivalent to building a multilingual GF 6EDC-22253, webalt .math .he l inki . fi s 7IST-507802, 2004–2006, www .t alk-pro j e ct .org grammar, which in turn consists of two kinds of components: • a language-independent abstract syntax, giving tahe l snegmuaangtei-ci nmdeopdeenl dveinat tw ahbisctrha ctrtan ssylnattiaoxn, gisi performed; • for each language, a concrete syntax mapping abfstorrac eta syntax turaegese ,t oa strings ien s tyhnatta language. While abstract syntax construction is an extra task compared to many other kinds of translation methods, it is technically relatively simple, and its cost is moreover amortized as the system is extended to new languages. Concrete syntax construction can be much more demanding in terms of programming skills and linguistic knowledge, due to the complexity of natural languages. This task is where GF claims perhaps the highest advantage over other approaches to special-purpose grammars. The two main assets are: • • Programming language support: GF is a modern fPuroncgtriaomnaml programming language, w isith a a powerful type system and module system supporting modular and collaborative programming and reuse of code. RGL, the GF Resource Grammar Library, implementing Fthe R bsoausicrc linguistic dre Ltaiiblsr orfy l iamn-guages: inflectional morphology and syntactic combination functions. The RGL covers fifteen languages at the moment, shown in Figure 1; see also Khegai 2006, El Dada and Ranta 2007, Angelov 2008, Ranta 2009a,b, and Enache et al. 2010. To give an example of what the library provides, let us first consider the inflectional morphology. It is presented as a set of lexicon-building functions such as, in English, mkV : St r -> V i.e. function mkV, which takes a string (St r) as its argument and returns a verb (V) as its value. The verb is, internally, an inflection table containing all forms of a verb. The function mkV derives all these forms from its argument string, which is the infinitive form. It predicts all regular variations: (mkV</p><p>same-paper 2 0.82455963 <a title="17-lda-2" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>Author: Yotaro Watanabe ; Masayuki Asahara ; Yuji Matsumoto</p><p>Abstract: In predicate-argument structure analysis, it is important to capture non-local dependencies among arguments and interdependencies between the sense of a predicate and the semantic roles of its arguments. However, no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments. In this paper we propose a structured model that overcomes the limitation of existing approaches; the model captures both types of dependencies simultaneously by introducing four types of factors including a global factor type capturing non-local dependencies among arguments and a pairwise factor type capturing local dependencies between a predicate and an argument. In experiments the proposed model achieved competitive results compared to the stateof-the-art systems without applying any feature selection procedure.</p><p>3 0.80062789 <a title="17-lda-3" href="./acl-2010-Hierarchical_A%2A_Parsing_with_Bridge_Outside_Scores.html">131 acl-2010-Hierarchical A* Parsing with Bridge Outside Scores</a></p>
<p>Author: Adam Pauls ; Dan Klein</p><p>Abstract: Hierarchical A∗ (HA∗) uses of a hierarchy of coarse grammars to speed up parsing without sacrificing optimality. HA∗ prioritizes search in refined grammars using Viterbi outside costs computed in coarser grammars. We present Bridge Hierarchical A∗ (BHA∗), a modified Hierarchial A∗ algorithm which computes a novel outside cost called a bridge outside cost. These bridge costs mix finer outside scores with coarser inside scores, and thus constitute tighter heuristics than entirely coarse scores. We show that BHA∗ substantially outperforms HA∗ when the hierarchy contains only very coarse grammars, while achieving comparable performance on more refined hierarchies.</p><p>4 0.6868946 <a title="17-lda-4" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>Author: Alan Ritter ; Mausam Mausam ; Oren Etzioni</p><p>Abstract: The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al. ’s system (Pantel et al., 2007).</p><p>5 0.68426371 <a title="17-lda-5" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>Author: Stefan Thater ; Hagen Furstenau ; Manfred Pinkal</p><p>Abstract: We present a syntactically enriched vector model that supports the computation of contextualized semantic representations in a quasi compositional fashion. It employs a systematic combination of first- and second-order context vectors. We apply our model to two different tasks and show that (i) it substantially outperforms previous work on a paraphrase ranking task, and (ii) achieves promising results on a wordsense similarity task; to our knowledge, it is the first time that an unsupervised method has been applied to this task.</p><p>6 0.68085021 <a title="17-lda-6" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>7 0.66292918 <a title="17-lda-7" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>8 0.66290647 <a title="17-lda-8" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>9 0.65426004 <a title="17-lda-9" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>10 0.65206385 <a title="17-lda-10" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>11 0.64492261 <a title="17-lda-11" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>12 0.64423054 <a title="17-lda-12" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>13 0.64275789 <a title="17-lda-13" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>14 0.64189261 <a title="17-lda-14" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>15 0.62854546 <a title="17-lda-15" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>16 0.62780416 <a title="17-lda-16" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>17 0.62734175 <a title="17-lda-17" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>18 0.6268568 <a title="17-lda-18" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>19 0.6265583 <a title="17-lda-19" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>20 0.62586051 <a title="17-lda-20" href="./acl-2010-Exemplar-Based_Models_for_Word_Meaning_in_Context.html">107 acl-2010-Exemplar-Based Models for Word Meaning in Context</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
