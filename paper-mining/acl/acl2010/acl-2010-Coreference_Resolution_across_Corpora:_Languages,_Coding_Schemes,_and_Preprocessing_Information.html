<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-72" href="#">acl2010-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</h1>
<br/><p>Source: <a title="acl-2010-72-pdf" href="http://aclweb.org/anthology//P/P10/P10-1144.pdf">pdf</a></p><p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>Reference: <a title="acl-2010-72-reference" href="../acl2010_reference/acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. [sent-2, score-0.547]
</p><p>2 By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. [sent-3, score-0.735]
</p><p>3 The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. [sent-4, score-0.576]
</p><p>4 They also ex-  pose systematic biases in the coreference evaluation metrics. [sent-5, score-0.451]
</p><p>5 1 Introduction The task of coreference resolution, which aims to automatically identify the expressions in a text that refer to the same discourse entity, has been an increasing research topic in NLP ever since MUC-6 made available the first coreferentially annotated corpus in 1995. [sent-7, score-0.521]
</p><p>6 Most research has centered around the rules by which mentions are allowed to corefer, the features characterizing mention pairs, the algorithms for building coreference chains, and coreference evaluation methods. [sent-8, score-1.115]
</p><p>7 We demonstrate the extent to which a system will be evaluated as performing differently depending on parameters such as the corpus language, the way coreference relations are defined in the corresponding coding scheme, and the  nature and source of preprocessing information. [sent-10, score-0.526]
</p><p>8 Third, we compare the performance using goldstandard preprocessing information with that using automatic preprocessing tools. [sent-15, score-0.184]
</p><p>9 Throughout, we apply the three principal coreference evaluation measures in use today: MUC, B3, and CEAF. [sent-16, score-0.429]
</p><p>10 This raises the difficult question of why one should use one or another evaluation measure, and how one should interpret their differences in reporting changes of performance score due to ‘secondary’ factors like preprocessing information. [sent-18, score-0.092]
</p><p>11 We apply the same coreference resolution system in all cases. [sent-23, score-0.547]
</p><p>12 Our goal is not to achieve the best performance to date, but rather to expose various issues raised by the choices of corpus preparation and evaluation measure and to shed light on the definition, methods, evaluation, and complexities of the coreference resolution task. [sent-25, score-0.574]
</p><p>13 2  Background  The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al. [sent-33, score-0.547]
</p><p>14 For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types. [sent-39, score-0.183]
</p><p>15 1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. [sent-40, score-0.117]
</p><p>16 A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al. [sent-41, score-0.083]
</p><p>17 Unlike ACE, it is not application-oriented, so coreference relations between all types of NPs are annotated. [sent-44, score-0.405]
</p><p>18 Since the MUC and ACE corpora are annotated with only coreference information,2 existing systems first preprocess the data using automatic tools (POS taggers, parsers, etc. [sent-46, score-0.468]
</p><p>19 However, given that the output from automatic tools is far from perfect, it is hard to determine the level of performance of a coreference module act-  ing on gold-standard preprocessing information. [sent-48, score-0.497]
</p><p>20 OntoNotes makes it possible to separate the coreference resolution problem from other tasks. [sent-49, score-0.547]
</p><p>21 (2009) that differences in corpora and in the task definitions need to be taken into account when comparing coreference resolution systems. [sent-51, score-0.584]
</p><p>22 Third, all our experiments use true mentions3 to avoid effects due to spurious system mentions. [sent-59, score-0.064]
</p><p>23 Finally, including different  baselines and variations ofthe resolution model allows us to reveal biases of the metrics. [sent-60, score-0.188]
</p><p>24 Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. [sent-61, score-0.262]
</p><p>25 4 Several coreference systems have been developed in the past (Culotta et al. [sent-64, score-0.405]
</p><p>26 A key element in this model are the entities the discourse is about, as they form the discourse backbone, especially those that are mentioned multiple times. [sent-72, score-0.148]
</p><p>27 Consider the growth of the entity Mount Popocat e´petl in  (1). [sent-74, score-0.075]
</p><p>28 5Following the ACE terminology, we use the term mention for an instance of reference to an object, and entity for a collection of mentions referring to the same object. [sent-78, score-0.38]
</p><p>29 Entities 1424  (1)  We have an update tonight on [this, the volcano in Mexico, they call El Popo]m3 . [sent-79, score-0.065]
</p><p>30 Mentions can be pronouns (m20), they can be a (shortened) string repetition using either the name (m7) or the type (m11), or they can add new information about the entity: m15 provides the supertype and informs the reader about the height of the volcano and its ranking position. [sent-85, score-0.14]
</p><p>31 In CISTELL,6 discourse entities are conceived as ‘baskets’ : they are empty at the beginning of the discourse, but keep growing as new attributes (e. [sent-86, score-0.153]
</p><p>32 Baskets are filled with this information, which can appear within a mention or elsewhere in the sentence. [sent-89, score-0.089]
</p><p>33 The ever-growing amount of information in a basket allows richer comparisons to new mentions encountered in the text. [sent-90, score-0.281]
</p><p>34 CISTELL follows the learning-based coreference architecture in which the task is split into classification and clustering (Soon et al. [sent-91, score-0.405]
</p><p>35 Clustering is identified with basket-  growing, the core process, and a pairwise classifier is called every time CISTELL considers whether a basket must be clustered into a (growing) basket, which might contain one or more mentions. [sent-93, score-0.122]
</p><p>36 We include this baseline given the high number of singletons in the datasets, since some evaluation measures are affected by large numbers of singletons. [sent-101, score-0.207]
</p><p>37 All non-pronominal NPs that have the same head are clustered into the same entity. [sent-104, score-0.117]
</p><p>38 containing one single mention are referred to as singletons. [sent-105, score-0.089]
</p><p>39 Like HEAD MATCH, plus allowing personal and possessive pronouns to link to the closest noun with which they agree in gender and number. [sent-109, score-0.142]
</p><p>40 , m11) is paired with previous mentions starting from the beginning of the document (m1–m11, m2– m11, etc. [sent-114, score-0.216]
</p><p>41 , m3–m11) is classified as coreferent, additional pairwise checks are performed with all the mentions contained in the (growing) entity basket (e. [sent-118, score-0.389]
</p><p>42 Only if all the pairs are classified as coreferent is the mention under consideration attached to the existing growing entity. [sent-121, score-0.198]
</p><p>43 Thus, the mention under analysis is linked to the most confident men-  tion among the previous ones, using TiMBL’s confidence score. [sent-131, score-0.089]
</p><p>44 A simplified version of STRONG MATCH: not all mentions in the growing entity need to be classified as coreferent with the mention under analysis. [sent-134, score-0.489]
</p><p>45 A single positive pairwise decision suffices for the mention to be clustered into that entity. [sent-135, score-0.146]
</p><p>46 8Taking the first mention classified as coreferent follows Soon et al. [sent-142, score-0.16]
</p><p>47 4 Evaluation Since they sometimes provide quite different results, we evaluate using three coreference measures, as there is no agreement on a standard. [sent-151, score-0.405]
</p><p>48 R and P are computed for each mention and averaged at the end. [sent-158, score-0.089]
</p><p>49 For each mention, the number of  common mentions between the true and the system entity is divided by the number of mentions in the true entity or in the system entity to obtain R and P, respectively. [sent-159, score-0.713]
</p><p>50 Using true mentions and the φ3 similarity function, R and P are the same and correspond to the number of common mentions between the aligned entities divided by the total number of mentions. [sent-163, score-0.542]
</p><p>51 Parameter 1: Language  The first experiment compared the performance of a coreference resolution system on a Germanic and a Romance language—English and Spanish— to explore to what extent language-specific issues such as zero subjects11 or grammatical gender might influence a system. [sent-164, score-0.628]
</p><p>52 More importantly, very similar coreference annotation guidelines make AnCora the ideal Spanish counterpart to OntoNotes. [sent-166, score-0.405]
</p><p>53 Corpus statistics about the distribution of mentions and entities are shown in Tables 1and 2. [sent-169, score-0.298]
</p><p>54 Given that this paper is focused on coreference between NPs, the number of mentions only includes NPs. [sent-170, score-0.621]
</p><p>55 Both AnCora and OntoNotes annotate only multi-mention entities (i. [sent-171, score-0.082]
</p><p>56 , those containing two or more coreferent mentions), so singleton entities are assumed to correspond to NPs with no coreference annotation. [sent-173, score-0.615]
</p><p>57 Apart from a larger number of mentions in  Spanish (Table 1), the two datasets look very similar in the distribution of singletons and multimention entities: about 85% and 15%, respectively. [sent-174, score-0.435]
</p><p>58 The distribution of mention types (Table 2), however, differs in two important respects: AnCora has a smaller number of personal pronouns as Spanish typically uses zero subjects, and it has a smaller number of bare NPs as the definite article accompanies more NPs than in English. [sent-178, score-0.215]
</p><p>59 , including features about zero subjects or removing those about possessive phrases. [sent-317, score-0.069]
</p><p>60 Comparing the feature ranks, we find that the features that work best for each language largely overlap and are language independent, like head match, is-a match, and whether the mentions are pronominal. [sent-318, score-0.309]
</p><p>61 Datasets Since the two annotation schemes differ significantly, we made the results comparable by mapping the ACE entities (the simpler scheme) onto the information contained in OntoNotes. [sent-324, score-0.116]
</p><p>62 13 The mapping allowed us to focus exclusively on the differences expressed on both corpora: the types of mentions that were annotated, the definition of identity of reference, etc. [sent-325, score-0.216]
</p><p>63 The mapping was not straightforward due to several problems: there was no match for some mentions due to syntactic or spelling reasons (e. [sent-327, score-0.474]
</p><p>64 ACE mentions for which there was no parse tree node in the OntoNotes gold-standard tree were omitted, as creating a new node could have damaged the tree. [sent-331, score-0.216]
</p><p>65 Given that only seven entity types are annotated in ACE, the number of OntoNotes mentions is al13Both OntoNotes  entities and dataset. [sent-332, score-0.399]
</p><p>66 01  Table 5: CISTELL results varying the annotation scheme on gold-standard data. [sent-438, score-0.082]
</p><p>67 Also, given that ACE entities correspond to types that are usually coreferred (e. [sent-447, score-0.082]
</p><p>68 ), singletons only represent 61% of all entities, while they are 85% in OntoNotes. [sent-450, score-0.183]
</p><p>69 A second major difference is the definition of coreference relations, illustrated here: (2)  [This] was [an all-white, all-Christian community  tghroatu palsl]. [sent-453, score-0.405]
</p><p>70 In ACE, nominal predicates corefer with their subject (2), and appositive phrases corefer with the noun they are modifying (3). [sent-462, score-0.176]
</p><p>71 In contrast, they do not fall under the identity relation in  OntoNotes, which follows the linguistic understanding of coreference according to which nominal predicates and appositives express properties of an entity rather than refer to a second (coreferent) entity (van Deemter and Kibble, 2000). [sent-463, score-0.58]
</p><p>72 Finally, the two schemes frequently disagree on borderline cases in which coreference turns out to be especially complex (4). [sent-464, score-0.439]
</p><p>73 In contrast, the  score improvement achieved by HEAD MATCH is much more noticeable on ACE than on OntoNotes, which indicates that many of its coreferent mentions share the same head. [sent-472, score-0.287]
</p><p>74 The systematic biases of the measures that were observed in Table 3 appear again in the case of 1428  MUC and B3. [sent-473, score-0.07]
</p><p>75 The feature rankings obtained for each dataset generally coincide as to which features are ranked best (namely NE match, is-a match, and head match), but differ in their particular ordering. [sent-476, score-0.121]
</p><p>76 , head match) that systematically govern coreference relationships; rather, coreference appeals to individual unique phenomena appearing in each context, and thus after a point adding more training data does not add much new generalizable information. [sent-481, score-0.903]
</p><p>77 6  Parameter 3: Preprocessing  The goal of the third experiment was to determine how much the source and nature of preprocessing information matters. [sent-484, score-0.092]
</p><p>78 Since it is often stated that coreference resolution depends on many levels of analysis, we again compared the two corpora, which differ in the amount and correctness of such information. [sent-485, score-0.547]
</p><p>79 However, in this experiment, entity mapping was applied in the opposite direction: the OntoNotes entities were mapped onto the automatically preprocessed ACE dataset. [sent-486, score-0.208]
</p><p>80 This ex-  poses the shortcomings of automated preprocessing in ACE for identifying all the mentions identified and linked in OntoNotes. [sent-487, score-0.308]
</p><p>81 Missing parse tree nodes in the automatically parsed data account for the considerably lower number of OntoNotes mentions (approx. [sent-496, score-0.216]
</p><p>82 14 However, the proportions of singleton:multi-mention entities as  well as the average entity size do not vary. [sent-498, score-0.157]
</p><p>83 Results and Discussion The ACE scores for the automatically preprocessed models in Table 7 are about 3% lower than those based on OntoNotes gold-standard data in Table 5, providing evidence for the advantage offered by gold-standard preprocessing information. [sent-499, score-0.143]
</p><p>84 In contrast, the similar—if not higher—scores of OntoNotes can be attributed to the use of the annotated ACE entity types. [sent-500, score-0.101]
</p><p>85 The fact that these are annotated not only for proper nouns (as predicted by an automatic NER) but also for pronouns and full NPs is a very helpful feature for a coreference resolution system. [sent-501, score-0.646]
</p><p>86 Since we are evaluating now on the ACE data, the NE match feature is also ranked first for OntoNotes. [sent-504, score-0.258]
</p><p>87 Head and is-a  match are still ranked among the best, yet syntactic features are not. [sent-505, score-0.258]
</p><p>88 Given that the noise brought by automatic preprocessing can be harmful, we tried leaving out the grammatical function feature. [sent-508, score-0.092]
</p><p>89 This points out that conclusions drawn from automatically preprocessed data about the kind of knowledge relevant for coreference resolution might be mistaken. [sent-510, score-0.598]
</p><p>90 Using the most successful basic features can lead to the best results when only automatic preprocessing is available. [sent-511, score-0.092]
</p><p>91 14Inordertomakethesetofmentionsassimilaraspossible to the set in Section 5, OntoNotes singletons were mapped from the ones detected in the gold-standard treebank. [sent-512, score-0.183]
</p><p>92 08  Table 7: CISTELL results varying the annotation scheme on automatically preprocessed data. [sent-618, score-0.133]
</p><p>93 It is a cause for concern that they provide contradictory indications about the core of coreference, namely the resolution models—for example, the model ranked highest by B3 in Table 7 is ranked lowest by MUC. [sent-621, score-0.168]
</p><p>94 The performance of the six participating systems shows similar problems with the evaluation metrics, and the singleton baseline was hard to beat even by the highest-performing systems. [sent-625, score-0.057]
</p><p>95 Since the measures imply different conclusions about the nature of the corpora and the preprocessing information applied, should we use them now to constrain the ways our corpora are created in the first place, and what preprocessing we include or omit? [sent-626, score-0.282]
</p><p>96 Global joint models for coreference resolution and named entity classification. [sent-658, score-0.622]
</p><p>97 Simple coreference resolution with rich syntactic and semantic features. [sent-675, score-0.547]
</p><p>98 A mentionsynchronous coreference resolution algorithm based on the Bell tree. [sent-702, score-0.547]
</p><p>99 A machine learning approach to coreference resolution of noun phrases. [sent-750, score-0.547]
</p><p>100 Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. [sent-754, score-0.405]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ontonotes', 0.418), ('coreference', 0.405), ('ace', 0.329), ('match', 0.258), ('mentions', 0.216), ('cistell', 0.194), ('muc', 0.19), ('singletons', 0.183), ('resolution', 0.142), ('ancora', 0.142), ('recasens', 0.113), ('ceaf', 0.11), ('tterastining', 0.097), ('head', 0.093), ('preprocessing', 0.092), ('pron', 0.091), ('mention', 0.089), ('super', 0.084), ('nps', 0.082), ('entities', 0.082), ('entity', 0.075), ('pronouns', 0.073), ('coreferent', 0.071), ('strong', 0.069), ('basket', 0.065), ('popo', 0.065), ('weak', 0.064), ('spanish', 0.064), ('luo', 0.063), ('singleton', 0.057), ('corefer', 0.057), ('coreferentially', 0.057), ('scheme', 0.055), ('preprocessed', 0.051), ('marta', 0.049), ('timbl', 0.049), ('docs', 0.049), ('biases', 0.046), ('possessive', 0.042), ('mart', 0.041), ('bengtson', 0.039), ('volcano', 0.039), ('growing', 0.038), ('corpora', 0.037), ('hovy', 0.037), ('appositive', 0.037), ('datasets', 0.036), ('soon', 0.036), ('spurious', 0.036), ('culotta', 0.035), ('xiaoqiang', 0.035), ('schemes', 0.034), ('discourse', 0.033), ('denis', 0.033), ('nia', 0.033), ('daelemans', 0.033), ('doddington', 0.033), ('stoyanov', 0.033), ('pairwise', 0.033), ('baskets', 0.032), ('hyman', 0.032), ('kudoh', 0.032), ('mayor', 0.032), ('postville', 0.032), ('deemter', 0.032), ('scoring', 0.03), ('lance', 0.029), ('ramshaw', 0.029), ('coding', 0.029), ('ant', 0.029), ('pradhan', 0.029), ('isolate', 0.028), ('supertype', 0.028), ('rankings', 0.028), ('true', 0.028), ('varying', 0.027), ('zero', 0.027), ('gender', 0.027), ('issues', 0.027), ('annotated', 0.026), ('definite', 0.026), ('tonight', 0.026), ('tjong', 0.026), ('contradictory', 0.026), ('hirschman', 0.026), ('lynette', 0.026), ('tdt', 0.026), ('vilain', 0.026), ('nominal', 0.025), ('cardie', 0.024), ('clustered', 0.024), ('sang', 0.024), ('kibble', 0.024), ('demonstrative', 0.024), ('malt', 0.024), ('romance', 0.024), ('tnt', 0.024), ('ng', 0.024), ('measures', 0.024), ('finkel', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="72-tfidf-1" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>2 0.42356139 <a title="72-tfidf-2" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>3 0.35266161 <a title="72-tfidf-3" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>4 0.33029714 <a title="72-tfidf-4" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>5 0.22561535 <a title="72-tfidf-5" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>Author: Cosmin Bejan ; Sanda Harabagiu</p><p>Abstract: This paper examines how a new class of nonparametric Bayesian models can be effectively applied to an open-domain event coreference task. Designed with the purpose of clustering complex linguistic objects, these models consider a potentially infinite number of features and categorical outcomes. The evaluation performed for solving both within- and cross-document event coreference shows significant improvements of the models when compared against two baselines for this task.</p><p>6 0.21456851 <a title="72-tfidf-6" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>7 0.18860367 <a title="72-tfidf-7" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>8 0.15311755 <a title="72-tfidf-8" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>9 0.13690881 <a title="72-tfidf-9" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>10 0.11048301 <a title="72-tfidf-10" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>11 0.10598299 <a title="72-tfidf-11" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>12 0.096633382 <a title="72-tfidf-12" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>13 0.096048027 <a title="72-tfidf-13" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>14 0.083920173 <a title="72-tfidf-14" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>15 0.078911312 <a title="72-tfidf-15" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>16 0.063190825 <a title="72-tfidf-16" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>17 0.062440958 <a title="72-tfidf-17" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>18 0.060860299 <a title="72-tfidf-18" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>19 0.060042098 <a title="72-tfidf-19" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>20 0.049722701 <a title="72-tfidf-20" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.191), (1, 0.114), (2, 0.03), (3, -0.299), (4, -0.167), (5, 0.446), (6, -0.006), (7, 0.032), (8, 0.064), (9, 0.188), (10, 0.058), (11, -0.069), (12, -0.001), (13, -0.116), (14, 0.057), (15, -0.025), (16, 0.007), (17, -0.056), (18, -0.054), (19, -0.002), (20, -0.037), (21, 0.004), (22, 0.001), (23, -0.077), (24, -0.002), (25, -0.014), (26, -0.023), (27, -0.002), (28, 0.024), (29, -0.03), (30, 0.019), (31, -0.007), (32, 0.032), (33, 0.013), (34, -0.079), (35, -0.035), (36, -0.068), (37, 0.011), (38, 0.028), (39, 0.02), (40, 0.003), (41, -0.009), (42, 0.022), (43, 0.05), (44, -0.011), (45, 0.034), (46, 0.046), (47, 0.003), (48, -0.01), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97173774 <a title="72-lsi-1" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>2 0.95413977 <a title="72-lsi-2" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>3 0.92472404 <a title="72-lsi-3" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>4 0.88339156 <a title="72-lsi-4" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>5 0.62564975 <a title="72-lsi-5" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>Author: Aria Haghighi ; Dan Klein</p><p>Abstract: We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.</p><p>6 0.57016152 <a title="72-lsi-6" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>7 0.48494056 <a title="72-lsi-7" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>8 0.46251908 <a title="72-lsi-8" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>9 0.45375416 <a title="72-lsi-9" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>10 0.43657798 <a title="72-lsi-10" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>11 0.40340051 <a title="72-lsi-11" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>12 0.33282652 <a title="72-lsi-12" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>13 0.27634004 <a title="72-lsi-13" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>14 0.27012223 <a title="72-lsi-14" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>15 0.26535773 <a title="72-lsi-15" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>16 0.25953257 <a title="72-lsi-16" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>17 0.24809304 <a title="72-lsi-17" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>18 0.2368781 <a title="72-lsi-18" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>19 0.21117574 <a title="72-lsi-19" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>20 0.2108312 <a title="72-lsi-20" href="./acl-2010-Comparable_Entity_Mining_from_Comparative_Questions.html">63 acl-2010-Comparable Entity Mining from Comparative Questions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.011), (25, 0.041), (42, 0.022), (44, 0.011), (54, 0.01), (59, 0.063), (73, 0.043), (76, 0.017), (78, 0.024), (80, 0.015), (83, 0.511), (84, 0.02), (88, 0.027), (98, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98361516 <a title="72-lda-1" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>Author: Beata Beigman Klebanov ; Eyal Beigman ; Daniel Diermeier</p><p>Abstract: We establish the following characteristics of the task of perspective classification: (a) using term frequencies in a document does not improve classification achieved with absence/presence features; (b) for datasets allowing the relevant comparisons, a small number of top features is found to be as effective as the full feature set and indispensable for the best achieved performance, testifying to the existence of perspective-specific keywords. We relate our findings to research on word frequency distributions and to discourse analytic studies of perspective.</p><p>same-paper 2 0.97690237 <a title="72-lda-2" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>3 0.97572362 <a title="72-lda-3" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>Author: Katrin Tomanek ; Udo Hahn ; Steffen Lohmann ; Jurgen Ziegler</p><p>Abstract: We report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eyetracking device. As experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics. Our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and, more interestingly, indicates that fullscale context is mostly negligible with – the exception of semantic high-complexity cases. We then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models. Our data reveals that the cognitively founded model explains annotation costs (expressed in annotation time) more adequately than non-cognitive ones.</p><p>4 0.96073741 <a title="72-lda-4" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>Author: Emily Pitler ; Annie Louis ; Ani Nenkova</p><p>Abstract: To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference informa- tion, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.</p><p>5 0.95838332 <a title="72-lda-5" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>Author: Jenny Rose Finkel ; Christopher D. Manning</p><p>Abstract: One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. Experiments on joint parsing and named entity recog- nition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.</p><p>6 0.87569833 <a title="72-lda-6" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>7 0.85698223 <a title="72-lda-7" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>8 0.84783691 <a title="72-lda-8" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>9 0.78158844 <a title="72-lda-9" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>10 0.78017831 <a title="72-lda-10" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>11 0.77327985 <a title="72-lda-11" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>12 0.7692554 <a title="72-lda-12" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>13 0.75148946 <a title="72-lda-13" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>14 0.74380183 <a title="72-lda-14" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>15 0.74236494 <a title="72-lda-15" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>16 0.74044853 <a title="72-lda-16" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>17 0.73371291 <a title="72-lda-17" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>18 0.72634786 <a title="72-lda-18" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>19 0.72349489 <a title="72-lda-19" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>20 0.72273952 <a title="72-lda-20" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
