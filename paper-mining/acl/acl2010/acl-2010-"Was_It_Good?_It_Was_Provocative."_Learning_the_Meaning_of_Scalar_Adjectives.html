<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-2" href="#">acl2010-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</h1>
<br/><p>Source: <a title="acl-2010-2-pdf" href="http://aclweb.org/anthology//P/P10/P10-1018.pdf">pdf</a></p><p>Author: Marie-Catherine de Marneffe ; Christopher D. Manning ; Christopher Potts</p><p>Abstract: Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.</p><p>Reference: <a title="acl-2010-2-reference" href="../acl2010_reference/acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ” Learning the meaning of scalar adjectives Marie-Catherine de Marneffe, Christopher D. [sent-3, score-0.303]
</p><p>2 For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. [sent-6, score-0.256]
</p><p>3 In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. [sent-12, score-0.519]
</p><p>4 We show how to ground scalar modifier meaning based on data collected from the Web. [sent-13, score-0.351]
</p><p>5 We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . [sent-14, score-0.634]
</p><p>6 To evaluate the methods,  we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . [sent-15, score-0.921]
</p><p>7 Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference. [sent-16, score-0.386]
</p><p>8 For instance, answers to polar (yes/no) questions do not always explicitly contain a ‘yes’ or ‘no’, but rather give information that the hearer can use to infer such an answer in a context with some degree of certainty. [sent-18, score-0.594]
</p><p>9 (1997) find that 27% of answers to polar questions do not contain a direct  ‘yes’ or ‘no’ word, 44% of which they regard as failing to convey a clear ‘yes’ or ‘no’ response. [sent-20, score-0.365]
</p><p>10 In some cases, interpreting the answer is straightforward (Was it bad? [sent-21, score-0.268]
</p><p>11 ), but in others, what to infer from the answer is unclear (Was it good? [sent-23, score-0.263]
</p><p>12 It is even common for the speaker to explicitly convey his own uncertainty with such answers. [sent-26, score-0.208]
</p><p>13 In this paper, we focus on the interpretation of answers to a particular class of polar questions: ones in which the main predication involves a gradable modifier (e. [sent-27, score-0.598]
</p><p>14 , highly unusual, not good, little) and the answer either involves another gradable modifier or a numerical expression (e. [sent-29, score-0.623]
</p><p>15 Interpreting such question–answer pairs requires dealing with modifier meanings, specifically, learning context-dependent scales of expressions (Horn, 1972; Fauconnier, 1975) that determine how, and to what extent, the answer as a whole resolves the issue raised by the question. [sent-32, score-0.417]
</p><p>16 We propose two methods for learning the knowledge necessary for interpreting indirect an-  swers to questions involving gradable adjectives, depending on the type of predications in the question and the answer. [sent-33, score-0.612]
</p><p>17 We thus use a large collection of online reviews to learn orderings between adjectives based on contextual entailment (good < excellent), and employ this scalar relationship to infer a yes/no answer (subject to negation and other qualifiers). [sent-35, score-0.742]
</p><p>18 c As2s0o1c0ia Atisosnoc foiart Cionom fopru Ctaotmiopnuatla Lti onngaulis Lti cnsg,u piasgtiecss 167–176,  tive or negative instance of the adjective in question. [sent-39, score-0.297]
</p><p>19 In the computational literature, Green and Carberry (1994, 1999) provide an extensive model that interprets and generates indirect answers to polar questions. [sent-43, score-0.37]
</p><p>20 However, to adequately interpret indirect answers, the uncertainty inherent in some answers needs to be captured (de Marneffe et al. [sent-45, score-0.383]
</p><p>21 While a straightforward ‘yes’ or ‘no’ response is clear in some indirect answers, such as in (1), the intended answer is less  certain in other cases (2):1 (1) A: Do you think that’s a good idea, that we just begin to ignore these numbers? [sent-47, score-0.711]
</p><p>22 As proposed by Zeevat (1994) in his work on partial answers, the speaker’s indirect answer might indicate that he is deliberately leaving the original question only partially addressed, while giving a fully resolving answer to another one. [sent-52, score-0.631]
</p><p>23 The hearer must then interpret the answer to work out the other question. [sent-53, score-0.267]
</p><p>24 The speaker is uncertain of ‘yes’ or ‘no’ and conveys this uncertainty to the hearer. [sent-61, score-0.418]
</p><p>25 The speaker is uncertain of ‘yes’ or ‘no’ , but the hearer infers one of those with confidence. [sent-63, score-0.282]
</p><p>26 The uncertainty is especially pressing for predications built around scalar modifiers, which are inherently vague and highly context-dependent (Kamp and Partee, 1995; Kennedy and McNally, 2005; Kennedy, 2007). [sent-64, score-0.411]
</p><p>27 We used regular expressions and manual filtering to find examples of twoutterance dialogues in which the question and the reply contain some kind of gradable modifier. [sent-72, score-0.402]
</p><p>28 1 Types of question–answer pairs In total, we ended up with 224 question–answer pairs involving gradable adjectives. [sent-74, score-0.225]
</p><p>29 However our collection contains different types of answers,  which naturally fall into two categories: (I) in 205 dialogues, both the question and the answer contain a gradable modifier; (II) in 19 dialogues, the reply contains a numerical measure (as in (3) above and (4)). [sent-75, score-0.659]
</p><p>30 168  Modification in answerExampleCount IOther adjective(1), (2)125 Adverb - same adjective (5) 55 Negation - same adjective (6), (7) 21 Omitted adjective (8) 4 IINumerical measure(3), (4)19 Table 1: Types of question–answer counts in the corpus. [sent-76, score-0.684]
</p><p>31 In most dialogues, the answer contains another adjective than the one used in the question, such as in (1). [sent-91, score-0.437]
</p><p>32 In others, the answer contains the same adjective as in the question, but modified by an adverb (e. [sent-92, score-0.503]
</p><p>33 The negation can be present in the main clause when the adjectival predication is embedded, as in example (7). [sent-100, score-0.258]
</p><p>34 In a few cases, when the question contains an adjective modifying a noun, the adjective is omitted in the answer: (8) A: Is that a huge gap in the system? [sent-106, score-0.524]
</p><p>35 2 Answer assignment To assess the degree to which each answer conveys ‘yes’ or ‘no’ in context, we use response distributions from Mechanical Turk workers. [sent-110, score-0.518]
</p><p>36 Given a written dialogue between speakers A and B, Turkers were asked to judge what B’s answer conveys: ‘definite yes’ , ‘probable yes’, ‘uncertain’, ‘probable no’, ‘definite no’ . [sent-111, score-0.244]
</p><p>37 Within each of the two ‘yes’ and ‘no’ pairs, there is a scalar relationship, but the pairs themselves are not in a scalar relationship with each other, and ‘uncertain’ is arguably a separate judgment. [sent-112, score-0.518]
</p><p>38 For each dialogue, we got answers from 30 Turkers, and we took the dominant response as the correct one though we make extensive use of the full response distribu-  tions in evaluating our approach. [sent-114, score-0.622]
</p><p>39 2 We also computed entropy values for the distribution of answers for each item. [sent-115, score-0.217]
</p><p>40 0 11 in the “adjective” category, 9 in the “adverb-adjective” category and 1 in the “negation” category), and 80 items are such that a single response got chosen 20 or more times (entropy < 0. [sent-117, score-0.365]
</p><p>41 The Fleiss’ Kappa score for the five response categories is 0. [sent-134, score-0.239]
</p><p>42 For the three-category response system used in section 5, which arguably has no scalar ordering, the Fleiss’ Kappa is 0. [sent-136, score-0.459]
</p><p>43 Interestingly, the pairs involving an adverbial modifi-  cation in the answer all received a positive answer (‘yes’ or ‘probable yes’) as dominant response. [sent-146, score-0.583]
</p><p>44 All 19 dialogues involving a numerical measure had either ‘probable yes’ or ‘uncertain’ as dominant response. [sent-147, score-0.335]
</p><p>45 There is thus a significant bias for positive answers: 70% of the category Iitems and 74% of the category II items have a positive answer as dominant response. [sent-148, score-0.696]
</p><p>46 4  Methods  In this section, we present the methods we propose for grounding the meanings of scalar modifiers. [sent-151, score-0.332]
</p><p>47 1 Learning modifier scales and inferring yes/no answers The first technique targets items such as the ones  in category I our corpus. [sent-153, score-0.569]
</p><p>48 Our central hypothesis of is that, in polar question dialogues, the semantic relationship between the main predication PQ in the question and the main predication PA in the answer is the primary factor in determining whether, and to what extent, ‘yes’ or ‘no’ was intended. [sent-154, score-0.609]
</p><p>49 If PA is at least as strong as PQ, the intended answer is ‘yes’ ; if PA is weaker than PQ, the intended answer is ‘no’ ; and, where no reliable entailment relationship exists between PA and PQ, the result is uncertainty. [sent-155, score-0.5]
</p><p>50 For example, good is weaker (lower on the relevant scale) than excellent, and thus speakers infer that the reply in example (1) above is meant to convey ‘yes’ . [sent-156, score-0.242]
</p><p>51 ; It’s a good idea then speakers infer that the answer conveys ‘no’ . [sent-158, score-0.445]
</p><p>52 Had B replied with It’s a complicated idea in (1), then uncertainty would likely have resulted, since good and complicated are not in a reliable scalar relationship. [sent-159, score-0.382]
</p><p>53 When both the question and the answer contain a modifier (such as in (9–1 1)), the yes/no response should correlate with the extent to which the pair of modifiers can be put into a scale based on contextual entailment. [sent-161, score-0.754]
</p><p>54 Table 3 summarizes the distribution of reviews as well as the number of words and vocabulary across the ten rating categories. [sent-164, score-0.214]
</p><p>55 However, since we do not want to incorporate the linguistically uninteresting fact that people tend to write a lot of ten-star reviews, we assume uniform priors for the rating categories. [sent-168, score-0.192]
</p><p>56 Let count(w, r) be the number of tokens of word w in reviews in rating category r, and let count(r) be the total word  count for all words in rating category r. [sent-169, score-0.56]
</p><p>57 The probability of w given a rating category r is simply Pr(w|r)  =  count(w, r)/ count(r). [sent-170, score-0.244]
</p><p>58 25  Table 3: Numbers of reviews, words and vocabulary size per rating  category  in the IMDB review  corpus,  as well as the average number of words per review. [sent-185, score-0.244]
</p><p>59 3 4 -  -  -  -  -  -  -  -  -  -  Rating (centered at 0)  Figure 2: The distribution of some scalar modifiers across the ten rating categories. [sent-226, score-0.505]
</p><p>60 e5 negative end of the scale (bad, awful, terrible) are not linguistically comparable to those at the positive end of the scale (good, excellent, superb). [sent-247, score-0.203]
</p><p>61 In our experiments, we use expected rating values to characterize the polarity and strength of modifiers. [sent-252, score-0.192]
</p><p>62 The weak scalar modifiers all the way on the left are most common near the middle of the scale, with a slight positive bias in the top row and a slight negative bias in the bottom row. [sent-256, score-0.566]
</p><p>63 As we move from left to right, the bias for one end of the scale grows more extreme, until the words in question are almost never used outside of the most extreme rating category. [sent-257, score-0.288]
</p><p>64 The resulting scales correspond well with linguistic intuitions and thus provide an initial indication that the rating categories are a reliable guide to strength and polarity for scalar modifiers. [sent-258, score-0.642]
</p><p>65 We put this information to use in our dialogue corpus via the decision procedure 171  frequencies across rating categories in the review corpus to decide what a given answer conveys. [sent-259, score-0.39]
</p><p>66 2 Interpreting numerical answers The second technique aims at determining whether a numerical answer counts as a positive  or negative instance of the adjective in the question (category IIin our corpus). [sent-262, score-1.1]
</p><p>67 , a six foot tall woman can clearly be called “a tall woman” whereas a five foot tall woman cannot), there are borderline cases for which it is difficult to say whether the adjectival predication can truthfully be ascribed to them. [sent-265, score-0.366]
</p><p>68 For instance, in the case of (3), we can retrieve from the Web positive and negative examples of age in relation to the adjective and the modified entity “little kids”. [sent-268, score-0.412]
</p><p>69 The question contains the adjective and the modified entity. [sent-269, score-0.296]
</p><p>70 The reply contains the unit of measure (here “year-old”) and the numerical answer. [sent-270, score-0.197]
</p><p>71 BOSS (Academic) for “little kids” yearold (positive instances) as well as for “not little  kids ” year-old (negative instances). [sent-272, score-0.395]
</p><p>72 We then extract ages from the positive and negative snippets obtained, and we fit a logistic regression to these data. [sent-276, score-0.226]
</p><p>73 We also replace the negation and the adjective by the antonyms given in WordNet (using the first sense). [sent-282, score-0.387]
</p><p>74 If the fac—  tor is a significant predictor, we can use the probabilities from the model to decide whether the answer qualifies as a positive or negative instance of the adjective in the question, and thus interpret the indirect response as a ‘yes’ or a ‘no’ . [sent-285, score-0.907]
</p><p>75 The probabilistic nature of this technique adheres perfectly to the fact that indirect answers are intimately tied up with uncertainty. [sent-286, score-0.341]
</p><p>76 5  Evaluation and results  Our primary goal is to evaluate how well we can learn the relevant scalar and entailment relationships from the Web. [sent-287, score-0.259]
</p><p>77 For the adjectival scales, we annotated each example for its main predication (modifier, or adverb–modifier bigram), including whether that predication was negated. [sent-289, score-0.25]
</p><p>78 For the numerical cases, we manually constructed the initial queries: we identified the adjective and the modified entity in the question, and the unit ofmeasure in the answer. [sent-290, score-0.365]
</p><p>79 ResponsePrecisionRecallF1 IYes877681 No 57 71 63 IIYes1003653 Uncertain 67 40 50  Table 5: Precision, recall, and F1 (%) per response category. [sent-295, score-0.2]
</p><p>80 In the case of the scalar modifiers experiment, there were just two examples whose dominant response from the Turkers was ‘Uncertain’, so we have left that category out of the results. [sent-296, score-0.734]
</p><p>81 We count an inference as successful if it matches the dominant Turker response category. [sent-300, score-0.269]
</p><p>82 Table 5 summarizes the results  per response category, for the examples in which both the question and answer contain a gradable modifier (category I), and for the numerical cases (category II). [sent-309, score-0.891]
</p><p>83 6  Analysis and discussion  Performance is extremely good on the “Adverb same adjective” and “Negation same adjective” cases because the ‘Yes’ answer is fairly direct for them (though adverbs like basically introduce an interesting level of uncertainty). [sent-310, score-0.247]
</p><p>84 5  (items I)  No  60  56  58  Table 6: Precision, recall, and F1 (%) per response category for the WordNet-based approach. [sent-313, score-0.302]
</p><p>85 Inferring the relation between scalar adjectives has some connection with work in sentiment de-  tection. [sent-315, score-0.343]
</p><p>86 Even though most of the research in that domain focuses on the orientation of one term using seed sets, techniques which provide the orientation strength could be used to infer a scalar relation between adjectives. [sent-316, score-0.363]
</p><p>87 Table 6 summarizes the results per response category for the WordNet-based approach (which can thus be com-  pared to the category I results in table 5). [sent-323, score-0.404]
</p><p>88 In the “Other adjective” category, 31 items involve oppositional terms: canonical antonyms (e. [sent-326, score-0.195]
</p><p>89 Our technique accurately deals with most 173 little kids  young kids  Age  Age  Figure 4: Probabilities of being appropriately described as “little”,  warm weather  “young”  or  “warm”, fitted  retrieved when querying the Web for “little kids”,  “young  kids” and  “warm  on  data  weather”. [sent-332, score-0.951]
</p><p>90 The lack of data comes from the fact that the queries are very specific, since the adjective depends on the product (e. [sent-345, score-0.228]
</p><p>91 For each dialogue, we plot a circle at Turker response entropy and either 1 = correct inference or 0 = incorrect inference, except the points are jittered a little vertically to show where the mass of data lies. [sent-361, score-0.333]
</p><p>92 The fitted logistic regression model (black line) has a statistically significant coefficient for response entropy (p < 0. [sent-365, score-0.406]
</p><p>93 This happens for 8 items: “expensive to call (50 cents a minute)”, “little kids (7 and 10 year-old)”, “long growing season (3 months)”, “lot of land (80 acres)”, “warm weather (80 degrees)”, “young kids (5 and 2 year-old)”, “young person (3 1 yearold)” and “large house (2450 square feet)”. [sent-368, score-0.631]
</p><p>94 In the latter case only, the system output (uncertain) doesn’t correlate with the Turkers’ judgment (where the dominant answer is ‘probable yes’ with  15 responses, and 11 answers are ‘uncertain’). [sent-369, score-0.431]
</p><p>95 The logistic curves in figure 4 capture nicely the intuitions that people have about the relation between age and “little kids” or “young kids”, as well as between Fahrenheit degrees and “warm weather”. [sent-370, score-0.207]
</p><p>96 In sum, when the data is available, this method delivers models which fit humans’ intuitions about the relation between numerical measure and adjective, and can handle pragmatic inference. [sent-373, score-0.213]
</p><p>97 Figure 5 plots the relationship between the response entropy and the accu-  racy of our decision procedure, along with a fitted logistic regression model using response entropy to predict whether our system’s inference was correct. [sent-375, score-0.67]
</p><p>98 We focus on gradable modifiers, seeking to learn scalar relationships between their meanings and to obtain an empirically grounded, probabilistic understanding of the clear and fuzzy cases that they often give rise to (Kamp and Partee, 1995). [sent-382, score-0.554]
</p><p>99 We show that it is possible to learn the requisite scales between modifiers using review corpora, and to use that knowledge to drive inference in indirect responses. [sent-383, score-0.365]
</p><p>100 When the relation in question is not too specific, we show that it is also possible to learn the strength of the relation between an adjective and a numerical measure. [sent-384, score-0.483]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kids', 0.288), ('yes', 0.287), ('scalar', 0.259), ('adjective', 0.228), ('answer', 0.209), ('response', 0.2), ('gradable', 0.185), ('turkers', 0.169), ('uncertain', 0.156), ('answers', 0.153), ('indirect', 0.145), ('rating', 0.142), ('numerical', 0.137), ('scales', 0.116), ('conveys', 0.109), ('negation', 0.104), ('modifiers', 0.104), ('category', 0.102), ('young', 0.099), ('predication', 0.096), ('modifier', 0.092), ('dialogues', 0.089), ('uncertainty', 0.085), ('oppositional', 0.077), ('pq', 0.077), ('meanings', 0.073), ('polar', 0.072), ('reviews', 0.072), ('dominant', 0.069), ('little', 0.069), ('negative', 0.069), ('speaker', 0.068), ('warm', 0.068), ('question', 0.068), ('kamp', 0.067), ('predications', 0.067), ('adverb', 0.066), ('entropy', 0.064), ('items', 0.063), ('logistic', 0.062), ('probable', 0.061), ('kennedy', 0.06), ('reply', 0.06), ('age', 0.059), ('interpreting', 0.059), ('mechanical', 0.058), ('adjectival', 0.058), ('hearer', 0.058), ('positive', 0.056), ('convey', 0.055), ('antonyms', 0.055), ('weather', 0.055), ('infer', 0.054), ('turk', 0.053), ('marneffe', 0.052), ('definite', 0.052), ('strength', 0.05), ('mohammad', 0.05), ('partee', 0.05), ('perrault', 0.05), ('people', 0.05), ('questions', 0.048), ('tall', 0.046), ('adjectives', 0.044), ('firm', 0.043), ('vagueness', 0.043), ('technique', 0.043), ('extent', 0.042), ('agreement', 0.042), ('pr', 0.042), ('christopher', 0.041), ('think', 0.041), ('intended', 0.041), ('fitted', 0.041), ('inferences', 0.04), ('pragmatic', 0.04), ('involving', 0.04), ('sentiment', 0.04), ('scale', 0.039), ('regression', 0.039), ('categories', 0.039), ('bias', 0.039), ('acres', 0.038), ('blairgoldensohn', 0.038), ('chevalier', 0.038), ('iinumerical', 0.038), ('iother', 0.038), ('mcnally', 0.038), ('munro', 0.038), ('potts', 0.038), ('superb', 0.038), ('yearold', 0.038), ('good', 0.038), ('allen', 0.037), ('woman', 0.037), ('clear', 0.037), ('web', 0.036), ('excellent', 0.036), ('intuitions', 0.036), ('speakers', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="2-tfidf-1" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>Author: Marie-Catherine de Marneffe ; Christopher D. Manning ; Christopher Potts</p><p>Abstract: Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.</p><p>2 0.1556911 <a title="2-tfidf-2" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>Author: Mattia Tomasoni ; Minlie Huang</p><p>Abstract: This paper presents a framework for automatically processing information coming from community Question Answering (cQA) portals with the purpose of generating a trustful, complete, relevant and succinct summary in response to a question. We exploit the metadata intrinsically present in User Generated Content (UGC) to bias automatic multi-document summarization techniques toward high quality information. We adopt a representation of concepts alternative to n-grams and propose two concept-scoring functions based on semantic overlap. Experimental re- sults on data drawn from Yahoo! Answers demonstrate the effectiveness of our method in terms of ROUGE scores. We show that the information contained in the best answers voted by users of cQA portals can be successfully complemented by our method.</p><p>3 0.12260086 <a title="2-tfidf-3" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>Author: Cigdem Toprak ; Niklas Jakob ; Iryna Gurevych</p><p>Abstract: In this paper, we introduce a corpus of consumer reviews from the rateitall and the eopinions websites annotated with opinion-related information. We present a two-level annotation scheme. In the first stage, the reviews are analyzed at the sentence level for (i) relevancy to a given topic, and (ii) expressing an evaluation about the topic. In the second stage, on-topic sentences containing evaluations about the topic are further investigated at the expression level for pinpointing the properties (semantic orientation, intensity), and the functional components of the evaluations (opinion terms, targets and holders). We discuss the annotation scheme, the inter-annotator agreement for different subtasks and our observations.</p><p>4 0.11884617 <a title="2-tfidf-4" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>Author: Israela Becker ; Vered Aharonson</p><p>Abstract: Two psycholinguistic and psychophysical experiments show that in order to efficiently extract polarity of written texts such as customerreviews on the Internet, one should concentrate computational efforts on messages in the final position of the text.</p><p>5 0.11048915 <a title="2-tfidf-5" href="./acl-2010-Modeling_Semantic_Relevance_for_Question-Answer_Pairs_in_Web_Social_Communities.html">174 acl-2010-Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities</a></p>
<p>Author: Baoxun Wang ; Xiaolong Wang ; Chengjie Sun ; Bingquan Liu ; Lin Sun</p><p>Abstract: Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. In this paper, a deep belief network is proposed to model the semantic relevance for question-answer pairs. Observing the textual similarity between the community-driven questionanswering (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora.</p><p>6 0.1011181 <a title="2-tfidf-6" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>7 0.096505895 <a title="2-tfidf-7" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>8 0.096379258 <a title="2-tfidf-8" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>9 0.090972096 <a title="2-tfidf-9" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>10 0.089502126 <a title="2-tfidf-10" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>11 0.089111648 <a title="2-tfidf-11" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>12 0.086407423 <a title="2-tfidf-12" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>13 0.080195799 <a title="2-tfidf-13" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>14 0.077394515 <a title="2-tfidf-14" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>15 0.072197728 <a title="2-tfidf-15" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>16 0.070152625 <a title="2-tfidf-16" href="./acl-2010-Optimizing_Question_Answering_Accuracy_by_Maximizing_Log-Likelihood.html">189 acl-2010-Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</a></p>
<p>17 0.067242637 <a title="2-tfidf-17" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>18 0.065142281 <a title="2-tfidf-18" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>19 0.062594257 <a title="2-tfidf-19" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>20 0.060745195 <a title="2-tfidf-20" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.181), (1, 0.11), (2, -0.113), (3, -0.035), (4, -0.037), (5, -0.072), (6, -0.032), (7, 0.065), (8, -0.024), (9, -0.01), (10, -0.009), (11, 0.045), (12, -0.072), (13, -0.039), (14, 0.024), (15, 0.109), (16, -0.019), (17, -0.085), (18, 0.045), (19, 0.008), (20, -0.024), (21, 0.034), (22, 0.143), (23, -0.087), (24, -0.023), (25, 0.049), (26, -0.006), (27, -0.001), (28, -0.121), (29, 0.023), (30, -0.116), (31, 0.029), (32, -0.039), (33, 0.024), (34, 0.074), (35, 0.056), (36, 0.11), (37, 0.141), (38, -0.037), (39, 0.013), (40, 0.037), (41, 0.078), (42, -0.128), (43, -0.006), (44, -0.059), (45, -0.021), (46, 0.057), (47, -0.06), (48, 0.164), (49, -0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96274984 <a title="2-lsi-1" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>Author: Marie-Catherine de Marneffe ; Christopher D. Manning ; Christopher Potts</p><p>Abstract: Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.</p><p>2 0.60565281 <a title="2-lsi-2" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>Author: Israela Becker ; Vered Aharonson</p><p>Abstract: Two psycholinguistic and psychophysical experiments show that in order to efficiently extract polarity of written texts such as customerreviews on the Internet, one should concentrate computational efforts on messages in the final position of the text.</p><p>3 0.59435755 <a title="2-lsi-3" href="./acl-2010-Modeling_Semantic_Relevance_for_Question-Answer_Pairs_in_Web_Social_Communities.html">174 acl-2010-Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities</a></p>
<p>Author: Baoxun Wang ; Xiaolong Wang ; Chengjie Sun ; Bingquan Liu ; Lin Sun</p><p>Abstract: Quantifying the semantic relevance between questions and their candidate answers is essential to answer detection in social media corpora. In this paper, a deep belief network is proposed to model the semantic relevance for question-answer pairs. Observing the textual similarity between the community-driven questionanswering (cQA) dataset and the forum dataset, we present a novel learning strategy to promote the performance of our method on the social community datasets without hand-annotating work. The experimental results show that our method outperforms the traditional approaches on both the cQA and the forum corpora.</p><p>4 0.56456691 <a title="2-lsi-4" href="./acl-2010-Optimizing_Question_Answering_Accuracy_by_Maximizing_Log-Likelihood.html">189 acl-2010-Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</a></p>
<p>Author: Matthias H. Heie ; Edward W. D. Whittaker ; Sadaoki Furui</p><p>Abstract: In this paper we demonstrate that there is a strong correlation between the Question Answering (QA) accuracy and the log-likelihood of the answer typing component of our statistical QA model. We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question-and-answer pairs. Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters.</p><p>5 0.55962801 <a title="2-lsi-5" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>6 0.55087769 <a title="2-lsi-6" href="./acl-2010-Comparable_Entity_Mining_from_Comparative_Questions.html">63 acl-2010-Comparable Entity Mining from Comparative Questions</a></p>
<p>7 0.53835285 <a title="2-lsi-7" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>8 0.52123559 <a title="2-lsi-8" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>9 0.51732737 <a title="2-lsi-9" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>10 0.51526368 <a title="2-lsi-10" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>11 0.4904182 <a title="2-lsi-11" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>12 0.48879236 <a title="2-lsi-12" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>13 0.46102118 <a title="2-lsi-13" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>14 0.41381985 <a title="2-lsi-14" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>15 0.38305143 <a title="2-lsi-15" href="./acl-2010-Automatically_Generating_Term_Frequency_Induced_Taxonomies.html">43 acl-2010-Automatically Generating Term Frequency Induced Taxonomies</a></p>
<p>16 0.38109082 <a title="2-lsi-16" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>17 0.38075191 <a title="2-lsi-17" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>18 0.3778919 <a title="2-lsi-18" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>19 0.37696061 <a title="2-lsi-19" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>20 0.369261 <a title="2-lsi-20" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.063), (39, 0.344), (42, 0.056), (59, 0.069), (71, 0.015), (72, 0.027), (73, 0.046), (76, 0.011), (78, 0.038), (80, 0.019), (83, 0.099), (84, 0.026), (98, 0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87577611 <a title="2-lda-1" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>Author: Matthew Skala ; Victoria Krakovna ; Janos Kramar ; Gerald Penn</p><p>Abstract: Constructing an encoding of a concept lattice using short bit vectors allows for efficient computation of join operations on the lattice. Join is the central operation any unification-based parser must support. We extend the traditional bit vector encoding, which represents join failure using the zero vector, to count any vector with less than a fixed number of one bits as failure. This allows non-joinable elements to share bits, resulting in a smaller vector size. A constraint solver is used to construct the encoding, and a variety of techniques are employed to find near-optimal solutions and handle timeouts. An evaluation is provided comparing the extended representation of failure with traditional bit vector techniques.</p><p>same-paper 2 0.87192702 <a title="2-lda-2" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>Author: Marie-Catherine de Marneffe ; Christopher D. Manning ; Christopher Potts</p><p>Abstract: Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.</p><p>3 0.79691166 <a title="2-lda-3" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>Author: Yufeng Chen ; Chengqing Zong ; Keh-Yih Su</p><p>Abstract: We observe that (1) how a given named entity (NE) is translated (i.e., either semantically or phonetically) depends greatly on its associated entity type, and (2) entities within an aligned pair should share the same type. Also, (3) those initially detected NEs are anchors, whose information should be used to give certainty scores when selecting candidates. From this basis, an integrated model is thus proposed in this paper to jointly identify and align bilingual named entities between Chinese and English. It adopts a new mapping type ratio feature (which is the proportion of NE internal tokens that are semantically translated), enforces an entity type consistency constraint, and utilizes additional monolingual candidate certainty factors (based on those NE anchors). The experi- ments show that this novel approach has substantially raised the type-sensitive F-score of identified NE-pairs from 68.4% to 81.7% (42.1% F-score imperfection reduction) in our Chinese-English NE alignment task.</p><p>4 0.78595692 <a title="2-lda-4" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>Author: Michael Bloodgood ; Chris Callison-Burch</p><p>Abstract: We explore how to improve machine translation systems by adding more translation data in situations where we already have substantial resources. The main challenge is how to buck the trend of diminishing returns that is commonly encountered. We present an active learning-style data solicitation algorithm to meet this challenge. We test it, gathering annotations via Amazon Mechanical Turk, and find that we get an order of magnitude increase in performance rates of improvement.</p><p>5 0.50801468 <a title="2-lda-5" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>Author: Stephen Wu ; Asaf Bachrach ; Carlos Cardenas ; William Schuler</p><p>Abstract: Hierarchical HMM (HHMM) parsers make promising cognitive models: while they use a bounded model of working memory and pursue incremental hypotheses in parallel, they still achieve parsing accuracies competitive with chart-based techniques. This paper aims to validate that a right-corner HHMM parser is also able to produce complexity metrics, which quantify a reader’s incremental difficulty in understanding a sentence. Besides defining standard metrics in the HHMM framework, a new metric, embedding difference, is also proposed, which tests the hypothesis that HHMM store elements represents syntactic working memory. Results show that HHMM surprisal outperforms all other evaluated metrics in predicting reading times, and that embedding difference makes a significant, independent contribution.</p><p>6 0.5069927 <a title="2-lda-6" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>7 0.50411147 <a title="2-lda-7" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>8 0.49607104 <a title="2-lda-8" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>9 0.48903841 <a title="2-lda-9" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>10 0.48657414 <a title="2-lda-10" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>11 0.48320648 <a title="2-lda-11" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>12 0.48222214 <a title="2-lda-12" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>13 0.48216879 <a title="2-lda-13" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>14 0.481924 <a title="2-lda-14" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>15 0.48181233 <a title="2-lda-15" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>16 0.4814449 <a title="2-lda-16" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>17 0.47923392 <a title="2-lda-17" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>18 0.47906613 <a title="2-lda-18" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>19 0.4779827 <a title="2-lda-19" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>20 0.47758895 <a title="2-lda-20" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
