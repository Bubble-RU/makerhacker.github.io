<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>238 acl-2010-Towards Open-Domain Semantic Role Labeling</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-238" href="#">acl2010-238</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>238 acl-2010-Towards Open-Domain Semantic Role Labeling</h1>
<br/><p>Source: <a title="acl-2010-238-pdf" href="http://aclweb.org/anthology//P/P10/P10-1025.pdf">pdf</a></p><p>Author: Danilo Croce ; Cristina Giannone ; Paolo Annesi ; Roberto Basili</p><p>Abstract: Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations.</p><p>Reference: <a title="acl-2010-238-reference" href="../acl2010_reference/acl-2010-Towards_Open-Domain_Semantic_Role_Labeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. [sent-6, score-0.244]
</p><p>2 The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. [sent-7, score-0.311]
</p><p>3 Building upon the so called frame semantic model (Fillmore, 1985), the Berkeley FrameNet project has developed a semantic lexicon for the core vocabulary of English, since 1997. [sent-11, score-0.364]
</p><p>4 A frame is evoked in texts through the occurrence of its lexical units (LU), i. [sent-12, score-0.202]
</p><p>5 predicate words such verbs, nouns, or adjectives, and specifies the participants and properties of the situation it describes, the so called frame elements (FEs). [sent-14, score-0.13]
</p><p>6 frame elements) as they are grammatically realized in input sentences. [sent-17, score-0.13]
</p><p>7 Some critical limitations have been outlined in literature, some of them independent from the underlying semantic paradigm. [sent-25, score-0.132]
</p><p>8 More recently, the stateof-art frame-based semantic role labeling system discussed in (Johansson and Nugues, 2008b) re-  ports a 19% drop in accuracy for the argument classification task when a different test domain is targeted (i. [sent-50, score-0.629]
</p><p>9 Out-of-domain tests seem to suggest the models trained on BNC do not generalize well to novel grammatical and lexical phenomena. [sent-53, score-0.166]
</p><p>10 , 2008), the major drawback is the poor generalization power affecting lexical features. [sent-55, score-0.144]
</p><p>11 The above problems are particularly critical for frame-based shallow semantic parsing where, as opposed to more syntactic-oriented semantic labeling schemes (as Propbank (Palmer et al. [sent-57, score-0.258]
</p><p>12 9% for the accuracy of the argument identification task is reported, it is due to the complexity in projecting frame element boundaries out from the depen-  dency graph: more than 16% of the roles in the annotated material lack of a clear grammatical status. [sent-60, score-0.531]
</p><p>13 A semi-supervised statistical model exploiting useful lexical information from unlabeled corpora is proposed. [sent-72, score-0.159]
</p><p>14 Moreover, it generalizes lexical information about the annotated examples by applying a ge-  ometrical model, in a Latent Semantic Analysis style, inspired by a distributional paradigm (Pado and Lapata, 2007). [sent-74, score-0.232]
</p><p>15 As we will see, the accuracy reachable through a restricted feature space is still quite close to the state-of-art, but interestingly the performance drops in out-of-domain tests are avoided. [sent-75, score-0.132]
</p><p>16 In the following, after discussing existing approaches to SRL (Section 2), a distributional approach is defined in Section 3. [sent-76, score-0.11]
</p><p>17 2 discusses the proposed HMM-based treatment of joint inferences in argument classification. [sent-78, score-0.175]
</p><p>18 SRL proceeds through two main steps: the localization of arguments in a sentence, called boundary detection (BD), and the as-  signment of the proper role to the detected constituents, that is the argument classification, (AC) step. [sent-86, score-0.348]
</p><p>19 , 2008) a SRL model over Propbank that effectively exploits the semantic argument frame as a joint structure, is presented. [sent-88, score-0.439]
</p><p>20 It incorporates strong dependencies within a comprehensive statistical joint model with a rich set of features over multiple argument phrases. [sent-89, score-0.209]
</p><p>21 First local models are applied to produce role labels over individual arguments, then the joint model is used to decide the entire argument sequence among the set of the n-best competing solutions. [sent-95, score-0.39]
</p><p>22 In (Johansson and Nugues, 2008b) the impact of different grammatical representations on the task  of frame-based shallow semantic parsing is studied and the poor lexical generalization problem is outlined. [sent-97, score-0.344]
</p><p>23 The argument classification 238  component is thus shown to be heavily domaindependent whereas the inclusion of grammatical function features is just able to mitigate this sensitivity. [sent-105, score-0.298]
</p><p>24 , 2008), it is suggested that lexical features are domain specific and their suitable generalization is not achieved. [sent-107, score-0.144]
</p><p>25 The lack of suitable lexical information is also discussed in (F ¨urstenau and Lapata, 2009) through an approach aiming to support the creation of novel annotated resources. [sent-108, score-0.161]
</p><p>26 1 The role of Lexical Semantic Information It is widely accepted that lexical information (as features directly derived from word forms) is crucial for training accurate systems in a number of NLP tasks. [sent-112, score-0.171]
</p><p>27 In (Goldberg and Elhadad, 2009), a different strategy to incorporate lexical features into classification models is proposed. [sent-117, score-0.134]
</p><p>28 The suggested perspective here is that effective semantic knowledge can be collected from sources exter-  nal to the annotated corpora (very large unannotated corpora or on manually constructed lexical resources) rather than learned from the raw lexical counts of the annotated corpus. [sent-121, score-0.344]
</p><p>29 In particular, (Collobert and Weston, 2008) proposes an embedding of lexical information using Wikipedia as source, and exploiting the resulting language model within the multitask learning process. [sent-125, score-0.149]
</p><p>30 The idea of (Collobert and Weston, 2008) to obtain an embedding of lexical information by acquiring a language model from unlabeled data is an interesting approach to the problem of performance degradation in out-of-domain tests, as already pursued by (Deschacht and Moens, 2009). [sent-126, score-0.159]
</p><p>31 The extensive use of unlabeled texts allows to achieve a significant level of lexical generalization that seems better capitalize the smaller annotated data sets. [sent-127, score-0.328]
</p><p>32 3  A Distributional Model for Argument Classification  High quality lexical information is crucial for robust open-domain SRL, as semantic generalization highly depends on lexical information. [sent-128, score-0.316]
</p><p>33 SPEAKER  (1)  MEDIUM  (2)  In sentence (1), for example, President Kennedy is the grammatical subject of the verb say and this justifies its role of SPEAKER. [sent-132, score-0.16]
</p><p>34 However, syntax does not entirely characterize argument seman-  tics. [sent-133, score-0.175]
</p><p>35 Second, we improve the lexical semantic information available to the learning algorithm. [sent-140, score-0.172]
</p><p>36 The proposed ”minimalistic” approach will consider only two independent features: •  the semantic head (h) of a role, as it can bthee eo sbesmeravnetdic cin h tehaed grammatical set,ru acstu irte. [sent-141, score-0.198]
</p><p>37 t nIng (2), the semantic head report is connected to the  LU stated through the subject (SBJ) relation. [sent-144, score-0.137]
</p><p>38 In the rest ofthe section the distributional model for the argument classification step is presented. [sent-145, score-0.381]
</p><p>39 A lexicalized model for individual semantic roles is first defined in order to achieve robust semantic classification local to each argument. [sent-146, score-0.433]
</p><p>40 Then a Hidden Markov Model is introduced in order to exploit the local probability estimators, sensitive to lexical similarity, as well as the global information on the entire argument sequence. [sent-147, score-0.329]
</p><p>41 1 Distributional Local Models As the classification of semantic roles is strictly related to the lexical meaning of argument heads, we adopt a distributional perspective, where the meaning is described by the set of textual contexts in which words appear. [sent-149, score-0.574]
</p><p>42 In distributional models, words are thus represented through vectors built over these observable contexts: similar vectors suggest semantic relatedness as a function of the distance between two words, capturing paradigmatic (e. [sent-150, score-0.314]
</p><p>43 In the argument classification task, the similarity between two argument heads h1 and h2 observed in FrameNet can be computed over h→1 and The model for a given frame element FEk is built around the semantic heads h observed in the role FEk: →  →h2. [sent-160, score-1.027]
</p><p>44 These LSA vectors →h express the individual annotated examples as they are immerse in the LSA  TSMRPOoEleDAI,CKUFEMRkc C78543621lu: s{ fphacbteroi ,ntsgvchielrdtoa}fng,uprsethw,ifmoaupernt,hypeodtr,suiclmp}otrahsen ,pmaievrhdu,snleti}. [sent-162, score-0.118]
</p><p>45 sr  Table 1: Clusters of semantic heads in the Sub j position for the frame STATEMENT with σ = 0. [sent-163, score-0.356]
</p><p>46 Moreover, given FEk, a model for each individual syntactic relation r (i. [sent-165, score-0.11]
</p><p>47 As the LSA vectors  →h are available  for the se-  −F − →Ek  mantic heads h, a vector representation for the role FEk can be obtained from the annotated data. [sent-173, score-0.308]
</p><p>48 However, one single vector is a too simplistic representation given the rich nature of semantic roles FEk. [sent-174, score-0.155]
</p><p>49 QT is a generalization of k-mean where a variable number of clusters can be obtained. [sent-178, score-0.158]
</p><p>50 Given a syntactic relation r, denotes the clusters derived by QT  HrFEk  CrFEk  HrFEk. [sent-182, score-0.127]
</p><p>51 ect Eora →hc , c computed as the geometric centroid of its semantic heads h ∈ c. [sent-184, score-0.226]
</p><p>52 l  of every frame elements FEk: it consists of centroids with c ⊆ Each c represents FEk through a sietth ocf ⊆ ⊆sim Hilar heads, as role fillers observed in FrameNet. [sent-186, score-0.229]
</p><p>53 Table 1 represents clusters for the heads of the STATEMENT frame. [sent-187, score-0.212]
</p><p>54 In argument classification we assume that the evoking predicate word for the frame F in an input sentence s is known. [sent-188, score-0.367]
</p><p>55 , (rn, hn)} where the heads hi are i n{ (trhe syntactic relation)} ri whietrhe eth teh underlying lexical unit of F. [sent-193, score-0.383]
</p><p>56 Given the syntactic relation r, the clusters c ∈ whose centroid vector c~ ris, tchloesce rlu tsote r~hs are s Celected. [sent-195, score-0.127]
</p><p>57 ckj ∈ and h is the usual  CrFEk, cosine similarity: simcos(h,ckj) =k−→h−→h k · k−→c→−ckkjjk  Then, through a k-nearest neighbours (k-NN) strategy within Dr,h, the m clusters ckj most simi-  set D(rm,h). [sent-198, score-0.206]
</p><p>58 1 the preference estimation for the incoming head h = professor connected to a LU by the Sub j relation is shown. [sent-200, score-0.123]
</p><p>59 Clusters for the heads in Table 1 are also reported. [sent-201, score-0.126]
</p><p>60 First, in the set of clusters whose similarity with professor is higher than a threshold τ the m = 5 most similar clusters are selected. [sent-202, score-0.217]
</p><p>61 5 to the source (r, h) arguments, by rejecting cases only when no information about the head h is available from the unlabeled corpus or no example of relation r for the role FEk is available from the annotated corpus. [sent-210, score-0.28]
</p><p>62 If the head h has never been met in the unlabeled corpus or the high grammatical ambiguity of the sentence does not allow to locate it reliably, Eq. [sent-214, score-0.151]
</p><p>63 A more robust argument preference function for all arguments (ri, hi) ∈ s of the frame F is thus given by:  prob(FEk|ri,  hi) = λ1prob(FEk|ri,  λ2prob(FEk|ri)  hi) +  + λ3prob(FEk)  (6)  where weights λ1, λ2, λ3 can be heuristically assigned or estimated from the training set2. [sent-220, score-0.379]
</p><p>64 6 defines roles preferences local to individual arguments (ri, hi). [sent-226, score-0.246]
</p><p>65 However, an argument frame  is a joint structure, with strong dependencies between arguments. [sent-227, score-0.305]
</p><p>66 We thus propose to model the reranking phase (RR) as a HMM sequence labeling task. [sent-228, score-0.156]
</p><p>67 6 is available, the best frame element sequence for the entire sentence s can be selected by defining the function θ(·) that maps arguments (ri, hi) ∈ s eto f nfractmioen e θle(·m)e tnhatst FEk:  FE(k1,. [sent-237, score-0.204]
</p><p>68 241  Figure 1: A k-NN approach to the role classification for hi  = professor  Notice that different transfer functions θ(·) are usually possible. [sent-250, score-0.308]
</p><p>69 On the other hand, transition probabilities model role sequences and support the expectations about argument frames of a sentence. [sent-275, score-0.349]
</p><p>70 Again the emission probability can be rewritten as:  P(ri,hi|FEθ(i)) =P(FEθ(Pi)(|rFi,Ehθi()i) P)(ri,hi) (10) Since P(ri, hi) does not depend on the role labeling, maximizing Eq. [sent-282, score-0.148]
</p><p>71 4  Empirical Analysis  The aim of the evaluation is to measure the reachable accuracy of the simple model proposed and to compare its impact over in-domain and out-ofdomain semantic role labeling tasks. [sent-296, score-0.429]
</p><p>72 In particular, we will evaluate the argument classification (AC) task in Section 4. [sent-297, score-0.237]
</p><p>73 Vectors representing semantic heads have  h~  been computed according to the ”dependencybased” vector space discussed in (Pado and Lapata, 2007). [sent-315, score-0.265]
</p><p>74 1, allows to generalize lexical information: similar heads within the latent semantic space are built from the annotated examples and they allow to predict the behavior of new unseen words as found in the test sentences. [sent-327, score-0.348]
</p><p>75 704  Table 3: Accuracy on Arg classification tasks wrt different clustering policies  1then many singleton clusters are promoted (i. [sent-343, score-0.203]
</p><p>76 We measured the performance on the argument classification tasks of different models obtained by combing different choices of σ with Eq. [sent-347, score-0.237]
</p><p>77 2 Argument Classification Accuracy In these experiments we evaluate the quality of the argument classification step against the lexical knowledge acquired from unlabeled texts and the reranking step. [sent-366, score-0.426]
</p><p>78 The accuracy reachable on the gold standard argument boundaries has been compared across several experimental settings. [sent-367, score-0.274]
</p><p>79 Table 4 reports the accuracy results obtained over the three corpora (defined as in Table 2): the accuracy scores are averaged over different values of m in Eq. [sent-404, score-0.12]
</p><p>80 Notice how the positive impact of the backoff models and the HMM reranking policy is similarly reflected by all the collections. [sent-414, score-0.16]
</p><p>81 This seems to confirm the hypothesis that the model is able to properly generalize the required lexical information across different domains. [sent-423, score-0.152]
</p><p>82 It is interesting to outline that the individual stages of the proposed model play different roles in the different domains, as Table 4 suggests. [sent-424, score-0.124]
</p><p>83 Although the positive contributions of the individual processing stages are uniformly confirmed, some differences can be outlined: •  The beneficial impact of the lexical informatTiohne (i. [sent-425, score-0.146]
</p><p>84 The ANC domain seems not to significantly benefit when the distributional model (Eq. [sent-428, score-0.19]
</p><p>85 5 depends both from the evidence gathered in the corpus about lexical heads h as well as about the relation r. [sent-431, score-0.239]
</p><p>86 The different syntactic style of ANC seems thus the main responsible of the poor impact of distributional information, as it is often unapplicable to ANC test cases. [sent-440, score-0.195]
</p><p>87 244  lections seems characterized by a lower level of complexity (see for example the accuracy of the Local prior model, that is about 51% as for the ANC). [sent-443, score-0.139]
</p><p>88 The BNC-FN test collection seems the most complex one, and the impact of the lexical information brought by the  distributional model is here maximal. [sent-445, score-0.301]
</p><p>89 This is mainly due to the coherence between the distributions of lexical and grammatical phenomena in the test and training data. [sent-446, score-0.133]
</p><p>90 •  The role of HMM reranking is an effective way rtoo compensate errors ning t ihse nloc eaflf argument classifications for all the three domains. [sent-447, score-0.37]
</p><p>91 It seems that the HMM model well captures some information on the global semantic structure of a sentence: this is helpful in cases where errors in the grammatical recognition (of individual arguments or at sentence level) are more frequent and afflict the local distributional model. [sent-454, score-0.542]
</p><p>92 in the NTI and ANC data sets), the higher seems the impact of the reranking phase. [sent-457, score-0.149]
</p><p>93 +  5  +  Conclusions  In this paper, a distributional approach for acquiring a semi-supervised model of argument classification (AC) preferences has been proposed. [sent-491, score-0.416]
</p><p>94 It aims at improving the generalization capability of the inductive SRL approach by reducing the complexity of the employed grammatical features and through a distributional representation of lexical features. [sent-492, score-0.315]
</p><p>95 The model seems to capitalize from simple methods of lexical modeling (i. [sent-495, score-0.187]
</p><p>96 the estimation of lexico-grammatical preferences through distributional analysis over unlabeled data), estimation (through syntactic or lexical back-off where necessary) and reranking. [sent-497, score-0.27]
</p><p>97 Semi-supervised semantic role labeling using the latent words language model. [sent-539, score-0.257]
</p><p>98 On the role of lexical features in sequence labeling. [sent-566, score-0.171]
</p><p>99 The effect of syntactic representation on semantic role labeling. [sent-583, score-0.199]
</p><p>100 The proposition bank: A corpus annotated with semantic roles. [sent-612, score-0.15]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fek', 0.463), ('nti', 0.241), ('prob', 0.226), ('srl', 0.223), ('nugues', 0.197), ('johansson', 0.189), ('anc', 0.181), ('argument', 0.175), ('bd', 0.151), ('framenet', 0.145), ('frame', 0.13), ('heads', 0.126), ('distributional', 0.11), ('sbj', 0.106), ('hi', 0.102), ('semantic', 0.1), ('role', 0.099), ('coppola', 0.097), ('clusters', 0.086), ('bnc', 0.083), ('ri', 0.083), ('fe', 0.082), ('crfek', 0.08), ('rr', 0.076), ('arguments', 0.074), ('lexical', 0.072), ('generalization', 0.072), ('pado', 0.068), ('lsa', 0.065), ('collobert', 0.065), ('reranking', 0.064), ('classification', 0.062), ('grammatical', 0.061), ('abdc', 0.06), ('ckj', 0.06), ('heyer', 0.06), ('accuracy', 0.06), ('toutanova', 0.059), ('labeling', 0.058), ('backoff', 0.057), ('roles', 0.055), ('clustering', 0.055), ('fes', 0.053), ('grain', 0.053), ('unlabeled', 0.053), ('notice', 0.052), ('medium', 0.051), ('annotated', 0.05), ('ac', 0.049), ('emission', 0.049), ('urstenau', 0.048), ('propbank', 0.047), ('local', 0.047), ('seems', 0.046), ('weston', 0.045), ('professor', 0.045), ('hmm', 0.045), ('speaker', 0.045), ('multitask', 0.043), ('relation', 0.041), ('frames', 0.041), ('qt', 0.041), ('moschitti', 0.041), ('hfek', 0.04), ('hrfek', 0.04), ('promotes', 0.04), ('threat', 0.04), ('reachable', 0.039), ('discussed', 0.039), ('impact', 0.039), ('baker', 0.039), ('kennedy', 0.038), ('observable', 0.038), ('head', 0.037), ('katrin', 0.037), ('drop', 0.036), ('erk', 0.035), ('pradhan', 0.035), ('lu', 0.035), ('individual', 0.035), ('croce', 0.035), ('cascade', 0.035), ('dependencybased', 0.035), ('capitalize', 0.035), ('global', 0.035), ('preferences', 0.035), ('architecture', 0.035), ('goldberg', 0.034), ('model', 0.034), ('prior', 0.033), ('sub', 0.033), ('vectors', 0.033), ('tests', 0.033), ('tjong', 0.032), ('syntagmatic', 0.032), ('ning', 0.032), ('spectral', 0.032), ('backed', 0.032), ('nuclear', 0.032), ('outlined', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="238-tfidf-1" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>Author: Danilo Croce ; Cristina Giannone ; Paolo Annesi ; Roberto Basili</p><p>Abstract: Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations.</p><p>2 0.25154284 <a title="238-tfidf-2" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>3 0.23080245 <a title="238-tfidf-3" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>Author: Junhui Li ; Guodong Zhou ; Hwee Tou Ng</p><p>Abstract: This paper explores joint syntactic and semantic parsing of Chinese to further improve the performance of both syntactic and semantic parsing, in particular the performance of semantic parsing (in this paper, semantic role labeling). This is done from two levels. Firstly, an integrated parsing approach is proposed to integrate semantic parsing into the syntactic parsing process. Secondly, semantic information generated by semantic parsing is incorporated into the syntactic parsing model to better capture semantic information in syntactic parsing. Evaluation on Chinese TreeBank, Chinese PropBank, and Chinese NomBank shows that our integrated parsing approach outperforms the pipeline parsing approach on n-best parse trees, a natural extension of the widely used pipeline parsing approach on the top-best parse tree. Moreover, it shows that incorporating semantic role-related information into the syntactic parsing model significantly improves the performance of both syntactic parsing and semantic parsing. To our best knowledge, this is the first research on exploring syntactic parsing and semantic role labeling for both verbal and nominal predicates in an integrated way. 1</p><p>4 0.19948345 <a title="238-tfidf-4" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>Author: Rasoul Samad Zadeh Kaljahi</p><p>Abstract: Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1</p><p>5 0.19604816 <a title="238-tfidf-5" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>Author: Michael Connor ; Yael Gertner ; Cynthia Fisher ; Dan Roth</p><p>Abstract: A fundamental step in sentence comprehension involves assigning semantic roles to sentence constituents. To accomplish this, the listener must parse the sentence, find constituents that are candidate arguments, and assign semantic roles to those constituents. Each step depends on prior lexical and syntactic knowledge. Where do children learning their first languages begin in solving this problem? In this paper we focus on the parsing and argumentidentification steps that precede Semantic Role Labeling (SRL) training. We combine a simplified SRL with an unsupervised HMM part of speech tagger, and experiment with psycholinguisticallymotivated ways to label clusters resulting from the HMM so that they can be used to parse input for the SRL system. The results show that proposed shallow representations of sentence structure are robust to reductions in parsing accuracy, and that the contribution of alternative representations of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argumentidentification stages.</p><p>6 0.18779634 <a title="238-tfidf-6" href="./acl-2010-Semantics-Driven_Shallow_Parsing_for_Chinese_Semantic_Role_Labeling.html">207 acl-2010-Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling</a></p>
<p>7 0.16864553 <a title="238-tfidf-7" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>8 0.16512559 <a title="238-tfidf-8" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>9 0.16066292 <a title="238-tfidf-9" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>10 0.15889664 <a title="238-tfidf-10" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>11 0.14960374 <a title="238-tfidf-11" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>12 0.13095137 <a title="238-tfidf-12" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>13 0.12248739 <a title="238-tfidf-13" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>14 0.11311346 <a title="238-tfidf-14" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>15 0.10795328 <a title="238-tfidf-15" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>16 0.09984868 <a title="238-tfidf-16" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>17 0.095219575 <a title="238-tfidf-17" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>18 0.093660533 <a title="238-tfidf-18" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>19 0.089034259 <a title="238-tfidf-19" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<p>20 0.088934928 <a title="238-tfidf-20" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.266), (1, 0.15), (2, 0.256), (3, 0.107), (4, 0.07), (5, 0.01), (6, -0.153), (7, -0.015), (8, 0.007), (9, -0.008), (10, 0.008), (11, 0.015), (12, 0.049), (13, -0.041), (14, -0.071), (15, -0.045), (16, -0.026), (17, -0.006), (18, -0.038), (19, 0.008), (20, 0.069), (21, 0.04), (22, -0.011), (23, 0.009), (24, -0.011), (25, -0.018), (26, -0.038), (27, 0.025), (28, 0.043), (29, -0.1), (30, 0.006), (31, -0.056), (32, 0.053), (33, 0.02), (34, 0.066), (35, -0.01), (36, 0.011), (37, -0.015), (38, -0.007), (39, 0.011), (40, -0.014), (41, 0.03), (42, -0.061), (43, -0.004), (44, -0.003), (45, 0.042), (46, -0.049), (47, -0.021), (48, -0.099), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93348545 <a title="238-lsi-1" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>Author: Danilo Croce ; Cristina Giannone ; Paolo Annesi ; Roberto Basili</p><p>Abstract: Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations.</p><p>2 0.85467046 <a title="238-lsi-2" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>3 0.84280419 <a title="238-lsi-3" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>Author: Michael Connor ; Yael Gertner ; Cynthia Fisher ; Dan Roth</p><p>Abstract: A fundamental step in sentence comprehension involves assigning semantic roles to sentence constituents. To accomplish this, the listener must parse the sentence, find constituents that are candidate arguments, and assign semantic roles to those constituents. Each step depends on prior lexical and syntactic knowledge. Where do children learning their first languages begin in solving this problem? In this paper we focus on the parsing and argumentidentification steps that precede Semantic Role Labeling (SRL) training. We combine a simplified SRL with an unsupervised HMM part of speech tagger, and experiment with psycholinguisticallymotivated ways to label clusters resulting from the HMM so that they can be used to parse input for the SRL system. The results show that proposed shallow representations of sentence structure are robust to reductions in parsing accuracy, and that the contribution of alternative representations of sentence structure to successful semantic role labeling varies with the integrity of the parsing and argumentidentification stages.</p><p>4 0.74051422 <a title="238-lsi-4" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>Author: Junhui Li ; Guodong Zhou ; Hwee Tou Ng</p><p>Abstract: This paper explores joint syntactic and semantic parsing of Chinese to further improve the performance of both syntactic and semantic parsing, in particular the performance of semantic parsing (in this paper, semantic role labeling). This is done from two levels. Firstly, an integrated parsing approach is proposed to integrate semantic parsing into the syntactic parsing process. Secondly, semantic information generated by semantic parsing is incorporated into the syntactic parsing model to better capture semantic information in syntactic parsing. Evaluation on Chinese TreeBank, Chinese PropBank, and Chinese NomBank shows that our integrated parsing approach outperforms the pipeline parsing approach on n-best parse trees, a natural extension of the widely used pipeline parsing approach on the top-best parse tree. Moreover, it shows that incorporating semantic role-related information into the syntactic parsing model significantly improves the performance of both syntactic parsing and semantic parsing. To our best knowledge, this is the first research on exploring syntactic parsing and semantic role labeling for both verbal and nominal predicates in an integrated way. 1</p><p>5 0.73049545 <a title="238-lsi-5" href="./acl-2010-Semantics-Driven_Shallow_Parsing_for_Chinese_Semantic_Role_Labeling.html">207 acl-2010-Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling</a></p>
<p>Author: Weiwei Sun</p><p>Abstract: One deficiency of current shallow parsing based Semantic Role Labeling (SRL) methods is that syntactic chunks are too small to effectively group words. To partially resolve this problem, we propose semantics-driven shallow parsing, which takes into account both syntactic structures and predicate-argument structures. We also introduce several new “path” features to improve shallow parsing based SRL method. Experiments indicate that our new method obtains a significant improvement over the best reported Chinese SRL result.</p><p>6 0.72511852 <a title="238-lsi-6" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>7 0.72477907 <a title="238-lsi-7" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>8 0.70086265 <a title="238-lsi-8" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>9 0.67319828 <a title="238-lsi-9" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>10 0.6415509 <a title="238-lsi-10" href="./acl-2010-Automatic_Selectional_Preference_Acquisition_for_Latin_Verbs.html">41 acl-2010-Automatic Selectional Preference Acquisition for Latin Verbs</a></p>
<p>11 0.63862127 <a title="238-lsi-11" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>12 0.62290972 <a title="238-lsi-12" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>13 0.57793844 <a title="238-lsi-13" href="./acl-2010-Expanding_Verb_Coverage_in_Cyc_with_VerbNet.html">108 acl-2010-Expanding Verb Coverage in Cyc with VerbNet</a></p>
<p>14 0.56916106 <a title="238-lsi-14" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>15 0.56806058 <a title="238-lsi-15" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>16 0.54178822 <a title="238-lsi-16" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>17 0.53123528 <a title="238-lsi-17" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>18 0.52279371 <a title="238-lsi-18" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>19 0.51833987 <a title="238-lsi-19" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>20 0.51467186 <a title="238-lsi-20" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.019), (25, 0.056), (42, 0.015), (59, 0.107), (73, 0.388), (78, 0.07), (80, 0.018), (83, 0.089), (84, 0.026), (98, 0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95426035 <a title="238-lda-1" href="./acl-2010-WebLicht%3A_Web-Based_LRT_Services_for_German.html">259 acl-2010-WebLicht: Web-Based LRT Services for German</a></p>
<p>Author: Erhard Hinrichs ; Marie Hinrichs ; Thomas Zastrow</p><p>Abstract: This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a webbased service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project1. WebLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services. WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.) Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service. 1</p><p>2 0.93339115 <a title="238-lda-2" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<p>Author: Sindhu Raghavan ; Adriana Kovashka ; Raymond Mooney</p><p>Abstract: In this paper, we present a novel approach for authorship attribution, the task of identifying the author of a document, using probabilistic context-free grammars. Our approach involves building a probabilistic context-free grammar for each author and using this grammar as a language model for classification. We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy.</p><p>3 0.91283268 <a title="238-lda-3" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>Author: Nikolaos Trogkanis ; Charles Elkan</p><p>Abstract: Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random fields. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages.</p><p>4 0.90829086 <a title="238-lda-4" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>Author: Jesus Gonzalez Rubio ; Daniel Ortiz Martinez ; Francisco Casacuberta</p><p>Abstract: This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.</p><p>5 0.89241827 <a title="238-lda-5" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>Author: Ahmed Hassan ; Dragomir Radev</p><p>Abstract: Automatically identifying the polarity of words is a very important task in Natural Language Processing. It has applications in text classification, text filtering, analysis of product review, analysis of responses to surveys, and mining online discussions. We propose a method for identifying the polarity of words. We apply a Markov random walk model to a large word relatedness graph, producing a polarity estimate for any given word. A key advantage of the model is its ability to accurately and quickly assign a polarity sign and magnitude to any word. The method could be used both in a semi-supervised setting where a training set of labeled words is used, and in an unsupervised setting where a handful of seeds is used to define the two polarity classes. The method is experimentally tested using a manually labeled set of positive and negative words. It outperforms the state of the art methods in the semi-supervised setting. The results in the unsupervised setting is comparable to the best reported values. However, the proposed method is faster and does not need a large corpus.</p><p>same-paper 6 0.89159834 <a title="238-lda-6" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>7 0.85784674 <a title="238-lda-7" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>8 0.68943512 <a title="238-lda-8" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>9 0.67936611 <a title="238-lda-9" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>10 0.64723319 <a title="238-lda-10" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>11 0.64460367 <a title="238-lda-11" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>12 0.6392926 <a title="238-lda-12" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>13 0.63797104 <a title="238-lda-13" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>14 0.63365072 <a title="238-lda-14" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>15 0.63145816 <a title="238-lda-15" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>16 0.6295923 <a title="238-lda-16" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>17 0.62587142 <a title="238-lda-17" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>18 0.62406808 <a title="238-lda-18" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>19 0.61706227 <a title="238-lda-19" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>20 0.61384904 <a title="238-lda-20" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
