<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-129" href="#">acl2010-129</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</h1>
<br/><p>Source: <a title="acl-2010-129-pdf" href="http://aclweb.org/anthology//P/P10/P10-3009.pdf">pdf</a></p><p>Author: Yabin Zheng ; Zhiyuan Liu ; Lixing Xie</p><p>Abstract: Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance. 1</p><p>Reference: <a title="acl-2010-129-reference" href="../acl2010_reference/acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. [sent-4, score-1.633]
</p><p>2 Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. [sent-5, score-0.519]
</p><p>3 First, we utilize user behaviors to generate candidate words. [sent-6, score-0.898]
</p><p>4 Then, we utilize search engine to enrich candidate words with adequate semantic features. [sent-7, score-0.725]
</p><p>5 Finally, we reorder candidate words according to their semantic relatedness to the seed word. [sent-8, score-1.079]
</p><p>6 Problems arise when we want to find more words related to the input query/seed word. [sent-12, score-0.291]
</p><p>7 For example, if seed word “ 自 然 语 言 处 理 ” (Natural Language Processing) is entered into Google Sets (Google, 2010), Google Sets returns an ordered list of related words such as “人工智能” (Artificial Intelligence) and “计算机” (Computer). [sent-13, score-0.533]
</p><p>8 In this paper, we want to investigate the advantage of user behaviors and re-ranking framework in related words retrieval task using Chinese input method user records. [sent-15, score-1.312]
</p><p>9 We construct a User-Word bipartite graph to represent the information hiding in user records. [sent-16, score-0.64]
</p><p>10 The bipartite graph keeps users on one side and words on the other side. [sent-17, score-0.526]
</p><p>11 The underlying idea is that the more frequently two words co-occur in user records, the more related they are. [sent-18, score-0.434]
</p><p>12 As a result, user behaviors offer a new perspective for measuring relatedness between words. [sent-20, score-0.745]
</p><p>13 On the other hand, we can also recommend related words to users in order to enhance user experiences. [sent-21, score-0.572]
</p><p>14 Sahami and Helman (2006) utilize search engine to supply web queries with more semantic context and gains better results for query suggestion task. [sent-25, score-0.404]
</p><p>15 User behaviors provide statistic information to generate candidate words. [sent-27, score-0.583]
</p><p>16 Then, we can enrich candidate words with additional semantic features using search engine to retrieve more relevant candidates earlier. [sent-28, score-0.747]
</p><p>17 First, we introduce user behaviors in related  word retrieval task and construct a User-Word bipartite graph from user behaviors. [sent-32, score-1.506]
</p><p>18 Words are used by users, and it is reasonable to measure relatedness between words by analyzing user behaviors. [sent-33, score-0.485]
</p><p>19 Second, we take the advantage of semantic features using search engine to reorder candidate words. [sent-34, score-0.713]
</p><p>20 Then we introduce our method for related words retrieval in Section 3. [sent-39, score-0.416]
</p><p>21 0c S20tu1d0e Ants Roecsiea tirconh f Woror Cksomhop u,t pa tgioensa 4l9 L–in5g4u,istics 2  Related Work  For related words retrieval task, Google Sets (Google, 2010) provides a remarkably interesting  tool for finding words related to an input word. [sent-44, score-0.579]
</p><p>22 Bayesian Sets (Ghahramani and Heller, 2006) offers an alternative method for related words retrieval under the framework of Bayesian inference. [sent-47, score-0.374]
</p><p>23 It computes a score for each candidate word by comparing the posterior probability of that word given the input, to the prior probability of that candidate word. [sent-48, score-0.742]
</p><p>24 Then, it returns a ranked list of candidate words according to their computed scores. [sent-49, score-0.362]
</p><p>25 (2009) introduce user behaviors in new word detection task via a collaborative filtering manner. [sent-51, score-0.657]
</p><p>26 They extend their method to related word retrieval task. [sent-52, score-0.368]
</p><p>27 Moreover, they prove that user behaviors provide a new point for new word detection and related word retrieval tasks. [sent-53, score-0.947]
</p><p>28 We can regard related word retrieval task as problem of measuring the semantic relatedness between pairs of very short texts. [sent-55, score-0.648]
</p><p>29 Sahami and Helman (2006) introduce a web kernel function  for measuring semantic similarities using snippets of search results. [sent-56, score-0.293]
</p><p>30 In this paper, we follow the similar idea of using search engine to enrich semantic features of a query word. [sent-60, score-0.385]
</p><p>31 We regard the returned snippets as the context of a query word. [sent-61, score-0.282]
</p><p>32 And then we reorder candidate words and expect more relevant candidate words can be retrieved earlier. [sent-62, score-0.912]
</p><p>33 3  Related Words Retrieval  In this section, we will introduce how to find related words from a single seed word via user behaviors and re-ranking framework. [sent-64, score-1.128]
</p><p>34 Users can install Sogou on their computers and  the word lists they have used are kept in their user records. [sent-68, score-0.355]
</p><p>35 Volunteers are encouraged to upload their anonymous user records to the server side. [sent-69, score-0.415]
</p><p>36 In order to preserve user privacy, usernames are hidden using MD5 hash algorithm. [sent-70, score-0.264]
</p><p>37 Then we demonstrate how to build a UserWord bipartite graph based on the dataset. [sent-71, score-0.315]
</p><p>38 Intuitively, two words are supposed to be related if there are a lot of users who have used both of them. [sent-76, score-0.278]
</p><p>39 In other words, the two words always co-occur in user records. [sent-77, score-0.332]
</p><p>40 Starting from a single seed word, we can generate a set of candidate words. [sent-78, score-0.595]
</p><p>41 Third, in order to take the advantage of semantic features, we carry out feature extraction techniques to represent generated candidate words with enriched semantic context. [sent-80, score-0.598]
</p><p>42 After this step, input  seed word and candidate words are represented as feature vectors in the vector space. [sent-82, score-0.796]
</p><p>43 Finally, we can reorder generated candidate words according to their semantic relatedness of the input seed word. [sent-83, score-1.15]
</p><p>44 We expect to retrieve more relevant candidate words earlier. [sent-84, score-0.362]
</p><p>45 1 Bipartite Graph Construction As stated before, we first construct a User-Word bipartite graph from the dataset. [sent-87, score-0.353]
</p><p>46 The bipartite graph has two layers, with users on one side and the words on the other side. [sent-88, score-0.491]
</p><p>47 We traverse the user records, and add a link between user u and word w if w appears in the user record of u. [sent-89, score-0.897]
</p><p>48 In order to give better explanations of bipartite graph construction step, we show some user records in Figure 1 and the corresponding bipartite graph in Figure 2. [sent-91, score-1.143]
</p><p>49 2  Candidates Generation  After the construction of bipartite graph, we can measure the relatedness of words from the bipartite graph. [sent-98, score-0.731]
</p><p>50 Intuitively, if two words always cooccur in user records, they are related to each other. [sent-99, score-0.483]
</p><p>51 In particular, the conditional probability of word j occurs given that word i has already appeared is the number of users that used both word iand word j divided by the total number of users that used word i. [sent-101, score-0.623]
</p><p>52 The disadvantage is that each word i tends to have a close relationship with stop words that are  used quite frequently in user records, such as “的” (of) and “一个” (a). [sent-104, score-0.444]
</p><p>53 Word iand word j is said to be quite related if conditional probabilities P(j|i) and P(i|j) are both relatively high. [sent-106, score-0.311]
</p><p>54 In their paper, a weighted harmonic averaging is used to define the relatedness score between word iand word j because either P(j|i) or P(i|j) being too small is a severe detriment. [sent-108, score-0.456]
</p><p>55 So far, we have introduced how to calculate the relatedness Score(i, j) between word i and word j. [sent-114, score-0.277]
</p><p>56 When a user enters an input seed word w, we can compute Score(w,c) between seed word w and each candidate word c, and then sort candidate words in a descending order. [sent-115, score-1.779]
</p><p>57 Top N candidate words are kept for re-ranking, we aim to reorder top N candidate words and return the more related candidate words earlier. [sent-116, score-1.499]
</p><p>58 Alternatively, we can also set a threshold for Score(w,c), which keeps the candidate word c with Score(w,c) larger than the threshold. [sent-117, score-0.391]
</p><p>59 We argue that this threshold is difficult to set because different seed words have different score thresholds. [sent-118, score-0.399]
</p><p>60 Note that this candidate generation step is completely statistical method as we only consider the co-occurrence of words. [sent-119, score-0.33]
</p><p>61 3  Semantic Feature Representation and Re-ranking As stated before, we utilize search engine to enrich semantic features of the input seed word  and top N candidate words. [sent-122, score-1.17]
</p><p>62 To be more specific, we issue a word to a search engine (Sogou, 2004) and get top 20 returned snippets. [sent-123, score-0.361]
</p><p>63 For an input seed word w, we can generate top N candidate words using formula (2). [sent-125, score-0.872]
</p><p>64 We issue each word to search engine and get returned snippets. [sent-126, score-0.32]
</p><p>65 Following the conventional approach, we calculate the relatedness between the input seed word w and a candidate word c as the cosine similarity between their feature vectors. [sent-128, score-0.972]
</p><p>66 Intuitively, if we introduce more candidate words, we are more likely to find related words in the candidate sets. [sent-129, score-0.8]
</p><p>67 51  As a result, candidate words with higher semantic similarities can be returned earlier with enriched semantic features. [sent-132, score-0.65]
</p><p>68 Re-ranking can be regarded as a complementary step after candidate  generation. [sent-133, score-0.294]
</p><p>69 We can improve the performance of related word retrieval task if we consider user behaviors and re-ranking together. [sent-134, score-0.885]
</p><p>70 Then, we build our ground truth for related word retrieval task using Baidu encyclopedia. [sent-137, score-0.453]
</p><p>71 Third, we give some example of related word retrieval task. [sent-138, score-0.364]
</p><p>72 We show that more related words can be returned earlier if we consider semantic features. [sent-139, score-0.347]
</p><p>73 The dataset contains 10,000 users and 183,870 words, and the number of edges in the constructed bipartite graph is 42,250,718. [sent-143, score-0.482]
</p><p>74 For related word retrieval task, we need to  judge whether a candidate word is related to the input seed word. [sent-145, score-1.162]
</p><p>75 In Baidu encyclopedia, volunteers give a set of words that are related to the particular seed word. [sent-149, score-0.543]
</p><p>76 We randomly select 2,000 seed words as our validation set. [sent-151, score-0.369]
</p><p>77 We just want to investigate whether user behaviors and re-ranking framework is helpful in the related word retrieval task under various evaluation metrics. [sent-154, score-0.935]
</p><p>78 The input seed word is “机器 学 ” (Machine Learning). [sent-156, score-0.434]
</p><p>79 Generally speaking, all these returned candidate words are relevant to the seed word to certain degree, which indicates the effectiveness of our method. [sent-157, score-0.827]
</p><p>80 As we cannot list all the related words of an input seed word, we use Bpref to evaluate our method. [sent-166, score-0.542]
</p><p>81 For an input seed word with R judged candidate words where r is a related word and n is a nonrelated word. [sent-167, score-0.96]
</p><p>82 For a sample of input seed words W, ranki is the rank of the first related candidate word for the input seed word wi, MRR is the average of the reciprocal ranks of results, which is defined as follow:  MRR 1Wiran1ki  (4)  4. [sent-170, score-1.397]
</p><p>83 The input seed word is “爱 立信” (Ericsson), and if we only take user behaviors into consideration, top 5 words returned are shown on the left side. [sent-172, score-1.198]
</p><p>84 After using search engine and semantic representation, we reorder the candidate words as shown on the right side. [sent-173, score-0.781]
</p><p>85 52  As shown in Table 2, we can clearly see that we return the most related candidate words such as “索尼 爱立信” (Sony Ericsson) and “索爱” (the abbreviation of Sony Ericsson in Chinese) in the first two places. [sent-174, score-0.557]
</p><p>86 Moreover, after re-ranking, top candidate words are some famous brands that are quite related to query word “爱立信” (Ericsson). [sent-175, score-0.69]
</p><p>87 Some words like “核心 网” (Core Network) that are not quite related to the query word are removed from the top list. [sent-176, score-0.396]
</p><p>88 The first is the parameter λ in the candidate generation step, and the other is the parameter N in the re-ranking step. [sent-180, score-0.396]
</p><p>89 5 to generate candidate words, those candidates are used for re-ranking. [sent-198, score-0.367]
</p><p>90 We can see that N = 20 gives relatively best results, which indicates that we should select Top 20 candidate words for re-ranking. [sent-219, score-0.362]
</p><p>91 h2@0o31d0951 5  Conclusions and Future Work  In this paper, we have proposed a novel method for related word retrieval task. [sent-225, score-0.368]
</p><p>92 Different from other method, we consider user behaviors, semantic features and re-ranking framework together. [sent-226, score-0.339]
</p><p>93 We make a reasonable assumption that if two words always co-occur in user records, then 53  they tend to have a close relationship with each other. [sent-227, score-0.332]
</p><p>94 Based on this assumption, we first gener-  ate a set of candidate words that are related to an input seed word via user behaviors. [sent-228, score-1.162]
</p><p>95 Second, we utilize search engine to enrich candidates with semantic features. [sent-229, score-0.436]
</p><p>96 Finally, we can reorder the candidate words to return more related candidates earlier. [sent-230, score-0.778]
</p><p>97 Furthermore, we want to take the advantage of learning to rank literature (Liu, 2009) to further improve the performance of related word retrieval task. [sent-235, score-0.412]
</p><p>98 Another important issue is how to build a complete and accurate ground truth for related word retrieval task. [sent-238, score-0.453]
</p><p>99 Thirdly, our method can only process a single  seed word, so we aim to extend our method to process multiple seed words. [sent-240, score-0.674]
</p><p>100 Finally, our dataset provides a new perspective for many interesting research tasks like new word detection, social network analysis, user behavior analysis, and so on. [sent-247, score-0.427]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('seed', 0.301), ('candidate', 0.294), ('behaviors', 0.289), ('user', 0.264), ('sogou', 0.246), ('bipartite', 0.24), ('reorder', 0.188), ('baidu', 0.173), ('retrieval', 0.168), ('bpref', 0.161), ('relatedness', 0.153), ('records', 0.151), ('ericsson', 0.123), ('engine', 0.113), ('google', 0.108), ('users', 0.108), ('related', 0.102), ('returned', 0.102), ('zheng', 0.1), ('sahami', 0.092), ('chinese', 0.088), ('enrich', 0.081), ('deshpande', 0.081), ('encyclopedia', 0.077), ('semantic', 0.075), ('graph', 0.075), ('query', 0.073), ('candidates', 0.073), ('input', 0.071), ('words', 0.068), ('ground', 0.067), ('word', 0.062), ('helman', 0.061), ('hiding', 0.061), ('maosong', 0.061), ('pinyin', 0.061), ('sony', 0.061), ('stepped', 0.061), ('wwoor', 0.061), ('yabin', 0.061), ('zhiyuan', 0.061), ('mrr', 0.06), ('dataset', 0.059), ('snippets', 0.058), ('truth', 0.054), ('karypis', 0.054), ('tsinghua', 0.054), ('return', 0.053), ('iand', 0.051), ('bayesian', 0.051), ('parameter', 0.051), ('utilize', 0.051), ('carry', 0.05), ('want', 0.05), ('quite', 0.05), ('gains', 0.049), ('cooccur', 0.049), ('regard', 0.049), ('pp', 0.047), ('conditional', 0.046), ('search', 0.043), ('record', 0.043), ('network', 0.042), ('introduce', 0.042), ('borrow', 0.042), ('yih', 0.042), ('top', 0.041), ('abbreviation', 0.04), ('volunteers', 0.04), ('measuring', 0.039), ('buckley', 0.039), ('accomplished', 0.039), ('metzler', 0.039), ('stated', 0.038), ('noisy', 0.037), ('method', 0.036), ('explanations', 0.036), ('enriched', 0.036), ('kernel', 0.036), ('jaccard', 0.035), ('reciprocal', 0.035), ('keeps', 0.035), ('formula', 0.035), ('harmonic', 0.033), ('severe', 0.033), ('averaging', 0.032), ('ghahramani', 0.032), ('comparative', 0.032), ('give', 0.032), ('alleviate', 0.031), ('experiment', 0.031), ('sets', 0.031), ('score', 0.03), ('rank', 0.03), ('enhance', 0.03), ('adopt', 0.03), ('construction', 0.03), ('similarity', 0.029), ('kept', 0.029), ('intuitively', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="129-tfidf-1" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>Author: Yabin Zheng ; Zhiyuan Liu ; Lixing Xie</p><p>Abstract: Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance. 1</p><p>2 0.23119146 <a title="129-tfidf-2" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>Author: David Vickrey ; Oscar Kipersztok ; Daphne Koller</p><p>Abstract: We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</p><p>3 0.14869566 <a title="129-tfidf-3" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>Author: Xiao-Li Li ; Lei Zhang ; Bing Liu ; See-Kiong Ng</p><p>Abstract: Distributional similarity is a classic technique for entity set expansion, where the system is given a set of seed entities of a particular class, and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds. This paper shows that a machine learning model called positive and unlabeled learning (PU learning) can model the set expansion problem better. Based on the test results of 10 corpora, we show that a PU learning technique outperformed distributional similarity significantly. 1</p><p>4 0.13310519 <a title="129-tfidf-4" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>Author: Xianpei Han ; Jun Zhao</p><p>Abstract: Name ambiguity problem has raised urgent demands for efficient, high-quality named entity disambiguation methods. In recent years, the increasing availability of large-scale, rich semantic knowledge sources (such as Wikipedia and WordNet) creates new opportunities to enhance the named entity disambiguation by developing algorithms which can exploit these knowledge sources at best. The problem is that these knowledge sources are heterogeneous and most of the semantic knowledge within them is embedded in complex structures, such as graphs and networks. This paper proposes a knowledge-based method, called Structural Semantic Relatedness (SSR), which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge sources. Empirical results show that, in comparison with the classical BOW based methods and social network based methods, our method can significantly improve the disambiguation performance by respectively 8.7% and 14.7%. 1</p><p>5 0.1296858 <a title="129-tfidf-5" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>6 0.11486632 <a title="129-tfidf-6" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>7 0.10135176 <a title="129-tfidf-7" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>8 0.091103166 <a title="129-tfidf-8" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>9 0.090943009 <a title="129-tfidf-9" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>10 0.090592742 <a title="129-tfidf-10" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>11 0.08836931 <a title="129-tfidf-11" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>12 0.086358249 <a title="129-tfidf-12" href="./acl-2010-Learning_Phrase-Based_Spelling_Error_Models_from_Clickthrough_Data.html">164 acl-2010-Learning Phrase-Based Spelling Error Models from Clickthrough Data</a></p>
<p>13 0.083755046 <a title="129-tfidf-13" href="./acl-2010-Modeling_Semantic_Relevance_for_Question-Answer_Pairs_in_Web_Social_Communities.html">174 acl-2010-Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities</a></p>
<p>14 0.083748005 <a title="129-tfidf-14" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>15 0.082719371 <a title="129-tfidf-15" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>16 0.08132457 <a title="129-tfidf-16" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>17 0.08037515 <a title="129-tfidf-17" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>18 0.079670697 <a title="129-tfidf-18" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>19 0.071066774 <a title="129-tfidf-19" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>20 0.068536572 <a title="129-tfidf-20" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.2), (1, 0.093), (2, -0.089), (3, 0.01), (4, 0.063), (5, -0.089), (6, -0.061), (7, 0.05), (8, -0.059), (9, 0.023), (10, -0.055), (11, 0.002), (12, -0.063), (13, -0.177), (14, 0.067), (15, 0.11), (16, 0.033), (17, -0.133), (18, -0.142), (19, -0.013), (20, 0.016), (21, -0.168), (22, -0.002), (23, 0.185), (24, 0.014), (25, -0.041), (26, -0.107), (27, 0.005), (28, 0.1), (29, 0.117), (30, 0.045), (31, 0.019), (32, -0.106), (33, -0.101), (34, -0.029), (35, -0.147), (36, -0.063), (37, -0.07), (38, 0.007), (39, -0.114), (40, -0.068), (41, 0.005), (42, -0.028), (43, -0.025), (44, -0.127), (45, -0.066), (46, 0.061), (47, 0.03), (48, -0.039), (49, 0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96749353 <a title="129-lsi-1" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>Author: Yabin Zheng ; Zhiyuan Liu ; Lixing Xie</p><p>Abstract: Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance. 1</p><p>2 0.82284045 <a title="129-lsi-2" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>Author: David Vickrey ; Oscar Kipersztok ; Daphne Koller</p><p>Abstract: We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</p><p>3 0.62355745 <a title="129-lsi-3" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>Author: Xiao-Li Li ; Lei Zhang ; Bing Liu ; See-Kiong Ng</p><p>Abstract: Distributional similarity is a classic technique for entity set expansion, where the system is given a set of seed entities of a particular class, and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds. This paper shows that a machine learning model called positive and unlabeled learning (PU learning) can model the set expansion problem better. Based on the test results of 10 corpora, we show that a PU learning technique outperformed distributional similarity significantly. 1</p><p>4 0.61332697 <a title="129-lsi-4" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>Author: Jia Wang ; Qing Li ; Yuanzhu Peter Chen ; Zhangxi Lin</p><p>Abstract: The variety of engaging interactions among users in social medial distinguishes it from traditional Web media. Such a feature should be utilized while attempting to provide intelligent services to social media participants. In this article, we present a framework to recommend relevant information in Internet forums and blogs using user comments, one of the most representative of user behaviors in online discussion. When incorporating user comments, we consider structural, semantic, and authority information carried by them. One of the most important observation from this work is that semantic contents of user comments can play a fairly different role in a different form of social media. When designing a recommendation system for this purpose, such a difference must be considered with caution.</p><p>5 0.52330083 <a title="129-lsi-5" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>Author: Ahmed Hassan ; Dragomir Radev</p><p>Abstract: Automatically identifying the polarity of words is a very important task in Natural Language Processing. It has applications in text classification, text filtering, analysis of product review, analysis of responses to surveys, and mining online discussions. We propose a method for identifying the polarity of words. We apply a Markov random walk model to a large word relatedness graph, producing a polarity estimate for any given word. A key advantage of the model is its ability to accurately and quickly assign a polarity sign and magnitude to any word. The method could be used both in a semi-supervised setting where a training set of labeled words is used, and in an unsupervised setting where a handful of seeds is used to define the two polarity classes. The method is experimentally tested using a manually labeled set of positive and negative words. It outperforms the state of the art methods in the semi-supervised setting. The results in the unsupervised setting is comparable to the best reported values. However, the proposed method is faster and does not need a large corpus.</p><p>6 0.51785338 <a title="129-lsi-6" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>7 0.49966246 <a title="129-lsi-7" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>8 0.48996222 <a title="129-lsi-8" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>9 0.48548442 <a title="129-lsi-9" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>10 0.48431447 <a title="129-lsi-10" href="./acl-2010-Comparable_Entity_Mining_from_Comparative_Questions.html">63 acl-2010-Comparable Entity Mining from Comparative Questions</a></p>
<p>11 0.47981375 <a title="129-lsi-11" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>12 0.47463873 <a title="129-lsi-12" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>13 0.45896572 <a title="129-lsi-13" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>14 0.45210207 <a title="129-lsi-14" href="./acl-2010-Learning_Phrase-Based_Spelling_Error_Models_from_Clickthrough_Data.html">164 acl-2010-Learning Phrase-Based Spelling Error Models from Clickthrough Data</a></p>
<p>15 0.44284284 <a title="129-lsi-15" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>16 0.43943366 <a title="129-lsi-16" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>17 0.40453616 <a title="129-lsi-17" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<p>18 0.39872193 <a title="129-lsi-18" href="./acl-2010-Online_Generation_of_Locality_Sensitive_Hash_Signatures.html">183 acl-2010-Online Generation of Locality Sensitive Hash Signatures</a></p>
<p>19 0.39826083 <a title="129-lsi-19" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>20 0.39809856 <a title="129-lsi-20" href="./acl-2010-A_Bayesian_Method_for_Robust_Estimation_of_Distributional_Similarities.html">3 acl-2010-A Bayesian Method for Robust Estimation of Distributional Similarities</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.04), (39, 0.012), (44, 0.014), (59, 0.038), (73, 0.027), (78, 0.022), (83, 0.058), (98, 0.694)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99480897 <a title="129-lda-1" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>Author: Yabin Zheng ; Zhiyuan Liu ; Lixing Xie</p><p>Abstract: Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance. 1</p><p>2 0.98988467 <a title="129-lda-2" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>Author: Kotaro Kitagawa ; Kumiko Tanaka-Ishii</p><p>Abstract: Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average pars- ing time not much slower than O(n).</p><p>3 0.98735648 <a title="129-lda-3" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>4 0.98427683 <a title="129-lda-4" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>Author: David Vickrey ; Oscar Kipersztok ; Daphne Koller</p><p>Abstract: We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</p><p>5 0.97611648 <a title="129-lda-5" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>Author: Xiangyu Duan ; Min Zhang ; Haizhou Li</p><p>Abstract: The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus. But word appears to be too fine-grained in some cases such as non-compositional phrasal equivalences, where no clear word alignments exist. Using words as inputs to PBSMT pipeline has inborn deficiency. This paper proposes pseudo-word as a new start point for PB-SMT pipeline. Pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation. By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in a monolingual way and a bilingual synchronous way. Experiments show that pseudo-word significantly outperforms word for PB-SMT model in both travel translation domain and news translation domain. 1</p><p>6 0.97605199 <a title="129-lda-6" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>7 0.96538752 <a title="129-lda-7" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>8 0.93892056 <a title="129-lda-8" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>9 0.93587637 <a title="129-lda-9" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>10 0.91639304 <a title="129-lda-10" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>11 0.9090535 <a title="129-lda-11" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>12 0.87569368 <a title="129-lda-12" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>13 0.86522561 <a title="129-lda-13" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>14 0.85752726 <a title="129-lda-14" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>15 0.85716575 <a title="129-lda-15" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>16 0.84680498 <a title="129-lda-16" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>17 0.84462804 <a title="129-lda-17" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>18 0.83653045 <a title="129-lda-18" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>19 0.83427751 <a title="129-lda-19" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>20 0.83415306 <a title="129-lda-20" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
