<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-24" href="#">acl2010-24</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</h1>
<br/><p>Source: <a title="acl-2010-24-pdf" href="http://aclweb.org/anthology//P/P10/P10-2067.pdf">pdf</a></p><p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>Reference: <a title="acl-2010-24-reference" href="../acl2010_reference/acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 lleond uUniversity 5000 Forbes Avenue, Pittsburgh, PA 15213, USA  Abstract Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. [sent-5, score-1.235]
</p><p>2 Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. [sent-6, score-1.885]
</p><p>3 Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner. [sent-7, score-1.758]
</p><p>4 1 Introduction Corpus-based approaches to machine translation have become predominant, with phrase-based sta-  tistical machine translation (PB-SMT) (Koehn et al. [sent-8, score-0.158]
</p><p>5 Parameters of these alignment models are learnt in an unsupervised manner using the EM algorithm over sentence-level aligned parallel corpora. [sent-12, score-0.604]
</p><p>6 Increased parallel data enables better estimation of the model parameters, but a large number of language pairs still lack such resources. [sent-14, score-0.132]
</p><p>7 The second is to use extra annotation, typically word-level human alignment for some sentence pairs, in conjunction with the parallel data to learn alignment in a semi-supervised manner. [sent-17, score-1.113]
</p><p>8 Our research is in the direction of the latter, and aims to reduce the effort involved in hand-generation of word alignments by using active learning strategies for careful selection of word pairs to seek alignment. [sent-18, score-1.042]
</p><p>9 In this paper we explore active learning for word alignment, where the input to the active learner is a sentence pair (S, T) and the annotation elicited from human is a set of links {aij ,∀si ∈ S, tj ∈ T}. [sent-21, score-1.104]
</p><p>10 require e plricevitiaotiuosn a poffull alignment for the sentence pair, which could be effort-intensive. [sent-23, score-0.506]
</p><p>11 We propose active learning query strategies to selectively elicit partial align–  ment information. [sent-24, score-0.649]
</p><p>12 Experiments in Section 5 show that our selection strategies reduce alignment error rates significantly over baseline. [sent-25, score-0.749]
</p><p>13 Fraser and Marcu (2006) pose the problem of alignment as a search problem in log-linear space with features coming from the IBM alignment models. [sent-27, score-1.012]
</p><p>14 They propose a semisupervised training algorithm which alternates between discriminative error training on the labeled data to learn the weighting parameters and maximum-likelihood EM training on unlabeled data to estimate the parameters. [sent-31, score-0.133]
</p><p>15 (2004) also improve alignment by interpolating human alignments with automatic alignments. [sent-33, score-0.691]
</p><p>16 They observe that while working with such data sets, alignments of higher quality should be given a much higher weight than the lower-quality alignments. [sent-34, score-0.184]
</p><p>17 To our knowledge, there is no prior work that has looked at reducing human effort by selective elicitation of partial word alignment using active learning techniques. [sent-38, score-1.197]
</p><p>18 Several studies (Tong and Koller, 2002; Nguyen and Smeulders, 2004; Donmez and Carbonell, 2008) show that active learning greatly helps to reduce the labeling effort in various classification tasks. [sent-41, score-0.449]
</p><p>19 1 Active Learning Setup We discuss our active learning setup for word alignment in Algorithm 1. [sent-43, score-0.915]
</p><p>20 We start with an unlabeled dataset U = {(Sk, Tk)}, indexed by k, laanbde a dse dedat pool Uof partial alignment dlienxkesd A0 = {aikj ,∀si ∈ Sk, tj ∈ Tk}. [sent-44, score-0.679]
</p><p>21 We take a pool-based active learning strategy, where we have access to all the automatically aligned links and we can score the links based on our active learning query strategy. [sent-47, score-1.317]
</p><p>22 The query strategy uses the automatically trained alignment model Mt from current iteration t for scoring the links. [sent-48, score-0.782]
</p><p>23 Re-training and re-tuning an SMT system for each link at a time is computationally infeasible. [sent-49, score-0.202]
</p><p>24 We therefore perform batch learning by se-  lecting a set of N links scored high by our query strategy. [sent-50, score-0.531]
</p><p>25 We seek manual corrections for the selected links and add the alignment data to the current labeled data set. [sent-51, score-0.973]
</p><p>26 The word-level aligned labeled data is provided to our semi-supervised word alignment algorithm for training an alignment model Mt+1 over U. [sent-52, score-1.12]
</p><p>27 In a more typical scenario, since reducing human effort or cost of elicitation is the objective, we iterate until the available budget is exhausted. [sent-54, score-0.271]
</p><p>28 Manual alignments are incorporated in the EM training phase of these models as constraints that restrict the summation over all possible alignment paths. [sent-57, score-0.711]
</p><p>29 The manual alignments allow for one-tomany alignments and many-to-many alignments in both directions. [sent-59, score-0.544]
</p><p>30 Therefore, the restriction of the alignment paths reduces to restricting the summation in EM. [sent-62, score-0.564]
</p><p>31 4  Query Strategies for Link Selection  We propose multiple query selection strategies for our active learning setup. [sent-63, score-0.661]
</p><p>32 The scoring criteria is designed to select alignment links across sentence pairs that are highly uncertain under current automatic translation models. [sent-64, score-1.074]
</p><p>33 These links are difficult to align correctly by automatic alignment and will cause incorrect phrase pairs to be extracted in the translation model, in turn hurting the translation quality of the SMT system. [sent-65, score-0.971]
</p><p>34 Manual correction of such links produces the maximal benefit to the model. [sent-66, score-0.311]
</p><p>35 We would ideally like to elicit the least number of manual corrections possible in order to reduce the cost of data acquisition. [sent-67, score-0.282]
</p><p>36 In this section we discuss our link selection strategies based on the standard active learning paradigm of ‘uncer-  tainty sampling’(Lewis and Catlett, 1994). [sent-68, score-0.735]
</p><p>37 We use the automatically trained translation model θt for scoring each link for uncertainty, which consists of bidirectional translation lexicon tables computed from the bidirectional alignments. [sent-69, score-0.549]
</p><p>38 1 Uncertainty Sampling: Bidirectional Alignment Scores The automatic Viterbi alignment produced by the alignment models is used to obtain translation lexicons. [sent-71, score-1.091]
</p><p>39 These lexicons capture the conditional distributions of source-given-target P(s/t) and target-given-source P(t/s) probabilities at the word level where si ∈ S and tj ∈ T. [sent-72, score-0.151]
</p><p>40 We define certainty of a link as tShe a hnadrm ton∈ic mean oef tdheebidirectional probabilities. [sent-73, score-0.202]
</p><p>41 The selection strategy selects the least scoring links according to the formula below which corresponds to links with maximum uncertainty:  Score(aij/s1I,t1J) =2P ∗( Ptj(/tsj/is)i +) ∗ P P(s(is/it/jt)j) 4. [sent-74, score-0.733]
</p><p>42 Given a sentence pair (sI1, t1J) and its word alignment, we compute two confidence metrics at alignment link level based on the posterior link probability as seen in Equation 5. [sent-77, score-1.076]
</p><p>43 We select the alignment links that the initial word aligner is least confident according to our metric and seek manual correction of the links. [sent-78, score-1.001]
</p><p>44 Targeting some of the uncertain parts of word alignment has already been shown to improve translation quality in SMT (Huang, 2009). [sent-80, score-0.718]
</p><p>45 We use confidence metrics as an active learning sampling strategy to obtain most informative links. [sent-81, score-0.78]
</p><p>46 We also experimented with other confidence metrics as discussed in (Ueffing and Ney, 2007), especially the IBM 1 model score metric, but it did not show significant improvement in this task. [sent-82, score-0.13]
</p><p>47 3 Query by Committee The generative alignments produced differ based on the choice of direction of the language pair. [sent-84, score-0.217]
</p><p>48 We use As2t to denote alignment in the source to target direction and At2s to denote the target to source direction. [sent-85, score-0.542]
</p><p>49 We consider these alignments to be two experts that have two different views of the alignment process. [sent-86, score-0.691]
</p><p>50 We formulate our query strategy to select links where the agreement differs across these two alignments. [sent-87, score-0.48]
</p><p>51 In general query by committee is a standard sampling strategy in active learning(Freund et al. [sent-88, score-0.852]
</p><p>52 We formulate a query by committee sampling strategy for word alignment as shown in Equation 6. [sent-90, score-1.056]
</p><p>53 In order to  break ties, we extend this approach to select the link with higher average frequency of occurrence of words involved in the link. [sent-91, score-0.28]
</p><p>54 4 Margin Sampling The strategy for confidence based sampling only considers information about the best scoring link 367  conf(aij/S, T). [sent-93, score-0.639]
</p><p>55 However we could benefit from information about the second best scoring link as well. [sent-94, score-0.279]
</p><p>56 , 2001), where the difference between the probabilities assigned by the underlying model to the first best and second best labels is used as a sampling criteria. [sent-96, score-0.194]
</p><p>57 Our margin technique is formulated below, where and are potential first best and second best scoring alignment links for a word at position iin the source sentence S with translation T. [sent-98, score-1.033]
</p><p>58 The word with minimum margin value is chosen for human alignment. [sent-99, score-0.136]
</p><p>59 1 Data Setup Our aim in this paper is to show that active learning can help select the most informative alignment links that have high uncertainty according to a given automatically trained model. [sent-102, score-1.235]
</p><p>60 We also show that fixing such alignments leads to the maximum reduction of error in word alignment, as measured by AER. [sent-103, score-0.265]
</p><p>61 We compare this with a baseline where links are selected at random for manual correction. [sent-104, score-0.342]
</p><p>62 To run our experiments iteratively, we automate the setup by using a parallel corpus for which the gold-standard human alignment is already available. [sent-105, score-0.642]
</p><p>63 We select the Chinese-English language pair, where we have access to 21,863 sentence pairs along with complete manual alignment. [sent-106, score-0.179]
</p><p>64 We then use the learned model in running our link selection algorithm over the entire corpus to determine the most uncertain links according to each active learning strategy. [sent-109, score-0.983]
</p><p>65 The links are then looked up in the gold-standard human alignment database and corrected. [sent-110, score-0.783]
</p><p>66 In case a link is not present in the gold-standard data, we introduce a NULL alignment, else we propose the alignment as given in  Figure 1: Performance of active sampling strategies for link selection the gold standard. [sent-111, score-1.637]
</p><p>67 We select the partial alignment as a set of alignment links and provide it to our semi-supervised word aligner. [sent-112, score-1.377]
</p><p>68 We plot performance curves as number of links used in each iteration vs. [sent-113, score-0.239]
</p><p>69 Query by committee performs worse than random indicating that two alignments differing in direction are not sufficient in deciding for uncertainty. [sent-115, score-0.304]
</p><p>70 We observe that confidence based metrics perform significantly better than the baseline. [sent-117, score-0.167]
</p><p>71 From the scatter plots in Figure 1 1 we can say that using our best selection strategy one achieves similar performance to the baseline, but at a much lower cost of elicitation assuming cost per link is uniform. [sent-118, score-0.587]
</p><p>72 We also perform end-to-end machine translation experiments to show that our improvement of alignment quality leads to an improvement of translation scores. [sent-119, score-0.664]
</p><p>73 We first obtain the baseline score where no manual alignment was used. [sent-123, score-0.609]
</p><p>74 We also train a configuration using gold standard manual align-  ment data for the parallel corpus. [sent-124, score-0.166]
</p><p>75 This is the maximum translation accuracy that we can achieve by any link selection algorithm. [sent-125, score-0.388]
</p><p>76 We now take the best link selection criteria, which is the confidence 1X axis has number of links elicited on a log-scale 368  HumaBSnay sAsetl ei mngenmentB1198L. [sent-126, score-0.687]
</p><p>77 Therefore we achieve 45% of the possible improvement by only using 20% elicitation effort. [sent-138, score-0.133]
</p><p>78 3  Batch Selection  Re-training the word alignment models after eliciting every individual alignment link is infeasible. [sent-140, score-1.308]
</p><p>79 In our data set of 21,863 sentences with 588,075 links, it would be computationally intensive to retrain after eliciting even 100 links in a batch. [sent-141, score-0.297]
</p><p>80 We therefore sample links as a discrete batch, and train alignment models to report performance at fixed points. [sent-142, score-0.745]
</p><p>81 Such a batch selection is only going to be sub-optimal as the underlying model changes with every alignment link and therefore becomes ‘stale’ for future selections. [sent-143, score-0.979]
</p><p>82 We observe that in some scenarios while fixing one alignment link could potentially fix all the mis-alignments in a sentence pair, our batch selection mechanism still samples from the rest of the links in the sentence pair. [sent-144, score-1.294]
</p><p>83 We experimented with an exponential decay function over the number of links previously selected, in order to discourage repeated sampling from the same sentence pair. [sent-145, score-0.538]
</p><p>84 We performed an experiment by selecting one of our best performing selection  strategies (conf) and ran it in both configurations - one with the decay parameter (batchdecay) and one without it (batch). [sent-146, score-0.3]
</p><p>85 As seen in Figure 2, the decay function has an effect in the initial part of the curve where sampling is sparse but the effect gradually fades away as we observe more samples. [sent-147, score-0.336]
</p><p>86 In the reported results we do not use batch decay, but an optimal estimation of ‘staleness’ could lead to better gains in batch link selection using active learning. [sent-148, score-1.01]
</p><p>87 Figure 2: Batch decay effects on Conf-posterior sampling strategy 6  Conclusion and Future Work  Word-Alignment is a particularly challenging problem and has been addressed in a completely unsupervised manner thus far (Brown et al. [sent-149, score-0.37]
</p><p>88 While generative alignment models have been suc-  cessful, lack of sufficient data, model assumptions and local optimum during training are well known problems. [sent-151, score-0.575]
</p><p>89 Semi-supervised techniques use partial manual alignment data to address some of these issues. [sent-152, score-0.657]
</p><p>90 We have shown that active learning strategies can reduce the effort involved in eliciting human alignment data. [sent-153, score-1.175]
</p><p>91 The reduction in effort is due to careful selection of maximally uncertain links that provide the most benefit to the alignment model when used in a semi-supervised training fashion. [sent-154, score-1.087]
</p><p>92 In future we wish to work with word alignments for other language pairs like Arabic and English. [sent-156, score-0.217]
</p><p>93 We have tested out the feasibility of obtaining human word alignment data using Amazon Mechanical Turk and plan to obtain more data reduce the cost of annotation. [sent-157, score-0.665]
</p><p>94 The first author would like to thank Qin Gao for the semi-supervised word alignment  software and help with running  experiments. [sent-160, score-0.542]
</p><p>95 Statistical machine translation with word- and sentence-aligned parallel corpora. [sent-176, score-0.142]
</p><p>96 Soft syntactic constraints for word alignment through discriminative training. [sent-181, score-0.542]
</p><p>97 Optimizing estimated loss reduction for active sampling in rank learning. [sent-186, score-0.575]
</p><p>98 Parallel implementa-  tions of word alignment tool. [sent-213, score-0.542]
</p><p>99 Support vector machine active learning with applications to text classification. [sent-265, score-0.338]
</p><p>100 Boosting statistical word alignment using labeled and unlabeled data. [sent-275, score-0.633]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alignment', 0.506), ('active', 0.338), ('links', 0.239), ('link', 0.202), ('sampling', 0.194), ('batch', 0.164), ('alignments', 0.147), ('elicitation', 0.133), ('query', 0.128), ('committee', 0.121), ('selection', 0.107), ('decay', 0.105), ('manual', 0.103), ('fraser', 0.101), ('uncertain', 0.097), ('confidence', 0.095), ('strategies', 0.088), ('translation', 0.079), ('scoring', 0.077), ('aer', 0.075), ('strategy', 0.071), ('tj', 0.071), ('mt', 0.065), ('effort', 0.063), ('parallel', 0.063), ('uncertainty', 0.063), ('aikj', 0.062), ('donmez', 0.062), ('scheffer', 0.062), ('margin', 0.062), ('smt', 0.059), ('summation', 0.058), ('eliciting', 0.058), ('ueffing', 0.058), ('morristown', 0.057), ('bidirectional', 0.056), ('sk', 0.055), ('aug', 0.054), ('conf', 0.054), ('haffari', 0.054), ('unlabeled', 0.054), ('ibm', 0.052), ('tk', 0.052), ('aij', 0.05), ('vamshi', 0.05), ('nj', 0.05), ('partial', 0.048), ('reduce', 0.048), ('em', 0.048), ('informative', 0.047), ('corrections', 0.047), ('elicit', 0.047), ('qin', 0.047), ('gao', 0.046), ('elicited', 0.044), ('meteor', 0.044), ('lt', 0.044), ('si', 0.044), ('ney', 0.044), ('reduction', 0.043), ('semisupervised', 0.042), ('carbonell', 0.042), ('blatz', 0.042), ('select', 0.042), ('koehn', 0.042), ('seek', 0.041), ('jaime', 0.04), ('lavie', 0.04), ('nicola', 0.04), ('fixing', 0.039), ('human', 0.038), ('freund', 0.038), ('tong', 0.038), ('experts', 0.038), ('maximal', 0.038), ('cost', 0.037), ('labeled', 0.037), ('observe', 0.037), ('vogel', 0.036), ('involved', 0.036), ('direction', 0.036), ('marcu', 0.036), ('word', 0.036), ('estimation', 0.035), ('lewis', 0.035), ('selective', 0.035), ('nguyen', 0.035), ('metrics', 0.035), ('aligned', 0.035), ('setup', 0.035), ('assumptions', 0.035), ('iin', 0.034), ('pairs', 0.034), ('generative', 0.034), ('align', 0.034), ('stephan', 0.034), ('correction', 0.034), ('cherry', 0.033), ('careful', 0.032), ('reached', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="24-tfidf-1" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>2 0.36561239 <a title="24-tfidf-2" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>3 0.35356748 <a title="24-tfidf-3" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>4 0.32651657 <a title="24-tfidf-4" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>5 0.30380481 <a title="24-tfidf-5" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>6 0.25312909 <a title="24-tfidf-6" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>7 0.23682754 <a title="24-tfidf-7" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>8 0.23645023 <a title="24-tfidf-8" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>9 0.19125463 <a title="24-tfidf-9" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>10 0.18786886 <a title="24-tfidf-10" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>11 0.1800935 <a title="24-tfidf-11" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>12 0.14977269 <a title="24-tfidf-12" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>13 0.12945917 <a title="24-tfidf-13" href="./acl-2010-Learning_Phrase-Based_Spelling_Error_Models_from_Clickthrough_Data.html">164 acl-2010-Learning Phrase-Based Spelling Error Models from Clickthrough Data</a></p>
<p>14 0.11764578 <a title="24-tfidf-14" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>15 0.11569145 <a title="24-tfidf-15" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>16 0.11508211 <a title="24-tfidf-16" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>17 0.11390848 <a title="24-tfidf-17" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>18 0.11159629 <a title="24-tfidf-18" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>19 0.10437876 <a title="24-tfidf-19" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>20 0.1022153 <a title="24-tfidf-20" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.295), (1, -0.341), (2, -0.11), (3, -0.017), (4, 0.138), (5, 0.101), (6, -0.253), (7, 0.096), (8, 0.159), (9, -0.099), (10, -0.151), (11, -0.075), (12, -0.199), (13, 0.051), (14, -0.073), (15, 0.024), (16, 0.021), (17, -0.039), (18, -0.058), (19, -0.022), (20, 0.033), (21, 0.021), (22, 0.069), (23, -0.037), (24, 0.01), (25, -0.099), (26, 0.135), (27, 0.059), (28, 0.073), (29, 0.113), (30, 0.005), (31, 0.017), (32, -0.055), (33, -0.034), (34, -0.011), (35, -0.029), (36, -0.003), (37, -0.009), (38, 0.062), (39, 0.027), (40, -0.026), (41, -0.054), (42, -0.074), (43, 0.025), (44, 0.011), (45, 0.1), (46, 0.004), (47, -0.023), (48, 0.055), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98282272 <a title="24-lsi-1" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>2 0.87440377 <a title="24-lsi-2" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>Author: Sittichai Jiampojamarn ; Grzegorz Kondrak</p><p>Abstract: Letter-phoneme alignment is usually generated by a straightforward application of the EM algorithm. We explore several alternative alignment methods that employ phonetics, integer programming, and sets of constraints, and propose a novel approach of refining the EM alignment by aggregation of best alignments. We perform both intrinsic and extrinsic evaluation of the assortment of methods. We show that our proposed EM-Aggregation algorithm leads to the improvement of the state of the art in letter-to-phoneme conversion on several different data sets.</p><p>3 0.87393731 <a title="24-lsi-3" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>4 0.83483356 <a title="24-lsi-4" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>5 0.81411588 <a title="24-lsi-5" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>6 0.780783 <a title="24-lsi-6" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>7 0.74380815 <a title="24-lsi-7" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>8 0.63464803 <a title="24-lsi-8" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>9 0.57641554 <a title="24-lsi-9" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>10 0.53420222 <a title="24-lsi-10" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>11 0.50438088 <a title="24-lsi-11" href="./acl-2010-Learning_Phrase-Based_Spelling_Error_Models_from_Clickthrough_Data.html">164 acl-2010-Learning Phrase-Based Spelling Error Models from Clickthrough Data</a></p>
<p>12 0.47083285 <a title="24-lsi-12" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>13 0.46929035 <a title="24-lsi-13" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>14 0.46464708 <a title="24-lsi-14" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>15 0.4450171 <a title="24-lsi-15" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>16 0.42691106 <a title="24-lsi-16" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>17 0.42069453 <a title="24-lsi-17" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>18 0.39119875 <a title="24-lsi-18" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>19 0.39116853 <a title="24-lsi-19" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>20 0.38281348 <a title="24-lsi-20" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.023), (25, 0.026), (39, 0.02), (59, 0.093), (73, 0.053), (78, 0.014), (83, 0.068), (98, 0.611)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9963553 <a title="24-lda-1" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>Author: Yabin Zheng ; Zhiyuan Liu ; Lixing Xie</p><p>Abstract: Motivated by Google Sets, we study the problem of growing related words from a single seed word by leveraging user behaviors hiding in user records of Chinese input method. Our proposed method is motivated by the observation that the more frequently two words cooccur in user records, the more related they are. First, we utilize user behaviors to generate candidate words. Then, we utilize search engine to enrich candidate words with adequate semantic features. Finally, we reorder candidate words according to their semantic relatedness to the seed word. Experimental results on a Chinese input method dataset show that our method gains better performance. 1</p><p>2 0.99563742 <a title="24-lda-2" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>Author: Kotaro Kitagawa ; Kumiko Tanaka-Ishii</p><p>Abstract: Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average pars- ing time not much slower than O(n).</p><p>3 0.99525607 <a title="24-lda-3" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>4 0.99213582 <a title="24-lda-4" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>Author: David Vickrey ; Oscar Kipersztok ; Daphne Koller</p><p>Abstract: We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</p><p>5 0.99082541 <a title="24-lda-5" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>Author: Xiangyu Duan ; Min Zhang ; Haizhou Li</p><p>Abstract: The pipeline of most Phrase-Based Statistical Machine Translation (PB-SMT) systems starts from automatically word aligned parallel corpus. But word appears to be too fine-grained in some cases such as non-compositional phrasal equivalences, where no clear word alignments exist. Using words as inputs to PBSMT pipeline has inborn deficiency. This paper proposes pseudo-word as a new start point for PB-SMT pipeline. Pseudo-word is a kind of basic multi-word expression that characterizes minimal sequence of consecutive words in sense of translation. By casting pseudo-word searching problem into a parsing framework, we search for pseudo-words in a monolingual way and a bilingual synchronous way. Experiments show that pseudo-word significantly outperforms word for PB-SMT model in both travel translation domain and news translation domain. 1</p><p>same-paper 6 0.98715115 <a title="24-lda-6" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>7 0.98708522 <a title="24-lda-7" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>8 0.96299338 <a title="24-lda-8" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>9 0.95101058 <a title="24-lda-9" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>10 0.94780523 <a title="24-lda-10" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>11 0.94035709 <a title="24-lda-11" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>12 0.90857035 <a title="24-lda-12" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>13 0.90479445 <a title="24-lda-13" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>14 0.89711756 <a title="24-lda-14" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>15 0.89710677 <a title="24-lda-15" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>16 0.88317525 <a title="24-lda-16" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>17 0.88273168 <a title="24-lda-17" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>18 0.87946057 <a title="24-lda-18" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>19 0.87653434 <a title="24-lda-19" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>20 0.87305212 <a title="24-lda-20" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
