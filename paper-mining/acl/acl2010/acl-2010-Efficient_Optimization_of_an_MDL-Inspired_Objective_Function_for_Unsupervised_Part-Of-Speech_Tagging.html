<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-96" href="#">acl2010-96</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</h1>
<br/><p>Source: <a title="acl-2010-96-pdf" href="http://aclweb.org/anthology//P/P10/P10-2039.pdf">pdf</a></p><p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>Reference: <a title="acl-2010-96-reference" href="../acl2010_reference/acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. [sent-2, score-0.244]
</p><p>2 Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). [sent-3, score-0.714]
</p><p>3 We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this  function. [sent-4, score-0.092]
</p><p>4 Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. [sent-5, score-0.499]
</p><p>5 The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets. [sent-6, score-0.59]
</p><p>6 1 Introduction The Minimum Description Length (MDL) principle is a method for model selection that provides a generic solution to the overfitting problem (Barron et al. [sent-7, score-0.156]
</p><p>7 A formalization of Ockham’s Razor, it says that the parameters are to be chosen that minimize the description length of the data given the model plus the description length of the model itself. [sent-9, score-0.416]
</p><p>8 It has been successfully shown that minimizing the model size in a Hidden Markov Model (HMM) for part-of-speech (POS) tagging leads to higher accuracies than simply running the Expectation-  Maximization (EM) algorithm (Dempster et al. [sent-10, score-0.711]
</p><p>9 Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. [sent-12, score-0.563]
</p><p>10 More re2Computer Science Division University of California at Berkeley Soda Hall Berkeley, CA 94720 adpauls@eecs berkeley edu  . [sent-13, score-0.062]
</p><p>11 cently, Ravi and Knight (2009) alternately minimize the model using an integer linear program and maximize likelihood using EM to achieve the highest accuracies on the task so far. [sent-15, score-0.597]
</p><p>12 However, in the latter approach, because there is no single objective function to optimize, it is not entirely clear how to generalize this technique to other problems. [sent-16, score-0.331]
</p><p>13 In this paper, inspired by the MDL principle, we develop an objective function for generative models that captures both the description of the data by the model (log-likelihood) and the description of the model (model size). [sent-17, score-0.714]
</p><p>14 By using a simple prior that encourages sparsity, we cast our  problem as a search for the maximum a posteriori (MAP) hypothesis and present a variant of EM to approximately search for the minimumdescription-length model. [sent-18, score-0.039]
</p><p>15 Applying our approach to the POS tagging problem, we obtain higher accuracies than both EM and Bayesian inference as reported by Goldwater and Griffiths (2007). [sent-19, score-0.528]
</p><p>16 On a Italian POS tagging task, we obtain even larger improvements. [sent-20, score-0.316]
</p><p>17 We find that our objective function correlates well with accuracy, suggesting that this technique might be useful for other problems. [sent-21, score-0.417]
</p><p>18 1 Objective function In the unsupervised POS tagging task, we are given a word sequence w = w1,. [sent-23, score-0.455]
</p><p>19 We adopt the problem fo∈rm Tul,a tthioen t aogf vMoceraibaludloar (1994), ionp wt hthiceh p we are given a dictionary of possible tags for each word type. [sent-30, score-0.128]
</p><p>20 We define a bigram HMM  P(w,t | θ) =YiN=1P(w,t | θ) · P(ti| ti−1)  (1)  In maximum likelihood estimation, the goal is to 209  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-31, score-0.149]
</p><p>21 However, we would like to maximize likelihood and minimize the size of the model simultaneously. [sent-34, score-0.301]
</p><p>22 We define the size of a model as the number of non-zero probabilities in its parameter vector. [sent-35, score-0.085]
</p><p>23 (4)  where kθk0, called the L0 norm of θ, simply counts wtheh rneu kmθkber of non-zero parameters in θ. [sent-42, score-0.254]
</p><p>24 The hyperparameter α controls the tradeoff between likelihood maximization and model minimization. [sent-43, score-0.251]
</p><p>25 Note the similarity of this objective function with MDL’s, where α would be the space (measured in nats) needed to describe one parameter of the model. [sent-44, score-0.331]
</p><p>26 Unfortunately, minimization of the L0 norm is known to be NP-hard (Hyder and Mahata, 2009). [sent-45, score-0.259]
</p><p>27 It is not smooth, making it unamenable to gradient-based optimization algorithms. [sent-46, score-0.069]
</p><p>28 We perform this optimization for each instance of (15). [sent-57, score-0.069]
</p><p>29 These optimizations could easily be per-  formed in parallel for greater scalability. [sent-58, score-0.041]
</p><p>30 3  Experiments  We carried out POS tagging experiments on English and Italian. [sent-59, score-0.366]
</p><p>31 1 English POS tagging To set the hyperparameters αt and β, we prepared three held-out sets H1,H2, and H3 from the Penn Treebank. [sent-61, score-0.481]
</p><p>32 We ran MAP-EM for 100 iterations, with uniform probability initialization, for a suite of hyperparameters and averaged their tagging accuracies over the three held-out sets. [sent-63, score-0.816]
</p><p>33 We then picked the hyperparameter setting with the highest average accuracy. [sent-65, score-0.098]
</p><p>34 We then ran MAP-EM again on the test data with these hyperparameters and achieved a tagging accuracy of 87. [sent-68, score-0.612]
</p><p>35 2% that Goldwater and Griffiths (2007) obtain using Bayesian methods for inferring both POS tags and hyperparameters. [sent-71, score-0.039]
</p><p>36 4% that standard EM achieves on the test set when run for 100 iterations. [sent-73, score-0.101]
</p><p>37 05, we ran multiple random restarts on the test set (see Figure 2). [sent-75, score-0.321]
</p><p>38 We find that the objective function correlates well with accuracy, and picking the point with the highest objective function value achieves 87. [sent-76, score-0.908]
</p><p>39 0025  Table 2: Average accuracies over three held-out sets for English. [sent-87, score-0.212]
</p><p>40 4 + random restarts (Goldwater and Griffiths, 2007) our approach + random restarts  84. [sent-89, score-0.528]
</p><p>41 1  Table 1: MAP-EM with a L0 norm achieves higher  tagging accuracy on English than (2007) and much higher than standard EM. [sent-93, score-0.657]
</p><p>42 bigram types maximum possible1389– EM, 100 iterations 444 924 MAP-EM, 100 iterations 695 648 system  zero parameters  Table 3: MAP-EM with a smoothed L0 norm yields much smaller models than standard EM. [sent-94, score-0.56]
</p><p>43 We also carried out the same experiment with standard EM (Figure 3), where picking the point with the highest corpus probability achieves 84. [sent-95, score-0.249]
</p><p>44 We also measured the minimization effect of the sparse prior against that of standard EM. [sent-97, score-0.241]
</p><p>45 Since our method lower-bounds all the parameters by ? [sent-98, score-0.044]
</p><p>46 We also measured the number of unique tag bigram types in the Viterbi tagging of the word sequence. [sent-101, score-0.412]
</p><p>47 Table 3 shows that our method produces much smaller models than EM, and produces Viterbi taggings with many fewer tag-bigram types. [sent-102, score-0.044]
</p><p>48 2 Italian POS tagging We also carried out POS tagging experiments on an Italian corpus from the Italian Turin Univerαt=80,β=0. [sent-104, score-0.682]
</p><p>49 05,Test Set 24115 Words  objective function value  Figure 2: Tagging accuracy vs. [sent-105, score-0.405]
</p><p>50 objective function for 1152 random restarts of MAP-EM with smoothed L0 norm. [sent-106, score-0.698]
</p><p>51 This test set comprises 21, 878 words annotated with POS tags and a dictionary for each word type. [sent-109, score-0.039]
</p><p>52 Since this is all the available data, we could not tune the hyperparameters on a held-out data set. [sent-110, score-0.165]
</p><p>53 Using the hyper-  parameters tuned on English (αt = 80,β = 0. [sent-111, score-0.044]
</p><p>54 7% tagging accuracy (see Table 4), which was a large improvement over 81. [sent-113, score-0.39]
</p><p>55 When we tuned the hyperparameters on the test set, the best setting (αt = 120, β = 0. [sent-115, score-0.165]
</p><p>56 4  Conclusion  A variety of other techniques in the literature have been applied to this unsupervised POS tagging task. [sent-118, score-0.36]
</p><p>57 Smith and Eisner (2005) use conditional random fields with contrastive estimation to achieve 212  αt  0. [sent-119, score-0.148]
</p><p>58 0025  objective function value  Figure 3: Tagging accuracy vs. [sent-129, score-0.405]
</p><p>59 (2008) provide a linguistically-informed starting point for EM to achieve 91. [sent-134, score-0.042]
</p><p>60 (2010) use GIbbs sampling for Bayesian inference along with automatic run selection and achieve 90. [sent-137, score-0.042]
</p><p>61 In this paper, our goal has been to investigate whether EM can be extended in a generic way to use an MDL-like objective function that simultaneously maximizes likelihood and mini-  mizes model size. [sent-139, score-0.468]
</p><p>62 We have presented an efficient search procedure that optimizes this function for generative models and demonstrated that maximizing this function leads to improvement in tagging accuracy over standard EM. [sent-140, score-0.703]
</p><p>63 We infer the hyperparameters of our model using held out data and achieve better accuracies than (Goldwater and Griffiths, 2007). [sent-141, score-0.463]
</p><p>64 We have also shown that the objective function correlates well with tagging accuracy supporting the MDL principle. [sent-142, score-0.807]
</p><p>65 Our approach performs quite well on POS tagging for both English and Italian. [sent-143, score-0.316]
</p><p>66 This research was supported in part by DARPA contract HR001 1-06-C-0022 under subcontract to BBN Technologies and DARPA con-  tract HR001 1-09-1-0028. [sent-146, score-0.039]
</p><p>67 The minimum description length principle in coding and modeling. [sent-163, score-0.257]
</p><p>68 Converting a dependency treebank to a categorical grammar treebank for italian. [sent-175, score-0.082]
</p><p>69 Maximum likelihood from incomplete data via the EM algorithm. [sent-194, score-0.093]
</p><p>70 An approximate L0 norm minimization algorithm for compressed sensing. [sent-214, score-0.259]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tagging', 0.316), ('em', 0.301), ('objective', 0.236), ('mdl', 0.223), ('restarts', 0.219), ('accuracies', 0.212), ('goldwater', 0.187), ('italian', 0.182), ('pos', 0.18), ('norm', 0.166), ('hyperparameters', 0.165), ('griffiths', 0.116), ('principle', 0.112), ('hyder', 0.109), ('mohimani', 0.109), ('smoothed', 0.103), ('description', 0.1), ('ravi', 0.096), ('barron', 0.096), ('function', 0.095), ('minimization', 0.093), ('likelihood', 0.093), ('bayesian', 0.09), ('equality', 0.088), ('correlates', 0.086), ('arg', 0.084), ('minimize', 0.084), ('accuracy', 0.074), ('sparse', 0.069), ('optimization', 0.069), ('dempster', 0.069), ('suite', 0.066), ('chiang', 0.066), ('ti', 0.065), ('achieves', 0.062), ('goldberg', 0.062), ('berkeley', 0.062), ('hmm', 0.061), ('bos', 0.061), ('contrastive', 0.061), ('knight', 0.059), ('minimizing', 0.059), ('hyperparameter', 0.059), ('picking', 0.059), ('ran', 0.057), ('bigram', 0.056), ('maximization', 0.055), ('te', 0.054), ('iterations', 0.053), ('ep', 0.053), ('gradient', 0.053), ('develop', 0.05), ('priors', 0.05), ('carried', 0.05), ('cently', 0.048), ('iofth', 0.048), ('bosco', 0.048), ('eecs', 0.048), ('hessian', 0.048), ('hthiceh', 0.048), ('martnez', 0.048), ('nats', 0.048), ('rissanen', 0.048), ('sity', 0.048), ('turin', 0.048), ('zero', 0.046), ('darpa', 0.045), ('generative', 0.045), ('minimum', 0.045), ('random', 0.045), ('parameters', 0.044), ('model', 0.044), ('unsupervised', 0.044), ('yin', 0.044), ('soda', 0.044), ('alternately', 0.044), ('taggings', 0.044), ('weh', 0.044), ('wtheh', 0.044), ('signal', 0.043), ('achieve', 0.042), ('optimize', 0.042), ('viterbi', 0.041), ('km', 0.041), ('optimizations', 0.041), ('tthioen', 0.041), ('derivatives', 0.041), ('ashish', 0.041), ('treebank', 0.041), ('size', 0.041), ('measured', 0.04), ('standard', 0.039), ('tags', 0.039), ('highest', 0.039), ('admiralty', 0.039), ('tract', 0.039), ('adler', 0.039), ('posteriori', 0.039), ('leads', 0.039), ('maximize', 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="96-tfidf-1" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>2 0.24311623 <a title="96-tfidf-2" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>3 0.15707259 <a title="96-tfidf-3" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>4 0.13865432 <a title="96-tfidf-4" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><p>5 0.12984757 <a title="96-tfidf-5" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>6 0.12409788 <a title="96-tfidf-6" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>7 0.10054084 <a title="96-tfidf-7" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>8 0.096574023 <a title="96-tfidf-8" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>9 0.084892549 <a title="96-tfidf-9" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>10 0.081080899 <a title="96-tfidf-10" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>11 0.080499373 <a title="96-tfidf-11" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>12 0.078637205 <a title="96-tfidf-12" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>13 0.069332615 <a title="96-tfidf-13" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>14 0.067579374 <a title="96-tfidf-14" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>15 0.067276306 <a title="96-tfidf-15" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>16 0.065565713 <a title="96-tfidf-16" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>17 0.063518345 <a title="96-tfidf-17" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>18 0.062929139 <a title="96-tfidf-18" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>19 0.062251698 <a title="96-tfidf-19" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>20 0.059448 <a title="96-tfidf-20" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.182), (1, -0.015), (2, 0.038), (3, -0.001), (4, -0.006), (5, -0.067), (6, 0.089), (7, -0.007), (8, 0.215), (9, 0.016), (10, -0.089), (11, 0.038), (12, 0.029), (13, -0.126), (14, -0.049), (15, -0.171), (16, -0.095), (17, -0.026), (18, 0.036), (19, -0.048), (20, 0.059), (21, 0.138), (22, -0.041), (23, 0.042), (24, -0.116), (25, 0.091), (26, 0.142), (27, -0.081), (28, 0.036), (29, 0.057), (30, 0.048), (31, -0.048), (32, -0.079), (33, -0.006), (34, 0.049), (35, -0.135), (36, 0.122), (37, 0.063), (38, 0.018), (39, 0.007), (40, 0.124), (41, -0.087), (42, 0.051), (43, 0.086), (44, 0.013), (45, -0.04), (46, -0.131), (47, -0.063), (48, 0.029), (49, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97290576 <a title="96-lsi-1" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>2 0.72372031 <a title="96-lsi-2" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><p>3 0.69743258 <a title="96-lsi-3" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>4 0.67085713 <a title="96-lsi-4" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>5 0.66883767 <a title="96-lsi-5" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>6 0.6293273 <a title="96-lsi-6" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>7 0.61375129 <a title="96-lsi-7" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>8 0.5958032 <a title="96-lsi-8" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>9 0.42503312 <a title="96-lsi-9" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>10 0.41375828 <a title="96-lsi-10" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>11 0.40065956 <a title="96-lsi-11" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>12 0.38935155 <a title="96-lsi-12" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>13 0.35417974 <a title="96-lsi-13" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>14 0.34424621 <a title="96-lsi-14" href="./acl-2010-Intelligent_Selection_of_Language_Model_Training_Data.html">151 acl-2010-Intelligent Selection of Language Model Training Data</a></p>
<p>15 0.336734 <a title="96-lsi-15" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>16 0.32932654 <a title="96-lsi-16" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>17 0.32887217 <a title="96-lsi-17" href="./acl-2010-A_Bayesian_Method_for_Robust_Estimation_of_Distributional_Similarities.html">3 acl-2010-A Bayesian Method for Robust Estimation of Distributional Similarities</a></p>
<p>18 0.32724628 <a title="96-lsi-18" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>19 0.31232461 <a title="96-lsi-19" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>20 0.31029692 <a title="96-lsi-20" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.277), (14, 0.031), (25, 0.052), (39, 0.012), (42, 0.012), (59, 0.154), (71, 0.018), (73, 0.057), (78, 0.019), (80, 0.013), (83, 0.115), (84, 0.036), (98, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85685778 <a title="96-lda-1" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>Author: Yorick Wilks ; Roberta Catizone ; Alexiei Dingli ; Weiwei Cheng</p><p>Abstract: This paper describes an initial prototype demonstrator of a Companion, designed as a platform for novel approaches to the following: 1) The use of Information Extraction (IE) techniques to extract the content of incoming dialogue utterances after an Automatic Speech Recognition (ASR) phase, 2) The conversion of the input to Resource Descriptor Format (RDF) to allow the generation of new facts from existing ones, under the control of a Dialogue Manger (DM), that also has access to stored knowledge and to open knowledge accessed in real time from the web, all in RDF form, 3) A DM implemented as a stack and network virtual machine that models mixed initiative in dialogue control, and 4) A tuned dialogue act detector based on corpus evidence. The prototype platform was evaluated, and we describe this briefly; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning.</p><p>same-paper 2 0.80604106 <a title="96-lda-2" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>3 0.66118526 <a title="96-lda-3" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>4 0.65295601 <a title="96-lda-4" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>5 0.64944881 <a title="96-lda-5" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>Author: Nathanael Chambers ; Dan Jurafsky</p><p>Abstract: This paper improves the use of pseudowords as an evaluation framework for selectional preferences. While pseudowords originally evaluated word sense disambiguation, they are now commonly used to evaluate selectional preferences. A selectional preference model ranks a set of possible arguments for a verb by their semantic fit to the verb. Pseudo-words serve as a proxy evaluation for these decisions. The evaluation takes an argument of a verb like drive (e.g. car), pairs it with an alternative word (e.g. car/rock), and asks a model to identify the original. This paper studies two main aspects of pseudoword creation that affect performance results. (1) Pseudo-word evaluations often evaluate only a subset of the words. We show that selectional preferences should instead be evaluated on the data in its entirety. (2) Different approaches to selecting partner words can produce overly optimistic evaluations. We offer suggestions to address these factors and present a simple baseline that outperforms the state-ofthe-art by 13% absolute on a newspaper domain.</p><p>6 0.64904046 <a title="96-lda-6" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>7 0.64634514 <a title="96-lda-7" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>8 0.64533085 <a title="96-lda-8" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>9 0.64500332 <a title="96-lda-9" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>10 0.64360726 <a title="96-lda-10" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>11 0.64132804 <a title="96-lda-11" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>12 0.64102674 <a title="96-lda-12" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>13 0.64096731 <a title="96-lda-13" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>14 0.63969105 <a title="96-lda-14" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>15 0.63894904 <a title="96-lda-15" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>16 0.63887441 <a title="96-lda-16" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>17 0.63807636 <a title="96-lda-17" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>18 0.63785994 <a title="96-lda-18" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>19 0.63725793 <a title="96-lda-19" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>20 0.63708282 <a title="96-lda-20" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
