<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-253" href="#">acl2010-253</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</h1>
<br/><p>Source: <a title="acl-2010-253-pdf" href="http://aclweb.org/anthology//P/P10/P10-1037.pdf">pdf</a></p><p>Author: Manabu Sassano ; Sadao Kurohashi</p><p>Abstract: We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.</p><p>Reference: <a title="acl-2010-253-reference" href="../acl2010_reference/acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing  Manabu Sassano Yahoo Japan Corporation Midtown Tower, 9-7-1 Akasaka, Minato-ku, Tokyo 107-621 1, Japan ms a s s ano @ yahoo-corp . [sent-1, score-0.077]
</p><p>2 j p Abstract We investigate active learning methods for Japanese dependency parsing. [sent-2, score-0.391]
</p><p>3 We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. [sent-3, score-0.492]
</p><p>4 Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. [sent-4, score-0.267]
</p><p>5 Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. [sent-5, score-0.111]
</p><p>6 1 Introduction Reducing annotation cost is very important because supervised learning approaches, which have been successful in natural language processing, require typically a large number of labeled examples. [sent-9, score-0.092]
</p><p>7 Preparing many labeled examples is time consuming and labor intensive. [sent-10, score-0.127]
</p><p>8 One of most promising approaches to this issue is active learning. [sent-11, score-0.28]
</p><p>9 Various tasks have been targeted in the research on active learning. [sent-13, score-0.28]
</p><p>10 It is the main purpose of this study to propose  methods of improving active learning for parsing by using a smaller constituent than a sentence as a unit that is selected at each iteration of active learning. [sent-23, score-0.786]
</p><p>11 Typically in active learning for parsing a Sadao Kurohashi Graduate School of Informatics, Kyoto University Yoshida-honmachi, Sakyo-ku, Kyoto 606-8501, Japan kuro @ i . [sent-24, score-0.407]
</p><p>12 jp sentence has been considered to be a basic unit for selection. [sent-27, score-0.058]
</p><p>13 Small constituents such as chunks have not been used in sample selection for parsing. [sent-28, score-0.237]
</p><p>14 We use Japanese dependency parsing as a target task in this study since a simple and efficient algorithm of parsing is proposed and, to our knowledge, active learning for Japanese dependency parsing has never been studied. [sent-29, score-0.819]
</p><p>15 Section 2 describes the basic framework of active learning which is employed in this research. [sent-31, score-0.333]
</p><p>16 Section 3 describes the syntactic characteristics of  Japanese and the parsing algorithm that we use. [sent-32, score-0.141]
</p><p>17 Section 4 briefly reviews previous work on active learning for parsing and discusses several research challenges. [sent-33, score-0.407]
</p><p>18 In Section 5 we describe our proposed methods and others of active learning for Japanese dependency parsing. [sent-34, score-0.391]
</p><p>19 1 Pool-based Active Learning Our base framework of active learning is based on the algorithm of (Lewis and Gale, 1994), which is called pool-based active learning. [sent-38, score-0.626]
</p><p>20 Following their sequential sampling algorithm, we show in Figure 1 the basic flow of pool-based active learning. [sent-39, score-0.379]
</p><p>21 Various methods for selecting informative examples can be combined with this framework. [sent-40, score-0.107]
</p><p>22 2  Selection Algorithm for Large Margin Classifiers  One of the most accurate approaches to classification tasks is an approach with large margin classifiers. [sent-42, score-0.153]
</p><p>23 t Shautp tphoes eas tshoacti awteed a rlaeb geliv yi w dail ta a be p oeinithtser { x−1} or 1, and we have a hyperplane ofw some large margin classifier defined by {x : f(x) = 0} where the 356  ProceedinUgspp osfa tlhae, 4S8wthed Aennn,u 1a1l-1 M6e Jeutilnyg 2 o0f1 t0h. [sent-45, score-0.3]
</p><p>24 Build an initial classifier from an initial labeled training set. [sent-48, score-0.166]
</p><p>25 An English translation is “Lisa gave that pen to him. [sent-53, score-0.023]
</p><p>26 In pool-based nact fiuvnec learning w(xit)h large margin }c. [sent-55, score-0.179]
</p><p>27 la Isnsifiers, selection of examples can be done as follows: 1. [sent-56, score-0.166]
</p><p>28 Compute f(xi) over all unlabeled examples xi in the pool. [sent-57, score-0.149]
</p><p>29 This type of selection methods with SVMs is discussed in (Tong and Koller, 2000; Schohn and Cohn, 2000). [sent-62, score-0.105]
</p><p>30 1 Syntactic Units A basic syntactic unit used in Japanese parsing is a bunsetsu, the concept of which was initially introduced by Hashimoto (1934). [sent-66, score-0.159]
</p><p>31 We assume that in Japanese we have a sequence of bunsetsus before parsing a sentence. [sent-67, score-0.39]
</p><p>32 A bunsetsu contains one or more content words and zero or more function words. [sent-68, score-0.591]
</p><p>33 A sample sentence in Japanese is shown in Figure 2. [sent-69, score-0.083]
</p><p>34 This sentence consists of five bunsetsus: Lisa-ga, kare-ni, ano, pen-wo, and age-ta where ga, ni, and wo are postpositions and ta is a verb ending for past tense. [sent-70, score-0.05]
</p><p>35 2  Constraints of Japanese Dependency Analysis Japanese is a head final language and in written Japanese we usually hypothesize the following: •  Each bunsetsu has only one head except the rightmost one. [sent-72, score-0.723]
</p><p>36 •  •  Dependency links between bunsetsus fDroepme lnedfet ntcoy right. [sent-73, score-0.289]
</p><p>37 go  We can see that these constraints are satisfied in the sample sentence in Figure 2. [sent-75, score-0.106]
</p><p>38 In this paper we also assume that the above constraints hold true when we discuss algorithms of Japanese parsing and active learning for it. [sent-76, score-0.43]
</p><p>39 3  Algorithm of Japanese Dependency Parsing We use Sassano’s algorithm (Sassano, 2004) for Japanese dependency parsing. [sent-78, score-0.125]
</p><p>40 His algorithm is one of the simplest form of shift-reduce parsers and runs in linear-time. [sent-81, score-0.04]
</p><p>41 2 Since Japanese is a head final language and its dependencies are projective as described in Section 3. [sent-82, score-0.066]
</p><p>42 The basic flow of Sassano’s algorithm is shown in Figure 3, which is slightly simplified from the original by Sassano (2004). [sent-84, score-0.138]
</p><p>43 When we use this al-  gorithm with a machine learning-based classifier, function Dep() in Figure 3 uses the classifier to decide whether two bunsetsus have a dependency relation. [sent-85, score-0.474]
</p><p>44 In order to prepare training examples for the trainable classifier used with his algorithm, we first have to convert a treebank to suitable labeled instances by using the algorithm in Figure 4. [sent-86, score-0.295]
</p><p>45 (2008) compare their proposed algorithm with various ones that include Sassano’s, cascaded chunking (Kudo and Matsumoto, 2002), and one in (McDonald et al. [sent-88, score-0.108]
</p><p>46 Kudo and Matsumoto (2002) compare cascaded chunking with the CYK method (Kudo and Matsumoto, 2000). [sent-90, score-0.068]
</p><p>47 2Roughly speaking, Sassano’s is considered to be a simplified version, which is modified for head final languages, of Nivre’s (Nivre, 2003). [sent-92, score-0.089]
</p><p>48 Classifiers with Nivre’s are required to handle multiclass prediction, while binary classifiers can work with Sassano’s for Japanese. [sent-93, score-0.037]
</p><p>49 Dep(j, i, w): returns true when wj should modify wi. [sent-99, score-0.067]
</p><p>50 3  4  Active Learning for Parsing  Most of the methods of active learning for parsing in previous work use selection of sentences that seem to contribute to the improvement of accuracy (Tang et al. [sent-102, score-0.557]
</p><p>51 Although Hwa suggests that sample selection for parsing would be improved by selecting finer grained constituents rather than sentences (Hwa, 2004), such methods have not been investigated so far. [sent-104, score-0.405]
</p><p>52 Typical methods of selecting sentences are 3We show a sample set of generated examples for training the classifier of the parser in Figure 3. [sent-105, score-0.311]
</p><p>53 By using the algorithm in Figure 4, we can obtain labeled examples from the sample sentences in Figure 2: {0, 1, “O”}, {1, 2, “O”}, {2, 3, “D”}, asnendt {n1c,e 3s, i “nO F”ig}u. [sent-106, score-0.295]
</p><p>54 xample, an actual labeled instance generated from {2, 3, “D”} will be like ”label=D, features={modifierfcroonmte {nt2-w,3o,rd“D=a”n}o w, . [sent-110, score-0.066]
</p><p>55 Function: Dep(j, i, w, h): returns true if hj = i. [sent-118, score-0.111]
</p><p>56 Also prints a feature vector with a label according to hj. [sent-120, score-0.058]
</p><p>57 We cannot use this kind of measures when we want to select other smaller constituents than sentences. [sent-125, score-0.176]
</p><p>58 Other bigger problem is an algorithm of parsing itself. [sent-126, score-0.141]
</p><p>59 If we sample smaller units rather than sentences, we have partially annotated sentences and have to use a parsing algorithm that can be trained from incompletely annotated sentences. [sent-127, score-0.337]
</p><p>60 4 5  Active Learning for Japanese Dependency Parsing  In this section we describe sample selection methods which we investigated. [sent-129, score-0.188]
</p><p>61 1 Sentence-wise Sample Selection Passive Selection (Passive) This method is to select sequentially sentences that appear in the training corpus. [sent-131, score-0.104]
</p><p>62 Since it gets harder for the read-  ers to reproduce the same experimental setting, we 4We did not employ query-by-committee (QBC) (Seung et al. [sent-132, score-0.024]
</p><p>63 , 1992), which is another important general framework of active learning, since the selection strategy with large margin classifiers (Section 2. [sent-133, score-0.575]
</p><p>64 2) is much simpler and seems more practical for active learning in Japanese dependency parsing with smaller constituents. [sent-134, score-0.56]
</p><p>65 Minimum Margin Selection (Min) This method is to select sentences that contain bunsetsu pairs which have smaller margin values of outputs of the classifier used in parsing. [sent-136, score-1.1]
</p><p>66 The procedure of selection of MIN are summarized as follows. [sent-137, score-0.105]
</p><p>67 Assume that we have sentences si in the pool of unlabeled sentences. [sent-138, score-0.226]
</p><p>68 Sort si with min |f(xk) | where xk are bunsetsu pairs hin m tihne fse(nxten)|ce w si. [sent-142, score-0.885]
</p><p>69 eN xote that xk are not all possible bunsetsu pairs in si and they are limited to bunsetsu pairs checked in the process of parsing si. [sent-143, score-1.506]
</p><p>70 Averaged Margin Selection (Avg) This method is to select sentences that have smaller values of averaged margin values of outputs of the classifier in a give sentences over the number of decisions which are carried out in parsing. [sent-146, score-0.541]
</p><p>71 The difference between AVG and MIN is that for AVG we use ∑ |f(xk) |/l where l is the number of calling Dep∑() |inf Figure 3w hfoerre th le i s tehnete nnucme si in osft ceaaldli nogf min |f(xk) | for MIN. [sent-147, score-0.145]
</p><p>72 2 Chunk-wise Sample Selection In chunk-wise sample selection, we select bunsetsu pairs rather than sentences. [sent-149, score-0.769]
</p><p>73 Bunsetsu pairs are selected from different sentences in a pool. [sent-150, score-0.081]
</p><p>74 This means that structures of sentences in the pool are partially annotated. [sent-151, score-0.132]
</p><p>75 Note that we do not use every bunsetsu pair in a sentence. [sent-152, score-0.591]
</p><p>76 When we use Sassano’s algorithm, we have to generate training examples for the classifier by using the algorithm in Figure 4. [sent-153, score-0.234]
</p><p>77 In other words, we should not sample bunsetsu pairs inde-  pendently from a given sentence. [sent-154, score-0.71]
</p><p>78 Therefore, we select bunsetsu pairs that have smaller margin values of outputs given by the classifier during the parsing process. [sent-155, score-1.156]
</p><p>79 All the sentences in the pool are processed by the current parser. [sent-156, score-0.132]
</p><p>80 We cannot simply split the sentences in the pool into labeled and unlabeled ones because we do not select every bunsetsu pair in a given sentence. [sent-157, score-0.885]
</p><p>81 Naive Selection (Naive) This method is to select bunsetsu pairs that have smaller margin values of outputs of the classifier. [sent-158, score-0.955]
</p><p>82 Then it is assumed that annotators would label either “D” for the two bunsetsu having a dependency relation or “O”, which represents the two does not. [sent-159, score-0.757]
</p><p>83 MODSIMPLE is to select bunsetsu pairs that have smaller margin values of outputs of the classifier, which is the same as in NAIVE. [sent-161, score-0.955]
</p><p>84 The difference between MODSIMPLE and NAIVE is the way annotators label examples. [sent-162, score-0.081]
</p><p>85 Assume that we have an annotator and the learner selects some  bunsetsu pair of the j-th bunsetsu and the i-th bunsetsu such that j < i. [sent-163, score-1.797]
</p><p>86 The annotator is then asked what the head of the j-th bunsetsu is. [sent-164, score-0.681]
</p><p>87 We define here the head bunsetsu is the k-th one. [sent-165, score-0.657]
</p><p>88 We differently generate labeled examples from the information annotators give according to the relation among bunsetsus j,i, and k. [sent-166, score-0.5]
</p><p>89 Below we use the notation {s, t, “D”} to denote tlohawt t whee s u-sthe tb huens neotstuat imonod {ifsi,est, t“hDe” }t-th to one. [sent-167, score-0.028]
</p><p>90 The use of “O” instead of “D” indicates that the s-th does not modify the t-th. [sent-168, score-0.025]
</p><p>91 That is generating {s, t, “D”} means outputting an example with the l{asb,etl, ““DD””. [sent-169, score-0.023]
</p><p>92 } Case 1 if j < i< k, then generate {j, i, “O”} and {j, k, “D”}. [sent-170, score-0.033]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bunsetsu', 0.591), ('bunsetsus', 0.289), ('sassano', 0.284), ('active', 0.28), ('japanese', 0.272), ('margin', 0.153), ('push', 0.149), ('dep', 0.145), ('selection', 0.105), ('parsing', 0.101), ('classifier', 0.1), ('modsimple', 0.096), ('xk', 0.094), ('pop', 0.094), ('pool', 0.087), ('dependency', 0.085), ('sample', 0.083), ('hwa', 0.081), ('ano', 0.077), ('avg', 0.069), ('hj', 0.069), ('smaller', 0.068), ('head', 0.066), ('labeled', 0.066), ('begin', 0.065), ('endj', 0.064), ('hile', 0.064), ('tang', 0.063), ('examples', 0.061), ('min', 0.06), ('select', 0.059), ('passive', 0.058), ('kudo', 0.058), ('si', 0.057), ('naive', 0.056), ('jw', 0.056), ('stack', 0.053), ('matsumoto', 0.053), ('annotators', 0.051), ('xi', 0.051), ('constituents', 0.049), ('ids', 0.048), ('flow', 0.048), ('outputs', 0.048), ('sentences', 0.045), ('nivre', 0.045), ('returns', 0.042), ('algorithm', 0.04), ('japan', 0.039), ('kyoto', 0.038), ('cascaded', 0.038), ('unlabeled', 0.037), ('classifiers', 0.037), ('pairs', 0.036), ('generate', 0.033), ('unit', 0.031), ('label', 0.03), ('chunking', 0.03), ('modifier', 0.028), ('hashimoto', 0.028), ('ringger', 0.028), ('prepare', 0.028), ('schohn', 0.028), ('nogf', 0.028), ('whee', 0.028), ('preparing', 0.028), ('prints', 0.028), ('fse', 0.028), ('oor', 0.028), ('adti', 0.028), ('sort', 0.028), ('basic', 0.027), ('learning', 0.026), ('postpositions', 0.026), ('pops', 0.026), ('tower', 0.026), ('ples', 0.026), ('manabu', 0.026), ('kurohashi', 0.026), ('modify', 0.025), ('annotator', 0.024), ('informative', 0.024), ('sadao', 0.024), ('hin', 0.024), ('var', 0.024), ('seung', 0.024), ('ers', 0.024), ('ta', 0.024), ('sampling', 0.024), ('constraints', 0.023), ('averaged', 0.023), ('simplified', 0.023), ('tihne', 0.023), ('eas', 0.023), ('pen', 0.023), ('outputting', 0.023), ('laws', 0.023), ('hde', 0.023), ('selecting', 0.022), ('ean', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="253-tfidf-1" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>Author: Manabu Sassano ; Sadao Kurohashi</p><p>Abstract: We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.</p><p>2 0.14977269 <a title="253-tfidf-2" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>3 0.1428615 <a title="253-tfidf-3" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>Author: Hirotoshi Taira ; Sanae Fujita ; Masaaki Nagata</p><p>Abstract: Maintaining high annotation consistency in large corpora is crucial for statistical learning; however, such work is hard, especially for tasks containing semantic elements. This paper describes predicate argument structure analysis using transformation-based learning. An advantage of transformation-based learning is the readability of learned rules. A disadvantage is that the rule extraction procedure is time-consuming. We present incremental-based, transformation-based learning for semantic processing tasks. As an example, we deal with Japanese predicate argument analysis and show some tendencies of annotators for constructing a corpus with our method.</p><p>4 0.12424743 <a title="253-tfidf-4" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>Author: Kotaro Kitagawa ; Kumiko Tanaka-Ishii</p><p>Abstract: Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average pars- ing time not much slower than O(n).</p><p>5 0.098812424 <a title="253-tfidf-5" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>Author: Wenbin Jiang ; Qun Liu</p><p>Abstract: In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this clas- , sifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.</p><p>6 0.093432315 <a title="253-tfidf-6" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>7 0.093271457 <a title="253-tfidf-7" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>8 0.082433894 <a title="253-tfidf-8" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>9 0.075290971 <a title="253-tfidf-9" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>10 0.073746875 <a title="253-tfidf-10" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>11 0.07324931 <a title="253-tfidf-11" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>12 0.062871978 <a title="253-tfidf-12" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>13 0.062737286 <a title="253-tfidf-13" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>14 0.062638052 <a title="253-tfidf-14" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>15 0.051204372 <a title="253-tfidf-15" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>16 0.049551763 <a title="253-tfidf-16" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>17 0.048497636 <a title="253-tfidf-17" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>18 0.048310906 <a title="253-tfidf-18" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>19 0.048015784 <a title="253-tfidf-19" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>20 0.047628667 <a title="253-tfidf-20" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.144), (1, -0.009), (2, 0.037), (3, 0.039), (4, -0.024), (5, -0.041), (6, -0.008), (7, -0.006), (8, -0.044), (9, 0.136), (10, -0.139), (11, 0.028), (12, -0.065), (13, 0.058), (14, 0.057), (15, -0.018), (16, 0.03), (17, -0.03), (18, -0.043), (19, 0.017), (20, 0.012), (21, 0.033), (22, 0.045), (23, -0.097), (24, -0.014), (25, -0.066), (26, 0.139), (27, 0.076), (28, 0.075), (29, 0.053), (30, -0.012), (31, 0.009), (32, -0.054), (33, -0.053), (34, -0.092), (35, 0.085), (36, 0.026), (37, -0.007), (38, 0.078), (39, 0.041), (40, 0.048), (41, -0.095), (42, -0.075), (43, -0.09), (44, -0.006), (45, 0.055), (46, 0.007), (47, 0.077), (48, 0.062), (49, -0.115)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94029588 <a title="253-lsi-1" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>Author: Manabu Sassano ; Sadao Kurohashi</p><p>Abstract: We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.</p><p>2 0.63259125 <a title="253-lsi-2" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>Author: Martin Haulrich</p><p>Abstract: We show that using confidence-weighted classification in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. We also compare with other online learning algorithms and investigate the effect of pruning features when using confidenceweighted classification.</p><p>3 0.59265256 <a title="253-lsi-3" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>Author: Kotaro Kitagawa ; Kumiko Tanaka-Ishii</p><p>Abstract: Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average pars- ing time not much slower than O(n).</p><p>4 0.52106977 <a title="253-lsi-4" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>Author: Wenbin Jiang ; Qun Liu</p><p>Abstract: In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this clas- , sifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.</p><p>5 0.51520789 <a title="253-lsi-5" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>Author: Liang Huang ; Kenji Sagae</p><p>Abstract: Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce depen- dency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.</p><p>6 0.49514255 <a title="253-lsi-6" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>7 0.47687244 <a title="253-lsi-7" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>8 0.44781515 <a title="253-lsi-8" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>9 0.44674596 <a title="253-lsi-9" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>10 0.44293591 <a title="253-lsi-10" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>11 0.43563175 <a title="253-lsi-11" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>12 0.42379484 <a title="253-lsi-12" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>13 0.41541323 <a title="253-lsi-13" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>14 0.40801913 <a title="253-lsi-14" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>15 0.40287775 <a title="253-lsi-15" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>16 0.39365301 <a title="253-lsi-16" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>17 0.39302018 <a title="253-lsi-17" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>18 0.37304175 <a title="253-lsi-18" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>19 0.35334525 <a title="253-lsi-19" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>20 0.35063696 <a title="253-lsi-20" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.013), (14, 0.019), (25, 0.021), (39, 0.038), (42, 0.015), (49, 0.249), (59, 0.079), (73, 0.047), (78, 0.025), (80, 0.017), (83, 0.062), (84, 0.034), (98, 0.28)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85799956 <a title="253-lda-1" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>Author: Manabu Sassano ; Sadao Kurohashi</p><p>Abstract: We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.</p><p>2 0.83506697 <a title="253-lda-2" href="./acl-2010-Cross_Lingual_Adaptation%3A_An_Experiment_on_Sentiment_Classifications.html">80 acl-2010-Cross Lingual Adaptation: An Experiment on Sentiment Classifications</a></p>
<p>Author: Bin Wei ; Christopher Pal</p><p>Abstract: In this paper, we study the problem of using an annotated corpus in English for the same natural language processing task in another language. While various machine translation systems are available, automated translation is still far from perfect. To minimize the noise introduced by translations, we propose to use only key ‘reliable” parts from the translations and apply structural correspondence learning (SCL) to find a low dimensional representation shared by the two languages. We perform experiments on an EnglishChinese sentiment classification task and compare our results with a previous cotraining approach. To alleviate the problem of data sparseness, we create extra pseudo-examples for SCL by making queries to a search engine. Experiments on real-world on-line review data demonstrate the two techniques can effectively improvetheperformancecomparedtoprevious work.</p><p>3 0.80462462 <a title="253-lda-3" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>Author: Duo Zhang ; Qiaozhu Mei ; ChengXiang Zhai</p><p>Abstract: Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way. One common deficiency of existing topic models, though, is that they would not work well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other. In this paper, we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages. Specifically, we propose a new topic model called Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA) which extends the Proba- bilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary. Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data.</p><p>4 0.7611838 <a title="253-lda-4" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>5 0.76057535 <a title="253-lda-5" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>6 0.7592501 <a title="253-lda-6" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>7 0.75847119 <a title="253-lda-7" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>8 0.75809801 <a title="253-lda-8" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>9 0.75484443 <a title="253-lda-9" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>10 0.75464928 <a title="253-lda-10" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>11 0.74921906 <a title="253-lda-11" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>12 0.74828994 <a title="253-lda-12" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>13 0.74802107 <a title="253-lda-13" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>14 0.73997593 <a title="253-lda-14" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>15 0.73856533 <a title="253-lda-15" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>16 0.73590684 <a title="253-lda-16" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>17 0.7252863 <a title="253-lda-17" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>18 0.72385412 <a title="253-lda-18" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>19 0.72304749 <a title="253-lda-19" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>20 0.72231889 <a title="253-lda-20" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
