<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-12" href="#">acl2010-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</h1>
<br/><p>Source: <a title="acl-2010-12-pdf" href="http://aclweb.org/anthology//P/P10/P10-3004.pdf">pdf</a></p><p>Author: Federico Sangati</p><p>Abstract: We present a probabilistic model extension to the Tesni `ere Dependency Structure (TDS) framework formulated in (Sangati and Mazza, 2009). This representation incorporates aspects from both constituency and dependency theory. In addition, it makes use of junction structures to handle coordination constructions. We test our model on parsing the English Penn WSJ treebank using a re-ranking framework. This technique allows us to efficiently test our model without needing a specialized parser, and to use the standard evaluation metric on the original Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking.</p><p>Reference: <a title="acl-2010-12-reference" href="../acl2010_reference/acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A probabilistic generative model for an intermediate constituency-dependency representation Federico Sangati Institute for Logic, Language and Computation University of Amsterdam, the Netherlands f . [sent-1, score-0.156]
</p><p>2 nl @  Abstract We present a probabilistic model extension to the Tesni `ere Dependency Structure (TDS) framework formulated in (Sangati and Mazza, 2009). [sent-3, score-0.065]
</p><p>3 This representation incorporates aspects from both constituency and dependency theory. [sent-4, score-0.163]
</p><p>4 In addition, it makes use of junction structures to handle coordination constructions. [sent-5, score-0.479]
</p><p>5 We test our model on parsing the English Penn WSJ treebank using a re-ranking framework. [sent-6, score-0.09]
</p><p>6 This technique allows us to efficiently test our model without needing a specialized parser, and to use the standard evaluation metric on the original Phrase Structure version of the treebank. [sent-7, score-0.149]
</p><p>7 We obtain encouraging results: we achieve a small improvement over state-of-the-art results  when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking. [sent-8, score-0.084]
</p><p>8 1 Introduction Since its origin, computational linguistics has been dominated by Constituency/Phrase Structure (PS) representation of sentence structure. [sent-9, score-0.057]
</p><p>9 Dependency theory is historically accredited to Lucien Tesni `ere (1959), although the relation of dependency between words was only one of the various key elements proposed to represent sentence structures. [sent-12, score-0.115]
</p><p>10 In fact, the original formulation incorporates the notion of chunk, as well as a special type of structure to represent coordination. [sent-13, score-0.124]
</p><p>11 The Tesni `ere Dependency Structure (TDS) rep-  resentation we propose in (Sangati and Mazza, 2009), is an attempt to formalize the original work of Tesni `ere, with the intention to develop a simple but consistent representation which combines constituencies and dependencies. [sent-14, score-0.108]
</p><p>12 As part ofthis work, we have implemented an automatic conversion1 of the English Penn Wall Street Journal (WSJ) treebank into the new annotation scheme. [sent-15, score-0.089]
</p><p>13 In the current work, after introducing the key elements of TDS (section 2), we describe a first probabilistic extension to this framework, which aims at modeling the different levels of the representation (section 3). [sent-16, score-0.169]
</p><p>14 We test our model on parsing the WSJ treebank using a re-ranking framework. [sent-17, score-0.09]
</p><p>15 This technique allows us to efficiently test our system without needing a specialized parser, and to use the standard evaluation metric on the original PS version of the treebank. [sent-18, score-0.118]
</p><p>16 4 we also introduce new evaluation schemes on specific aspects ofthe new TDS representation which we will include in the results presented in section 3. [sent-20, score-0.057]
</p><p>17 2  TDS representation  It is beyond the scope of this paper to provide an exhaustive description of the TDS representation of the WSJ. [sent-22, score-0.114]
</p><p>18 Figure 1 shows the original PS of a WSJ tree (a), together with 3 other representations: (b) TDS, (c) and (d) CCG (Hockenmaier and Steedman, 2007). [sent-24, score-0.051]
</p><p>19 nl/ ˜fsangati/TDS 2The DS representation is taken from the conversion procedure used in the CoNLL 2007 Shared Task on dependency parsing (Nivre et al. [sent-28, score-0.194]
</p><p>20 Although more elaborate representation have been proposed (de Marneffe and Manning, 2008; Cinkov´ a et al. [sent-30, score-0.057]
</p><p>21 , 2009) we have chosen this DS representation because it is one of the most commonly used within the CL community, given that it relies on a fully automatic conversion procedure. [sent-31, score-0.092]
</p><p>22 Words and Blocks In TDS, words are divided in functional words (determiners, prepositions, etc. [sent-36, score-0.132]
</p><p>23 Blocks are the basic elements (chunks) of a structure, which can be combined either via the dependency relation or the junction operation. [sent-39, score-0.468]
</p><p>24 Blocks can be of two types: standard and junction blocks. [sent-40, score-0.353]
</p><p>25 Both types may contain any sequence of functional words. [sent-41, score-0.132]
</p><p>26 Standard blocks (depicted as black boxes) represent the elementary chunks of the original PS, and include exactly one content word. [sent-42, score-0.26]
</p><p>27 Coordination Junction blocks (depicted as yellow boxes) are used to represent coordinated structures. [sent-43, score-0.228]
</p><p>28 They contain two or more blocks (con-  juncts) possibly coordinated by means of functional words (conjunctions). [sent-44, score-0.301]
</p><p>29 In Figure 1(d) the yellow junction block contains three separate standard blocks. [sent-45, score-0.812]
</p><p>30 This representation allows to capture the fact that these conjuncts occupy the same role: they all share the relativizer ‘that’, they all depend on the noun ‘activities’, and they all govern the noun ‘abortion’ . [sent-46, score-0.145]
</p><p>31 In Figure 1(a,c), we can notice that both PS and DS do not adequately represent coordination structures: the PS annotation is rather flat, avoiding to group the three verbs in a unique unit, while in the DS the last noun ‘abortion’ is at the same level of the verbs it should be a dependent of. [sent-47, score-0.245]
</p><p>32 On the other hand, the CCG structure of Figure 1(d), properly represents the coordination. [sent-48, score-0.04]
</p><p>33 It does so by grouping the first three verbs in a unique constituent which is in turn binarized in a right-branching structure. [sent-49, score-0.034]
</p><p>34 One of the strongest advantages of the CCG formalism, is that every structure can be automatically mapped to a logical-form representation. [sent-50, score-0.04]
</p><p>35 This is one reason why it needs to handle coordinations properly. [sent-51, score-0.037]
</p><p>36 Nevertheless, we conjecture that this representa-  tion of coordination might introduce some difficulties for parsing: it is very hard to capture the relation between ‘advocate’ and ‘abortion’ since they are several levels away in the structure. [sent-52, score-0.113]
</p><p>37 Categories and Transference There are 4 different block categories, which are indicated with little colored bricks (as well as one-letter abbreviation) on top and at the bottom of the corresponding blocks: verbs (red, V), nouns (blue, N), adverbs (yellow, A), and adjectives (green, J). [sent-53, score-0.434]
</p><p>38 Every block displays at the bottom the original category determined by the content word (or the original category of the conjuncts if it is a junction structure), and at the top, the derived category which relates to the grammatical role of the whole block in relation to the governing block. [sent-54, score-1.66]
</p><p>39 In several cases we can observe a shift in the categories of a block, from the original to the derived category. [sent-55, score-0.13]
</p><p>40 This phenomenon is called transference and often occurs by means of functional words in the block. [sent-56, score-0.205]
</p><p>41 In Figure 1(b) we can observe the transference of the junction block, which has the original category of a verb, but takes the role of an adjective (through  the relativizer ‘that’) in modifying the noun ‘activities’ . [sent-57, score-0.575]
</p><p>42 3  A probabilistic Model for TDS  This section describes the probabilistic generative model which was implemented in order to disambiguate TDS structures. [sent-59, score-0.162]
</p><p>43 The idea consists of utilizing a state of the art parser to compute a list of k-best candidates of a test sentence, and evaluate the new model by using it as a reranker. [sent-62, score-0.168]
</p><p>44 How well does it select the most probable structure among the given candidates? [sent-63, score-0.04]
</p><p>45 Since no parser currently exists for the TDS representation, we utilize a state of the art parser for PS trees (Charniak, 1999), and transform each  candidate to TDS. [sent-64, score-0.098]
</p><p>46 1 Model description In order to compute the probability of a given TDS structure, we make use of three separate probabilistic generative models, each responsible for a specific aspect of the structure being generated. [sent-67, score-0.133]
</p><p>47 The probability of a TDS structure is obtained by multiplying its probabilities in the three models, as reported in the first equation of Table 2. [sent-68, score-0.068]
</p><p>48 The first model (equation 2) is the Block Generation Model (BGM). [sent-69, score-0.031]
</p><p>49 It describes the event of generating a block B as a dependent of its parent block (governor). [sent-70, score-0.961]
</p><p>50 The dependent block B is identified with its categories (both original and derived), and its functional words, while the parent block is characterized by the original category only. [sent-71, score-1.244]
</p><p>51 Moreover, in the conditioning context we specify the direction of the dependent with respect to the parent3, and its adjacent left sister (null if not present) specified with the same level of details of B. [sent-72, score-0.103]
</p><p>52 The second model (equation 3) is the Block Expansion Model (BEM). [sent-74, score-0.031]
</p><p>53 It computes the probability of a generic block B of known derived category, to expand to the list of elements it is composed of. [sent-75, score-0.518]
</p><p>54 The list includes the category of the content word, in case the expansion leads to a standard block. [sent-76, score-0.154]
</p><p>55 In case of a junction structure, it contains the conjunctions and the conjunct blocks (each identified with its categories and its functional words) in the order they appear. [sent-77, score-0.731]
</p><p>56 Moreover, all functional words in the block are added to the list5. [sent-78, score-0.532]
</p><p>57 The third model (equation 4) is the Word Filling Model (WFM), which applies to each standard block B of the structure. [sent-80, score-0.459]
</p><p>58 It models the event of filling B with a content word (cw), given the content word of the governing block, the categories (cats) and functional words (fw) of B, and further information about the context6 in which B occurs. [sent-81, score-0.417]
</p><p>59 This model becomes particularly interest3A dependent block can have three different positions with respect to the parent block: left, right, inner. [sent-82, score-0.559]
</p><p>60 The inner case occurs when the dependent block starts after the beginning of the parent block  but ends before it (e. [sent-84, score-0.928]
</p><p>61 4A block is a dependent block if it is not a conjunct. [sent-87, score-0.873]
</p><p>62 5The attentive reader might notice that the functional words are generated twice (in BGM and BEM). [sent-89, score-0.159]
</p><p>63 This decision, although not fully justified from a statistical viewpoint, seems to drive the model towards a better disambiguation. [sent-90, score-0.031]
</p><p>64 6context(B) comprises information about the grandparent block (original category), the adjacent left sibling block (derived category), the direction of the content word with respect to its governor (in this case only left and right), and the absolute distance between the two words. [sent-91, score-0.916]
</p><p>65 21  ing when a standard block is a dependent of ajunction block (such as ‘abortion’  in Figure 1(d)). [sent-92, score-0.873]
</p><p>66 In  this case, the model needs to capture the dependency relation between the content word of the dependent block and each of the content words belonging to the junction 3. [sent-93, score-1.022]
</p><p>67 The different back-off estimates, which are listed in decreasing levels of details, are interpolated with confidence  weights8 derived from the training corpus. [sent-96, score-0.082]
</p><p>68 The first two models are implemented with two levels of back-off, in which the last is a constant value (10−6) to make the overall probability small but not zero, for unknown events. [sent-97, score-0.065]
</p><p>69 The third model is implemented with three levels of back-off: the last is set to the same constant value (10−6), the first encodes the dependency event using both pos-tags and lexical information of the governor and the dependent word, while the second specifies only pos-tags. [sent-98, score-0.345]
</p><p>70 3 Experiment Setup We have tested our model on the WSJ section of Penn Treebank (Marcus et al. [sent-100, score-0.031]
</p><p>71 We employ the Max-Ent parser, implemented by Charniak (1999), to generate a list of k-best PS candidates for the test sentences, which are then converted into TDS representation. [sent-102, score-0.117]
</p><p>72 Instead of using Charniak’s parser in its original settings, we train it on a version of the corpus in which we add a special suffix to constituents which have circumstantial role9. [sent-103, score-0.1]
</p><p>73 This decision is based on the observation that the TDS formalism  µ  well captures the argument structure of verbs, and 7In order to derive the probability of this multi-event we compute the average between the probabilities of the single events which compose it. [sent-104, score-0.072]
</p><p>74 It was surprising to notice that the performance ofthis slightly modified parser (in terms of F-score) is only slightly lower than how it performs out-of-the-box (0. [sent-112, score-0.106]
</p><p>75 We then applied our probabilistic  model to re-  rank the list of available k-best TDS, and evaluate the selected candidates  using several metrics  which will be introduced next. [sent-115, score-0.237]
</p><p>76 4  Evaluation Metrics for TDS  The re-ranking framework described above, allows us to keep track of the original PS of each TDS candidate. [sent-117, score-0.051]
</p><p>77 it allows us to evaluate the re-ranked structures both in terms of the standard evaluation benchmark on the original PS (F-score) as well as on more refined metrics derived from the converted TDS representation. [sent-119, score-0.255]
</p><p>78 In addition, the specific head assignment that the TDS conversion procedure performs on the original PS, allows us to convert every PS candidate to a standard projective DS, and from this representation we can in turn compute the standard benchmark evaluation for DS, i. [sent-120, score-0.168]
</p><p>79 Block Attachment  Score (BAS): the accuracy  of detecting the correct governing block of each block in the  structure12. [sent-125, score-0.953]
</p><p>80 Junction Detection Score (JDS): the accuracy of detecting the correct list of content-words posing each junction block in the  com-  structure13. [sent-126, score-0.809]
</p><p>81 10UAS measures the percentage of words (excluding punctuation) having the correct governing word. [sent-127, score-0.127]
</p><p>82 11It is calculated as the harmonic mean between recall and precision between the test and gold set of blocks, where each block is identified with two numerical values representing the start and the end position (punctuation words are discarded). [sent-128, score-0.426]
</p><p>83 12It is computed as the percentage of words (both functional and content words, excluding punctuation) having the correct governing block. [sent-129, score-0.305]
</p><p>84 The governing block of a word, is defined as the governor of the block it belongs to. [sent-130, score-0.997]
</p><p>85 If the block is a conjunct, its governing block is computed recursively as the governing block of the junction block it belongs to. [sent-131, score-2.207]
</p><p>86 13It is calculated as the harmonic mean between recall and precision between the test and gold set of junction blocks expansions, where each expansion is identified with the list of content words belonging to the junction block. [sent-132, score-0.975]
</p><p>87 A recursive junction structure expands to a list of lists of content-words. [sent-133, score-0.423]
</p><p>88 Figure 2: Left: results of the TDS-reranking model according to several evaluation metrics as in Table 2. [sent-135, score-0.115]
</p><p>89 5  Results  Table 2 reports the results we obtain when reranking with our model an increasing number of k-best candidates provided by Charniak’s parser (the same results are shown in the left graph of Figure 2). [sent-138, score-0.138]
</p><p>90 We also report the results relative to a PCFG-reranker obtained by computing the probability of the k-best candidates using a standard vanilla-PCFG model derived from the same train-  ing corpus. [sent-139, score-0.135]
</p><p>91 Moreover, we evaluate, by means of an oracle, the upper and lower bound of the F-Score and JDS metric, by selecting the structures which maximizes/minimizes the results. [sent-140, score-0.049]
</p><p>92 Our re-ranking model performs rather well for a limited number of candidate structures, and outperforms Charniak’s model when k = 5. [sent-141, score-0.062]
</p><p>93 In this case we observe a small boost in performance for the detection of junction structures, as well as for all other evaluation metrics, except for the BDS. [sent-142, score-0.386]
</p><p>94 Concerning the other metrics, as the number of k-best candidates increases, the PCFG model outperforms the TDS-reranker both according to the BDS and the JDS. [sent-145, score-0.089]
</p><p>95 We find that this is primarily due to the lack of robustness of the model in detecting the block boundaries. [sent-147, score-0.457]
</p><p>96 In addition the same module could detect local (intra-clausal) coordinations, as illustrated by (Marinˇ ci cˇ et al. [sent-150, score-0.086]
</p><p>97 23  4  Conclusions  In this paper, we have presented a probabilistic generative model for parsing TDS syntactic representation of English sentences. [sent-152, score-0.185]
</p><p>98 We have therefore proposed a special evaluation metrics for junction detection, with the hope that other researchers might benefit from it in the future. [sent-154, score-0.437]
</p><p>99 Remarkably, Charniak’s parser per-  forms extremely well in all the evaluation metrics besides the one related to coordination. [sent-155, score-0.133]
</p><p>100 Our parsing results are encouraging: the overall system, although only when the candidates are highly reliable, can improve on Charniak’s parser on all the evaluation metrics with the exception of chunking score (BDS). [sent-156, score-0.22]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tds', 0.537), ('block', 0.4), ('junction', 0.353), ('ps', 0.189), ('sangati', 0.15), ('tesni', 0.147), ('blocks', 0.138), ('functional', 0.132), ('governing', 0.127), ('ds', 0.116), ('ci', 0.086), ('abortion', 0.086), ('metrics', 0.084), ('coordination', 0.077), ('bds', 0.073), ('bem', 0.073), ('bgm', 0.073), ('mazza', 0.073), ('transference', 0.073), ('dependent', 0.073), ('dependency', 0.073), ('governor', 0.07), ('ccg', 0.068), ('ere', 0.064), ('wsj', 0.063), ('charniak', 0.061), ('yellow', 0.059), ('candidates', 0.058), ('representation', 0.057), ('parent', 0.055), ('original', 0.051), ('conjunct', 0.049), ('jds', 0.049), ('lucien', 0.049), ('marin', 0.049), ('pbem', 0.049), ('pbgm', 0.049), ('pwfm', 0.049), ('relativizer', 0.049), ('wfm', 0.049), ('category', 0.049), ('structures', 0.049), ('parser', 0.049), ('content', 0.046), ('cw', 0.046), ('derived', 0.046), ('nivre', 0.045), ('cats', 0.043), ('cinkov', 0.043), ('krist', 0.043), ('needing', 0.043), ('yna', 0.043), ('elements', 0.042), ('structure', 0.04), ('conjuncts', 0.039), ('rens', 0.039), ('coordinations', 0.037), ('sang', 0.037), ('levels', 0.036), ('uas', 0.035), ('conversion', 0.035), ('probabilistic', 0.034), ('penn', 0.034), ('generative', 0.034), ('verbs', 0.034), ('categories', 0.033), ('event', 0.033), ('incorporates', 0.033), ('detection', 0.033), ('formalism', 0.032), ('boxes', 0.032), ('federico', 0.031), ('model', 0.031), ('coordinated', 0.031), ('netherlands', 0.031), ('treebank', 0.03), ('list', 0.03), ('conditioning', 0.03), ('ofthis', 0.03), ('implemented', 0.029), ('expansion', 0.029), ('fw', 0.029), ('activities', 0.029), ('hockenmaier', 0.029), ('parsing', 0.029), ('punctuation', 0.029), ('equation', 0.028), ('applies', 0.028), ('notice', 0.027), ('marneffe', 0.026), ('chunk', 0.026), ('harmonic', 0.026), ('conjunctions', 0.026), ('detecting', 0.026), ('conll', 0.025), ('responsible', 0.025), ('benchmark', 0.025), ('chunks', 0.025), ('metric', 0.024), ('depicted', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="12-tfidf-1" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>Author: Federico Sangati</p><p>Abstract: We present a probabilistic model extension to the Tesni `ere Dependency Structure (TDS) framework formulated in (Sangati and Mazza, 2009). This representation incorporates aspects from both constituency and dependency theory. In addition, it makes use of junction structures to handle coordination constructions. We test our model on parsing the English Penn WSJ treebank using a re-ranking framework. This technique allows us to efficiently test our model without needing a specialized parser, and to use the standard evaluation metric on the original Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking.</p><p>2 0.070404865 <a title="12-tfidf-2" href="./acl-2010-The_Importance_of_Rule_Restrictions_in_CCG.html">228 acl-2010-The Importance of Rule Restrictions in CCG</a></p>
<p>Author: Marco Kuhlmann ; Alexander Koller ; Giorgio Satta</p><p>Abstract: Combinatory Categorial Grammar (CCG) is generally construed as a fully lexicalized formalism, where all grammars use one and the same universal set of rules, and crosslinguistic variation is isolated in the lexicon. In this paper, we show that the weak generative capacity of this ‘pure’ form of CCG is strictly smaller than that of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG.</p><p>3 0.069841266 <a title="12-tfidf-3" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>Author: Timothy A. D. Fowler ; Gerald Penn</p><p>Abstract: The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.</p><p>4 0.068631046 <a title="12-tfidf-4" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>Author: Terry Koo ; Michael Collins</p><p>Abstract: We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.</p><p>5 0.067404933 <a title="12-tfidf-5" href="./acl-2010-Semantics-Driven_Shallow_Parsing_for_Chinese_Semantic_Role_Labeling.html">207 acl-2010-Semantics-Driven Shallow Parsing for Chinese Semantic Role Labeling</a></p>
<p>Author: Weiwei Sun</p><p>Abstract: One deficiency of current shallow parsing based Semantic Role Labeling (SRL) methods is that syntactic chunks are too small to effectively group words. To partially resolve this problem, we propose semantics-driven shallow parsing, which takes into account both syntactic structures and predicate-argument structures. We also introduce several new “path” features to improve shallow parsing based SRL method. Experiments indicate that our new method obtains a significant improvement over the best reported Chinese SRL result.</p><p>6 0.066332243 <a title="12-tfidf-6" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>7 0.065686367 <a title="12-tfidf-7" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>8 0.062007584 <a title="12-tfidf-8" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>9 0.061586495 <a title="12-tfidf-9" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>10 0.060736716 <a title="12-tfidf-10" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>11 0.056941092 <a title="12-tfidf-11" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>12 0.056903761 <a title="12-tfidf-12" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>13 0.05680529 <a title="12-tfidf-13" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>14 0.056747541 <a title="12-tfidf-14" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>15 0.056293212 <a title="12-tfidf-15" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>16 0.055085074 <a title="12-tfidf-16" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>17 0.055033524 <a title="12-tfidf-17" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>18 0.051681887 <a title="12-tfidf-18" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>19 0.051408466 <a title="12-tfidf-19" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>20 0.049959455 <a title="12-tfidf-20" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, 0.001), (2, 0.041), (3, -0.002), (4, -0.04), (5, -0.053), (6, 0.041), (7, -0.004), (8, 0.048), (9, 0.081), (10, -0.005), (11, 0.05), (12, -0.057), (13, 0.059), (14, 0.027), (15, -0.01), (16, 0.018), (17, 0.002), (18, -0.038), (19, -0.024), (20, 0.022), (21, -0.041), (22, 0.009), (23, -0.011), (24, -0.006), (25, -0.071), (26, 0.002), (27, 0.038), (28, 0.011), (29, -0.017), (30, 0.022), (31, 0.087), (32, 0.043), (33, 0.005), (34, 0.013), (35, 0.021), (36, 0.024), (37, -0.022), (38, -0.019), (39, -0.04), (40, -0.056), (41, 0.033), (42, -0.054), (43, 0.028), (44, 0.076), (45, -0.074), (46, 0.008), (47, -0.001), (48, -0.009), (49, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90174186 <a title="12-lsi-1" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>Author: Federico Sangati</p><p>Abstract: We present a probabilistic model extension to the Tesni `ere Dependency Structure (TDS) framework formulated in (Sangati and Mazza, 2009). This representation incorporates aspects from both constituency and dependency theory. In addition, it makes use of junction structures to handle coordination constructions. We test our model on parsing the English Penn WSJ treebank using a re-ranking framework. This technique allows us to efficiently test our model without needing a specialized parser, and to use the standard evaluation metric on the original Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking.</p><p>2 0.60490435 <a title="12-lsi-2" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>Author: Bharat Ram Ambati</p><p>Abstract: Statistical systems with high accuracy are very useful in real-world applications. If these systems can capture basic linguistic information, then the usefulness of these statistical systems improve a lot. This paper is an attempt at incorporating linguistic constraints in statistical dependency parsing. We consider a simple linguistic constraint that a verb should not have multiple subjects/objects as its children in the dependency tree. We first describe the importance of this constraint considering Machine Translation systems which use dependency parser output, as an example application. We then show how the current state-ofthe-art dependency parsers violate this constraint. We present two new methods to handle this constraint. We evaluate our methods on the state-of-the-art dependency parsers for Hindi and Czech. 1</p><p>3 0.60446483 <a title="12-lsi-3" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>Author: Terry Koo ; Michael Collins</p><p>Abstract: We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.</p><p>4 0.60443467 <a title="12-lsi-4" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>Author: Timothy A. D. Fowler ; Gerald Penn</p><p>Abstract: The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.</p><p>5 0.59667605 <a title="12-lsi-5" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>Author: Wenbin Jiang ; Qun Liu</p><p>Abstract: In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this clas- , sifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.</p><p>6 0.58822703 <a title="12-lsi-6" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>7 0.56915855 <a title="12-lsi-7" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>8 0.55873656 <a title="12-lsi-8" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>9 0.5524137 <a title="12-lsi-9" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>10 0.54270494 <a title="12-lsi-10" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>11 0.51940829 <a title="12-lsi-11" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>12 0.50833672 <a title="12-lsi-12" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>13 0.50213975 <a title="12-lsi-13" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>14 0.49705672 <a title="12-lsi-14" href="./acl-2010-The_Importance_of_Rule_Restrictions_in_CCG.html">228 acl-2010-The Importance of Rule Restrictions in CCG</a></p>
<p>15 0.47285098 <a title="12-lsi-15" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>16 0.46891332 <a title="12-lsi-16" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>17 0.46814787 <a title="12-lsi-17" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>18 0.46271157 <a title="12-lsi-18" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>19 0.46057504 <a title="12-lsi-19" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>20 0.44889143 <a title="12-lsi-20" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.01), (13, 0.292), (14, 0.028), (16, 0.025), (25, 0.084), (39, 0.013), (42, 0.016), (44, 0.027), (59, 0.077), (73, 0.057), (76, 0.025), (78, 0.041), (83, 0.084), (84, 0.029), (98, 0.117)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77209729 <a title="12-lda-1" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>Author: Federico Sangati</p><p>Abstract: We present a probabilistic model extension to the Tesni `ere Dependency Structure (TDS) framework formulated in (Sangati and Mazza, 2009). This representation incorporates aspects from both constituency and dependency theory. In addition, it makes use of junction structures to handle coordination constructions. We test our model on parsing the English Penn WSJ treebank using a re-ranking framework. This technique allows us to efficiently test our model without needing a specialized parser, and to use the standard evaluation metric on the original Phrase Structure version of the treebank. We obtain encouraging results: we achieve a small improvement over state-of-the-art results when re-ranking a small number of candidate structures, on all the evaluation metrics except for chunking.</p><p>2 0.73803443 <a title="12-lda-2" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>Author: Anders Sogaard</p><p>Abstract: Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004).</p><p>3 0.69533932 <a title="12-lda-3" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>Author: Francois Mairesse ; Milica Gasic ; Filip Jurcicek ; Simon Keizer ; Blaise Thomson ; Kai Yu ; Steve Young</p><p>Abstract: Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation perfor- mance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.</p><p>4 0.68360651 <a title="12-lda-4" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>Author: Ivan Titov ; Mikhail Kozhevnikov</p><p>Abstract: We argue that groups of unannotated texts with overlapping and non-contradictory semantics represent a valuable source of information for learning semantic representations. A simple and efficient inference method recursively induces joint semantic representations for each group and discovers correspondence between lexical entries and latent semantic concepts. We consider the generative semantics-text correspondence model (Liang et al., 2009) and demonstrate that exploiting the noncontradiction relation between texts leads to substantial improvements over natural baselines on a problem of analyzing human-written weather forecasts.</p><p>5 0.55506355 <a title="12-lda-5" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>6 0.55141646 <a title="12-lda-6" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>7 0.53983581 <a title="12-lda-7" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>8 0.53906351 <a title="12-lda-8" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>9 0.53756595 <a title="12-lda-9" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>10 0.53734767 <a title="12-lda-10" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>11 0.53722876 <a title="12-lda-11" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>12 0.53689903 <a title="12-lda-12" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>13 0.53680354 <a title="12-lda-13" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>14 0.53660357 <a title="12-lda-14" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>15 0.53509235 <a title="12-lda-15" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>16 0.53505599 <a title="12-lda-16" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>17 0.53428394 <a title="12-lda-17" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>18 0.53351963 <a title="12-lda-18" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>19 0.5334267 <a title="12-lda-19" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>20 0.53300142 <a title="12-lda-20" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
