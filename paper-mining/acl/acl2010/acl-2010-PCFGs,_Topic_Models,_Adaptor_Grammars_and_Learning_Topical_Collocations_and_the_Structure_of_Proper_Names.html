<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-191" href="#">acl2010-191</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</h1>
<br/><p>Source: <a title="acl-2010-191-pdf" href="http://aclweb.org/anthology//P/P10/P10-1117.pdf">pdf</a></p><p>Author: Mark Johnson</p><p>Abstract: This paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as “topic models” to produce a lowdimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names.</p><p>Reference: <a title="acl-2010-191-reference" href="../acl2010_reference/acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. [sent-6, score-0.312]
</p><p>2 The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. [sent-9, score-0.35]
</p><p>3 1 Introduction Over the last few years there has been considerable interest in Bayesian inference for complex hierarchical models both in machine learning and in computational linguistics. [sent-11, score-0.112]
</p><p>4 This paper establishes a theoretical connection between two very different kinds of probabilistic models: Probabilistic Context-Free Grammars (PCFGs) and a class of models known as Latent Dirichlet Allocation (Blei et al. [sent-12, score-0.127]
</p><p>5 Specifically, we show that an LDA model can be expressed as a certain kind of PCFG, so Bayesian inference for PCFGs can be used to  learn LDA topic models as well. [sent-14, score-0.312]
</p><p>6 However, once this link is established it suggests a variety of extensions to the LDA topic models, two of which we explore in this paper. [sent-16, score-0.2]
</p><p>7 The first involves extending the LDA topic model so that it generates collocations (sequences of words) rather than individual words. [sent-17, score-0.324]
</p><p>8 The second applies this idea to the problem of automatically learning internal structure of proper names (NPs), which is useful for definite NP coreference models and other applications. [sent-18, score-0.152]
</p><p>9 The next section reviews Latent Dirichlet Allocation (LDA) topic models, and the following section reviews Probabilistic Context-Free Grammars (PCFGs). [sent-20, score-0.2]
</p><p>10 Section 4 shows how an LDA topic model can be expressed as a PCFG, which provides the fundamental connection between LDA and PCFGs that we exploit in the rest of the paper, and shows how it can be used to define  a “sticky topic” version of LDA. [sent-21, score-0.242]
</p><p>11 Section 6 exploits the connection between LDA and PCFGs to propose an AG-based topic model that extends LDA by defining distributions over collocations rather than individual words, and section 7 applies this extension to the problem of finding the structure of proper names. [sent-24, score-0.436]
</p><p>12 c s 2o0c1ia0ti Aosnso focria Ctio nm fpourta Ctoiomnpault Laitniognuaislt Licisn,g puaigsetisc 1s148–1 57,  αθβZWφn‘m  Figure 1: A graphical model “plate” representation of an LDA topic model. [sent-29, score-0.2]
</p><p>13 It generates a collection of documents by first generating multinomials φi over the vocabulary V for each topic i ∈ 1, . [sent-36, score-0.279]
</p><p>14 t,h‘e, probability of generating word w in topic i. [sent-41, score-0.2]
</p><p>15 , m in turn by first generating a multinomial θj over topics, where θj,i is the probability of topic i appearing in document j. [sent-45, score-0.262]
</p><p>16 Finally it generates each of the n words of document Dj by first selecting a topic z for the word according to θj, and then drawing a word from φz. [sent-47, score-0.302]
</p><p>17 (The adaptor grammar software we used in the experiments described below automatically does this kind of hyper-parameter inference). [sent-75, score-0.464]
</p><p>18 The inference task is to find the topic probability vector θj of each document Dj given the words wj,k of the documents; in general this also requires inferring the topic to word distributions φ and the topic assigned to each word zj,k. [sent-76, score-0.791]
</p><p>19 (2003) describe a Variational Bayes inference algorithm for LDA models based on a mean-field approximation, while Griffiths and Steyvers (2004) describe an Markov Chain Monte Carlo inference algorithm based on Gibbs sampling; both are quite effective in practice. [sent-78, score-0.196]
</p><p>20 A Context-Free Grammar (CFG) is a quadruple (N, W, R, S) where N and W are disjoint finite sets of nonterminal and terminal symbols respectively, R is a finite set of productions or rules ofthe form A → β where A ∈ N adundc β ∈ (N∪W)? [sent-80, score-0.128]
</p><p>21 1149  The set of trees generated by the CFG is TS, whTerhee S se its othfe t rseteasrt symbol, dan bdy th thee es CetF FoGf strings generated by the CFG is the set of yields (i. [sent-119, score-0.178]
</p><p>22 nAa lP srtorbinabgisl)ios tfic t hCeo tnreteexsti- nFr Tee Grammar (PCFG) is a pair consisting of a CFG and set of multinomial probability vectors θX indexed by nonterminals X ∈ N, where θX is a distribution over the  rnualless RX (i. [sent-122, score-0.187]
</p><p>23 More formally, a PCFG athsseo rcuilaete Xs e →ach β βX ∈ ∈ N W with a distribution GX over sth eea trees TX as f ∪ol lWows w. [sent-126, score-0.112]
</p><p>24 The PCFG generates the distribution over trees GS, where S is the start symbol. [sent-158, score-0.152]
</p><p>25 The distribution over the strings it generates is obtained by marginalising over the trees. [sent-159, score-0.134]
</p><p>26 In a Bayesian PCFG one puts Dirichlet priors Dir(αX) on each of the multinomial rule probability vectors θX for each nonterminal X ∈ N. [sent-160, score-0.143]
</p><p>27 for a PCFG one is given a CFG, parameters αX for the Dirichlet priors over the rule probabilities, and a corpus of strings. [sent-163, score-0.114]
</p><p>28 The task is to infer the corresponding posterior distribution over rule probabilities θX. [sent-164, score-0.134]
</p><p>29 (There are several different ways of encoding LDA models as PCFGs; the one presented here is not the most succinct it is possible to collapse the Doc and Doc0 nonterminals but it has the advantage that the LDA distributions map straight-forwardly onto PCFG nonterminals). [sent-169, score-0.252]
</p><p>30 ,em mP,C wFGhe encoding strings ferro mof document j are prefixed with “j”; this indicates to the grammar which document the string comes from. [sent-177, score-0.325]
</p><p>31 The nonterminals consist of the start symbol Sentence, Docj and Doc0j for each j ∈ 1, . [sent-178, score-0.168]
</p><p>32 In these trees the leftbranching spine of nodes labelled Doc0j propagate the document identifier throughout the whole tree. [sent-212, score-0.196]
</p><p>33 The nodes labelled Topici indicate the topics assigned to particular words, and the local trees expanding Docj to Topici (one per word in the document) indicate the distribution of topics in the document. [sent-213, score-0.314]
</p><p>34 The probabilities θTopici associated with the rules expanding the Topici nonterminals indicate how  words are distributed across topics; the θTopici probabilities correspond exactly to to the φi probabilities in the LDA model. [sent-215, score-0.35]
</p><p>35 The probabilities 1150  Sentence  Doc3' Doc3' Doc3' Doc3'  Doc3 Doc3 Topic7  Doc3  Topic4 faster  Doc3' Doc3 Topic4 compute  _3sThoalpliocw4circuits Figure 2: A tree generated by the CFG encoding an LDA topic model. [sent-216, score-0.361]
</p><p>36 θDocj associated with rules expanding Docj specify the distribution of topics in document j; they correspond exactly to the probabilities θj of the LDA model. [sent-219, score-0.292]
</p><p>37 However, it is easy to see that these distributions do not influence the topic distributions; indeed, the expansions of the Sentence nonterminal are completely determined by the document distribution in the corpus, and are not affected by θSentence). [sent-222, score-0.454]
</p><p>38 A Bayesian PCFG places Dirichlet priors Dir(αA) on the corresponding rule probabilities θA for each A ∈ N. [sent-223, score-0.129]
</p><p>39 In the PCFG encoding an LDAfo model, Athe ∈ αTopici parameters correspond exactly to the β parameters ofthe LDA model, and the αDocj parameters correspond to the α parameters of the LDA model. [sent-224, score-0.18]
</p><p>40 As suggested above, each document Dj in the LDA model is mapped to a string in the corpus used to train the corresponding PCFG by prefixing it with a document identifier “j”. [sent-225, score-0.184]
</p><p>41 Given this  training data, the posterior distribution over rule probabilities θDocj→ Topici is the same as the posterior distribution over topics given documents θj,i in the original LDA model. [sent-226, score-0.286]
</p><p>42 As we will see below, this connection between PCFGs and LDA topic models suggests a number of interesting variants of both PCFGs and topic models. [sent-227, score-0.47]
</p><p>43 Note that we are not suggesting that Bayesian inference for PCFGs is necessarily a good way of estimating LDA topic models. [sent-228, score-0.284]
</p><p>44 Current Bayesian PCFG inference algorithms require time proportional to the cube of the length of the longest string in the training corpus, and since these strings correspond to entire documents in our embedding, blindly applying a Bayesian PCFG inference algorithm is likely to be impractical. [sent-229, score-0.28]
</p><p>45 A little reflection shows that the embedding still holds if the strings in the PCFG corpus correspond to sentences or even smaller units of the original document collection, so a single document would be mapped to multiple strings in the PCFG inference task. [sent-230, score-0.356]
</p><p>46 However, we believe that the primary value of the embedding ofLDA topic models into Bayesian PCFGs is theoretical: it suggests a number of novel extensions of both topic models and grammars that may be worth exploring. [sent-233, score-0.598]
</p><p>47 The nonterminals Sentence and Topici for i = 1, . [sent-239, score-0.135]
</p><p>48 , ‘ have the same interpretation as before, but we introduce new nonterminals Docj,i that indicate we have just generated a nonterminal in document j belonging to topic i. [sent-242, score-0.498]
</p><p>49 , ‘; w ∈ V A sample parse generated by a “sticky topic” →  Docj,i  1151  SDeoncte3n,7ce Doc3,4 Doc3,4 Doc3,4  Topic7 Topic4 faster  Topic4 compute  Doc3,1Topic4circuits _3  shallow  Figure 3: A tree generated by the “sticky topic” CFG. [sent-261, score-0.106]
</p><p>50 Here a nonterminal Doc3, 7 indicates we have just generated a word in document 3 belonging to topic 7. [sent-262, score-0.363]
</p><p>51 The probabilities of the rules Docj,i → Docj,i0 Topici in this PCFG encode the probability ocf shifting from topic ito topic i0 (this PCFG can be viewed as generating the string from right to left). [sent-264, score-0.513]
</p><p>52 We can use non-uniform sparse Dirichlet priors on the probabilities of these rules to encourage “topic stickiness”. [sent-265, score-0.129]
</p><p>53 =  5  Adaptor Grammars  Non-parametric Bayesian inference, where the inference task involves learning not just the values of a finite vector of parameters but which parameters are relevant, has been the focus of intense research in machine learning recently. [sent-267, score-0.152]
</p><p>54 In the topicmodelling community this has lead to work on Dirichlet Processes and Chinese Restaurant Processes, which can be used to estimate the number of topics as well as their distribution across documents (Teh et al. [sent-268, score-0.152]
</p><p>55 In the first we regard the set of nonterminals N as potentially unbounded, and try to learn the set of nonterminals required to describe the training corpus. [sent-271, score-0.27]
</p><p>56 The inference task is non-parametric if there are an unbounded number of such subtrees. [sent-285, score-0.11]
</p><p>57 We review the adaptor grammar generative process below; for an informal introduction see Johnson (2008) and for details of the adaptor grammar inference procedure see Johnson and Goldwater (2009). [sent-286, score-1.012]
</p><p>58 An adaptor grammar (N, W, R, S, θ, A, C) consists of a PCFG (N, W, R, S, θ) in which a sub-  set A ⊆ N of the nonterminals are adapted, and swehte Are ⊆eac Nh adapted onnotnertmerimnianlasl a rXe ∈ pAt hda,s a an awshseocreia eteadc adaptor CX. [sent-287, score-1.01]
</p><p>59 nAtenr adaptor CX f Aor hXas sis a a function that maps a distribution over trees TX to a dnicsttiroinbu tthioant over d ais dtirsitbruitbiuotnios over TX (we give examples oiofn adaptors below). [sent-288, score-0.551]
</p><p>60 Just as for a PCFG, an adaptor grammar defines distributions GX over trees TX for each X ∈ N ∪ W. [sent-289, score-0.569]
</p><p>61 Ym ∈RX  That is, the distribution GX associated with an adapted nonterminal X ∈ A is a sample from adapting (i. [sent-303, score-0.146]
</p><p>62 For example, with the adaptors used here GX typically concentrates mass on a smaller subset of the trees  TX than HX does. [sent-307, score-0.119]
</p><p>63 Just as with the PCFG, an adaptor grammar generates the distribution over trees GS, where S ∈ N 152  1  is the start symbol. [sent-308, score-0.616]
</p><p>64 However, while GS in a PCFG is a fixed distribution (given the rule probabilities θ), in an adaptor grammar the distribution GS is itself a random variable (because each GX for X ∈ A is random), i. [sent-309, score-0.65]
</p><p>65 , an adaptor grammar geneXra ∈tes A a sdi rsatrnidboumtio)n, i over nd aisdtarpibtuotrio gnrsa over t greeensTS. [sent-311, score-0.464]
</p><p>66 In the context of adaptor grammars, the CRP is: CRP(t | t, αX, HX)  ∝  nt(t) + αXHX(t)  where nt(t) is the number of times t appears in t and αX > 0 is a user-settable “concentration parameter”. [sent-329, score-0.38]
</p><p>67 In order to generate the next tree tn+1 a CRP either reuses a tree t with probability proportional to number of times t has been previously generated, or else it “backs off” to the “base distribution” HX and generates a fresh tree t with probability proportional to αXHX (t). [sent-330, score-0.13]
</p><p>68 The PYP can mitigate this somewhat by reducing the effective count of previously generated trees and redistributing that probability mass to new trees generated from HX. [sent-335, score-0.196]
</p><p>69 Adaptor grammars have previously been used primarily to study grammatical inference in the context of language acquisition. [sent-338, score-0.162]
</p><p>70 For example, the phoneme string corresponding to “you want to see the book” (with its correct segmentation indicated) is as follows: y u Nw a n t Nt u Ns iND 6 Nb U k We can represent any possible segmentation of any possible sentence as a tree generated by the following unigram adaptor grammar. [sent-340, score-0.479]
</p><p>71 Sentence → Word SSeenntteennccee → Word Sentence Word → P →ho Wneomrdes S PWhoorndem →e sP → nPemhoenseme PPhhoonneemmeess → PPhhoonneemmee Phonemes The trees generated by this adaptor grammar are  the same as the trees generated by the CFG rules. [sent-341, score-0.66]
</p><p>72 For example, the following skeletal parse in which all but the Word nonterminals are suppressed (the others are deterministically inferrable) shows the parse that corresponds to the correct segmentation of the string above. [sent-342, score-0.225]
</p><p>73 (Word y u) (Word w a n t) (Word t u) (Word s i) (Word d 6) (Word b u k) Because the Word nonterminal is adapted (indicated here by underlining) the adaptor grammar learns the probability of the entire Word subtrees (e. [sent-343, score-0.558]
</p><p>74 1153  6  Topic models with collocations  Here we combine ideas from the unigram word segmentation adaptor grammar above and the PCFG encoding of LDA topic models to present a novel topic model that learns topical collocations. [sent-346, score-1.048]
</p><p>75 Specifically, we take the PCFG encoding of the LDA topic model  described above, but modify it so that the Topici nodes generate sequences of words rather than single words. [sent-349, score-0.244]
</p><p>76 Because there is no gen-  erally accepted evaluation for collocation-finding, we merely present some of the sample analyses found by our adaptor grammar. [sent-368, score-0.38]
</p><p>77 We ran our adaptor grammar with ‘ = 20 topics (i. [sent-369, score-0.525]
</p><p>78 Adaptor grammar inference on this corpus is actually relatively efficient because the corpus provided by Griffiths et al. [sent-372, score-0.168]
</p><p>79 The following are some examples of collocations found by our adaptor grammar: Topic0 Topic0 Topic0 Topic0  → → → →  cost function fcioxsetd f point gradient ndtescent learning rdaetsecse  →  1http://web. [sent-375, score-0.464]
</p><p>80 Here we explore the potential for using Bayesian inference for learning linear ordering constraints that hold between elements within proper names. [sent-386, score-0.149]
</p><p>81 Here we present an adaptor grammar based on the insights of the PCFG encoding of LDA topic models that learns some of the structure of proper names. [sent-395, score-0.801]
</p><p>82 The key idea is that elements in proper names typically appear in a fixed order; we expect honorifics to appear before first names, which appear before middle names, which in turn appear before surnames, etc. [sent-396, score-0.124]
</p><p>83 We deal with this by permitting multi-word collocations to fill the corresponding positions, and use the adaptor grammar machinery to learn these collocations. [sent-402, score-0.548]
</p><p>84 (2009), our adaptor grammar is as follows, where adapted nonterminals are indicated by underlining as before. [sent-404, score-0.656]
</p><p>85 Word+ B6 → Word+ Unordered →  Word+  In this grammar parentheses indicate optionality, and the Kleene plus indicates iteration (these were manually expanded into ordinary CFG rules  in our experiments). [sent-419, score-0.117]
</p><p>86 The grammar provides three different expansions for proper names. [sent-420, score-0.181]
</p><p>87 The first expansion says that a proper name can consist of some subset of the six different collocation classes A0 through A6 in that order, while the second expansion says that a proper name can consist of some subset of the collocation classes B0 through B6, again in that order. [sent-421, score-0.312]
</p><p>88 Finally, the third expansion says that a proper name can consist of an arbitrary sequence of “unordered” collocations (this is intended as a “catch-all” expansion to provide analyses for proper names that don’t fit either of the first two expansions). [sent-422, score-0.364]
</p><p>89 , phrases of category NNP and NNPS) in the Penn WSJ treebank and used them as the training corpora for the adaptor grammar just described. [sent-425, score-0.464]
</p><p>90 The adaptor grammar inference procedure found skeletal sample parses such as the following: (A0 barrett) (A3 smith) (A0 albert) (A2 j. [sent-426, score-0.581]
</p><p>91 8  Conclusion  This paper establishes a connection between two very different kinds of probabilistic models; LDA  models of the kind used for topic modelling, and PCFGs, which are a standard model of hierarchical structure in language. [sent-433, score-0.327]
</p><p>92 The embedding we presented shows how to express an LDA model as a PCFG, and has the property that Bayesian inference of the parameters of that PCFG produces an equivalent model to that produced by Bayesian inference of the LDA model’s parameters. [sent-434, score-0.266]
</p><p>93 Instead, we claim that the embedding suggests novel extensions to both the LDA topic models and PCFG-style grammars. [sent-436, score-0.32]
</p><p>94 We justified this claim by presenting several hybrid models that combine aspects of both topic models and 1155  grammars. [sent-437, score-0.284]
</p><p>95 We don’t claim that these are necessarily the best models for performing any particular tasks; rather, we present them as examples of models inspired by a combination of PCFGs and LDA topic models. [sent-438, score-0.284]
</p><p>96 We then discussed adaptor grammars, and inspired by the LDA topic models, presented a novel topic model whose primitive elements are multi-word collocations rather than words. [sent-440, score-0.864]
</p><p>97 We concluded with an adaptor grammar that learns aspects of the internal structure of proper names. [sent-441, score-0.529]
</p><p>98 Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. [sent-512, score-0.38]
</p><p>99 Using adaptor grammars to identifying synergies in the unsupervised acquisition of linguistic structure. [sent-531, score-0.458]
</p><p>100 Topical n-grams: Phrase and topic discovery, with an application to information retrieval. [sent-568, score-0.2]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lda', 0.441), ('adaptor', 0.38), ('topici', 0.312), ('pcfg', 0.256), ('pcfgs', 0.242), ('topic', 0.2), ('docj', 0.178), ('gx', 0.169), ('nonterminals', 0.135), ('bayesian', 0.119), ('hx', 0.116), ('cfg', 0.11), ('dirichlet', 0.108), ('pyp', 0.104), ('tx', 0.088), ('collocations', 0.084), ('inference', 0.084), ('grammar', 0.084), ('crp', 0.078), ('grammars', 0.078), ('sticky', 0.074), ('griffiths', 0.069), ('ax', 0.066), ('proper', 0.065), ('embedding', 0.064), ('nonterminal', 0.063), ('document', 0.062), ('johnson', 0.062), ('topics', 0.061), ('trees', 0.06), ('adaptors', 0.059), ('bx', 0.059), ('names', 0.059), ('dj', 0.056), ('distribution', 0.052), ('clinton', 0.05), ('probabilities', 0.049), ('cx', 0.048), ('rx', 0.048), ('priors', 0.047), ('tn', 0.046), ('distributions', 0.045), ('labelled', 0.045), ('tbn', 0.045), ('treex', 0.045), ('encoding', 0.044), ('allocation', 0.042), ('goldwater', 0.042), ('connection', 0.042), ('strings', 0.042), ('blei', 0.04), ('generates', 0.04), ('sharon', 0.039), ('documents', 0.039), ('lsi', 0.039), ('elsner', 0.039), ('generated', 0.038), ('gs', 0.037), ('dir', 0.036), ('expanding', 0.035), ('informally', 0.034), ('parameters', 0.034), ('macquarie', 0.033), ('skeletal', 0.033), ('beal', 0.033), ('consist', 0.033), ('rule', 0.033), ('rules', 0.033), ('terminal', 0.032), ('expansions', 0.032), ('string', 0.031), ('name', 0.031), ('adapted', 0.031), ('ij', 0.031), ('nt', 0.03), ('latent', 0.03), ('tree', 0.03), ('ags', 0.03), ('nnpp', 0.03), ('oxve', 0.03), ('sudderth', 0.03), ('tda', 0.03), ('xhx', 0.03), ('establishes', 0.029), ('identifier', 0.029), ('models', 0.028), ('steyvers', 0.028), ('probabilistic', 0.028), ('claim', 0.028), ('infinite', 0.028), ('jordan', 0.028), ('np', 0.027), ('says', 0.027), ('nips', 0.027), ('mr', 0.026), ('unbounded', 0.026), ('variational', 0.026), ('underlining', 0.026), ('suppressed', 0.026), ('kurihara', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999905 <a title="191-tfidf-1" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>Author: Mark Johnson</p><p>Abstract: This paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as “topic models” to produce a lowdimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names.</p><p>2 0.25679925 <a title="191-tfidf-2" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>Author: Diarmuid O Seaghdha</p><p>Abstract: This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.</p><p>3 0.21407369 <a title="191-tfidf-3" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>Author: Tomoharu Iwata ; Daichi Mochihashi ; Hiroshi Sawada</p><p>Abstract: We propose a corpus-based probabilistic framework to extract hidden common syntax across languages from non-parallel multilingual corpora in an unsupervised fashion. For this purpose, we assume a generative model for multilingual corpora, where each sentence is generated from a language dependent probabilistic contextfree grammar (PCFG), and these PCFGs are generated from a prior grammar that is common across languages. We also develop a variational method for efficient inference. Experiments on a non-parallel multilingual corpus of eleven languages demonstrate the feasibility of the proposed method.</p><p>4 0.15371956 <a title="191-tfidf-4" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>Author: Alan Ritter ; Mausam Mausam ; Oren Etzioni</p><p>Abstract: The computation of selectional preferences, the admissible argument values for a relation, is a well-known NLP task with broad applicability. We present LDA-SP, which utilizes LinkLDA (Erosheva et al., 2004) to model selectional preferences. By simultaneously inferring latent topics and topic distributions over relations, LDA-SP combines the benefits of previous approaches: like traditional classbased approaches, it produces humaninterpretable classes describing each relation’s preferences, but it is competitive with non-class-based methods in predictive power. We compare LDA-SP to several state-ofthe-art methods achieving an 85% increase in recall at 0.9 precision over mutual information (Erk, 2007). We also evaluate LDA-SP’s effectiveness at filtering improper applications of inference rules, where we show substantial improvement over Pantel et al. ’s system (Pantel et al., 2007).</p><p>5 0.1343454 <a title="191-tfidf-5" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>Author: Trevor Cohn ; Phil Blunsom</p><p>Abstract: Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently con- verges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</p><p>6 0.13282801 <a title="191-tfidf-6" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>7 0.12719096 <a title="191-tfidf-7" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>8 0.12325761 <a title="191-tfidf-8" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>9 0.12159595 <a title="191-tfidf-9" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<p>10 0.1117034 <a title="191-tfidf-10" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>11 0.10668432 <a title="191-tfidf-11" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>12 0.10155421 <a title="191-tfidf-12" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>13 0.10052229 <a title="191-tfidf-13" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>14 0.096547015 <a title="191-tfidf-14" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>15 0.095779963 <a title="191-tfidf-15" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>16 0.080210008 <a title="191-tfidf-16" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>17 0.069709204 <a title="191-tfidf-17" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>18 0.057050478 <a title="191-tfidf-18" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>19 0.056867816 <a title="191-tfidf-19" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>20 0.056417648 <a title="191-tfidf-20" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.186), (1, 0.021), (2, 0.008), (3, -0.01), (4, -0.047), (5, -0.04), (6, 0.121), (7, -0.077), (8, 0.173), (9, -0.221), (10, -0.03), (11, -0.086), (12, 0.2), (13, 0.025), (14, 0.14), (15, -0.065), (16, -0.051), (17, -0.086), (18, -0.012), (19, -0.117), (20, -0.174), (21, -0.119), (22, -0.011), (23, 0.055), (24, -0.14), (25, -0.034), (26, 0.018), (27, 0.103), (28, -0.08), (29, -0.016), (30, 0.04), (31, -0.026), (32, -0.129), (33, 0.155), (34, -0.0), (35, 0.004), (36, 0.051), (37, -0.001), (38, 0.091), (39, -0.084), (40, -0.005), (41, 0.035), (42, 0.029), (43, -0.079), (44, -0.026), (45, 0.022), (46, 0.008), (47, 0.043), (48, 0.04), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96687144 <a title="191-lsi-1" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>Author: Mark Johnson</p><p>Abstract: This paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as “topic models” to produce a lowdimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names.</p><p>2 0.67043501 <a title="191-lsi-2" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>Author: Tomoharu Iwata ; Daichi Mochihashi ; Hiroshi Sawada</p><p>Abstract: We propose a corpus-based probabilistic framework to extract hidden common syntax across languages from non-parallel multilingual corpora in an unsupervised fashion. For this purpose, we assume a generative model for multilingual corpora, where each sentence is generated from a language dependent probabilistic contextfree grammar (PCFG), and these PCFGs are generated from a prior grammar that is common across languages. We also develop a variational method for efficient inference. Experiments on a non-parallel multilingual corpus of eleven languages demonstrate the feasibility of the proposed method.</p><p>3 0.66580409 <a title="191-lsi-3" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>Author: Duo Zhang ; Qiaozhu Mei ; ChengXiang Zhai</p><p>Abstract: Probabilistic latent topic models have recently enjoyed much success in extracting and analyzing latent topics in text in an unsupervised way. One common deficiency of existing topic models, though, is that they would not work well for extracting cross-lingual latent topics simply because words in different languages generally do not co-occur with each other. In this paper, we propose a way to incorporate a bilingual dictionary into a probabilistic topic model so that we can apply topic models to extract shared latent topics in text data of different languages. Specifically, we propose a new topic model called Probabilistic Cross-Lingual Latent Semantic Analysis (PCLSA) which extends the Proba- bilistic Latent Semantic Analysis (PLSA) model by regularizing its likelihood function with soft constraints defined based on a bilingual dictionary. Both qualitative and quantitative experimental results show that the PCLSA model can effectively extract cross-lingual latent topics from multilingual text data.</p><p>4 0.63091391 <a title="191-lsi-4" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<p>Author: Sindhu Raghavan ; Adriana Kovashka ; Raymond Mooney</p><p>Abstract: In this paper, we present a novel approach for authorship attribution, the task of identifying the author of a document, using probabilistic context-free grammars. Our approach involves building a probabilistic context-free grammar for each author and using this grammar as a language model for classification. We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy.</p><p>5 0.59209847 <a title="191-lsi-5" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>Author: Diarmuid O Seaghdha</p><p>Abstract: This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.</p><p>6 0.58139759 <a title="191-lsi-6" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>7 0.52306926 <a title="191-lsi-7" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>8 0.48755074 <a title="191-lsi-8" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>9 0.47270095 <a title="191-lsi-9" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>10 0.46800512 <a title="191-lsi-10" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>11 0.45089823 <a title="191-lsi-11" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>12 0.3951008 <a title="191-lsi-12" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>13 0.39483026 <a title="191-lsi-13" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>14 0.35894555 <a title="191-lsi-14" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>15 0.35424978 <a title="191-lsi-15" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>16 0.34932622 <a title="191-lsi-16" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>17 0.34656581 <a title="191-lsi-17" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>18 0.34153312 <a title="191-lsi-18" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>19 0.34150788 <a title="191-lsi-19" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>20 0.34027171 <a title="191-lsi-20" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.011), (25, 0.086), (33, 0.019), (39, 0.021), (42, 0.016), (44, 0.018), (59, 0.074), (69, 0.273), (71, 0.012), (73, 0.071), (76, 0.013), (78, 0.041), (80, 0.012), (83, 0.073), (84, 0.047), (98, 0.126)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79856801 <a title="191-lda-1" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>Author: Mark Johnson</p><p>Abstract: This paper establishes a connection between two apparently very different kinds of probabilistic models. Latent Dirichlet Allocation (LDA) models are used as “topic models” to produce a lowdimensional representation of documents, while Probabilistic Context-Free Grammars (PCFGs) define distributions over trees. The paper begins by showing that LDA topic models can be viewed as a special kind of PCFG, so Bayesian inference for PCFGs can be used to infer Topic Models as well. Adaptor Grammars (AGs) are a hierarchical, non-parameteric Bayesian extension of PCFGs. Exploiting the close relationship between LDA and PCFGs just described, we propose two novel probabilistic models that combine insights from LDA and AG models. The first replaces the unigram component of LDA topic models with multi-word sequences or collocations generated by an AG. The second extension builds on the first one to learn aspects of the internal structure of proper names.</p><p>2 0.72885144 <a title="191-lda-2" href="./acl-2010-Finding_Cognate_Groups_Using_Phylogenies.html">116 acl-2010-Finding Cognate Groups Using Phylogenies</a></p>
<p>Author: David Hall ; Dan Klein</p><p>Abstract: A central problem in historical linguistics is the identification of historically related cognate words. We present a generative phylogenetic model for automatically inducing cognate group structure from unaligned word lists. Our model represents the process of transformation and transmission from ancestor word to daughter word, as well as the alignment between the words lists of the observed languages. We also present a novel method for simplifying complex weighted automata created during inference to counteract the otherwise exponential growth of message sizes. On the task of identifying cognates in a dataset of Romance words, our model significantly outperforms a baseline ap- proach, increasing accuracy by as much as 80%. Finally, we demonstrate that our automatically induced groups can be used to successfully reconstruct ancestral words.</p><p>3 0.68825942 <a title="191-lda-3" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>4 0.64463317 <a title="191-lda-4" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>Author: Carlos Gomez-Rodriguez ; Joakim Nivre</p><p>Abstract: Finding a class of structures that is rich enough for adequate linguistic representation yet restricted enough for efficient computational processing is an important problem for dependency parsing. In this paper, we present a transition system for 2-planar dependency trees trees that can be decomposed into at most two planar graphs and show that it can be used to implement a classifier-based parser that runs in linear time and outperforms a stateof-the-art transition-based parser on four data sets from the CoNLL-X shared task. In addition, we present an efficient method – – for determining whether an arbitrary tree is 2-planar and show that 99% or more of the trees in existing treebanks are 2-planar.</p><p>5 0.57949436 <a title="191-lda-5" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>6 0.57730001 <a title="191-lda-6" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>7 0.57146084 <a title="191-lda-7" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>8 0.57043469 <a title="191-lda-8" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>9 0.56923175 <a title="191-lda-9" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>10 0.56908381 <a title="191-lda-10" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>11 0.5688448 <a title="191-lda-11" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>12 0.56874138 <a title="191-lda-12" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>13 0.56521565 <a title="191-lda-13" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>14 0.56515038 <a title="191-lda-14" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>15 0.56502432 <a title="191-lda-15" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>16 0.56496352 <a title="191-lda-16" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>17 0.5642066 <a title="191-lda-17" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>18 0.56417155 <a title="191-lda-18" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>19 0.56376338 <a title="191-lda-19" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>20 0.56374347 <a title="191-lda-20" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
