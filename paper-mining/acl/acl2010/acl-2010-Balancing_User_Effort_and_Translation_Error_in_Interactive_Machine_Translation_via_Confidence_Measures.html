<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-45" href="#">acl2010-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</h1>
<br/><p>Source: <a title="acl-2010-45-pdf" href="http://aclweb.org/anthology//P/P10/P10-2032.pdf">pdf</a></p><p>Author: Jesus Gonzalez Rubio ; Daniel Ortiz Martinez ; Francisco Casacuberta</p><p>Abstract: This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.</p><p>Reference: <a title="acl-2010-45-reference" href="../acl2010_reference/acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de Valencia 46021 Valencia, Spain 46021 Valencia, Spain j egon z ale z @ it i upv . [sent-10, score-0.063]
</p><p>2 e s c  Abstract This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. [sent-21, score-0.33]
</p><p>3 If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. [sent-22, score-0.864]
</p><p>4 We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. [sent-23, score-0.589]
</p><p>5 Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort. [sent-24, score-0.313]
</p><p>6 1 Introduction  In Statistical Machine Translation (SMT), the translation is modelled as a decission process. [sent-25, score-0.155]
</p><p>7 (1)  Within the Interactive-predictive Machine Translation (IMT) framework, a state-of-the-art SMT system is employed in the following way: For a given source sentence, the SMT system fully automatically generates an initial translation. [sent-39, score-0.163]
</p><p>8 A human translator checks this translation from left to right, correcting the first error. [sent-40, score-0.204]
</p><p>9 The SMT system then proposes a new extension, taking the correct prefix ei1 = e1 . [sent-41, score-0.06]
</p><p>10 These steps are repeated until the whole input sentence has been correctly translated. [sent-45, score-0.038]
</p><p>11 IMT aims at reducing the effort and increasing the productivity of translators, while preserving high-quality translation. [sent-52, score-0.107]
</p><p>12 In this work, we integrate Confidence Measures (CMs) within the IMT framework to further reduce the user effort. [sent-53, score-0.114]
</p><p>13 As will be shown, our proposal allows to balance the ratio between user effort and final translation error. [sent-54, score-0.546]
</p><p>14 1 Confidence Measures Confidence estimation have been extensively studied for speech recognition. [sent-56, score-0.039]
</p><p>15 Different TransType-style MT systems use confidence information to improve translation prediction accuracy (Gandrabur and Foster, 2003; Ueffing and Ney, 2005). [sent-59, score-0.303]
</p><p>16 In this work, we propose a fo-  cus shift in which CMs are used to modify the interaction between the user and the system instead of modify the IMT translation predictions. [sent-60, score-0.317]
</p><p>17 To compute CMs we have to select suitable confidence features and define a binary classifier. [sent-61, score-0.148]
</p><p>18 Typically, the classification is carried out depending on whether the confidence value exceeds a given threshold or not. [sent-62, score-0.363]
</p><p>19 2  IMT with Sentence CMs  In the conventional IMT scenario a human translator and a SMT system collaborate in order to obtain the translation the user has in mind. [sent-63, score-0.481]
</p><p>20 Once the user has interactively translated the source sentences, the output translations are error-free. [sent-64, score-0.347]
</p><p>21 We propose an alternative scenario where not all the source sentences are interactively translated by the user. [sent-65, score-0.199]
</p><p>22 Specifically, only those source sentences 173  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-66, score-0.034]
</p><p>23 c C2o0n1f0er Aenscseoc Sihatoirotn P faopre Crso,m papguetsat 1io7n3a–l1 L7i7n,guistics whose initial fully automatic translation are incorrect, according to some quality criterion, are in-  teractively translated. [sent-68, score-0.257]
</p><p>24 We propose to use CMs as the quality criterion to classify those initial translations. [sent-69, score-0.055]
</p><p>25 Our approach implies a modification of the user-machine interaction protocol. [sent-70, score-0.021]
</p><p>26 For a given source sentence, the SMT system generates an initial translation. [sent-71, score-0.089]
</p><p>27 Then, if the CM classifies this translation as correct, we output it as our final translation. [sent-72, score-0.205]
</p><p>28 On the contrary, if the initial translation is classified as incorrect, we perform a conventional IMT procedure, validating correct prefixes and generating new suffixes, until the sentence that the user has in mind is reached. [sent-73, score-0.52]
</p><p>29 In our scenario, we allow the final translations to be different from the ones the user has in mind. [sent-74, score-0.198]
</p><p>30 If a small loss in translation can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those sentences that the CMs classify as incorrect. [sent-76, score-0.577]
</p><p>31 It is worth of notice that our proposal can be seen as a generalisation of the conventional IMT approach. [sent-77, score-0.173]
</p><p>32 Varying the value of the CM classifi-  cation matic sified where  threshold, we can range from a fully autoSMT system where all sentences are clasas correct to a conventional IMT system all sentences are classified as incorrect. [sent-78, score-0.338]
</p><p>33 1 Selecting a CM for IMT We compute sentence CMs by combining the scores given by a word CM based on the IBM model 1 (Brown et al. [sent-80, score-0.067]
</p><p>34 The word confidence value of word ei, cw (ei), is given by  cw(ei) =0 m≤ja≤xJp(ei|fj)  ,  (3)  where p(ei |fj) is the IBM model 1lexicon probability, and f0 is the empty source word. [sent-88, score-0.273]
</p><p>35 From this word CM, we compute two sentence CMs which differ in the way the word confidence  pora. [sent-89, score-0.186]
</p><p>36 scores cw (ei) are combined: MEAN CM (cM (eI1)) is computed as the geo-  metric mean of the confidence scores of the words in the sentence:  cM(e1I) =tuuvIiY=I1cw(ei) . [sent-91, score-0.303]
</p><p>37 (4)  RATIO CM (cR(e1I)) is computed as the percentage of words classified as correct in the sentence. [sent-92, score-0.095]
</p><p>38 A word is classified as correct if its confidence exceeds a word classification threshold τw. [sent-93, score-0.391]
</p><p>39 cR(e1I) =|{ei/ cw(Iei) > τw}|  (5)  After computing the confidence value, each sentence is classified as either correct or incorrect, depending on whether its confidence value exceeds or not a sentence clasiffication threshold τs. [sent-94, score-0.616]
</p><p>40 0 then all the sentences will be classified as correct whereas if τs = 1. [sent-96, score-0.095]
</p><p>41 3  Experimentation  The aim of the experimentation was to study the possibly trade-off between saved user effort and translation error obtained when using sentence CMs within the IMT framework. [sent-98, score-0.488]
</p><p>42 WSR is used in the context of IMT to measure the effort required by the user to generate her 174  Threshold (τs)  Threshold (τs) Figure 1: BLEU translation scores versus WSR for different values of the sentence classification threshold using the MEAN CM. [sent-102, score-0.523]
</p><p>43 WSR is computed as the ratio between the number of word-strokes a user would need to achieve the translation she has in mind and the total number of words in the sentence. [sent-104, score-0.33]
</p><p>44 In this context, a word-stroke is interpreted as a single action, in which the user types a complete word, and  is assumed to have constant cost. [sent-105, score-0.114]
</p><p>45 Additionally, and because our proposal allows differences between its output and the reference translation, we will also present translation quality results in terms of BiLingual Evaluation Understudy (BLEU) (Papineni et al. [sent-106, score-0.292]
</p><p>46 BLEU computes a geometric mean of the precision of ngrams multiplied by a factor to penalise short sentences. [sent-108, score-0.037]
</p><p>47 2 Experimental Setup Our experiments were carried out on the EU corpora (Barrachina et al. [sent-110, score-0.036]
</p><p>48 As a first step, be built a SMT system to translate from Spanish into English. [sent-117, score-0.027]
</p><p>49 This was done by means of the Thot toolkit (Ortiz et al. [sent-118, score-0.035]
</p><p>50 , 2005), which is a complete system for building phrase-  based SMT models. [sent-119, score-0.027]
</p><p>51 The IMT system which we have implemented relies on the use of word graphs (Ueffing et al. [sent-123, score-0.063]
</p><p>52 A word graph has to be generated for each sentence to be interactively translated. [sent-125, score-0.154]
</p><p>53 For this purpose, we used a multi-stack phrase-based decoder which will be distributed in the near future together with the Thot toolkit. [sent-126, score-0.032]
</p><p>54 We discarded to use the state-of-the-art Moses toolkit (Koehn et al. [sent-127, score-0.035]
</p><p>55 , 2007) because preliminary experiments performed with it revealed that the decoder by OrtizMart ı´nez et al. [sent-128, score-0.032]
</p><p>56 (2005) performs better in terms of  WSR when used to generate word graphs for their use in IMT (Sanchis-Trilles et al. [sent-129, score-0.036]
</p><p>57 The decoder was set to only consider monotonic translation, since in real IMT scenarios considering non-monotonic translation leads to excessive response time for the user. [sent-132, score-0.208]
</p><p>58 Finally, the obtained word graphs were used within the IMT procedure to produce the reference translations in the test set, measuring WSR and BLEU. [sent-133, score-0.122]
</p><p>59 3 Results We carried out a series of experiments ranging the value of the sentence classification threshold τs, between 0. [sent-135, score-0.242]
</p><p>60 0 (equivalent to a fully automatic SMT system) and 1. [sent-136, score-0.047]
</p><p>61 0 (equivalent to a conventional IMT system), for both the MEAN and RATIO CMs. [sent-137, score-0.09]
</p><p>62 For each threshold value, we calculated the effort of the user in terms of WSR, and the translation quality of the final output as measured by BLEU. [sent-138, score-0.478]
</p><p>63 175  Figure 1 shows WSR (WSR IMT-CM) and BLEU (BLEU IMT-CM) scores obtained varying τs for the MEAN CM. [sent-139, score-0.062]
</p><p>64 Additionally, we also show the BLEU score (BLEU SMT) obtained by a fully automatic SMT system as translation quality baseline, and the WSR score (WSR IMT) obtained by a conventional IMT system as user effort baseline. [sent-140, score-0.564]
</p><p>65 This figure shows a continuous transition between the fully automatic SMT system and the conventional IMT system. [sent-141, score-0.188]
</p><p>66 This is an undesired effect, since for almost a half of the possible values for τs there is no change in the behaviour of our proposed IMT system. [sent-145, score-0.048]
</p><p>67 The RATIO CM confidence values depend on a word classification threshold τw. [sent-146, score-0.258]
</p><p>68 We have carried out experimentation ranging τw between 0. [sent-147, score-0.118]
</p><p>69 0 and found that this value can be used to solve the above mentioned undesired effect for the MEAN CM. [sent-149, score-0.079]
</p><p>70 Specifically, varying the value of τw we can stretch the interval in which the transition between the fully automatic SMT system and the conventional IMT system is produced, allowing us to obtain smother transitions. [sent-150, score-0.3]
</p><p>71 Figure 2 shows WSR and BLEU scores for different values of the sentence classification threshold τs using τw = 0. [sent-151, score-0.177]
</p><p>72 We show results only for this value of τw due to paper space limitations and because τw = 0. [sent-153, score-0.031]
</p><p>73 According to Figure 2, using a sentence classification threshold value of 0. [sent-155, score-0.179]
</p><p>74 6 we obtain a WSR reduction of 20% relative and an almost perfect translation quality of 87 BLEU points. [sent-156, score-0.209]
</p><p>75 It is worth of notice that the final translations are compared with only one reference, therefore, the reported translation quality scores are clearly pessimistic. [sent-157, score-0.295]
</p><p>76 Example 1 shows the source sentence (src), the reference translation (ref) and the final translation (tra) for three of the  initial fully automatically generated translations that were classified as correct by our CMs, and thus, were not interactively translated by the user. [sent-159, score-0.803]
</p><p>77 The first translation (tra-1) is identical to the corresponding reference translation (ref-1). [sent-160, score-0.337]
</p><p>78 The second translation (tra-2) corresponds to a correct translation of the source sentence (src-2) that is different from the corresponding reference (ref-2). [sent-161, score-0.442]
</p><p>79 Finally, the third translation (tra-3) is an example of a slightly incorrect translation. [sent-162, score-0.181]
</p><p>80 4 Concluding Remarks In this paper, we have presented a novel proposal that introduces sentence CMs into an IMT system to reduce user effort. [sent-163, score-0.262]
</p><p>81 Our proposal entails a modification of the user-machine interaction protocol that allows to achieve a balance between the user effort and the final translation error. [sent-164, score-0.506]
</p><p>82 We have carried out experimentation using two different sentence CMs. [sent-165, score-0.129]
</p><p>83 Varying the value of the sentence classification threshold, we can range from a fully automatic SMT system to a conventional IMT system. [sent-166, score-0.263]
</p><p>84 Empirical results show that  our proposal allows to obtain almost perfect translations while significantly reducing user effort. [sent-167, score-0.313]
</p><p>85 Acknowledgments Work supported by the EC (FEDER/FSE) and the Spanish MEC/MICINN under the MIPRCV “Consolider Ingenio 2010” program (CSD200700018), the iTransDoc (TIN2006-15694-CO2-01) and iTrans2 (TIN2009-1451 1) projects and the FPU scholarship AP2006-00691. [sent-169, score-0.021]
</p><p>86 Moses: Open source toolkit  for statistical machine translation. [sent-261, score-0.069]
</p><p>87 Thot: a toolkit to train phrase-based statistical translation models. [sent-283, score-0.19]
</p><p>88 Application of wordlevel confidence measures in interactive statistical machine translation. [sent-310, score-0.189]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('imt', 0.65), ('wsr', 0.337), ('cms', 0.295), ('cm', 0.19), ('translation', 0.155), ('confidence', 0.148), ('smt', 0.137), ('valencia', 0.116), ('interactively', 0.116), ('user', 0.114), ('ueffing', 0.106), ('bleu', 0.102), ('blatz', 0.098), ('barrachina', 0.096), ('conventional', 0.09), ('ei', 0.087), ('proposal', 0.083), ('gandrabur', 0.082), ('threshold', 0.08), ('effort', 0.077), ('thot', 0.072), ('foster', 0.071), ('polit', 0.063), ('classified', 0.062), ('ratio', 0.061), ('cw', 0.06), ('fj', 0.06), ('translations', 0.059), ('experimentation', 0.055), ('eu', 0.053), ('casacuberta', 0.051), ('nez', 0.051), ('saved', 0.049), ('translator', 0.049), ('civera', 0.048), ('eii', 0.048), ('esteban', 0.048), ('langlais', 0.048), ('ortiz', 0.048), ('transtype', 0.048), ('undesired', 0.048), ('fully', 0.047), ('spanish', 0.042), ('tolerated', 0.042), ('upv', 0.042), ('sist', 0.042), ('interactive', 0.041), ('estimation', 0.039), ('exceeds', 0.038), ('sentence', 0.038), ('mean', 0.037), ('graphs', 0.036), ('carried', 0.036), ('toolkit', 0.035), ('source', 0.034), ('spain', 0.034), ('ney', 0.034), ('correct', 0.033), ('fitzgerald', 0.033), ('sanchis', 0.033), ('varying', 0.033), ('decoder', 0.032), ('cr', 0.031), ('balance', 0.031), ('value', 0.031), ('classification', 0.03), ('goutte', 0.03), ('reducing', 0.03), ('scores', 0.029), ('initial', 0.028), ('perfect', 0.027), ('system', 0.027), ('reference', 0.027), ('quality', 0.027), ('ranging', 0.027), ('incorrect', 0.026), ('scenario', 0.025), ('ds', 0.025), ('final', 0.025), ('classifies', 0.025), ('translated', 0.024), ('sake', 0.024), ('transition', 0.024), ('mt', 0.022), ('de', 0.021), ('della', 0.021), ('excessive', 0.021), ('matic', 0.021), ('collaborate', 0.021), ('cus', 0.021), ('dure', 0.021), ('fpu', 0.021), ('gonz', 0.021), ('lapalme', 0.021), ('scholarship', 0.021), ('stretch', 0.021), ('modification', 0.021), ('papineni', 0.02), ('pietra', 0.02), ('moses', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="45-tfidf-1" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>Author: Jesus Gonzalez Rubio ; Daniel Ortiz Martinez ; Francisco Casacuberta</p><p>Abstract: This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.</p><p>2 0.18056455 <a title="45-tfidf-2" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from Nbest lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.</p><p>3 0.15620758 <a title="45-tfidf-3" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>4 0.11584479 <a title="45-tfidf-4" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>5 0.11259951 <a title="45-tfidf-5" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>6 0.11045263 <a title="45-tfidf-6" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>7 0.08708699 <a title="45-tfidf-7" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>8 0.086276777 <a title="45-tfidf-8" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>9 0.085683256 <a title="45-tfidf-9" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>10 0.080471061 <a title="45-tfidf-10" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>11 0.078165531 <a title="45-tfidf-11" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>12 0.076783143 <a title="45-tfidf-12" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>13 0.075900465 <a title="45-tfidf-13" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>14 0.071860254 <a title="45-tfidf-14" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>15 0.062458735 <a title="45-tfidf-15" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>16 0.06126475 <a title="45-tfidf-16" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>17 0.060946088 <a title="45-tfidf-17" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>18 0.060887031 <a title="45-tfidf-18" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>19 0.056976292 <a title="45-tfidf-19" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>20 0.055677753 <a title="45-tfidf-20" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.135), (1, -0.133), (2, -0.078), (3, -0.022), (4, 0.039), (5, -0.014), (6, -0.086), (7, -0.021), (8, -0.081), (9, 0.061), (10, 0.13), (11, 0.138), (12, 0.068), (13, -0.056), (14, 0.029), (15, 0.002), (16, -0.016), (17, 0.021), (18, -0.092), (19, 0.057), (20, -0.049), (21, -0.036), (22, 0.037), (23, 0.008), (24, -0.015), (25, -0.057), (26, 0.049), (27, 0.086), (28, 0.009), (29, 0.166), (30, 0.095), (31, 0.025), (32, 0.076), (33, 0.049), (34, 0.061), (35, -0.048), (36, 0.046), (37, -0.016), (38, 0.098), (39, -0.071), (40, -0.069), (41, 0.019), (42, -0.049), (43, 0.035), (44, -0.005), (45, 0.039), (46, 0.008), (47, 0.07), (48, -0.035), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94563484 <a title="45-lsi-1" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>Author: Jesus Gonzalez Rubio ; Daniel Ortiz Martinez ; Francisco Casacuberta</p><p>Abstract: This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.</p><p>2 0.80586332 <a title="45-lsi-2" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>Author: Yifan He ; Yanjun Ma ; Josef van Genabith ; Andy Way</p><p>Abstract: We propose a translation recommendation framework to integrate Statistical Machine Translation (SMT) output with Translation Memory (TM) systems. The framework recommends SMT outputs to a TM user when it predicts that SMT outputs are more suitable for post-editing than the hits provided by the TM. We describe an implementation of this framework using an SVM binary classifier. We exploit methods to fine-tune the classifier and investigate a variety of features of different types. We rely on automatic MT evaluation metrics to approximate human judgements in our experiments. Experimental results show that our system can achieve 0.85 precision at 0.89 recall, excluding ex- act matches. Furthermore, it is possible for the end-user to achieve a desired balance between precision and recall by adjusting confidence levels.</p><p>3 0.77765125 <a title="45-lsi-3" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>4 0.77462643 <a title="45-lsi-4" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>5 0.73544389 <a title="45-lsi-5" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from Nbest lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.</p><p>6 0.73029327 <a title="45-lsi-6" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>7 0.64470077 <a title="45-lsi-7" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>8 0.57975036 <a title="45-lsi-8" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>9 0.57527333 <a title="45-lsi-9" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>10 0.54087609 <a title="45-lsi-10" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>11 0.46412742 <a title="45-lsi-11" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>12 0.45074484 <a title="45-lsi-12" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>13 0.44693214 <a title="45-lsi-13" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>14 0.41510987 <a title="45-lsi-14" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>15 0.39645478 <a title="45-lsi-15" href="./acl-2010-Tools_for_Multilingual_Grammar-Based_Translation_on_the_Web.html">235 acl-2010-Tools for Multilingual Grammar-Based Translation on the Web</a></p>
<p>16 0.39110467 <a title="45-lsi-16" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>17 0.37984729 <a title="45-lsi-17" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>18 0.36460367 <a title="45-lsi-18" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>19 0.35625404 <a title="45-lsi-19" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>20 0.35340154 <a title="45-lsi-20" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(16, 0.02), (25, 0.033), (42, 0.012), (44, 0.016), (59, 0.116), (73, 0.507), (78, 0.016), (80, 0.01), (83, 0.061), (84, 0.015), (98, 0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9119429 <a title="45-lda-1" href="./acl-2010-WebLicht%3A_Web-Based_LRT_Services_for_German.html">259 acl-2010-WebLicht: Web-Based LRT Services for German</a></p>
<p>Author: Erhard Hinrichs ; Marie Hinrichs ; Thomas Zastrow</p><p>Abstract: This software demonstration presents WebLicht (short for: Web-Based Linguistic Chaining Tool), a webbased service environment for the integration and use of language resources and tools (LRT). WebLicht is being developed as part of the D-SPIN project1. WebLicht is implemented as a web application so that there is no need for users to install any software on their own computers or to concern themselves with the technical details involved in building tool chains. The integrated web services are part of a prototypical infrastructure that was developed to facilitate chaining of LRT services. WebLicht allows the integration and use of distributed web services with standardized APIs. The nature of these open and standardized APIs makes it possible to access the web services from nearly any programming language, shell script or workflow engine (UIMA, Gate etc.) Additionally, an application for integration of additional services is available, allowing anyone to contribute his own web service. 1</p><p>2 0.89373028 <a title="45-lda-2" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<p>Author: Sindhu Raghavan ; Adriana Kovashka ; Raymond Mooney</p><p>Abstract: In this paper, we present a novel approach for authorship attribution, the task of identifying the author of a document, using probabilistic context-free grammars. Our approach involves building a probabilistic context-free grammar for each author and using this grammar as a language model for classification. We evaluate the performance of our method on a wide range of datasets to demonstrate its efficacy.</p><p>same-paper 3 0.87314165 <a title="45-lda-3" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>Author: Jesus Gonzalez Rubio ; Daniel Ortiz Martinez ; Francisco Casacuberta</p><p>Abstract: This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.</p><p>4 0.86712915 <a title="45-lda-4" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>Author: Nikolaos Trogkanis ; Charles Elkan</p><p>Abstract: Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random fields. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages.</p><p>5 0.84758091 <a title="45-lda-5" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>Author: Ahmed Hassan ; Dragomir Radev</p><p>Abstract: Automatically identifying the polarity of words is a very important task in Natural Language Processing. It has applications in text classification, text filtering, analysis of product review, analysis of responses to surveys, and mining online discussions. We propose a method for identifying the polarity of words. We apply a Markov random walk model to a large word relatedness graph, producing a polarity estimate for any given word. A key advantage of the model is its ability to accurately and quickly assign a polarity sign and magnitude to any word. The method could be used both in a semi-supervised setting where a training set of labeled words is used, and in an unsupervised setting where a handful of seeds is used to define the two polarity classes. The method is experimentally tested using a manually labeled set of positive and negative words. It outperforms the state of the art methods in the semi-supervised setting. The results in the unsupervised setting is comparable to the best reported values. However, the proposed method is faster and does not need a large corpus.</p><p>6 0.81748527 <a title="45-lda-6" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>7 0.78256732 <a title="45-lda-7" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>8 0.58030498 <a title="45-lda-8" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>9 0.57615656 <a title="45-lda-9" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>10 0.54631478 <a title="45-lda-10" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>11 0.54028773 <a title="45-lda-11" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>12 0.52996486 <a title="45-lda-12" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>13 0.52952945 <a title="45-lda-13" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>14 0.52895641 <a title="45-lda-14" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>15 0.52743423 <a title="45-lda-15" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>16 0.52692294 <a title="45-lda-16" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>17 0.52060908 <a title="45-lda-17" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>18 0.51647538 <a title="45-lda-18" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>19 0.51166576 <a title="45-lda-19" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>20 0.51079792 <a title="45-lda-20" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
