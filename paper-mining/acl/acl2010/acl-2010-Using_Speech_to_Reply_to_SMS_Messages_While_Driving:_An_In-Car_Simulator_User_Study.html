<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-254" href="#">acl2010-254</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</h1>
<br/><p>Source: <a title="acl-2010-254-pdf" href="http://aclweb.org/anthology//P/P10/P10-2058.pdf">pdf</a></p><p>Author: Yun-Cheng Ju ; Tim Paek</p><p>Abstract: Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search. 1</p><p>Reference: <a title="acl-2010-254-reference" href="../acl2010_reference/acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. [sent-2, score-0.195]
</p><p>2 Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. [sent-3, score-0.665]
</p><p>3 Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. [sent-4, score-1.664]
</p><p>4 Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using  dictation than voice search. [sent-5, score-1.199]
</p><p>5 1  Introduction  Users love Short Message Service (SMS) text messaging; so much so that 3 trillion SMS messages are expected to have been sent in 2009 alone (Stross, 2008). [sent-6, score-0.108]
</p><p>6 Because research has shown that SMS messaging while driving results in 35% slower reaction time than being intoxicated (Reed & Robbins, 2008), campaigns have been launched by states, governments and even cell phone carriers to discourage and ban SMS messaging while driving (DOT, 2009). [sent-7, score-0.93]
</p><p>7 Yet, automobile manufacturers have started to offer infotainment systems, such as the Ford Sync, which feature the ability to listen to incoming SMS messages using text-to-speech (TTS). [sent-8, score-0.188]
</p><p>8 Automatic speech recognition (ASR) affords users a hands-free, eyes-free method of replying to SMS messages. [sent-9, score-0.272]
</p><p>9 However, to date, manufacturers have not established a safe and reliable method of leveraging ASR, though some researchers have begun to explore techniques. [sent-10, score-0.025]
</p><p>10 In previous research (Ju & Paek, 2009), we examined three  ASR approaches to replying to SMS messages: dictation using a language model trained on SMS responses, canned responses using a probabilistic context-free grammar (PCFG), and a “voice search” approach based on template matching. [sent-11, score-0.632]
</p><p>11 Voice search proceeds in two steps (Natarajan et al. [sent-12, score-0.044]
</p><p>12 , 2002): an utterance is first converted into text, which is then used as a search query to match the most similar items of an index using IR techniques (Yu et al. [sent-13, score-0.069]
</p><p>13 For SMS replies, we created an index of SMS response templates, with slots for semantic concepts such as time and place, from a large SMS corpus. [sent-15, score-0.043]
</p><p>14 After convolving recorded SMS replies so that the audio would exhibit the acoustic characteristics of in-car recognition, they compared how the three approaches handled the convolved audio with respect to the top n-best reply candidates. [sent-16, score-0.521]
</p><p>15 The voice search approach consistently outperformed dictation and canned responses, achieving as high as 89. [sent-17, score-0.735]
</p><p>16 7% task completion with respect to the top 5 reply candidates. [sent-18, score-0.304]
</p><p>17 Even if the voice search approach may be more robust to in-car noise, this does not guarantee that it will be more usable. [sent-19, score-0.251]
</p><p>18 Indeed, because  voice search can only match semantic concepts contained in the templates (which may or may not utilize the same wording as the reply), users must verify that a retrieved template matches the semantics of their intended reply. [sent-20, score-0.495]
</p><p>19 For example, suppose a user replies to the SMS message “how about lunch” with “can ’t right now running errands”. [sent-21, score-0.222]
</p><p>20 Voice search may find “nope, got errands to run” as the closest template match, in which case, users will have to decide whether this response has the same meaning as their reply. [sent-22, score-0.321]
</p><p>21 This of course entails cognitive effort, which is very limited in the context of driving. [sent-23, score-0.069]
</p><p>22 On the other hand, a dictation approach to replying to SMS messages may be far worse due to misrecognitions. [sent-24, score-0.627]
</p><p>23 For example, dictation may interpret “can ’t right now running errands” as “can right 313  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-25, score-0.453]
</p><p>24 We posited that voice search has the advantage because it always generates intelligible SMS replies (since response templates are manually filtered), as opposed to dictation, which can sometimes result in unpre-  dictable and nonsensical misrecognitions. [sent-28, score-0.557]
</p><p>25 However, this advantage has not been empirically demonstrated in a user study. [sent-29, score-0.025]
</p><p>26 This paper presents a user study investigating how the two approaches compare when users are actually driving – that is, when usability matters most. [sent-30, score-0.566]
</p><p>27 2  Driving Simulator Study  Although ASR affords users hands-free, eyesfree interaction, the benefits of leveraging speech can be forfeit if users are expending cognitive effort judging whether the speech interface correctly interpreted their utterances. [sent-31, score-0.403]
</p><p>28 Indeed, research has shown that the cognitive demands of dialogue seem to play a more important role in distracting drivers than physically handling cell phones (Nunes & Recarte, 2002; Strayer & Johnston, 2001). [sent-32, score-0.12]
</p><p>29 (2007) have found that when in-car speech interfaces encounter recognition problems, users tend to drive more dangerously as they attempt to figure out why their utterances are failing. [sent-34, score-0.189]
</p><p>30 Hence, any approach to replying to SMS messages in automobiles must avoid distracting drivers with er-  ×  rors and be highly usable while users are engaged in their primary task, driving. [sent-35, score-0.474]
</p><p>31 1  Method  To assess the usability and performance of both the voice search approach and dictation, we conducted a controlled experiment using the STISIM Drive™ simulator. [sent-37, score-0.289]
</p><p>32 We recruited 16 participants (9 males, 7 females) through an email sent to employees of our organization. [sent-40, score-0.057]
</p><p>33 All participants had a driver’s license and were compensated for their time. [sent-43, score-0.068]
</p><p>34 We examined two independent variables: SMS Reply Approach, consisting of voice search and dictation, and Driving Condition, consisting of no driving, easy driving and difficult driving. [sent-44, score-0.677]
</p><p>35 We included Driving Condition as a way of increasing cognitive demand (see next section). [sent-45, score-0.031]
</p><p>36 Overall, we conducted a 2 (SMS Reply Approach) 3 (Driving Condition) repeated measures, within-  subjects design experiment in which the order of SMS Reply for each Driving Condition was counter-balanced. [sent-46, score-0.018]
</p><p>37 Because our primary variable of interest was SMS Reply, we had users experience both voice search and dictation with no driving first, then easy driving, followed by difficult driving. [sent-47, score-1.262]
</p><p>38 This gave users a chance to adjust themselves to increasingly difficult road conditions. [sent-48, score-0.193]
</p><p>39 Driving Task: As the primary task, users were asked to drive two courses we developed with  easy driving and difficult driving conditions while obeying all rules of the road, as they would in real driving and not in a videogame. [sent-49, score-1.431]
</p><p>40 With speed limits ranging from 25 mph to 55 mph, both courses contained five sequential sections which took about 15-20 minutes to complete: a residential area, a country highway, and a small city with a downtown area as well as a business/industrial park. [sent-50, score-0.088]
</p><p>41 Although both courses were almost identical in the number of turns, curves, stops, and traffic lights, the easy course consisted mostly of simple road segments with relatively no traffic, whereas the difficult course had four times as many vehicles, cyclists, and pedestrians. [sent-51, score-0.252]
</p><p>42 The difficult course also included a foggy road section, a few busy construction sites, and many unexpected events, such as a car in front suddenly breaking, a parked car merging into traffic, and a pedestrian jaywalking. [sent-52, score-0.099]
</p><p>43 In short, the difficult course was designed to fully engage the attention and cognitive resources of drivers. [sent-53, score-0.088]
</p><p>44 SMS Reply Task: As the secondary task, we asked users to listen to an incoming SMS message together with a formulated reply, such as:  (1) Message Received: “Are you lost? [sent-54, score-0.231]
</p><p>45 ” Your Reply: “No, never with my GPS” The users were asked to repeat the reply back to the system. [sent-55, score-0.489]
</p><p>46 For Example (1) above, users would have to utter “No, never with my GPS”. [sent-56, score-0.16]
</p><p>47 Users 314  could also say “Repeat” if they had any difficulties understanding the TTS rendering or if they experienced lapses in attention. [sent-57, score-0.064]
</p><p>48 For each course, users engaged in 10 SMS reply tasks. [sent-58, score-0.463]
</p><p>49 SMS messages were cued every 3000 feet, roughly every 90 seconds, which provided enough time to complete each SMS dialogue. [sent-59, score-0.092]
</p><p>50 Once users uttered the formulated reply, they received a list of 4 possible reply candidates (each labeled as “One”, “Two”, etc. [sent-60, score-0.519]
</p><p>51 ), from which they were asked to either pick the correct reply (by stating its number at any time) or reject them all (by stating “All wrong”). [sent-61, score-0.421]
</p><p>52 We did not provide any feedback about whether the replies they picked were correct or incorrect in order to avoid priming users to pay more or less attention in subsequent messages. [sent-62, score-0.327]
</p><p>53 Users did not have to finish listening to the entire list before making their selection. [sent-63, score-0.046]
</p><p>54 However, all SMS replies and 4-best lists were derived from the logs of an actual SMS Reply interface which implemented the dictation and the voice search approaches (see Ju & Paek, 2009). [sent-65, score-0.906]
</p><p>55 For each course, 5 of the SMS replies were short (with 3 or fewer words) and 5 were long (with 4 to 7 words). [sent-66, score-0.18]
</p><p>56 The order of the short and long replies was randomized. [sent-70, score-0.18]
</p><p>57 We selected 4-best lists where the correct answer was in each of four possible positions (1-4) or All Wrong; that is, there were as many 4-best lists with the first choice correct as there were with the second choice correct, and so forth. [sent-71, score-0.137]
</p><p>58 , 1best), and second, our experimental design sought to identify whether the voice search approach was more usable than the dictation approach even when the ASR accuracy of the two approaches was the same. [sent-75, score-0.726]
</p><p>59 In the dictation condition, the correct answer was not always an exact copy of the reply in 0-2 of the 10 SMS messages. [sent-76, score-0.836]
</p><p>60 For instance, a correct dictation answer for Example (1) above was “no I’m never with my GPS”. [sent-77, score-0.542]
</p><p>61 On the other hand, the voice search condition had more cases (2-4 messages) in which the correct answer was not an exact copy (e. [sent-78, score-0.371]
</p><p>62 , “no I have GPS”) due to the nature of the template approach. [sent-80, score-0.031]
</p><p>63 To some degree, this could be seen as handicapping the voice search condition, though the results did not re-  flect the disadvantage, as we discuss later. [sent-81, score-0.251]
</p><p>64 Measures: Performance for both the driving task and the SMS reply tasks were recorded. [sent-82, score-0.693]
</p><p>65 For the driving task, we measured the numbers of collisions, speeding (exceeding 10 mph above the limit), traffic light and stop sign violations, and missed or incorrect turns. [sent-83, score-0.48]
</p><p>66 , time elapsed between the beginning of the 4-best list and when users ultimately provided their answer) and the number of times users correctly identified which of the 4 reply candidates contained the correct answer. [sent-86, score-0.663]
</p><p>67 Originally, we had an independent rater verify the position of the correct answer in all 4-best lists, however, we considered that some participants might be choosing replies that are semantically sufficient, even if they are not exactly correct. [sent-87, score-0.29]
</p><p>68 For example, a 4-best list generated by the dictation approach for Example (1) had: “One: no I’m never want my GPS. [sent-88, score-0.505]
</p><p>69 ” Although the rater identified the second reply as being  “correct”, a participant might view the first or third replies as sufficient. [sent-92, score-0.492]
</p><p>70 Participants were explicitly told that they could select multiple items from the 4-best list. [sent-94, score-0.025]
</p><p>71 In computing the number of “correct” answers, for each SMS reply, we counted an an315  swer to be correct if it was included among the participants’ set of semantically sufficient 4-best list items. [sent-97, score-0.056]
</p><p>72 Hence, we calculated the number of correct items in a personalized fashion for every  participant. [sent-98, score-0.057]
</p><p>73 2  Results  We conducted a series of repeated measures ANOVAs on all driving task and SMS reply task measures. [sent-100, score-0.711]
</p><p>74 For the driving task, we did not find any statistically significant differences between the voice search and dictation conditions. [sent-101, score-1.093]
</p><p>75 In other words, we could not reject the null hypothesis that the two approaches were the same in terms of their influence on driving performance. [sent-102, score-0.409]
</p><p>76 However, for the SMS reply task, we did find a main effect for SMS Reply Approach (F1,47 = 81. [sent-103, score-0.304]
</p><p>77 As shown in Figure 2, the average number of errors per driving course for dictation is roughly 6 times that for voice search. [sent-110, score-1.105]
</p><p>78 We also found a main effect for total duration (F1,47 = 11. [sent-111, score-0.069]
</p><p>79 We discuss our explanation for the shorter duration below. [sent-120, score-0.069]
</p><p>80 For both errors and duration, we did not find any interaction effects with Driving Conditions. [sent-121, score-0.018]
</p><p>81 =  3  Discussion  We conducted a simulator study in order to examine which was worse while driving: verifying whether SMS response templates matched the meaning of an intended reply, or deciphering the sometimes nonsensical misrecognitions of dictation. [sent-122, score-0.395]
</p><p>82 Our results suggest that deciphering dictation results under the duress of driving leads to more errors. [sent-123, score-0.88]
</p><p>83 In conducting a post-hoc error analysis, we noticed that participants tended to err when the 4-best lists generated by the dictation approach contained phonetically similar candidate replies. [sent-124, score-0.538]
</p><p>84 The voice search approach circumvents this problem in two ways: 1) templates were real responses and manually selected and cleaned up during the development phase so there were no grammatical mistakes,  and 2) semantically redundant templates can be  Figure 2. [sent-126, score-0.392]
</p><p>85 Mean number of errors for the dictation and voice search approaches. [sent-127, score-0.722]
</p><p>86 further discarded to only present the distinct concepts at the rendering time using the paraphrase detection algorithms reported in (Wu et al. [sent-129, score-0.036]
</p><p>87 However, in our error analysis we observed that most likely users did not discover the misrecognitions, and prematurely selected a reply candidate, resulting in shorter durations. [sent-132, score-0.436]
</p><p>88 The slightly higher duration for the voice search approach does not constitute a problem if users are listening to all of their choices and correctly selecting their intended SMS reply. [sent-133, score-0.518]
</p><p>89 Note that the duration did not bring about any significant driving performance differences. [sent-134, score-0.458]
</p><p>90 Although we did not find any significant driving performance differences, users experienced more difficulties confirming whether the dictation approach correctly interpreted their utterances than they did with the voice search approach. [sent-135, score-1.285]
</p><p>91 As such, if a user deems it absolutely necessary to respond to SMS messages while driving, our simulator study suggests that the most reliable (i. [sent-136, score-0.204]
</p><p>92 , least error-prone) way to respond may just well be the voice search approach. [sent-138, score-0.271]
</p><p>93 “What carriers aren’t eager to tell you about texting”, New York Times, Dec. [sent-192, score-0.025]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sms', 0.563), ('dictation', 0.453), ('driving', 0.389), ('reply', 0.304), ('voice', 0.207), ('replies', 0.163), ('users', 0.132), ('messages', 0.092), ('ju', 0.082), ('replying', 0.082), ('paek', 0.078), ('duration', 0.069), ('simulator', 0.067), ('errands', 0.055), ('messaging', 0.055), ('templates', 0.053), ('gps', 0.05), ('traffic', 0.05), ('asr', 0.049), ('automobiles', 0.047), ('courses', 0.047), ('misrecognitions', 0.047), ('nonsensical', 0.047), ('search', 0.044), ('response', 0.043), ('road', 0.042), ('condition', 0.041), ('mph', 0.041), ('affords', 0.041), ('drivers', 0.041), ('participants', 0.041), ('course', 0.038), ('deciphering', 0.038), ('verifying', 0.038), ('responses', 0.035), ('message', 0.034), ('correct', 0.032), ('cognitive', 0.031), ('automobile', 0.031), ('canned', 0.031), ('distracting', 0.031), ('natarajan', 0.031), ('nunes', 0.031), ('strayer', 0.031), ('voicesearch', 0.031), ('template', 0.031), ('answer', 0.029), ('never', 0.028), ('intended', 0.028), ('engaged', 0.027), ('compensated', 0.027), ('directory', 0.027), ('asked', 0.025), ('items', 0.025), ('manufacturers', 0.025), ('reed', 0.025), ('driver', 0.025), ('kun', 0.025), ('carriers', 0.025), ('rater', 0.025), ('user', 0.025), ('list', 0.024), ('drive', 0.023), ('candidates', 0.023), ('listening', 0.022), ('listen', 0.022), ('phonetically', 0.022), ('experienced', 0.022), ('usable', 0.022), ('tts', 0.022), ('lists', 0.022), ('difficulties', 0.022), ('assistance', 0.021), ('respond', 0.02), ('usability', 0.02), ('reject', 0.02), ('rendering', 0.02), ('stating', 0.02), ('acoustic', 0.02), ('difficult', 0.019), ('uttered', 0.019), ('sec', 0.018), ('errors', 0.018), ('copy', 0.018), ('incoming', 0.018), ('conducted', 0.018), ('easy', 0.018), ('speech', 0.017), ('cell', 0.017), ('short', 0.017), ('interface', 0.017), ('audio', 0.017), ('experimentally', 0.017), ('interfaces', 0.017), ('icassp', 0.017), ('received', 0.017), ('meaning', 0.016), ('correctly', 0.016), ('paraphrase', 0.016), ('sent', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="254-tfidf-1" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>Author: Yun-Cheng Ju ; Tim Paek</p><p>Abstract: Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search. 1</p><p>2 0.076092266 <a title="254-tfidf-2" href="./acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</a></p>
<p>Author: Mikko Kurimo ; William Byrne ; John Dines ; Philip N. Garner ; Matthew Gibson ; Yong Guan ; Teemu Hirsimaki ; Reima Karhila ; Simon King ; Hui Liang ; Keiichiro Oura ; Lakshmi Saheer ; Matt Shannon ; Sayaki Shiota ; Jilei Tian</p><p>Abstract: In the EMIME project we have studied unsupervised cross-lingual speaker adaptation. We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms to adapt the synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition). An important application for this research is personalised speech-to-speech translation that will use the voice of the speaker in the input language to utter the translated sentences in the output language. In mobile environments this enhances the users’ interaction across language barriers by making the output speech sound more like the original speaker’s way of speaking, even if she or he could not speak the output language.</p><p>3 0.075964399 <a title="254-tfidf-3" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>Author: Jessica Villing</p><p>Abstract: In-vehicle dialogue systems often contain more than one application, e.g. a navigation and a telephone application. This means that the user might, for example, interrupt the interaction with the telephone application to ask for directions from the navigation application, and then resume the dialogue with the telephone application. In this paper we present an analysis of interruption and resumption behaviour in human-human in-vehicle dialogues and also propose some implications for resumption strategies in an in-vehicle dialogue system.</p><p>4 0.048709903 <a title="254-tfidf-4" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>Author: Marie-Catherine de Marneffe ; Christopher D. Manning ; Christopher Potts</p><p>Abstract: Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.</p><p>5 0.047361966 <a title="254-tfidf-5" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>Author: Jia Wang ; Qing Li ; Yuanzhu Peter Chen ; Zhangxi Lin</p><p>Abstract: The variety of engaging interactions among users in social medial distinguishes it from traditional Web media. Such a feature should be utilized while attempting to provide intelligent services to social media participants. In this article, we present a framework to recommend relevant information in Internet forums and blogs using user comments, one of the most representative of user behaviors in online discussion. When incorporating user comments, we consider structural, semantic, and authority information carried by them. One of the most important observation from this work is that semantic contents of user comments can play a fairly different role in a different form of social media. When designing a recommendation system for this purpose, such a difference must be considered with caution.</p><p>6 0.046182141 <a title="254-tfidf-6" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>7 0.037173025 <a title="254-tfidf-7" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>8 0.035712156 <a title="254-tfidf-8" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>9 0.034899641 <a title="254-tfidf-9" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>10 0.034216758 <a title="254-tfidf-10" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>11 0.033519316 <a title="254-tfidf-11" href="./acl-2010-Modeling_Norms_of_Turn-Taking_in_Multi-Party_Conversation.html">173 acl-2010-Modeling Norms of Turn-Taking in Multi-Party Conversation</a></p>
<p>12 0.031242473 <a title="254-tfidf-12" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>13 0.029281303 <a title="254-tfidf-13" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>14 0.028715191 <a title="254-tfidf-14" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>15 0.027317515 <a title="254-tfidf-15" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>16 0.025442366 <a title="254-tfidf-16" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>17 0.025037881 <a title="254-tfidf-17" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>18 0.022912543 <a title="254-tfidf-18" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>19 0.022905879 <a title="254-tfidf-19" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>20 0.021353243 <a title="254-tfidf-20" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.06), (1, 0.029), (2, -0.033), (3, -0.055), (4, 0.003), (5, -0.058), (6, -0.032), (7, 0.015), (8, -0.012), (9, -0.007), (10, -0.005), (11, 0.02), (12, -0.008), (13, -0.037), (14, -0.006), (15, 0.017), (16, -0.017), (17, -0.024), (18, -0.003), (19, 0.02), (20, -0.008), (21, -0.038), (22, 0.065), (23, -0.013), (24, 0.011), (25, 0.034), (26, -0.011), (27, -0.019), (28, -0.012), (29, -0.03), (30, -0.021), (31, 0.071), (32, 0.018), (33, 0.016), (34, 0.02), (35, -0.057), (36, -0.032), (37, -0.013), (38, -0.014), (39, 0.069), (40, -0.066), (41, 0.032), (42, -0.008), (43, -0.019), (44, -0.16), (45, -0.039), (46, 0.139), (47, -0.036), (48, 0.081), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94855666 <a title="254-lsi-1" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>Author: Yun-Cheng Ju ; Tim Paek</p><p>Abstract: Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search. 1</p><p>2 0.64171505 <a title="254-lsi-2" href="./acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</a></p>
<p>Author: Mikko Kurimo ; William Byrne ; John Dines ; Philip N. Garner ; Matthew Gibson ; Yong Guan ; Teemu Hirsimaki ; Reima Karhila ; Simon King ; Hui Liang ; Keiichiro Oura ; Lakshmi Saheer ; Matt Shannon ; Sayaki Shiota ; Jilei Tian</p><p>Abstract: In the EMIME project we have studied unsupervised cross-lingual speaker adaptation. We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms to adapt the synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition). An important application for this research is personalised speech-to-speech translation that will use the voice of the speaker in the input language to utter the translated sentences in the output language. In mobile environments this enhances the users’ interaction across language barriers by making the output speech sound more like the original speaker’s way of speaking, even if she or he could not speak the output language.</p><p>3 0.51393092 <a title="254-lsi-3" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>Author: Daniil Umanski ; Federico Sangati</p><p>Abstract: The growing availability of spoken language corpora presents new opportunities for enriching the methodologies of speech and language therapy. In this paper, we present a novel approach for constructing speech motor exercises, based on linguistic knowledge extracted from spoken language corpora. In our study with the Dutch Spoken Corpus, syllabic inventories were obtained by means of automatic syllabification of the spoken language data. Our experimental syllabification method exhibited a reliable performance, and allowed for the acquisition of syllabic tokens from the corpus. Consequently, the syl- labic tokens were integrated in a tool for clinicians, a result which holds the potential of contributing to the current state of speech motor training methodologies.</p><p>4 0.47032341 <a title="254-lsi-4" href="./acl-2010-Comparable_Entity_Mining_from_Comparative_Questions.html">63 acl-2010-Comparable Entity Mining from Comparative Questions</a></p>
<p>Author: Shasha Li ; Chin-Yew Lin ; Young-In Song ; Zhoujun Li</p><p>Abstract: Comparing one thing with another is a typical part of human decision making process. However, it is not always easy to know what to compare and what are the alternatives. To address this difficulty, we present a novel way to automatically mine comparable entities from comparative questions that users posted online. To ensure high precision and high recall, we develop a weakly-supervised bootstrapping method for comparative question identification and comparable entity extraction by leveraging a large online question archive. The experimental results show our method achieves F1measure of 82.5% in comparative question identification and 83.3% in comparable entity extraction. Both significantly outperform an existing state-of-the-art method. 1</p><p>5 0.45734644 <a title="254-lsi-5" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>Author: Jessica Villing</p><p>Abstract: In-vehicle dialogue systems often contain more than one application, e.g. a navigation and a telephone application. This means that the user might, for example, interrupt the interaction with the telephone application to ask for directions from the navigation application, and then resume the dialogue with the telephone application. In this paper we present an analysis of interruption and resumption behaviour in human-human in-vehicle dialogues and also propose some implications for resumption strategies in an in-vehicle dialogue system.</p><p>6 0.4554238 <a title="254-lsi-6" href="./acl-2010-Correcting_Errors_in_Speech_Recognition_with_Articulatory_Dynamics.html">74 acl-2010-Correcting Errors in Speech Recognition with Articulatory Dynamics</a></p>
<p>7 0.44613618 <a title="254-lsi-7" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>8 0.42304295 <a title="254-lsi-8" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>9 0.41060859 <a title="254-lsi-9" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>10 0.39635536 <a title="254-lsi-10" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>11 0.39533865 <a title="254-lsi-11" href="./acl-2010-Domain_Adaptation_of_Maximum_Entropy_Language_Models.html">91 acl-2010-Domain Adaptation of Maximum Entropy Language Models</a></p>
<p>12 0.37867591 <a title="254-lsi-12" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>13 0.36671603 <a title="254-lsi-13" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>14 0.3552146 <a title="254-lsi-14" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<p>15 0.34976771 <a title="254-lsi-15" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>16 0.34569615 <a title="254-lsi-16" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>17 0.33846474 <a title="254-lsi-17" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>18 0.33146271 <a title="254-lsi-18" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>19 0.32367489 <a title="254-lsi-19" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>20 0.31672662 <a title="254-lsi-20" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.011), (25, 0.035), (26, 0.066), (39, 0.026), (42, 0.037), (44, 0.017), (59, 0.375), (72, 0.013), (73, 0.027), (78, 0.021), (80, 0.01), (83, 0.075), (84, 0.035), (88, 0.011), (98, 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97647911 <a title="254-lda-1" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>2 0.9721635 <a title="254-lda-2" href="./acl-2010-Intelligent_Selection_of_Language_Model_Training_Data.html">151 acl-2010-Intelligent Selection of Language Model Training Data</a></p>
<p>Author: Robert C. Moore ; William Lewis</p><p>Abstract: We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.</p><p>3 0.97157526 <a title="254-lda-3" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>Author: Galina Tremper</p><p>Abstract: Presupposition relations between verbs are not very well covered in existing lexical semantic resources. We propose a weakly supervised algorithm for learning presupposition relations between verbs that distinguishes five semantic relations: presupposition, entailment, temporal inclusion, antonymy and other/no relation. We start with a number of seed verb pairs selected manually for each semantic relation and classify unseen verb pairs. Our algorithm achieves an overall accuracy of 36% for type-based classification.</p><p>4 0.96701109 <a title="254-lda-4" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>Author: Chris Dyer ; Adam Lopez ; Juri Ganitkevitch ; Jonathan Weese ; Ferhan Ture ; Phil Blunsom ; Hendra Setiawan ; Vladimir Eidelman ; Philip Resnik</p><p>Abstract: Adam Lopez University of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</p><p>5 0.96573722 <a title="254-lda-5" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>Author: Joern Wuebker ; Arne Mauser ; Hermann Ney</p><p>Abstract: Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overfitting. We describe a novel leavingone-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering mod- els in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.</p><p>same-paper 6 0.91268301 <a title="254-lda-6" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>7 0.89874947 <a title="254-lda-7" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>8 0.83622074 <a title="254-lda-8" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>9 0.83033139 <a title="254-lda-9" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>10 0.822523 <a title="254-lda-10" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>11 0.8189171 <a title="254-lda-11" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>12 0.81603497 <a title="254-lda-12" href="./acl-2010-Domain_Adaptation_of_Maximum_Entropy_Language_Models.html">91 acl-2010-Domain Adaptation of Maximum Entropy Language Models</a></p>
<p>13 0.81345761 <a title="254-lda-13" href="./acl-2010-BabelNet%3A_Building_a_Very_Large_Multilingual_Semantic_Network.html">44 acl-2010-BabelNet: Building a Very Large Multilingual Semantic Network</a></p>
<p>14 0.81146103 <a title="254-lda-14" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>15 0.80767208 <a title="254-lda-15" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>16 0.80245131 <a title="254-lda-16" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>17 0.79879212 <a title="254-lda-17" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>18 0.79525769 <a title="254-lda-18" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>19 0.79403436 <a title="254-lda-19" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>20 0.7921977 <a title="254-lda-20" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
