<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-19" href="#">acl2010-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</h1>
<br/><p>Source: <a title="acl-2010-19-pdf" href="http://aclweb.org/anthology//P/P10/P10-1070.pdf">pdf</a></p><p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>Reference: <a title="acl-2010-19-reference" href="../acl2010_reference/acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. [sent-2, score-0.517]
</p><p>2 In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation. [sent-4, score-0.902]
</p><p>3 The interpretation ofnoun compounds is a difficult problem for various reasons (Spärck Jones, 1983). [sent-9, score-0.485]
</p><p>4 Regardless, automatic noun compound interpretation is the focus of an upcoming SEMEVAL task (Butnariu et al. [sent-11, score-0.638]
</p><p>5 Earlier work has often suffered from using taxonomies with coarse-grained, highly ambiguous predicates, such as prepositions, as various labels (Lauer, 1995) and/or unimpressive inter-annotator agreement among human judges (Kim and Baldwin, 2005). [sent-14, score-0.214]
</p><p>6 In addition, the datasets annotated according to these various schemes have often been too small to provide wide coverage of the noun compounds likely to occur in general text. [sent-15, score-0.626]
</p><p>7 In this paper, we present a large, fine-grained taxonomy of 43 noun compound relations, a dataset annotated according to this taxonomy, and a supervised, automatic classification method for determining the relation between the head and modifier words in a noun compound. [sent-16, score-1.142]
</p><p>8 We compare and map our relations to those in other taxonomies and report the promising results of an inter-annotator agreement study as well as an automatic classification experiment. [sent-17, score-0.368]
</p><p>9 Our dataset is, to the best of our knowledge, the largest noun compound dataset yet produced. [sent-19, score-0.759]
</p><p>10 1 Taxonomies The relations between the component nouns in noun compounds have been the subject of various linguistic studies performed throughout the years, including early work by Jespersen (1949). [sent-24, score-0.71]
</p><p>11 Lees created an early taxonomy based primarily upon grammar (Lees, 1960). [sent-26, score-0.234]
</p><p>12 Levi’s influential work postulated that complex nominals (Levi’s name for noun compounds that also permits certain adjectival modifiers) are all derived either via nominalization or 678  ProceedinUgspp osfa tlhae, 4S8wthed Aennn,u 1a1l-1 M6e Jeutilnyg 2 o0f1 t0h. [sent-27, score-0.626]
</p><p>13 Of the taxonomies presented by purely linguistic studies, our categories are most similar to those proposed by Warren (1978), whose categories (e. [sent-32, score-0.326]
</p><p>14 In contrast to studies that claim the existence of a relatively small number of semantic relations, Downing (1977) presents a strong case for the existence of an unbounded number of relations. [sent-35, score-0.136]
</p><p>15 While we agree with Downing’s belief that the number of relations is unbounded, we contend that the vast majority of noun compounds fits within a relatively small set of categories. [sent-36, score-0.668]
</p><p>16 Others use categories similar to Levi’s, such as Lauer’s (1995) set of prepositional paraphrases (i. [sent-39, score-0.114]
</p><p>17 , OF, FOR, IN, ON, AT, FROM, WITH,  ABOUT) to analyze noun compounds. [sent-41, score-0.208]
</p><p>18 , 2005; Kim and Baldwin, 2005) use sets of categories that are somewhat more similar to those proposed by Warren (1978). [sent-45, score-0.114]
</p><p>19 While most of the noun compound research to date is not domain specific, Rosario and Hearst (2001) create and experiment with a taxonomy tailored to biomedical text. [sent-46, score-0.712]
</p><p>20 Rosario and Hearst (2001) utilize neural networks to classify compounds according to their domain-specific relation taxonomy. [sent-53, score-0.388]
</p><p>21 Séaghdha and Copestake (2009) use SVMs and experiment with kernel methods on a dataset labeled using a relatively small taxonomy. [sent-60, score-0.107]
</p><p>22 1 Creation  Given the heterogeneity of past work, we decided to start fresh and build a new taxonomy of relations using naturally occurring noun pairs, and then compare the result to earlier relation sets. [sent-63, score-0.483]
</p><p>23 We collected 17509 noun pairs and over a period of 10 months assigned one or more relations to  each, gradually building and refining our taxonomy. [sent-64, score-0.28]
</p><p>24 More details regarding the dataset are provided in Section 4. [sent-65, score-0.107]
</p><p>25 The relations we produced were then compared to those present in other taxonomies (e. [sent-66, score-0.17]
</p><p>26 We tested the relation set with an initial inter-annotator agreement study (our latest interannotator agreement study results are presented in Section 6). [sent-72, score-0.274]
</p><p>27 However, the mediocre results indicated that the categories and/or their definitions needed refinement. [sent-73, score-0.152]
</p><p>28 Mechanical Turk has been previously used in a variety of NLP research, including recent work on noun compounds by Nakov (2008) to collect short  phrases for linking the nouns within noun compounds. [sent-75, score-0.846]
</p><p>29 For the Mechanical Turk annotation tests, we created five sets of 100 noun compounds from noun compounds automatically extracted from a random subset of New York Times articles written between 1987 and 2007 (Sandhaus, 2008). [sent-76, score-1.192]
</p><p>30 Turkers  were  asked  to  select  one or, if they deemed it appropriate, two categories for each noun pair. [sent-129, score-0.353]
</p><p>31 , the creation, deletion, and/or modification of categories) were incorporated into the taxonomy before the next set of 100 was uploaded. [sent-132, score-0.203]
</p><p>32 For example, the SUBSTANCE category has the definition n1 is one of the primary physical substances/materials/ingredients that n2 is made/composed out of/from. [sent-137, score-0.095]
</p><p>33 Defining the categories with sentences is advantageous because it is possible to create straightforward, explicit defintions that humans can easily test examples against. [sent-139, score-0.114]
</p><p>34 3 Taxonomy Groupings In addition to influencing the category definitions, some taxonomy groupings were altered with the hope that this would improve inter-annotator agreement for cases where Turker disagreement was systematic. [sent-141, score-0.414]
</p><p>35 The ambiguity between these categories has previously been observed by Girju (2009). [sent-143, score-0.114]
</p><p>36 Turkers also tended to disagree between the categories related to composition and containment. [sent-144, score-0.114]
</p><p>37 The ATTRIBUTE categories are positioned near the TOPIC group because some Turkers chose a TOPIC category when an ATTRIBUTE category was deemed more appropriate. [sent-146, score-0.335]
</p><p>38 4 Contrast with other Taxonomies In order to ensure completeness, we mapped into our taxonomy the relations proposed in most previous work including those of Barker and Szpakowicz (1998) and Girju et al. [sent-150, score-0.275]
</p><p>39 The results, shown in Table 1, demonstrate that our taxonomy is similar to several taxonomies used in other work. [sent-152, score-0.301]
</p><p>40 The second main difference is that our taxonomy does not include a PURPOSE category and, instead, has several smaller categories. [sent-155, score-0.298]
</p><p>41 Finally, instead of possessing a single TOPIC category, our taxonomy has several, finer-grained TOPIC categories. [sent-156, score-0.203]
</p><p>42 THEME/OBJECT is typically the category to which other researchers assign noun compounds whose head noun is a nominalized verb and whose modifier noun is the THEME/OBJECT of the verb. [sent-158, score-1.14]
</p><p>43 While including a THEME/OBJECT category has the advantage of simplicity, its disadvantages are significant. [sent-160, score-0.095]
</p><p>44 This category leads to a significant ambiguity in examples because many compounds fitting the THEME/OBJECT category also match some other category as well. [sent-161, score-0.673]
</p><p>45 Warren (1978) gives the examples of soup pot and soup container to illustrate this issue, and Girju (2009) notes a substantial overlap between THEME and MAKEPRODUCE. [sent-162, score-0.111]
</p><p>46 Our results from Mechanical Turk showed significant overlap between PURPOSE and OBJECT categories (present in an earlier version of the taxonomy). [sent-163, score-0.114]
</p><p>47 If it is important to know whether the modifier also holds a THEME/OBJECT relationship, we suggest treating this as a separate classification task. [sent-165, score-0.083]
</p><p>48 The absence of a single PURPOSE category is another distinguishing characteristic of our taxonomy. [sent-166, score-0.125]
</p><p>49 Instead, the taxonomy includes a  number of finer-grained categories (e. [sent-167, score-0.317]
</p><p>50 , PERFORM/ENGAGE_IN), which can be conflated to create a PURPOSE category if necessary. [sent-169, score-0.135]
</p><p>51 During our Mechanical Turk-based refinement process, our now-defunct PURPOSE category was found to be ambiguous with many other categories as well as difficult to define. [sent-170, score-0.209]
</p><p>52 The third major distinguishing different between our taxonomy and others is the absence of a single TOPIC/ABOUT relation. [sent-175, score-0.233]
</p><p>53 Instead, our taxonomy has several finer-grained categories that can be conflated into a TOPIC category. [sent-176, score-0.357]
</p><p>54 Two differentiating characteristics of less importance are the absence of BENEFICIARY or SOURCE categories (Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al. [sent-178, score-0.144]
</p><p>55 Our EMPLOYER, CONSUMER, and USER/RECIPIENT categories combined more or less cover BENEFICIARY. [sent-180, score-0.114]
</p><p>56 4  Dataset  Ó  Our noun compound dataset was created from two principal sources: an in-house collection of terms extracted from a large corpus using partof-speech tagging and mutual information and the Wall Street Journal section of the Penn Treebank. [sent-182, score-0.616]
</p><p>57 In total, the dataset contains 17509 unique, out-of-context examples, making it by far the largest hand-annotated compound noun dataset in existence that we are aware of. [sent-184, score-0.794]
</p><p>58 The next largest available datasets have a variety of drawbacks for noun compound interpretation in general text. [sent-186, score-0.672]
</p><p>59 Kim and Baldwin’s (2005) dataset is the second largest available dataset, but inter-annotator agreement was only 52. [sent-187, score-0.259]
</p><p>60 Rosario and Heart’s (2001) dataset is specific to the biomedical domain, while Séaghdha and Copestake’s (2009) data is labeled with only 5 extremely  coarse-grained categories. [sent-191, score-0.107]
</p><p>61 Table 2: Size of various available noun compound datasets labeled with relation annotations. [sent-194, score-0.539]
</p><p>62 Italics indicate that the dataset contains n-prep-n constructions and/or non-nouns. [sent-195, score-0.107]
</p><p>63 Maximum Entropy classifiers have been effective on a variety of NLP problems including preposition sense disambiguation (Ye and Baldwin, 2007), which is somewhat similar to noun compound interpretation. [sent-200, score-0.509]
</p><p>64 2 Cross Validation Experiments We performed 10-fold cross validation on our dataset, and, for the purpose of comparison, we also performed 5-fold cross validation on Séaghdha’s (2007) dataset using his folds. [sent-224, score-0.304]
</p><p>65 6% figure is similar to the best previously reported accuracy for this dataset of 63. [sent-230, score-0.107]
</p><p>66 Table 3: Patterns for extracting trigram and 4Gram features from the Web 1T Corpus for a given noun compound (n1 n2). [sent-238, score-0.509]
</p><p>67 As far as we know, this is the first time that WordNet definition words have been used as features for noun compound interpretation. [sent-244, score-0.509]
</p><p>68 They had a positive impact on the Séaghdha data, but their affect upon our dataset was limited and mixed, with the removal of the 4-gram features actually  improving performance slightly. [sent-247, score-0.138]
</p><p>69 1 Evaluation Data To assess the quality of our taxonomy and classification method, we performed an inter-annotator agreement study using 150 noun compounds extracted from a random subset of articles taken from New York Times articles dating back to 1987 (Sandhaus, 2008). [sent-252, score-0.965]
</p><p>70 , a compound occurring twice as often as another is twice as likely to be selected) to label for testing purposes. [sent-255, score-0.301]
</p><p>71 Using a heuristic similar to that used by Lauer (1995), we only extracted binary noun compounds not part of  a larger sequence. [sent-256, score-0.596]
</p><p>72 5% of the binary noun compound instances in recent New York Times articles. [sent-259, score-0.509]
</p><p>73 Using Mechanical Turk to obtain interannotator agreement figures has several drawbacks. [sent-262, score-0.158]
</p><p>74 3 Combining Annotators To overcome the shortfalls of using Turkers for an inter-annotator agreement study, we chose to request ten annotations per noun compound and then combine the annotations into a single set of selections using a weighted voting scheme. [sent-268, score-0.759]
</p><p>75 The score for each label for a particular compound was then computed as the sum of the Turker quality scores of the Turkers 684  who annotated the compound. [sent-271, score-0.301]
</p><p>76 We recomputed the κ statistics after conflating the category groups in two different ways. [sent-278, score-0.161]
</p><p>77 The first variation involved conflating all the TOPIC categories into a single topic category, resulting in a total of 37 categories (denoted by κ* in Table 5). [sent-279, score-0.382]
</p><p>78 For the second variation, in addition to conflating the TOPIC categories, we conflated the ATTRIBUTE categories into a single category and the PURPOSE/ACTIVITY categories into a single category, for a total of 27 categories (denoted by κ** in Table 5). [sent-280, score-0.543]
</p><p>79 67 κ figures achieved by the Voted annotations compare well with previously reported inter-annotator agreement figures for noun compounds using fine-grained taxonomies. [sent-284, score-0.762]
</p><p>80 Kim and Baldwin (2005) report an agreement of 52. [sent-285, score-0.116]
</p><p>81 3 1% (not κ) for their dataset using Barker and Szpakowicz’s (1998) 20 semantic relations. [sent-286, score-0.14]
</p><p>82 61 κ agreement using a similar set of 22 semantic relations for noun compound annotation in which the annotators are shown translations ofthe compound in foreign languages. [sent-292, score-1.031]
</p><p>83 68 κ for a relatively small set of relations (BE, HAVE, IN, INST, ACTOR, ABOUT) after removing compounds with non-specific associations or high lexicalization. [sent-294, score-0.46]
</p><p>84 Id annotator id; N number of annotations; Weight voting weight; –  –  –  Agree raw agreement versus the author’s annotations; κ Cohen’s κ agreement; κ* and κ** Cohen’s κ results after conflating certain categories. [sent-296, score-0.216]
</p><p>85 –  –  –  –  –  least three annotations and their simple agreement with our annotations was very strong at 0. [sent-298, score-0.216]
</p><p>86 51 automatic classification figure is respectable given the larger number of categories in the taxonomy. [sent-301, score-0.196]
</p><p>87 It is also important to remember that the training set covers a large portion of the two-word noun compound instances in recent New York Times articles, so substantially higher accuracy can be expected on many texts. [sent-302, score-0.509]
</p><p>88 Interestingly, conflating categories only improved the κ statistics for the Turkers, not the automatic classifier. [sent-303, score-0.212]
</p><p>89 7  Conclusion  In this paper, we present a novel, fine-grained taxonomy of 43 noun-noun semantic relations, the largest annotated noun compound dataset yet cre-  ated, and a supervised classification method for automatic noun compound interpretation. [sent-304, score-1.479]
</p><p>90 We describe our taxonomy and provide mappings to taxonomies used by others. [sent-305, score-0.301]
</p><p>91 Our interannotator agreement study, which utilized nonexperts, shows good inter-annotator agreement 685  given the difficulty of the task, indicating that our category definitions are relatively straightforward. [sent-306, score-0.407]
</p><p>92 67% of our 150 inter-annotator agreement data marked as such by the combined Turker (Voted) annotation set. [sent-309, score-0.116]
</p><p>93 We demonstrated the effectiveness of a straightforward, supervised classification approach to noun compound interpretation that uses a large variety of boolean features. [sent-310, score-0.656]
</p><p>94 8  Future Work  In the future, we plan to focus on the interpretation of noun compounds with 3 or more nouns, a problem that includes bracketing noun compounds into their dependency structures in addition to nounnoun semantic relation interpretation. [sent-312, score-1.322]
</p><p>95 Furthermore, we would like to build a system that can handle longer noun phrases, including prepositions and possessives. [sent-313, score-0.242]
</p><p>96 Eventually, we would like to expand our data set and relations to cover proper nouns as well. [sent-315, score-0.114]
</p><p>97 We are hopeful that our current dataset and relation definitions, which will be made available via http://www. [sent-316, score-0.107]
</p><p>98 Translation by machine of compound nominals: Getting it right. [sent-335, score-0.301]
</p><p>99 Improving the interpretation of noun phrases with cross-linguistic information. [sent-414, score-0.305]
</p><p>100 Learning noun-modifier semantic relations with corpus-based and Wordnet-based features. [sent-504, score-0.105]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('compounds', 0.388), ('compound', 0.301), ('turkers', 0.239), ('aghdha', 0.223), ('noun', 0.208), ('taxonomy', 0.203), ('bbggnnvv', 0.186), ('girju', 0.182), ('szpakowicz', 0.179), ('barker', 0.149), ('levi', 0.149), ('turker', 0.147), ('ll', 0.119), ('agreement', 0.116), ('categories', 0.114), ('ww', 0.113), ('dataset', 0.107), ('taxonomies', 0.098), ('warren', 0.098), ('lauer', 0.098), ('interpretation', 0.097), ('category', 0.095), ('aaccttiivviittyy', 0.093), ('topic', 0.088), ('baldwin', 0.082), ('mechanical', 0.075), ('butnariu', 0.074), ('ppuurrppoossee', 0.074), ('rosario', 0.074), ('substance', 0.074), ('relations', 0.072), ('nakov', 0.07), ('conflating', 0.066), ('nastase', 0.066), ('purpose', 0.063), ('kim', 0.062), ('turk', 0.062), ('downing', 0.056), ('lees', 0.056), ('sandhaus', 0.056), ('ssuubbjjeeccttmmaatttteerr', 0.056), ('indicators', 0.055), ('annotations', 0.05), ('classification', 0.05), ('roget', 0.049), ('svms', 0.045), ('interannotator', 0.042), ('nouns', 0.042), ('cohen', 0.04), ('conflated', 0.04), ('attribute', 0.039), ('definitions', 0.038), ('vanderwende', 0.038), ('attrib', 0.037), ('bbgnnvvww', 0.037), ('bgv', 0.037), ('bnv', 0.037), ('bwn', 0.037), ('employer', 0.037), ('gbn', 0.037), ('ianltle', 0.037), ('jespersen', 0.037), ('pot', 0.037), ('puwrp', 0.037), ('rck', 0.037), ('soup', 0.037), ('svmmulticlass', 0.037), ('tatu', 0.037), ('tratz', 0.037), ('copestake', 0.036), ('bl', 0.036), ('moldovan', 0.036), ('largest', 0.036), ('existence', 0.035), ('voted', 0.035), ('validation', 0.034), ('voting', 0.034), ('gloss', 0.034), ('prepositions', 0.034), ('cross', 0.033), ('semantic', 0.033), ('consumer', 0.033), ('unbounded', 0.033), ('modifier', 0.033), ('acta', 0.033), ('agentive', 0.033), ('beneficiary', 0.033), ('blg', 0.033), ('consumed', 0.033), ('divisions', 0.033), ('automatic', 0.032), ('upon', 0.031), ('deemed', 0.031), ('nw', 0.031), ('wordnet', 0.031), ('absence', 0.03), ('turney', 0.03), ('datasets', 0.03), ('nominalization', 0.03), ('finin', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="19-tfidf-1" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>2 0.14745703 <a title="19-tfidf-2" href="./acl-2010-Automatically_Generating_Term_Frequency_Induced_Taxonomies.html">43 acl-2010-Automatically Generating Term Frequency Induced Taxonomies</a></p>
<p>Author: Karin Murthy ; Tanveer A Faruquie ; L Venkata Subramaniam ; Hima Prasad K ; Mukesh Mohania</p><p>Abstract: We propose a novel method to automatically acquire a term-frequency-based taxonomy from a corpus using an unsupervised method. A term-frequency-based taxonomy is useful for application domains where the frequency with which terms occur on their own and in combination with other terms imposes a natural term hierarchy. We highlight an application for our approach and demonstrate its effectiveness and robustness in extracting knowledge from real-world data.</p><p>3 0.1011181 <a title="19-tfidf-3" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>Author: Marie-Catherine de Marneffe ; Christopher D. Manning ; Christopher Potts</p><p>Abstract: Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.</p><p>4 0.097065084 <a title="19-tfidf-4" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>Author: Shane Bergsma ; Emily Pitler ; Dekang Lin</p><p>Abstract: In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.</p><p>5 0.090706997 <a title="19-tfidf-5" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>Author: Matthew Honnibal ; James R. Curran ; Johan Bos</p><p>Abstract: Once released, treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of specific phenomena. Instead, separate resources are created to address such problems. In this paper we show how to improve the quality of a treebank, by integrating resources and implementing improved analyses for specific constructions. We demonstrate this rebanking process by creating an updated version of CCGbank that includes the predicate-argument structure of both verbs and nouns, baseNP brackets, verb-particle constructions, and restrictive and non-restrictive nominal modifiers; and evaluate the impact of these changes on a statistical parser.</p><p>6 0.077034041 <a title="19-tfidf-6" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>7 0.074760109 <a title="19-tfidf-7" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>8 0.071774282 <a title="19-tfidf-8" href="./acl-2010-On_Learning_Subtypes_of_the_Part-Whole_Relation%3A_Do_Not_Mix_Your_Seeds.html">181 acl-2010-On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds</a></p>
<p>9 0.065868333 <a title="19-tfidf-9" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>10 0.063749187 <a title="19-tfidf-10" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>11 0.062394176 <a title="19-tfidf-11" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>12 0.061579391 <a title="19-tfidf-12" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>13 0.060861889 <a title="19-tfidf-13" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>14 0.060408317 <a title="19-tfidf-14" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>15 0.060330659 <a title="19-tfidf-15" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>16 0.05762516 <a title="19-tfidf-16" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>17 0.055468179 <a title="19-tfidf-17" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>18 0.054375939 <a title="19-tfidf-18" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>19 0.053264569 <a title="19-tfidf-19" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>20 0.052786205 <a title="19-tfidf-20" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.17), (1, 0.087), (2, -0.021), (3, -0.011), (4, 0.032), (5, -0.017), (6, 0.055), (7, 0.039), (8, 0.024), (9, -0.002), (10, 0.028), (11, 0.043), (12, -0.053), (13, -0.012), (14, 0.059), (15, 0.093), (16, 0.039), (17, 0.023), (18, 0.071), (19, -0.007), (20, -0.007), (21, 0.012), (22, 0.051), (23, 0.023), (24, -0.028), (25, -0.05), (26, 0.017), (27, 0.06), (28, -0.095), (29, -0.011), (30, -0.081), (31, 0.07), (32, 0.028), (33, -0.001), (34, 0.016), (35, 0.022), (36, 0.008), (37, 0.072), (38, -0.125), (39, 0.071), (40, 0.096), (41, 0.066), (42, 0.05), (43, 0.034), (44, 0.055), (45, 0.068), (46, 0.023), (47, 0.0), (48, 0.139), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93441296 <a title="19-lsi-1" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>2 0.72470123 <a title="19-lsi-2" href="./acl-2010-Automatically_Generating_Term_Frequency_Induced_Taxonomies.html">43 acl-2010-Automatically Generating Term Frequency Induced Taxonomies</a></p>
<p>Author: Karin Murthy ; Tanveer A Faruquie ; L Venkata Subramaniam ; Hima Prasad K ; Mukesh Mohania</p><p>Abstract: We propose a novel method to automatically acquire a term-frequency-based taxonomy from a corpus using an unsupervised method. A term-frequency-based taxonomy is useful for application domains where the frequency with which terms occur on their own and in combination with other terms imposes a natural term hierarchy. We highlight an application for our approach and demonstrate its effectiveness and robustness in extracting knowledge from real-world data.</p><p>3 0.66599274 <a title="19-lsi-3" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>Author: Shane Bergsma ; Emily Pitler ; Dekang Lin</p><p>Abstract: In this paper, we systematically assess the value of using web-scale N-gram data in state-of-the-art supervised NLP classifiers. We compare classifiers that include or exclude features for the counts of various N-grams, where the counts are obtained from a web-scale auxiliary corpus. We show that including N-gram count features can advance the state-of-the-art accuracy on standard data sets for adjective ordering, spelling correction, noun compound bracketing, and verb part-of-speech disambiguation. More importantly, when operating on new domains, or when labeled training data is not plentiful, we show that using web-scale N-gram features is essential for achieving robust performance.</p><p>4 0.63678467 <a title="19-lsi-4" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>Author: Marie-Catherine de Marneffe ; Christopher D. Manning ; Christopher Potts</p><p>Abstract: Texts and dialogues often express information indirectly. For instance, speakers’ answers to yes/no questions do not always straightforwardly convey a ‘yes’ or ‘no’ answer. The intended reply is clear in some cases (Was it good? It was great!) but uncertain in others (Was it acceptable? It was unprecedented.). In this paper, we present methods for interpreting the answers to questions like these which involve scalar modifiers. We show how to ground scalar modifier meaning based on data collected from the Web. We learn scales between modifiers and infer the extent to which a given answer conveys ‘yes’ or ‘no’ . To evaluate the methods, we collected examples of question–answer pairs involving scalar modifiers from CNN transcripts and the Dialog Act corpus and use response distributions from Mechanical Turk workers to assess the degree to which each answer conveys ‘yes’ or ‘no’ . Our experimental results closely match the Turkers’ response data, demonstrating that meanings can be learned from Web data and that such meanings can drive pragmatic inference.</p><p>5 0.62585425 <a title="19-lsi-5" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>Author: Nils Reiter ; Anette Frank</p><p>Abstract: This paper presents a supervised approach for identifying generic noun phrases in context. Generic statements express rulelike knowledge about kinds or events. Therefore, their identification is important for the automatic construction of knowledge bases. In particular, the distinction between generic and non-generic statements is crucial for the correct encoding of generic and instance-level information. Generic expressions have been studied extensively in formal semantics. Building on this work, we explore a corpus-based learning approach for identifying generic NPs, using selections of linguistically motivated features. Our results perform well above the baseline and existing prior work.</p><p>6 0.57648444 <a title="19-lsi-6" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>7 0.5693211 <a title="19-lsi-7" href="./acl-2010-On_Learning_Subtypes_of_the_Part-Whole_Relation%3A_Do_Not_Mix_Your_Seeds.html">181 acl-2010-On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds</a></p>
<p>8 0.55894613 <a title="19-lsi-8" href="./acl-2010-Hunting_for_the_Black_Swan%3A_Risk_Mining_from_Text.html">138 acl-2010-Hunting for the Black Swan: Risk Mining from Text</a></p>
<p>9 0.51643836 <a title="19-lsi-9" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>10 0.50264776 <a title="19-lsi-10" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>11 0.50224257 <a title="19-lsi-11" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>12 0.48671049 <a title="19-lsi-12" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>13 0.48463428 <a title="19-lsi-13" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>14 0.48331136 <a title="19-lsi-14" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>15 0.47393334 <a title="19-lsi-15" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>16 0.47386336 <a title="19-lsi-16" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>17 0.46310413 <a title="19-lsi-17" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>18 0.45939761 <a title="19-lsi-18" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>19 0.4558593 <a title="19-lsi-19" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>20 0.4526878 <a title="19-lsi-20" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.013), (25, 0.077), (39, 0.024), (42, 0.019), (44, 0.016), (59, 0.073), (72, 0.012), (73, 0.054), (76, 0.011), (78, 0.032), (80, 0.387), (83, 0.08), (84, 0.028), (98, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80316323 <a title="19-lda-1" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>2 0.70553905 <a title="19-lda-2" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>3 0.66498482 <a title="19-lda-3" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>Author: Rasoul Samad Zadeh Kaljahi</p><p>Abstract: Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1</p><p>4 0.61408019 <a title="19-lda-4" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>5 0.52552998 <a title="19-lda-5" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>Author: Roni Ben Aharon ; Idan Szpektor ; Ido Dagan</p><p>Abstract: Idan Szpektor Ido Dagan Yahoo! Research Department of Computer Science Haifa, Israel Bar-Ilan University idan @ yahoo- inc .com Ramat Gan, Israel dagan @ c s .biu . ac . i l FrameNet is a manually constructed database based on Frame Semantics. It models the semantic Many NLP tasks need accurate knowledge for semantic inference. To this end, mostly WordNet is utilized. Yet WordNet is limited, especially for inference be- tween predicates. To help filling this gap, we present an algorithm that generates inference rules between predicates from FrameNet. Our experiment shows that the novel resource is effective and complements WordNet in terms of rule coverage.</p><p>6 0.47296298 <a title="19-lda-6" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>7 0.45627859 <a title="19-lda-7" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>8 0.44843501 <a title="19-lda-8" href="./acl-2010-Global_Learning_of_Focused_Entailment_Graphs.html">127 acl-2010-Global Learning of Focused Entailment Graphs</a></p>
<p>9 0.4385983 <a title="19-lda-9" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>10 0.43378669 <a title="19-lda-10" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>11 0.43254125 <a title="19-lda-11" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>12 0.42766228 <a title="19-lda-12" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>13 0.42652011 <a title="19-lda-13" href="./acl-2010-An_Open-Source_Package_for_Recognizing_Textual_Entailment.html">30 acl-2010-An Open-Source Package for Recognizing Textual Entailment</a></p>
<p>14 0.42583948 <a title="19-lda-14" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>15 0.42268297 <a title="19-lda-15" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>16 0.42162147 <a title="19-lda-16" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>17 0.41845056 <a title="19-lda-17" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>18 0.41821867 <a title="19-lda-18" href="./acl-2010-Predicate_Argument_Structure_Analysis_Using_Transformation_Based_Learning.html">198 acl-2010-Predicate Argument Structure Analysis Using Transformation Based Learning</a></p>
<p>19 0.41665858 <a title="19-lda-19" href="./acl-2010-On_Learning_Subtypes_of_the_Part-Whole_Relation%3A_Do_Not_Mix_Your_Seeds.html">181 acl-2010-On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds</a></p>
<p>20 0.41595724 <a title="19-lda-20" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
