<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>233 acl-2010-The Same-Head Heuristic for Coreference</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-233" href="#">acl2010-233</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>233 acl-2010-The Same-Head Heuristic for Coreference</h1>
<br/><p>Source: <a title="acl-2010-233-pdf" href="http://aclweb.org/anthology//P/P10/P10-2007.pdf">pdf</a></p><p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>Reference: <a title="acl-2010-233-reference" href="../acl2010_reference/acl-2010-The_Same-Head_Heuristic_for_Coreference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu sne  Abstract We investigate coreference relationships between NPs with the same head noun. [sent-3, score-0.418]
</p><p>2 It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. [sent-4, score-0.42]
</p><p>3 We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision. [sent-5, score-0.333]
</p><p>4 1 Introduction Full NP coreference, the task of discovering which non-pronominal NPs in a discourse refer to the  same entity, is widely known to be challenging. [sent-6, score-0.041]
</p><p>5 In practice, however, most work focuses on the subtask of linking NPs with different head words. [sent-7, score-0.217]
</p><p>6 Decisions involving NPs with the same head word have not attracted nearly as much attention, and many systems, especially unsupervised ones, operate under the assumption that all same-head pairs corefer. [sent-8, score-0.334]
</p><p>7 This is by no means always the case– there are several systematic exceptions to the rule. [sent-9, score-0.034]
</p><p>8 In this paper, we show that these exceptions are fairly common, and describe an unsupervised system which learns to distinguish them from coreferent same-head pairs. [sent-10, score-0.311]
</p><p>9 Primarily, this is because they are a comparatively easy subtask in a notoriously difficult area; Stoyanov et al. [sent-12, score-0.043]
</p><p>10 (2009) shows that, among NPs headed by common nouns, those which have an exact match earlier in the document are the easiest to resolve (variant MUC score . [sent-13, score-0.146]
</p><p>11 53), by far the worst performance is on  those without any match at all (. [sent-15, score-0.042]
</p><p>12 This effect is magnified by most popular metrics for coreference, which reward finding links within large clusters more than they punish proposing spurious links, making it hard to improve performance by linking conservatively. [sent-17, score-0.264]
</p><p>13 Systems that use gold mention boundaries (the locations of NPs marked by annotators)1 have even less need to worry about same-head relationships, since most NPs which disobey the conventional assumption are not marked as mentions. [sent-18, score-0.396]
</p><p>14 In this paper, we count how often same-head pairs fail to corefer in the MUC-6 corpus, showing that gold mention detection hides most such pairs, but more realistic detection finds large numbers. [sent-19, score-0.566]
</p><p>15 We also present an unsupervised generative model which learns to make certain samehead pairs non-coreferent. [sent-20, score-0.291]
</p><p>16 The model is based on the idea that pronoun referents are likely to be salient noun phrases in the discourse, so we can learn about NP antecedents using pronominal antecedents as a starting point. [sent-21, score-0.461]
</p><p>17 Since our model links fewer NPs than the baseline, it improves precision but decreases recall. [sent-23, score-0.13]
</p><p>18 This tradeoff is favorable for CEAF, but not for b3. [sent-24, score-0.056]
</p><p>19 2  Related work  Unsupervised systems specify the assumption of same-head coreference in several ways: by as1Gold mention detection means something slightly different in the ACE corpus, where the system input contains every NP annotated with an entity type. [sent-25, score-0.481]
</p><p>20 (These three systems, perhaps not coincidentally, use gold mentions. [sent-29, score-0.112]
</p><p>21 ) An exception is Ng (2008), who points out that head identity is not an entirely reliable cue and instead uses exact string match (minus deter-  miners) for common NPs and an alias detection system for proper NPs. [sent-30, score-0.292]
</p><p>22 This work uses mentions extracted with an NP chunker. [sent-31, score-0.227]
</p><p>23 However, while using exact string match raises precision, many non-matching phrases are still coreferent, so this approach cannot be considered a full solution to the problem. [sent-33, score-0.091]
</p><p>24 , 2009) attempts to determine the contributions of various categories of NP to coreference scores, and shows (as stated above) that common NPs which partially match an earlier mention are not well resolved by the state-of-the-art RECONCILE system, which uses pairwise classification. [sent-36, score-0.458]
</p><p>25 They also show that using gold mention boundaries makes the coreference task substantially easier, and argue that this experimental setting is “rather unrealistic”. [sent-37, score-0.574]
</p><p>26 3  Descriptive study: MUC-6  We begin by examining how often non-same-head pairs appear in the MUC-6 coreference dataset. [sent-38, score-0.415]
</p><p>27 To do so, we compare two artificial coreference systems: the link-all strategy links all, and only, full (non-pronominal) NP pairs with the same head which occur within 10 sentences of one another. [sent-39, score-0.546]
</p><p>28 We compare our results to the gold standard using two metrics. [sent-41, score-0.112]
</p><p>29 b3(Bagga and Baldwin, 1998) is a standard metric which calculates a precision and recall for each mention. [sent-42, score-0.06]
</p><p>30 The mention CEAF (Luo, 2005) constructs a maximum-weight bipar2The choice of 10 sentences as the window size captures most, but not all, of the available recall. [sent-43, score-0.142]
</p><p>31 Using nouns mention detection, it misses 117 possible same-head links, or about 10%. [sent-44, score-0.245]
</p><p>32 However, precision drops further as the window size increases. [sent-45, score-0.06]
</p><p>33 tite matching between gold and proposed clusters, then gives the percentage of entities whose gold  label and proposed label match. [sent-46, score-0.224]
</p><p>34 b3 gives more weight to errors involving larger clusters (since these lower scores for several mentions at once); for mention CEAF, all mentions are weighted equally. [sent-47, score-0.668]
</p><p>35 The gold mentions method takes only mentions marked by annotators. [sent-50, score-0.612]
</p><p>36 The nps method takes all base noun phrases detected by the parser. [sent-51, score-0.636]
</p><p>37 Finally, the nouns method takes all nouns, even those that do not head NPs; this method maximizes recall, since it does not exclude prenominals in phrases like “a Bush spokesman”. [sent-52, score-0.217]
</p><p>38 ) For each experimental setting, we show the number of mentions detected, and how many of them are linked to some antecedent by the system. [sent-54, score-0.322]
</p><p>39 b3 shows a large drop in precision when all same-head pairs are linked; in fact, in the nps and nouns settings, only  about halfthe same-headed NPs are actually coreferent (864 real links, 1592 pairs for nps). [sent-56, score-1.052]
</p><p>40 This demonstrates that non-coreferent same-head pairs not only occur, but are actually rather common in the dataset. [sent-57, score-0.123]
</p><p>41 The drop in precision is much less obvious in the gold mentions setting, however; most unlinked same-head pairs are not annotated as mentions in the gold data, which is one reason why systems run in this experimental setting can afford to ignore them. [sent-58, score-0.904]
</p><p>42 Improperly linking same-head pairs causes a loss in precision, but scores are dominated by recall3. [sent-59, score-0.15]
</p><p>43 Thus, reporting b3 helps to mask the impact of these pairs when examining the final f-score. [sent-60, score-0.141]
</p><p>44 39 pairs denoted different entities (“recent employees” vs “employees who have worked for longer”) disambiguated by modifiers or sometimes by discourse position. [sent-62, score-0.174]
</p><p>45 The next largest group (24) consists of time and measure phrases like “ten miles”. [sent-63, score-0.049]
</p><p>46 12 pairs refer to parts or quantities  3This bias is exaggerated for systems which only link same-head pairs, but continues to apply to real systems; for instance (Haghighi and Klein, 2009) has a b3 precision of 84 and recall of 67. [sent-64, score-0.219]
</p><p>47 34  MentionsLinkedb3prrecFmention CEAF Gold mentions  LAOirlnaigkcnleamlent1 19 92 29 91 41 968542N891P03s0. [sent-65, score-0.227]
</p><p>48 Gold mentions leave little room for improvement between baseline and oracle; detecting more mentions widens the gap between them. [sent-78, score-0.485]
</p><p>49 With realistic mention detection, precision and CEAF scores improve over baselines, while recall and f-scores drop. [sent-79, score-0.264]
</p><p>50 involving  a generic  The remaining  proper  9 4  noun phrases  headed by Inc. [sent-88, score-0.204]
</p><p>51 To define our generative model, we assume that the parse trees for the entire document D are given, except for the subtrees with root nonterminal NP, denoted ni, which our system will generate. [sent-91, score-0.105]
</p><p>52 These subtrees are related by a hidden set of alignments, ai, which link each NP to another NP (which we call a generator) appearing somewhere before it in the document, or to a null antecedent. [sent-92, score-0.161]
</p><p>53 The generative process fills in all the NP nodes in order, from left to right. [sent-94, score-0.039]
</p><p>54 When deciding on a generator for NP ni, we can extract features characterizing its relationship to a potential generator gj. [sent-96, score-0.19]
</p><p>55 These fea-  tures, which we denote f(ni, gj , D), may depend on their relative position in the document D, and on any features of gj, since we have already generated its tree. [sent-97, score-0.277]
</p><p>56 As usual for IBM models, we learn using EM, and we need to start our alignment function off with a good initial set of parameters. [sent-99, score-0.056]
</p><p>57 Since antecedents of NPs and pronouns (both salient NPs) often occur in similar syntactic environments, we use an alignment function for pronoun coreference as a starting point. [sent-100, score-0.612]
</p><p>58 This alignment can be learned from raw data, making our approach unsupervised. [sent-101, score-0.056]
</p><p>59 We take the pronoun model of Charniak and Elsner (2009)4 as our starting point. [sent-102, score-0.152]
</p><p>60 Then our alignment (parameterized by feature weights w) is: p(ai = j |G, D)  ∝  exp(f(ni,  gj  , D)  •  w)  The weights w are learned by gradient descent  on the log-likelihood. [sent-104, score-0.367]
</p><p>61 To use this model within EM, we alternate an E-step where we calculate the expected alignments E[ai = j], then an Mstep where we run gradient descent. [sent-105, score-0.034]
</p><p>62 (We have also had some success with stepwise EM as in (Liang and Klein, 2009), but this requires some tuning to work properly. [sent-106, score-0.031]
</p><p>63 35  As features, we take the same features as Charniak and Elsner (2009): sentence and word-count distance between ni and gj, sentence position of each, syntactic role of each, and head type of gj (proper, common or pronoun). [sent-111, score-0.62]
</p><p>64 We designed this feature set to distinguish prominent NPs in the discourse, and also to be able to detect  abstract or partitive phrases by examining modifiers and determiners. [sent-113, score-0.176]
</p><p>65 To produce full NPs and learn same-head coreference, we focus on learning a good alignment using the pronoun model as a starting point. [sent-114, score-0.208]
</p><p>66 For translation, we use a trivial model, p(ni |gai ) = 1 if the two have the same head, and 0 otherwise, except for the null antecedent, which draws heads from a multinomial distribution over words. [sent-115, score-0.086]
</p><p>67 While we could learn an alignment and then treat all generators as antecedents, so that only NPs aligned to the null antecedent were not labeled coreferent, in practice this model would align nearly all the same-head pairs. [sent-116, score-0.293]
</p><p>68 Therefore, our model is actually a mixture of two IBM models, pC and pN, where pC produces NPs with antecedents and pN produces pairs that share a head, but are not coreferent. [sent-118, score-0.22]
</p><p>69 In all experimental settings, the model improves precision over the baseline while decreasing recall– that is, it misses some legitimate coreferent pairs while correctly excluding many of the spurious ones. [sent-125, score-0.443]
</p><p>70 Because of the precision-recall tradeoff at which the systems operate, this results in reduced b3 and link F. [sent-126, score-0.098]
</p><p>71 However, for the nps and nouns settings, where the parser is responsible for finding mentions, the tradeoff is positive for the CEAF metrics. [sent-127, score-0.632]
</p><p>72 For instance, in the nps setting, it improves over baseline by 57%. [sent-128, score-0.521]
</p><p>73 As expected, the model does poorly in the gold mentions setting, doing worse than baseline on both metrics. [sent-129, score-0.339]
</p><p>74 Although it is possible to get very high precision in this setting, the model is far too conservative, linking less than half of the available mentions to anything, when in fact about 60% of them are coreferent. [sent-130, score-0.348]
</p><p>75 As we explain above, this experimental setting makes it mostly unnecessary to worry about non-coreferent same-head pairs because the MUC-6 annotators don’t often mark them. [sent-131, score-0.185]
</p><p>76 6  Conclusions  While same-head pairs are easier to resolve than same-other pairs, they are still non-trivial and deserve further attention in coreference research. [sent-132, score-0.423]
</p><p>77 It is also important to report results using a realistic mention detector as well as gold mentions. [sent-134, score-0.316]
</p><p>78 An Expecta-  tion Maximization approach to pronoun resolution. [sent-147, score-0.121]
</p><p>79 Simple coreference resolution with rich syntactic and semantic features. [sent-160, score-0.329]
</p><p>80 Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. [sent-187, score-0.307]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nps', 0.521), ('gj', 0.277), ('coreference', 0.274), ('ni', 0.23), ('mentions', 0.227), ('coreferent', 0.176), ('ceaf', 0.158), ('mention', 0.142), ('np', 0.128), ('pronoun', 0.121), ('head', 0.113), ('gold', 0.112), ('antecedents', 0.097), ('antecedent', 0.095), ('generator', 0.095), ('elsner', 0.092), ('pairs', 0.089), ('charniak', 0.088), ('null', 0.086), ('haghighi', 0.078), ('pn', 0.078), ('stoyanov', 0.072), ('links', 0.07), ('detection', 0.065), ('oracle', 0.065), ('unsupervised', 0.062), ('realistic', 0.062), ('samehead', 0.062), ('linking', 0.061), ('precision', 0.06), ('ibm', 0.058), ('pt', 0.058), ('generators', 0.056), ('alignment', 0.056), ('tradeoff', 0.056), ('resolution', 0.055), ('nouns', 0.055), ('em', 0.054), ('pc', 0.053), ('employees', 0.053), ('qp', 0.053), ('vadas', 0.053), ('examining', 0.052), ('worry', 0.05), ('phrases', 0.049), ('misses', 0.048), ('micha', 0.046), ('bagga', 0.046), ('marked', 0.046), ('setting', 0.046), ('headed', 0.044), ('mcclosky', 0.044), ('modifiers', 0.044), ('subtask', 0.043), ('eugene', 0.042), ('match', 0.042), ('link', 0.042), ('poon', 0.041), ('discourse', 0.041), ('proper', 0.041), ('learns', 0.039), ('spurious', 0.039), ('klein', 0.039), ('generative', 0.039), ('cherry', 0.037), ('involving', 0.037), ('clusters', 0.035), ('ai', 0.034), ('actually', 0.034), ('exceptions', 0.034), ('nothing', 0.034), ('gradient', 0.034), ('noun', 0.033), ('salient', 0.033), ('nonterminal', 0.033), ('aria', 0.033), ('operate', 0.033), ('subtrees', 0.033), ('detected', 0.033), ('honolulu', 0.032), ('plays', 0.031), ('starting', 0.031), ('punish', 0.031), ('alias', 0.031), ('corefer', 0.031), ('spokesman', 0.031), ('sne', 0.031), ('bursty', 0.031), ('legitimate', 0.031), ('afford', 0.031), ('sourcelanguage', 0.031), ('easiest', 0.031), ('stepwise', 0.031), ('partitive', 0.031), ('deserve', 0.031), ('widens', 0.031), ('anaphora', 0.03), ('resolve', 0.029), ('real', 0.028), ('magnified', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="233-tfidf-1" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>2 0.4648225 <a title="233-tfidf-2" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>3 0.33029714 <a title="233-tfidf-3" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>4 0.26336932 <a title="233-tfidf-4" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>5 0.24104145 <a title="233-tfidf-5" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>Author: Aria Haghighi ; Dan Klein</p><p>Abstract: We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.</p><p>6 0.18132792 <a title="233-tfidf-6" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>7 0.16668795 <a title="233-tfidf-7" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>8 0.15161325 <a title="233-tfidf-8" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>9 0.1391909 <a title="233-tfidf-9" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>10 0.1261656 <a title="233-tfidf-10" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>11 0.11820383 <a title="233-tfidf-11" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>12 0.1128782 <a title="233-tfidf-12" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>13 0.098828718 <a title="233-tfidf-13" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>14 0.095413469 <a title="233-tfidf-14" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>15 0.092728905 <a title="233-tfidf-15" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>16 0.083288819 <a title="233-tfidf-16" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>17 0.081088357 <a title="233-tfidf-17" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>18 0.077237979 <a title="233-tfidf-18" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>19 0.077217706 <a title="233-tfidf-19" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>20 0.076843783 <a title="233-tfidf-20" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.23), (1, 0.042), (2, 0.03), (3, -0.273), (4, -0.166), (5, 0.424), (6, -0.005), (7, 0.032), (8, 0.145), (9, 0.143), (10, 0.054), (11, -0.144), (12, -0.042), (13, -0.104), (14, 0.078), (15, -0.039), (16, -0.0), (17, -0.081), (18, -0.068), (19, -0.001), (20, -0.037), (21, 0.024), (22, 0.037), (23, -0.056), (24, 0.007), (25, -0.045), (26, -0.005), (27, 0.006), (28, -0.018), (29, -0.026), (30, -0.005), (31, -0.05), (32, 0.022), (33, 0.009), (34, -0.023), (35, -0.057), (36, -0.028), (37, -0.007), (38, -0.033), (39, 0.037), (40, 0.014), (41, -0.025), (42, 0.053), (43, 0.031), (44, -0.032), (45, 0.01), (46, 0.005), (47, 0.023), (48, 0.074), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96383381 <a title="233-lsi-1" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>2 0.92420077 <a title="233-lsi-2" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>Author: Marta Recasens ; Eduard Hovy</p><p>Abstract: This paper explores the effect that different corpus configurations have on the performance of a coreference resolution system, as measured by MUC, B3, and CEAF. By varying separately three parameters (language, annotation scheme, and preprocessing information) and applying the same coreference resolution system, the strong bonds between system and corpus are demonstrated. The experiments reveal problems in coreference resolution evaluation relating to task definition, coding schemes, and features. They also ex- pose systematic biases in the coreference evaluation metrics. We show that system comparison is only possible when corpus parameters are in exact agreement.</p><p>3 0.91888654 <a title="233-lsi-3" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>4 0.91651118 <a title="233-lsi-4" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>5 0.60502392 <a title="233-lsi-5" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>Author: Aria Haghighi ; Dan Klein</p><p>Abstract: We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.</p><p>6 0.51989275 <a title="233-lsi-6" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>7 0.50778502 <a title="233-lsi-7" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>8 0.48558694 <a title="233-lsi-8" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>9 0.44898078 <a title="233-lsi-9" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>10 0.44119999 <a title="233-lsi-10" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>11 0.37252977 <a title="233-lsi-11" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>12 0.34159079 <a title="233-lsi-12" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>13 0.30018887 <a title="233-lsi-13" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>14 0.28649867 <a title="233-lsi-14" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>15 0.27757078 <a title="233-lsi-15" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>16 0.27710018 <a title="233-lsi-16" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>17 0.26146704 <a title="233-lsi-17" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>18 0.25428787 <a title="233-lsi-18" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>19 0.2497161 <a title="233-lsi-19" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>20 0.2478565 <a title="233-lsi-20" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.012), (25, 0.106), (33, 0.012), (42, 0.037), (59, 0.121), (73, 0.04), (78, 0.047), (80, 0.015), (83, 0.169), (84, 0.02), (88, 0.19), (98, 0.129)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93137115 <a title="233-lda-1" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>Author: David Elson ; Nicholas Dames ; Kathleen McKeown</p><p>Abstract: We present a method for extracting social networks from literature, namely, nineteenth-century British novels and serials. We derive the networks from dialogue interactions, and thus our method depends on the ability to determine when two characters are in conversation. Our approach involves character name chunking, quoted speech attribution and conversation detection given the set of quotes. We extract features from the social networks and examine their correlation with one another, as well as with metadata such as the novel’s setting. Our results provide evidence that the majority of novels in this time period do not fit two characterizations provided by literacy scholars. Instead, our results suggest an alternative explanation for differences in social networks.</p><p>same-paper 2 0.90840793 <a title="233-lda-2" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>3 0.86773193 <a title="233-lda-3" href="./acl-2010-Speech-Driven_Access_to_the_Deep_Web_on_Mobile_Devices.html">215 acl-2010-Speech-Driven Access to the Deep Web on Mobile Devices</a></p>
<p>Author: Taniya Mishra ; Srinivas Bangalore</p><p>Abstract: The Deep Web is the collection of information repositories that are not indexed by search engines. These repositories are typically accessible through web forms and contain dynamically changing information. In this paper, we present a system that allows users to access such rich repositories of information on mobile devices using spoken language.</p><p>4 0.82198656 <a title="233-lda-4" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>5 0.80789727 <a title="233-lda-5" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>Author: Veselin Stoyanov ; Claire Cardie ; Nathan Gilbert ; Ellen Riloff ; David Buttler ; David Hysom</p><p>Abstract: Despite the existence of several noun phrase coreference resolution data sets as well as several formal evaluations on the task, it remains frustratingly difficult to compare results across different coreference resolution systems. This is due to the high cost of implementing a complete end-to-end coreference resolution system, which often forces researchers to substitute available gold-standard information in lieu of implementing a module that would compute that information. Unfortunately, this leads to inconsistent and often unrealistic evaluation scenarios. With the aim to facilitate consistent and realistic experimental evaluations in coreference resolution, we present Reconcile, an infrastructure for the development of learning-based noun phrase (NP) coreference resolution systems. Reconcile is designed to facilitate the rapid creation of coreference resolution systems, easy implementation of new feature sets and approaches to coreference res- olution, and empirical evaluation of coreference resolvers across a variety of benchmark data sets and standard scoring metrics. We describe Reconcile and present experimental results showing that Reconcile can be used to create a coreference resolver that achieves performance comparable to state-ofthe-art systems on six benchmark data sets.</p><p>6 0.80189413 <a title="233-lda-6" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>7 0.80172074 <a title="233-lda-7" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>8 0.79347235 <a title="233-lda-8" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>9 0.79085863 <a title="233-lda-9" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>10 0.78951347 <a title="233-lda-10" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>11 0.7847783 <a title="233-lda-11" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>12 0.78375584 <a title="233-lda-12" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>13 0.78352535 <a title="233-lda-13" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>14 0.7804324 <a title="233-lda-14" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>15 0.7794013 <a title="233-lda-15" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>16 0.77937382 <a title="233-lda-16" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>17 0.77926177 <a title="233-lda-17" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>18 0.77618855 <a title="233-lda-18" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>19 0.77615952 <a title="233-lda-19" href="./acl-2010-Collocation_Extraction_beyond_the_Independence_Assumption.html">60 acl-2010-Collocation Extraction beyond the Independence Assumption</a></p>
<p>20 0.77562863 <a title="233-lda-20" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
