<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-213" href="#">acl2010-213</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</h1>
<br/><p>Source: <a title="acl-2010-213-pdf" href="http://aclweb.org/anthology//P/P10/P10-2063.pdf">pdf</a></p><p>Author: Seth Kulick</p><p>Abstract: We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.</p><p>Reference: <a title="acl-2010-213-reference" href="../acl2010_reference/acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu ck  Abstract We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. [sent-3, score-0.519]
</p><p>2 By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties. [sent-4, score-0.476]
</p><p>3 In additional to inflectional morphology, the POS tags encode more complex tokenization sequences, such as preposition + noun or noun + possessive pronoun. [sent-6, score-0.469]
</p><p>4 One approach taken to this problem is to use a morphological analyzer such as BAMA-v2. [sent-7, score-0.229]
</p><p>5 , 2009c)1 , which generates a list of all possible morphological analyses for a given token. [sent-10, score-0.193]
</p><p>6 Machine learning approaches can model separate aspects of a solution (e. [sent-11, score-0.095]
</p><p>7 , “has a pronominal clitic”) and then combine them to select the most appropriate solution from among this list. [sent-13, score-0.095]
</p><p>8 A benefit of this approach is that by picking a single solution from the morphological analyzer, the part-ofspeech and tokenization comes as a unit (Habash and Rambow, 2005; Roth et al. [sent-14, score-0.45]
</p><p>9 In contrast, other approaches have used a  pipelined approach, with separate models to first do tokenization and then part-of-speech tagging (Diab et al. [sent-18, score-0.265]
</p><p>10 While these approaches have somewhat lower performance than the joint approach, they have the advantage that they do not rely on the presence of a full-blown morphological analyzer, which may not always be available or appropriate as the data shifts to different genres or Arabic dialects. [sent-20, score-0.134]
</p><p>11 In this work we present a novel approach to this problem that allows us to do simultaneous tokenization and core part-of-speech tagging with a simple classifier, without using a full-blown morphological analyzer. [sent-21, score-0.543]
</p><p>12 We distinguish between closed-class and open-class categories of words, and encode regular expressions that express the morphological patterns for the former, and simple regular expressions for the latter that provide only the generic templates for affixation. [sent-22, score-0.618]
</p><p>13 This is however sufficient for tokeniza-  tion and core POS tagging, since the stem identifies the appropriate regular expression, which then in turn makes explicit, simultaneously, the tokenization and part-of-speech information. [sent-24, score-0.819]
</p><p>14 2  Background  The Arabic Treebank (ATB) contains a full morphological analysis of each “source token”, a whitespace/punctuation-delimited string from the source text. [sent-25, score-0.215]
</p><p>15 2 TEXT is the actual source token text, to be analyzed. [sent-27, score-0.291]
</p><p>16 Each VOC segment has associated with it a POS tag and 2This is the analysis for one particular instance of ktbh. [sent-29, score-0.163]
</p><p>17 The same source token may receive another analysis elsewhere in the treebank. [sent-30, score-0.291]
</p><p>18 As shown in the second part of Table 1, the first two segments remain together as one tree token, and the pronoun is separated as a separate tree token. [sent-35, score-0.201]
</p><p>19 3 Each tree token’s POS tag therefore consists of what can be considered an “ATB core tag”, together with inflectional material (case, gender, number). [sent-37, score-0.387]
</p><p>20 For example, in Table 1, the “core tag” ofthe first tree token is NOUN. [sent-38, score-0.29]
</p><p>21 In this work, we aim  to recover the separation of a source token TEXT into the corresponding separate tree token TEXTs, together with a “reduced core tag” for each tree token. [sent-39, score-0.795]
</p><p>22 By “reduced core tag”, we mean an ATB core tag that has been reduced in two ways: (1) All inflectional material [infl] is stripped off six ATB core tags: PRON[infl], POSS PRON[infl], DEM[infl], [IV|PV|CV]SUFF DO[infl] (2) Collapsing of some ATB core tags, as listed in Table 2. [sent-40, score-0.69]
</p><p>23 These two steps result in a total of 40 reduced core tags, and each tree token has exactly one such reduced core tag. [sent-41, score-0.695]
</p><p>24 , 2010) for a detailed discussion of how this splitting is done and how the tree token TEXT field (called INPUT STRING in the ATB releases) is created. [sent-45, score-0.29]
</p><p>25 has 339710 source tokens and 402291 tree tokens, where the latter are derived from the former as discussed above. [sent-46, score-0.219]
</p><p>26 Table 3 lists the 40 reduced tags we use, and their frequency among the ATB3-v3. [sent-47, score-0.177]
</p><p>27 3  Description of Approach  Given a source token, we wish to recover (1) the tree tokens (which amounts to recovering the ATB tokenization), and (2) the reduced core POS tag for each tree token. [sent-49, score-0.755]
</p><p>28 For example, in Table 1, given the input source token TEXT ktbh, we wish to recover the tree tokens ktb/NOA and h/POSS PRON. [sent-50, score-0.497]
</p><p>29 As mentioned in the introduction, we use regular expressions that encode all the tokenization and POS tag possibilities. [sent-51, score-0.645]
</p><p>30 Each “group” (substring unit) in a regular expression (regex) is assigned an internal name, and a list is maintained of the possible reduced core POS tags that can occur with that regex group. [sent-52, score-0.995]
</p><p>31 It is possible, and indeed usually the case for groups representing affixes, that more than one such POS tag is possible. [sent-53, score-0.163]
</p><p>32 However, it is crucial for our approach that while some given source token TEXT may match many regular expressions (regexes), when the POS tag is also taken into account, there can be only one match among all the (open or closed-class) regexes. [sent-54, score-0.781]
</p><p>33 We say a source token “pos-matches” a regex if the  TEXT matches and POS tags match, and “textmatches” if the TEXT matches the regex regardless of the POS. [sent-55, score-1.294]
</p><p>34 A source token is considered to have an open-class solution if any of the tree tokens in that solution have an open-class tag. [sent-58, score-0.619]
</p><p>35 For example, ktbh in Table 1 has an open-class solution because one of the tree to-  kens has an open-class tag (NOA), even though the other is closed-class (POSS PRON). [sent-59, score-0.386]
</p><p>36 We encode the possible solutions for closedclass source tokens using the lists in the ATB morphological guidelines (Maamouri et al. [sent-60, score-0.359]
</p><p>37 The text wlm can text-match either REGEX #1 or #2, but when the POS tag for lm is taken into account, only one can pos-match. [sent-63, score-0.304]
</p><p>38 We return to the closed-class regexes in Section 4. [sent-64, score-0.239]
</p><p>39 We also encode regular expression for the openclass source tokens, but these are simply generic templates expressing the usual affix possibilities, such as: [ wf ] [ blk ] st em NOA pos s pronoun where there is no list of possible strings for stem_NOA, but which instead can match anything. [sent-65, score-0.862]
</p><p>40 While all parts except for the stem are optional, we do not make such parts optional in a single expression. [sent-66, score-0.345]
</p><p>41 Instead, we multiple out the possibilities into different expressions with different parts (e. [sent-67, score-0.107]
</p><p>42 The reason for this is that we give different names to the stem  in each case, and this is the basis of the features for the classifier. [sent-70, score-0.345]
</p><p>43 As with the closed-class regexes, we associate a list of possible POS tags for each named group within a regular expression. [sent-71, score-0.3]
</p><p>44 Here the stem NOA group can only have the tag NOA. [sent-72, score-0.508]
</p><p>45 We create features for a classifier for the openclass words as follows. [sent-73, score-0.142]
</p><p>46 Each word is run through all of the open-class regular expressions. [sent-74, score-0.2]
</p><p>47 For each expression that text-matches, we make a feature which is the name of the stem part of the regular expression, along with the characters that match the stem. [sent-75, score-0.724]
</p><p>48 The stem name encodes whether there is a prefix or suffix, but does not include a POS tag. [sent-76, score-0.475]
</p><p>49 However, the source token pos-matches exactly one of the regular expressions, and the pos tag for the stem is appended to the named stem for that expression to form the gold label for training and the target for testing. [sent-77, score-1.667]
</p><p>50 For example, Table 4 lists the matching regular expression for three words. [sent-78, score-0.256]
</p><p>51 The first, yjry, textmatches the generic regular expressions for any string/NOA, any string/IV, etc. [sent-79, score-0.319]
</p><p>52 The name of the  stem for all these expressions is the same, just stem, and so they all give rise to the same feature, stem=y j ry. [sent-81, score-0.515]
</p><p>53 It also matches the expression for a NOA with a possessive pronoun4, and in this case the stem name in the regular expression is stem_spp (which stands for “stem with a possessive pronoun suffix”), and this gives rise to the feature st em_spp=y j r. [sent-82, score-1.08]
</p><p>54 Similarly, for wAfAdt the stem of the second expression has the name p_stem, for a prefix. [sent-83, score-0.516]
</p><p>55 The third example shows the different stem names that occur when there are both prefix and suffix possibilities. [sent-84, score-0.449]
</p><p>56 For each example, there is exactly one regex that not only textmatches, but also pos-matches. [sent-85, score-0.433]
</p><p>57 The combination of the stem name in these cases together with the gold tag forms the gold label, as indicated in column 3. [sent-86, score-0.725]
</p><p>58 Therefore, for each source token TEXT, the features include the ones arising from the named stems of all the regexes that text-match that TEXT, as shown in column 4, and the gold label is the appropriate stem name together with the POS tag, as shown in column 3. [sent-87, score-1.167]
</p><p>59 ample,  wAfAdt  For ex-  would also have the features  st em_fl=w, p_stem_fl=A, indicating that the first letter of stem is w and the first letter of p_stem is A. [sent-89, score-0.371]
</p><p>60 1 as a temporary proxy for a named entity list, and include a feature for a stem if that stem is in the list (stem_in_l i st, p_stem_in_l i st, etc. [sent-91, score-0.749]
</p><p>61 There is a dependency  between the  4The regex listed is slightly simplified. [sent-93, score-0.406]
</p><p>62 It actually contains a reference to the list of all possessive pronouns, not just y. [sent-94, score-0.137]
</p><p>63 344  feature shown in column 4, based on the stem of that regular expression. [sent-95, score-0.535]
</p><p>64 A p before a stem means that it has a prefix, spp after means that it has a possessive pronouns suffix, and svop means that it has a (verbal) object pronoun suffix. [sent-96, score-0.464]
</p><p>65 “all” in the matching regular expression is shorthand for text-matching all the corresponding regular expressions with NOA, IV, etc. [sent-97, score-0.479]
</p><p>66 For each word, exactly one regex also pos-matches, which results in the gold label, shown in column 3. [sent-98, score-0.523]
</p><p>67 possibility of a prefix and the likelihood of the remaining stem, and so we focus on the likelihood of the possible stems, where the open-class regexes enumerate the possible stems. [sent-99, score-0.298]
</p><p>68 A gold label together with the source token TEXT maps back to a single regex, and so for a given label, the TEXT is parsed by that regular expression, resulting in a tokenization along with list of possible POS tags for each affix group in the regex. [sent-100, score-0.951]
</p><p>69 Textmatches for an open-class regex give rise to features as just described. [sent-102, score-0.438]
</p><p>70 During training, if the correct match for the word is one of the closed-class expressions, then the gold label is CLOSED. [sent-104, score-0.153]
</p><p>71 The classifier is used only to get solutions for the openclass words, although we wish to give the classifier all the words for the sentence. [sent-105, score-0.21]
</p><p>72 The cross-product of the stem name and (open-class) reduced core POS tags, plus the CLOSED tag, yields 24 labels for a CRF classifier in Mallet (McCallum, 2002). [sent-106, score-0.642]
</p><p>73 We keep a listing (List #1) of all (source token TEXT, solution) pairs seen during training. [sent-110, score-0.293]
</p><p>74 For an open-class solution, “solution” is the gold label as described in 5In Section 4 we discuss how these one POS tag. [sent-111, score-0.101]
</p><p>75 In addition, for every regex seen during training that pos-matches some source token TEXT, we keep a listing (List #2) of all ((regex-group-name, text), POS-tag) tuples. [sent-114, score-0.78]
</p><p>76 We use the information in List #1 to choose a solution for all words seen in training in the Baseline and Run 2 below, and in Run 3, for words text-matching a closed-class expression. [sent-115, score-0.133]
</p><p>77 For example, if wlm is seen during testing, List #1 will be consulted to find the most common solution (REGEX #1 or #2), and in either case, List #2 will be consulted to determine the most frequent tag for w as a prefix. [sent-117, score-0.428]
</p><p>78 While there is certainly room for improvement here, this works quite well since the tags for the affixes do not vary much. [sent-118, score-0.119]
</p><p>79 We score the solution for a source token instance as correct for tokenization if it exactly matches the TEXT split for the tree tokens derived from that source token instance in the ATB. [sent-119, score-1.116]
</p><p>80 It is correct for POS if correct for tokenization and if each tree token has the same POS tag as the reduced core tag for that tree token in the ATB. [sent-120, score-1.316]
</p><p>81 For a simple baseline, if a source token TEXT is in List #1 then we simply use the most frequent stored solution. [sent-121, score-0.332]
</p><p>82 If it text-matches any closed-class expression, we pick a random choice from among those regexes and otherwise from the open-class regexes that it text-matches. [sent-123, score-0.478]
</p><p>83 Any POS ambiguities for a regex group are disambiguated  345  the list stored during training. [sent-124, score-0.506]
</p><p>84 Origins “open” and “closed” are random choices from the open or closed regexes for the source token. [sent-125, score-0.37]
</p><p>85 For run 2, we continue to use the stored solution if the token was seen in training. [sent-131, score-0.428]
</p><p>86 There is a significant improvement in the score for the openclass items, and therefore in the overall results. [sent-134, score-0.105]
</p><p>87 If a word matches any closed-class expres-  sion, we either use the most frequent occurence during training (if it was seen), or use a random maching closed-class expression (if not). [sent-136, score-0.153]
</p><p>88 If the word doesn’t match a closed-class expression, we use the mallet result. [sent-137, score-0.13]
</p><p>89 The mallet score goes up, almost certainly because the score is now including results on words that were seen during training. [sent-138, score-0.116]
</p><p>90 It is not a simple matter to compare results with previous work, due to differing evaluation techniques, data sets, and POS tag sets. [sent-143, score-0.163]
</p><p>91 6% on the LDC-supplied reduced tag set, and Diab et al. [sent-149, score-0.255]
</p><p>92 The LDCsupplied tag set used is smaller than the one in this paper (24 tags), but does distinguish between NOUN and ADJ. [sent-152, score-0.163]
</p><p>93 , 2007) assume gold tokenization for evaluation of POS results, which  we do not. [sent-154, score-0.277]
</p><p>94 4%, is somewhat similar to ours in that it scores on a “core tag”, but unlike for us there is only one such tag for a source token (easier) but it distinguishes between NOUN and ADJ (harder). [sent-157, score-0.454]
</p><p>95 However, this unfortunately has to wait until new versions are released that work with the current version of the SAMA morphological analyzer and ATB. [sent-159, score-0.229]
</p><p>96 There are various possibilities for recovering this information, such as (1) using a different module combining NOUN/ADJ disambiguation together with NP chunking, or (2) simply including NOUN/ADJ in the current classifier instead of NOA. [sent-161, score-0.139]
</p><p>97 0): Fast and robust tokenization, pos tagging, and base phrase chunking. [sent-182, score-0.139]
</p><p>98 Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop. [sent-186, score-0.204]
</p><p>99 Consistent and flexible integration of morphological annotation in the Arabic Treebank. [sent-191, score-0.134]
</p><p>100 Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking. [sent-219, score-0.134]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('regex', 0.406), ('stem', 0.345), ('regexes', 0.239), ('tokenization', 0.221), ('token', 0.21), ('atb', 0.192), ('arabic', 0.165), ('tag', 0.163), ('regular', 0.156), ('maamouri', 0.146), ('pos', 0.139), ('morphological', 0.134), ('diab', 0.126), ('noa', 0.126), ('infl', 0.119), ('habash', 0.119), ('kulick', 0.115), ('openclass', 0.105), ('expression', 0.1), ('core', 0.097), ('textmatches', 0.096), ('analyzer', 0.095), ('solution', 0.095), ('reduced', 0.092), ('tags', 0.085), ('seth', 0.085), ('conj', 0.084), ('source', 0.081), ('tree', 0.08), ('possessive', 0.078), ('mallet', 0.078), ('mohamed', 0.077), ('rambow', 0.075), ('sama', 0.072), ('name', 0.071), ('bies', 0.068), ('expressions', 0.067), ('voc', 0.063), ('krouna', 0.063), ('sondos', 0.063), ('list', 0.059), ('prefix', 0.059), ('tokens', 0.058), ('pron', 0.058), ('lm', 0.056), ('gold', 0.056), ('nizar', 0.054), ('matches', 0.053), ('match', 0.052), ('stems', 0.052), ('mona', 0.051), ('closed', 0.05), ('consortium', 0.048), ('bouziri', 0.048), ('closedclass', 0.048), ('ktbh', 0.048), ('poss', 0.048), ('wafadt', 0.048), ('wlm', 0.048), ('inflectional', 0.047), ('simultaneous', 0.047), ('label', 0.045), ('suffix', 0.045), ('listing', 0.045), ('roth', 0.044), ('tagging', 0.044), ('run', 0.044), ('consulted', 0.042), ('basma', 0.042), ('fatma', 0.042), ('gaddeche', 0.042), ('glos', 0.042), ('buckwalter', 0.042), ('cv', 0.042), ('stored', 0.041), ('pronoun', 0.041), ('possibilities', 0.04), ('ann', 0.039), ('affix', 0.038), ('encode', 0.038), ('seen', 0.038), ('text', 0.037), ('classifier', 0.037), ('tim', 0.037), ('recover', 0.037), ('recovering', 0.036), ('crf', 0.035), ('column', 0.034), ('affixes', 0.034), ('rise', 0.032), ('iv', 0.032), ('wish', 0.031), ('pv', 0.03), ('graff', 0.03), ('owen', 0.03), ('exactly', 0.027), ('wf', 0.027), ('sub', 0.027), ('disambiguation', 0.026), ('st', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="213-tfidf-1" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>Author: Seth Kulick</p><p>Abstract: We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.</p><p>2 0.16393307 <a title="213-tfidf-2" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>3 0.16215381 <a title="213-tfidf-3" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>Author: Yassine Benajiba ; Imed Zitouni ; Mona Diab ; Paolo Rosso</p><p>Abstract: Building an accurate Named Entity Recognition (NER) system for languages with complex morphology is a challenging task. In this paper, we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute).</p><p>4 0.1416871 <a title="213-tfidf-4" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>5 0.09961921 <a title="213-tfidf-5" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>6 0.07176365 <a title="213-tfidf-6" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>7 0.071714684 <a title="213-tfidf-7" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>8 0.071044192 <a title="213-tfidf-8" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>9 0.065741755 <a title="213-tfidf-9" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>10 0.062251698 <a title="213-tfidf-10" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>11 0.055942982 <a title="213-tfidf-11" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>12 0.055575009 <a title="213-tfidf-12" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>13 0.054611094 <a title="213-tfidf-13" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<p>14 0.05375002 <a title="213-tfidf-14" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>15 0.052276522 <a title="213-tfidf-15" href="./acl-2010-Combining_Orthogonal_Monolingual_and_Multilingual_Sources_of_Evidence_for_All_Words_WSD.html">62 acl-2010-Combining Orthogonal Monolingual and Multilingual Sources of Evidence for All Words WSD</a></p>
<p>16 0.051405985 <a title="213-tfidf-16" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>17 0.050694998 <a title="213-tfidf-17" href="./acl-2010-The_Use_of_Formal_Language_Models_in_the_Typology_of_the_Morphology_of_Amerindian_Languages.html">234 acl-2010-The Use of Formal Language Models in the Typology of the Morphology of Amerindian Languages</a></p>
<p>18 0.049780585 <a title="213-tfidf-18" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>19 0.049431931 <a title="213-tfidf-19" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>20 0.049088791 <a title="213-tfidf-20" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, -0.017), (2, 0.025), (3, -0.017), (4, -0.009), (5, -0.011), (6, 0.029), (7, 0.03), (8, 0.063), (9, 0.104), (10, 0.033), (11, 0.088), (12, 0.002), (13, -0.083), (14, -0.071), (15, -0.082), (16, -0.025), (17, 0.124), (18, 0.248), (19, -0.048), (20, 0.019), (21, 0.087), (22, 0.031), (23, 0.066), (24, 0.1), (25, 0.097), (26, -0.092), (27, -0.042), (28, -0.034), (29, 0.091), (30, 0.026), (31, -0.026), (32, -0.077), (33, 0.009), (34, -0.157), (35, -0.033), (36, 0.018), (37, -0.077), (38, 0.122), (39, 0.018), (40, -0.003), (41, 0.013), (42, 0.055), (43, -0.011), (44, 0.045), (45, 0.072), (46, -0.042), (47, 0.011), (48, 0.045), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95124876 <a title="213-lsi-1" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>Author: Seth Kulick</p><p>Abstract: We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.</p><p>2 0.64470071 <a title="213-lsi-2" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>Author: Yassine Benajiba ; Imed Zitouni ; Mona Diab ; Paolo Rosso</p><p>Abstract: Building an accurate Named Entity Recognition (NER) system for languages with complex morphology is a challenging task. In this paper, we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute).</p><p>3 0.58675301 <a title="213-lsi-3" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>4 0.56240505 <a title="213-lsi-4" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>Author: Reyyan Yeniterzi ; Kemal Oflazer</p><p>Abstract: We present a novel scheme to apply factored phrase-based SMT to a language pair with very disparate morphological structures. Our approach relies on syntactic analysis on the source side (English) and then encodes a wide variety of local and non-local syntactic structures as complex structural tags which appear as additional factors in the training data. On the target side (Turkish), we only perform morphological analysis and disambiguation but treat the complete complex morphological tag as a factor, instead of separating morphemes. We incrementally explore capturing various syntactic substructures as complex tags on the English side, and evaluate how our translations improve in BLEU scores. Our maximal set of source and target side transformations, coupled with some additional techniques, provide an 39% relative improvement from a baseline 17.08 to 23.78 BLEU, all averaged over 10 training and test sets. Now that the syntactic analysis on the English side is available, we also experiment with more long distance constituent reordering to bring the English constituent order close to Turkish, but find that these transformations do not provide any additional consistent tangible gains when averaged over the 10 sets.</p><p>5 0.54964864 <a title="213-lsi-5" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>6 0.4857952 <a title="213-lsi-6" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>7 0.38383287 <a title="213-lsi-7" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>8 0.38229835 <a title="213-lsi-8" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>9 0.37796426 <a title="213-lsi-9" href="./acl-2010-The_Use_of_Formal_Language_Models_in_the_Typology_of_the_Morphology_of_Amerindian_Languages.html">234 acl-2010-The Use of Formal Language Models in the Typology of the Morphology of Amerindian Languages</a></p>
<p>10 0.36010516 <a title="213-lsi-10" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>11 0.34960169 <a title="213-lsi-11" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>12 0.34026605 <a title="213-lsi-12" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>13 0.33745268 <a title="213-lsi-13" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>14 0.33304939 <a title="213-lsi-14" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>15 0.33259362 <a title="213-lsi-15" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>16 0.32218188 <a title="213-lsi-16" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>17 0.31914547 <a title="213-lsi-17" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>18 0.31810573 <a title="213-lsi-18" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>19 0.31300196 <a title="213-lsi-19" href="./acl-2010-Efficient_Inference_through_Cascades_of_Weighted_Tree_Transducers.html">95 acl-2010-Efficient Inference through Cascades of Weighted Tree Transducers</a></p>
<p>20 0.30939138 <a title="213-lsi-20" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.014), (25, 0.06), (39, 0.017), (42, 0.032), (59, 0.126), (73, 0.032), (78, 0.031), (80, 0.012), (83, 0.073), (84, 0.037), (89, 0.351), (98, 0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73719728 <a title="213-lda-1" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>Author: Seth Kulick</p><p>Abstract: We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.</p><p>2 0.4944098 <a title="213-lda-2" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>Author: Junhui Li ; Guodong Zhou ; Hwee Tou Ng</p><p>Abstract: This paper explores joint syntactic and semantic parsing of Chinese to further improve the performance of both syntactic and semantic parsing, in particular the performance of semantic parsing (in this paper, semantic role labeling). This is done from two levels. Firstly, an integrated parsing approach is proposed to integrate semantic parsing into the syntactic parsing process. Secondly, semantic information generated by semantic parsing is incorporated into the syntactic parsing model to better capture semantic information in syntactic parsing. Evaluation on Chinese TreeBank, Chinese PropBank, and Chinese NomBank shows that our integrated parsing approach outperforms the pipeline parsing approach on n-best parse trees, a natural extension of the widely used pipeline parsing approach on the top-best parse tree. Moreover, it shows that incorporating semantic role-related information into the syntactic parsing model significantly improves the performance of both syntactic parsing and semantic parsing. To our best knowledge, this is the first research on exploring syntactic parsing and semantic role labeling for both verbal and nominal predicates in an integrated way. 1</p><p>3 0.48677105 <a title="213-lda-3" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>4 0.48497674 <a title="213-lda-4" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>Author: Simone Paolo Ponzetto ; Roberto Navigli</p><p>Abstract: One of the main obstacles to highperformance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.</p><p>5 0.48326772 <a title="213-lda-5" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>6 0.48302883 <a title="213-lda-6" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>7 0.48244417 <a title="213-lda-7" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>8 0.48244336 <a title="213-lda-8" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>9 0.48021907 <a title="213-lda-9" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>10 0.48019493 <a title="213-lda-10" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>11 0.47891122 <a title="213-lda-11" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>12 0.47776419 <a title="213-lda-12" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>13 0.47745717 <a title="213-lda-13" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>14 0.47695339 <a title="213-lda-14" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>15 0.47692096 <a title="213-lda-15" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>16 0.47671127 <a title="213-lda-16" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>17 0.47671115 <a title="213-lda-17" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>18 0.47661996 <a title="213-lda-18" href="./acl-2010-BabelNet%3A_Building_a_Very_Large_Multilingual_Semantic_Network.html">44 acl-2010-BabelNet: Building a Very Large Multilingual Semantic Network</a></p>
<p>19 0.47632736 <a title="213-lda-19" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>20 0.47451538 <a title="213-lda-20" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
