<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-110" href="#">acl2010-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</h1>
<br/><p>Source: <a title="acl-2010-110-pdf" href="http://aclweb.org/anthology//P/P10/P10-1032.pdf">pdf</a></p><p>Author: Jun Sun ; Min Zhang ; Chew Lim Tan</p><p>Abstract: We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1</p><p>Reference: <a title="acl-2010-110-reference" href="../acl2010_reference/acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 sg  Abstract We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. [sent-10, score-1.011]
</p><p>2 Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. [sent-11, score-1.366]
</p><p>3 The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. [sent-12, score-0.576]
</p><p>4 We further apply the sub-tree alignment in machine translation with two methods. [sent-13, score-0.312]
</p><p>5 It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. [sent-14, score-0.368]
</p><p>6 However, most of the syntax based systems construct the syntactic translation rules based on word alignment, which not only suffers from the pipeline errors, but also fails to effectively utilize the syntactic structural features. [sent-16, score-0.318]
</p><p>7 (2007) attempt to directly capture the syntactic translational equivalences by automatically conducting sub-tree alignment, which can be defined as follows: A sub-tree alignment process pairs up sub-tree pairs across bilingual parse trees whose contexts are semantically translational equivalent. [sent-18, score-0.983]
</p><p>8 Each pair consists of both the lexical constituents and their maximum tree structures generated over the lexical sequences in the original parse trees. [sent-22, score-0.371]
</p><p>9 Due to the 1-to-1 mapping between sub-trees and tree nodes, sub-tree alignment can also be considered as node alignment by conducting multiple links across the internal nodes as shown in Fig. [sent-23, score-0.897]
</p><p>10 (2004) conduct sub-tree alignment by using some heuristic rules, lack of extensibility and generality. [sent-27, score-0.333]
</p><p>11 These works fail to utilize the structural features, rendering the syntactic rich task of sub-tree alignment less convincing and attractive. [sent-32, score-0.404]
</p><p>12 This may be due to the fact that the syntactic structures in a parse tree pair are hard to describe using plain features. [sent-33, score-0.655]
</p><p>13 In addition, explicitly utilizing syntactic tree  fragments results in exponentially high dimensional feature vectors, which is hard to compute. [sent-34, score-0.328]
</p><p>14 Alternatively, convolution parse tree kernels (Collins and Duffy, 2001), which implicitly explore the tree structure information, have been successfully applied in many NLP tasks, such as Semantic parsing (Moschitti, 2004) and Relation Extraction (Zhang et al. [sent-35, score-0.65]
</p><p>15 In multilingual tasks such as machine translation, tree kernels are seldom applied. [sent-38, score-0.367]
</p><p>16 In this paper, we propose Bilingual Tree Kernels (BTKs) to model the bilingual translational equivalences, in our case, to conduct sub-tree alignment. [sent-39, score-0.302]
</p><p>17 This is motivated by the decent effectiveness of tree kernels in expressing the similarity between tree structures. [sent-40, score-0.643]
</p><p>18 Along with BTKs, various lexical and syntactic structural features are proposed to capture the cor-  respondence between bilingual sub-trees using a polynomial kernel. [sent-43, score-0.463]
</p><p>19 We then attempt to combine the polynomial kernel and BTKs to construct a composite kernel. [sent-44, score-0.571]
</p><p>20 We employ a kernel based classifier with the composite kernel to classify each candidate of sub-tree pair as aligned or unaligned. [sent-46, score-0.886]
</p><p>21 Then a greedy search algorithm is performed according to the three criteria of sub-tree alignment within the space of candidates classified as aligned. [sent-47, score-0.303]
</p><p>22 We evaluate the sub-tree alignment on both the gold standard tree bank and an automatically parsed corpus. [sent-48, score-0.531]
</p><p>23 Experimental results show that the proposed BTKs benefit sub-tree alignment on both corpora, along with the lexical features and the plain structural features. [sent-49, score-0.776]
</p><p>24 Further experiments in machine translation also suggest that the obtained sub-tree alignment can improve the performance of both phrase and syntax based SMT systems. [sent-50, score-0.371]
</p><p>25 2 Bilingual Tree Kernels In this section, we propose the two BTKs and study their capability and complexity in modeling the bilingual structural similarity. [sent-51, score-0.289]
</p><p>26 refers to the source tree substructure space; while ? [sent-69, score-0.4]
</p><p>27 In order to compute the dot product of the feature vectors in the exponentially high dimensional feature space, we introduce the tree kernel functions as follows:  ? [sent-160, score-0.695]
</p><p>28 The iBTK is defined as a composite kernel consisting of a source tree kernel and a target tree kernel which measures the source and the target structural similarity respectively. [sent-187, score-1.817]
</p><p>29 Therefore, the composite kernel can be computed using the ordinary monolingual tree kernels (Collins and Duffy, 2001). [sent-188, score-0.891]
</p><p>30 is the decay factor used to make the kernel value less variable with respect to the number of sub-structures. [sent-402, score-0.311]
</p><p>31 Similarly, we can decompose the target kernel as ? [sent-403, score-0.352]
</p><p>32 However, the composite style of constructing the iBTK helps keep the computational complexity comparable to the monolingual tree kernel, which is ? [sent-431, score-0.415]
</p><p>33 As an alternative, we further define a kernel to capture the relationship across the counterparts without increasing the computational complexity. [sent-448, score-0.341]
</p><p>34 As a result, we propose the dependent Bilingual Tree kernel (dBTK) to jointly evaluate the similarity across sub-tree pairs by enlarging the feature space to the Cartesian product of the two substructure sets. [sent-449, score-0.627]
</p><p>35 It is infeasible to explicitly compute the kernel  function by expressing the sub-trees as feature vectors. [sent-692, score-0.381]
</p><p>36 In order to achieve convenient computation, we deduce the kernel function as the above. [sent-693, score-0.337]
</p><p>37 The deduction from (1) to (2) is derived according to the fact that the number of identical substructure pairs rooted in the node pairs ? [sent-694, score-0.355]
</p><p>38 As a result, the dBTK can be evaluated as a product of two monolingual tree kernels. [sent-709, score-0.281]
</p><p>39 Here we verify the correctness of the kernel by directly constructing the feature space for the inner product. [sent-710, score-0.394]
</p><p>40 The decomposition benefits the efficient computation to use the algorithm for the monolingual tree kernel in Section 2. [sent-712, score-0.565]
</p><p>41 3  Sub-structure Spaces for BTKs  The syntactic translational equivalences under BTKs are evaluated with respective to the substructures factorized from the candidate sub-tree pairs. [sent-729, score-0.28]
</p><p>42 Since the proposed BTKs can be computed by individually evaluating the source and target monolingual tree kernels, the definition of the sub-structure can be simplified to base only on monolingual sub-trees. [sent-731, score-0.398]
</p><p>43 However, the sub-tree alignment task requires strong capability of discriminating the sub-trees with their roots across adjacent generations, because those candidates share many identical SSTs. [sent-739, score-0.351]
</p><p>44 As illustrated in Fig 2, the source sub-tree rooted at VP*, which should  be aligned to the target sub-tree rooted at NP*, may be likely aligned to the sub-tree rooted at PP*, 308  Figure 2: Illustration of SST, RdSST and RgSST  which shares quite a similar context with NP*. [sent-740, score-0.472]
</p><p>45 In consequence, the values of the SST based kernel function are quite similar between the candidate sub-tree pair rooted at (VP*,NP*) and (VP*,PP*). [sent-742, score-0.455]
</p><p>46 Although defined for individual SST, the indicator function can be evaluated outside the summation, without increasing the computational complexity of the kernel function. [sent-831, score-0.337]
</p><p>47 Therefore the kernel function can be simplified to only capture the sub-structure rooted at the root of the sub-tree. [sent-838, score-0.579]
</p><p>48 4 Root only More aggressively, we can simplify the kernel to only measure the common root node without considering the complex tree structures. [sent-880, score-0.695]
</p><p>49 Therefore the kernel function is simplified to be a binary function with time complexity ? [sent-881, score-0.363]
</p><p>50 4  Plain features  Besides BTKs, we introduce various plain lexical features and structural features which can be expressed as feature functions. [sent-900, score-0.638]
</p><p>51 The plain syntactic structural features can deal with the structural divergence of bilingual parse trees in a more general perspective. [sent-902, score-0.796]
</p><p>52 Internal Word Alignment Features: The word alignment links account much for the cooccurrence of the aligned terms. [sent-1085, score-0.36]
</p><p>53 We define the internal word alignment features as follows: ? [sent-1086, score-0.347]
</p><p>54 Internal-External Word Alignment Features: Similar to the lexical features, we also introduce the internal-external word alignment features as follows: ? [sent-1161, score-0.348]
</p><p>55 oist ahleirgwneisde  Online Structural Features  In addition to the lexical correspondence, we also capture the structural divergence by introducing the following tree structural features. [sent-1285, score-0.507]
</p><p>56 | are the respective span length of the parse tree used for normalization. [sent-1331, score-0.323]
</p><p>57 Tree Depth difference: Intuitively, translational equivalent sub-tree pairs tend to have similar depth from the root of the parse tree. [sent-1364, score-0.341]
</p><p>58 We allow the model to penalize the candidate sub-tree pairs with quite different distance of path from the root of the parse tree to the root of the sub-tree. [sent-1365, score-0.55]
</p><p>59 Alignment Model  Given feature spaces defined in the last two sections, we propose a 2-phase sub-tree alignment model as follows: In the 1st phase, a kernel based classifier, SVM in our study, is employed to classify each candidate sub-tree pair as aligned or unaligned. [sent-1410, score-0.784]
</p><p>60 , which is a polynomial kernel with the degree of 2, utilizing the plain features. [sent-1463, score-0.709]
</p><p>61 The composite kernel can be constructed using the polynomial kernel for plain features and various BTKs for tree structure by linear combination with coefficient ? [sent-1480, score-1.424]
</p><p>62 Since SVM is a large margin based discriminative classifier rather than a probabilistic model, we introduce a sigmoid function to convert the distance against the hyperplane to a posterior alignment probability as follows: 310  ? [sent-1490, score-0.29]
</p><p>63 6  Experiments on Sub-Tree Alignments  In order to evaluate the effectiveness of the alignment model and its capability in the applications requiring syntactic translational equivalences, we employ two corpora to carry out the sub-tree alignment evaluation. [sent-1531, score-0.742]
</p><p>64 The other is the automatically parsed bilingual tree pairs selected from FBIS corpus (allowing minor parsing errors) with human annotated sub-tree alignment. [sent-1533, score-0.446]
</p><p>65 Thus, it is important to evaluate the sub-tree alignment on the automatically parsed corpus with parsing errors. [sent-1562, score-0.327]
</p><p>66 The selected plain sentence pairs are further parsed by Stanford parser (Klein and Manning, 2003) on both the English and Chinese sides. [sent-1567, score-0.405]
</p><p>67 We  manually annotate the sub-tree alignment for the automatically parsed tree pairs according to the definition in Section 1. [sent-1568, score-0.542]
</p><p>68 The corpus is further divided into 200 aligned tree pairs for training and 100 for testing as shown in Table 2. [sent-1571, score-0.338]
</p><p>69 , the baseline approach first takes all the links between the sub-tree pairs as alignment hypotheses, i. [sent-1584, score-0.334]
</p><p>70 …,  By using the lexical translation probabilities, each hypothesis is assigned an alignment score. [sent-1603, score-0.355]
</p><p>71 Since the highest-scoring hypotheses tend to appear on the leaf nodes, it may introduce ambiguity when conducting the alignment for a POS node whose child word appears twice in a sentence. [sent-1659, score-0.399]
</p><p>72 for the composite kernel are tuned with respect to F-measure (F) on the development set of HIT corpus. [sent-1707, score-0.472]
</p><p>73 To learn the lexical and word alignment features for both the proposed model and the baseline method, we train GIZA++ on the entire FBIS bilingual corpus (240k). [sent-1732, score-0.516]
</p><p>74 4  Experimental results  In Tables 3 and 4, we incrementally enlarge the  feature spaces in certain order for both corpora and examine the feature contribution to the alignment results. [sent-1735, score-0.414]
</p><p>75 In detail, the iBTKs and dBTKs are firstly combined with the polynomial kernel for plain features individually, then the best iBTK and dBTK are chosen to construct a more complex composite kernel along with the polynomial kernel for both corpora. [sent-1736, score-1.632]
</p><p>76 The improvement suggests that the proposed framework with syntactic structural features is more effective in modeling the bilingual syntactic correspondence. [sent-1739, score-0.362]
</p><p>77 •  By introducing BTKs to construct a composite kernel, the performance in both corpora is significantly improved against only using the polynomial kernel for plain features. [sent-1740, score-0.9]
</p><p>78 This suggests that the structural features captured by BTKs are quite useful for the sub-tree alignment task. [sent-1741, score-0.404]
</p><p>79 We  also try to use BTKs alone without the polynomial kernel for plain features; however, the performance is rather low. [sent-1742, score-0.709]
</p><p>80 This suggests that the structure correspondence cannot be used to measure the semantically equivalent tree structures alone, since the same syntactic structure tends to be reused in the same parse tree and lose the ability of disambiguation to some extent. [sent-1743, score-0.492]
</p><p>81 The reason that dBTK outperforms iBTK in the feature space of Root in FBIS corpus is that although it is a joint feature space, the Root node pairs can be constructed from a close set of grammar tags and to form a relatively low dimensional space. [sent-1749, score-0.289]
</p><p>82 Other than the factor of the amount of training data, this is also because the plain features in Table 3 are not as effective as those in Table 4, since they are trained on FBIS corpus which facilitates Table 4 more with respect to the domains. [sent-1753, score-0.398]
</p><p>83 That’s why we attempt to construct a more complex composite kernel in adoption of the kernel of dBTK-Root as below. [sent-1758, score-0.813]
</p><p>84 To gain an extra performance boosting, we further construct a composite kernel which includes the best iBTK and the best dBTK for each corpus along with the polynomial kernel for plain features. [sent-1759, score-1.239]
</p><p>85 Experiments on Machine Translation  In addition to the intrinsic alignment evaluation,  we further conduct the extrinsic MT evaluation. [sent-1762, score-0.303]
</p><p>86 We explore the effectiveness of sub-tree alignment for both phrase based and linguistically motivated syntax based SMT systems. [sent-1763, score-0.323]
</p><p>87 For the syntax based system, since sub-tree alignment can directly benefit Tree-2-Tree based systems, we apply the sub-tree alignment in a syntax system based on Synchronous Tree Substitution Grammar (STSG) (Zhang et al. [sent-1783, score-0.646]
</p><p>88 Recent research on tree based systems shows that relaxing the restriction from tree structure to tree sequence structure (Synchronous Tree Sequence Substitution Grammar: STSSG) significantly improves the translation performance (Zhang et al. [sent-1786, score-0.654]
</p><p>89 The STSG based decoding can be easily performed with the STSSG decoder by restricting the translation rule set to be elementary tree pairs only. [sent-1791, score-0.383]
</p><p>90 As for the alignment setting, we use the word alignment trained on the entire FBIS (240k) corpus by GIZA++ with heuristic grow-diag-final for both Moses and the syntax system. [sent-1792, score-0.645]
</p><p>91 For sub-treealignment, we use the above word alignment to learn lexical/word alignment feature, and train with the FBIS training corpus (200) using the composite kernel of Plain+dBTK-Root+iBTKRdSTT. [sent-1793, score-1.028]
</p><p>92 2  Experimental results  Compared with the adoption of word alignment, translational equivalences generated from structural alignment tend to be more grammatically 3 An elementary tree is a fragment whose leaf nodes either non-terminal symbols or terminal symbols. [sent-1795, score-0.804]
</p><p>93 However, utilizing syntactic translational equivalences alone for machine translation loses the capability of modeling non-syntactic phrases (Koehn et al. [sent-1797, score-0.351]
</p><p>94 Consequently, instead of using phrases constraint by sub-tree alignment alone, we attempt to combine word alignment and sub-tree alignment and deploy the capability of both with two methods. [sent-1799, score-0.842]
</p><p>95 •  •  Directly Concatenate (DirC) is operated by directly concatenating the rule set genereted from sub-tree alignment and the original rule set generated from word alignment (Tinsley et al. [sent-1800, score-0.584]
</p><p>96 We constrain the bilingual phrases to be consistent with Either Word alignment or Sub-tree alignment (EWoS) instead of being originally consistent with the word alignment only. [sent-1804, score-0.932]
</p><p>97 8  Conclusion  In this paper, we explore syntactic structure features by means of Bilingual Tree Kernels and apply them to bilingual sub-tree alignment along with various lexical and plain structural features. [sent-1808, score-0.957]
</p><p>98 We use both gold standard tree bank and the automatically parsed corpus for the sub-tree alignment evaluation. [sent-1809, score-0.559]
</p><p>99 Further experiment shows that the obtained sub-tree alignment benefits both phrase and syntax based MT systems by delivering more weight on syntactic phrases. [sent-1811, score-0.364]
</p><p>100 Acknowledgments We thank MITLAB4 in Harbin Institute of Tech-  nology for licensing us their sub-tree alignment corpus for our research. [sent-1812, score-0.292]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('btks', 0.339), ('plain', 0.329), ('kernel', 0.311), ('alignment', 0.264), ('dbtk', 0.254), ('ibtk', 0.237), ('tree', 0.202), ('kernels', 0.165), ('composite', 0.161), ('hit', 0.154), ('tinsley', 0.152), ('bilingual', 0.14), ('dbtks', 0.135), ('root', 0.13), ('translational', 0.123), ('fbis', 0.122), ('substructure', 0.102), ('structural', 0.099), ('sst', 0.095), ('equivalences', 0.089), ('dirc', 0.085), ('rooted', 0.082), ('stssg', 0.074), ('polynomial', 0.069), ('ewos', 0.068), ('ibtks', 0.068), ('rdsst', 0.068), ('rgsst', 0.068), ('aligned', 0.067), ('spaces', 0.062), ('stsg', 0.06), ('syntax', 0.059), ('lex', 0.055), ('node', 0.052), ('monolingual', 0.052), ('source', 0.051), ('capability', 0.05), ('descendants', 0.048), ('translation', 0.048), ('parse', 0.047), ('span', 0.047), ('chew', 0.046), ('subtree', 0.045), ('refers', 0.045), ('ssts', 0.044), ('hearne', 0.044), ('conducting', 0.044), ('feature', 0.044), ('lexical', 0.043), ('internal', 0.042), ('syntactic', 0.041), ('pairs', 0.041), ('features', 0.041), ('target', 0.041), ('dimensional', 0.041), ('hypotheses', 0.039), ('conduct', 0.039), ('space', 0.039), ('str', 0.038), ('decent', 0.038), ('zhang', 0.038), ('lim', 0.038), ('identical', 0.037), ('decoder', 0.037), ('duffy', 0.036), ('similarity', 0.036), ('pair', 0.036), ('parsed', 0.035), ('cartesian', 0.034), ('convolution', 0.034), ('fig', 0.034), ('oist', 0.034), ('pisces', 0.034), ('smt', 0.032), ('min', 0.032), ('chinese', 0.031), ('mt', 0.03), ('linked', 0.03), ('bank', 0.03), ('link', 0.03), ('construct', 0.03), ('sure', 0.03), ('groves', 0.03), ('heuristic', 0.03), ('capture', 0.03), ('links', 0.029), ('moses', 0.029), ('corpus', 0.028), ('rule', 0.028), ('svm', 0.028), ('thresholds', 0.027), ('andy', 0.027), ('product', 0.027), ('respective', 0.027), ('enlarging', 0.027), ('skips', 0.027), ('elementary', 0.027), ('function', 0.026), ('functions', 0.026), ('vp', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="110-tfidf-1" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>Author: Jun Sun ; Min Zhang ; Chew Lim Tan</p><p>Abstract: We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1</p><p>2 0.29767036 <a title="110-tfidf-2" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>3 0.23501183 <a title="110-tfidf-3" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>Author: WenTing Wang ; Jian Su ; Chew Lim Tan</p><p>Abstract: Syntactic knowledge is important for discourse relation recognition. Yet only heuristically selected flat paths and 2-level production rules have been used to incorporate such information so far. In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations. The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well. University of Singapore Singapore 117417 sg tacl @ comp .nus .edu . sg 1</p><p>4 0.19635646 <a title="110-tfidf-4" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>5 0.18786886 <a title="110-tfidf-5" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>Author: Vamshi Ambati ; Stephan Vogel ; Jaime Carbonell</p><p>Abstract: Semi-supervised word alignment aims to improve the accuracy of automatic word alignment by incorporating full or partial manual alignments. Motivated by standard active learning query sampling frameworks like uncertainty-, margin- and query-by-committee sampling we propose multiple query strategies for the alignment link selection task. Our experiments show that by active selection of uncertain and informative links, we reduce the overall manual effort involved in elicitation of alignment link data for training a semisupervised word aligner.</p><p>6 0.15832756 <a title="110-tfidf-6" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>7 0.15708953 <a title="110-tfidf-7" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>8 0.14869633 <a title="110-tfidf-8" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>9 0.14774141 <a title="110-tfidf-9" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>10 0.14635843 <a title="110-tfidf-10" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>11 0.14604385 <a title="110-tfidf-11" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>12 0.14447172 <a title="110-tfidf-12" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>13 0.1346201 <a title="110-tfidf-13" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>14 0.13418648 <a title="110-tfidf-14" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>15 0.13047558 <a title="110-tfidf-15" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>16 0.12935045 <a title="110-tfidf-16" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>17 0.12877254 <a title="110-tfidf-17" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>18 0.12795371 <a title="110-tfidf-18" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>19 0.12096195 <a title="110-tfidf-19" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>20 0.10516869 <a title="110-tfidf-20" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.258), (1, -0.269), (2, 0.021), (3, 0.03), (4, -0.02), (5, 0.056), (6, -0.026), (7, 0.068), (8, -0.11), (9, -0.104), (10, -0.084), (11, -0.17), (12, -0.022), (13, 0.035), (14, -0.037), (15, 0.089), (16, 0.134), (17, -0.076), (18, 0.18), (19, -0.073), (20, 0.101), (21, -0.042), (22, -0.053), (23, 0.093), (24, 0.035), (25, 0.062), (26, -0.017), (27, -0.029), (28, 0.013), (29, 0.062), (30, -0.098), (31, 0.027), (32, 0.171), (33, 0.07), (34, -0.009), (35, 0.026), (36, 0.017), (37, -0.092), (38, -0.009), (39, -0.085), (40, 0.139), (41, -0.07), (42, 0.034), (43, 0.146), (44, -0.004), (45, 0.065), (46, 0.038), (47, 0.032), (48, 0.054), (49, -0.088)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95533681 <a title="110-lsi-1" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>Author: Jun Sun ; Min Zhang ; Chew Lim Tan</p><p>Abstract: We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1</p><p>2 0.78892756 <a title="110-lsi-2" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>3 0.64653271 <a title="110-lsi-3" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>Author: WenTing Wang ; Jian Su ; Chew Lim Tan</p><p>Abstract: Syntactic knowledge is important for discourse relation recognition. Yet only heuristically selected flat paths and 2-level production rules have been used to incorporate such information so far. In this paper we propose using tree kernel based approach to automatically mine the syntactic information from the parse trees for discourse analysis, applying kernel function to the tree structures directly. These structural syntactic features, together with other normal flat features are incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification for both explicit and implicit relations. The experiment shows tree kernel approach is able to give statistical significant improvements over flat syntactic path feature. We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further incorporate information from a higher dimension space for possible better discrimination. Besides, we further propose to leverage on temporal ordering information to constrain the interpretation of discourse relation, which also demonstrate statistical significant improvements for discourse relation recognition on PDTB 2.0 for both explicit and implicit as well. University of Singapore Singapore 117417 sg tacl @ comp .nus .edu . sg 1</p><p>4 0.61718446 <a title="110-lsi-4" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>5 0.5850094 <a title="110-lsi-5" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>Author: Wenliang Chen ; Jun'ichi Kazama ; Kentaro Torisawa</p><p>Abstract: This paper proposes a dependency parsing method that uses bilingual constraints to improve the accuracy of parsing bilingual texts (bitexts). In our method, a targetside tree fragment that corresponds to a source-side tree fragment is identified via word alignment and mapping rules that are automatically learned. Then it is verified by checking the subtree list that is collected from large scale automatically parsed data on the target side. Our method, thus, requires gold standard trees only on the source side of a bilingual corpus in the training phase, unlike the joint parsing model, which requires gold standard trees on the both sides. Compared to the reordering constraint model, which requires the same training data as ours, our method achieved higher accuracy because ofricher bilingual constraints. Experiments on the translated portion of the Chinese Treebank show that our system outperforms monolingual parsers by 2.93 points for Chinese and 1.64 points for English.</p><p>6 0.56366014 <a title="110-lsi-6" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>7 0.54119521 <a title="110-lsi-7" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>8 0.5315631 <a title="110-lsi-8" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>9 0.52160627 <a title="110-lsi-9" href="./acl-2010-A_Tree_Transducer_Model_for_Synchronous_Tree-Adjoining_Grammars.html">21 acl-2010-A Tree Transducer Model for Synchronous Tree-Adjoining Grammars</a></p>
<p>10 0.47217929 <a title="110-lsi-10" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>11 0.46257544 <a title="110-lsi-11" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>12 0.46068484 <a title="110-lsi-12" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>13 0.45712373 <a title="110-lsi-13" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>14 0.4527683 <a title="110-lsi-14" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>15 0.45012671 <a title="110-lsi-15" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>16 0.44180542 <a title="110-lsi-16" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>17 0.44006366 <a title="110-lsi-17" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>18 0.43315852 <a title="110-lsi-18" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>19 0.40836552 <a title="110-lsi-19" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>20 0.39191923 <a title="110-lsi-20" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.012), (16, 0.291), (25, 0.042), (33, 0.023), (59, 0.092), (73, 0.037), (78, 0.043), (83, 0.094), (84, 0.024), (98, 0.231)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92441124 <a title="110-lda-1" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>Author: Ondrej Bojar ; Kamil Kos ; David Marecek</p><p>Abstract: We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.</p><p>2 0.91760367 <a title="110-lda-2" href="./acl-2010-Untangling_the_Cross-Lingual_Link_Structure_of_Wikipedia.html">250 acl-2010-Untangling the Cross-Lingual Link Structure of Wikipedia</a></p>
<p>Author: Gerard de Melo ; Gerhard Weikum</p><p>Abstract: Wikipedia articles in different languages are connected by interwiki links that are increasingly being recognized as a valuable source of cross-lingual information. Unfortunately, large numbers of links are imprecise or simply wrong. In this paper, techniques to detect such problems are identified. We formalize their removal as an optimization task based on graph repair operations. We then present an algorithm with provable properties that uses linear programming and a region growing technique to tackle this challenge. This allows us to transform Wikipedia into a much more consistent multilingual register of the world’s entities and concepts.</p><p>same-paper 3 0.83678293 <a title="110-lda-3" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>Author: Jun Sun ; Min Zhang ; Chew Lim Tan</p><p>Abstract: We propose Bilingual Tree Kernels (BTKs) to capture the structural similarities across a pair of syntactic translational equivalences and apply BTKs to sub-tree alignment along with some plain features. Our study reveals that the structural features embedded in a bilingual parse tree pair are very effective for sub-tree alignment and the bilingual tree kernels can well capture such features. The experimental results show that our approach achieves a significant improvement on both gold standard tree bank and automatically parsed tree pairs against a heuristic similarity based method. We further apply the sub-tree alignment in machine translation with two methods. It is suggested that the subtree alignment benefits both phrase and syntax based systems by relaxing the constraint of the word alignment. 1</p><p>4 0.81695026 <a title="110-lda-4" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>Author: Coskun Mermer ; Ahmet Afsin Akin</p><p>Abstract: We tackle the previously unaddressed problem of unsupervised determination of the optimal morphological segmentation for statistical machine translation (SMT) and propose a segmentation metric that takes into account both sides of the SMT training corpus. We formulate the objective function as the posterior probability of the training corpus according to a generative segmentation-translation model. We describe how the IBM Model-1 translation likelihood can be computed incrementally between adjacent segmentation states for efficient computation. Submerging the proposed segmentation method in a SMT task from morphologically-rich Turkish to English does not exhibit the expected improvement in translation BLEU scores and confirms the robustness of phrase-based SMT to translation unit combinatorics. A positive outcome of this work is the described modification to the sequential search algorithm of Morfessor (Creutz and Lagus, 2007) that enables arbitrary-fold parallelization of the computation, which unexpectedly improves the translation performance as measured by BLEU.</p><p>5 0.69685864 <a title="110-lda-5" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>Author: Jinsong Su ; Yang Liu ; Yajuan Lv ; Haitao Mi ; Qun Liu</p><p>Abstract: Lexicalized reordering models play a crucial role in phrase-based translation systems. They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1</p><p>6 0.67875898 <a title="110-lda-6" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>7 0.67836577 <a title="110-lda-7" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>8 0.67671978 <a title="110-lda-8" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>9 0.67649442 <a title="110-lda-9" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>10 0.67633307 <a title="110-lda-10" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>11 0.67618889 <a title="110-lda-11" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>12 0.67535639 <a title="110-lda-12" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>13 0.67467034 <a title="110-lda-13" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>14 0.67418796 <a title="110-lda-14" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>15 0.6727699 <a title="110-lda-15" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>16 0.67205554 <a title="110-lda-16" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>17 0.67103744 <a title="110-lda-17" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>18 0.66906714 <a title="110-lda-18" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>19 0.66906118 <a title="110-lda-19" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>20 0.6683042 <a title="110-lda-20" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
