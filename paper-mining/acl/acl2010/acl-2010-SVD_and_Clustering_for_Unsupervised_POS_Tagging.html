<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-205" href="#">acl2010-205</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</h1>
<br/><p>Source: <a title="acl-2010-205-pdf" href="http://aclweb.org/anthology//P/P10/P10-2040.pdf">pdf</a></p><p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>Reference: <a title="acl-2010-205-reference" href="../acl2010_reference/acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 SVD and Clustering for Unsupervised POS Tagging  Michael Lamar* Division of Applied Mathematics Brown University Providence, RI, USA mlamar@ dam brown edu  . [sent-1, score-0.092]
</p><p>2 au Abstract We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. [sent-6, score-0.08]
</p><p>3 The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. [sent-7, score-0.384]
</p><p>4 As implemented here, it achieves state-of-the-art  tagging accuracy at considerably less cost than more recent methods. [sent-8, score-0.23]
</p><p>5 1  Introduction  While supervised approaches are able to solve the part-of-speech (POS) tagging problem with over 97% accuracy (Collins 2002; Toutanova et al. [sent-10, score-0.23]
</p><p>6 These models attempt to tag text without resources such as an annotated corpus, a dictionary, etc. [sent-12, score-0.071]
</p><p>7 The use of singular value decomposition (SVD) for this problem was introduced in Schütze (1995). [sent-13, score-0.156]
</p><p>8 Subsequently, a number of methods for POS tagging without a dictionary were examined, e. [sent-14, score-0.153]
</p><p>9 Third, it demonstrates that state-of-the-art accuracy can be realized without disambiguation, i. [sent-25, score-0.077]
</p><p>10 , without attempting to assign different tags to different tokens of the same type. [sent-27, score-0.186]
</p><p>11 Finally, with no  significant increase in computational cost, SVD2 can create much finer-grained labelings than typically produced by other algorithms. [sent-28, score-0.059]
</p><p>12 When combined with some minimal supervision in postprocessing, this makes the approach useful for tagging languages that lack the resources required by fully supervised models. [sent-29, score-0.198]
</p><p>13 2  Methods  Following the original work of Schütze (1995), we begin by constructing a right context matrix, R, and a left context matrix, L. [sent-30, score-0.193]
</p><p>14 Rij counts the number of times in the corpus a token of word type i is immediately followed by a token of word type j. [sent-31, score-0.27]
</p><p>15 Similarly, Lij counts the number of times a token of type iis preceded by a token of type j. [sent-32, score-0.27]
</p><p>16 We truncate these matrices, including, in the right and left contexts, only the w1 most frequent word types. [sent-33, score-0.162]
</p><p>17 The resulting L and R are of dimension Ntypes×w1, where Ntypes is the number of word types (spelling forms) in the corpus, and  wm1a tirsi cseest staot i1s0fy0 R0. [sent-34, score-0.036]
</p><p>18 e) ful Ntypes× Ntypes context 215  UppsalaP,r Sowceeeddeinn,g 1s1 o-f16 th Jeu AlyC 2L0 21001. [sent-36, score-0.042]
</p><p>19 c C2o0n1f0er Aenscseoc Sihatoirotn P faopre Crso,m papguetsat 2io1n5a–l2 L1i9n,guistics  Next, both context matrices are factored using singular value decomposition: L = UL SL R = UR SR VRT. [sent-38, score-0.36]
</p><p>20 The diagonal matrices SL and SR (each of rank 1000) are reduced down to rank r1 = 100 by replacing the 900 smallest singular values in each matrix with zeros, yielding SL* and SR*. [sent-39, score-0.503]
</p><p>21 We then form a pair of latent-descriptor matrices defined by:  VLT  L* = UL SL*  R* = UR SR*. [sent-40, score-0.218]
</p><p>22 We next include a normalization step in which each row in each of L* and R* is scaled to unit length, yielding matrices L** and R**. [sent-44, score-0.458]
</p><p>23 Finally, we form a single descriptor matrix D by concatenating these matrices into D = [L** R**]. [sent-45, score-0.556]
</p><p>24 Row iin matrix D is the complete latent descriptor for word type i; this latent descriptor sits on the Cartesian product of two 100-dimensional unit spheres, hereafter the 2-sphere. [sent-46, score-1.008]
</p><p>25 We next categorize these descriptors into  ×  k1= 500 groups, using a k-means clustering algorithm. [sent-47, score-0.39]
</p><p>26 Centroid initialization is done by placing the k initial centroids on the descriptors of the k most frequent words in the corpus. [sent-48, score-0.632]
</p><p>27 As the descriptors sit on the 2-sphere, we measure the proximity of a descriptor to a centroid by the dot product between them; this is equal to the sum of the cosines of the angles—computed on the left and right parts—between them. [sent-49, score-0.805]
</p><p>28 We update each cluster’s centroid as the weighted average of its constituents, the weight being the frequency of the word type; the centroids are then scaled, so they sit on the 2-sphere. [sent-50, score-0.313]
</p><p>29 Typically, only a few dozen iterations are required for full convergence of the clustering algorithm. [sent-51, score-0.079]
</p><p>30 We then apply a second pass of this entire SVD-and-clustering procedure. [sent-52, score-0.09]
</p><p>31 In this second pass, we use the k1= 500 clusters from the first iteration to assemble a new pair of context matrices. [sent-53, score-0.042]
</p><p>32 Now, Rij counts all the cluster-j (j=1 k1) words to the right of word i, and Lij counts all the cluster-j words to the left of word i. [sent-54, score-0.201]
</p><p>33 The new matrices L and R have dimension Ntypes k1. [sent-55, score-0.254]
</p><p>34 As in the first pass, we perform reduced-rank SVD, this time down to rank r2 = 300, and we …  …  …  again normalize the descriptors to unit length, yielding a new pair of latent descriptor matrices L** and R**. [sent-56, score-0.978]
</p><p>35 Finally, we concatenate L** and R** into a single matrix of descriptors, and cluster these descriptors into k2 groups, where k2 is the desired number of induced tags. [sent-57, score-0.597]
</p><p>36 We use the same weighted k-means algorithm as in the first pass, again placing the k initial centroids on the descriptors of the k most frequent words in the corpus. [sent-58, score-0.594]
</p><p>37 The final tag of any token in the corpus is the cluster number of its type. [sent-59, score-0.185]
</p><p>38 Capitalization was ignored, resulting in Ntypes = 43,766, with only a minor effect on accuracy. [sent-61, score-0.049]
</p><p>39 Evaluation was done against the POS-tag annotations of the 45-tag PTB tagset (hereafter PTB45), and against the Smith and Eisner (2005) coarse version of the PTB tagset (hereafter PTB17). [sent-62, score-0.278]
</p><p>40 We selected the three evaluation criteria of Gao and Johnson (2008): M-to-1, 1-to-1, and VI. [sent-63, score-0.045]
</p><p>41 M-to-1 and 1-to-  1 are the tagging accuracies under the best manyto-one map and the greedy one-to-one map respectively; VI is a map-free informationtheoretic criterion—see Gao and Johnson (2008) for details. [sent-64, score-0.374]
</p><p>42 Although we find M-to-1 to be the most reliable criterion of the three, we include the other two criteria for completeness. [sent-65, score-0.08]
</p><p>43 To construct this map, we first find, for each induced tag t, the word type with which it co-occurs most frequently; we call this word type the prototype of t. [sent-67, score-0.39]
</p><p>44 We then query the annotated data for the most common gold tag for each prototype, and we map induced tag t to this gold tag. [sent-68, score-0.449]
</p><p>45 This prototype-based M-to-1 map produces accuracy scores no greater—typically lower—than the best M-to-1 map. [sent-69, score-0.17]
</p><p>46 Here we present the performance of the SVD2 model when k2, the num-  ber of induced tags, is the same or roughly the same as the number of tags in the gold standard—hence small. [sent-72, score-0.287]
</p><p>47 Following Gao and Johnson (2008), the number of induced tags is 17 for PTB17 evaluation and 50 for PTB45 evaluation. [sent-74, score-0.252]
</p><p>48 (2009) who use 45 induced tags for PTB45, the number of induced tags is the same across each column of Table 1. [sent-76, score-0.504]
</p><p>49 082B4 5 VI, for the full PTB45 tagset and the reduced PTB17 tagset. [sent-86, score-0.158]
</p><p>50 To examine the sensitivity of the algorithm to its four parameters, w1, r1, k1, and r2, we changed each of these parameters separately by a multiplicative factor of either 0. [sent-99, score-0.037]
</p><p>51 5 or 2; in neither case did M-to-1 accuracy drop by more than 0. [sent-100, score-0.077]
</p><p>52 Not suffering from the same computational limitations as other models, SVD2 can easily accommodate high numbers of induced tags, resulting in fine-grained labelings. [sent-105, score-0.144]
</p><p>53 Figure 1 shows, as a function of k2, the tagging accuracy of SVD2 under both the best and the prototype-based M-to-1 maps (see Section 3), for both the PTB45 and the PTB17 tagsets. [sent-107, score-0.23]
</p><p>54 The horizontal one-tag-per-word-type line in each panel is the theoretical upper limit  for tagging accuracy in non-disambiguating models (such as SVD2). [sent-108, score-0.325]
</p><p>55 This limit is the fraction of all tokens in the corpus whose gold tag is the most frequent for their type. [sent-109, score-0.285]
</p><p>56 5  Discussion  At the heart of the algorithm presented here is the reduced-rank SVD method of Schütze (1995), which transforms bigram counts into latent descriptors. [sent-110, score-0.153]
</p><p>57 Performance of the SVD2 algorithm as a function of the number of induced tags. [sent-112, score-0.144]
</p><p>58 Each plot shows the tagging accuracy under the best and the prototype-based M-to-1 maps, as well as the upper limit for nondisambiguating taggers. [sent-114, score-0.325]
</p><p>59 which achieves state-of-the-art performance when evaluation is done with the criteria now in common use, Schütze's original work should rightly be praised as ahead of its time. [sent-115, score-0.081]
</p><p>60 Failure to incorporate any one of them signifi217  cantly reduces the performance of the algorithm (M-to-1 reduced by 0. [sent-118, score-0.04]
</p><p>61 First, the reduced-rank left-singular vectors (for the right and left context matrices) are scaled, i. [sent-121, score-0.242]
</p><p>62 While the resulting descriptors, the rows of L* and R*, live in a much lower-dimensional space than the original context vectors, they are mapped by an angle-preserving map (defined by the matrices of right-singular vectors VL and VR) into vectors in the original space. [sent-124, score-0.535]
</p><p>63 These mapped vectors best approximate (in the least-squares sense) the original context vectors; they have the same geometric relationships as their equivalent high-dimensional images, making them good candidates for the role of word-type descriptors. [sent-125, score-0.133]
</p><p>64 A second important feature of the SVD2 algo-  rithm is the unit-length normalization of the latent descriptors, along with the computation of cluster centroids as the weighted averages of their constituent vectors. [sent-126, score-0.37]
</p><p>65 Thanks to this combined device, rare words are treated equally to frequent words regarding the length of their descriptor vectors, yet contribute less to the placement of centroids. [sent-127, score-0.367]
</p><p>66 The values used here were obtained by a coarse, greedy strategy, where each parameter was optimized independently. [sent-132, score-0.035]
</p><p>67 It is worth noting that dispensing with the second pass altogether, i. [sent-133, score-0.09]
</p><p>68 , clustering directly the latent descriptor vectors obtained in the first pass into the desired  number of induced tags, results in a drop of Many-to-1 score of only 0. [sent-135, score-0.759]
</p><p>69 An obvious limitation of SVD2 is that it is a non-disambiguating tagger, assigning the same label to all tokens of a type. [sent-139, score-0.121]
</p><p>70 However, this limitation per se is unlikely to be the main obstacle to the improvement of low-k performance, since, as is well known, the theoretical upper limit for the tagging accuracy of non-disambiguating models (shown in Fig. [sent-140, score-0.368]
</p><p>71 1) is much higher than the current state-of-the-art for unsupervised taggers, whether disambiguating or not. [sent-141, score-0.129]
</p><p>72 To answer this question, we determined, for each word type, the modal HMM state, i. [sent-143, score-0.078]
</p><p>73 , the state most frequently assigned by  the HMM to tokens of that type. [sent-145, score-0.078]
</p><p>74 We then relabeled all words with their modal label. [sent-146, score-0.078]
</p><p>75 The effect of thus eliminating the disambiguation capacity of the model was to slightly increase the tagging accuracy under the best M-to-1 map for every HMM-VB run (the average increase was 0. [sent-147, score-0.438]
</p><p>76 We view this as a further indication that, in the current state of the art and with regards to tagging accuracy, limiting oneself to non-disambiguating models may not adversely affect performance. [sent-150, score-0.153]
</p><p>77 To the contrary, this limitation may actually benefit an approach such as SVD2. [sent-151, score-0.043]
</p><p>78 HMMs are powerful since they can, in theory, induce both a system of tags and a system of contextual patterns that allow them to disambiguate word types in terms of these tags. [sent-154, score-0.108]
</p><p>79 However, carrying out both of these unsupervised learning tasks at once is problematic in view of the very large number of parameters to be estimated compared to the size of the training data set. [sent-155, score-0.08]
</p><p>80 The POS-tagging subtask of disambiguation  may then be construed as a challenge in its own right: demonstrate effective disambiguation in an unsupervised model. [sent-156, score-0.216]
</p><p>81 Specifically, show that tagging accuracy decreases when the model's disambiguation capacity is removed, by re-labeling all tokens with their modal label, defined above. [sent-157, score-0.501]
</p><p>82 We believe that the SVD2 algorithm presented here could provide a launching pad for an approach that would successfully address the disambiguation challenge. [sent-158, score-0.068]
</p><p>83 An important feature of the SVD2 algorithm is its ability to produce a fine-grained labeling of the data, using a number of clusters much larger than the number of tags 218  in a syntax-motivated POS-tag system. [sent-162, score-0.152]
</p><p>84 To achieve a fine-grained labeling, only the final clustering step in the SVD2 algorithm needs to be changed; the computation-  al cost this entails is negligible. [sent-164, score-0.079]
</p><p>85 A high-quality fine-grained labeling, such as achieved by the SVD2 approach, may be of practical interest as an input to various types of unsupervised grammar-induction algorithms (Headden et al. [sent-165, score-0.08]
</p><p>86 One potentially important practical application of a high-quality fine-grained labeling is its use for languages which lack any kind of annotated data. [sent-169, score-0.044]
</p><p>87 By first applying the SVD2 algorithm, word types are grouped together into a few hundred clusters. [sent-170, score-0.078]
</p><p>88 Then, a prototype word is automatically extracted from each cluster. [sent-171, score-0.075]
</p><p>89 This produces, in a completely unsupervised way, a list of only a few hundred words that need to be hand-tagged by an expert. [sent-172, score-0.158]
</p><p>90 1 indicate that these prototype tags can then be used to tag the entire corpus with only a minor decrease in accuracy compared to the best M-to-1 map—the construction of which requires a fully annotated corpus. [sent-174, score-0.425]
</p><p>91 1 also indicates that, with only a few hundred prototypes, the gap left between the accuracy thus achieved and the upper bound for  non-disambiguating models is fairly small. [sent-176, score-0.261]
</p><p>92 A comparison of bayesian estimators for unsupervised Hidden Markov Model POS taggers. [sent-191, score-0.08]
</p><p>93 A fully  Bayesian approach to unsupervised part-of-speech tagging. [sent-199, score-0.125]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('descriptors', 0.311), ('descriptor', 0.248), ('ntypes', 0.248), ('matrices', 0.218), ('gra', 0.186), ('centroids', 0.176), ('tze', 0.17), ('gao', 0.163), ('sch', 0.157), ('johnson', 0.157), ('tagging', 0.153), ('induced', 0.144), ('svd', 0.13), ('tagset', 0.118), ('tags', 0.108), ('latent', 0.107), ('singular', 0.1), ('sl', 0.097), ('map', 0.093), ('vectors', 0.091), ('pass', 0.09), ('matrix', 0.09), ('sr', 0.087), ('bienenstock', 0.083), ('lij', 0.083), ('unsupervised', 0.08), ('clustering', 0.079), ('tokens', 0.078), ('hundred', 0.078), ('modal', 0.078), ('hmm', 0.077), ('accuracy', 0.077), ('centroid', 0.075), ('prototype', 0.075), ('hereafter', 0.073), ('nvi', 0.072), ('elie', 0.072), ('scaled', 0.071), ('tag', 0.071), ('disambiguation', 0.068), ('placement', 0.066), ('headden', 0.066), ('token', 0.062), ('rij', 0.062), ('sit', 0.062), ('reichart', 0.062), ('providence', 0.059), ('geman', 0.059), ('labelings', 0.059), ('ur', 0.059), ('left', 0.059), ('decomposition', 0.056), ('brown', 0.056), ('yielding', 0.055), ('placing', 0.054), ('frequent', 0.053), ('vi', 0.052), ('cluster', 0.052), ('ptb', 0.052), ('right', 0.05), ('type', 0.05), ('minor', 0.049), ('disambiguating', 0.049), ('limit', 0.048), ('capacity', 0.047), ('goldwater', 0.047), ('hmms', 0.047), ('upper', 0.047), ('counts', 0.046), ('iin', 0.046), ('fully', 0.045), ('criteria', 0.045), ('ul', 0.045), ('labeling', 0.044), ('limitation', 0.043), ('context', 0.042), ('coarse', 0.042), ('toutanova', 0.041), ('row', 0.04), ('pos', 0.04), ('reduced', 0.04), ('unit', 0.039), ('initialization', 0.038), ('haghighi', 0.037), ('changed', 0.037), ('neural', 0.037), ('angles', 0.036), ('matlab', 0.036), ('dam', 0.036), ('altogether', 0.036), ('meil', 0.036), ('neuroscience', 0.036), ('praised', 0.036), ('warmuth', 0.036), ('dimension', 0.036), ('gold', 0.035), ('normalization', 0.035), ('greedy', 0.035), ('criterion', 0.035), ('division', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="205-tfidf-1" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>2 0.24069043 <a title="205-tfidf-2" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>3 0.14928181 <a title="205-tfidf-3" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>Author: Sebastian Rudolph ; Eugenie Giesbrecht</p><p>Abstract: We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.</p><p>4 0.12984757 <a title="205-tfidf-4" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>5 0.12226711 <a title="205-tfidf-5" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>6 0.11691455 <a title="205-tfidf-6" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>7 0.094277292 <a title="205-tfidf-7" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>8 0.086599134 <a title="205-tfidf-8" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>9 0.083161108 <a title="205-tfidf-9" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>10 0.080459572 <a title="205-tfidf-10" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>11 0.07992135 <a title="205-tfidf-11" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>12 0.079426825 <a title="205-tfidf-12" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>13 0.072999924 <a title="205-tfidf-13" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>14 0.072487839 <a title="205-tfidf-14" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>15 0.072309867 <a title="205-tfidf-15" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>16 0.07176365 <a title="205-tfidf-16" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>17 0.069155008 <a title="205-tfidf-17" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>18 0.067326158 <a title="205-tfidf-18" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>19 0.062031209 <a title="205-tfidf-19" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>20 0.059271019 <a title="205-tfidf-20" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.204), (1, 0.039), (2, 0.038), (3, 0.0), (4, 0.046), (5, -0.054), (6, 0.053), (7, -0.007), (8, 0.195), (9, 0.063), (10, -0.092), (11, 0.066), (12, 0.064), (13, -0.113), (14, -0.071), (15, -0.085), (16, -0.091), (17, -0.012), (18, 0.07), (19, -0.02), (20, 0.144), (21, 0.157), (22, 0.019), (23, 0.071), (24, -0.007), (25, 0.166), (26, -0.007), (27, 0.006), (28, -0.003), (29, -0.049), (30, 0.001), (31, -0.133), (32, 0.054), (33, 0.075), (34, 0.044), (35, -0.098), (36, 0.109), (37, -0.126), (38, -0.013), (39, 0.009), (40, -0.05), (41, -0.017), (42, 0.02), (43, 0.033), (44, 0.089), (45, 0.03), (46, -0.001), (47, -0.119), (48, -0.017), (49, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95247746 <a title="205-lsi-1" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>2 0.87155616 <a title="205-lsi-2" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>3 0.70224518 <a title="205-lsi-3" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>4 0.62360317 <a title="205-lsi-4" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>Author: Joseph Turian ; Lev-Arie Ratinov ; Yoshua Bengio</p><p>Abstract: If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http ://metaoptimize com/proj ects/wordreprs/ .</p><p>5 0.60870862 <a title="205-lsi-5" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>Author: Sebastian Rudolph ; Eugenie Giesbrecht</p><p>Abstract: We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.</p><p>6 0.57358521 <a title="205-lsi-6" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>7 0.57255369 <a title="205-lsi-7" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>8 0.55385929 <a title="205-lsi-8" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>9 0.51448005 <a title="205-lsi-9" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>10 0.51040161 <a title="205-lsi-10" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>11 0.49775556 <a title="205-lsi-11" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>12 0.47171572 <a title="205-lsi-12" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>13 0.46259698 <a title="205-lsi-13" href="./acl-2010-Online_Generation_of_Locality_Sensitive_Hash_Signatures.html">183 acl-2010-Online Generation of Locality Sensitive Hash Signatures</a></p>
<p>14 0.42276365 <a title="205-lsi-14" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>15 0.41365179 <a title="205-lsi-15" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>16 0.41117439 <a title="205-lsi-16" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>17 0.41035098 <a title="205-lsi-17" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>18 0.40537694 <a title="205-lsi-18" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>19 0.40217909 <a title="205-lsi-19" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>20 0.3951219 <a title="205-lsi-20" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.012), (25, 0.034), (42, 0.015), (59, 0.642), (73, 0.028), (78, 0.025), (83, 0.055), (84, 0.022), (98, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97831488 <a title="205-lda-1" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>2 0.97727722 <a title="205-lda-2" href="./acl-2010-Intelligent_Selection_of_Language_Model_Training_Data.html">151 acl-2010-Intelligent Selection of Language Model Training Data</a></p>
<p>Author: Robert C. Moore ; William Lewis</p><p>Abstract: We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.</p><p>3 0.96384102 <a title="205-lda-3" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>Author: Galina Tremper</p><p>Abstract: Presupposition relations between verbs are not very well covered in existing lexical semantic resources. We propose a weakly supervised algorithm for learning presupposition relations between verbs that distinguishes five semantic relations: presupposition, entailment, temporal inclusion, antonymy and other/no relation. We start with a number of seed verb pairs selected manually for each semantic relation and classify unseen verb pairs. Our algorithm achieves an overall accuracy of 36% for type-based classification.</p><p>4 0.95960522 <a title="205-lda-4" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>Author: Joern Wuebker ; Arne Mauser ; Hermann Ney</p><p>Abstract: Several attempts have been made to learn phrase translation probabilities for phrasebased statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with overfitting. We describe a novel leavingone-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering mod- els in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%.</p><p>5 0.95846075 <a title="205-lda-5" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>Author: Chris Dyer ; Adam Lopez ; Juri Ganitkevitch ; Jonathan Weese ; Ferhan Ture ; Phil Blunsom ; Hendra Setiawan ; Vladimir Eidelman ; Philip Resnik</p><p>Abstract: Adam Lopez University of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</p><p>6 0.84268332 <a title="205-lda-6" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>7 0.83531135 <a title="205-lda-7" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>8 0.77037764 <a title="205-lda-8" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>9 0.7701329 <a title="205-lda-9" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>10 0.7555151 <a title="205-lda-10" href="./acl-2010-Domain_Adaptation_of_Maximum_Entropy_Language_Models.html">91 acl-2010-Domain Adaptation of Maximum Entropy Language Models</a></p>
<p>11 0.74890989 <a title="205-lda-11" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>12 0.74200809 <a title="205-lda-12" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>13 0.7400701 <a title="205-lda-13" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>14 0.73598582 <a title="205-lda-14" href="./acl-2010-BabelNet%3A_Building_a_Very_Large_Multilingual_Semantic_Network.html">44 acl-2010-BabelNet: Building a Very Large Multilingual Semantic Network</a></p>
<p>15 0.73373812 <a title="205-lda-15" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>16 0.72451097 <a title="205-lda-16" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>17 0.72378999 <a title="205-lda-17" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>18 0.72096151 <a title="205-lda-18" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>19 0.71330494 <a title="205-lda-19" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>20 0.70518845 <a title="205-lda-20" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
