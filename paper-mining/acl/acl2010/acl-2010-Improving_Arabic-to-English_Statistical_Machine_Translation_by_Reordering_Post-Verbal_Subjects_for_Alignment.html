<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-145" href="#">acl2010-145</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</h1>
<br/><p>Source: <a title="acl-2010-145-pdf" href="http://aclweb.org/anthology//P/P10/P10-2033.pdf">pdf</a></p><p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>Reference: <a title="acl-2010-145-reference" href="../acl2010_reference/acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). [sent-3, score-0.582]
</p><p>2 We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. [sent-4, score-0.796]
</p><p>3 In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. [sent-5, score-0.67]
</p><p>4 We therefore propose to reorder VS constructions into SV order for SMT word alignment only. [sent-6, score-0.425]
</p><p>5 This strategy significantly improves BLEU and TER scores, even on a strong large-scale  baseline and despite noisy parses. [sent-7, score-0.033]
</p><p>6 1 Introduction Modern Standard Arabic (MSA) is a morphosyntactically complex language, with different phenomena from English, a fact that raises many interesting issues for natural language processing and Arabic-to-English statistical machine translation (SMT). [sent-8, score-0.112]
</p><p>7 While comprehensive Arabic preprocessing schemes have been widely adopted for handling Arabic morphology in SMT (e. [sent-9, score-0.041]
</p><p>8 (2006), Lee (2004)), syntactic issues have not received as much attention by comparison (Green et al. [sent-12, score-0.029]
</p><p>9 Arabic verbal constructions are particularly challenging since subjects can occur in pre-verbal (SV), post-verbal (VS) or pro-dropped (“null subject”) constructions. [sent-14, score-0.327]
</p><p>10 As a result, training data for learning verbal construction translations is split between the different constructions and their patterns; and complex reordering schemas are needed in order to translate them into primarily  pre-verbal subject languages (SVO) such as English. [sent-15, score-0.776]
</p><p>11 Standard phrase-based SMT systems memorize phrasal translation of verb and subject constructions as observed in the training bitext. [sent-18, score-0.465]
</p><p>12 In addition, their distance-based reordering models are not well suited to handling complex reordering operations which can include long distance dependencies, and may vary by context. [sent-20, score-0.74]
</p><p>13 1 However, error analysis shows that verbs are still often dropped or incorrectly translated, and subjects are split or garbled in translation. [sent-22, score-0.108]
</p><p>14 We attempt to get a better understanding of translation patterns for Arabic verb constructions, particularly VS constructions, by studying their  occurrence and reordering patterns in a handaligned Arabic-English parallel treebank. [sent-24, score-0.587]
</p><p>15 Our analysis shows that VS reordering rules are not straightforward and that SMT should therefore benefit from direct modeling of Arabic verb subject translation. [sent-25, score-0.518]
</p><p>16 In order to detect VS constructions, we use our state-of-the-art Arabic dependency parser, which is essentially the CATIBEX baseline in our subsequent parsing work in Marton et al. [sent-26, score-0.058]
</p><p>17 We show that VS subjects and their exact boundaries are hard to identify accurately. [sent-28, score-0.085]
</p><p>18 Given the noise in VS detection, existing strategies for source-side reordering (e. [sent-29, score-0.37]
</p><p>19 c C2o0n1f0er Aenscseoc Sihatoirotn P faopre Crso,m papguetsat 1io7n8a–l1 L8i3n,guistics Table 1: How are Arabic SV and VS translated in the manually word-aligned Arabic-English parallel treebank? [sent-39, score-0.06]
</p><p>20 “Overlap” represents instances where translations of the Arabic verb and subject have some English words in common, and are not monotone nor inverted. [sent-41, score-0.261]
</p><p>21 pendency parses as cohesion constraints in decoding (e. [sent-42, score-0.119]
</p><p>22 While these approaches have been successful for language pairs such as German-English for which syntactic parsers are more developed and relevant reordering patterns might be less ambiguous, their impact potential on  Arabic-English translation is still unclear. [sent-46, score-0.527]
</p><p>23 In this work, we focus on VS constructions only, and propose a new strategy in order to benefit from their noisy detection: for the word alignment stage only, we reorder phrases detected as VS constructions into an SV order. [sent-47, score-0.667]
</p><p>24 This approach significantly improves both BLEU and TER on top of strong medium and large-scale phrase-based SMT baselines. [sent-49, score-0.066]
</p><p>25 2  VS reordering in gold Arabic-English translation  We use the manually word-aligned parallel Arabic-English Treebank (LDC2009E82) to study how Arabic VS constructions are translated into English by humans. [sent-50, score-0.77]
</p><p>26 Given the gold Arabic syntactic parses and the manual Arabic-English word alignments, we can determine the gold reorderings for SV and VS constructions. [sent-51, score-0.115]
</p><p>27 We extract VS representations from the gold constituent parses  by deterministic conversion to a simplified dependency structure, CATiB (Habash and Roth, 2009) (see Section 3). [sent-52, score-0.124]
</p><p>28 We then check whether the English translations of the Arabic verb and the Arabic subject occur in the same order as in Arabic (monotone) or not (inverted). [sent-53, score-0.195]
</p><p>29 Table 1 summarizes the reordering patterns for each category. [sent-54, score-0.402]
</p><p>30 As expected, 98% of Arabic SV are translated in a monotone order in English. [sent-55, score-0.102]
</p><p>31 The monotone VS translations are mostly explained by changes to passive voice or to non-verbal constructions (such as nominalization) in the English translation. [sent-57, score-0.355]
</p><p>32 In addition, Table 1shows that verb subjects occur more frequently in VS order (70%) than in SV order (30%). [sent-58, score-0.139]
</p><p>33 3  Arabic VS construction detection  Even if the SMT system had perfect knowledge of VS reordering, it has to accurately detect VS constructions and their spans in order to apply  the reordering correctly. [sent-60, score-0.698]
</p><p>34 We convert the PATB3 representation into the succinct CATiB format, with 8 dependency relations and 6 POS tags, which we then extend to a set of 44 tags using regular expressions of the basic POS and the normalized surface word form, similarly to Marton et al. [sent-71, score-0.036]
</p><p>35 For analysis purposes, we evaluate our subject and verb detection on the development part of PATB3 using gold POS tags. [sent-74, score-0.235]
</p><p>36 We argue that combined detection statistics of constructions of verbs and their subjects (VATS), for which we achieve an F-score of 74%, are more telling for the task at hand. [sent-76, score-0.391]
</p><p>37 2 2We divert from the CATiB representation in that a nonmatrix subject of a pseudo verb (An and her sisters) is treated as a subject of the verb that is under the same pseudo verb. [sent-77, score-0.344]
</p><p>38 This treatment of said subjects is comparable to the PATB’s. [sent-78, score-0.085]
</p><p>39 179  These scores take into account the spans of both the subject and the specific verb it belongs to, and potentially reorder with. [sent-79, score-0.298]
</p><p>40 We also provide statistics of VS detection separately (F-score 63%), since we only handle VS here. [sent-80, score-0.064]
</p><p>41 This low score can be explained by the difficulty in detecting the postverbal subject’s end boundary, and the correct verb the subject belongs to. [sent-81, score-0.196]
</p><p>42 The SV construction scores are higher, presumably since the pre-verbal subject’s end is bounded by the verb it belongs to. [sent-82, score-0.109]
</p><p>43 Although not directly comparable, our VS scores are similar to those of Green et al. [sent-84, score-0.03]
</p><p>44 Their VS detection technique with conditional  random fields (CRF) is different from ours in bypassing full syntactic parsing, and in only detecting maximal (non-nested) subjects of verb-initial clauses. [sent-86, score-0.201]
</p><p>45 Note that a closer score comparison should take into account their reported verb detection accuracy of 98. [sent-91, score-0.118]
</p><p>46 Table 2: Precision, Recall and F-scores for constructions of Arabic verbs and their subjects, evaluated on our development part of PATB3. [sent-93, score-0.242]
</p><p>47 4  Reordering Arabic VS for SMT word alignment  Based on these analyses, we propose a new method to help phrase-based SMT systems deal with Arabic-English word order differences due to VS constructions. [sent-94, score-0.088]
</p><p>48 As in related work on syntactic reordering by preprocessing, our method attempts to make Arabic and English word order closer to each other by reordering Arabic VS constructions into SV. [sent-95, score-1.011]
</p><p>49 However, unlike in previous work, the reordered Arabic sentences are used only for word alignment. [sent-96, score-0.052]
</p><p>50 Phrase translation extraction and decoding are performed on the original Arabic word order. [sent-97, score-0.129]
</p><p>51 Limiting reordering to alignment allows the sys-  tem to be more robust and recover from incorrect changes introduced either by incorrect VS detection, or by incorrect reordering of a correctly detected VS. [sent-103, score-0.828]
</p><p>52 generate new sentence a0 = reorder(a) by reordering Arabic VS into SV 3. [sent-106, score-0.37]
</p><p>53 get word alignment wa0 on new sentence pair (a0, e) 4. [sent-107, score-0.088]
</p><p>54 using mapping from a to a0, get corresponding word alignment wa = unreorder(wa0) for the original sentence pair (a, e) 5  Experiment set-up  We use the open-source Moses toolkit (Koehn et al. [sent-108, score-0.088]
</p><p>55 , 2007) to build two phrase-based SMT systems trained on two different data conditions: •  medium-scale the bitext consists of 12M wmoedrdisu on tchael eA trhaebic b tseixdet (LDC2007E103). [sent-109, score-0.041]
</p><p>56 •  large-scale the bitext consists of several lnaerwgsew-sicrae eLD tChe corpora, acnonds ihsatss 6 o4fM s wevoerrdasl on the Arabic side. [sent-111, score-0.041]
</p><p>57 The language model is trained on the English side of the bitext augmented with Gigaword data. [sent-112, score-0.041]
</p><p>58 The parallel corpus is word-aligned using the GIZA++ (Och and Ney, 2003), which sequentially learns word alignments for the IBM1, HMM, IBM3 and IBM4 models. [sent-115, score-0.053]
</p><p>59 The resulting alignments in both translation directions are intersected and augmented using the grow-diag-final-and heuristic (Koehn et al. [sent-116, score-0.104]
</p><p>60 Phrase translations of up to 10 words are extracted in the Moses phrase-table. [sent-118, score-0.047]
</p><p>61 We apply statistical significance tests to prune unreliable phrase-pairs 180  and score remaining phrase-table entries (Chen et al. [sent-119, score-0.037]
</p><p>62 6  Results  We evaluate translation quality using both BLEU (Papineni et al. [sent-127, score-0.075]
</p><p>63 , 2006) scores on three standard evaluation test sets from the NIST evaluations, which yield more than 4400 test sentences with 4 reference translations. [sent-129, score-0.03]
</p><p>64 On this large data set, our VS reordering method remarkably yields statistically significant improvements in BLEU and TER on the medium and large SMT systems at the 99% confidence level (Table 3). [sent-130, score-0.438]
</p><p>65 TER  scores are improved in all 10 test configurations, and BLEU scores are improved in 8 out of the 10 configurations. [sent-132, score-0.06]
</p><p>66 Results on the MT08 test set show that improvements are obtained both on newswire and on web text as measured by TER (but not BLEU score on the web section. [sent-133, score-0.035]
</p><p>67 ) It is worth noting that consistent improvements are obtained even on the large-scale system, and that both baselines are full-fledged systems, which include lexicalized reordering and large 5-gram language models. [sent-134, score-0.405]
</p><p>68 Analysis shows that our VS reordering technique improves word alignment coverage (yielding 48k and 330k additional links on the medium and large scale systems respectively). [sent-135, score-0.524]
</p><p>69 This results in larger phrase-tables which improve translation quality. [sent-136, score-0.075]
</p><p>70 7  Related work  To the best of our knowledge, the only other approach to detecting and using Arabic verb-subject constructions for SMT is that of Green et al. [sent-137, score-0.265]
</p><p>71 5264  ages a phrase-based SMT decoder to use phrasal translations that do not break subject boundaries. [sent-143, score-0.141]
</p><p>72 Syntactically motivated reordering for phrasebased SMT has been more successful on language pairs other than Arabic-English, perhaps due to more accurate parsers and less ambiguous reordering patterns than for Arabic VS. [sent-144, score-0.794]
</p><p>73 (2005) apply six manually defined transformations to German parse trees which improve German-English translation by 0. [sent-146, score-0.075]
</p><p>74 Xia and McCord (2004) learn reordering rules for French to English translations, which arguably presents less syntactic distortion than Arabic-English. [sent-148, score-0.399]
</p><p>75 (2007) limit reordering to decoding for Chinese-English SMT using a lattice representation. [sent-150, score-0.446]
</p><p>76 Cherry (2008) uses dependency parses as cohesion constraints in decoding for French-English SMT. [sent-151, score-0.155]
</p><p>77 For Arabic-English phrase-based SMT, the impact of syntactic reordering as preprocessing is less clear. [sent-152, score-0.461]
</p><p>78 Habash (2007) proposes to learn syntactic reordering rules targeting Arabic-English word order differences and integrates them as deterministic preprocessing. [sent-153, score-0.424]
</p><p>79 He reports improvements in BLEU compared to phrase-based SMT limited to monotonic decoding, but these improvements do not hold with distortion. [sent-154, score-0.07]
</p><p>80 Instead of applying reordering rules deterministically, Crego and Habash (2008) use a lattice input to represent alternate word orders which improves a ngram-based SMT system. [sent-155, score-0.425]
</p><p>81 Most previous syntax-aware word alignment  models were specifically designed for syntaxbased SMT systems. [sent-157, score-0.088]
</p><p>82 These models are often bootstrapped from existing word alignments, and could therefore benefit from our VS reordering approach. [sent-158, score-0.37]
</p><p>83 5 BLEU on Arabic translation by learning to delete alignment 181 Table 4: VS reordering improves BLEU and TER scores in almost all test conditions on 5 test sets, 2 metrics, and 2 MT systems  BLEU r4n4 (%)  mm++lleeVVaaddrrSSgiigtutueermmrsebbestotoaabssrdreedaatstellirnirnelgeng454M2M68T2. [sent-162, score-0.596]
</p><p>84 7162959))  links if they degrade their syntax-based translation system. [sent-182, score-0.075]
</p><p>85 Departing from commonly-used alignment models, Hermjakob (2009) aligns Arabic and English content words using pointwise mutual information, and in this process indirectly uses English sentences reordered into VS order to collect cooccurrence counts. [sent-183, score-0.14]
</p><p>86 The approach outperforms GIZA++ on a small-scale translation task, but the  impact of reordering alone is not evaluated. [sent-184, score-0.466]
</p><p>87 8  Conclusion and future work  We presented a novel method for improving overall SMT quality using a noisy syntactic parser: we use these parses to reorder VS constructions into SV for word alignment only. [sent-185, score-0.494]
</p><p>88 This approach increases word alignment coverage and significantly improves BLEU and TER scores on two strong SMT baselines. [sent-186, score-0.151]
</p><p>89 In subsequent work, we show that matrix (mainclause) VS constructions are reordered much more frequently than non-matrix VS, and that limiting reordering to matrix VS constructions for word alignment further improves translation quality (Carpuat et al. [sent-187, score-1.164]
</p><p>90 In the future, we plan to improve robustness to parsing errors by using not just one, but multiple subject boundary hypotheses. [sent-189, score-0.094]
</p><p>91 We will also investigate the integration of VS reordering in SMT decoding. [sent-190, score-0.37]
</p><p>92 Using shallow syntax information to improve word alignment and reordering for SMT. [sent-217, score-0.458]
</p><p>93 Using syntax to improve word alignment precision for syntax-based machine translation. [sent-221, score-0.088]
</p><p>94 Enhancing the arabic treebank: a collaborative effort toward new annotation guidelines. [sent-263, score-0.42]
</p><p>95 Improving arabic dependency parsing with lexical and inflectional morphological features. [sent-272, score-0.485]
</p><p>96 Combination of arabic preprocessing schemes for statistical machine translation. [sent-296, score-0.498]
</p><p>97 A study of translation edit rate with targeted human annotation. [sent-300, score-0.075]
</p><p>98 Improving a statistical mt system with automatically learned rewrite patterns. [sent-308, score-0.037]
</p><p>99 Chunklevel reordering of source language sentences with automatically learned rules for statistical machine translation. [sent-312, score-0.407]
</p><p>100 Bridging the inflection morphology gap for arabic statistical machine translation. [sent-323, score-0.457]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('vs', 0.502), ('arabic', 0.42), ('reordering', 0.37), ('constructions', 0.242), ('smt', 0.214), ('habash', 0.185), ('sv', 0.149), ('nizar', 0.13), ('bleu', 0.115), ('reorder', 0.095), ('maamouri', 0.095), ('subject', 0.094), ('alignment', 0.088), ('catib', 0.087), ('ter', 0.086), ('subjects', 0.085), ('marton', 0.079), ('translation', 0.075), ('monotone', 0.066), ('crego', 0.065), ('detection', 0.064), ('nivre', 0.06), ('verb', 0.054), ('decoding', 0.054), ('marine', 0.052), ('reordered', 0.052), ('koehn', 0.051), ('green', 0.05), ('carpuat', 0.049), ('xia', 0.049), ('translations', 0.047), ('alif', 0.043), ('catibex', 0.043), ('mccord', 0.043), ('patb', 0.043), ('preprocessing', 0.041), ('joakim', 0.041), ('yuval', 0.041), ('bitext', 0.041), ('owen', 0.041), ('parses', 0.04), ('columbia', 0.039), ('philipp', 0.038), ('statistical', 0.037), ('dependency', 0.036), ('translated', 0.036), ('improvements', 0.035), ('mohamed', 0.035), ('sadat', 0.035), ('medium', 0.033), ('bach', 0.033), ('inverted', 0.033), ('fossum', 0.033), ('improves', 0.033), ('treebank', 0.032), ('patterns', 0.032), ('seth', 0.031), ('zitouni', 0.031), ('bies', 0.031), ('zollmann', 0.031), ('maltparser', 0.031), ('roth', 0.03), ('scores', 0.03), ('collins', 0.029), ('alignments', 0.029), ('syntactic', 0.029), ('morphological', 0.029), ('ann', 0.028), ('cohesive', 0.028), ('snover', 0.028), ('moses', 0.028), ('proceedings', 0.027), ('belongs', 0.025), ('cohesion', 0.025), ('deterministic', 0.025), ('meeting', 0.024), ('parallel', 0.024), ('pseudo', 0.024), ('association', 0.024), ('stephan', 0.023), ('split', 0.023), ('gold', 0.023), ('nist', 0.023), ('cherry', 0.023), ('detecting', 0.023), ('lattice', 0.022), ('tokenization', 0.022), ('limiting', 0.022), ('companion', 0.022), ('english', 0.022), ('detect', 0.022), ('arbor', 0.022), ('ambiguous', 0.022), ('papers', 0.021), ('impact', 0.021), ('boston', 0.021), ('alexandra', 0.021), ('och', 0.02), ('matrix', 0.02), ('ryan', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="145-tfidf-1" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>2 0.2985318 <a title="145-tfidf-2" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>Author: Jinsong Su ; Yang Liu ; Yajuan Lv ; Haitao Mi ; Qun Liu</p><p>Abstract: Lexicalized reordering models play a crucial role in phrase-based translation systems. They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1</p><p>3 0.26528829 <a title="145-tfidf-3" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>Author: Yassine Benajiba ; Imed Zitouni ; Mona Diab ; Paolo Rosso</p><p>Abstract: Building an accurate Named Entity Recognition (NER) system for languages with complex morphology is a challenging task. In this paper, we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute).</p><p>4 0.20428413 <a title="145-tfidf-4" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>Author: Bing Xiang ; Yonggang Deng ; Bowen Zhou</p><p>Abstract: We present a novel method to improve word alignment quality and eventually the translation performance by producing and combining complementary word alignments for low-resource languages. Instead of focusing on the improvement of a single set of word alignments, we generate multiple sets of diversified alignments based on different motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better transla- tion performance.</p><p>5 0.19677271 <a title="145-tfidf-5" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>6 0.18404099 <a title="145-tfidf-6" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>7 0.17896809 <a title="145-tfidf-7" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>8 0.17032297 <a title="145-tfidf-8" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>9 0.16393307 <a title="145-tfidf-9" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>10 0.16387464 <a title="145-tfidf-10" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>11 0.13567163 <a title="145-tfidf-11" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>12 0.13389474 <a title="145-tfidf-12" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>13 0.11882523 <a title="145-tfidf-13" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>14 0.11764578 <a title="145-tfidf-14" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>15 0.11228514 <a title="145-tfidf-15" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>16 0.10869961 <a title="145-tfidf-16" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>17 0.10379012 <a title="145-tfidf-17" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>18 0.094275273 <a title="145-tfidf-18" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>19 0.093323797 <a title="145-tfidf-19" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>20 0.090833813 <a title="145-tfidf-20" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.219), (1, -0.262), (2, -0.047), (3, 0.011), (4, 0.063), (5, 0.063), (6, -0.097), (7, 0.027), (8, -0.015), (9, 0.154), (10, 0.148), (11, 0.196), (12, 0.021), (13, 0.005), (14, 0.043), (15, -0.008), (16, 0.004), (17, 0.195), (18, 0.169), (19, -0.131), (20, -0.094), (21, 0.034), (22, 0.067), (23, 0.051), (24, 0.099), (25, 0.015), (26, -0.166), (27, -0.005), (28, -0.071), (29, 0.004), (30, 0.109), (31, 0.014), (32, -0.199), (33, 0.076), (34, -0.073), (35, 0.084), (36, -0.093), (37, -0.008), (38, 0.155), (39, 0.021), (40, 0.079), (41, -0.009), (42, 0.038), (43, 0.019), (44, -0.136), (45, 0.024), (46, -0.127), (47, -0.015), (48, 0.047), (49, -0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9566105 <a title="145-lsi-1" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>2 0.71300846 <a title="145-lsi-2" href="./acl-2010-Learning_Lexicalized_Reordering_Models_from_Reordering_Graphs.html">163 acl-2010-Learning Lexicalized Reordering Models from Reordering Graphs</a></p>
<p>Author: Jinsong Su ; Yang Liu ; Yajuan Lv ; Haitao Mi ; Qun Liu</p><p>Abstract: Lexicalized reordering models play a crucial role in phrase-based translation systems. They are usually learned from the word-aligned bilingual corpus by examining the reordering relations of adjacent phrases. Instead of just checking whether there is one phrase adjacent to a given phrase, we argue that it is important to take the number of adjacent phrases into account for better estimations of reordering models. We propose to use a structure named reordering graph, which represents all phrase segmentations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1</p><p>3 0.66171384 <a title="145-lsi-3" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>Author: Yassine Benajiba ; Imed Zitouni ; Mona Diab ; Paolo Rosso</p><p>Abstract: Building an accurate Named Entity Recognition (NER) system for languages with complex morphology is a challenging task. In this paper, we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute).</p><p>4 0.62763834 <a title="145-lsi-4" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>5 0.54951125 <a title="145-lsi-5" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>Author: Seth Kulick</p><p>Abstract: We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.</p><p>6 0.53971386 <a title="145-lsi-6" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>7 0.49434561 <a title="145-lsi-7" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>8 0.49376836 <a title="145-lsi-8" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>9 0.48375019 <a title="145-lsi-9" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>10 0.46227261 <a title="145-lsi-10" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>11 0.42560834 <a title="145-lsi-11" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>12 0.39942688 <a title="145-lsi-12" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>13 0.3914938 <a title="145-lsi-13" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>14 0.38248664 <a title="145-lsi-14" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>15 0.3755883 <a title="145-lsi-15" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>16 0.35537529 <a title="145-lsi-16" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>17 0.35304868 <a title="145-lsi-17" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>18 0.33080024 <a title="145-lsi-18" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>19 0.32506287 <a title="145-lsi-19" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>20 0.3223812 <a title="145-lsi-20" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.017), (16, 0.029), (18, 0.034), (25, 0.072), (39, 0.013), (42, 0.012), (44, 0.011), (51, 0.15), (54, 0.013), (59, 0.141), (73, 0.038), (76, 0.021), (78, 0.02), (80, 0.011), (83, 0.119), (84, 0.019), (89, 0.026), (98, 0.176)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96513468 <a title="145-lda-1" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>Author: Daniil Umanski ; Federico Sangati</p><p>Abstract: The growing availability of spoken language corpora presents new opportunities for enriching the methodologies of speech and language therapy. In this paper, we present a novel approach for constructing speech motor exercises, based on linguistic knowledge extracted from spoken language corpora. In our study with the Dutch Spoken Corpus, syllabic inventories were obtained by means of automatic syllabification of the spoken language data. Our experimental syllabification method exhibited a reliable performance, and allowed for the acquisition of syllabic tokens from the corpus. Consequently, the syl- labic tokens were integrated in a tool for clinicians, a result which holds the potential of contributing to the current state of speech motor training methodologies.</p><p>2 0.93208134 <a title="145-lda-2" href="./acl-2010-A_Bayesian_Method_for_Robust_Estimation_of_Distributional_Similarities.html">3 acl-2010-A Bayesian Method for Robust Estimation of Distributional Similarities</a></p>
<p>Author: Jun'ichi Kazama ; Stijn De Saeger ; Kow Kuroda ; Masaki Murata ; Kentaro Torisawa</p><p>Abstract: Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words’ context profiles obtained from a limited amount of data. This paper proposes a Bayesian method for robust distributional word similarities. The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution. When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is . the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation. For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures.</p><p>same-paper 3 0.91299868 <a title="145-lda-3" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>Author: Marine Carpuat ; Yuval Marton ; Nizar Habash</p><p>Abstract: We study the challenges raised by Arabic verb and subject detection and reordering in Statistical Machine Translation (SMT). We show that post-verbal subject (VS) constructions are hard to translate because they have highly ambiguous reordering patterns when translated to English. In addition, implementing reordering is difficult because the boundaries of VS constructions are hard to detect accurately, even with a state-of-the-art Arabic dependency parser. We therefore propose to reorder VS constructions into SV order for SMT word alignment only. This strategy significantly improves BLEU and TER scores, even on a strong large-scale baseline and despite noisy parses.</p><p>4 0.84836477 <a title="145-lda-4" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>5 0.84708714 <a title="145-lda-5" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>Author: Xianpei Han ; Jun Zhao</p><p>Abstract: Name ambiguity problem has raised urgent demands for efficient, high-quality named entity disambiguation methods. In recent years, the increasing availability of large-scale, rich semantic knowledge sources (such as Wikipedia and WordNet) creates new opportunities to enhance the named entity disambiguation by developing algorithms which can exploit these knowledge sources at best. The problem is that these knowledge sources are heterogeneous and most of the semantic knowledge within them is embedded in complex structures, such as graphs and networks. This paper proposes a knowledge-based method, called Structural Semantic Relatedness (SSR), which can enhance the named entity disambiguation by capturing and leveraging the structural semantic knowledge in multiple knowledge sources. Empirical results show that, in comparison with the classical BOW based methods and social network based methods, our method can significantly improve the disambiguation performance by respectively 8.7% and 14.7%. 1</p><p>6 0.84584939 <a title="145-lda-6" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>7 0.84445739 <a title="145-lda-7" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>8 0.84396851 <a title="145-lda-8" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>9 0.84351814 <a title="145-lda-9" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>10 0.84271765 <a title="145-lda-10" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>11 0.84218585 <a title="145-lda-11" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>12 0.84209621 <a title="145-lda-12" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>13 0.84116375 <a title="145-lda-13" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>14 0.84093153 <a title="145-lda-14" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>15 0.84038591 <a title="145-lda-15" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>16 0.84010941 <a title="145-lda-16" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>17 0.83958912 <a title="145-lda-17" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>18 0.83944166 <a title="145-lda-18" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>19 0.83873713 <a title="145-lda-19" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>20 0.83760512 <a title="145-lda-20" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
