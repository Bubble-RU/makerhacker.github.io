<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-33" href="#">acl2010-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</h1>
<br/><p>Source: <a title="acl-2010-33-pdf" href="http://aclweb.org/anthology//P/P10/P10-1123.pdf">pdf</a></p><p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>Reference: <a title="acl-2010-33-reference" href="../acl2010_reference/acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 i l  ,  Abstract Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. [sent-4, score-0.862]
</p><p>2 On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal. [sent-5, score-1.359]
</p><p>3 1 Introduction The detection and resolution of discourse references such as coreference and bridging anaphora  play an important role in text understanding applications, like question answering and information extraction. [sent-6, score-1.147]
</p><p>4 However, the utilization of discourse information for such inferences has been so far limited mainly to the substitution of nominal coreferents, while many aspects of the interface between discourse and semantic inference needs remain unexplored. [sent-20, score-0.962]
</p><p>5 Our focus is on a manual, in-depth assessment that results in a classification and quantification of discourse reference phenomena and their utilization for inference. [sent-25, score-0.596]
</p><p>6 On this basis, we develop an account of formal devices for incorporating discourse references into the inference computation. [sent-26, score-0.435]
</p><p>7 An additional point of interest is the interrelation between entailment knowledge and coreference. [sent-27, score-0.481]
</p><p>8 Conversely, coreference resolution can often be used to overcome gaps in entailment knowledge. [sent-31, score-0.885]
</p><p>9 Reference information provided by discourse is also useful for text understanding tasks such as question answering (QA), information extraction (IE) and information retrieval (IR) (Vicedo and Ferrndez, 2006; Zelenko et al. [sent-42, score-0.394]
</p><p>10 A second, more complex type of information stems from bridging references, such as in the following discourse (Asher and Lascarides, 1998): (2) “I’ve just arrived. [sent-54, score-0.667]
</p><p>11 ” While coreference indicates equivalence, bridging points to the existence of a salient semantic relation between two distinct entities or events. [sent-56, score-0.677]
</p><p>12 Here, it is (informally) ‘means of transport’, which would make the discourse (2) relevant for a question like How did I arrive here? [sent-57, score-0.395]
</p><p>13 An important reason is the unavailability of tools to resolve the more complex (and difficult) forms of discourse reference such as event coreference and bridging. [sent-65, score-0.88]
</p><p>14 The goal of transformation-based TE models is to determine the entailment relation T ⇒ H by finding a “proof”, i. [sent-77, score-0.505]
</p><p>15 While transformation- and alignment-based entailment models look different at first glance, they ultimately have the same goal, namely obtaining  a maximal coverage of H by T, i. [sent-91, score-0.437]
</p><p>16 , 2004), address the resolution of a few specific kinds of bridging relations; yet, wide-scope systems for bridging resolution are unavailable. [sent-98, score-0.89]
</p><p>17 2Clearly, the details of how the final entailment decision is made based on the attained coverage differ substantially among models. [sent-99, score-0.437]
</p><p>18 A first step towards a more comprehensive notion of entailment was taken with RTE-3 (Giampiccolo et al. [sent-106, score-0.437]
</p><p>19 A number of systems have tried to address the question of coreference in RTE as a preprocessing step prior to inference proper, with most systems using off-the-shelf coreference resolvers such as JavaRap (Qiu et al. [sent-112, score-0.697]
</p><p>20 Results were inconclusive, however, with several reports about errors introduced by automatic coreference resolution (Agichtein et al. [sent-115, score-0.413]
</p><p>21 Specific evaluations of the contribution of coreference resolution yielded both small negative (Bar-Haim et al. [sent-118, score-0.413]
</p><p>22 2, seem to show that current resolution of discourse references in RTE systems hardly affects performance. [sent-122, score-0.504]
</p><p>23 net  sented; (2) the off-the-shelf coreference resolution systems which may have been not robust enough; (3) the limitation to nominal coreference; and (4)  overly simple integration of reference information into the inference engines. [sent-125, score-0.732]
</p><p>24 The goal of this paper is to assess the impact of discourse references on entailment with an annotation study which removes these limitations. [sent-126, score-0.8]
</p><p>25 With regards to (3), our annotation scheme covers coreference and bridging relations of all syntactic categories and classifies them. [sent-129, score-0.628]
</p><p>26 As for (4), we suggest several operations necessary to integrate the discourse information into an entailment engine. [sent-130, score-0.8]
</p><p>27 Furthermore, we analyze each instance from an entailment perspective, characterizing the relevant factors that have an impact on inference. [sent-135, score-0.469]
</p><p>28 Second, they point out potential directions for the developers of inference systems by specifying what additional inference mechanisms are needed to utilize discourse information. [sent-139, score-0.507]
</p><p>29 i /˜nlp / downl oads / l 1211  depends  on  reference information from the discourse  sentences  T0 / T00. [sent-151, score-0.54]
</p><p>30 4  Analysis Scheme  For annotating the RTE-5 data, we operationalize reference relations that are relevant for entailment as those that improve coverage. [sent-153, score-0.698]
</p><p>31 For each T − H pair, we annotate all relevant disFcoorur esaec h ref Ter −en Hces p aini t,e wrmes a nofn othtarteee iltle mresle: tahnet target component in H, the focus term in T, and the reference term which stands in a reference relation to the focus term. [sent-163, score-0.919]
</p><p>32 By resolving this reference, the target component can usually be inferred; sometimes, however, more than one reference term needs to be found. [sent-164, score-0.461]
</p><p>33 An example for a tree component is Example (v), where the target component AS-28 mini submarine in H cannot be inferred from the pronoun it in T. [sent-167, score-0.453]
</p><p>34 For each target component, we identify its focus term as the expression in T that does not cover the target component itself but participates in a reference relation that can help covering it. [sent-174, score-0.677]
</p><p>35 We follow the focus term’s reference chain to a reference term which can, either separately or in combination with the focus term, help covering the target component. [sent-175, score-0.631]
</p><p>36 1212  target component in H is 2003 UB313, Xena is the focus term in T and the reference term is a mention of 2003 UB313 in a previous sentence, T0. [sent-177, score-0.586]
</p><p>37 In this case, the reference term covers the entire target component on its own. [sent-178, score-0.428]
</p><p>38 An additional attribute that we record for each instance is whether resolving the discourse reference is mandatory for determining entailment, or optional. [sent-179, score-0.623]
</p><p>39 It can be done either by identifying their coreference relation, or by using background knowledge in the form of an entailment rule, ‘Xena ↔ 2003 UB313’, that is applicable rinu teh,e ‘ Xceonntaex ↔t ↔o f2 astronomy. [sent-182, score-0.786]
</p><p>40 Optional dpipscliocuabrslee references represent instances where discourse information and TE knowledge are interchange-  able. [sent-183, score-0.407]
</p><p>41 At the same time, this scheme allows investigating how much TE knowledge can be replaced by (perfect) discourse processing. [sent-186, score-0.407]
</p><p>42 When choosing a reference term, we search the reference chain of the focus term for the nearest expression that is identical to the target component or a subcomponent of it. [sent-187, score-0.661]
</p><p>43 If we find such an expression, covering the identical part of the target component requires no entailment knowledge. [sent-188, score-0.586]
</p><p>44 If no identical reference term exists, we choose the semantically ‘closest’ term from the reference chain, i. [sent-189, score-0.558]
</p><p>45 For instance, we may  pick permafrost as the semantically closet term to the target ice if the latter is not found in the focus term’s reference chain. [sent-192, score-0.493]
</p><p>46 First, the reference type: Is the relation a coreference or a bridging reference? [sent-194, score-0.821]
</p><p>47 Third, the focus/reference terms entailment status does some kind of entailment relation hold between the two terms? [sent-196, score-0.975]
</p><p>48 Fourth, the operation that should be performed on the focus and reference terms to obtain coverage of the target component (as specified in Section 5). [sent-197, score-0.415]
</p><p>49 –  5  Integrating Discourse References into Entailment Recognition  In initial analysis we found that the standard substitution operation applied by virtually all previous studies for integrating coreference into entailment is insufficient. [sent-198, score-0.803]
</p><p>50 We identified three distinct cases for the integration of discourse reference knowl-  edge in entailment, which correspond to different relations between the target component, the focus term and the reference term. [sent-199, score-1.024]
</p><p>51 We assume that we have access to a dependency tree for H, a dependency forest for T and its discourse context, as well as the output of a perfect discourse processor, i. [sent-208, score-0.768]
</p><p>52 , a complete set of both coreference and bridging relations, including the type of bridging relation (e. [sent-210, score-0.948]
</p><p>53 We write C(x, y) for a coreference relation between Sx and Sy, the corresponding trees of the focus and reference terms, respectively. [sent-218, score-0.573]
</p><p>54 Section 6), substitution applies also to some types of bridging relations, such as set-membership, when the member is sufficient for representing the entire set for the necessary inference. [sent-226, score-0.398]
</p><p>55 ” In a parse tree representation, given a coreference relation C(x, y) (or Br(x, y)), the newly generated tree, T1, consists of a copy of T, where the entire tree Sx is replaced by a copy of Sy . [sent-230, score-0.576]
</p><p>56 An alternative way to recover the missing information in Example (iii) is to find a reference term whose head word itself (rather than one of its modifiers) matches the target component’s missing dependent, as with AS-28 in Figure 2 in the bottom left corner (Tb0). [sent-254, score-0.448]
</p><p>57 (3) Insertion: The last transformation, insertion, is used when a relation that is realized in H is missing from T and is only implied via a bridging relation. [sent-265, score-0.408]
</p><p>58 In Example (iv), the location that is explicitly mentioned in H can only be covered by T by resolving a bridging reference with China in T0. [sent-266, score-0.514]
</p><p>59 To connect the bridging referents, a new tree component representing the bridging relation is inserted into the consequent tree T1. [sent-267, score-0.846]
</p><p>60 Formally, given a bridging relation Br(x, y), we introduce a new subtree Szr into T1, where z is a child of x and lab(z) = labr. [sent-269, score-0.416]
</p><p>61 These two items form the dependency representation of the bridging relation Br and must be provided by the interface between the discourse and the inference systems. [sent-273, score-0.807]
</p><p>62 Clearly,  their exact form depends on the set of bridging relations provided by the discourse resolver as well as the details of the dependency parses. [sent-274, score-0.719]
</p><p>63 As shown in Figure 3, the bridging relation located-in (r) is represented by inserting a subtree Szr headed by in (z) into T1 and connecting it to accident (x) as a modifier (labr). [sent-275, score-0.495]
</p><p>64 We wish to cover AS-28 mini submarine in H from the coreferring it in T, mini submarine in T0 and AS-28 vehicle in T00. [sent-284, score-0.464]
</p><p>65 We found that 44% of the  pairs contained reference relations whose resolution was mandatory for inference. [sent-290, score-0.42]
</p><p>66 The most common bridging relation was the location of events (e. [sent-297, score-0.372]
</p><p>67 1215  (%) Focus  Reference term  Pronoun  NE  NP  VP  term9194923 -  43  43  14  Table 2: Syntactic types of discourse references (%)  Sub. [sent-303, score-0.465]
</p><p>68 Table 2 shows that 77% of all focus terms and 86% of the reference terms were nominal phrases, which justifies their prominent position in work on anaphora and coreference resolution. [sent-305, score-0.677]
</p><p>69 We found these focus terms to be frequently crucial for entailment since they included the main predicate of the hypothesis. [sent-307, score-0.526]
</p><p>70 Again, we found that the “default” transformation, substitution, is the most frequent one, and is helpful for  both coreference and bridging relations. [sent-311, score-0.576]
</p><p>71 Substitution is particularly useful for handling pronouns (14% of all substitution instances), the replacement of named entities by synonymous names (32%), the replacement of other NPs (38%), and the substitution of verbal head nodes in event coreference (16%). [sent-312, score-0.561]
</p><p>72 The distance between the focus and the reference terms varied considerably, ranging from intra-sentential reference relations and up to several dozen sentences. [sent-321, score-0.495]
</p><p>73 For more than a quarter of the focus terms, we  6The lower proportion of VPs among reference terms stems from bridging relations between VPs and nominal dependents, such as the abovementioned “location” relation. [sent-322, score-0.692]
</p><p>74 had to go to other documents to find reference terms that, possibly in conjunction with the focus term, could cover the target components. [sent-323, score-0.391]
</p><p>75 Interestingly, all such cases involved coreference (about equally divided between the merge transformations and substitutions), while bridging was always “document-local”. [sent-324, score-0.746]
</p><p>76 This result reaffirms the usefulness of cross-document coreference resolution for inference (Huang et al. [sent-325, score-0.485]
</p><p>77 In existing RTE systems, discourse references are typically resolved as a preprocessing step. [sent-328, score-0.394]
</p><p>78 While our annotation was manual and cannot yield direct results about processing considerations, we observed that discourse relations often hold between complex, and deeply embedded, expressions, which makes their automatic resolution difficult. [sent-329, score-0.556]
</p><p>79 Since the resolution of discourse references is likely to profit from these steps, it seems desirable to “postpone” it until after simplification. [sent-337, score-0.504]
</p><p>80 In transformation-based systems, it might be natural to add discourse-based transformations to the set of inference operations, while in alignment-based systems, discourse references can be integrated into the computation of alignment scores. [sent-338, score-0.56]
</p><p>81 We have stated before that even if a discourse reference is not strictly necessary for entailment, it may be interesting because it represents an alternative to the use of knowledge rules to cover the hypothesis. [sent-341, score-0.646]
</p><p>82 ” Here, the H modifier serial, which does not occur in T, can be covered either by world knowledge (a person who killed 10 people is a serial killer), or by resolving the coreference of BTK to the term the serial killer BTK which occurs in the discourse around T. [sent-357, score-1.073]
</p><p>83 Our conclusion is that not only can discourse references often replace world knowledge in principle, in practice it often seems easier to resolve discourse references than to determine whether a rule is applicable in a given context or to formalize complex world knowledge as inference rules. [sent-358, score-0.953]
</p><p>84 Our annotation provides further empirical support to this claim: An entailment relation exists between the focus and reference terms in 60% of the focus-reference term pairs, and in many of the remainder, entailment holds between the terms’ heads. [sent-359, score-1.31]
</p><p>85 Thus, discourse provides relations which are many times equivalent to entailment knowledge rules and can therefore be utilized in their stead. [sent-360, score-0.896]
</p><p>86 7  Conclusions  This work has presented an analysis of the relation between discourse references and textual entailment. [sent-361, score-0.553]
</p><p>87 We have identified a set of limitations common to the handling of discourse relations in virtually all entailment systems. [sent-362, score-0.852]
</p><p>88 Since in practical settings, discourse plays an important role, our goal was to develop an agenda for improving the handling of discourse references in entailment-based inference. [sent-364, score-0.726]
</p><p>89 Our manual analysis of the RTE-5 dataset shows that while the majority of discourse references that affect inference are nominal coreference relations, another substantial part is made up by verbal terms and bridging relations. [sent-365, score-1.114]
</p><p>90 Furthermore, we have demonstrated that substitution alone is insufficient to extract all relevant information from  the wide range of discourse references that are frequently relevant for inference. [sent-366, score-0.521]
</p><p>91 Furthermore, our evidence suggests that for practical reasons, the resolution of discourse references should be tightly integrated into entailment systems instead of treating it as a preprocessing step. [sent-368, score-0.972]
</p><p>92 A particularly interesting result concerns the interplay between discourse references and entailment knowledge. [sent-369, score-0.8]
</p><p>93 , from WordNet or Wikipedia) has been used beneficially for coreference resolution (Soon et al. [sent-372, score-0.413]
</p><p>94 , 2001 ; Ponzetto and Strube, 2006), reference resolution has, to our knowledge, not yet been employed to validate entailment rules’ applicability. [sent-373, score-0.755]
</p><p>95 Our analyses suggest that in the context of deciding textual entailment, reference resolution and entailment knowledge can be seen as complementary ways of achieving the same goal, namely enriching T with additional knowledge to allow the inference of H. [sent-374, score-1.037]
</p><p>96 Given that both of the technolo-  gies are still imperfect, we envisage the way forward as a joint strategy, where reference resolution and entailment rules mutually fill each other’s gaps (cf. [sent-375, score-0.79]
</p><p>97 In sum, our study shows that textual entailment can profit substantially from better discourse handling. [sent-377, score-0.922]
</p><p>98 Textual entailment through extended lexical overlap and lexico-semantic matching. [sent-387, score-0.437]
</p><p>99 Combining lexical, syntactic, and semantic evidence for textual entailment classification. [sent-394, score-0.559]
</p><p>100 A machine learning approach to coreference resolution of noun phrases. [sent-551, score-0.413]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('entailment', 0.437), ('discourse', 0.363), ('bridging', 0.304), ('coreference', 0.272), ('reference', 0.177), ('resolution', 0.141), ('transformations', 0.125), ('rte', 0.124), ('textual', 0.122), ('dagan', 0.108), ('term', 0.102), ('giampiccolo', 0.1), ('melting', 0.1), ('szr', 0.1), ('substitution', 0.094), ('mini', 0.093), ('transformation', 0.09), ('ido', 0.087), ('component', 0.086), ('arctic', 0.083), ('submarine', 0.083), ('copy', 0.076), ('sx', 0.075), ('inference', 0.072), ('bentivogli', 0.071), ('nominal', 0.07), ('relation', 0.068), ('killed', 0.067), ('target', 0.063), ('ice', 0.062), ('cover', 0.062), ('te', 0.06), ('btk', 0.058), ('danilo', 0.056), ('focus', 0.056), ('chambers', 0.054), ('br', 0.053), ('serial', 0.053), ('kennedy', 0.052), ('relations', 0.052), ('resolvers', 0.05), ('mandatory', 0.05), ('coreferring', 0.05), ('homosexual', 0.05), ('killer', 0.05), ('melt', 0.05), ('murdered', 0.05), ('oswald', 0.05), ('receding', 0.05), ('xena', 0.05), ('bernardo', 0.049), ('sy', 0.049), ('merge', 0.045), ('subtree', 0.044), ('coreferences', 0.044), ('zanzotto', 0.044), ('knowledge', 0.044), ('accident', 0.043), ('tree', 0.042), ('president', 0.042), ('mirkin', 0.04), ('shachar', 0.04), ('insertion', 0.038), ('entailed', 0.037), ('agichtein', 0.037), ('strassel', 0.037), ('pascal', 0.037), ('modifier', 0.036), ('lab', 0.036), ('anaphora', 0.036), ('missing', 0.036), ('luisa', 0.035), ('asher', 0.035), ('gaps', 0.035), ('modifiers', 0.035), ('edge', 0.034), ('head', 0.034), ('resolve', 0.034), ('hoa', 0.034), ('trang', 0.034), ('architectures', 0.034), ('event', 0.034), ('entities', 0.033), ('resolving', 0.033), ('abad', 0.033), ('balahur', 0.033), ('dali', 0.033), ('headmerge', 0.033), ('knife', 0.033), ('permafrost', 0.033), ('seabed', 0.033), ('subjmod', 0.033), ('terms', 0.033), ('applicable', 0.033), ('woman', 0.032), ('bill', 0.032), ('recognizing', 0.032), ('relevant', 0.032), ('preprocessing', 0.031), ('understanding', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="33-tfidf-1" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>2 0.35281911 <a title="33-tfidf-2" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>Author: Mark Sammons ; V.G.Vinod Vydiswaran ; Dan Roth</p><p>Abstract: We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment. We argue that the single global label with which RTE examples are annotated is insufficient to effectively evaluate RTE system performance; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will benefit not just RTE researchers, but the NLP community as a whole. We use insights from successful RTE systems to propose a model for identifying and annotating textual infer- ence phenomena in textual entailment examples, and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful.</p><p>3 0.29880977 <a title="33-tfidf-3" href="./acl-2010-Global_Learning_of_Focused_Entailment_Graphs.html">127 acl-2010-Global Learning of Focused Entailment Graphs</a></p>
<p>Author: Jonathan Berant ; Ido Dagan ; Jacob Goldberger</p><p>Abstract: We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms.</p><p>4 0.29440075 <a title="33-tfidf-4" href="./acl-2010-An_Open-Source_Package_for_Recognizing_Textual_Entailment.html">30 acl-2010-An Open-Source Package for Recognizing Textual Entailment</a></p>
<p>Author: Milen Kouylekov ; Matteo Negri</p><p>Abstract: This paper presents a general-purpose open source package for recognizing Textual Entailment. The system implements a collection of algorithms, providing a configurable framework to quickly set up a working environment to experiment with the RTE task. Fast prototyping of new solutions is also allowed by the possibility to extend its modular architecture. We present the tool as a useful resource to approach the Textual Entailment problem, as an instrument for didactic purposes, and as an opportunity to create a collaborative environment to promote research in the field.</p><p>5 0.28313285 <a title="33-tfidf-5" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>6 0.25341657 <a title="33-tfidf-6" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>7 0.23281087 <a title="33-tfidf-7" href="./acl-2010-Discourse_Structure%3A_Theory%2C_Practice_and_Use.html">86 acl-2010-Discourse Structure: Theory, Practice and Use</a></p>
<p>8 0.21873838 <a title="33-tfidf-8" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>9 0.21351071 <a title="33-tfidf-9" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>10 0.18860367 <a title="33-tfidf-10" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>11 0.1811499 <a title="33-tfidf-11" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>12 0.17265211 <a title="33-tfidf-12" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>13 0.1391909 <a title="33-tfidf-13" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>14 0.12829426 <a title="33-tfidf-14" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>15 0.12767388 <a title="33-tfidf-15" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>16 0.10081687 <a title="33-tfidf-16" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>17 0.094851211 <a title="33-tfidf-17" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>18 0.094655789 <a title="33-tfidf-18" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>19 0.088747621 <a title="33-tfidf-19" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>20 0.086043954 <a title="33-tfidf-20" href="./acl-2010-Edit_Tree_Distance_Alignments_for_Semantic_Role_Labelling.html">94 acl-2010-Edit Tree Distance Alignments for Semantic Role Labelling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.252), (1, 0.145), (2, 0.027), (3, -0.281), (4, -0.142), (5, 0.35), (6, 0.064), (7, 0.099), (8, -0.289), (9, -0.192), (10, -0.083), (11, 0.214), (12, -0.064), (13, 0.099), (14, -0.092), (15, -0.107), (16, -0.085), (17, -0.038), (18, 0.043), (19, -0.039), (20, 0.024), (21, -0.068), (22, -0.079), (23, 0.047), (24, -0.051), (25, 0.042), (26, 0.022), (27, -0.03), (28, -0.095), (29, 0.07), (30, 0.006), (31, 0.028), (32, -0.04), (33, 0.002), (34, 0.029), (35, 0.017), (36, 0.022), (37, -0.053), (38, -0.056), (39, 0.056), (40, -0.032), (41, -0.049), (42, -0.061), (43, 0.026), (44, 0.007), (45, -0.039), (46, -0.021), (47, -0.073), (48, -0.057), (49, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97202975 <a title="33-lsi-1" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>2 0.76735735 <a title="33-lsi-2" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>Author: Mark Sammons ; V.G.Vinod Vydiswaran ; Dan Roth</p><p>Abstract: We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment. We argue that the single global label with which RTE examples are annotated is insufficient to effectively evaluate RTE system performance; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will benefit not just RTE researchers, but the NLP community as a whole. We use insights from successful RTE systems to propose a model for identifying and annotating textual infer- ence phenomena in textual entailment examples, and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful.</p><p>3 0.71074909 <a title="33-lsi-3" href="./acl-2010-An_Open-Source_Package_for_Recognizing_Textual_Entailment.html">30 acl-2010-An Open-Source Package for Recognizing Textual Entailment</a></p>
<p>Author: Milen Kouylekov ; Matteo Negri</p><p>Abstract: This paper presents a general-purpose open source package for recognizing Textual Entailment. The system implements a collection of algorithms, providing a configurable framework to quickly set up a working environment to experiment with the RTE task. Fast prototyping of new solutions is also allowed by the possibility to extend its modular architecture. We present the tool as a useful resource to approach the Textual Entailment problem, as an instrument for didactic purposes, and as an opportunity to create a collaborative environment to promote research in the field.</p><p>4 0.65757108 <a title="33-lsi-4" href="./acl-2010-Global_Learning_of_Focused_Entailment_Graphs.html">127 acl-2010-Global Learning of Focused Entailment Graphs</a></p>
<p>Author: Jonathan Berant ; Ido Dagan ; Jacob Goldberger</p><p>Abstract: We propose a global algorithm for learning entailment relations between predicates. We define a graph structure over predicates that represents entailment relations as directed edges, and use a global transitivity constraint on the graph to learn the optimal set of edges, by formulating the optimization problem as an Integer Linear Program. We motivate this graph with an application that provides a hierarchical summary for a set of propositions that focus on a target concept, and show that our global algorithm improves performance by more than 10% over baseline algorithms.</p><p>5 0.6395244 <a title="33-lsi-5" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>Author: Roni Ben Aharon ; Idan Szpektor ; Ido Dagan</p><p>Abstract: Idan Szpektor Ido Dagan Yahoo! Research Department of Computer Science Haifa, Israel Bar-Ilan University idan @ yahoo- inc .com Ramat Gan, Israel dagan @ c s .biu . ac . i l FrameNet is a manually constructed database based on Frame Semantics. It models the semantic Many NLP tasks need accurate knowledge for semantic inference. To this end, mostly WordNet is utilized. Yet WordNet is limited, especially for inference be- tween predicates. To help filling this gap, we present an algorithm that generates inference rules between predicates from FrameNet. Our experiment shows that the novel resource is effective and complements WordNet in terms of rule coverage.</p><p>6 0.52483523 <a title="33-lsi-6" href="./acl-2010-Discourse_Structure%3A_Theory%2C_Practice_and_Use.html">86 acl-2010-Discourse Structure: Theory, Practice and Use</a></p>
<p>7 0.50359035 <a title="33-lsi-7" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>8 0.49204344 <a title="33-lsi-8" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>9 0.47023702 <a title="33-lsi-9" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>10 0.45948008 <a title="33-lsi-10" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>11 0.45447791 <a title="33-lsi-11" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>12 0.44066194 <a title="33-lsi-12" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>13 0.36151388 <a title="33-lsi-13" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>14 0.36072615 <a title="33-lsi-14" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>15 0.32570991 <a title="33-lsi-15" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>16 0.31655535 <a title="33-lsi-16" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>17 0.29796281 <a title="33-lsi-17" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>18 0.27781349 <a title="33-lsi-18" href="./acl-2010-Don%27t_%27Have_a_Clue%27%3F_Unsupervised_Co-Learning_of_Downward-Entailing_Operators..html">92 acl-2010-Don't 'Have a Clue'? Unsupervised Co-Learning of Downward-Entailing Operators.</a></p>
<p>19 0.27158996 <a title="33-lsi-19" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>20 0.27113363 <a title="33-lsi-20" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(2, 0.013), (7, 0.012), (14, 0.014), (25, 0.065), (39, 0.012), (42, 0.047), (44, 0.017), (59, 0.068), (72, 0.018), (73, 0.043), (76, 0.011), (78, 0.057), (80, 0.281), (83, 0.151), (84, 0.023), (98, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88056064 <a title="33-lda-1" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The automatic interpretation of noun-noun compounds is an important subproblem within many natural language processing applications and is an area of increasing interest. The problem is difficult, with disagreement regarding the number and nature of the relations, low inter-annotator agreement, and limited annotated data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation.</p><p>same-paper 2 0.8604517 <a title="33-lda-2" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>Author: Shachar Mirkin ; Ido Dagan ; Sebastian Pado</p><p>Abstract: Discourse references, notably coreference and bridging, play an important role in many text understanding applications, but their impact on textual entailment is yet to be systematically understood. On the basis of an in-depth analysis of entailment instances, we argue that discourse references have the potential of substantially improving textual entailment recognition, and identify a number of research directions towards this goal.</p><p>3 0.76556164 <a title="33-lda-3" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>Author: Rasoul Samad Zadeh Kaljahi</p><p>Abstract: Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1</p><p>4 0.69664896 <a title="33-lda-4" href="./acl-2010-Fixed_Length_Word_Suffix_for_Factored_Statistical_Machine_Translation.html">119 acl-2010-Fixed Length Word Suffix for Factored Statistical Machine Translation</a></p>
<p>Author: Narges Sharif Razavian ; Stephan Vogel</p><p>Abstract: Factored Statistical Machine Translation extends the Phrase Based SMT model by allowing each word to be a vector of factors. Experiments have shown effectiveness of many factors, including the Part of Speech tags in improving the grammaticality of the output. However, high quality part of speech taggers are not available in open domain for many languages. In this paper we used fixed length word suffix as a new factor in the Factored SMT, and were able to achieve significant improvements in three set of experiments: large NIST Arabic to English system, medium WMT Spanish to English system, and small TRANSTAC English to Iraqi system. 1</p><p>5 0.67154139 <a title="33-lda-5" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>Author: Mark Sammons ; V.G.Vinod Vydiswaran ; Dan Roth</p><p>Abstract: We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment. We argue that the single global label with which RTE examples are annotated is insufficient to effectively evaluate RTE system performance; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will benefit not just RTE researchers, but the NLP community as a whole. We use insights from successful RTE systems to propose a model for identifying and annotating textual infer- ence phenomena in textual entailment examples, and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful.</p><p>6 0.63672632 <a title="33-lda-6" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>7 0.61983538 <a title="33-lda-7" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>8 0.61565495 <a title="33-lda-8" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>9 0.60238266 <a title="33-lda-9" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>10 0.60057342 <a title="33-lda-10" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>11 0.59424335 <a title="33-lda-11" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>12 0.59213281 <a title="33-lda-12" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>13 0.58873868 <a title="33-lda-13" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>14 0.58676445 <a title="33-lda-14" href="./acl-2010-An_Open-Source_Package_for_Recognizing_Textual_Entailment.html">30 acl-2010-An Open-Source Package for Recognizing Textual Entailment</a></p>
<p>15 0.5854528 <a title="33-lda-15" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>16 0.58286393 <a title="33-lda-16" href="./acl-2010-Global_Learning_of_Focused_Entailment_Graphs.html">127 acl-2010-Global Learning of Focused Entailment Graphs</a></p>
<p>17 0.58062941 <a title="33-lda-17" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>18 0.57758355 <a title="33-lda-18" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>19 0.57743281 <a title="33-lda-19" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>20 0.5767625 <a title="33-lda-20" href="./acl-2010-Modeling_Norms_of_Turn-Taking_in_Multi-Party_Conversation.html">173 acl-2010-Modeling Norms of Turn-Taking in Multi-Party Conversation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
