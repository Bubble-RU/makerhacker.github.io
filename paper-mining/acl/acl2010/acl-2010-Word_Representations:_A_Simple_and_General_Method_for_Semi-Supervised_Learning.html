<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-263" href="#">acl2010-263</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</h1>
<br/><p>Source: <a title="acl-2010-263-pdf" href="http://aclweb.org/anthology//P/P10/P10-1040.pdf">pdf</a></p><p>Author: Joseph Turian ; Lev-Arie Ratinov ; Yoshua Bengio</p><p>Abstract: If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http ://metaoptimize com/proj ects/wordreprs/ .</p><p>Reference: <a title="acl-2010-263-reference" href="../acl2010_reference/acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca Abstract If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. [sent-3, score-0.39]
</p><p>2 We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. [sent-4, score-0.614]
</p><p>3 We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. [sent-5, score-0.312]
</p><p>4 However, the one-hot representation of a word suffers from data sparsity: Namely, for words that are rare in the labeled training data, their corresponding model parameters will be poorly estimated. [sent-24, score-0.182]
</p><p>5 These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. [sent-26, score-0.626]
</p><p>6 One common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. [sent-28, score-0.193]
</p><p>7 c As2s0o1c0ia Atisosnoc foiart Cionom fopru Ctaotmiopnuatla Lti on gaulis Lti cnsg,u piasgtiecs 384–394, word embeddings using unsupervised approaches. [sent-36, score-0.692]
</p><p>8 ) Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. [sent-38, score-0.265]
</p><p>9 But different word representations have never been systematically compared in a controlled way. [sent-39, score-0.237]
</p><p>10 2  Distributional representations  ×  Distributional word representations are based upon a cooccurrence matrix F of size W×C, where uWp oisn tahceo vocabulary size, xea Fch o row Fw ×isC t,h we ienrietial representation of word w, and each column Fc is some context. [sent-44, score-0.63]
</p><p>11 , 2003) induce distributional representations over F in which each column is a document context. [sent-62, score-0.295]
</p><p>12 They compute F over a corpus of 160 million word tokens with a vocabulary size W of 70K word types. [sent-67, score-0.175]
</p><p>13 There are 2·W types of context (columns): The first or second W are counted ifthe word c occurs within a window of 10 to the left or right of the word w, respectively. [sent-68, score-0.138]
</p><p>14 Rˇeh u˚ ˇrek and Sojka (2010) describe an incremental approach to inducing LSA and LDA topic models over 270 millions word tokens with a vocabulary of 3 15K word types. [sent-77, score-0.249]
</p><p>15 Kaski (1998) uses this technique to produce 100-dimensional representations of documents. [sent-81, score-0.219]
</p><p>16 It is not well-understood what settings are appropriate to induce distributional word representations for structured prediction tasks (like parsing and MT) and sequence labeling tasks (like chunking and NER). [sent-85, score-0.46]
</p><p>17 Previous research has achieved repeated successes on these tasks using clustering representations (Section 3) and distributed representations (Section 4), so we focus on these representations in our work. [sent-86, score-0.695]
</p><p>18 3 Clustering-based word representations Another type of word representation is to induce a clustering over words. [sent-87, score-0.471]
</p><p>19 1 Brown clustering The Brown algorithm is a hierarchical clustering  algorithm which clusters words to maximize the mutual information of bigrams (Brown et al. [sent-92, score-0.335]
</p><p>20 The hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. [sent-96, score-0.295]
</p><p>21 One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context. [sent-97, score-0.14]
</p><p>22 (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. [sent-104, score-0.156]
</p><p>23 2 Other work on cluster-based word representations Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. [sent-107, score-0.327]
</p><p>24 HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). [sent-108, score-0.176]
</p><p>25 ) However, the CRF chunker in Huang and Yates (2009), which uses their HMM word clusters as extra features, achieves F1 lower than a baseline CRF chunker (Sha & Pereira, 2003). [sent-113, score-0.231]
</p><p>26 4 Distributed representations Another approach to word representation is to learn a distributed representation. [sent-117, score-0.322]
</p><p>27 Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. [sent-121, score-0.149]
</p><p>28 A distributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions. [sent-122, score-0.208]
</p><p>29 Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). [sent-123, score-0.821]
</p><p>30 Historically, training and testing of neural language models has been slow, scaling  as the size of the vocabulary for each model computation (Bengio et al. [sent-124, score-0.261]
</p><p>31 However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin & Bengio, 2005; Collobert & Weston, 2008; Mnih & Hinton, 2009) and allow scaling to very large training corpora. [sent-127, score-0.185]
</p><p>32 The model concatenates the learned embeddings of the n words, giving e(w1) ⊕ . [sent-137, score-0.643]
</p><p>33 in Spimecizifei cthalilsy l:o Lss( stochastically over t+hes n-grams in the corpus, doing gradient descent simultaneously over the neural network parameters and the embedding lookup table. [sent-155, score-0.2]
</p><p>34 e embeddings da and s efpora atthee neaerunrianlg n reattweo frokr weights. [sent-160, score-0.614]
</p><p>35 We found that the embeddings should have a learning rate generally 1000–32000 times higher than the neural network weights. [sent-161, score-0.69]
</p><p>36 • Although their sampling technique makes training fast, testing ris s asmtilpl expensive qwuheen m tahkee ssi tzrae nofthe vocabulary is large. [sent-163, score-0.133]
</p><p>37 2  HLBL embeddings  The log-bilinear model (Mnih & Hinton, 2007) is a probabilistic and linear neural model. [sent-166, score-0.69]
</p><p>38 Given an n-gram, the model concatenates the embeddings of the n − 1 first words, and learns a linear model to predict 1th fei embedding odf l etharen sla ast l inweoardr. [sent-167, score-0.767]
</p><p>39 m oTdheel similarity between the predicted embedding and the current actual embedding is transformed into a probability by exponentiating and then  normalizing. [sent-168, score-0.248]
</p><p>40 5 Supervised evaluation tasks We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features. [sent-175, score-0.362]
</p><p>41 However, we wish to find out if certain word representations are preferable for certain tasks. [sent-177, score-0.265]
</p><p>42 Lin and Wu (2009) finds that the representations that are good for NER are poor for search query  classification, and vice-versa. [sent-178, score-0.187]
</p><p>43 We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008). [sent-179, score-0.321]
</p><p>44 As you can see, the Brown and embedding features are unigram features, and do not participate in conjunctions like the word features and tag features do. [sent-195, score-0.276]
</p><p>45 (2008) sees further accuracy improvements on dependency parsing when using word representations in compound features. [sent-197, score-0.265]
</p><p>46 The word embeddings also required a scaling hyperparameter, as described in Section 7. [sent-211, score-0.748]
</p><p>47 That is, if we have induced an embedding for 12/3/2008 , we will use the embedding of 12/3/2008 , and *DD*/*D*/*DDDD* in the baseline features listed above. [sent-231, score-0.337]
</p><p>48 (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters. [sent-258, score-1.003]
</p><p>49 (2009), the non-convex optimization process for Collobert and Weston (2008) embeddings might be adversely affected by noise and the statistical sparsity issues regarding rare words, especially at the beginning of training. [sent-261, score-0.679]
</p><p>50 For this reason, we hypothesize that learning representations over the most frequent words first and gradually increasing the vocabulary—a curriculum training strategy (Elman, 1993; Bengio et al. [sent-262, score-0.238]
</p><p>51 For this reason, NER results that use RCV1 word representations are a form of transductive learning. [sent-273, score-0.237]
</p><p>52 1 Details of inducing word representations The Brown clusters took roughly 3 days to induce, when we induced 1000 clusters, the baseline in prior work (Koo et al. [sent-275, score-0.489]
</p><p>53 (Because Brown clustering scales quadratically in the number of clusters, inducing 10000 clusters would have been prohibitive. [sent-278, score-0.287]
</p><p>54 The Collobert and Weston (2008) (C&W;) embeddings were induced over the course of a few weeks, and trained for about 50 epochs. [sent-282, score-0.669]
</p><p>55 One of the difficulties in inducing these embeddings is that there is no stopping criterion defined, and that the quality of the embeddings can keep improving as training continues. [sent-283, score-1.328]
</p><p>56 We induced embeddings with 25, 50, 100, or 200 dimensions over 5-gram windows. [sent-287, score-0.718]
</p><p>57 (2009), we use improved C&W; embeddings in this work: • They were trained for 50 epochs, not just 20 epochs. [sent-289, score-0.614]
</p><p>58 • We initialized all embedding dimensions uniformly niint tahliez range [-0. [sent-290, score-0.173]
</p><p>59 For rare words, which are typically updated only 143 times per epoch2, and given that our embedding learning rate was typically 1e-6 or 1e-7, this means that rare word embeddings will be concen-  trated around zero, instead of spread out randomly. [sent-293, score-0.918]
</p><p>60 The HLBL embeddings were trained for 100 epochs (7 Unlike our Collobert and Weston (2008) embeddings, we did not extensively tune the learning rates for HLBL. [sent-294, score-0.664]
</p><p>61 We induced embeddings with 100 dimensions over 5-gram windows, and embeddings with 50 dimensions over 5-gram windows. [sent-296, score-1.381]
</p><p>62 3  2A rare word will appear 5 (window size) times per epoch as a positive example, and 37M (training examples per epoch) / 269K (vocabulary size) = 138 times per epoch as a corruption example. [sent-298, score-0.241]
</p><p>63 389  approach using a random tree, not two passes with an updated tree and embeddings re-estimation. [sent-301, score-0.614]
</p><p>64 If the range of the word embeddings is too large, they will exert more influence than the binary features. [sent-305, score-0.664]
</p><p>65 We can scale the embeddings by a hyperparameter, to control their standard deviation. [sent-307, score-0.614]
</p><p>66 Assume that the embeddings are represented by a matrix E: E  ← σ ·  E/stddev(E)  (1)  is a scaling constant that sets the new standard deviation after scaling the embeddings. [sent-308, score-0.822]
</p><p>67 We experiment with Collobert and Weston (2008) and HLBL embeddings of various dimensionality. [sent-310, score-0.614]
</p><p>68 We were surprised to find that on both tasks, across Collobert and Weston (2008) and HLBL embeddings of various dimensionality, that all curves had similar shapes and optima. [sent-314, score-0.614]
</p><p>69 However, these curves demonstrate that a reasonable choice of scale factor is such that the embeddings have a standard deviation of 0. [sent-318, score-0.614]
</p><p>70 3 Capacity of Word Representations # of embedding 25  dimensions  50  100  200  # of Brown clusters # of embedding 25  dimensions  50  100  200  # of Brown clusters  Figure 2: Effect as we vary the capacity of the word representations on the validation set F1. [sent-321, score-0.896]
</p><p>71 There are capacity controls for the word representations: number of Brown clusters, and number of dimensions of the word embeddings. [sent-324, score-0.19]
</p><p>72 (2009), we hypothesized on the basis of solely the HLBL NER curve that  higher-dimensional word embeddings would give higher accuracy. [sent-329, score-0.664]
</p><p>73 For NER, the C&W; curve is almost flat, and we were suprised to find the even 25-dimensional C&W; word embeddings work so well. [sent-331, score-0.664]
</p><p>74 For chunking, 50-dimensional embeddings had the highest validation F1 for both C&W; and HLBL. [sent-332, score-0.64]
</p><p>75 These curves indicates that the optimal capacity of the word embeddings is task-specific. [sent-333, score-0.705]
</p><p>76 But, if only one word representation is to be used, Brown clusters have the highest accuracy. [sent-343, score-0.214]
</p><p>77 Given the improvements to the C&W; embeddings since Turian et al. [sent-344, score-0.614]
</p><p>78 Com-  Frequency of word in unlabeled data  Frequency of word in unlabeled data  Figure 3 :  For word tokens that have different frequency  in the unlabeled data, what is the total number of per-token errors incurred on the test set? [sent-347, score-0.3]
</p><p>79 bining representations leads to small increases in the test F1. [sent-350, score-0.187]
</p><p>80 In comparison to chunking, combining different word representations on NER seems gives larger improvements on the test F1. [sent-351, score-0.237]
</p><p>81 On NER, Brown clusters are superior to the word embeddings. [sent-352, score-0.173]
</p><p>82 Since much of the NER F1 is derived from decisions made over rare words, we suspected that Brown clustering has a superior representation for rare words. [sent-353, score-0.261]
</p><p>83 Brown makes a single hard clustering decision, whereas the embedding for a rare word is close to its initial value since it hasn’t received many training updates (see Footnote 2). [sent-354, score-0.355]
</p><p>84 For NER, Figure 3 (b) shows that most errors occur on rare words, and that Brown clusters do indeed incur fewer errors for rare words. [sent-356, score-0.253]
</p><p>85 This supports our hypothesis that, for rare words, Brown clustering produces  better representations than word embeddings that haven’t received sufficient training updates. [sent-357, score-1.032]
</p><p>86 For chunking, Brown clusters and C&W; embeddings incur almost identical numbers of errors, and errors are concentrated around the more common 391  words. [sent-358, score-0.737]
</p><p>87 For tasks like chunking in which a syntactic decision relies upon looking at several token simultaneously, compound features that use the word representations might increase accuracy more (Koo et al. [sent-360, score-0.414]
</p><p>88 Using word representations in NER brought larger gains on the out-of-domain data than on the in-domain data. [sent-362, score-0.237]
</p><p>89 This suggests that extending word representa-  tions to phrase representations investigation. [sent-375, score-0.237]
</p><p>90 Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. [sent-380, score-0.265]
</p><p>91 Ours is the first work to systematically compare different word representations in a controlled way. [sent-381, score-0.237]
</p><p>92 We found that Brown clusters and word embeddings both can improve the accuracy of a near-state-of-the-art supervised NLP system. [sent-382, score-0.862]
</p><p>93 We also found that com-  bining different word representations can improve accuracy further. [sent-383, score-0.265]
</p><p>94 Error analysis indicates that Brown clustering induces better representations for rare words than C&W; embeddings that have not received many training updates. [sent-384, score-0.982]
</p><p>95 Another contribution of our work is a default method for setting the scaling parameter for word embeddings. [sent-385, score-0.134]
</p><p>96 With this contribution, word embeddings can now be used off-the-shelf as word features, with no tuning. [sent-386, score-0.714]
</p><p>97 Future work should explore methods for inducing phrase representations, as well as techniques for increasing in accuracy by using word representations in compound features. [sent-387, score-0.339]
</p><p>98 Thank you to Andriy Mnih for inducing his embeddings on RCV1 for us. [sent-391, score-0.688]
</p><p>99 Distributional representations for handling sparsity in supervised sequence labeling. [sent-510, score-0.234]
</p><p>100 A preliminary evaluation of word representations for named-entity recognition. [sent-671, score-0.237]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('embeddings', 0.614), ('bengio', 0.224), ('collobert', 0.218), ('weston', 0.194), ('representations', 0.187), ('ner', 0.172), ('ratinov', 0.157), ('brown', 0.145), ('hlbl', 0.143), ('honkela', 0.129), ('turian', 0.129), ('embedding', 0.124), ('clusters', 0.123), ('chunking', 0.115), ('mnih', 0.114), ('suzuki', 0.108), ('sahlgren', 0.103), ('clustering', 0.09), ('isozaki', 0.086), ('scaling', 0.084), ('hinton', 0.08), ('neural', 0.076), ('vocabulary', 0.075), ('inducing', 0.074), ('ando', 0.071), ('ayrynen', 0.069), ('koo', 0.065), ('rare', 0.065), ('epoch', 0.063), ('roth', 0.06), ('distributional', 0.055), ('induced', 0.055), ('induce', 0.053), ('yates', 0.051), ('word', 0.05), ('epochs', 0.05), ('unlabeled', 0.05), ('dimensions', 0.049), ('supervised', 0.047), ('eal', 0.046), ('montr', 0.046), ('distributed', 0.044), ('bilou', 0.043), ('dddd', 0.043), ('ica', 0.043), ('kohonen', 0.043), ('morin', 0.043), ('crf', 0.041), ('representation', 0.041), ('capacity', 0.041), ('lund', 0.041), ('matrix', 0.04), ('window', 0.038), ('sha', 0.037), ('zhang', 0.036), ('pereira', 0.035), ('cleaning', 0.034), ('features', 0.034), ('fw', 0.034), ('hierarchical', 0.032), ('technique', 0.032), ('xi', 0.032), ('nlp', 0.031), ('burgess', 0.031), ('chunker', 0.029), ('liang', 0.029), ('concatenates', 0.029), ('corrupted', 0.029), ('crabb', 0.029), ('ddd', 0.029), ('diro', 0.029), ('ducharme', 0.029), ('ebec', 0.029), ('ecal', 0.029), ('epartement', 0.029), ('erationnelle', 0.029), ('gauvain', 0.029), ('highdimensional', 0.029), ('informatique', 0.029), ('kaski', 0.029), ('krishnan', 0.029), ('rek', 0.029), ('schwenk', 0.029), ('ushioda', 0.029), ('ti', 0.028), ('accuracy', 0.028), ('preferable', 0.028), ('unsupervised', 0.028), ('training', 0.026), ('validation', 0.026), ('semantic', 0.025), ('iro', 0.025), ('curriculum', 0.025), ('sojka', 0.025), ('ood', 0.025), ('recherche', 0.025), ('candito', 0.025), ('crfsuite', 0.025), ('stockholm', 0.025), ('yoshua', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="263-tfidf-1" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>Author: Joseph Turian ; Lev-Arie Ratinov ; Yoshua Bengio</p><p>Abstract: If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http ://metaoptimize com/proj ects/wordreprs/ .</p><p>2 0.14185369 <a title="263-tfidf-2" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>3 0.11220808 <a title="263-tfidf-3" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>4 0.098082215 <a title="263-tfidf-4" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>Author: Yassine Benajiba ; Imed Zitouni ; Mona Diab ; Paolo Rosso</p><p>Abstract: Building an accurate Named Entity Recognition (NER) system for languages with complex morphology is a challenging task. In this paper, we present research that explores the feature space using both gold and bootstrapped noisy features to build an improved highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute).</p><p>5 0.094575815 <a title="263-tfidf-5" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>Author: Jenny Rose Finkel ; Christopher D. Manning</p><p>Abstract: One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. Experiments on joint parsing and named entity recog- nition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.</p><p>6 0.093660533 <a title="263-tfidf-6" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>7 0.079426825 <a title="263-tfidf-7" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>8 0.072026581 <a title="263-tfidf-8" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>9 0.065184005 <a title="263-tfidf-9" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>10 0.062241849 <a title="263-tfidf-10" href="./acl-2010-Syntactic_and_Semantic_Factors_in_Processing_Difficulty%3A_An_Integrated_Measure.html">220 acl-2010-Syntactic and Semantic Factors in Processing Difficulty: An Integrated Measure</a></p>
<p>11 0.060696099 <a title="263-tfidf-11" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>12 0.059591185 <a title="263-tfidf-12" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>13 0.057456173 <a title="263-tfidf-13" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>14 0.057200588 <a title="263-tfidf-14" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>15 0.055743612 <a title="263-tfidf-15" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>16 0.055738874 <a title="263-tfidf-16" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>17 0.054865897 <a title="263-tfidf-17" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>18 0.054708317 <a title="263-tfidf-18" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>19 0.053202614 <a title="263-tfidf-19" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>20 0.049887635 <a title="263-tfidf-20" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.165), (1, 0.035), (2, 0.036), (3, 0.026), (4, 0.039), (5, -0.017), (6, 0.003), (7, -0.03), (8, 0.084), (9, 0.062), (10, -0.083), (11, 0.033), (12, 0.069), (13, -0.067), (14, -0.069), (15, -0.027), (16, -0.062), (17, 0.002), (18, 0.02), (19, -0.041), (20, 0.078), (21, 0.045), (22, -0.033), (23, 0.053), (24, -0.055), (25, 0.054), (26, -0.035), (27, -0.0), (28, 0.013), (29, -0.059), (30, -0.027), (31, -0.053), (32, -0.011), (33, 0.077), (34, -0.057), (35, 0.093), (36, -0.058), (37, 0.0), (38, 0.008), (39, 0.018), (40, -0.066), (41, 0.09), (42, -0.046), (43, -0.051), (44, 0.043), (45, 0.053), (46, 0.012), (47, -0.027), (48, -0.083), (49, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9062022 <a title="263-lsi-1" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>Author: Joseph Turian ; Lev-Arie Ratinov ; Yoshua Bengio</p><p>Abstract: If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http ://metaoptimize com/proj ects/wordreprs/ .</p><p>2 0.67001504 <a title="263-lsi-2" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>3 0.66325146 <a title="263-lsi-3" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>Author: Sebastian Rudolph ; Eugenie Giesbrecht</p><p>Abstract: We propose CMSMs, a novel type of generic compositional models for syntactic and semantic aspects of natural language, based on matrix multiplication. We argue for the structural and cognitive plausibility of this model and show that it is able to cover and combine various common compositional NLP approaches ranging from statistical word space models to symbolic grammar formalisms.</p><p>4 0.64329869 <a title="263-lsi-4" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>Author: David Jurgens ; Keith Stevens</p><p>Abstract: We present the S-Space Package, an open source framework for developing and evaluating word space algorithms. The package implements well-known word space algorithms, such as LSA, and provides a comprehensive set of matrix utilities and data structures for extending new or existing models. The package also includes word space benchmarks for evaluation. Both algorithms and libraries are designed for high concurrency and scalability. We demonstrate the efficiency of the reference implementations and also provide their results on six benchmarks.</p><p>5 0.62302017 <a title="263-lsi-5" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>Author: Jenny Rose Finkel ; Christopher D. Manning</p><p>Abstract: One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. Experiments on joint parsing and named entity recog- nition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.</p><p>6 0.59409058 <a title="263-lsi-6" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>7 0.57003999 <a title="263-lsi-7" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>8 0.56129599 <a title="263-lsi-8" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>9 0.55449134 <a title="263-lsi-9" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>10 0.55300921 <a title="263-lsi-10" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>11 0.53213578 <a title="263-lsi-11" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>12 0.51481342 <a title="263-lsi-12" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>13 0.51187879 <a title="263-lsi-13" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>14 0.50874674 <a title="263-lsi-14" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>15 0.49877021 <a title="263-lsi-15" href="./acl-2010-Online_Generation_of_Locality_Sensitive_Hash_Signatures.html">183 acl-2010-Online Generation of Locality Sensitive Hash Signatures</a></p>
<p>16 0.48217261 <a title="263-lsi-16" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>17 0.48189634 <a title="263-lsi-17" href="./acl-2010-Authorship_Attribution_Using_Probabilistic_Context-Free_Grammars.html">34 acl-2010-Authorship Attribution Using Probabilistic Context-Free Grammars</a></p>
<p>18 0.46088862 <a title="263-lsi-18" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>19 0.45668116 <a title="263-lsi-19" href="./acl-2010-Domain_Adaptation_of_Maximum_Entropy_Language_Models.html">91 acl-2010-Domain Adaptation of Maximum Entropy Language Models</a></p>
<p>20 0.45477492 <a title="263-lsi-20" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.019), (25, 0.044), (35, 0.285), (39, 0.02), (42, 0.02), (44, 0.012), (59, 0.132), (71, 0.015), (73, 0.048), (76, 0.014), (78, 0.029), (80, 0.016), (83, 0.091), (84, 0.045), (98, 0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88056946 <a title="263-lda-1" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>Author: Costanza Navarretta ; Patrizia Paggio</p><p>Abstract: This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback.</p><p>2 0.82452536 <a title="263-lda-2" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>Author: Vipul Mittal</p><p>Abstract: In this paper, we propose a novel method for automatic segmentation of a Sanskrit string into different words. The input for our segmentizer is a Sanskrit string either encoded as a Unicode string or as a Roman transliterated string and the output is a set of possible splits with weights associated with each of them. We followed two different approaches to segment a Sanskrit text using sandhi1 rules extracted from a parallel corpus of manually sandhi split text. While the first approach augments the finite state transducer used to analyze Sanskrit morphology and traverse it to segment a word, the second approach generates all possible segmentations and validates each constituent using a morph an- alyzer.</p><p>same-paper 3 0.75888151 <a title="263-lda-3" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>Author: Joseph Turian ; Lev-Arie Ratinov ; Yoshua Bengio</p><p>Abstract: If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http ://metaoptimize com/proj ects/wordreprs/ .</p><p>4 0.75176567 <a title="263-lda-4" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>Author: Wolfgang Seeker ; Ines Rehbein ; Jonas Kuhn ; Josef Van Genabith</p><p>Abstract: For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebankbased (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation.</p><p>5 0.58494508 <a title="263-lda-5" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>6 0.58298141 <a title="263-lda-6" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>7 0.58026439 <a title="263-lda-7" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>8 0.57951432 <a title="263-lda-8" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>9 0.57845491 <a title="263-lda-9" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>10 0.57720959 <a title="263-lda-10" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>11 0.57693094 <a title="263-lda-11" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>12 0.57662654 <a title="263-lda-12" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>13 0.57330865 <a title="263-lda-13" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>14 0.57308829 <a title="263-lda-14" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>15 0.57305336 <a title="263-lda-15" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>16 0.57286805 <a title="263-lda-16" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>17 0.57268059 <a title="263-lda-17" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>18 0.57228923 <a title="263-lda-18" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>19 0.57227278 <a title="263-lda-19" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>20 0.57212377 <a title="263-lda-20" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
