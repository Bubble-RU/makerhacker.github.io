<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 acl-2010-Collocation Extraction beyond the Independence Assumption</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-60" href="#">acl2010-60</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 acl-2010-Collocation Extraction beyond the Independence Assumption</h1>
<br/><p>Source: <a title="acl-2010-60-pdf" href="http://aclweb.org/anthology//P/P10/P10-2020.pdf">pdf</a></p><p>Author: Gerlof Bouma</p><p>Abstract: In this paper we start to explore two-part collocation extraction association measures that do not estimate expected probabilities on the basis of the independence assumption. We propose two new measures based upon the well-known measures of mutual information and pointwise mutual information. Expected probabilities are derived from automatically trained Aggregate Markov Models. On three collocation gold standards, we find the new association measures vary in their effectiveness.</p><p>Reference: <a title="acl-2010-60-reference" href="../acl2010_reference/acl-2010-Collocation_Extraction_beyond_the_Independence_Assumption_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract In this paper we start to explore two-part collocation extraction association measures that do not estimate expected probabilities on the basis of the independence assumption. [sent-3, score-0.733]
</p><p>2 We propose two new measures based upon the well-known measures of mutual information and pointwise mutual information. [sent-4, score-0.282]
</p><p>3 On three collocation gold standards, we find the new association measures vary in their effectiveness. [sent-6, score-0.506]
</p><p>4 1 Introduction Collocation extraction typically proceeds by scoring collocation candidates with an association mea-  sure, where high scores are taken to indicate likely collocationhood. [sent-7, score-0.429]
</p><p>5 Two well-known such measures are pointwise mutual information (PMI) and mutual information (MI). [sent-8, score-0.194]
</p><p>6 (1)  (2)  x∈{wX1 ,¬w1 } yx∈∈{{ww2,,¬¬ww2}} PMI (1) is the logged ratio of the observed bigramme probability and the expected bigramme probability under independence of the two words in the combination. [sent-10, score-0.89]
</p><p>7 MI (2) is the expected outcome of PMI, and measures how much information of the distribution of one word is contained in the distribution of the other. [sent-11, score-0.122]
</p><p>8 PMI was introduced into the collocation extraction field by Church and Hanks (1990). [sent-12, score-0.391]
</p><p>9 First, the observed occurrence probability pobs is compared to the expected occurrence probability pexp. [sent-15, score-0.15]
</p><p>10 Secondly, the independence assumption underlies the estimation of pexp. [sent-16, score-0.207]
</p><p>11 For instance, the bigramme of the is uninteresting from a collocation extraction perspective, although it probably is amongst the most frequent bigrammes for any English corpus. [sent-18, score-0.725]
</p><p>12 Looking at pobs andpexp together allows us to recognize these cases (Manning and Schu¨tze (1999) and Evert (2007) for more discussion). [sent-20, score-0.072]
</p><p>13 The second aspect, the independence assumption in the estimation of pexp, is more problematic, however, even in the context of collocation extraction. [sent-21, score-0.546]
</p><p>14 As Evert (2007, p42) notes, the assumption of “independence is extremely unrealistic,” because it ignores “a variety of syntactic, semantic  and lexical restrictions. [sent-22, score-0.056]
</p><p>15 The independence assumption leads to overestimated expectation and the the will need to be very frequent for it to show up as a likely collocation. [sent-26, score-0.207]
</p><p>16 A less contrived example of how the independence assumption might mislead collocation extraction is when bigramme distribution is influenced by compositional, non-collocational, semantic dependencies. [sent-27, score-0.939]
</p><p>17 Investigating adjective-noun combinations in a corpus, we might find that beige cloth gets a high PMI, whereas beige thought does not. [sent-28, score-0.154]
</p><p>18 This does not make the former a collocation or multiword unit. [sent-29, score-0.438]
</p><p>19 Rather, what we would measure is the tendency to use colours with visible things and not with abstract objects. [sent-30, score-0.039]
</p><p>20 c C2o0n1f0er Aenscseoc Sihatoirotn P faopre Crso,m papguetsat 1io0n9a–l1 L1i4n,guistics  associations between words are real dependencies, but they need not be collocational in nature. [sent-33, score-0.078]
</p><p>21 Because of the independence assumption, PMI and  MI measure these syntactic and semantic associations just as much as they measure collocational association. [sent-34, score-0.229]
</p><p>22 In this paper, we therefore experimentally investigate the use of a more informed pexp in the context of collocation extraction. [sent-35, score-0.636]
</p><p>23 2  Aggregate Markov Models  To replace pexp under independence, one might consider models with explicit linguistic information, such as a POS-tag bigramme model. [sent-36, score-0.606]
</p><p>24 We might not know exactly what factors are needed to estimate pexp and even if we do, we might lack the resources to train the resulting models. [sent-39, score-0.352]
</p><p>25 The only thing we know about estimating pexp is that we need more information than a unigramme model but less than a bigramme model (as this would make pobs/pexp uninformative). [sent-40, score-0.626]
</p><p>26 In an AMM, bigramme probability is not directly modeled, but mediated by a hidden class variable c: pamm(w2|w1)  = Xp(c|w1)p(w2|c). [sent-44, score-0.427]
</p><p>27 (3)  Xc  The number of classes in an AMM determines the amount of dependency that can be captured. [sent-45, score-0.046]
</p><p>28 In the case of just one class, AMM is equivalent to a unigramme model. [sent-46, score-0.078]
</p><p>29 AMMs become equivalent to the full bigramme model when the number of classes equals the size of the smallest of the vocabularies of the parts of the combination. [sent-47, score-0.389]
</p><p>30 AMMs can be trained with EM, using no more information than one would need for ML bigramme probability estimates. [sent-49, score-0.335]
</p><p>31 Their use in collocation extraction is to our knowledge novel. [sent-54, score-0.391]
</p><p>32 according to:  p(c|w1) ←PPww,c0nn((ww11,,ww))pp((cc|w0|w1,1w,w) , p(w2|c) ←PPPww,wn0(nw(w,w,w2)0p)(pc(|cw|w,w,w2)0),  (4) (5)  where n(w1 , w2)P are bigramme counts and the posterior probability of a hidden category c is estimated by:  p(c|w1,w2) =Pcp0(pc(|wc01|w)p1()wp(2w|c2)|c0). [sent-55, score-0.401]
</p><p>33 The definition of the counterparts to (P)MI without the independence assumption, the AMM-ratio and AMM-divergence, is now straightforward:  ramm(w1,w2) = logp(w1)pp(wam1,mw(2w)2|w1),  (7)  damm(w1,w2)  = X  p(x,y)ramm(x,y). [sent-57, score-0.151]
</p><p>34 (8)  x∈{wX1 ,¬w1 } yx∈∈{{ww2,,¬¬ww2}} The free parameter in these association measures is the number of hidden classes in the AMM, that is, the amount of dependency between the bigramme parts used to estimate pexp. [sent-58, score-0.555]
</p><p>35 Note that AMM-ratio and AMM-divergence with one hidden class are equivalent to PMI and MI, respectively. [sent-59, score-0.122]
</p><p>36 1 Data and procedure We apply AMM-ratio and AMM-divergence to three collocation gold standards. [sent-62, score-0.38]
</p><p>37 The effectiveness of association measures in collocation extraction is measured by ranking collocation candidates after the scores defined by the measures, and calculating average precision of these lists against the gold standard annotation. [sent-63, score-0.923]
</p><p>38 We consider the newly pro-  posed AMM-based measures for a varying number of hidden categories. [sent-64, score-0.127]
</p><p>39 The new measures are compared against two baselines: ranking by frequency (pobs) and random ordering. [sent-65, score-0.137]
</p><p>40 Because AMM-ratio and -divergence with one hidden class boil down to PMI and MI (and thus log-likelihood ratio), the evaluation contains an implicit comparison with 110  these canonical measures, too. [sent-66, score-0.092]
</p><p>41 However, the results will not be state-of-the-art: for the datasets investigated below, there are more effective extraction methods based on supervised machine learning (Pecina, 2008). [sent-67, score-0.052]
</p><p>42 The first gold standard used is the German adjective-noun dataset (Evert, 2008). [sent-68, score-0.041]
</p><p>43 We used the bigramme frequency data included in the resource. [sent-71, score-0.362]
</p><p>44 The second gold standard consists of 5 102 German PP-verb combinations, also sampled from newspaper texts (Krenn, 2008). [sent-73, score-0.064]
</p><p>45 The data contains annotation for support verb constructions (FVGs) and figurative expressions. [sent-74, score-0.066]
</p><p>46 This resource also comes with its own frequency data. [sent-75, score-0.049]
</p><p>47 After frequency thresholding, AMMs are trained on 46k PPs, 7. [sent-76, score-0.049]
</p><p>48 Third and last is the English verb-particle construction (VPC) gold standard (Baldwin, 2008), consisting of 3078 verb-particle pairs and annotation for transitive and intransitive idiomatic VPCs. [sent-78, score-0.138]
</p><p>49 We extract frequency data from the BNC, following the methods described in Baldwin (2005). [sent-79, score-0.049]
</p><p>50 For the transitive VPCs, we have 5k Vs, 35 particles and 54k pair types. [sent-83, score-0.106]
</p><p>51 All our EM runs start with randomly initialized model vectors. [sent-84, score-0.035]
</p><p>52 3 we discuss the impact of model variation due to this random factor. [sent-86, score-0.036]
</p><p>53 2  Results  German A-N collocations The top slice in Table 1 shows results for the three subtasks of the A-N dataset. [sent-88, score-0.193]
</p><p>54 We see that using AMM-based pexp initially improves average precision, for each task and for both the ratio and the divergence measure. [sent-89, score-0.3]
</p><p>55 At their maxima, the informed measures outperform both baselines as well as PMI and MI/loglikelihood ratio (# classes=1). [sent-90, score-0.18]
</p><p>56 It is likely that the drop in performance for the larger AMM-based measures is due to the AMMs learning the collocations themselves. [sent-92, score-0.194]
</p><p>57 That is, the AMMs become rich enough to not only capture the broadly applicative distributional influences of syntax and semantics, but also provide accurate pexps for individual, distributionally deviant combinations like collocations. [sent-93, score-0.102]
</p><p>58 An accurate pexp results in a low association score. [sent-94, score-0.303]
</p><p>59 (2005), we take the 200 most frequent adjectives and assign them to –  the category that maximizes p(c|w1) ; likewise for nouns and p(w2 |c). [sent-97, score-0.073]
</p><p>60 Four selepc(tce|dw clusters (out of 16) are given in |Tca)ble 2. [sent-98, score-0.04]
</p><p>61 2 The esoteric class 1contains ordinal numbers and nouns that one typically uses those with, including references to temporal concepts. [sent-99, score-0.075]
</p><p>62 Class 4 shows a group of adjectives denoting colours and/or political affiliations and a less coherent set of nouns, although the noun cluster can be understood if we consider individual adjectives that are associated with this class. [sent-101, score-0.14]
</p><p>63 Our informal impression from looking at clusters is that this is a common situation: as a whole, a cluster cannot be easily characterized, although for subsets or individual pairs, one can get an intuition for why they are in the same class. [sent-102, score-0.07]
</p><p>64 Unfortunately, we also see that some actual collocations are clustered in class 4, such as gelbe Karte ‘warning’ (lit. [sent-103, score-0.159]
</p><p>65 German PP-Verb collocations  The second slice  in Table 1 shows that, for both subtypes of PP-V collocation, better pexp-estimates lead to decreased average precision. [sent-106, score-0.164]
</p><p>66 The most effective AMM-ratio and -distance measures are those equivalent to (P)MI. [sent-107, score-0.118]
</p><p>67 Apparently, the better pexps are unfortunate for the extraction of the type of collocations in this dataset. [sent-108, score-0.23]
</p><p>68 The poor performance of PMI on these data clearly below frequency has been noticed before by Krenn and Evert (2001). [sent-109, score-0.049]
</p><p>69 A possible explanation for the lack of improvement in the AMMs lies in the relatively high performing frequency baselines. [sent-110, score-0.049]
</p><p>70 The frequency baseline for FVGs is five times the  –  –  2An anonymous reviewer rightly warns against sketching an overly positive picture of the knowledge captured in the AMMs by only presenting a few clusters. [sent-111, score-0.049]
</p><p>71 However, the clustering performed here is only secondary to our main goal of improving collocation extraction. [sent-112, score-0.339]
</p><p>72 111 # classes 1  A-N category 1  2  4  ramm damm  8  16  45. [sent-114, score-0.627]
</p><p>73 2  category 1–2 category 1–3  ramm damm ramm damm  55. [sent-122, score-1.162]
</p><p>74 8  PP-V  figurative FVG VPC intransitive transitive  4468. [sent-170, score-0.131]
</p><p>75 1  Table 1: Average precision for AMM-based association measures and baselines on three datasets. [sent-250, score-0.177]
</p><p>76 Since the AMMs provide a better fit for the more frequent pairs in the training data, they might end up providing too good pexp-estimates for the true collocations from the beginning. [sent-253, score-0.134]
</p><p>77 Further investigation is needed to find out whether this situation can be ameliorated and, if not, whether we can systematically identify for what kind of collocation extraction tasks using better pexps is simply not a good idea. [sent-254, score-0.463]
</p><p>78 English Verb-Particle constructions The last gold standard is the English VPC dataset, shown in the bottom slice of Table 1. [sent-255, score-0.131]
</p><p>79 We can clearly see the effect of the largest  AMMs approaching the full bigramme model as average precision here approaches the random baseline. [sent-257, score-0.339]
</p><p>80 The VPC extraction task shows a difference between the two AMM-based measures: AMMratio does not improve at all, remaining below the frequency baseline. [sent-258, score-0.101]
</p><p>81 AMM-divergence, however, shows a slight decrease in precision first, but ends up performing above the frequency baseline for the 8-class AMMs in both subtasks. [sent-259, score-0.075]
</p><p>82 Table 3 shows four clusters of verbs and particles. [sent-260, score-0.04]
</p><p>83 The large first cluster contains verbs that involve motion/displacement of the subject or object and associated particles, for instance walk about or push away. [sent-261, score-0.08]
</p><p>84 Interestingly, the description of the gold standard gives exactly such cases as negatives, since they constitute compositional verbparticle constructions (Baldwin, 2008). [sent-262, score-0.1]
</p><p>85 collocation extraction by decreasing the impact of verb-preposition associations that are due to PPselecting verbs. [sent-264, score-0.43]
</p><p>86 Class 4 shows a third type of distributional generalization: the verbs in this class are all frequently used in the passive. [sent-265, score-0.053]
</p><p>87 3 Variation due to local optima We start each EM run with a random initialization of the model parameters. [sent-267, score-0.042]
</p><p>88 Since EM finds local rather than global optima, each run may lead to different AMMs, which in turn will affect AMMbased collocation extraction. [sent-268, score-0.339]
</p><p>89 ), so that a run with the same data and the same number of classes will always learn (almost) the same model. [sent-276, score-0.046]
</p><p>90 On the assumption that an average over several runs will vary less than individual runs, we have also constructed a combined pexp by averaging over 40 pexps. [sent-277, score-0.356]
</p><p>91 The last column Variation in avg precision min A-N cat 1 cat 1–2 cat 1–3  ramm damm ramm damm ramm damm  46. [sent-278, score-1.805]
</p><p>92 9  Table 4: Variation on A-N data over 40 EM runs and result of combining pexps. [sent-314, score-0.035]
</p><p>93 in Table 4 shows this combined estimator leads to good extraction results. [sent-315, score-0.052]
</p><p>94 4  Conclusions  In this paper, we have started to explore collocation extraction beyond the assumption of independence. [sent-316, score-0.447]
</p><p>95 We have introduced two new association measures that do away with this assumption in the estimation of expected probabilities. [sent-317, score-0.216]
</p><p>96 A possible obstacle in the adoption of AMMs in collocation extraction is that we have not provided any heuristic for setting the number of classes for the AMMs. [sent-320, score-0.437]
</p><p>97 In general, considering these smaller models might suffice for tasks that have a fairly restricted definition of collocation candidate, like the tasks in our evaluation do. [sent-323, score-0.367]
</p><p>98 Because AMM fitting is unsupervised, selecting a class size is in this respect no different from selecting a suitable association measure from the canon of existing measures. [sent-324, score-0.119]
</p><p>99 Future research into association measures that  are not based on the independence assumption will also include considering different EM variants and other automatically learnable models besides the AMMs used in this paper. [sent-325, score-0.333]
</p><p>100 Finally, the idea of using an informed estimate of expected probability in an association measure need not be confined to (P)MI, as there are many other measures that employ expected probabilities. [sent-326, score-0.279]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('amms', 0.433), ('collocation', 0.339), ('bigramme', 0.313), ('ramm', 0.289), ('damm', 0.265), ('pexp', 0.265), ('amm', 0.217), ('independence', 0.151), ('pmi', 0.13), ('vpc', 0.12), ('collocations', 0.106), ('mi', 0.103), ('multiword', 0.099), ('measures', 0.088), ('mwe', 0.072), ('evert', 0.072), ('krenn', 0.072), ('pexps', 0.072), ('pobs', 0.072), ('german', 0.062), ('em', 0.059), ('particles', 0.058), ('slice', 0.058), ('aggregate', 0.057), ('assumption', 0.056), ('saul', 0.054), ('class', 0.053), ('extraction', 0.052), ('intransitive', 0.049), ('frequency', 0.049), ('transitive', 0.048), ('beige', 0.048), ('fvgs', 0.048), ('karte', 0.048), ('unigramme', 0.048), ('vpcs', 0.048), ('classes', 0.046), ('stefan', 0.042), ('auto', 0.042), ('brigitte', 0.042), ('memo', 0.042), ('optima', 0.042), ('gold', 0.041), ('mutual', 0.041), ('team', 0.04), ('baldwin', 0.04), ('clusters', 0.04), ('associations', 0.039), ('blitzer', 0.039), ('cat', 0.039), ('hidden', 0.039), ('fat', 0.039), ('yellow', 0.039), ('colours', 0.039), ('collocational', 0.039), ('yx', 0.039), ('association', 0.038), ('vs', 0.036), ('variation', 0.036), ('ratio', 0.035), ('runs', 0.035), ('expected', 0.034), ('figurative', 0.034), ('potsdam', 0.034), ('organisation', 0.034), ('expressions', 0.034), ('rooth', 0.033), ('particle', 0.033), ('constructions', 0.032), ('informed', 0.032), ('bouma', 0.031), ('estimate', 0.031), ('equivalent', 0.03), ('combinations', 0.03), ('cluster', 0.03), ('lrec', 0.03), ('subtasks', 0.029), ('card', 0.029), ('hofmann', 0.029), ('fitting', 0.028), ('might', 0.028), ('compositional', 0.027), ('timothy', 0.027), ('category', 0.027), ('precision', 0.026), ('walk', 0.025), ('baselines', 0.025), ('push', 0.025), ('markov', 0.024), ('pointwise', 0.024), ('car', 0.024), ('church', 0.024), ('pc', 0.024), ('adjectives', 0.024), ('newspaper', 0.023), ('denoting', 0.023), ('probability', 0.022), ('nouns', 0.022), ('uninteresting', 0.021), ('tce', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="60-tfidf-1" href="./acl-2010-Collocation_Extraction_beyond_the_Independence_Assumption.html">60 acl-2010-Collocation Extraction beyond the Independence Assumption</a></p>
<p>Author: Gerlof Bouma</p><p>Abstract: In this paper we start to explore two-part collocation extraction association measures that do not estimate expected probabilities on the basis of the independence assumption. We propose two new measures based upon the well-known measures of mutual information and pointwise mutual information. Expected probabilities are derived from automatically trained Aggregate Markov Models. On three collocation gold standards, we find the new association measures vary in their effectiveness.</p><p>2 0.27417192 <a title="60-tfidf-2" href="./acl-2010-Automatic_Collocation_Suggestion_in_Academic_Writing.html">36 acl-2010-Automatic Collocation Suggestion in Academic Writing</a></p>
<p>Author: Jian-Cheng Wu ; Yu-Chia Chang ; Teruko Mitamura ; Jason S. Chang</p><p>Abstract: In recent years, collocation has been widely acknowledged as an essential characteristic to distinguish native speakers from non-native speakers. Research on academic writing has also shown that collocations are not only common but serve a particularly important discourse function within the academic community. In our study, we propose a machine learning approach to implementing an online collocation writing assistant. We use a data-driven classifier to provide collocation suggestions to improve word choices, based on the result of classifica- tion. The system generates and ranks suggestions to assist learners’ collocation usages in their academic writing with satisfactory results. 1</p><p>3 0.2715292 <a title="60-tfidf-3" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>Author: Zhanyi Liu ; Haifeng Wang ; Hua Wu ; Sheng Li</p><p>Abstract: This paper proposes to use monolingual collocations to improve Statistical Machine Translation (SMT). We make use of the collocation probabilities, which are estimated from monolingual corpora, in two aspects, namely improving word alignment for various kinds of SMT systems and improving phrase table for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1</p><p>4 0.084834956 <a title="60-tfidf-4" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>Author: Omri Abend ; Ari Rappoport</p><p>Abstract: The core-adjunct argument distinction is a basic one in the theory of argument structure. The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategorization acquisition. This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms. This is the first work to tackle this task in a fully unsupervised scenario.</p><p>5 0.060834378 <a title="60-tfidf-5" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>6 0.059055887 <a title="60-tfidf-6" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>7 0.056481529 <a title="60-tfidf-7" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>8 0.053437702 <a title="60-tfidf-8" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>9 0.049843039 <a title="60-tfidf-9" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>10 0.046423893 <a title="60-tfidf-10" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>11 0.044708479 <a title="60-tfidf-11" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>12 0.042008717 <a title="60-tfidf-12" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>13 0.040949587 <a title="60-tfidf-13" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>14 0.040805481 <a title="60-tfidf-14" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>15 0.039645817 <a title="60-tfidf-15" href="./acl-2010-A_Latent_Dirichlet_Allocation_Method_for_Selectional_Preferences.html">10 acl-2010-A Latent Dirichlet Allocation Method for Selectional Preferences</a></p>
<p>16 0.038530838 <a title="60-tfidf-16" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>17 0.038479913 <a title="60-tfidf-17" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>18 0.037839893 <a title="60-tfidf-18" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>19 0.037498791 <a title="60-tfidf-19" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>20 0.037281394 <a title="60-tfidf-20" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.138), (1, 0.003), (2, -0.008), (3, 0.001), (4, 0.061), (5, 0.008), (6, -0.039), (7, 0.01), (8, 0.109), (9, -0.002), (10, -0.002), (11, 0.069), (12, -0.011), (13, 0.06), (14, 0.189), (15, 0.036), (16, -0.034), (17, -0.18), (18, 0.189), (19, 0.371), (20, -0.084), (21, 0.065), (22, -0.221), (23, 0.094), (24, -0.023), (25, 0.081), (26, 0.041), (27, -0.009), (28, -0.031), (29, -0.066), (30, 0.071), (31, 0.01), (32, 0.018), (33, 0.013), (34, -0.093), (35, 0.04), (36, -0.024), (37, -0.033), (38, 0.031), (39, -0.024), (40, 0.005), (41, -0.025), (42, 0.013), (43, -0.023), (44, 0.001), (45, 0.011), (46, -0.033), (47, -0.018), (48, 0.034), (49, 0.003)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91913998 <a title="60-lsi-1" href="./acl-2010-Collocation_Extraction_beyond_the_Independence_Assumption.html">60 acl-2010-Collocation Extraction beyond the Independence Assumption</a></p>
<p>Author: Gerlof Bouma</p><p>Abstract: In this paper we start to explore two-part collocation extraction association measures that do not estimate expected probabilities on the basis of the independence assumption. We propose two new measures based upon the well-known measures of mutual information and pointwise mutual information. Expected probabilities are derived from automatically trained Aggregate Markov Models. On three collocation gold standards, we find the new association measures vary in their effectiveness.</p><p>2 0.91496181 <a title="60-lsi-2" href="./acl-2010-Automatic_Collocation_Suggestion_in_Academic_Writing.html">36 acl-2010-Automatic Collocation Suggestion in Academic Writing</a></p>
<p>Author: Jian-Cheng Wu ; Yu-Chia Chang ; Teruko Mitamura ; Jason S. Chang</p><p>Abstract: In recent years, collocation has been widely acknowledged as an essential characteristic to distinguish native speakers from non-native speakers. Research on academic writing has also shown that collocations are not only common but serve a particularly important discourse function within the academic community. In our study, we propose a machine learning approach to implementing an online collocation writing assistant. We use a data-driven classifier to provide collocation suggestions to improve word choices, based on the result of classifica- tion. The system generates and ranks suggestions to assist learners’ collocation usages in their academic writing with satisfactory results. 1</p><p>3 0.59375924 <a title="60-lsi-3" href="./acl-2010-Improving_Statistical_Machine_Translation_with_Monolingual_Collocation.html">147 acl-2010-Improving Statistical Machine Translation with Monolingual Collocation</a></p>
<p>Author: Zhanyi Liu ; Haifeng Wang ; Hua Wu ; Sheng Li</p><p>Abstract: This paper proposes to use monolingual collocations to improve Statistical Machine Translation (SMT). We make use of the collocation probabilities, which are estimated from monolingual corpora, in two aspects, namely improving word alignment for various kinds of SMT systems and improving phrase table for phrase-based SMT. The experimental results show that our method improves the performance of both word alignment and translation quality significantly. As compared to baseline systems, we achieve absolute improvements of 2.40 BLEU score on a phrase-based SMT system and 1.76 BLEU score on a parsing-based SMT system. 1</p><p>4 0.34054795 <a title="60-lsi-4" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>Author: Omri Abend ; Ari Rappoport</p><p>Abstract: The core-adjunct argument distinction is a basic one in the theory of argument structure. The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategorization acquisition. This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms. This is the first work to tackle this task in a fully unsupervised scenario.</p><p>5 0.26723295 <a title="60-lsi-5" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>Author: Daria Bogdanova</p><p>Abstract: Various text mining algorithms require the process offeature selection. High-level semantically rich features, such as figurative language uses, speech errors etc., are very promising for such problems as e.g. writing style detection, but automatic extraction of such features is a big challenge. In this paper, we propose a framework for figurative language use detection. This framework is based on the idea of sense differentiation. We describe two algorithms illustrating the mentioned idea. We show then how these algorithms work by applying them to Russian language data.</p><p>6 0.24035066 <a title="60-lsi-6" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>7 0.23853128 <a title="60-lsi-7" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>8 0.21607119 <a title="60-lsi-8" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>9 0.21116559 <a title="60-lsi-9" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>10 0.20880014 <a title="60-lsi-10" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>11 0.20702139 <a title="60-lsi-11" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>12 0.20480211 <a title="60-lsi-12" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>13 0.2044642 <a title="60-lsi-13" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>14 0.20241746 <a title="60-lsi-14" href="./acl-2010-Extracting_Sequences_from_the_Web.html">111 acl-2010-Extracting Sequences from the Web</a></p>
<p>15 0.19907449 <a title="60-lsi-15" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>16 0.19659957 <a title="60-lsi-16" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>17 0.19570309 <a title="60-lsi-17" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>18 0.19217259 <a title="60-lsi-18" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>19 0.19214833 <a title="60-lsi-19" href="./acl-2010-Optimizing_Question_Answering_Accuracy_by_Maximizing_Log-Likelihood.html">189 acl-2010-Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</a></p>
<p>20 0.19161835 <a title="60-lsi-20" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.019), (25, 0.06), (39, 0.011), (42, 0.026), (44, 0.015), (59, 0.106), (71, 0.011), (73, 0.039), (75, 0.293), (76, 0.017), (78, 0.048), (80, 0.014), (83, 0.118), (84, 0.043), (98, 0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81536722 <a title="60-lda-1" href="./acl-2010-Models_of_Metaphor_in_NLP.html">175 acl-2010-Models of Metaphor in NLP</a></p>
<p>Author: Ekaterina Shutova</p><p>Abstract: Automatic processing of metaphor can be clearly divided into two subtasks: metaphor recognition (distinguishing between literal and metaphorical language in a text) and metaphor interpretation (identifying the intended literal meaning of a metaphorical expression). Both of them have been repeatedly addressed in NLP. This paper is the first comprehensive and systematic review of the existing computational models of metaphor, the issues of metaphor annotation in corpora and the available resources.</p><p>same-paper 2 0.76472235 <a title="60-lda-2" href="./acl-2010-Collocation_Extraction_beyond_the_Independence_Assumption.html">60 acl-2010-Collocation Extraction beyond the Independence Assumption</a></p>
<p>Author: Gerlof Bouma</p><p>Abstract: In this paper we start to explore two-part collocation extraction association measures that do not estimate expected probabilities on the basis of the independence assumption. We propose two new measures based upon the well-known measures of mutual information and pointwise mutual information. Expected probabilities are derived from automatically trained Aggregate Markov Models. On three collocation gold standards, we find the new association measures vary in their effectiveness.</p><p>3 0.60427308 <a title="60-lda-3" href="./acl-2010-Error_Detection_for_Statistical_Machine_Translation_Using_Linguistic_Features.html">102 acl-2010-Error Detection for Statistical Machine Translation Using Linguistic Features</a></p>
<p>Author: Deyi Xiong ; Min Zhang ; Haizhou Li</p><p>Abstract: Automatic error detection is desired in the post-processing to improve machine translation quality. The previous work is largely based on confidence estimation using system-based features, such as word posterior probabilities calculated from Nbest lists or word lattices. We propose to incorporate two groups of linguistic features, which convey information from outside machine translation systems, into error detection: lexical and syntactic features. We use a maximum entropy classifier to predict translation errors by integrating word posterior probability feature and linguistic features. The experimental results show that 1) linguistic features alone outperform word posterior probability based confidence estimation in error detection; and 2) linguistic features can further provide complementary information when combined with word confidence scores, which collectively reduce the classification error rate by 18.52% and improve the F measure by 16.37%.</p><p>4 0.56712079 <a title="60-lda-4" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>Author: Diarmuid O Seaghdha</p><p>Abstract: This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.</p><p>5 0.56206775 <a title="60-lda-5" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>6 0.5584963 <a title="60-lda-6" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>7 0.55772233 <a title="60-lda-7" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>8 0.55658495 <a title="60-lda-8" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>9 0.5560782 <a title="60-lda-9" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>10 0.55446446 <a title="60-lda-10" href="./acl-2010-Cognitively_Plausible_Models_of_Human_Language_Processing.html">59 acl-2010-Cognitively Plausible Models of Human Language Processing</a></p>
<p>11 0.55434811 <a title="60-lda-11" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>12 0.5539341 <a title="60-lda-12" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>13 0.55047792 <a title="60-lda-13" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>14 0.55044544 <a title="60-lda-14" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>15 0.55041695 <a title="60-lda-15" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>16 0.55014479 <a title="60-lda-16" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>17 0.54998362 <a title="60-lda-17" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>18 0.54994375 <a title="60-lda-18" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>19 0.54908228 <a title="60-lda-19" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>20 0.54881388 <a title="60-lda-20" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
