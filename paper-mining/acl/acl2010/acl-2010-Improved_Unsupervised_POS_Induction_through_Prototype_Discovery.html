<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-144" href="#">acl2010-144</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</h1>
<br/><p>Source: <a title="acl-2010-144-pdf" href="http://aclweb.org/anthology//P/P10/P10-1132.pdf">pdf</a></p><p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>Reference: <a title="acl-2010-144-reference" href="../acl2010_reference/acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 i ri l  Abstract We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. [sent-4, score-0.317]
</p><p>2 The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. [sent-5, score-0.732]
</p><p>3 We utilize morphological and distributional representations computed in a fully unsupervised manner. [sent-7, score-0.47]
</p><p>4 Automatic induction of POS tags from plain text can greatly alleviate this problem, as well as eliminate the efforts incurred by manual annotations. [sent-13, score-0.255]
</p><p>5 Our algorithm first clusters words based on a fine morphological representation. [sent-19, score-0.816]
</p><p>6 It then clusters the most frequent words,  defining landmark clusters which constitute the cores of the categories. [sent-20, score-0.881]
</p><p>7 The last two stages utilize a distributional representation that has been shown to be effective for unsupervised parsing (Seginer, 2007). [sent-22, score-0.298]
</p><p>8 We evaluated the algorithm in both English and German, using four different mapping-based and information theoretic clustering evaluation measures. [sent-23, score-0.267]
</p><p>9 The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with  the likelihood function of the Brown algorithm (Brown et al. [sent-30, score-0.456]
</p><p>10 Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. [sent-48, score-0.705]
</p><p>11 While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. [sent-55, score-0.278]
</p><p>12 Dasgupta and Ng (2007) use the output of the Morfessor segmentation algorithm for their morphological representation. [sent-63, score-0.298]
</p><p>13 Morfessor (Creutz and Lagus, 2005), which we use here as well, is an unsupervised algorithm that segments words and classifies each segment as being a stem or an affix. [sent-64, score-0.285]
</p><p>14 First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the words according to their association with the prototypes. [sent-67, score-0.431]
</p><p>15 Second, we use a distributional representation which has been shown to be effective for unsupervised parsing (Seginer, 2007). [sent-68, score-0.298]
</p><p>16 Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or deriva-  tional morphology (Goldsmith, 2001). [sent-69, score-0.304]
</p><p>17 3  Distributional Algorithm  Our algorithm is given a plain text corpus and optionally a desired number of clusters k. [sent-70, score-0.467]
</p><p>18 In the next section we describe the morphological representation and its integration into the base algorithm. [sent-74, score-0.274]
</p><p>19 The algorithm consists of two main stages: landmark clusters discovery, and word mapping. [sent-76, score-0.583]
</p><p>20 We then cluster the coordinates corresponding to high frequency words. [sent-78, score-0.402]
</p><p>21 In the word mapping stage we map each word to the most similar landmark cluster. [sent-80, score-0.312]
</p><p>22 The right context entry of a word x ∈ W is a pair Tofh mappings r intx : W of → [0, 1] ∈an Wd r adjx : W → [0, 1] . [sent-89, score-0.387]
</p><p>23 For eac:h w ∈ W, r adjx (w) disj an adjacency score orf e w tho x, reflecting wdj’s tendency to appear on the right hand side of x. [sent-90, score-0.255]
</p><p>24 For each w ∈ W, r intx (w) is an interchange-  ability score wo f∈ x Ww,i trh w, reflecting the tendency of w to appear to the left of words that tend to appear to the right of x. [sent-91, score-0.265]
</p><p>25 Left context parameters l intx and l adjx are defined analogously. [sent-94, score-0.348]
</p><p>26 First, for two words x, w ∈ W, r adjx (w) itiso generally dfoifrfe twreont w forordms l adjw (x). [sent-96, score-0.255]
</p><p>27 Fro ard example, if w is a high frequency word and x is a low frequency word, it is likely that w appears many times to the right of x, yielding a high r adjx (w), but that x appears only a few times to the left of w yielding a low l adjw (x). [sent-97, score-0.346]
</p><p>28 Also, r intx (w) and r adjx (w) might become larger than 1. [sent-103, score-0.348]
</p><p>29 We update l intx and l adjx analogously using the word z immediately to the left of x. [sent-105, score-0.348]
</p><p>30 Two additional coordinates represent the frequency in which the word appears to the left and to the right of a stopping punctuation. [sent-109, score-0.255]
</p><p>31 Of the 4|W| coordinates corresponding ttou words, we aell 4ow|W only o2rnd ntoa ebes non-zero: tinheg n top scoring among the right side coordinates (those of r intx and r adjx), and the n top scoring among the left side coordinates (those of l intx and l adjx). [sent-110, score-0.892]
</p><p>32 Each of our landmark clusters will correspond to a set of high frequency words (HFWs). [sent-114, score-0.579]
</p><p>33 Our algorithm does that by unifying some of the non-zero coordinates corresponding to HFWs in the distributional representation defined above. [sent-117, score-0.459]
</p><p>34 AVGLINKα means running the average link algorithm until the two closest clusters have a distance larger than α. [sent-121, score-0.43]
</p><p>35 We then use the induced clustering to update the distributional representation, by collapsing all coordinates corresponding to words appearing in the same cluster into a single coordinate whose value is the sum of the collapsed coordinates’ values. [sent-122, score-0.906]
</p><p>36 Since all eventual POS categories correspond to clusters produced at this stage, to reduce noise we delete clusters of less than five elements. [sent-129, score-0.648]
</p><p>37 We define landmark clusters using the clustering obtained in the final iteration of the coordinate clustering stage. [sent-131, score-0.872]
</p><p>38 However, the number of clusters might be greater than the desired number k, which is an optional parameter of the algorithm. [sent-132, score-0.324]
</p><p>39 In this case we select a subset of k clusters that best covers the HFW space. [sent-133, score-0.324]
</p><p>40 1300  ter farthest from the clusters already selected. [sent-136, score-0.324]
</p><p>41 The distance between two clusters is defined to be the average distance between their members. [sent-137, score-0.386]
</p><p>42 A cluster’s distance from a set of clusters is defined to be its minimal distance from the clusters in the  set. [sent-138, score-0.71]
</p><p>43 Each word w ∈ W is assigned the cluster Li thEata ccohnt waionrsd i wts n ∈ea Wrest i prototype: d(w, Li) = minx∈Li {1 cos(vw, vx)} Map(w) = argminLi {d(w, Li)} Words that appear less than 5 times are considered as unknown words. [sent-145, score-0.26]
</p><p>44 However, when the number k of landmark clusters is relatively large, it is beneficial to assign all unknown words to a separate new cluster (after running the algorithm with k −1). [sent-148, score-0.873]
</p><p>45 This clustering is integrated with the distributional model as described below. [sent-151, score-0.303]
</p><p>46 The morphological representation of a word type is then defined to be its stem’s signature in conjunction with its specific affixes2 (See Figure 1). [sent-156, score-0.354]
</p><p>47 For instance, if the words joined and painted are found to have the same signature, they would share the same cluster since both have the affix ‘ ed’ . [sent-158, score-0.282]
</p><p>48 This results in coarse-grained clusters exclusively defined according to morphology. [sent-160, score-0.324]
</p><p>49 In addition, we incorporate capitalization information into the model, by constraining all words that appear capitalized in more than half of their instances to belong to a separate cluster, regardless of their morphological representation. [sent-163, score-0.335]
</p><p>50 We do not assign a morphological representation to words including more than one stem (like weatherman), to words that have a null affix (i. [sent-170, score-0.491]
</p><p>51 Words that were not assigned a morphological representation are included as singletons in the morphological clustering. [sent-173, score-0.497]
</p><p>52 2 Distributional-Morphological Algorithm We detail the modifications made to our base distributional algorithm given the morphological clustering defined above. [sent-175, score-0.601]
</p><p>53 We constrain AVGLINKα to begin by forming links between words appearing in the same morphological cluster. [sent-177, score-0.291]
</p><p>54 Only when the distance between the two closest clusters gets above α we remove this constraint and proceed as before. [sent-178, score-0.355]
</p><p>55 This is equivalent to performing AVGLINKα separately within each morphological cluster and then using the re-  sult as an initial condition for an AVGLINKα coordinate clustering. [sent-179, score-0.483]
</p><p>56 The modified algorithm in this stage is otherwise identical to the distributional algorithm. [sent-180, score-0.286]
</p><p>57 In this stage words that are not prototypes are mapped to one of the landmark 1301  clusters. [sent-182, score-0.394]
</p><p>58 A reasonable strategy would be to map all words sharing a morphological cluster as a single unit. [sent-183, score-0.439]
</p><p>59 We therefore begin by partitioning the morphological clusters into sub-clusters according to their distributional behavior. [sent-185, score-0.705]
</p><p>60 5  Clustering Evaluation  We evaluate the clustering produced by our algorithm using an external quality measure: we take a corpus tagged by gold standard tags, tag it using the induced tags, and compare the two taggings. [sent-190, score-0.45]
</p><p>61 After the induced clusters are mapped, we can compute a derived accuracy. [sent-196, score-0.424]
</p><p>62 The Many-to-1 measure finds the mapping between the gold standard clusters and the induced clusters which maximizes accuracy, allowing several induced clusters to be mapped to the same gold standard cluster. [sent-197, score-1.412]
</p><p>63 The 1-to-1 measure finds the mapping between the induced and gold standard clusters which max-  imizes accuracy such that no two induced clusters are mapped to the same gold cluster. [sent-198, score-1.088]
</p><p>64 These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. [sent-202, score-0.375]
</p><p>65 25 (cluster separation during landmark cluster generation), β = 0. [sent-208, score-0.37]
</p><p>66 For  this reason we do not assign a cluster to punctuation marks and we report results using this policy, which we recommend for future work. [sent-215, score-0.351]
</p><p>67 We do so by assigning a singleton cluster to each punctuation mark (in addition to the k required clusters). [sent-217, score-0.314]
</p><p>68 There are 45 clusters in this annotation scheme, 34 of which are not punctuation. [sent-237, score-0.364]
</p><p>69 We compare the output to two annotation schemes: the fine grained PTB WSJ scheme, and the coarse grained tags defined in (Smith and Eisner, 2005). [sent-239, score-0.467]
</p><p>70 The output of the k=13 run is evaluated both against the coarse POS tag annotation (the ‘Coarse k=13 ’ scenario) and against the full PTB-WSJ annotation scheme (the ‘Fine k=13 ’ scenario). [sent-240, score-0.307]
</p><p>71 The POS cluster frequency distribution tends to be skewed: each of the 13 most frequent clusters in the PTB-WSJ cover more than 2. [sent-242, score-0.551]
</p><p>72 We therefore chose k=13, since it is both the number of coarse POS tags (excluding punctuation) as well as the number of frequent POS tags in the PTB-WSJ annotation scheme. [sent-245, score-0.337]
</p><p>73 There are 62 clusters in this annotation scheme, 5 1 of which are not punctuation. [sent-255, score-0.364]
</p><p>74 k=26 was chosen since it is the number of clusters that cover each more than 0. [sent-257, score-0.324]
</p><p>75 We do not report results for k=51 (where the number of gold clusters is the same as the number of induced clusters), since our algorithm produced only 42 clusters in the landmark detection stage. [sent-262, score-1.072]
</p><p>76 B is the strictly distributional algorithm, B+M adds the morphological model, B+C adds capitalization to B, F(I=1) consists of all components, where only one iteration of coordinate clustering is performed, and F is the full model. [sent-278, score-0.682]
</p><p>77 The models differ in the annotation scheme, the corpus size and the number of induced clusters (k) that they used. [sent-280, score-0.464]
</p><p>78 , has only one nonsingleton morphological class, that of words appearing capitalized in most of their instances) and a variant which uses no capitalization information, defining the morphological clusters according to the morphological representation alone. [sent-302, score-1.194]
</p><p>79 A significant difference between our algorithm and Clark’s is that the latter, like most algorithms which addressed the task, induces the clustering  0 . [sent-306, score-0.262]
</p><p>80 The full coordinate clustering stage (several iterations, F) considerably improves the score  over a single iteration (F(I=1)). [sent-327, score-0.31]
</p><p>81 Capitalization information increases the score more than the morphological information, which might stem from the granularity of the POS tag set with respect to names. [sent-328, score-0.379]
</p><p>82 There, the decrease in performance was only of 1%–2% in the mapping 3The fluctuations inflicted on our algorithm by the random mapping of unknown words are of less than 0. [sent-330, score-0.329]
</p><p>83 Finally, Table 4 presents reported results for all recent algorithms we are aware of that tackled the task of unsupervised POS induction from plain text. [sent-351, score-0.284]
</p><p>84 The settings of the various experiments vary in terms of the exact annotation scheme used (coarse or fine  grained) and the size of the test set. [sent-353, score-0.273]
</p><p>85 However, the score differences are sufficiently large to justify the claim that our algorithm is currently the best performing algorithm on the PTB-WSJ corpus for POS induction from plain text4. [sent-354, score-0.303]
</p><p>86 8  Discussion  In this work we presented a novel unsupervised algorithm for POS induction from plain text. [sent-359, score-0.317]
</p><p>87 The algorithm first generates relatively accurate clusters of high frequency words, which are subsequently  used to bootstrap the entire clustering. [sent-360, score-0.44]
</p><p>88 The distributional and morphological representations that we use are novel for this task. [sent-361, score-0.381]
</p><p>89 We experimented on two languages with mapping and information theoretic clustering evaluation measures. [sent-362, score-0.267]
</p><p>90 In order to asses this quantitatively, let us define a random variable for each of the gold clusters, which receives a value corresponding to each induced cluster with probability proportional to their intersection size. [sent-371, score-0.351]
</p><p>91 In ad-  dition, we greedily map each induced cluster to a gold cluster and compute the ratio between their intersection size and the size of the gold cluster (mapping accuracy). [sent-373, score-0.788]
</p><p>92 The clusters that obtained the best scores were (brackets indicate mapping accuracy and entropy for each of these clusters) coordinating conjunctions (95%, 0. [sent-375, score-0.399]
</p><p>93 In order to  assess the performance loss caused by the monosemous nature of our algorithm, we took the M-1 greedy mapping computed for the entire dataset and used it to compute accuracy over the monosemous and polysemous words separately. [sent-402, score-0.497]
</p><p>94 We define a word to be monosemous if more than 95% of its tokens are assigned the same gold standard tag. [sent-404, score-0.324]
</p><p>95 For English, there are approximately 255K polysemous tokens and 578K monosemous ones. [sent-405, score-0.331]
</p><p>96 It could be expected that a monosemous algorithm such as ours would perform poorly in a type level evaluation. [sent-411, score-0.265]
</p><p>97 Our results were better than Clark’s (the only other monosemous algorithm evaluated there) on all measures in a margin of 5–21%. [sent-417, score-0.276]
</p><p>98 The fact that our monosemous algorithm was better than good polysemous algorithms in a type level evaluation can be explained by the prototypical nature of the POS phenomenon (a longer discussion is given in (Reichart et al. [sent-418, score-0.379]
</p><p>99 However, the quality upper bound for monosemous algorithms is obviously much lower than that for polysemous algorithms, and we expect polysemous algorithms to outperform monosemous algorithms in the future in both type level and token level evaluations. [sent-420, score-0.62]
</p><p>100 The skewed (Zipfian) distribution of POS class frequencies in corpora is a problem for many POS  induction algorithms, which by default tend to induce a clustering having a balanced distribution. [sent-421, score-0.268]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clusters', 0.324), ('morphological', 0.223), ('cluster', 0.186), ('landmark', 0.184), ('adjx', 0.184), ('pos', 0.182), ('clark', 0.18), ('coordinates', 0.175), ('reichart', 0.169), ('fine', 0.164), ('avglink', 0.164), ('intx', 0.164), ('monosemous', 0.16), ('distributional', 0.158), ('clustering', 0.145), ('punctuation', 0.128), ('hfws', 0.123), ('negra', 0.123), ('tags', 0.102), ('induced', 0.1), ('tokens', 0.099), ('gael', 0.098), ('seginer', 0.098), ('coarse', 0.093), ('prototypes', 0.092), ('stem', 0.091), ('unsupervised', 0.089), ('induction', 0.085), ('capitalization', 0.082), ('rappoport', 0.08), ('scenario', 0.079), ('roi', 0.077), ('algorithm', 0.075), ('mapping', 0.075), ('unknown', 0.074), ('coordinate', 0.074), ('polysemous', 0.072), ('nvi', 0.072), ('scheme', 0.069), ('plain', 0.068), ('ari', 0.066), ('affix', 0.066), ('omri', 0.066), ('gold', 0.065), ('german', 0.065), ('tag', 0.065), ('johnson', 0.062), ('gra', 0.061), ('dewac', 0.061), ('hfw', 0.061), ('schemes', 0.061), ('goldwater', 0.058), ('dasgupta', 0.054), ('hmm', 0.053), ('stage', 0.053), ('tagging', 0.052), ('representation', 0.051), ('signature', 0.05), ('cores', 0.049), ('goldsmith', 0.049), ('theoretic', 0.047), ('lagus', 0.046), ('morfessor', 0.046), ('haghighi', 0.046), ('griffiths', 0.043), ('algorithms', 0.042), ('creutz', 0.042), ('frequency', 0.041), ('adjw', 0.041), ('meila', 0.041), ('munkres', 0.041), ('zipfian', 0.041), ('measures', 0.041), ('gao', 0.04), ('annotation', 0.04), ('right', 0.039), ('considerably', 0.038), ('appearing', 0.038), ('induce', 0.038), ('marks', 0.037), ('prototype', 0.037), ('van', 0.037), ('yoav', 0.036), ('biemann', 0.036), ('ihmm', 0.036), ('merialdo', 0.036), ('mapped', 0.035), ('eisner', 0.035), ('grained', 0.034), ('taylor', 0.034), ('english', 0.033), ('abend', 0.033), ('azrieli', 0.033), ('reflecting', 0.032), ('excluding', 0.031), ('distance', 0.031), ('rosenberg', 0.031), ('nanc', 0.031), ('words', 0.03), ('type', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="144-tfidf-1" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>2 0.24069043 <a title="144-tfidf-2" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>3 0.16837189 <a title="144-tfidf-3" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>Author: Omri Abend ; Ari Rappoport</p><p>Abstract: The core-adjunct argument distinction is a basic one in the theory of argument structure. The task of distinguishing between the two has strong relations to various basic NLP tasks such as syntactic parsing, semantic role labeling and subcategorization acquisition. This paper presents a novel unsupervised algorithm for the task that uses no supervised models, utilizing instead state-of-the-art syntactic induction algorithms. This is the first work to tackle this task in a fully unsupervised scenario.</p><p>4 0.1416871 <a title="144-tfidf-4" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>Author: Seth Kulick</p><p>Abstract: We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.</p><p>5 0.12409788 <a title="144-tfidf-5" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>6 0.11220808 <a title="144-tfidf-6" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>7 0.11148409 <a title="144-tfidf-7" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>8 0.10928433 <a title="144-tfidf-8" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>9 0.10888885 <a title="144-tfidf-9" href="./acl-2010-Optimizing_Question_Answering_Accuracy_by_Maximizing_Log-Likelihood.html">189 acl-2010-Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</a></p>
<p>10 0.10661997 <a title="144-tfidf-10" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>11 0.10254817 <a title="144-tfidf-11" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>12 0.10109994 <a title="144-tfidf-12" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>13 0.098970972 <a title="144-tfidf-13" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>14 0.095219575 <a title="144-tfidf-14" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>15 0.094945766 <a title="144-tfidf-15" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>16 0.094315134 <a title="144-tfidf-16" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>17 0.089676313 <a title="144-tfidf-17" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>18 0.089672059 <a title="144-tfidf-18" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>19 0.087350629 <a title="144-tfidf-19" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>20 0.08097861 <a title="144-tfidf-20" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.23), (1, 0.045), (2, 0.046), (3, -0.005), (4, 0.052), (5, -0.037), (6, 0.052), (7, 0.004), (8, 0.203), (9, 0.065), (10, -0.046), (11, 0.091), (12, 0.011), (13, -0.124), (14, -0.041), (15, -0.107), (16, -0.081), (17, -0.006), (18, 0.165), (19, -0.025), (20, 0.097), (21, 0.137), (22, 0.017), (23, 0.059), (24, 0.006), (25, 0.192), (26, -0.056), (27, -0.002), (28, -0.004), (29, -0.048), (30, 0.037), (31, -0.142), (32, 0.079), (33, 0.068), (34, -0.038), (35, -0.198), (36, 0.212), (37, -0.02), (38, 0.072), (39, 0.057), (40, -0.07), (41, -0.082), (42, -0.052), (43, -0.017), (44, 0.107), (45, 0.046), (46, 0.036), (47, 0.001), (48, -0.006), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96810198 <a title="144-lsi-1" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>2 0.84489447 <a title="144-lsi-2" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>Author: Michael Lamar ; Yariv Maron ; Mark Johnson ; Elie Bienenstock</p><p>Abstract: We revisit the algorithm of Schütze (1995) for unsupervised part-of-speech tagging. The algorithm uses reduced-rank singular value decomposition followed by clustering to extract latent features from context distributions. As implemented here, it achieves state-of-the-art tagging accuracy at considerably less cost than more recent methods. It can also produce a range of finer-grained taggings, with potential applications to various tasks. 1</p><p>3 0.65488893 <a title="144-lsi-3" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>Author: Seth Kulick</p><p>Abstract: We describe an approach to simultaneous tokenization and part-of-speech tagging that is based on separating the closed and open-class items, and focusing on the likelihood of the possible stems of the openclass words. By encoding some basic linguistic information, the machine learning task is simplified, while achieving stateof-the-art tokenization results and competitive POS results, although with a reduced tag set and some evaluation difficulties.</p><p>4 0.64172053 <a title="144-lsi-4" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>5 0.54918379 <a title="144-lsi-5" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>Author: Anders Sogaard</p><p>Abstract: Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004).</p><p>6 0.54904449 <a title="144-lsi-6" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>7 0.54706031 <a title="144-lsi-7" href="./acl-2010-Enhanced_Word_Decomposition_by_Calibrating_the_Decision_Threshold_of_Probabilistic_Models_and_Using_a_Model_Ensemble.html">100 acl-2010-Enhanced Word Decomposition by Calibrating the Decision Threshold of Probabilistic Models and Using a Model Ensemble</a></p>
<p>8 0.52193147 <a title="144-lsi-8" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>9 0.51078159 <a title="144-lsi-9" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>10 0.47731209 <a title="144-lsi-10" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>11 0.46350318 <a title="144-lsi-11" href="./acl-2010-Syntax-to-Morphology_Mapping_in_Factored_Phrase-Based_Statistical_Machine_Translation_from_English_to_Turkish.html">221 acl-2010-Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish</a></p>
<p>12 0.45817548 <a title="144-lsi-12" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>13 0.45054418 <a title="144-lsi-13" href="./acl-2010-Optimizing_Question_Answering_Accuracy_by_Maximizing_Log-Likelihood.html">189 acl-2010-Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</a></p>
<p>14 0.42306682 <a title="144-lsi-14" href="./acl-2010-A_Framework_for_Figurative_Language_Detection_Based_on_Sense_Differentiation.html">5 acl-2010-A Framework for Figurative Language Detection Based on Sense Differentiation</a></p>
<p>15 0.39904648 <a title="144-lsi-15" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>16 0.38818762 <a title="144-lsi-16" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>17 0.38556865 <a title="144-lsi-17" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>18 0.3815617 <a title="144-lsi-18" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>19 0.36843821 <a title="144-lsi-19" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>20 0.36479416 <a title="144-lsi-20" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.026), (16, 0.019), (25, 0.071), (39, 0.011), (42, 0.024), (44, 0.014), (59, 0.179), (64, 0.204), (73, 0.044), (76, 0.011), (78, 0.03), (83, 0.092), (84, 0.035), (97, 0.012), (98, 0.15)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90594298 <a title="144-lda-1" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<p>Author: Keun Chan Park ; Yoonjae Jeong ; Sung Hyon Myaeng</p><p>Abstract: Weblogs are a source of human activity knowledge comprising valuable information such as facts, opinions and personal experiences. In this paper, we propose a method for mining personal experiences from a large set of weblogs. We define experience as knowledge embedded in a collection of activities or events which an individual or group has actually undergone. Based on an observation that experience-revealing sentences have a certain linguistic style, we formulate the problem of detecting experience as a classification task using various features including tense, mood, aspect, modality, experiencer, and verb classes. We also present an activity verb lexicon construction method based on theories of lexical semantics. Our results demonstrate that the activity verb lexicon plays a pivotal role among selected features in the classification perfor- , mance and shows that our proposed method outperforms the baseline significantly.</p><p>same-paper 2 0.86150038 <a title="144-lda-2" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>3 0.78305709 <a title="144-lda-3" href="./acl-2010-Knowledge-Rich_Word_Sense_Disambiguation_Rivaling_Supervised_Systems.html">156 acl-2010-Knowledge-Rich Word Sense Disambiguation Rivaling Supervised Systems</a></p>
<p>Author: Simone Paolo Ponzetto ; Roberto Navigli</p><p>Abstract: One of the main obstacles to highperformance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.</p><p>4 0.77957678 <a title="144-lda-4" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>Author: Yun-Cheng Ju ; Tim Paek</p><p>Abstract: Speech recognition affords automobile drivers a hands-free, eyes-free method of replying to Short Message Service (SMS) text messages. Although a voice search approach based on template matching has been shown to be more robust to the challenging acoustic environment of automobiles than using dictation, users may have difficulties verifying whether SMS response templates match their intended meaning, especially while driving. Using a high-fidelity driving simulator, we compared dictation for SMS replies versus voice search in increasingly difficult driving conditions. Although the two approaches did not differ in terms of driving performance measures, users made about six times more errors on average using dictation than voice search. 1</p><p>5 0.77741718 <a title="144-lda-5" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>6 0.77604622 <a title="144-lda-6" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>7 0.77404284 <a title="144-lda-7" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>8 0.77356267 <a title="144-lda-8" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>9 0.77351034 <a title="144-lda-9" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>10 0.77200133 <a title="144-lda-10" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>11 0.77184772 <a title="144-lda-11" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>12 0.77151597 <a title="144-lda-12" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>13 0.77040118 <a title="144-lda-13" href="./acl-2010-All_Words_Domain_Adapted_WSD%3A_Finding_a_Middle_Ground_between_Supervision_and_Unsupervision.html">26 acl-2010-All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision</a></p>
<p>14 0.76648605 <a title="144-lda-14" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>15 0.76638258 <a title="144-lda-15" href="./acl-2010-Discriminative_Pruning_for_Discriminative_ITG_Alignment.html">88 acl-2010-Discriminative Pruning for Discriminative ITG Alignment</a></p>
<p>16 0.76605457 <a title="144-lda-16" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>17 0.76568395 <a title="144-lda-17" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>18 0.76565027 <a title="144-lda-18" href="./acl-2010-BabelNet%3A_Building_a_Very_Large_Multilingual_Semantic_Network.html">44 acl-2010-BabelNet: Building a Very Large Multilingual Semantic Network</a></p>
<p>19 0.76544237 <a title="144-lda-19" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>20 0.76518023 <a title="144-lda-20" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
