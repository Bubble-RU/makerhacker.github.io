<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 acl-2010-Evaluating Machine Translations Using mNCD</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-104" href="#">acl2010-104</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>104 acl-2010-Evaluating Machine Translations Using mNCD</h1>
<br/><p>Source: <a title="acl-2010-104-pdf" href="http://aclweb.org/anthology//P/P10/P10-2015.pdf">pdf</a></p><p>Author: Marcus Dobrinkat ; Tero Tapiovaara ; Jaakko Vayrynen ; Kimmo Kettunen</p><p>Abstract: This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English.</p><p>Reference: <a title="acl-2010-104-reference" href="../acl2010_reference/acl-2010-Evaluating_Machine_Translations_Using_mNCD_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 fi  Abstract This paper introduces mNCD, a method for automatic evaluation of machine translations. [sent-12, score-0.028]
</p><p>2 The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. [sent-13, score-0.265]
</p><p>3 The mNCD measure outperforms NCD in system-level correlation to human judgments in English. [sent-14, score-0.162]
</p><p>4 1 Introduction Automatic evaluation of machine translation (MT) systems requires automated procedures to ensure consistency and efficient handling of large  amounts of data. [sent-15, score-0.053]
</p><p>5 In statistical MT systems, automatic evaluation of translations is essential for parameter optimization and system development. [sent-16, score-0.09]
</p><p>6 However, manual evaluation is important in the comparison ofdifferent MT systems and for the validation and development of automatic MT evaluation measures, which try to model human assessments oftranslations as closely as possible. [sent-18, score-0.088]
</p><p>7 Recently, normalized compression distance (NCD) has been applied to the evaluation of machine translations. [sent-20, score-0.147]
</p><p>8 NCD is a general information theoretic measure of string similarity, whereas most MT evaluation measures, e. [sent-21, score-0.098]
</p><p>9 Parker (2008) introduced BADGER, an MT evaluation measure that uses NCD and a language independent word normalization method. [sent-24, score-0.051]
</p><p>10 BADGER scores were directly compared against the scores of METEOR and word error  rate (WER). [sent-25, score-0.03]
</p><p>11 The correlation between BADGER and METEOR were low and correlations between BADGER and WER high. [sent-26, score-0.146]
</p><p>12 NCD was not compared to human assessments of translations, but correlations of NCD and METEOR scores were very high for all the three language pairs. [sent-30, score-0.137]
</p><p>13 (2010) have extended the work by including NCD in the ACL WMT08 evaluation framework and showing that NCD is correlated to human judgments. [sent-32, score-0.069]
</p><p>14 The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it presented a viable alternative to de facto standard BLEU (Papineni et al. [sent-33, score-0.085]
</p><p>15 Some recent advances in automatic MT evaluation have included non-binary matching between compared items (Banerjee and Lavie, 2005; Agar-  wal and Lavie, 2008; Chan and Ng, 2009), which is implicitly present in the string-based NCD measure. [sent-36, score-0.046]
</p><p>16 We experiment with relaxed word matching using stemming and a lexical database to allow lexical changes. [sent-38, score-0.073]
</p><p>17 These additional modules attempt to make the reference sentences more similar to the evaluated translations on the string level. [sent-39, score-0.142]
</p><p>18 We report an experiment showing that document-level NCD and aggregated NCD scores for individual sentences produce very similar correlations to human judgments. [sent-40, score-0.16]
</p><p>19 2 Normalized Compression Distance Normalized compression distance (NCD) is a similarity measure based on the idea that a string x is similar to another string y when both share substrings. [sent-44, score-0.181]
</p><p>20 The description of y can reference shared substrings in the known x without repetition, indicating shared information. [sent-45, score-0.027]
</p><p>21 Figure 1 shows an example in which the compression of the concatenation of x and y results in a shorter output than individual compressions of x and y. [sent-46, score-0.096]
</p><p>22 The normalized compression distance, as defined by Cilibrasi and Vitanyi (2005), is given in Equation 1, with C(x) as length of the compression of x and C(x, y) as the length of the compression of the concatenation of x and y. [sent-47, score-0.283]
</p><p>23 NCD is an approximation of the uncomputable  normalized information distance (NID), a general measure for the similarity of two objects. [sent-49, score-0.084]
</p><p>24 NID is based on the notion of Kolmogorov complexity K(x), a theoretical measure for the information content of a string x, defined as the shortest universal Turing machine that prints x and stops (Solomonoff, 1964). [sent-50, score-0.056]
</p><p>25 3  mNCD  Normalized compression distance was not conceived with MT evaluation in mind, but rather it is a general measure of string similarity. [sent-52, score-0.176]
</p><p>26 Variation in language leads to several acceptable translations for each source sentence, which  is why multiple reference translations are preferred in evaluation. [sent-55, score-0.163]
</p><p>27 Unfortunately, it is typical to have only one reference translation. [sent-56, score-0.027]
</p><p>28 Paraphrasing techniques can produce additional translation variants (Russo-Lassner et al. [sent-57, score-0.059]
</p><p>29 The proposed method, mNCD, works analogously to M-BLEU and M-TER, which use the flexible word matching modules from METEOR to find relaxed word-to-word alignments (Agarwal and Lavie, 2008). [sent-61, score-0.115]
</p><p>30 The modules are able to align words even if they do not share the same surface form, but instead have a common stem or are synonyms of each other. [sent-62, score-0.064]
</p><p>31 A similarized translation reference is generated by replacing words in the reference with their aligned counterparts from the translation hypothesis. [sent-63, score-0.21]
</p><p>32 The NCD score is computed between the translations and the similarized references to get the mNCD score. [sent-64, score-0.146]
</p><p>33 Table 1 shows some hand-picked German– English candidate translations along with a) the  reference translations including the 1-NCD score to easily compare with METEOR and b) the similarized references including the mNCD score. [sent-65, score-0.235]
</p><p>34 For comparison, the corresponding METEOR scores without implicit relaxed matching are shown. [sent-66, score-0.075]
</p><p>35 4  Experiments  The proposed mNCD and the basic NCD measure were evaluated by computing correlation to human judgments of translations. [sent-67, score-0.162]
</p><p>36 A high correlation value between an MT evaluation measure and human judgments indicates that the measure is able to evaluate translations in a more similar way to humans. [sent-68, score-0.275]
</p><p>37 Relaxed alignments with the METEOR modules exact, stem and synonym were created for English for the computation of the mNCD score. [sent-69, score-0.128]
</p><p>38 The synonym module was not available with other target languages. [sent-70, score-0.098]
</p><p>39 There is no good way to halt gossip that has already begun to spread. [sent-74, score-0.048]
</p><p>40 There is no e ffect ive means to stop gossip that has already begun to spread. [sent-75, score-0.048]
</p><p>41 Nevertheless, the crisis should not have influenced the entire economy. [sent-87, score-0.053]
</p><p>42 Nevertheless, the crisis should not have Influence the entire economy. [sent-88, score-0.053]
</p><p>43 Perhaps you see the pen you thought you lost lying on your colleague’s desk. [sent-94, score-0.065]
</p><p>44 Perhaps you meet ing the pen you thought you lost lying on your colleague’s desk. [sent-95, score-0.065]
</p><p>45 13  Table 1: Example German–English translations showing the effect of relaxed matching in the 1-mNCD score (for rows S) compared with METEOR using the exact module only, since the modules stem and synonym are already used in the similarized reference. [sent-100, score-0.381]
</p><p>46 The RANK category has human quality rankings of five translations for one sentence from different MT systems. [sent-105, score-0.137]
</p><p>47 The CONST category contains rankings for short phrases (constituents), and the YES/NO category contains binary answers if a short phrase is an acceptable translation or not. [sent-106, score-0.117]
</p><p>48 For the translation tasks into English, the relaxed alignment using a stem module and the synonym module affected 7. [sent-107, score-0.229]
</p><p>49 For NCD we kept the data as is, which we called real casing (rc). [sent-111, score-0.036]
</p><p>50 Since the used METEOR align module lowercases all text, we restored the case information in mNCD by copying the correct case from the reference translation to the similarized reference, based on METEOR’s alignment. [sent-112, score-0.18]
</p><p>51 2 System-level correlation We follow the same evaluation methodology as in Callison-Burch et al. [sent-115, score-0.084]
</p><p>52 (2008), which allows us to measure how well MT evaluation measures correlate with human judgments on the system level. [sent-116, score-0.132]
</p><p>53 From the annotators’ input, the n systems were ranked based on the number of times each system’s output was selected as the best translation divided by the number of times each system was part of a judgment. [sent-118, score-0.036]
</p><p>54 We computed system-level correlations for tasks with English, French, Spanish and German as the target language1 . [sent-119, score-0.092]
</p><p>55 (2010) computed NCD between a set of candidate translations and references at the same time regardless of the sentence alignments, analogously to document comparison. [sent-123, score-0.074]
</p><p>56 We experi-  mented with segmentation of the candidate translations into smaller blocks, which were individually evaluated with NCD and aggregated into a single value with arithmetic mean. [sent-124, score-0.087]
</p><p>57 The resulting system-level correlations between NCD and human judgments are shown in Figure 2 as a function of the block size. [sent-125, score-0.179]
</p><p>58 The correlations are very similar with all block sizes, except for Spanish, where smaller block size produces higher correlation. [sent-126, score-0.157]
</p><p>59 The reported results with mNCD use maximum block size, similar to V ¨ayrynen et al. [sent-128, score-0.039]
</p><p>60 1The English-Spanish news task was left out as most measures had negative correlation with human judgments. [sent-130, score-0.114]
</p><p>61 82  block size in lines  Figure 2: The block size has very little effect on the correlation between NCD and human judgments. [sent-131, score-0.172]
</p><p>62 The right side corresponds to document comparison and the left side to aggregated NCD scores for sentences. [sent-132, score-0.04]
</p><p>63 2 mNCD against NCD Table 2 shows the average system level correlation of different NCD and mNCD variants for trans-  lations into English. [sent-134, score-0.09]
</p><p>64 PPMZ is slower to compute but performs slightly better compared to bz2, except for the  Method  Parameters  KN RA  mNCD  PPMZ  rc  . [sent-136, score-0.127]
</p><p>65 69  NCD  PPMZ  rc  mNCD  bz2  NCD  STN CO  /NOS  YE  na eM  . [sent-137, score-0.127]
</p><p>66 69  Table 2: Mean system level correlations over all translation tasks into English for variants of mNCD and NCD. [sent-168, score-0.138]
</p><p>67 Parameters are the compressor PPMZ or bz2 and  the preprocessing choice lowercasing (lc) or real casing (rc). [sent-170, score-0.06]
</p><p>68 Target Lang Corr Method  Parameters  EN  DE  FR  ES  mNCD  PPMZ  rc  . [sent-171, score-0.127]
</p><p>69 15  Table 3: mNCD versus NCD system correlation RANK results with different parameters (the same as in Table 2) for each target language. [sent-203, score-0.08]
</p><p>70 Target languages DE, FR and ES use only the stem module. [sent-205, score-0.047]
</p><p>71 Table 2 shows that real casing improves RANK correlation slightly throughout NCD and mNCD variants, whereas it reduces correlation in the categories CONST, YES/NO as well as the mean. [sent-207, score-0.181]
</p><p>72 Table 3 shows the correlation results for the RANK category by target language. [sent-213, score-0.101]
</p><p>73 Correlations for other languages show mixed results and on average, mNCD gives lower correlations than NCD. [sent-215, score-0.093]
</p><p>74 3 mNCD versus other methods Table 4 presents the results for the selected mNCD (PPMZ rc) and NCD (bz2 rc) variants along with the correlations for other MT evaluation methods from the WMT’08 data, based on the results in  Callison-Burch et al. [sent-217, score-0.119]
</p><p>75 Although mNCD correlation with human evaluations improved over NCD, the ranking among other measures was not affected. [sent-220, score-0.128]
</p><p>76 Language and task specific results not shown here, reveal very low mNCD and NCD correlations in the Spanish-English news task, which significantly 83  Method DP ULCh DR meteor-ranking ULC posbleu SR posF4gram-gm  KN RA  STN CO  /YEON S  na eM  . [sent-221, score-0.115]
</p><p>77 68  meteor-baseline posF4gram-am mNCD (PPMZ rc) NCD (PPMZ rc) mbleu bleu mter svm-rank  . [sent-253, score-0.078]
</p><p>78 66  Table 4: Average system-level correlations over translation tasks into English for NCD, mNCD and other MT evaluations measures degrades the averages. [sent-289, score-0.149]
</p><p>79 Considering the mean of the categories instead, mNCD’s correlation of . [sent-290, score-0.083]
</p><p>80 The table is shorter since many of the better MT measures use language specific linguistic resources that are not easily available for languages other than English. [sent-293, score-0.034]
</p><p>81 6  Discussion  We have introduced a new MT evaluation measure, mNCD, which is based on normalized compression distance and METEOR’s relaxed alignment modules. [sent-295, score-0.189]
</p><p>82 The mNCD measure outperforms NCD in English with all tested parameter combinations, whereas results with other target languages are unclear. [sent-296, score-0.072]
</p><p>83 The improved correlations with mNCD did not change the position in the RANK category of the MT evaluation measures in the 2008 ACL WMT shared task. [sent-297, score-0.137]
</p><p>84 The improvement in English was expected on the grounds of the synonym module, and indicated also by the larger number of affected words in the Method  Target Lang Corr DE  posbleu posF4gram-am posF4gram-gm bleu  FR  ES  Mean  . [sent-298, score-0.118]
</p><p>85 68  NCD (bz2 rc) svm-rank mbleu mNCD (PPMZ rc) meteor-baseline meteor-ranking mter  . [sent-314, score-0.048]
</p><p>86 65  Table 5: Average system-level correlations for the RANK category from English for NCD, mNCD and other MT evaluation measures. [sent-346, score-0.117]
</p><p>87 We believe there is potential for improvement in other languages as well if synonym lexicons are available. [sent-348, score-0.081]
</p><p>88 We have also extended the basic NCD measure to scale between a document comparison measure and aggregated sentence-level measure. [sent-349, score-0.093]
</p><p>89 The rather surprising result is that NCD produces quite similar scores with all block sizes. [sent-350, score-0.054]
</p><p>90 (2008), we have doubts whether it presents the most effective method exploiting all the given human evaluations in the best way. [sent-353, score-0.041]
</p><p>91 The system-level correlation measure only awards the winner of the ranking of five different systems. [sent-354, score-0.101]
</p><p>92 In addition, the human knowledge that gave the lower rankings is not exploited. [sent-356, score-0.054]
</p><p>93 In future work with mNCD as an MT evaluation measure, we are planning to evaluate synonym dictionaries for other languages than English. [sent-357, score-0.083]
</p><p>94 The synonym module for English does not distinguish between different senses of words. [sent-358, score-0.085]
</p><p>95 Therefore, synonym lexicons found with statistical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006). [sent-359, score-0.096]
</p><p>96 METEOR, M-BLEU and M-TER: evaluation metrics for highcorrelation with human rankings of machine translation output. [sent-362, score-0.107]
</p><p>97 METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. [sent-367, score-0.122]
</p><p>98 Re-evaluating the role of BLEU in machine translation research. [sent-372, score-0.036]
</p><p>99 Packing it all up in search for a language independent MT quality measure tool. [sent-393, score-0.034]
</p><p>100 BLEU: a method for automatic evaluation of machine translation. [sent-406, score-0.028]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ncd', 0.72), ('mncd', 0.528), ('ppmz', 0.216), ('meteor', 0.137), ('rc', 0.127), ('lc', 0.117), ('mt', 0.1), ('similarized', 0.084), ('compression', 0.08), ('correlations', 0.079), ('correlation', 0.067), ('translations', 0.062), ('badger', 0.06), ('crisis', 0.053), ('synonym', 0.052), ('ayrynen', 0.048), ('const', 0.048), ('relaxed', 0.042), ('kimmo', 0.042), ('block', 0.039), ('casing', 0.036), ('posbleu', 0.036), ('translation', 0.036), ('measure', 0.034), ('judgments', 0.034), ('stem', 0.033), ('module', 0.033), ('rank', 0.033), ('kauchak', 0.032), ('nid', 0.032), ('modules', 0.031), ('bleu', 0.03), ('rankings', 0.027), ('normalized', 0.027), ('reference', 0.027), ('human', 0.027), ('aggregated', 0.025), ('compressor', 0.024), ('kolmogorov', 0.024), ('mbleu', 0.024), ('mter', 0.024), ('stn', 0.024), ('tapiovaara', 0.024), ('lavie', 0.023), ('variants', 0.023), ('distance', 0.023), ('string', 0.022), ('category', 0.021), ('jaakko', 0.021), ('kettunen', 0.021), ('cilibrasi', 0.021), ('colleague', 0.021), ('gossip', 0.021), ('tero', 0.021), ('measures', 0.02), ('lost', 0.019), ('aalto', 0.019), ('finland', 0.019), ('lying', 0.018), ('corr', 0.018), ('agarwal', 0.018), ('wmt', 0.018), ('matching', 0.018), ('evaluation', 0.017), ('kn', 0.017), ('pen', 0.017), ('spanish', 0.016), ('assessments', 0.016), ('wer', 0.016), ('concatenation', 0.016), ('mean', 0.016), ('english', 0.015), ('begun', 0.015), ('banerjee', 0.015), ('lexicons', 0.015), ('scores', 0.015), ('fr', 0.015), ('showing', 0.014), ('lang', 0.014), ('alon', 0.014), ('evaluations', 0.014), ('theoretic', 0.014), ('viable', 0.014), ('languages', 0.014), ('chan', 0.013), ('stemming', 0.013), ('target', 0.013), ('already', 0.012), ('alignments', 0.012), ('paraphrasing', 0.012), ('analogously', 0.012), ('box', 0.012), ('acceptable', 0.012), ('barzilay', 0.011), ('es', 0.011), ('thought', 0.011), ('ra', 0.011), ('automatic', 0.011), ('whereas', 0.011), ('correlated', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="104-tfidf-1" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>Author: Marcus Dobrinkat ; Tero Tapiovaara ; Jaakko Vayrynen ; Kimmo Kettunen</p><p>Abstract: This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English.</p><p>2 0.070681274 <a title="104-tfidf-2" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>Author: Hiroshi Echizen-ya ; Kenji Araki</p><p>Abstract: As described in this paper, we propose a new automatic evaluation method for machine translation using noun-phrase chunking. Our method correctly determines the matching words between two sentences using corresponding noun phrases. Moreover, our method determines the similarity between two sentences in terms of the noun-phrase order of appearance. Evaluation experiments were conducted to calculate the correlation among human judgments, along with the scores produced us- ing automatic evaluation methods for MT outputs obtained from the 12 machine translation systems in NTCIR7. Experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and fluency.</p><p>3 0.065917164 <a title="104-tfidf-3" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>4 0.051308796 <a title="104-tfidf-4" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>Author: Xiaojun Wan ; Huiying Li ; Jianguo Xiao</p><p>Abstract: Cross-language document summarization is a task of producing a summary in one language for a document set in a different language. Existing methods simply use machine translation for document translation or summary translation. However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in readability and content. In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process. First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process. Finally, the English sentences with high translation quality and high informativeness are selected and translated to form the Chinese summary. Experimental results demonstrate the effectiveness and usefulness of the proposed approach. 1</p><p>5 0.047193773 <a title="104-tfidf-5" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>Author: Ondrej Bojar ; Kamil Kos ; David Marecek</p><p>Abstract: We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.</p><p>6 0.043727882 <a title="104-tfidf-6" href="./acl-2010-Word_Alignment_with_Synonym_Regularization.html">262 acl-2010-Word Alignment with Synonym Regularization</a></p>
<p>7 0.03930369 <a title="104-tfidf-7" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>8 0.03742671 <a title="104-tfidf-8" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>9 0.036314435 <a title="104-tfidf-9" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>10 0.035176333 <a title="104-tfidf-10" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>11 0.034519114 <a title="104-tfidf-11" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>12 0.034361027 <a title="104-tfidf-12" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>13 0.03081324 <a title="104-tfidf-13" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>14 0.030776935 <a title="104-tfidf-14" href="./acl-2010-Diversify_and_Combine%3A_Improving_Word_Alignment_for_Machine_Translation_on_Low-Resource_Languages.html">90 acl-2010-Diversify and Combine: Improving Word Alignment for Machine Translation on Low-Resource Languages</a></p>
<p>15 0.029439233 <a title="104-tfidf-15" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>16 0.028697638 <a title="104-tfidf-16" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>17 0.026508885 <a title="104-tfidf-17" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>18 0.026467538 <a title="104-tfidf-18" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>19 0.026265137 <a title="104-tfidf-19" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>20 0.025811234 <a title="104-tfidf-20" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.069), (1, -0.041), (2, -0.034), (3, -0.003), (4, 0.01), (5, 0.01), (6, -0.025), (7, -0.028), (8, 0.006), (9, 0.014), (10, 0.043), (11, 0.036), (12, -0.004), (13, -0.011), (14, -0.019), (15, 0.006), (16, 0.027), (17, -0.03), (18, -0.015), (19, 0.009), (20, -0.033), (21, -0.019), (22, 0.041), (23, 0.022), (24, 0.01), (25, -0.033), (26, 0.046), (27, 0.104), (28, -0.019), (29, 0.09), (30, -0.022), (31, -0.027), (32, 0.06), (33, -0.007), (34, -0.066), (35, 0.025), (36, 0.092), (37, -0.071), (38, -0.0), (39, -0.028), (40, -0.081), (41, -0.049), (42, -0.032), (43, 0.024), (44, 0.009), (45, -0.026), (46, 0.022), (47, 0.059), (48, -0.014), (49, 0.119)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89481926 <a title="104-lsi-1" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>Author: Marcus Dobrinkat ; Tero Tapiovaara ; Jaakko Vayrynen ; Kimmo Kettunen</p><p>Abstract: This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English.</p><p>2 0.68112457 <a title="104-lsi-2" href="./acl-2010-TrustRank%3A_Inducing_Trust_in_Automatic_Translations_via_Ranking.html">244 acl-2010-TrustRank: Inducing Trust in Automatic Translations via Ranking</a></p>
<p>Author: Radu Soricut ; Abdessamad Echihabi</p><p>Abstract: The adoption ofMachine Translation technology for commercial applications is hampered by the lack of trust associated with machine-translated output. In this paper, we describe TrustRank, an MT system enhanced with a capability to rank the quality of translation outputs from good to bad. This enables the user to set a quality threshold, granting the user control over the quality of the translations. We quantify the gains we obtain in translation quality, and show that our solution works on a wide variety of domains and language pairs.</p><p>3 0.66501451 <a title="104-lsi-3" href="./acl-2010-Tackling_Sparse_Data_Issue_in_Machine_Translation_Evaluation.html">223 acl-2010-Tackling Sparse Data Issue in Machine Translation Evaluation</a></p>
<p>Author: Ondrej Bojar ; Kamil Kos ; David Marecek</p><p>Abstract: We illustrate and explain problems of n-grams-based machine translation (MT) metrics (e.g. BLEU) when applied to morphologically rich languages such as Czech. A novel metric SemPOS based on the deep-syntactic representation of the sentence tackles the issue and retains the performance for translation to English as well.</p><p>4 0.6442405 <a title="104-lsi-4" href="./acl-2010-Automatic_Evaluation_Method_for_Machine_Translation_Using_Noun-Phrase_Chunking.html">37 acl-2010-Automatic Evaluation Method for Machine Translation Using Noun-Phrase Chunking</a></p>
<p>Author: Hiroshi Echizen-ya ; Kenji Araki</p><p>Abstract: As described in this paper, we propose a new automatic evaluation method for machine translation using noun-phrase chunking. Our method correctly determines the matching words between two sentences using corresponding noun phrases. Moreover, our method determines the similarity between two sentences in terms of the noun-phrase order of appearance. Evaluation experiments were conducted to calculate the correlation among human judgments, along with the scores produced us- ing automatic evaluation methods for MT outputs obtained from the 12 machine translation systems in NTCIR7. Experimental results show that our method obtained the highest correlations among the methods in both sentence-level adequacy and fluency.</p><p>5 0.50704718 <a title="104-lsi-5" href="./acl-2010-Balancing_User_Effort_and_Translation_Error_in_Interactive_Machine_Translation_via_Confidence_Measures.html">45 acl-2010-Balancing User Effort and Translation Error in Interactive Machine Translation via Confidence Measures</a></p>
<p>Author: Jesus Gonzalez Rubio ; Daniel Ortiz Martinez ; Francisco Casacuberta</p><p>Abstract: This work deals with the application of confidence measures within an interactivepredictive machine translation system in order to reduce human effort. If a small loss in translation quality can be tolerated for the sake of efficiency, user effort can be saved by interactively translating only those initial translations which the confidence measure classifies as incorrect. We apply confidence estimation as a way to achieve a balance between user effort savings and final translation error. Empirical results show that our proposal allows to obtain almost perfect translations while significantly reducing user effort.</p><p>6 0.48166311 <a title="104-lsi-6" href="./acl-2010-Bridging_SMT_and_TM_with_Translation_Recommendation.html">56 acl-2010-Bridging SMT and TM with Translation Recommendation</a></p>
<p>7 0.47818244 <a title="104-lsi-7" href="./acl-2010-Bucking_the_Trend%3A_Large-Scale_Cost-Focused_Active_Learning_for_Statistical_Machine_Translation.html">57 acl-2010-Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation</a></p>
<p>8 0.47341064 <a title="104-lsi-8" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>9 0.39482808 <a title="104-lsi-9" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>10 0.36889118 <a title="104-lsi-10" href="./acl-2010-The_Human_Language_Project%3A_Building_a_Universal_Corpus_of_the_World%27s_Languages.html">226 acl-2010-The Human Language Project: Building a Universal Corpus of the World's Languages</a></p>
<p>11 0.36176938 <a title="104-lsi-11" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>12 0.33400819 <a title="104-lsi-12" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>13 0.32361472 <a title="104-lsi-13" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>14 0.31516725 <a title="104-lsi-14" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>15 0.28730446 <a title="104-lsi-15" href="./acl-2010-Tools_for_Multilingual_Grammar-Based_Translation_on_the_Web.html">235 acl-2010-Tools for Multilingual Grammar-Based Translation on the Web</a></p>
<p>16 0.26838687 <a title="104-lsi-16" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>17 0.26830199 <a title="104-lsi-17" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>18 0.2678732 <a title="104-lsi-18" href="./acl-2010-Active_Learning-Based_Elicitation_for_Semi-Supervised_Word_Alignment.html">24 acl-2010-Active Learning-Based Elicitation for Semi-Supervised Word Alignment</a></p>
<p>19 0.26524067 <a title="104-lsi-19" href="./acl-2010-Pseudo-Word_for_Phrase-Based_Machine_Translation.html">201 acl-2010-Pseudo-Word for Phrase-Based Machine Translation</a></p>
<p>20 0.26366669 <a title="104-lsi-20" href="./acl-2010-Intelligent_Selection_of_Language_Model_Training_Data.html">151 acl-2010-Intelligent Selection of Language Model Training Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.02), (25, 0.03), (39, 0.013), (42, 0.016), (44, 0.021), (59, 0.071), (73, 0.032), (78, 0.03), (83, 0.068), (84, 0.025), (90, 0.384), (98, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69464862 <a title="104-lda-1" href="./acl-2010-Evaluating_Machine_Translations_Using_mNCD.html">104 acl-2010-Evaluating Machine Translations Using mNCD</a></p>
<p>Author: Marcus Dobrinkat ; Tero Tapiovaara ; Jaakko Vayrynen ; Kimmo Kettunen</p><p>Abstract: This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English.</p><p>2 0.64996469 <a title="104-lda-2" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>Author: Bharat Ram Ambati</p><p>Abstract: Statistical systems with high accuracy are very useful in real-world applications. If these systems can capture basic linguistic information, then the usefulness of these statistical systems improve a lot. This paper is an attempt at incorporating linguistic constraints in statistical dependency parsing. We consider a simple linguistic constraint that a verb should not have multiple subjects/objects as its children in the dependency tree. We first describe the importance of this constraint considering Machine Translation systems which use dependency parser output, as an example application. We then show how the current state-ofthe-art dependency parsers violate this constraint. We present two new methods to handle this constraint. We evaluate our methods on the state-of-the-art dependency parsers for Hindi and Czech. 1</p><p>3 0.49967611 <a title="104-lda-3" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>Author: Tong Xiao ; Jingbo Zhu ; Muhua Zhu ; Huizhen Wang</p><p>Abstract: In this paper, we present a simple and effective method to address the issue of how to generate diversified translation systems from a single Statistical Machine Translation (SMT) engine for system combination. Our method is based on the framework of boosting. First, a sequence of weak translation systems is generated from a baseline system in an iterative manner. Then, a strong translation system is built from the ensemble of these weak translation systems. To adapt boosting to SMT system combination, several key components of the original boosting algorithms are redesigned in this work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1</p><p>4 0.39442781 <a title="104-lda-4" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>Author: Fei Huang ; Alexander Yates</p><p>Abstract: Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data. Semantic role labeling techniques are typically trained on newswire text, and in tests their performance on fiction is as much as 19% worse than their performance on newswire text. We investigate techniques for building open-domain semantic role labeling systems that approach the ideal of a train-once, use-anywhere system. We leverage recently-developed techniques for learning representations of text using latent-variable language models, and extend these techniques to ones that provide the kinds of features that are useful for semantic role labeling. In experiments, our novel system reduces error by 16% relative to the previous state of the art on out-of-domain text.</p><p>5 0.39398235 <a title="104-lda-5" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>Author: Jason Riesa ; Daniel Marcu</p><p>Abstract: We present a simple yet powerful hierarchical search algorithm for automatic word alignment. Our algorithm induces a forest of alignments from which we can efficiently extract a ranked k-best list. We score a given alignment within the forest with a flexible, linear discriminative model incorporating hundreds of features, and trained on a relatively small amount of annotated data. We report results on Arabic-English word alignment and translation tasks. Our model outperforms a GIZA++ Model-4 baseline by 6.3 points in F-measure, yielding a 1.1 BLEU score increase over a state-of-the-art syntax-based machine translation system.</p><p>6 0.39386946 <a title="104-lda-6" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>7 0.39347911 <a title="104-lda-7" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>8 0.39283794 <a title="104-lda-8" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>9 0.39185619 <a title="104-lda-9" href="./acl-2010-Cross-Lingual_Latent_Topic_Extraction.html">79 acl-2010-Cross-Lingual Latent Topic Extraction</a></p>
<p>10 0.39183164 <a title="104-lda-10" href="./acl-2010-Finding_Cognate_Groups_Using_Phylogenies.html">116 acl-2010-Finding Cognate Groups Using Phylogenies</a></p>
<p>11 0.39049351 <a title="104-lda-11" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>12 0.39026302 <a title="104-lda-12" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>13 0.38996729 <a title="104-lda-13" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>14 0.38960528 <a title="104-lda-14" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>15 0.38941151 <a title="104-lda-15" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>16 0.38917172 <a title="104-lda-16" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>17 0.38903832 <a title="104-lda-17" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>18 0.38887405 <a title="104-lda-18" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>19 0.38879657 <a title="104-lda-19" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>20 0.38863415 <a title="104-lda-20" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
