<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 acl-2010-Classification of Feedback Expressions in Multimodal Data</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-58" href="#">acl2010-58</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 acl-2010-Classification of Feedback Expressions in Multimodal Data</h1>
<br/><p>Source: <a title="acl-2010-58-pdf" href="http://aclweb.org/anthology//P/P10/P10-2059.pdf">pdf</a></p><p>Author: Costanza Navarretta ; Patrizia Paggio</p><p>Abstract: This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback.</p><p>Reference: <a title="acl-2010-58-reference" href="../acl2010_reference/acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 dk Abstract This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i. [sent-3, score-0.495]
</p><p>2 head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. [sent-5, score-0.323]
</p><p>3 The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback. [sent-6, score-1.303]
</p><p>4 1 Introduction Several authors in communication studies have  pointed out that head movements are relevant to feedback phenomena (see McClave (2000) for an overview). [sent-7, score-0.53]
</p><p>5 Others have looked at the application of machine learning algorithms to annotated multimodal corpora. [sent-8, score-0.183]
</p><p>6 (2008) find that machine learning algorithms can be trained to recognise some of the functions of head movements, while Reidsma et al. [sent-10, score-0.154]
</p><p>7 (2009) show that there is a dependence between focus of attention and assignment of dialogue act labels. [sent-11, score-0.302]
</p><p>8 Related are also the studies by Rieks op den Akker and Schulz (2008) and Murray and Renals (2008): both achieve promising results in the automatic segmentation of dialogue acts using the annotations in a large multimodal corpus. [sent-12, score-0.491]
</p><p>9 Work has also been done on prosody and gestures in the specific domain of map-task dialogues, also targeted in this paper. [sent-13, score-0.506]
</p><p>10 (2006) and Louwerse Patrizia Paggio University of Copenhagen Centre for Language Technology (CST) Njalsgade 140, 2300-DK Copenhagen paggio @ hum . [sent-17, score-0.142]
</p><p>11 (2007) study the relation between eye gaze, facial expression, pauses and dialogue structure in annotated English map-task dialogues (Anderson et al. [sent-20, score-0.359]
</p><p>12 Finally, feedback expressions (head nods and shakes) are successfully predicted from speech, prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication (Fujie et al. [sent-22, score-0.717]
</p><p>13 Our work is in line with these studies, all of which focus on the relation between linguistic expressions, prosody, dialogue content and ges-  tures. [sent-27, score-0.2]
</p><p>14 In this paper, we investigate how feedback expressions can be classified into different dialogue act categories based on prosodic and gesture features. [sent-28, score-1.355]
</p><p>15 Our data are made up by a collection of eight video-recorded map-task dialogues in Danish, which were annotated with phonetic and prosodic information. [sent-29, score-0.384]
</p><p>16 We find that prosodic features improve the classification of dialogue acts and that head gestures, where they occur, contribute to the semantic interpretation of feedback expressions. [sent-30, score-0.92]
</p><p>17 The results, which partly confirm those obtained on a smaller dataset in Paggio and Navarretta (2010), must be seen in light of the fact that our gesture annotation scheme comprises more fine-grained categories than most ofthe studies mentioned earlier for both head movements and face expressions. [sent-31, score-0.815]
</p><p>18 The classification results improve, however, if similar categories such as head nods and jerks are collapsed into a more general category. [sent-32, score-0.416]
</p><p>19 In Section 2 we describe the multimodal Danish corpus. [sent-33, score-0.145]
</p><p>20 In Section 3, we describe how the prosody offeedback expressions is annotated, how their content is coded in terms of dialogue act, turn  and agreement labels, and we provide inter-coder agreement measures. [sent-34, score-0.828]
</p><p>21 In Section 4 we account for the annotation of head gestures, including inter318  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-35, score-0.226]
</p><p>22 Section 5 contains a description of the resulting datasets and a discussion of the results obtained in the classification experiments. [sent-38, score-0.049]
</p><p>23 2  The multimodal corpus  The Danish map-task dialogues from the DanPASS corpus (Grønnum, 2006) are a collection of dialogues in which 11 speaker pairs cooperate on a map task. [sent-40, score-0.301]
</p><p>24 The dialogue participants are seated in different rooms and cannot see each other. [sent-41, score-0.2]
</p><p>25 They talk through headsets, and one of them is recorded with a video camera. [sent-42, score-0.071]
</p><p>26 The material is transcribed orthographically with an indication  of stress, articulatory hesitations and pauses. [sent-44, score-0.043]
</p><p>27 In addition to this, the acoustic signals are segmented into words, syllables and prosodic phrases, and annotated with POS-tags, phonological and phonetic transcriptions, pitch and intonation contours. [sent-45, score-0.306]
</p><p>28 Phonetic and prosodic segmentation and annotation were performed independently and in parallel by two annotators and then an agreed upon version was produced with the supervision of an expert annotator, for more information see Grønnum (2006). [sent-46, score-0.393]
</p><p>29 The feedback expressions we analyse here are Yes and No expressions, i. [sent-48, score-0.401]
</p><p>30 Yes and No feedback expressions represent about 9% of the approximately 47,000 running words in the corpus. [sent-52, score-0.401]
</p><p>31 This is a rather high proportion compared to other corpora, both spoken and written, and a reason why we decided to use the DanPASS videos in spite of the fact that the  gesture behaviour is relatively limited given the fact that the two dialogue participants cannot see each other. [sent-53, score-0.619]
</p><p>32 Furthermore, the restricted contexts in which feedback expressions occur in these dialogues allow for a very fine-grained analysis of the relation of these expressions with prosody and gestures. [sent-54, score-0.783]
</p><p>33 3  Annotation of feedback expressions  As already mentioned, all words in DanPASS are phonetically and prosodically annotated. [sent-58, score-0.401]
</p><p>34 In the subset of the corpus considered here, 82% of the feedback expressions bear stress or tone information, and 12% are unstressed; 7% of them are  marked with onset or offset hesitation, or both. [sent-59, score-0.531]
</p><p>35 For this study, we added semantic labels including dialogue acts and gesture annotation. [sent-60, score-0.647]
</p><p>36 Both kinds of annotation were carried out using ANVIL (Kipp, 2004). [sent-61, score-0.072]
</p><p>37 To distinguish among the various functions that feedback expressions have in the dialogues, we selected a subset of the categories defined in the emerging ISO 24617-2 standard for semantic annotation of language resources. [sent-62, score-0.512]
</p><p>38 Moreover, all feedback expressions were annotated with an agreement feature (Agree, NonAgree) where relevant. [sent-64, score-0.551]
</p><p>39 Finally, the two turn management categories TurnTake and TurnElicit were also coded. [sent-65, score-0.08]
</p><p>40 –  –  It should be noted that the same expression may be annotated with a label for each of the three semantic dimensions. [sent-66, score-0.08]
</p><p>41 For example, a yes can be an Answer to a question, an Agree and a TurnElicit at the same time, thus making the semantic classification very fine-grained. [sent-67, score-0.164]
</p><p>42 Table 1 shows how the various types are distributed across the 466 feedback expressions in our data. [sent-68, score-0.401]
</p><p>43 1 Inter-coder agreement on feedback expression annotation In general, dialogue act, agreement and turn annotations were coded by an expert annotator and the annotations were subsequently checked by a second expert annotator. [sent-70, score-1.072]
</p><p>44 However, one dialogue was coded independently and in parallel by two expert annotators to measure inter-coder agreement. [sent-71, score-0.348]
</p><p>45 A measure was derived for each annotated feature  using the agreement analysis facility provided in ANVIL. [sent-72, score-0.15]
</p><p>46 Agreement between two annotation sets is calculated here in terms of Cohen’s kappa (Cohen, 1960)1 and corrected kappa (Brennan and Prediger, 1981)2. [sent-73, score-0.154]
</p><p>47 Anvil divides the annotations in slices and compares each slice. [sent-74, score-0.081]
</p><p>48 The inter-coder agreement figures obtained for the three types of annotation are given in Table 2. [sent-77, score-0.184]
</p><p>49 Looking at the cases of disagreement we could see that many of these are due to the fact that the annotators had forgotten to remove some of the features automatically proposed by ANVIL from the latest annotated element. [sent-81, score-0.079]
</p><p>50 4  Gesture annotation  All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme (Allwood et al. [sent-82, score-0.856]
</p><p>51 The MUMIN scheme is a general framework for the study of gestures in interpersonal communication. [sent-84, score-0.384]
</p><p>52 In this study, we do not deal with functional classification of the gestures in themselves, but rather 1(Pa − Pe)/(1 − Pe). [sent-85, score-0.433]
</p><p>53 with how gestures contribute to the semantic interpretations of linguistic expressions. [sent-87, score-0.384]
</p><p>54 Therefore,  only a subset of the MUMIN attributes has been used, i. [sent-88, score-0.049]
</p><p>55 Smile, Laughter, Scowl, FaceOther for facial expressions, and Nod, Jerk, Tilt, SideTurn, Shake, Waggle, Other for head movements. [sent-90, score-0.197]
</p><p>56 A link was also established in ANVIL between the gesture under consideration and the relevant speech sequence where appropriate. [sent-91, score-0.381]
</p><p>57 The link was then used to extract gesture information together with the relevant linguistic annotations on which to apply machine learning. [sent-92, score-0.424]
</p><p>58 The total number of head gestures annotated is 264. [sent-93, score-0.576]
</p><p>59 Of these, 114 (43%) co-occur with feedback expressions, with Nod as by far the most frequent type (70 occurrences) followed by FaceOther as the second most frequent (16). [sent-94, score-0.219]
</p><p>60 The remaining 150 gestures, linked to different linguistic expressions or to no expression at all, comprise many face expressions and a number of tilts. [sent-96, score-0.476]
</p><p>61 The annotations of this video were then used to measure inter-coder agreement in ANVIL as it was the case for the annotations on feedback expressions. [sent-101, score-0.488]
</p><p>62 In the case of gestures we also measured agreement on gesture segmentation. [sent-102, score-0.877]
</p><p>63 r73a2d45ecgtdsukre annotation  These results are slightly worse than those obtained in previous studies using the same annotation scheme (Jokinen et al. [sent-106, score-0.144]
</p><p>64 A distinction that seemed particularly difficult was that between nods and jerks: although the direction of the two movement types is different (down-up and up-down, respectively), the movement quality is very similar, and makes it difficult to see the direction clearly. [sent-108, score-0.231]
</p><p>65 These two sets of data were used for automatic dialogue act classification, which was run in the Weka system (Witten and Frank, 2005). [sent-111, score-0.302]
</p><p>66 In the first group of experiments we took into consideration all the Yes and No expressions (420 Yes and 46 No) without, however, considering ges-  ture information. [sent-117, score-0.182]
</p><p>67 The purpose was to see how prosodic information contributes to the classification of dialogue acts. [sent-118, score-0.481]
</p><p>68 only the orthographic transcription (Yes and No expressions) was considered; then we included information about stress (stressed or unstressed); in the third run we added tone attributes, and in the fourth information on hesitation. [sent-121, score-0.13]
</p><p>69 Agreement and turn attributes were used in all experiments, while Dialogue act annotation was only used in the training phase. [sent-122, score-0.264]
</p><p>70 4a513tures The results indicate that prosodic information improves the classification of dialogue acts with respect to the baseline in all four experiments with improvements of 10, 10. [sent-129, score-0.547]
</p><p>71 The best results are obtained using information on stress and tone, although the decrease in accuracy when hesitations are introduced is not significant. [sent-133, score-0.114]
</p><p>72 This result if not surprising since the former type is much more frequent in the data than the latter, and since prosodic information does not correlate  with RepeatRephrase in any systematic way. [sent-135, score-0.232]
</p><p>73 The second group of experiments was conducted on the dataset where feedback expressions are accompanied by gestures (102 Yes and 12 No). [sent-136, score-0.82]
</p><p>74 The purpose this time was to see whether gesture information improves dialogue act classification. [sent-137, score-0.683]
</p><p>75 We believe it makes sense to perform the test based on this restricted dataset, rather than the entire material, because the portion of data where gestures do accompany feedback expressions is rather small (about 20%). [sent-138, score-0.785]
</p><p>76 In a different domain, where subjects are less constrained by the technical setting, we expect gestures would make for a stronger and more widespread effect. [sent-139, score-0.384]
</p><p>77 For these experiments, however, we used as a baseline the results obtained based on stress, tone and hesitation information, the combination that gave the best results on the larger 321  dataset. [sent-144, score-0.097]
</p><p>78 Together with the prosodic information, Agreement and turn attributes were included just  as earlier, while the dialogue act annotation was only used in the training phase. [sent-145, score-0.696]
</p><p>79 Face expression and head movement attributes were disregarded in the baseline. [sent-146, score-0.317]
</p><p>80 We then added face expression alone, head movement alone, and finally both gesture types together. [sent-147, score-0.719]
</p><p>81 s9324ture features  These results indicate that adding head gesture information improves the classification of dialogue acts in this reduced dataset, although the improvement is not impressive. [sent-152, score-0.85]
</p><p>82 The best results are achieved when both face expressions and head movements are taken into consideration. [sent-153, score-0.505]
</p><p>83 We already explained that in our annotation a large number of feedback utterances have an agreement or turn label without necessarily having been assigned to one of our task-related dialogue act categories. [sent-155, score-0.783]
</p><p>84 This means that head gestures help distinguishing utterances with an agreement or turn function from other kinds. [sent-156, score-0.728]
</p><p>85 Looking closer at these utterances, we can see that nods and jerks often occur together with TurnElicit, while tilts, side turns and smiles tend to occur with Agree. [sent-157, score-0.174]
</p><p>86 An issue that worries us is the granularity of the annotation categories. [sent-158, score-0.072]
</p><p>87 To investigate this, in a third group of experiments we collapsed Nod and Jerk into a more general category: the distinction had proven difficult for the annotators, and we don’t have many jerks in the data. [sent-159, score-0.087]
</p><p>88 6  Conclusion  In this study we have experimented with the automatic classification of feedback expressions into different dialogue acts in a multimodal corpus of  Tab+dYlfheatsc7Naesd:+omthCeladsmifcaH AtiN lognB resuP45l731t. [sent-162, score-0.861]
</p><p>89 The results indicate that prosodic features improve the classification, and that in those cases where feedback expressions are accompanied by head ges-  tures, gesture information is also useful. [sent-167, score-1.203]
</p><p>90 The results also show that using a more coarse-grained distinction of head movements improves classification in these data. [sent-168, score-0.302]
</p><p>91 Slightly more than half of the head gestures in our data co-occur with other linguistic utterances than those targeted in this study. [sent-169, score-0.575]
</p><p>92 The occurrence of gestures in the data studied here is undoubtedly limited by the technical setup, since the two speakers do not see each other. [sent-171, score-0.384]
</p><p>93 Therefore, we want to investigate the role played by head gestures in other types of video and larger materials. [sent-172, score-0.609]
</p><p>94 Extending the analysis to larger datasets will also shed more light on whether our gesture annotation categories are too fine-grained for automatic classification. [sent-173, score-0.492]
</p><p>95 We would also like to thank Nina Grønnum for allowing us to use the DanPASS corpus, and our gesture annotators Josephine Bødker Arrild and Sara Andersen. [sent-175, score-0.422]
</p><p>96 A conversation robot using head gesture recognition as para-linguistic information. [sent-215, score-0.535]
</p><p>97 Clustering experiments on the communicative prop- erties of gaze and gestures. [sent-234, score-0.098]
</p><p>98 Linguistic functions of head movements in the context of speech. [sent-267, score-0.253]
</p><p>99 Head gestures for perceptual interfaces: The role of context in improving recognition. [sent-275, score-0.384]
</p><p>100 Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. [sent-301, score-0.38]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gestures', 0.384), ('gesture', 0.381), ('prosodic', 0.232), ('feedback', 0.219), ('dialogue', 0.2), ('expressions', 0.182), ('head', 0.154), ('anvil', 0.152), ('danpass', 0.152), ('multimodal', 0.145), ('jokinen', 0.13), ('morency', 0.13), ('prosody', 0.122), ('yes', 0.115), ('agreement', 0.112), ('paggio', 0.109), ('patrizia', 0.109), ('act', 0.102), ('movements', 0.099), ('danish', 0.095), ('costanza', 0.087), ('jerks', 0.087), ('mumin', 0.087), ('navarretta', 0.087), ('nods', 0.087), ('dialogues', 0.078), ('copenhagen', 0.077), ('louwerse', 0.076), ('movement', 0.072), ('annotation', 0.072), ('stress', 0.071), ('video', 0.071), ('face', 0.07), ('acts', 0.066), ('gr', 0.065), ('kristiina', 0.065), ('nnum', 0.065), ('rieks', 0.065), ('turnelicit', 0.065), ('coded', 0.059), ('tone', 0.059), ('communication', 0.058), ('mlmi', 0.057), ('nod', 0.057), ('weka', 0.051), ('attributes', 0.049), ('gaze', 0.049), ('communicative', 0.049), ('classification', 0.049), ('expert', 0.048), ('cues', 0.046), ('akker', 0.043), ('allwood', 0.043), ('boersma', 0.043), ('dirk', 0.043), ('faceother', 0.043), ('facial', 0.043), ('fujie', 0.043), ('gwineth', 0.043), ('hesitations', 0.043), ('heylen', 0.043), ('hoque', 0.043), ('jerk', 0.043), ('jeuniaux', 0.043), ('kipp', 0.043), ('miyake', 0.043), ('njalsgade', 0.043), ('praat', 0.043), ('repeatrephrase', 0.043), ('sridhar', 0.043), ('utrecht', 0.043), ('weenink', 0.043), ('zeror', 0.043), ('annotations', 0.043), ('expression', 0.042), ('annotators', 0.041), ('turn', 0.041), ('kappa', 0.041), ('categories', 0.039), ('videos', 0.038), ('brennan', 0.038), ('hesitation', 0.038), ('murray', 0.038), ('reidsma', 0.038), ('slices', 0.038), ('unstressed', 0.038), ('annotated', 0.038), ('op', 0.037), ('utterances', 0.037), ('phonetic', 0.036), ('accompanied', 0.035), ('follower', 0.035), ('mahwah', 0.035), ('artstein', 0.035), ('cst', 0.035), ('gravano', 0.035), ('nina', 0.035), ('annotator', 0.033), ('candace', 0.033), ('hum', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="58-tfidf-1" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>Author: Costanza Navarretta ; Patrizia Paggio</p><p>Abstract: This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback.</p><p>2 0.11093095 <a title="58-tfidf-2" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>Author: Pierre Lison</p><p>Abstract: Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions ofthe state and action spaces.</p><p>3 0.10995405 <a title="58-tfidf-3" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>4 0.10097852 <a title="58-tfidf-4" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>Author: Katrin Tomanek ; Udo Hahn ; Steffen Lohmann ; Jurgen Ziegler</p><p>Abstract: We report on an experiment to track complex decision points in linguistic metadata annotation where the decision behavior of annotators is observed with an eyetracking device. As experimental conditions we investigate different forms of textual context and linguistic complexity classes relative to syntax and semantics. Our data renders evidence that annotation performance depends on the semantic and syntactic complexity of the decision points and, more interestingly, indicates that fullscale context is mostly negligible with – the exception of semantic high-complexity cases. We then induce from this observational data a cognitively grounded cost model of linguistic meta-data annotations and compare it with existing non-cognitive models. Our data reveals that the cognitively founded model explains annotation costs (expressed in annotation time) more adequately than non-cognitive ones.</p><p>5 0.094757639 <a title="58-tfidf-5" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>Author: Ryu Iida ; Syumpei Kobayashi ; Takenobu Tokunaga</p><p>Abstract: This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001 ; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on refer- ence resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.</p><p>6 0.093260892 <a title="58-tfidf-6" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>7 0.084521085 <a title="58-tfidf-7" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>8 0.082179718 <a title="58-tfidf-8" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>9 0.078276053 <a title="58-tfidf-9" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>10 0.077551328 <a title="58-tfidf-10" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>11 0.077394515 <a title="58-tfidf-11" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>12 0.075049408 <a title="58-tfidf-12" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>13 0.073723115 <a title="58-tfidf-13" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>14 0.073300265 <a title="58-tfidf-14" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>15 0.073022515 <a title="58-tfidf-15" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>16 0.07166671 <a title="58-tfidf-16" href="./acl-2010-Multilingual_Pseudo-Relevance_Feedback%3A_Performance_Study_of_Assisting_Languages.html">177 acl-2010-Multilingual Pseudo-Relevance Feedback: Performance Study of Assisting Languages</a></p>
<p>17 0.066197976 <a title="58-tfidf-17" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>18 0.052543603 <a title="58-tfidf-18" href="./acl-2010-Combining_Data_and_Mathematical_Models_of_Language_Change.html">61 acl-2010-Combining Data and Mathematical Models of Language Change</a></p>
<p>19 0.051432688 <a title="58-tfidf-19" href="./acl-2010-Annotation.html">31 acl-2010-Annotation</a></p>
<p>20 0.050316084 <a title="58-tfidf-20" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.128), (1, 0.08), (2, -0.043), (3, -0.127), (4, -0.047), (5, -0.115), (6, -0.117), (7, 0.073), (8, -0.01), (9, 0.045), (10, 0.023), (11, -0.015), (12, 0.01), (13, 0.04), (14, 0.028), (15, -0.058), (16, 0.025), (17, 0.039), (18, 0.053), (19, 0.04), (20, -0.0), (21, 0.009), (22, 0.079), (23, 0.007), (24, -0.046), (25, -0.024), (26, 0.052), (27, 0.019), (28, 0.068), (29, -0.079), (30, -0.106), (31, 0.124), (32, -0.009), (33, -0.014), (34, -0.006), (35, -0.028), (36, 0.053), (37, -0.009), (38, -0.01), (39, 0.091), (40, 0.112), (41, -0.057), (42, -0.067), (43, -0.045), (44, 0.074), (45, 0.13), (46, -0.053), (47, -0.009), (48, 0.031), (49, -0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94871932 <a title="58-lsi-1" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>Author: Costanza Navarretta ; Patrizia Paggio</p><p>Abstract: This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback.</p><p>2 0.74595839 <a title="58-lsi-2" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>Author: Brian Pluss</p><p>Abstract: This paper presents ongoing research on computational models for non-cooperative dialogue. We start by analysing different levels of cooperation in conversation. Then, inspired by findings from an empirical study, we propose a technique for measuring non-cooperation in political interviews. Finally, we describe a research programme towards obtaining a suitable model and discuss previous accounts for conflictive dialogue, identifying the differences with our work.</p><p>3 0.69811523 <a title="58-lsi-3" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>Author: Trung H. Bui ; Stanley Peters</p><p>Abstract: We investigate hierarchical graphical models (HGMs) for automatically detecting decisions in multi-party discussions. Several types of dialogue act (DA) are distinguished on the basis of their roles in formulating decisions. HGMs enable us to model dependencies between observed features of discussions, decision DAs, and subdialogues that result in a decision. For the task of detecting decision regions, an HGM classifier was found to outperform non-hierarchical graphical models and support vector machines, raising the F1-score to 0.80 from 0.55.</p><p>4 0.61906695 <a title="58-lsi-4" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>Author: Francois Mairesse ; Milica Gasic ; Filip Jurcicek ; Simon Keizer ; Blaise Thomson ; Kai Yu ; Steve Young</p><p>Abstract: Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation perfor- mance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.</p><p>5 0.61118984 <a title="58-lsi-5" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>Author: Jessica Villing</p><p>Abstract: In-vehicle dialogue systems often contain more than one application, e.g. a navigation and a telephone application. This means that the user might, for example, interrupt the interaction with the telephone application to ask for directions from the navigation application, and then resume the dialogue with the telephone application. In this paper we present an analysis of interruption and resumption behaviour in human-human in-vehicle dialogues and also propose some implications for resumption strategies in an in-vehicle dialogue system.</p><p>6 0.60905486 <a title="58-lsi-6" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>7 0.55579251 <a title="58-lsi-7" href="./acl-2010-Modeling_Norms_of_Turn-Taking_in_Multi-Party_Conversation.html">173 acl-2010-Modeling Norms of Turn-Taking in Multi-Party Conversation</a></p>
<p>8 0.48767146 <a title="58-lsi-8" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>9 0.47909981 <a title="58-lsi-9" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>10 0.47262251 <a title="58-lsi-10" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>11 0.4679229 <a title="58-lsi-11" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>12 0.46006209 <a title="58-lsi-12" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>13 0.44950107 <a title="58-lsi-13" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>14 0.44102696 <a title="58-lsi-14" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>15 0.42456627 <a title="58-lsi-15" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>16 0.41594461 <a title="58-lsi-16" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>17 0.41353729 <a title="58-lsi-17" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>18 0.41269886 <a title="58-lsi-18" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>19 0.38983744 <a title="58-lsi-19" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>20 0.38434434 <a title="58-lsi-20" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.038), (35, 0.429), (42, 0.073), (59, 0.065), (71, 0.014), (73, 0.024), (78, 0.025), (80, 0.021), (83, 0.099), (84, 0.028), (98, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76165432 <a title="58-lda-1" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>Author: Costanza Navarretta ; Patrizia Paggio</p><p>Abstract: This paper addresses the issue of how linguistic feedback expressions, prosody and head gestures, i.e. head movements and face expressions, relate to one another in a collection of eight video-recorded Danish map-task dialogues. The study shows that in these data, prosodic features and head gestures significantly improve automatic classification of dialogue act labels for linguistic expressions of feedback.</p><p>2 0.64256555 <a title="58-lda-2" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>Author: Vipul Mittal</p><p>Abstract: In this paper, we propose a novel method for automatic segmentation of a Sanskrit string into different words. The input for our segmentizer is a Sanskrit string either encoded as a Unicode string or as a Roman transliterated string and the output is a set of possible splits with weights associated with each of them. We followed two different approaches to segment a Sanskrit text using sandhi1 rules extracted from a parallel corpus of manually sandhi split text. While the first approach augments the finite state transducer used to analyze Sanskrit morphology and traverse it to segment a word, the second approach generates all possible segmentations and validates each constituent using a morph an- alyzer.</p><p>3 0.5309577 <a title="58-lda-3" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>Author: Wolfgang Seeker ; Ines Rehbein ; Jonas Kuhn ; Josef Van Genabith</p><p>Abstract: For languages with (semi-) free word order (such as German), labelling grammatical functions on top of phrase-structural constituent analyses is crucial for making them interpretable. Unfortunately, most statistical classifiers consider only local information for function labelling and fail to capture important restrictions on the distribution of core argument functions such as subject, object etc., namely that there is at most one subject (etc.) per clause. We augment a statistical classifier with an integer linear program imposing hard linguistic constraints on the solution space output by the classifier, capturing global distributional restrictions. We show that this improves labelling quality, in particular for argument grammatical functions, in an intrinsic evaluation, and, importantly, grammar coverage for treebankbased (Lexical-Functional) grammar acquisition and parsing, in an extrinsic evaluation.</p><p>4 0.53033996 <a title="58-lda-4" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>Author: Joseph Turian ; Lev-Arie Ratinov ; Yoshua Bengio</p><p>Abstract: If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http ://metaoptimize com/proj ects/wordreprs/ .</p><p>5 0.35082993 <a title="58-lda-5" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynam- ically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.</p><p>6 0.34990048 <a title="58-lda-6" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>7 0.34886098 <a title="58-lda-7" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>8 0.34624767 <a title="58-lda-8" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>9 0.34402964 <a title="58-lda-9" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>10 0.34264424 <a title="58-lda-10" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>11 0.34244341 <a title="58-lda-11" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>12 0.34127039 <a title="58-lda-12" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>13 0.33838218 <a title="58-lda-13" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>14 0.3383323 <a title="58-lda-14" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>15 0.33739775 <a title="58-lda-15" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>16 0.3358075 <a title="58-lda-16" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>17 0.33496457 <a title="58-lda-17" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>18 0.33351564 <a title="58-lda-18" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>19 0.33165231 <a title="58-lda-19" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>20 0.33004114 <a title="58-lda-20" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
