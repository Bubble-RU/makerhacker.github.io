<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-241" href="#">acl2010-241</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</h1>
<br/><p>Source: <a title="acl-2010-241-pdf" href="http://aclweb.org/anthology//P/P10/P10-3010.pdf">pdf</a></p><p>Author: Martin Haulrich</p><p>Abstract: We show that using confidence-weighted classification in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. We also compare with other online learning algorithms and investigate the effect of pruning features when using confidenceweighted classification.</p><p>Reference: <a title="acl-2010-241-reference" href="../acl2010_reference/acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Transition-based parsing with Confidence-Weighted Classification Martin Haulrich Dept. [sent-1, score-0.143]
</p><p>2 dk  Abstract We show that using confidence-weighted classification in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. [sent-4, score-0.489]
</p><p>3 We also compare with other online learning algorithms and investigate the effect of pruning features when using confidenceweighted classification. [sent-5, score-0.486]
</p><p>4 1 Introduction There has been a lot of work on data-driven dependency parsing. [sent-6, score-0.119]
</p><p>5 , 2005a; McDonald and Pereira, 2006) and for transition-based parsing Support-Vector Machines (Hall et al. [sent-16, score-0.143]
</p><p>6 (2008) introduce a new approach to margin-based online learning called confidenceweighted classification (CW) and show that the performance of this approach is comparable to that of Support-Vector Machines. [sent-20, score-0.397]
</p><p>7 In this work we use confidence-weighted classification with transition-based parsing and show that this leads to results comparable to the state-of-the-art results obtained using SVMs. [sent-21, score-0.291]
</p><p>8 We also compare training time and the effect of pruning when using confidence-weighted learning. [sent-22, score-0.178]
</p><p>9 2  Transition-based parsing  Transition-based parsing builds on the idea that parsing can be viewed as a sequence of transitions between states. [sent-23, score-0.544]
</p><p>10 A classifier The focus here is on the classifier but we will briefly describe the parsing algorithm in order to understand the classification task better. [sent-27, score-0.344]
</p><p>11 The parsing algorithm consists of two components, a transition system and an oracle. [sent-28, score-0.198]
</p><p>12 Nivre (2008) defines a transition system S = (C, T, cs, Ct) in the following way: 1. [sent-29, score-0.055]
</p><p>13 C is a set of configurations, each of which contains a buffer β of (remaining) nodes and a set A of dependency arcs, 2. [sent-30, score-0.277]
</p><p>14 A transition sequence for a sentence x in S is a sequence C0,m = (c0, c1 . [sent-40, score-0.055]
</p><p>15 for every i(1 ≤ i≤ m)ci = t(ci−1) for some fto ∈ eTve The oracle is used during training to determine a transition sequence that leads to the correct parse. [sent-46, score-0.22]
</p><p>16 The job of the classifier is to ’imitate’ the oracle, i. [sent-47, score-0.065]
</p><p>17 to try to always pick the transitions that 55  UppsaPlra,o Scewe didnegn,s o 1f3 t Jhuely AC 20L10 20. [sent-49, score-0.155]
</p><p>18 The information given to the classifier is the current configuration. [sent-52, score-0.065]
</p><p>19 Therefore the training data for the classifier consists of a number of configurations and the transitions the  oracle chose with these configurations. [sent-53, score-0.36]
</p><p>20 σ  is a stack of tokens i≤ k (for some k ≤ n),  2. [sent-59, score-0.093]
</p><p>21 A is a set of dependency arcs such that G = (0, 1, . [sent-61, score-0.166]
</p><p>22 (Nivre, 2008) In the work presented here we use the NivreEager algorithm which has four transitions: Shift Push the token at the head of the buffer onto the stack. [sent-65, score-0.336]
</p><p>23 Left-Arcl Add to the analysis an arc with label l from the token at the head ofthe buffer to the token on the top of the stack, and push the buffer-token  onto the stack. [sent-67, score-0.524]
</p><p>24 Right-Arcl Add to the analysis an arc with label lfrom the token on the top of the stack to the token at the head of the buffer, and pop the stack. [sent-68, score-0.427]
</p><p>25 1 Classification Transition-based dependency parsing reduces parsing to consecutive multiclass classification. [sent-70, score-0.458]
</p><p>26 From each configuration one amongst some predefined number of transitions has to be chosen. [sent-71, score-0.115]
</p><p>27 This means that any classifier can be plugged into the system. [sent-72, score-0.065]
</p><p>28 The training instances are created by the oracle so the training is offline. [sent-73, score-0.172]
</p><p>29 So even though we use online learners in the experiments these are used in a batch setting. [sent-74, score-0.121]
</p><p>30 The best results have been achieved using Support-Vector Machines placing the MaltParser very high in both the CoNNL shared tasks on dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al. [sent-75, score-0.262]
</p><p>31 The standard setting in the MaltParser is to use a 2nd-  degree polynomial kernel with the SVM. [sent-78, score-0.126]
</p><p>32 (2008) introduce confidenceweighted linear classifiers which are onlineclassifiers that maintain a confidence parameter for each weight and uses this to control how to change the weights in each update. [sent-80, score-0.476]
</p><p>33 A problem with online algorithms is that because they have no memory of previously seen examples they do not know if a given weight has been updated many times or few times. [sent-81, score-0.212]
</p><p>34 If a weight has been updated many times the current estimation of the weight is probably relatively good and therefore should not be changed too much. [sent-82, score-0.129]
</p><p>35 On the other hand if it has never been updated before the estimation is probably very bad. [sent-83, score-0.053]
</p><p>36 CW classification deals with this by having a confidence-parameter for each weight, modeled by a Gaussian distribution, and this parameter is used to make more aggressive updates on weights with lower confidence (Dredze et al. [sent-84, score-0.147]
</p><p>37 The classifiers also use Passive-Aggressive updates (Crammer et al. [sent-86, score-0.211]
</p><p>38 , 2006) to try to maximize  the margin between positive and negative training instances. [sent-87, score-0.087]
</p><p>39 CW classifiers are online-algorithms and are therefore fast to train, and it is not necessary to keep all training examples in memory. [sent-88, score-0.217]
</p><p>40 (2009) extend the approach to multiclass classification and show that also in this setting the classifiers often outperform SVMs. [sent-92, score-0.294]
</p><p>41 (2008) present different updaterules for CW classification and show that the ones based on standard deviation rather than variance yield the best results. [sent-96, score-0.071]
</p><p>42 We have integrated confidenceweighted, perceptron and MIRA classifiers into the code. [sent-101, score-0.269]
</p><p>43 The code for the online classifiers has 1We have used version 1. [sent-102, score-0.291]
</p><p>44 3 Features The standard setting for the MaltParser is to use SVMs with polynomial kernels, and because of this it uses a relatively small number of features. [sent-110, score-0.077]
</p><p>45 In most of our experiments the default feature set  of MaltParser consisting of 14 features has been used. [sent-111, score-0.202]
</p><p>46 When using a linear-classifier without a kernel we need to extend the feature set in order to achieve good results. [sent-112, score-0.13]
</p><p>47 We have done this very uncritically by adding all pair wise combinations of all features. [sent-113, score-0.245]
</p><p>48 This leads to 91 additional features when using the standard 14 features. [sent-114, score-0.106]
</p><p>49 1 Online classifiers We compare CW-classifiers with other online algorithms for linear classification. [sent-117, score-0.356]
</p><p>50 We compare with perceptron (Rosenblatt, 1958) and MIRA (Crammer et al. [sent-118, score-0.099]
</p><p>51 With both these classifiers we use the same top-1 approach as with the CW-classifers and also averaging which has been shown to alleviate overfitting (Collins, 2002). [sent-120, score-0.17]
</p><p>52 Table 2 shows Labeled Attachment Score obtained  with the three online classifiers. [sent-121, score-0.121]
</p><p>53 (2009) and show that confidence-weighted classifiers are better than both perceptron and MIRA. [sent-124, score-0.269]
</p><p>54 2 Training and parsing time The training time of the CW-classifiers depends on the number of iterations used, and this of course affects the accuracy of the parser. [sent-126, score-0.272]
</p><p>55 Figure 1 shows Labeled Attachment Score as a function of the number of iterations used in training. [sent-127, score-0.082]
</p><p>56 The horizontal line shows the LAS obtained with SVM. [sent-128, score-0.051]
</p><p>57 Iterations Figure 1: LAS as a function of number of training iterations on Danish data. [sent-129, score-0.129]
</p><p>58 The dotted horizontal line shows the performance of the parser trained with SVM. [sent-130, score-0.138]
</p><p>59 We see that after 4 iterations the CW-classifier has the best performance for the data set (Danish) used in this experiment. [sent-131, score-0.082]
</p><p>60 Table 1compares training time (10 iterations) and parsing time of a parser using a CW-classifiers and a parser using SVM on the same data set. [sent-133, score-0.274]
</p><p>61 5CmWmin  Table 1: Training and parsing time on Danish data. [sent-137, score-0.143]
</p><p>62 3 Pruning features Because we explicitly represent pair wise combinations of all of the original features we get an extremely high number of binary features. [sent-139, score-0.293]
</p><p>63 For some of the larger data sets, the number of features is so big that we cannot hold the weight-vector in memory. [sent-140, score-0.124]
</p><p>64 57  Table 2: LAS on development  data for three online classifers, CW-classifiers  with manual feature se-  lection and SVM. [sent-143, score-0.279]
</p><p>65 Statistical significance is measuered between CW-classifiers without feature selection  and SVMs. [sent-144, score-0.135]
</p><p>66 To solve this problem we have tried to use pruning to remove the features occurring fewest times in the training data. [sent-145, score-0.292]
</p><p>67 If a feature occurs fewer times than a given cutofflimit the feature is not included. [sent-146, score-0.162]
</p><p>68 This goes against the idea of CW classifiers which are exactly developed so that rare features can be used. [sent-147, score-0.236]
</p><p>69 Figure 2 shows the labeled attachment score as a function of the cutoff limit on the Danish data. [sent-149, score-0.174]
</p><p>70 Cutoff limit Figure 2: LAS as a function of the cutoff limit when pruning rare features. [sent-150, score-0.28]
</p><p>71 The dotted line shows the number of features left after pruning. [sent-151, score-0.111]
</p><p>72 4 Manual feature selection Instead of pruning the features we tried manually removing some of the pair wise feature combina-  tions. [sent-153, score-0.543]
</p><p>73 We removed some of the combinations that lead to the most extra features, which is especially the case with combinations of lexical features. [sent-154, score-0.158]
</p><p>74 In the extended default feature set for instance we removed all combinations of lexical features except the combination of the word form of the token at the top of the stack and of the word form of the token at the head of the buffer. [sent-155, score-0.616]
</p><p>75 For comparison we have included the results from using the standard classifier in the MaltParser, i. [sent-159, score-0.065]
</p><p>76 6 Results with optimization The results presented above are suboptimal for the SVMs because default parameters have been used for these, and optimizing these can improve ac3In all tables statistical significance is marked with †. [sent-165, score-0.055]
</p><p>77 Manuel feature selection has been used for languages  marked with an *. [sent-170, score-0.135]
</p><p>78 In CoNNL-X both the hyper parameters for the SVMs and the features have been optimized. [sent-173, score-0.14]
</p><p>79 Here we do not do feature selection but use the features used by the  MaltParser in CoNNL-X4. [sent-174, score-0.201]
</p><p>80 The only hyper parameter for CW classification is the number of iterations. [sent-175, score-0.145]
</p><p>81 Although the manual feature selection has been shown to decrease accuracy this has been used for some languages to reduce the size of the model. [sent-177, score-0.175]
</p><p>82 We see that even though the feature set used are optimized for the SVMs there are not big differences between the parses that use SVMs and the parsers that use CW classification. [sent-179, score-0.221]
</p><p>83 In general though the parsers with SVMs does better than the parsers with CW classifiers and the difference seems to be biggest on the languages where we did manual feature selection. [sent-180, score-0.422]
</p><p>84 6  Conclusion  We have shown that using confidence-weighted classifiers with transition-based dependency parsing yields results comparable with the state-of-theart results achieved with Support Vector Machines - with faster training and parsing times. [sent-181, score-0.707]
</p><p>85 Currently  we need a very high number of features to achieve these results, and we have shown that pruning this big feature set uncritically hurts performance of 4Available at conl lx /  http :  / /maltpars  er  . [sent-182, score-0.572]
</p><p>86 org/  /  conl l  the confidence-weighted classifiers. [sent-183, score-0.084]
</p><p>87 7  Future work  Currently the biggest challenge in the approach outlined here is the very high number of features needed to achieve good results. [sent-184, score-0.117]
</p><p>88 A possible solution is to use kernels with confidence-weighted classification in the same way they are used with the SVMs. [sent-185, score-0.116]
</p><p>89 Another possibility is to extend the feature set in a more critical way than what is done now. [sent-186, score-0.081]
</p><p>90 This feature does not convey any information that the POS-tagfeature itself does not. [sent-188, score-0.081]
</p><p>91 All in  all a lot of non-informative features are added as things are now. [sent-190, score-0.066]
</p><p>92 We have not yet tried to use automatic features selection to select only the combinations that increase accuracy. [sent-191, score-0.247]
</p><p>93 We will also try to do feature selection on a more general level as this can boost accuracy a lot. [sent-192, score-0.175]
</p><p>94 The results in table 3 are obtained with the features optimized for the SVMs. [sent-193, score-0.108]
</p><p>95 These are not necessarily the optimal features for the CW-classifiers. [sent-194, score-0.066]
</p><p>96 Another comparison we would like to do is with linear SVMs. [sent-195, score-0.065]
</p><p>97 Unlike the polynomial kernel SVMs used as default in the MaltParser linear SVMs can be trained in linear time (Joachims, 2006). [sent-196, score-0.311]
</p><p>98 Trying to use the same extended feature set we use with the CW-classifiers with a linear SVM would provide an interesting comparison. [sent-197, score-0.146]
</p><p>99 Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. [sent-206, score-0.146]
</p><p>100 Malteval: An evaluation and visualization tool for dependency parsing. [sent-266, score-0.119]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('maltparser', 0.33), ('svms', 0.243), ('crammer', 0.233), ('nivre', 0.214), ('cw', 0.211), ('dredze', 0.192), ('classifiers', 0.17), ('confidenceweighted', 0.168), ('buffer', 0.158), ('parsing', 0.143), ('pruning', 0.131), ('online', 0.121), ('joakim', 0.119), ('dependency', 0.119), ('mcdonald', 0.119), ('jens', 0.117), ('transitions', 0.115), ('koby', 0.114), ('hall', 0.111), ('danish', 0.106), ('token', 0.102), ('perceptron', 0.099), ('nilsson', 0.096), ('las', 0.096), ('stack', 0.093), ('mira', 0.09), ('conl', 0.084), ('malteval', 0.084), ('uncritically', 0.084), ('iterations', 0.082), ('wise', 0.082), ('johan', 0.081), ('feature', 0.081), ('fernando', 0.08), ('combinations', 0.079), ('buchholz', 0.079), ('ryan', 0.078), ('oracle', 0.078), ('cutoff', 0.077), ('polynomial', 0.077), ('hyper', 0.074), ('classification', 0.071), ('svm', 0.069), ('hurts', 0.068), ('features', 0.066), ('classifier', 0.065), ('linear', 0.065), ('attachment', 0.061), ('big', 0.058), ('marsi', 0.057), ('machines', 0.056), ('configurations', 0.055), ('default', 0.055), ('transition', 0.055), ('selection', 0.054), ('updated', 0.053), ('multiclass', 0.053), ('biggest', 0.051), ('horizontal', 0.051), ('cm', 0.051), ('pop', 0.049), ('kernel', 0.049), ('tenth', 0.048), ('tried', 0.048), ('faster', 0.048), ('training', 0.047), ('arcs', 0.047), ('ct', 0.046), ('dotted', 0.045), ('kernels', 0.045), ('york', 0.045), ('arc', 0.043), ('push', 0.043), ('cs', 0.042), ('optimized', 0.042), ('parser', 0.042), ('updates', 0.041), ('try', 0.04), ('manual', 0.04), ('leads', 0.04), ('parsers', 0.04), ('head', 0.038), ('onto', 0.038), ('weight', 0.038), ('smallest', 0.037), ('ci', 0.037), ('ofer', 0.037), ('curacy', 0.037), ('mwh', 0.037), ('eryi', 0.037), ('lection', 0.037), ('aell', 0.037), ('isr', 0.037), ('morocco', 0.037), ('transitionbased', 0.037), ('yoshua', 0.037), ('comparable', 0.037), ('limit', 0.036), ('wn', 0.036), ('confidence', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="241-tfidf-1" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>Author: Martin Haulrich</p><p>Abstract: We show that using confidence-weighted classification in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. We also compare with other online learning algorithms and investigate the effect of pruning features when using confidenceweighted classification.</p><p>2 0.2145578 <a title="241-tfidf-2" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>Author: Kotaro Kitagawa ; Kumiko Tanaka-Ishii</p><p>Abstract: Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average pars- ing time not much slower than O(n).</p><p>3 0.17706279 <a title="241-tfidf-3" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>Author: Carlos Gomez-Rodriguez ; Joakim Nivre</p><p>Abstract: Finding a class of structures that is rich enough for adequate linguistic representation yet restricted enough for efficient computational processing is an important problem for dependency parsing. In this paper, we present a transition system for 2-planar dependency trees trees that can be decomposed into at most two planar graphs and show that it can be used to implement a classifier-based parser that runs in linear time and outperforms a stateof-the-art transition-based parser on four data sets from the CoNLL-X shared task. In addition, we present an efficient method – – for determining whether an arbitrary tree is 2-planar and show that 99% or more of the trees in existing treebanks are 2-planar.</p><p>4 0.16278379 <a title="241-tfidf-4" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>Author: Terry Koo ; Michael Collins</p><p>Abstract: We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.</p><p>5 0.14844255 <a title="241-tfidf-5" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>Author: Wenbin Jiang ; Qun Liu</p><p>Abstract: In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this clas- , sifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.</p><p>6 0.14482853 <a title="241-tfidf-6" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>7 0.11888769 <a title="241-tfidf-7" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>8 0.11854349 <a title="241-tfidf-8" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>9 0.093432315 <a title="241-tfidf-9" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>10 0.086776823 <a title="241-tfidf-10" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>11 0.081408314 <a title="241-tfidf-11" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>12 0.077557832 <a title="241-tfidf-12" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>13 0.076881118 <a title="241-tfidf-13" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>14 0.076811381 <a title="241-tfidf-14" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>15 0.075521909 <a title="241-tfidf-15" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>16 0.073847041 <a title="241-tfidf-16" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>17 0.069676906 <a title="241-tfidf-17" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>18 0.066332243 <a title="241-tfidf-18" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>19 0.065839216 <a title="241-tfidf-19" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>20 0.065744638 <a title="241-tfidf-20" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.201), (1, -0.029), (2, 0.066), (3, 0.034), (4, -0.066), (5, -0.075), (6, 0.056), (7, 0.012), (8, -0.038), (9, 0.274), (10, -0.244), (11, 0.049), (12, -0.056), (13, 0.132), (14, 0.1), (15, -0.024), (16, -0.015), (17, -0.019), (18, 0.004), (19, -0.049), (20, 0.042), (21, 0.001), (22, 0.024), (23, -0.08), (24, -0.012), (25, -0.036), (26, 0.062), (27, 0.046), (28, -0.024), (29, 0.101), (30, 0.046), (31, 0.042), (32, 0.011), (33, 0.106), (34, -0.045), (35, 0.062), (36, -0.05), (37, 0.006), (38, -0.003), (39, 0.029), (40, 0.005), (41, 0.051), (42, 0.015), (43, -0.069), (44, 0.009), (45, 0.063), (46, -0.033), (47, 0.029), (48, -0.075), (49, -0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95782715 <a title="241-lsi-1" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>Author: Martin Haulrich</p><p>Abstract: We show that using confidence-weighted classification in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. We also compare with other online learning algorithms and investigate the effect of pruning features when using confidenceweighted classification.</p><p>2 0.82829195 <a title="241-lsi-2" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>Author: Terry Koo ; Michael Collins</p><p>Abstract: We present algorithms for higher-order dependency parsing that are “third-order” in the sense that they can evaluate substructures containing three dependencies, and “efficient” in the sense that they require only O(n4) time. Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions. We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.</p><p>3 0.82293183 <a title="241-lsi-3" href="./acl-2010-Tree-Based_Deterministic_Dependency_Parsing_-_An_Application_to_Nivre%27s_Method_-.html">242 acl-2010-Tree-Based Deterministic Dependency Parsing - An Application to Nivre's Method -</a></p>
<p>Author: Kotaro Kitagawa ; Kumiko Tanaka-Ishii</p><p>Abstract: Nivre’s method was improved by enhancing deterministic dependency parsing through application of a tree-based model. The model considers all words necessary for selection of parsing actions by including words in the form of trees. It chooses the most probable head candidate from among the trees and uses this candidate to select a parsing action. In an evaluation experiment using the Penn Treebank (WSJ section), the proposed model achieved higher accuracy than did previous deterministic models. Although the proposed model’s worst-case time complexity is O(n2), the experimental results demonstrated an average pars- ing time not much slower than O(n).</p><p>4 0.77759093 <a title="241-lsi-4" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>Author: Wenbin Jiang ; Qun Liu</p><p>Abstract: In this paper we describe an intuitionistic method for dependency parsing, where a classifier is used to determine whether a pair of words forms a dependency edge. And we also propose an effective strategy for dependency projection, where the dependency relationships of the word pairs in the source language are projected to the word pairs of the target language, leading to a set of classification instances rather than a complete tree. Experiments show that, the classifier trained on the projected classification instances significantly outperforms previous projected dependency parsers. More importantly, when this clas- , sifier is integrated into a maximum spanning tree (MST) dependency parser, obvious improvement is obtained over the MST baseline.</p><p>5 0.75791538 <a title="241-lsi-5" href="./acl-2010-Using_Smaller_Constituents_Rather_Than_Sentences_in_Active_Learning_for_Japanese_Dependency_Parsing.html">253 acl-2010-Using Smaller Constituents Rather Than Sentences in Active Learning for Japanese Dependency Parsing</a></p>
<p>Author: Manabu Sassano ; Sadao Kurohashi</p><p>Abstract: We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning.</p><p>6 0.74765038 <a title="241-lsi-6" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>7 0.72794634 <a title="241-lsi-7" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>8 0.65366387 <a title="241-lsi-8" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>9 0.56859672 <a title="241-lsi-9" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>10 0.47746062 <a title="241-lsi-10" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>11 0.47497264 <a title="241-lsi-11" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>12 0.46820647 <a title="241-lsi-12" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>13 0.45595902 <a title="241-lsi-13" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>14 0.45292497 <a title="241-lsi-14" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>15 0.45167601 <a title="241-lsi-15" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>16 0.43207577 <a title="241-lsi-16" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>17 0.41865563 <a title="241-lsi-17" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>18 0.4185816 <a title="241-lsi-18" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>19 0.41467094 <a title="241-lsi-19" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>20 0.41029313 <a title="241-lsi-20" href="./acl-2010-Optimal_Rank_Reduction_for_Linear_Context-Free_Rewriting_Systems_with_Fan-Out_Two.html">186 acl-2010-Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.029), (25, 0.05), (44, 0.016), (59, 0.13), (73, 0.037), (76, 0.382), (78, 0.019), (83, 0.073), (84, 0.019), (98, 0.166)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8173036 <a title="241-lda-1" href="./acl-2010-Transition-Based_Parsing_with_Confidence-Weighted_Classification.html">241 acl-2010-Transition-Based Parsing with Confidence-Weighted Classification</a></p>
<p>Author: Martin Haulrich</p><p>Abstract: We show that using confidence-weighted classification in transition-based parsing gives results comparable to using SVMs with faster training and parsing time. We also compare with other online learning algorithms and investigate the effect of pruning features when using confidenceweighted classification.</p><p>2 0.79486442 <a title="241-lda-2" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>Author: Nils Reiter ; Anette Frank</p><p>Abstract: This paper presents a supervised approach for identifying generic noun phrases in context. Generic statements express rulelike knowledge about kinds or events. Therefore, their identification is important for the automatic construction of knowledge bases. In particular, the distinction between generic and non-generic statements is crucial for the correct encoding of generic and instance-level information. Generic expressions have been studied extensively in formal semantics. Building on this work, we explore a corpus-based learning approach for identifying generic NPs, using selections of linguistically motivated features. Our results perform well above the baseline and existing prior work.</p><p>3 0.76428771 <a title="241-lda-3" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>Author: Peng Li ; Jing Jiang ; Yinglin Wang</p><p>Abstract: In this paper, we propose a novel approach to automatic generation of summary templates from given collections of summary articles. This kind of summary templates can be useful in various applications. We first develop an entity-aspect LDA model to simultaneously cluster both sentences and words into aspects. We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects. Key features of our method include automatic grouping of semantically related sentence patterns and automatic identification of template slots that need to be filled in. We apply our method on five Wikipedia entity categories and compare our method with two baseline methods. Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method.</p><p>4 0.6653105 <a title="241-lda-4" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and treesubstitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.</p><p>5 0.55869687 <a title="241-lda-5" href="./acl-2010-Importance_of_Linguistic_Constraints_in_Statistical_Dependency_Parsing.html">143 acl-2010-Importance of Linguistic Constraints in Statistical Dependency Parsing</a></p>
<p>Author: Bharat Ram Ambati</p><p>Abstract: Statistical systems with high accuracy are very useful in real-world applications. If these systems can capture basic linguistic information, then the usefulness of these statistical systems improve a lot. This paper is an attempt at incorporating linguistic constraints in statistical dependency parsing. We consider a simple linguistic constraint that a verb should not have multiple subjects/objects as its children in the dependency tree. We first describe the importance of this constraint considering Machine Translation systems which use dependency parser output, as an example application. We then show how the current state-ofthe-art dependency parsers violate this constraint. We present two new methods to handle this constraint. We evaluate our methods on the state-of-the-art dependency parsers for Hindi and Czech. 1</p><p>6 0.54746687 <a title="241-lda-6" href="./acl-2010-Improving_Arabic-to-English_Statistical_Machine_Translation_by_Reordering_Post-Verbal_Subjects_for_Alignment.html">145 acl-2010-Improving Arabic-to-English Statistical Machine Translation by Reordering Post-Verbal Subjects for Alignment</a></p>
<p>7 0.54667771 <a title="241-lda-7" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>8 0.54553896 <a title="241-lda-8" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>9 0.54524612 <a title="241-lda-9" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>10 0.54073244 <a title="241-lda-10" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>11 0.5374102 <a title="241-lda-11" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>12 0.53691053 <a title="241-lda-12" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>13 0.5368191 <a title="241-lda-13" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>14 0.53661144 <a title="241-lda-14" href="./acl-2010-Generating_Image_Descriptions_Using_Dependency_Relational_Patterns.html">124 acl-2010-Generating Image Descriptions Using Dependency Relational Patterns</a></p>
<p>15 0.53455687 <a title="241-lda-15" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>16 0.5333339 <a title="241-lda-16" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>17 0.52911872 <a title="241-lda-17" href="./acl-2010-Cross_Lingual_Adaptation%3A_An_Experiment_on_Sentiment_Classifications.html">80 acl-2010-Cross Lingual Adaptation: An Experiment on Sentiment Classifications</a></p>
<p>18 0.52785617 <a title="241-lda-18" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>19 0.52768081 <a title="241-lda-19" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>20 0.52695537 <a title="241-lda-20" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
