<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-150" href="#">acl2010-150</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</h1>
<br/><p>Source: <a title="acl-2010-150-pdf" href="http://aclweb.org/anthology//P/P10/P10-1029.pdf">pdf</a></p><p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynam- ically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.</p><p>Reference: <a title="acl-2010-150-reference" href="../acl2010_reference/acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu o  Abstract This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. [sent-3, score-0.659]
</p><p>2 Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. [sent-6, score-0.874]
</p><p>3 The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. [sent-7, score-0.991]
</p><p>4 We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynam-  ically create semantic features. [sent-8, score-0.397]
</p><p>5 We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts. [sent-9, score-0.872]
</p><p>6 1 Introduction The goal of our research is to create semantic class taggers that can assign a semantic class label to every noun phrase in a sentence. [sent-10, score-0.834]
</p><p>7 Despite widespread interest in semantic tagging, nearly all semantic taggers for comprehensive NP tagging still rely on supervised learning, which requires annotated data for training. [sent-19, score-0.638]
</p><p>8 Our research explores the idea of inducing domain-specific semantic taggers using a small set of seed words as the only form of human supervision. [sent-23, score-0.563]
</p><p>9 Given an (unannotated) collection of domain-specific text, we automatically generate training instances by labelling every instance of a  seed word with its designated semantic class. [sent-24, score-0.687]
</p><p>10 We then train a classifier to do semantic tagging using these seed-based annotations, using bootstrapping to iteratively improve performance. [sent-25, score-0.735]
</p><p>11 The classifier must learn how to assign different semantic tags to different instances of the same word based on context (e. [sent-27, score-0.581]
</p><p>12 And yet, we plan to train the classifier using stand-alone seed words, making the assumption that every instance of the seed belongs to the same semantic class. [sent-30, score-0.687]
</p><p>13 We resolve this apparent contradiction by using semantically unambiguous seeds and by introducing an initial context-only training phase before bootstrapping begins. [sent-31, score-0.517]
</p><p>14 Then we apply the classifier to the corpus to automatically label new instances, and combine these new instances with the seed-based instances. [sent-35, score-0.438]
</p><p>15 Another challenge is that we want to use a small set of seeds to minimize the amount of human effort, and then use bootstrapping to fully exploit the domain-specific corpus. [sent-37, score-0.47]
</p><p>16 To address these issues, we simultaneously induce a suite of classifiers for multiple semantic categories, using the positive instances of one semantic category as negative instances for the others. [sent-40, score-1.236]
</p><p>17 As bootstrapping progresses, the classifiers gradually improve themselves, and each other, over many iterations. [sent-41, score-0.533]
</p><p>18 We also explore a onesemantic-class-per-discourse (OSCPD) heuristic that infuses the learning process with fresh training instances, which may be substantially different from the ones seen previously, and we use the labels produced by the classifiers to dynamically create semantic features. [sent-42, score-0.617]
</p><p>19 We evaluate our approach by creating six se-  mantic taggers using a collection ofmessage board posts in the domain of veterinary medicine. [sent-43, score-0.671]
</p><p>20 Our results show this approach produces high-quality semantic taggers after a sustained bootstrapping cycle that maintains good precision while steadily increasing recall over many iterations. [sent-44, score-0.866]
</p><p>21 2  Related Work  Semantic class tagging is most closely related to named entity recognition (NER), mention detection, and semantic lexicon induction. [sent-45, score-0.397]
</p><p>22 Our work, however, focuses on classification of NP instances in context, so the same phrase may be assigned to different semantic classes in different contexts. [sent-70, score-0.426]
</p><p>23 Consequently, our classifier  can also assign semantic class labels to pronouns. [sent-71, score-0.488]
</p><p>24 The idea of simulataneously learning multiple semantic categories to prevent semantic drift has been explored for other tasks, such as semantic lexicon induction (Thelen and Riloff, 2002; McIntosh and Curran, 2009) and pattern learning (Yangarber, 2003). [sent-83, score-0.669]
</p><p>25 1 Motivation Our goal is to create a bootstrapping model that can rapidly create semantic class taggers using just a small set of seed words and an unannotated domain-specific corpus. [sent-94, score-1.155]
</p><p>26 As an example of such a domain, we have been working with a collection of message board posts in the field of veterinary medicine. [sent-96, score-0.594]
</p><p>27 Given a  document, we want a semantic class tagger to label every NP with a semantic category, for example: [A 14yo doxy]ANIMAL owned by [a reputable breeder]HUMAN is being treated for [IBD]DISEASE with [pred]DRUG. [sent-97, score-0.493]
</p><p>28 Nearly all semantic class taggers are trained using supervised learning with manually annotated data. [sent-109, score-0.474]
</p><p>29 So we set out to create a bootstrapping model that can rapidly create domain-specific semantic taggers using just a few seed words and a domain-specific text collection. [sent-111, score-0.999]
</p><p>30 First, we train strictly contextual classifiers from the seed annotations. [sent-113, score-0.484]
</p><p>31 We then apply the classifiers to the unlabeled data to generate new annotated instances that are added to the training set. [sent-114, score-0.517]
</p><p>32 Second, we employ a cross-category bootstrapping process that simultaneously trains a suite of classifiers for multiple semantic categories, using the positive instances for one semantic class as negative instances for the others. [sent-115, score-1.685]
</p><p>33 This cross-category training process gives the learner sustained momentum over many bootstrapping iterations. [sent-116, score-0.553]
</p><p>34 Finally, we explore two additional enhancements: (1) a one-semantic-class-  per-discourse heuristic to automatically generate new training instances, and (2) dynamically created semantic features produced by the classifiers themselves. [sent-117, score-0.516]
</p><p>35 But even so, training a traditional classifier from seedbased instances would likely produce a classifier that learns to recognize the seeds but is unable to classify new examples. [sent-124, score-0.752]
</p><p>36 First, we generate training instances by automatically labeling each instance of a seed word with its designated semantic class. [sent-127, score-0.687]
</p><p>37 By essentially “masking” the seed words so the classifier can only see the contexts around them, we force the classifier to generalize. [sent-129, score-0.486]
</p><p>38 We create a suite of strictly contextual classifiers, one for each semantic category. [sent-130, score-0.43]
</p><p>39 Each classifier makes a binary decision as to whether a noun phrase belongs to its semantic category. [sent-131, score-0.383]
</p><p>40 We use the seed words for category Ck to generate positive training instances for the Ck classifier, and the seed words for all other categories to generate the negative training instances for Ck. [sent-132, score-1.111]
</p><p>41 The seed word, however, is discarded so that the classifier is essentially blind-folded and cannot see the seed that produced the training instance. [sent-136, score-0.554]
</p><p>42 Suppose that “FELV” is a seed for the DISEASE category and “test” is a seed for the TEST category. [sent-139, score-0.416]
</p><p>43 We use a confidence score to label only the instances that the classifiers are most certain about. [sent-141, score-0.568]
</p><p>44 We compute a confidence score for instance iwith respect to semantic class Ck by considering both the score of the Ck classifier as well as the scores of the competing classifiers. [sent-142, score-0.636]
</p><p>45 Intuitively, we have confidence in labeling an instance as category Ck if the Ck classifier gave it a positive score, and its score is much higher than the score of any other classifier. [sent-143, score-0.384]
</p><p>46 All instances that pass the confidence t ≥hr θeshold are labeled and added to the training set. [sent-147, score-0.392]
</p><p>47 In this initial learning step, the strictly contextual classifiers substantially increase the number of training instances for each semantic category, producing a more diverse mix of seed-generated instances and context-generated instances. [sent-149, score-1.074]
</p><p>48 The key challenge was to design a bootstrapping model that would not succumb to semantic drift and would have sustained momentum to continue learning over many iterations. [sent-152, score-0.749]
</p><p>49 After each training cycle, all of the classifiers are applied to the remaining  unlabeled instances and each classifier labels the (positive) instances that it is most confident about (i. [sent-158, score-0.975]
</p><p>50 All of the new instances produced by classifier Ck are then added to the set of positive training instances for Ck and to the set of negative training instances for all of the other classifiers. [sent-162, score-1.03]
</p><p>51 One potential problem with this scheme is that some categories are more prolific than others, plus we are collecting negative instances from a set of competing classifiers. [sent-163, score-0.395]
</p><p>52 Second, similar in spirit to co-training3 , this approach allows each classifier to obtain new training instances from an outside source that has a slightly different perspective. [sent-173, score-0.448]
</p><p>53 While independent self-training can quickly run out of steam, cross-category training supplies each classifier with a constant stream of new (negative) instances produced by competing classifiers. [sent-174, score-0.488]
</p><p>54 In Section 4, we will show that cross-category bootstrapping performs substantially better than an independent self-training model, where each classifier is bootstrapped separately. [sent-175, score-0.583]
</p><p>55 4  One Semantic Class Per Discourse  We also explored the idea of using a one semantic class per discourse (OSCPD) heuristic to gener-  ate additional training instances during bootstrapping. [sent-182, score-0.643]
</p><p>56 Inspired by Yarowsky’s one sense per discourse heuristic for word sense disambiguation (Yarowsky, 1995), we make the assumption that multiple instances of a word in the same discourse will nearly always correspond to the same semantic class. [sent-183, score-0.54]
</p><p>57 Since our data set consists of message board posts organized as threads, we consider all posts in the same thread to be a single discourse. [sent-184, score-0.432]
</p><p>58 For each newly labeled instance, the OSCPD heuristic collects all instances with the same head noun in the same discourse (thread) and unilaterally labels them with the same semantic class. [sent-186, score-0.645]
</p><p>59 our  feature  contexts, thereby infusing the bootstrapping process with “fresh” training examples. [sent-188, score-0.407]
</p><p>60 If the classifier labels a word incorrectly, however, then the OSCPD heuristic will compound the error and mislabel even more instances incorrectly. [sent-190, score-0.532]
</p><p>61 Therefore we only apply this heuristic to instances that are labeled with extremely high confidence (θcf ≥ 2. [sent-191, score-0.419]
</p><p>62 d9 unlabeled instances, respectively, wl/c is the # of instances labeled as c, and wu/c is the # of unlabeled instances that receive a positive confidence score for c when given to the classifier. [sent-198, score-0.746]
</p><p>63 The intuition behind the second term is that most instances are initially unlabeled and we want to make sure that many of the unlabeled instances are likely to belong to the same semantic class (even though the classifier isn’t ready to commit to them yet). [sent-199, score-1.025]
</p><p>64 5  Dynamic Semantic Features  For many NLP tasks, classifiers use semantic features to represent the semantic class of words. [sent-203, score-0.629]
</p><p>65 Our bootstrapping model incrementally trains semantic class taggers, so we explored the idea of using the labels assigned by the classifiers to create enhanced feature vectors by dynamically adding semantic features. [sent-205, score-1.189]
</p><p>66 This process allows later stages of bootstrapping to directly benefit from earlier stages. [sent-206, score-0.397]
</p><p>67 To create semantic features, we use the semantic tags that 279  have been assigned to the current set of labeled instances. [sent-209, score-0.444]
</p><p>68 When a feature vector is created for a target NP, we check every noun instance in its context window to see if it has been assigned a semantic tag, and if so, then we add a semantic feature. [sent-210, score-0.408]
</p><p>69 6 Thresholds and Stopping Criterion When new instances are automatically labeled during bootstrapping, it is critically important that most of the labels are correct or performance rapidly deteriorates. [sent-213, score-0.385]
</p><p>70 This suggests that we should only label instances in which the classifier has high confidence. [sent-214, score-0.438]
</p><p>71 On the other hand, a high threshold often yields few new instances, which can cause the bootstrapping process to sputter and halt. [sent-215, score-0.411]
</p><p>72 To balance these competing demands, we used a sliding threshold that begins conservatively but  gradually loosens the reins as bootstrapping progresses. [sent-216, score-0.524]
</p><p>73 0, which only labels instances that the classifier is highly confident about. [sent-218, score-0.458]
</p><p>74 The message board posts contain an abundance of veterinary terminology and jargon, so two domain experts5 from VIN created a test set (answer key) for our evaluation. [sent-237, score-0.594]
</p><p>75 We defined annotation guidelines6 for each semantic category and conducted an inter-annotator agreement study to measure the consistency of the two domain experts on 30 message board posts, which contained 1,473 noun phrases. [sent-238, score-0.541]
</p><p>76 The WordNet baseline yields low recall (21-32%) for every category except HUMAN, which confirms that many veterinary terms are not present in WordNet. [sent-263, score-0.383]
</p><p>77 The second baseline simply labels every instance of a seed with its designated semantic class. [sent-269, score-0.451]
</p><p>78 The exception is HUMAN, where 80% of the instances match a seed word, undoubtedly because five of the ten HUMAN seeds are 1st and 2nd person pronouns, which are extremely common. [sent-272, score-0.532]
</p><p>79 A third baseline trains semantic classifiers using supervised learning by performing 10-fold crossvalidation on the test set. [sent-273, score-0.455]
</p><p>80 9For all of our classifiers, supervised and bootstrapped, we label all instances of the seed words first and then apply the classifiers to the unlabeled (non-seed) instances. [sent-281, score-0.728]
</p><p>81 (We indicate the number of iterations until bootstrapping ended using the notation I. [sent-288, score-0.399]
</p><p>82 A good future strategy might be to use the OSCPD heuris-  tic only during the early stages of bootstrapping when the classifier’s decisions are most reliable. [sent-302, score-0.397]
</p><p>83 We suspect this is because the semantic features are sparse during most of the bootstrapping process. [sent-305, score-0.54]
</p><p>84 We created an analogous suite of classifiers using self-training, where each classifier independently labels the instances that it is most confident about, adds them only to its own training set, and then retrains Ind. [sent-311, score-0.749]
</p><p>85 The  the self-training model ended after just 13 bootstrapping cycles (I. [sent-317, score-0.399]
</p><p>86 Figure 3 tracks the recall and precision scores of the XCat+OSCPD+SF system as bootstrapping progresses. [sent-322, score-0.465]
</p><p>87 This graph shows the sustained momentum of cross-category bootstrapping: re-  # of iterations  Figure 3: Recall and Precision scores during cross-category bootstrapping call steadily improves while precision stays consistently high with only a slight dropoff at the end. [sent-323, score-0.563]
</p><p>88 st W syitshte jmust us1 i6ntgh o1 f6, th81e, t41ra,i21n,in43g, s aentd,  the system has about 1,600 message board posts to use for training, which yields a similar F score (roughly 61%) as the supervised baseline that used 100 manually annotated posts via 10-fold crossvalidation. [sent-327, score-0.53]
</p><p>89 So with 16 times more text, seed-based bootstrapping achieves roughly the same results as supervised learning. [sent-328, score-0.405]
</p><p>90 In contrast, seed-based bootstrapping exploits a small number of human-provided seeds, but needs a larger set of (unannotated) texts for training because the seeds produce relatively sparse annotations of the texts. [sent-331, score-0.556]
</p><p>91 An additional advantage of seed-based bootstrapping methods is that they can easily exploit unlimited amounts of training text. [sent-332, score-0.407]
</p><p>92 176weusd3times  as many negative instances as positive instances for every semantic category during bootstrapping. [sent-352, score-0.779]
</p><p>93 This ratio was based on early experiments where we needed to limit the number of negative instances per category because the crosscategory framework naturally produces an extremely skewed negative/positive training set. [sent-353, score-0.456]
</p><p>94 Our conclusion is that a 1:1 ratio is too restrictive but, in general, the cross-category bootstrapping process is relatively insensitive to the specific negative/positive ratio used. [sent-358, score-0.511]
</p><p>95 2410  Finally, we examined performance on gendered pronouns (he/she/him/her), which can refer to ei-  ther animals or people in the veterinary domain. [sent-361, score-0.427]
</p><p>96 5  Conclusions  We presented a novel technique for inducing domain-specific semantic class taggers from a handful of seed words and an unannotated text collection. [sent-365, score-0.719]
</p><p>97 Our results showed that the induced taggers achieve good performance on six semantic categories associated with the domain of veterinary medicine. [sent-366, score-0.67]
</p><p>98 Our technique allows semantic class taggers to be rapidly created for specialized domains with minimal human effort. [sent-367, score-0.546]
</p><p>99 Graph-based analysis of semantic drift in espresso-like bootstrapping algorithms. [sent-457, score-0.603]
</p><p>100 A bootstrapping approach to named entity classification using successive learners. [sent-509, score-0.405]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bootstrapping', 0.36), ('oscpd', 0.291), ('veterinary', 0.271), ('instances', 0.246), ('semantic', 0.18), ('seed', 0.176), ('classifiers', 0.173), ('animal', 0.171), ('classifier', 0.155), ('taggers', 0.153), ('board', 0.138), ('ck', 0.137), ('drug', 0.119), ('seeds', 0.11), ('posts', 0.109), ('disease', 0.102), ('xcategory', 0.097), ('class', 0.096), ('riloff', 0.095), ('ace', 0.092), ('contextual', 0.089), ('mcintosh', 0.078), ('doxy', 0.078), ('momentum', 0.078), ('message', 0.076), ('np', 0.076), ('specialized', 0.075), ('heuristic', 0.074), ('sliding', 0.073), ('suite', 0.071), ('cf', 0.068), ('bootstrapped', 0.068), ('sustained', 0.068), ('categories', 0.066), ('category', 0.064), ('drift', 0.063), ('thelen', 0.062), ('unannotated', 0.06), ('confidence', 0.059), ('vin', 0.058), ('animals', 0.058), ('dvm', 0.058), ('felv', 0.058), ('gendered', 0.058), ('veterinarians', 0.058), ('vetsulin', 0.058), ('labels', 0.057), ('trains', 0.057), ('precision', 0.057), ('ratio', 0.056), ('ner', 0.056), ('inducing', 0.054), ('score', 0.053), ('xcat', 0.051), ('diagnosed', 0.051), ('diseases', 0.051), ('unlabeled', 0.051), ('threshold', 0.051), ('recall', 0.048), ('noun', 0.048), ('mix', 0.047), ('jargon', 0.047), ('training', 0.047), ('strictly', 0.046), ('supervised', 0.045), ('named', 0.045), ('create', 0.044), ('threads', 0.044), ('negative', 0.043), ('lab', 0.042), ('thresholds', 0.042), ('rapidly', 0.042), ('dynamically', 0.042), ('nearly', 0.04), ('tagging', 0.04), ('competing', 0.04), ('labeled', 0.04), ('pronouns', 0.04), ('ended', 0.039), ('relatively', 0.039), ('abx', 0.039), ('gsc', 0.039), ('parvo', 0.039), ('seedbased', 0.039), ('symptoms', 0.039), ('curran', 0.038), ('designated', 0.038), ('wordnet', 0.037), ('label', 0.037), ('stages', 0.037), ('mention', 0.036), ('experts', 0.035), ('roark', 0.034), ('phillips', 0.034), ('keerthi', 0.034), ('yangarber', 0.034), ('mueller', 0.034), ('shepherd', 0.034), ('simultaneously', 0.033), ('singer', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="150-tfidf-1" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynam- ically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.</p><p>2 0.14895216 <a title="150-tfidf-2" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>Author: Zornitsa Kozareva ; Eduard Hovy</p><p>Abstract: A challenging problem in open information extraction and text mining is the learning of the selectional restrictions of semantic relations. We propose a minimally supervised bootstrapping algorithm that uses a single seed and a recursive lexico-syntactic pattern to learn the arguments and the supertypes of a diverse set of semantic relations from the Web. We evaluate the performance of our algorithm on multiple semantic relations expressed using “verb”, “noun”, and “verb prep” lexico-syntactic patterns. Humanbased evaluation shows that the accuracy of the harvested information is about 90%. We also compare our results with existing knowledge base to outline the similarities and differences of the granularity and diversity of the harvested knowledge.</p><p>3 0.14164947 <a title="150-tfidf-3" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>Author: Rasoul Samad Zadeh Kaljahi</p><p>Abstract: Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1</p><p>4 0.14098807 <a title="150-tfidf-4" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>Author: David Vickrey ; Oscar Kipersztok ; Daphne Koller</p><p>Abstract: We present a novel system that helps nonexperts find sets of similar words. The user begins by specifying one or more seed words. The system then iteratively suggests a series of candidate words, which the user can either accept or reject. Current techniques for this task typically bootstrap a classifier based on a fixed seed set. In contrast, our system involves the user throughout the labeling process, using active learning to intelligently explore the space of similar words. In particular, our system can take advantage of negative examples provided by the user. Our system combines multiple preexisting sources of similarity data (a standard thesaurus, WordNet, contextual similarity), enabling it to capture many types of similarity groups (“synonyms of crash,” “types of car,” etc.). We evaluate on a hand-labeled evaluation set; our system improves over a strong baseline by 36%.</p><p>5 0.12858374 <a title="150-tfidf-5" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>Author: Xiao-Li Li ; Lei Zhang ; Bing Liu ; See-Kiong Ng</p><p>Abstract: Distributional similarity is a classic technique for entity set expansion, where the system is given a set of seed entities of a particular class, and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds. This paper shows that a machine learning model called positive and unlabeled learning (PU learning) can model the set expansion problem better. Based on the test results of 10 corpora, we show that a PU learning technique outperformed distributional similarity significantly. 1</p><p>6 0.12185521 <a title="150-tfidf-6" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>7 0.11633526 <a title="150-tfidf-7" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<p>8 0.10867915 <a title="150-tfidf-8" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>9 0.10633978 <a title="150-tfidf-9" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>10 0.10304981 <a title="150-tfidf-10" href="./acl-2010-On_Learning_Subtypes_of_the_Part-Whole_Relation%3A_Do_Not_Mix_Your_Seeds.html">181 acl-2010-On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds</a></p>
<p>11 0.094829254 <a title="150-tfidf-11" href="./acl-2010-Improving_Chinese_Semantic_Role_Labeling_with_Rich_Syntactic_Features.html">146 acl-2010-Improving Chinese Semantic Role Labeling with Rich Syntactic Features</a></p>
<p>12 0.090592742 <a title="150-tfidf-12" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>13 0.084593676 <a title="150-tfidf-13" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>14 0.083506197 <a title="150-tfidf-14" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>15 0.081527919 <a title="150-tfidf-15" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>16 0.081400394 <a title="150-tfidf-16" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>17 0.081244744 <a title="150-tfidf-17" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>18 0.081107169 <a title="150-tfidf-18" href="./acl-2010-Semantic_Parsing%3A_The_Task%2C_the_State_of_the_Art_and_the_Future.html">206 acl-2010-Semantic Parsing: The Task, the State of the Art and the Future</a></p>
<p>19 0.078911312 <a title="150-tfidf-19" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>20 0.076843783 <a title="150-tfidf-20" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.243), (1, 0.115), (2, 0.013), (3, 0.01), (4, 0.052), (5, 0.043), (6, 0.032), (7, 0.067), (8, 0.026), (9, 0.119), (10, -0.046), (11, 0.063), (12, -0.075), (13, -0.159), (14, 0.015), (15, 0.115), (16, 0.123), (17, -0.05), (18, 0.04), (19, -0.028), (20, 0.013), (21, 0.092), (22, -0.012), (23, 0.13), (24, -0.085), (25, -0.128), (26, -0.064), (27, 0.104), (28, 0.04), (29, 0.105), (30, 0.013), (31, 0.099), (32, -0.051), (33, -0.038), (34, -0.006), (35, 0.007), (36, -0.112), (37, 0.085), (38, 0.015), (39, 0.026), (40, 0.048), (41, -0.092), (42, 0.023), (43, -0.077), (44, -0.018), (45, 0.011), (46, 0.035), (47, 0.025), (48, -0.054), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95322216 <a title="150-lsi-1" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynam- ically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.</p><p>2 0.69796747 <a title="150-lsi-2" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>Author: Xiao-Li Li ; Lei Zhang ; Bing Liu ; See-Kiong Ng</p><p>Abstract: Distributional similarity is a classic technique for entity set expansion, where the system is given a set of seed entities of a particular class, and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds. This paper shows that a machine learning model called positive and unlabeled learning (PU learning) can model the set expansion problem better. Based on the test results of 10 corpora, we show that a PU learning technique outperformed distributional similarity significantly. 1</p><p>3 0.69391572 <a title="150-lsi-3" href="./acl-2010-Weakly_Supervised_Learning_of_Presupposition_Relations_between_Verbs.html">258 acl-2010-Weakly Supervised Learning of Presupposition Relations between Verbs</a></p>
<p>Author: Galina Tremper</p><p>Abstract: Presupposition relations between verbs are not very well covered in existing lexical semantic resources. We propose a weakly supervised algorithm for learning presupposition relations between verbs that distinguishes five semantic relations: presupposition, entailment, temporal inclusion, antonymy and other/no relation. We start with a number of seed verb pairs selected manually for each semantic relation and classify unseen verb pairs. Our algorithm achieves an overall accuracy of 36% for type-based classification.</p><p>4 0.67994839 <a title="150-lsi-4" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>Author: Ahmed Hassan ; Dragomir Radev</p><p>Abstract: Automatically identifying the polarity of words is a very important task in Natural Language Processing. It has applications in text classification, text filtering, analysis of product review, analysis of responses to surveys, and mining online discussions. We propose a method for identifying the polarity of words. We apply a Markov random walk model to a large word relatedness graph, producing a polarity estimate for any given word. A key advantage of the model is its ability to accurately and quickly assign a polarity sign and magnitude to any word. The method could be used both in a semi-supervised setting where a training set of labeled words is used, and in an unsupervised setting where a handful of seeds is used to define the two polarity classes. The method is experimentally tested using a manually labeled set of positive and negative words. It outperforms the state of the art methods in the semi-supervised setting. The results in the unsupervised setting is comparable to the best reported values. However, the proposed method is faster and does not need a large corpus.</p><p>5 0.67592204 <a title="150-lsi-5" href="./acl-2010-Adapting_Self-Training_for_Semantic_Role_Labeling.html">25 acl-2010-Adapting Self-Training for Semantic Role Labeling</a></p>
<p>Author: Rasoul Samad Zadeh Kaljahi</p><p>Abstract: Supervised semantic role labeling (SRL) systems trained on hand-crafted annotated corpora have recently achieved state-of-the-art performance. However, creating such corpora is tedious and costly, with the resulting corpora not sufficiently representative of the language. This paper describes a part of an ongoing work on applying bootstrapping methods to SRL to deal with this problem. Previous work shows that, due to the complexity of SRL, this task is not straight forward. One major difficulty is the propagation of classification noise into the successive iterations. We address this problem by employing balancing and preselection methods for self-training, as a bootstrapping algorithm. The proposed methods could achieve improvement over the base line, which do not use these methods. 1</p><p>6 0.63515323 <a title="150-lsi-6" href="./acl-2010-Learning_Better_Data_Representation_Using_Inference-Driven_Metric_Learning.html">161 acl-2010-Learning Better Data Representation Using Inference-Driven Metric Learning</a></p>
<p>7 0.61171222 <a title="150-lsi-7" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>8 0.6102972 <a title="150-lsi-8" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>9 0.59187055 <a title="150-lsi-9" href="./acl-2010-An_Active_Learning_Approach_to_Finding_Related_Terms.html">27 acl-2010-An Active Learning Approach to Finding Related Terms</a></p>
<p>10 0.57474738 <a title="150-lsi-10" href="./acl-2010-On_Learning_Subtypes_of_the_Part-Whole_Relation%3A_Do_Not_Mix_Your_Seeds.html">181 acl-2010-On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds</a></p>
<p>11 0.56636506 <a title="150-lsi-11" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>12 0.56224525 <a title="150-lsi-12" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>13 0.55036169 <a title="150-lsi-13" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>14 0.55025923 <a title="150-lsi-14" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<p>15 0.54516727 <a title="150-lsi-15" href="./acl-2010-Automatically_Generating_Term_Frequency_Induced_Taxonomies.html">43 acl-2010-Automatically Generating Term Frequency Induced Taxonomies</a></p>
<p>16 0.54102015 <a title="150-lsi-16" href="./acl-2010-Comparable_Entity_Mining_from_Comparative_Questions.html">63 acl-2010-Comparable Entity Mining from Comparative Questions</a></p>
<p>17 0.5360083 <a title="150-lsi-17" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>18 0.53362644 <a title="150-lsi-18" href="./acl-2010-Hunting_for_the_Black_Swan%3A_Risk_Mining_from_Text.html">138 acl-2010-Hunting for the Black Swan: Risk Mining from Text</a></p>
<p>19 0.50172651 <a title="150-lsi-19" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>20 0.48519018 <a title="150-lsi-20" href="./acl-2010-Extracting_Sequences_from_the_Web.html">111 acl-2010-Extracting Sequences from the Web</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.083), (39, 0.01), (42, 0.372), (44, 0.021), (59, 0.111), (73, 0.046), (76, 0.011), (78, 0.028), (80, 0.015), (83, 0.087), (84, 0.024), (98, 0.107)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96906596 <a title="150-lda-1" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>Author: Brian Pluss</p><p>Abstract: This paper presents ongoing research on computational models for non-cooperative dialogue. We start by analysing different levels of cooperation in conversation. Then, inspired by findings from an empirical study, we propose a technique for measuring non-cooperation in political interviews. Finally, we describe a research programme towards obtaining a suitable model and discuss previous accounts for conflictive dialogue, identifying the differences with our work.</p><p>2 0.9361949 <a title="150-lda-2" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>Author: Ryu Iida ; Syumpei Kobayashi ; Takenobu Tokunaga</p><p>Abstract: This paper proposes an approach to reference resolution in situated dialogues by exploiting extra-linguistic information. Recently, investigations of referential behaviours involved in situations in the real world have received increasing attention by researchers (Di Eugenio et al., 2000; Byron, 2005; van Deemter, 2007; Spanger et al., 2009). In order to create an accurate reference resolution model, we need to handle extra-linguistic information as well as textual information examined by existing approaches (Soon et al., 2001 ; Ng and Cardie, 2002, etc.). In this paper, we incorporate extra-linguistic information into an existing corpus-based reference resolution model, and investigate its effects on refer- ence resolution problems within a corpus of Japanese dialogues. The results demonstrate that our proposed model achieves an accuracy of 79.0% for this task.</p><p>3 0.87722957 <a title="150-lda-3" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>same-paper 4 0.8265003 <a title="150-lda-4" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>Author: Ruihong Huang ; Ellen Riloff</p><p>Abstract: This research explores the idea of inducing domain-specific semantic class taggers using only a domain-specific text collection and seed words. The learning process begins by inducing a classifier that only has access to contextual features, forcing it to generalize beyond the seeds. The contextual classifier then labels new instances, to expand and diversify the training set. Next, a cross-category bootstrapping process simultaneously trains a suite of classifiers for multiple semantic classes. The positive instances for one class are used as negative instances for the others in an iterative bootstrapping cycle. We also explore a one-semantic-class-per-discourse heuristic, and use the classifiers to dynam- ically create semantic features. We evaluate our approach by inducing six semantic taggers from a collection of veterinary medicine message board posts.</p><p>5 0.73245913 <a title="150-lda-5" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>6 0.71464407 <a title="150-lda-6" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>7 0.70445222 <a title="150-lda-7" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>8 0.70244819 <a title="150-lda-8" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>9 0.68001771 <a title="150-lda-9" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>10 0.66447496 <a title="150-lda-10" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>11 0.63527763 <a title="150-lda-11" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>12 0.58431709 <a title="150-lda-12" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>13 0.58277053 <a title="150-lda-13" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>14 0.5797987 <a title="150-lda-14" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>15 0.56387246 <a title="150-lda-15" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>16 0.56382686 <a title="150-lda-16" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>17 0.56318939 <a title="150-lda-17" href="./acl-2010-Bilingual_Lexicon_Generation_Using_Non-Aligned_Signatures.html">50 acl-2010-Bilingual Lexicon Generation Using Non-Aligned Signatures</a></p>
<p>18 0.55682528 <a title="150-lda-18" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>19 0.55568993 <a title="150-lda-19" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>20 0.5532167 <a title="150-lda-20" href="./acl-2010-Detecting_Experiences_from_Weblogs.html">85 acl-2010-Detecting Experiences from Weblogs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
