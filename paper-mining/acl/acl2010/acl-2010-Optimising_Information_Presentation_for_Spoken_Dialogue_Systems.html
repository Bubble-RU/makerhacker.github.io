<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-187" href="#">acl2010-187</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</h1>
<br/><p>Source: <a title="acl-2010-187-pdf" href="http://aclweb.org/anthology//P/P10/P10-1103.pdf">pdf</a></p><p>Author: Verena Rieser ; Oliver Lemon ; Xingkun Liu</p><p>Abstract: We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive “lower level” features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS.</p><p>Reference: <a title="acl-2010-187-reference" href="../acl2010_reference/acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. [sent-10, score-0.274]
</p><p>2 6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91. [sent-14, score-0.261]
</p><p>3 from the NLG realiser) is important for optimising IP strategies according to user preferences. [sent-19, score-0.446]
</p><p>4 1 Introduction Work on evaluating SDS suggests that the Information Presentation (IP) phase is the primary contributor to dialogue duration (Walker et al. [sent-21, score-0.22]
</p><p>5 An inherent problem in this task is the trade-off between presenting “enough” information to the user (for example helping them to feel confident that they have a good overview of the search results) versus keeping the utterances short and understandable. [sent-24, score-0.319]
</p><p>6 A similar approach has been applied to the problem of Referring Expression Generation in dialogue (Janarthanam and Lemon, 2010). [sent-26, score-0.22]
</p><p>7 It draws the user’s attention to relevant attributes by grouping the current results from the database into clusters, e. [sent-31, score-0.29]
</p><p>8 This method employs utilitybased attribute selection with respect to how each attribute (e. [sent-47, score-0.292]
</p><p>9 Related work explores a user modelling approach, where attributes are ranked according to user preferences (Dem-  berg and Moore, 2006; Winterboer et al. [sent-52, score-0.767]
</p><p>10 combinations of Information Presentation strategies as well as attribute selection), and to show the utility of both lower-level features (e. [sent-57, score-0.274]
</p><p>11 how many attributes to generate, or when to use a SUMMARY), using a pipeline model for SDS with DM features as input, and where NLG has no knowledge of lower level features (e. [sent-64, score-0.283]
</p><p>12 In the following we use a Reinforcement Learning (RL) as a statistical planning framework (Sutton and Barto, 1998) to explore the contextual features for making these decisions, and propose a new joint optimisation method for IP strategies combining con-  tent structuring and attribute selection. [sent-69, score-0.457]
</p><p>13 The IP module has to decide which action to take next, how many attributes to mention, and when to stop generating. [sent-80, score-0.288]
</p><p>14 They were in-  structed to select IP structures and attributes for NLG so as to most efficiently allow users to find a restaurant matching their search constraints. [sent-83, score-0.302]
</p><p>15 2 for a list of IP strategies to choose from), which attributes to mention (e. [sent-86, score-0.298]
</p><p>16 cuisine, price range, location, food quality, and/or service quality), and whether to stop generating, given varying numbers of database matches, varying prompt realisations, and varying user behaviour. [sent-88, score-0.422]
</p><p>17 The user speech input was delivered to the wizard using Voice Over IP. [sent-90, score-0.471]
</p><p>18 Each user performed a total of 12 tasks, where no task set was seen twice by any one wizard. [sent-95, score-0.289]
</p><p>19 [A:] The wizard selects attribute values as specified by the user’s query. [sent-97, score-0.339]
</p><p>20 [C:] The wizard then chooses which strategy and which attributes to generate next, by clicking radio buttons. [sent-102, score-0.41]
</p><p>21 The attribute/s specified in the last user query are pre-selected by default. [sent-103, score-0.289]
</p><p>22 [D:] An utterance is automatically generated by the NLG realiser every time the wizard selects a strategy, and is displayed in an intermediate text panel. [sent-105, score-0.555]
</p><p>23 [E:] The wizard can decide to add the generated utterance to the final output panel or to start over again. [sent-106, score-0.231]
</p><p>24 The text in the final panel is sent to the user via TTS, once the wizard decides to stop generating. [sent-107, score-0.498]
</p><p>25 StrategyExample utterance  Table 1: Example realisations, generated when the user provided cui s ine=Indian, and where the wizard has also selected the additional attribute pri ce for presentation to the user. [sent-108, score-0.755]
</p><p>26 After each task the user answered a questionnaire on a 6 point Likert scale, regarding the perceived generation quality in that task. [sent-110, score-0.338]
</p><p>27 The data contains 2236 utterances in total: 1465 wizard utterances and 771 user utterances. [sent-115, score-0.531]
</p><p>28 2 NLG Realiser In the Wizard-of-Oz environment we implemented a NLG realiser for the chosen IP structures and attribute choices, in order to realise the wizards’ choices in real time. [sent-123, score-0.467]
</p><p>29 The length of an utterance also depends on the number of attributes chosen, i. [sent-134, score-0.238]
</p><p>30 The approach using a UM assumes that the user has certain preferences (e. [sent-139, score-0.289]
</p><p>31 listing all the attributes for the first item and then for the other) or by Attribute (i. [sent-144, score-0.249]
</p><p>32 IF (T (pHrd Ebe HvN HiNtL nGls g=S  :=10): ELSTEHE nNlg SnltrgaSttreagtye=gsuym=ma Rreyc;ommend; Figure 3: Rules learned by JRip for the wizard model (‘dbHit s ’= number of database matches, ‘prevNLG’= previous NLG action) The features selected by this model were only “high-level” features, i. [sent-167, score-0.33]
</p><p>33 A user simulation for  NLG is very similar, in that it is a predictive model of the most likely next user act. [sent-182, score-0.673]
</p><p>34 4 However, this NLG predicted user act does not actually change the overall dialogue state (e. [sent-183, score-0.564]
</p><p>35 In other words, 4Similar to the internal user models applied in recent work on POMDP (Partially Observable Markov Decision Process) dialogue managers (Young et al. [sent-186, score-0.509]
</p><p>36 1012  the NLG user simulation tells us what the user is most likely to do next, if we were to stop generating now. [sent-189, score-0.7]
</p><p>37 We are most interested in the following user reactions: 1. [sent-190, score-0.289]
</p><p>38 select : the user chooses one of the presented items, e. [sent-191, score-0.289]
</p><p>39 This reply type indicates that the Information Presentation was sufficient for the user to make a choice. [sent-195, score-0.325]
</p><p>40 This  reply type indicates that the user has more specific requests, which s/he wants to specify after being presented with the current information. [sent-201, score-0.325]
</p><p>41 reque stMore I nfo: The user asks for more information, e. [sent-203, score-0.289]
</p><p>42 This reply type indicates that the system failed to present the information the user was looking for. [sent-208, score-0.325]
</p><p>43 askRepeat : The user asks the system to repeat the same message again, e. [sent-210, score-0.289]
</p><p>44 This reply type indicates that the utterance was either too long or confusing for the user to remember, or the TTS quality was not good enough, or both. [sent-214, score-0.374]
</p><p>45 We build user simulations using n-gram models of system (s) and user (u) acts, as first  introduced by (Eckert et al. [sent-220, score-0.621]
</p><p>46 IP structure + att|rIiPbute choice) model for predicting user reactions to the system’s combined IP structure and attribute selection decisions: P(au,t |IPs,t, attributess,t). [sent-225, score-0.506]
</p><p>47 5Where au,t is the predicted next user action at time t, IPs,t was the system’s Information Presentation action at t, and attributess,t is the attributes selected by the system at t. [sent-226, score-0.677]
</p><p>48 We use the most similar user models for system training, and the most dissimilar user models for testing NLG policies, in order to test whether the learned policies are robust and adaptive to unseen dialogue contexts. [sent-240, score-0.987]
</p><p>49 For example, the user’s  focus after the SUMMARY (with UM) in Table 1 is DBhits = 10, since the user is only interested in cheap, Indian places. [sent-249, score-0.289]
</p><p>50 This reflects the fact that good IP strategies should help the user to select an item (valueUserReaction = +100) or provide more constraints addInfo (valueUserReaction = ±0), but the user should (nvoat udeoU anything eiolsne (valueUserReaction = −100). [sent-256, score-0.747]
</p><p>51 8he4 user sin is 1 a8c hseienvteednc beys6 p, einsuch a way that the user ends the conversation unsuccessfully. [sent-271, score-0.578]
</p><p>52 The top possible reward is achieved in the rare cases where the system can immediately present 1 item to the user using just 2 sentences, and the user then selects that item, i. [sent-272, score-0.78]
</p><p>53 the user simulation and realiser), and explicitly captures the uncertainty in the generation environment. [sent-283, score-0.467]
</p><p>54 The aim of the MDP is to maximise long-term expected reward of its decisions, resulting in a policy which maps each possible state to an appropriate action in that state. [sent-286, score-0.276]
</p><p>55 We treat IP as a hierarchical joint optimisation problem, where first one of the IP structures (13) is chosen and then the number of attributes is  decided, as shown in Figure 4. [sent-287, score-0.245]
</p><p>56 At each generation step, the MDP can choose 1-5 attributes (e. [sent-288, score-0.238]
</p><p>57 Generation stops as soon as the user is predicted to select an item, i. [sent-291, score-0.344]
</p><p>58 For attribute selection we choose a majority baseline (randomly choosing between 3 or 4 attributes) since the attribute selection models learned by Supervised Learning on the WoZ data didn’t show significant improvements. [sent-304, score-0.363]
</p><p>59 For training, we used the user simulation model  most similar to the data, see Section 4. [sent-305, score-0.384]
</p><p>60 For testing, we test with the different user simulation model (the one which is most dissimilar to the data). [sent-307, score-0.413]
</p><p>61 We first investigate how well IP structure (without attribute choice) can be learned in increasingly complex generation scenarios. [sent-308, score-0.224]
</p><p>62 A generation scenario is a combination of a particular kind of NLG realiser (template vs. [sent-309, score-0.418]
</p><p>63 stochastic) along with different levels ofvariation introduced by certain features of the dialogue context. [sent-310, score-0.253]
</p><p>64 In general, the stochastic realiser introduces more variation in lower level features than the template-based realiser. [sent-311, score-0.41]
</p><p>65 IP structure choice, Template realiser: Predicted next user action varies according to the bi-gram model (P(au,t |IPs,t)); Number of sentences and attributes per IP strategy is set by defaults, reflecting a template-based  realiser. [sent-316, score-0.589]
</p><p>66 set by the DM); Sentence generation according to the SPaRKy stochastic realiser model as described in Section 3. [sent-321, score-0.398]
</p><p>67 We then investigate different scenarios for jointly optimising IP structure (IPS) and attribute selection (Attr) decisions. [sent-323, score-0.271]
</p><p>68 IPS+Attr choice, Template realiser: Predicted next user action varies according to tri-gram (P(au,t |IPs,t, attributess,t)) model; Number of sentences per IP structure set to default. [sent-326, score-0.361]
</p><p>69 IPS+Attr choice, Template realiser+Focus model: Tri-gram user simulation with Template realiser and Focus of attention model with respect to #DBhits and #attributes as described in section 4. [sent-329, score-0.712]
</p><p>70 IPS+Attr choice, Stochastic realiser: Trigram user simulation with sentence/attribute  relationship according to Stochastic realiser as described in Section 3. [sent-333, score-0.683]
</p><p>71 the full model = Predicted next user action varies according to tri-gram model+ Focus of attention model + Sentence/attribute relationship according to stochastic realiser. [sent-339, score-0.44]
</p><p>72 2 Results We compare the average final reward (see Equation 1) gained by the baseline against the trained RL policies in the different scenarios for each 1000 test runs, using a paired samples t-test. [sent-341, score-0.297]
</p><p>73 The learned RL policies show that lower level features are important in gaining significant improvements over the baseline. [sent-348, score-0.248]
</p><p>74 Note that these strategies are context-dependent: the learner chooses how to proceed dependent on 7Note, that the baseline does reasonably well in scenarios with variation introduced by only higher level features (e. [sent-351, score-0.233]
</p><p>75 Table 4: RL strategies learned for the different scenarios, where (n) denotes the number of attributes generated. [sent-379, score-0.341]
</p><p>76 It will then stop generating if the user is predicted to select an item. [sent-382, score-0.371]
</p><p>77 If the number of database items is low, it will start with a COMPARE and then continue with a RECOMMEND, unless the user selects an item. [sent-384, score-0.437]
</p><p>78 2 learns to adapt to a more complex scenario: the number of attributes requested by the DM and produced by the stochastic sentence realiser. [sent-388, score-0.239]
</p><p>79 The RL policies for jointly optimising IP strategy and attribute selection learn to select the num-  ber of attributes according to the generation scenarios 2. [sent-392, score-0.665]
</p><p>80 1generates a RECOMMEND with 5 attributes if the database hits are low (< 13). [sent-396, score-0.298]
</p><p>81 If the user is predicted to narrow down his focus after the SUMMARY, the policy continues with a COMPARE using 1 attribute only, otherwise it helps the user by presenting 4 attributes. [sent-398, score-0.879]
</p><p>82 It then continues with RECOMMEND(5), and stops as soon as the user is predicted to select one item. [sent-399, score-0.344]
</p><p>83 the cumulative number of attributes generated in the whole NLG sequence, where the same attribute may be repeated within the sequence). [sent-404, score-0.321]
</p><p>84 This strategy primarily adapts to the variations from the user simulation (tri-gram model). [sent-405, score-0.423]
</p><p>85 6  Conclusion  We have presented a new data-driven method for Information Presentation (IP) in Spoken Dialogue Systems using a statistical optimisation framework for content structure planning and attribute selection. [sent-428, score-0.274]
</p><p>86 The WoZ data was used to build statistical models of user reactions to IP strategies, and a data-driven reward function for Reinforcement Learning (RL). [sent-431, score-0.463]
</p><p>87 We compared a model of human behaviour (the ‘human wizard baseline’) against policies optimised using Reinforcement Learning, in a variety of scenarios. [sent-433, score-0.369]
</p><p>88 Our optimised policies significantly outperform the IP structuring  and attribute selection present in the WoZ data, especially when performing in complex generation scenarios which require adaptation to, e. [sent-434, score-0.467]
</p><p>89 6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91. [sent-438, score-0.261]
</p><p>90 from the NLG realiser and a user reaction model, is important for learning optimal IP strategies according to user preferences. [sent-442, score-0.986]
</p><p>91 This methodology provides new insights into the nature of the IP problem, which has previously  been treated as a module following dialogue management with no access to lower-level context features. [sent-448, score-0.245]
</p><p>92 Learning to adapt to unknown users: Referring expression generation in spoken dialogue systems. [sent-519, score-0.361]
</p><p>93 Datadriven user simulation for automated evaluation of spoken dialog systems. [sent-523, score-0.505]
</p><p>94 Learning what to say and how to say it: joint optimization of spoken dialogue management and Natural Language Generation. [sent-539, score-0.337]
</p><p>95 A wizard-of-oz interface to study information presentation strategies for spoken dialogue systems. [sent-543, score-0.524]
</p><p>96 Learning  database content for spoken dialogue system design. [sent-553, score-0.384]
</p><p>97 Trainable sentence planning for complex information presentation in spoken dialog systems. [sent-583, score-0.31]
</p><p>98 Quantitative and qualitative evaluation of DARPA Communicator spoken dialogue systems. [sent-607, score-0.312]
</p><p>99 Fish or Fowl: A Wizard of Oz evaluation of dialogue strategies in the restaurant domain. [sent-616, score-0.376]
</p><p>100 The influence of user tailoring and cognitive load on user performance in spoken dialogue systems. [sent-622, score-0.89]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ip', 0.352), ('realiser', 0.299), ('user', 0.289), ('nlg', 0.241), ('dialogue', 0.22), ('woz', 0.213), ('attributes', 0.189), ('wizard', 0.182), ('rl', 0.162), ('wizards', 0.156), ('rieser', 0.139), ('sds', 0.137), ('attribute', 0.132), ('oliver', 0.125), ('lemon', 0.121), ('reward', 0.117), ('policies', 0.117), ('strategies', 0.109), ('walker', 0.103), ('presentation', 0.103), ('reinforcement', 0.1), ('dbhits', 0.1), ('polifroni', 0.1), ('simulation', 0.095), ('spoken', 0.092), ('recommend', 0.092), ('policy', 0.087), ('verena', 0.087), ('planning', 0.086), ('valueuserreaction', 0.085), ('database', 0.072), ('action', 0.072), ('attr', 0.071), ('scenario', 0.07), ('users', 0.066), ('marilyn', 0.064), ('scenarios', 0.063), ('item', 0.06), ('um', 0.058), ('ips', 0.057), ('reactions', 0.057), ('sparky', 0.057), ('optimisation', 0.056), ('predicted', 0.055), ('dm', 0.052), ('items', 0.051), ('summary', 0.05), ('stochastic', 0.05), ('utterance', 0.049), ('generation', 0.049), ('optimising', 0.048), ('restaurant', 0.047), ('realisations', 0.046), ('janarthanam', 0.046), ('restaurants', 0.045), ('discounting', 0.043), ('simulations', 0.043), ('learned', 0.043), ('addinfo', 0.043), ('stent', 0.043), ('xingkun', 0.043), ('structuring', 0.041), ('tts', 0.04), ('strategy', 0.039), ('hits', 0.037), ('optimised', 0.037), ('environment', 0.036), ('reply', 0.036), ('price', 0.034), ('uncertainty', 0.034), ('mdp', 0.034), ('features', 0.033), ('behaviour', 0.033), ('edinburgh', 0.032), ('young', 0.031), ('utterances', 0.03), ('ratings', 0.029), ('attention', 0.029), ('dissimilar', 0.029), ('dialog', 0.029), ('selection', 0.028), ('asru', 0.028), ('boidin', 0.028), ('clarkson', 0.028), ('cuay', 0.028), ('cuisine', 0.028), ('dbhit', 0.028), ('gasic', 0.028), ('jrip', 0.028), ('winterboer', 0.028), ('template', 0.028), ('level', 0.028), ('stop', 0.027), ('decisions', 0.027), ('narrow', 0.027), ('gaining', 0.027), ('johanna', 0.026), ('management', 0.025), ('selects', 0.025), ('kingdom', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="187-tfidf-1" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>Author: Verena Rieser ; Oliver Lemon ; Xingkun Liu</p><p>Abstract: We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive “lower level” features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS.</p><p>2 0.37160614 <a title="187-tfidf-2" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>3 0.2079792 <a title="187-tfidf-3" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>Author: Pierre Lison</p><p>Abstract: Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions ofthe state and action spaces.</p><p>4 0.20547915 <a title="187-tfidf-4" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>Author: Ethan Selfridge ; Peter Heeman</p><p>Abstract: Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dia- logues, and supports the improvement of mixed-initiative interaction.</p><p>5 0.16425937 <a title="187-tfidf-5" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>Author: Francois Mairesse ; Milica Gasic ; Filip Jurcicek ; Simon Keizer ; Blaise Thomson ; Kai Yu ; Steve Young</p><p>Abstract: Most previous work on trainable language generation has focused on two paradigms: (a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows that BAGEL can generate natural and informative utterances from unseen inputs in the information presentation domain. Additionally, generation perfor- mance on sparse datasets is improved significantly by using certainty-based active learning, yielding ratings close to the human gold standard with a fraction of the data.</p><p>6 0.14711928 <a title="187-tfidf-6" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>7 0.12445407 <a title="187-tfidf-7" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>8 0.12236305 <a title="187-tfidf-8" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>9 0.12113605 <a title="187-tfidf-9" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>10 0.10281092 <a title="187-tfidf-10" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>11 0.10258164 <a title="187-tfidf-11" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>12 0.10135176 <a title="187-tfidf-12" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>13 0.094250277 <a title="187-tfidf-13" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>14 0.089220092 <a title="187-tfidf-14" href="./acl-2010-Automated_Planning_for_Situated_Natural_Language_Generation.html">35 acl-2010-Automated Planning for Situated Natural Language Generation</a></p>
<p>15 0.088139176 <a title="187-tfidf-15" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>16 0.080303796 <a title="187-tfidf-16" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>17 0.073723115 <a title="187-tfidf-17" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>18 0.068425119 <a title="187-tfidf-18" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>19 0.067984663 <a title="187-tfidf-19" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>20 0.06734661 <a title="187-tfidf-20" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.156), (1, 0.099), (2, -0.119), (3, -0.228), (4, -0.064), (5, -0.292), (6, -0.22), (7, 0.084), (8, -0.046), (9, 0.002), (10, 0.057), (11, -0.102), (12, -0.015), (13, -0.047), (14, 0.062), (15, -0.141), (16, 0.102), (17, 0.048), (18, -0.1), (19, 0.027), (20, 0.013), (21, -0.014), (22, -0.025), (23, 0.134), (24, -0.004), (25, 0.103), (26, 0.026), (27, -0.029), (28, -0.052), (29, 0.037), (30, 0.03), (31, 0.001), (32, -0.03), (33, -0.015), (34, 0.017), (35, -0.046), (36, -0.06), (37, -0.028), (38, 0.094), (39, -0.03), (40, -0.025), (41, -0.016), (42, -0.004), (43, 0.042), (44, -0.002), (45, -0.097), (46, 0.105), (47, 0.079), (48, -0.11), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97000325 <a title="187-lsi-1" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>Author: Verena Rieser ; Oliver Lemon ; Xingkun Liu</p><p>Abstract: We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive “lower level” features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS.</p><p>2 0.91618359 <a title="187-lsi-2" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>3 0.9078297 <a title="187-lsi-3" href="./acl-2010-Importance-Driven_Turn-Bidding_for_Spoken_Dialogue_Systems.html">142 acl-2010-Importance-Driven Turn-Bidding for Spoken Dialogue Systems</a></p>
<p>Author: Ethan Selfridge ; Peter Heeman</p><p>Abstract: Current turn-taking approaches for spoken dialogue systems rely on the speaker releasing the turn before the other can take it. This reliance results in restricted interactions that can lead to inefficient dialogues. In this paper we present a model we refer to as Importance-Driven Turn-Bidding that treats turn-taking as a negotiative process. Each conversant bids for the turn based on the importance of the intended utterance, and Reinforcement Learning is used to indirectly learn this parameter. We find that Importance-Driven Turn-Bidding performs better than two current turntaking approaches in an artificial collaborative slot-filling domain. The negotiative nature of this model creates efficient dia- logues, and supports the improvement of mixed-initiative interaction.</p><p>4 0.75460941 <a title="187-lsi-4" href="./acl-2010-Towards_Relational_POMDPs_for_Adaptive_Dialogue_Management.html">239 acl-2010-Towards Relational POMDPs for Adaptive Dialogue Management</a></p>
<p>Author: Pierre Lison</p><p>Abstract: Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions ofthe state and action spaces.</p><p>5 0.73994362 <a title="187-lsi-5" href="./acl-2010-Demonstration_of_a_Prototype_for_a_Conversational_Companion_for_Reminiscing_about_Images.html">82 acl-2010-Demonstration of a Prototype for a Conversational Companion for Reminiscing about Images</a></p>
<p>Author: Yorick Wilks ; Roberta Catizone ; Alexiei Dingli ; Weiwei Cheng</p><p>Abstract: This paper describes an initial prototype demonstrator of a Companion, designed as a platform for novel approaches to the following: 1) The use of Information Extraction (IE) techniques to extract the content of incoming dialogue utterances after an Automatic Speech Recognition (ASR) phase, 2) The conversion of the input to Resource Descriptor Format (RDF) to allow the generation of new facts from existing ones, under the control of a Dialogue Manger (DM), that also has access to stored knowledge and to open knowledge accessed in real time from the web, all in RDF form, 3) A DM implemented as a stack and network virtual machine that models mixed initiative in dialogue control, and 4) A tuned dialogue act detector based on corpus evidence. The prototype platform was evaluated, and we describe this briefly; it is also designed to support more extensive forms of emotion detection carried by both speech and lexical content, as well as extended forms of machine learning.</p><p>6 0.67583781 <a title="187-lsi-6" href="./acl-2010-Now%2C_Where_Was_I%3F_Resumption_Strategies_for_an_In-Vehicle_Dialogue_System.html">179 acl-2010-Now, Where Was I? Resumption Strategies for an In-Vehicle Dialogue System</a></p>
<p>7 0.57171822 <a title="187-lsi-7" href="./acl-2010-Phrase-Based_Statistical_Language_Generation_Using_Graphical_Models_and_Active_Learning.html">194 acl-2010-Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning</a></p>
<p>8 0.56753802 <a title="187-lsi-8" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<p>9 0.51351541 <a title="187-lsi-9" href="./acl-2010-Non-Cooperation_in_Dialogue.html">178 acl-2010-Non-Cooperation in Dialogue</a></p>
<p>10 0.47608808 <a title="187-lsi-10" href="./acl-2010-Preferences_versus_Adaptation_during_Referring_Expression_Generation.html">199 acl-2010-Preferences versus Adaptation during Referring Expression Generation</a></p>
<p>11 0.43597585 <a title="187-lsi-11" href="./acl-2010-Automated_Planning_for_Situated_Natural_Language_Generation.html">35 acl-2010-Automated Planning for Situated Natural Language Generation</a></p>
<p>12 0.4187369 <a title="187-lsi-12" href="./acl-2010-Growing_Related_Words_from_Seed_via_User_Behaviors%3A_A_Re-Ranking_Based_Approach.html">129 acl-2010-Growing Related Words from Seed via User Behaviors: A Re-Ranking Based Approach</a></p>
<p>13 0.41786382 <a title="187-lsi-13" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>14 0.41435668 <a title="187-lsi-14" href="./acl-2010-Decision_Detection_Using_Hierarchical_Graphical_Models.html">81 acl-2010-Decision Detection Using Hierarchical Graphical Models</a></p>
<p>15 0.41042981 <a title="187-lsi-15" href="./acl-2010-Using_Speech_to_Reply_to_SMS_Messages_While_Driving%3A_An_In-Car_Simulator_User_Study.html">254 acl-2010-Using Speech to Reply to SMS Messages While Driving: An In-Car Simulator User Study</a></p>
<p>16 0.38261503 <a title="187-lsi-16" href="./acl-2010-Beetle_II%3A_A_System_for_Tutoring_and_Computational_Linguistics_Experimentation.html">47 acl-2010-Beetle II: A System for Tutoring and Computational Linguistics Experimentation</a></p>
<p>17 0.37005794 <a title="187-lsi-17" href="./acl-2010-Learning_to_Follow_Navigational_Directions.html">168 acl-2010-Learning to Follow Navigational Directions</a></p>
<p>18 0.34980145 <a title="187-lsi-18" href="./acl-2010-The_Impact_of_Interpretation_Problems_on_Tutorial_Dialogue.html">227 acl-2010-The Impact of Interpretation Problems on Tutorial Dialogue</a></p>
<p>19 0.34935287 <a title="187-lsi-19" href="./acl-2010-Classification_of_Feedback_Expressions_in_Multimodal_Data.html">58 acl-2010-Classification of Feedback Expressions in Multimodal Data</a></p>
<p>20 0.33213496 <a title="187-lsi-20" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.043), (25, 0.043), (39, 0.012), (41, 0.026), (42, 0.041), (44, 0.018), (59, 0.06), (72, 0.02), (73, 0.034), (76, 0.013), (78, 0.035), (83, 0.089), (84, 0.038), (94, 0.307), (98, 0.123)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75811654 <a title="187-lda-1" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>Author: Verena Rieser ; Oliver Lemon ; Xingkun Liu</p><p>Abstract: We present a novel approach to Information Presentation (IP) in Spoken Dialogue Systems (SDS) using a data-driven statistical optimisation framework for content planning and attribute selection. First we collect data in a Wizard-of-Oz (WoZ) experiment and use it to build a supervised model of human behaviour. This forms a baseline for measuring the performance of optimised policies, developed from this data using Reinforcement Learning (RL) methods. We show that the optimised policies significantly outperform the baselines in a variety of generation scenarios: while the supervised model is able to attain up to 87.6% of the possible reward on this task, the RL policies are significantly better in 5 out of 6 scenarios, gaining up to 91.5% of the total possible reward. The RL policies perform especially well in more complex scenarios. We are also the first to show that adding predictive “lower level” features (e.g. from the NLG realiser) is important for optimising IP strategies according to user preferences. This provides new insights into the nature of the IP problem for SDS.</p><p>2 0.55005908 <a title="187-lda-2" href="./acl-2010-Discriminative_Modeling_of_Extraction_Sets_for_Machine_Translation.html">87 acl-2010-Discriminative Modeling of Extraction Sets for Machine Translation</a></p>
<p>Author: John DeNero ; Dan Klein</p><p>Abstract: We present a discriminative model that directly predicts which set ofphrasal translation rules should be extracted from a sentence pair. Our model scores extraction sets: nested collections of all the overlapping phrase pairs consistent with an underlying word alignment. Extraction set models provide two principle advantages over word-factored alignment models. First, we can incorporate features on phrase pairs, in addition to word links. Second, we can optimize for an extraction-based loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments.</p><p>3 0.53467536 <a title="187-lda-3" href="./acl-2010-Learning_to_Adapt_to_Unknown_Users%3A_Referring_Expression_Generation_in_Spoken_Dialogue_Systems.html">167 acl-2010-Learning to Adapt to Unknown Users: Referring Expression Generation in Spoken Dialogue Systems</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon</p><p>Abstract: We present a data-driven approach to learn user-adaptive referring expression generation (REG) policies for spoken dialogue systems. Referring expressions can be difficult to understand in technical domains where users may not know the technical ‘jargon’ names of the domain entities. In such cases, dialogue systems must be able to model the user’s (lexical) domain knowledge and use appropriate referring expressions. We present a reinforcement learning (RL) framework in which the sys- tem learns REG policies which can adapt to unknown users online. Furthermore, unlike supervised learning methods which require a large corpus of expert adaptive behaviour to train on, we show that effective adaptive policies can be learned from a small dialogue corpus of non-adaptive human-machine interaction, by using a RL framework and a statistical user simulation. We show that in comparison to adaptive hand-coded baseline policies, the learned policy performs significantly better, with an 18.6% average increase in adaptation accuracy. The best learned policy also takes less dialogue time (average 1.07 min less) than the best hand-coded policy. This is because the learned policies can adapt online to changing evidence about the user’s domain expertise.</p><p>4 0.50332445 <a title="187-lda-4" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>5 0.50231737 <a title="187-lda-5" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>Author: Liang Huang ; Kenji Sagae</p><p>Abstract: Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming. We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging “equivalent” stacks based on feature values. Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce depen- dency parser with no loss in accuracy. Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.</p><p>6 0.49422824 <a title="187-lda-6" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>7 0.49412704 <a title="187-lda-7" href="./acl-2010-Combining_Orthogonal_Monolingual_and_Multilingual_Sources_of_Evidence_for_All_Words_WSD.html">62 acl-2010-Combining Orthogonal Monolingual and Multilingual Sources of Evidence for All Words WSD</a></p>
<p>8 0.49238339 <a title="187-lda-8" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>9 0.49229434 <a title="187-lda-9" href="./acl-2010-Reading_between_the_Lines%3A_Learning_to_Map_High-Level_Instructions_to_Commands.html">202 acl-2010-Reading between the Lines: Learning to Map High-Level Instructions to Commands</a></p>
<p>10 0.49107069 <a title="187-lda-10" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>11 0.49067944 <a title="187-lda-11" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>12 0.49028379 <a title="187-lda-12" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>13 0.48977643 <a title="187-lda-13" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>14 0.48953944 <a title="187-lda-14" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>15 0.4894678 <a title="187-lda-15" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>16 0.48943579 <a title="187-lda-16" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>17 0.48896331 <a title="187-lda-17" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>18 0.48844376 <a title="187-lda-18" href="./acl-2010-Complexity_Metrics_in_an_Incremental_Right-Corner_Parser.html">65 acl-2010-Complexity Metrics in an Incremental Right-Corner Parser</a></p>
<p>19 0.48819375 <a title="187-lda-19" href="./acl-2010-Finding_Cognate_Groups_Using_Phylogenies.html">116 acl-2010-Finding Cognate Groups Using Phylogenies</a></p>
<p>20 0.4880113 <a title="187-lda-20" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
