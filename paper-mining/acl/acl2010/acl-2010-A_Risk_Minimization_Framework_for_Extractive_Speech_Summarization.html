<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-14" href="#">acl2010-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</h1>
<br/><p>Source: <a title="acl-2010-14-pdf" href="http://aclweb.org/anthology//P/P10/P10-1009.pdf">pdf</a></p><p>Author: Shih-Hsiang Lin ; Berlin Chen</p><p>Abstract: In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. 1</p><p>Reference: <a title="acl-2010-14-reference" href="../acl2010_reference/acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. [sent-5, score-0.773]
</p><p>2 Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. [sent-6, score-1.196]
</p><p>3 1  Introduction  Automated summarization systems which enable user to quickly digest the important information conveyed by either a single or a cluster of documents are indispensible for managing the rapidly growing amount of textual information and multimedia content (Mani and Maybury, 1999). [sent-7, score-0.541]
</p><p>4 On the other hand, due to the maturity of text summarization, the research paradigm has been extended to speech summarization over the years (Furui et al. [sent-8, score-0.605]
</p><p>5 Speech summarization is expected to distill important information and remove redundant and incorrect information caused by recognition errors from spoken documents, enabling user to efficiently review spoken documents and understand the associated topics quickly. [sent-11, score-0.871]
</p><p>6 It would also be useful for improving the efficiency of a number of potential applications like retrieval and mining of large volumes of spoken documents. [sent-12, score-0.182]
</p><p>7 In abstractive summarization, a fluent and concise abstract that reflects the key concepts of  a document is generated, whereas in extractive summarization, the summary is usually formed by selecting salient sentences from the original document (Mani and Maybury, 1999). [sent-14, score-0.89]
</p><p>8 In addition to being extractive or abstractive, a summary may also be generated by considering several other aspects like being generic or query-oriented summarization, singledocument or multi-document summarization, and so forth. [sent-16, score-0.494]
</p><p>9 In this paper, we focus exclusively on generic, singledocument extractive summarization which forms the building block for many other summarization tasks. [sent-18, score-1.251]
</p><p>10 Aside from traditional ad-hoc extractive summarization methods (Mani and Maybury, 1999), machine-learning approaches with either super-  vised or unsupervised learning strategies have gained much attention and been applied with empirical success to many summarization tasks (Kupiec et al. [sent-19, score-1.291]
</p><p>11 ec A2s0s1o0ci Aatsiso nci faotrio Cno fomrp Cutoamtipounta lti Loin aglu Lisitnicgsu,ips atigces 79–87, without leveraging the dependence relationships among the sentences or the global structure of the document (Shen et al. [sent-28, score-0.231]
</p><p>12 Another line of thought attempts to conduct document summarization using unsupervised machine-learning approaches, getting around the need for manually labeled training data. [sent-30, score-0.703]
</p><p>13 Even though the performance of unsupervised summarizers is usually worse than that of supervised summarizers, their domain-independent and easy-to-implement properties still make them attractive. [sent-34, score-0.313]
</p><p>14 In this paper, we present a probabilistic summarization framework stemming from Bayes decision theory (Berger, 1985) for speech summarization. [sent-36, score-0.732]
</p><p>15 This framework can not only naturally integrate  the above-mentioned two modeling paradigms but also provide a flexible yet systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. [sent-37, score-0.184]
</p><p>16 Moreover, we also illustrate how the proposed framework can unify several existing summarization models. [sent-38, score-0.547]
</p><p>17 We start by reviewing related work on extractive summarization. [sent-40, score-0.241]
</p><p>18 In Section 3 we formulate the extractive summarization task as a risk minimization problem, followed by a detailed elucidation of the proposed methods in Section 4. [sent-41, score-0.951]
</p><p>19 2  Background  Speech summarization can be conducted using either supervised or unsupervised methods (Furui et al. [sent-44, score-0.623]
</p><p>20 In the following, we briefly review a few celebrated methods that have been applied to  extractive speech summarization tasks with good success. [sent-48, score-0.846]
</p><p>21 1 Supervised summarizers Extractive speech summarization can be treated as a two-class (positive/negative) classification problem. [sent-50, score-0.811]
</p><p>22 A spoken sentence Si is characterized by set of T indicative features Xi  xi1,  ,xiT , and they may include lexical features (Koumpis and Renals, 2000), structural features (Maskey and Hirschberg, 2003), acoustic features (Inoue et al. [sent-51, score-0.498]
</p><p>23 Another major shortcoming of these summarizers is that a set of handcrafted document-reference summary exemplars are required for training the summarizers; however, such summarizers tend to limit their generalization capability and might not be readily applicable for new tasks or domains. [sent-65, score-0.634]
</p><p>24 2  Unsupervised summarizers  The related work conducted along this direction usually relies on some heuristic rules or statistical evidences between each sentence and the document, avoiding the need of manually labeled  training data. [sent-67, score-0.319]
</p><p>25 For example, the vector space model (VSM) approach represents each sentence of a document and the document itself in vector space (Gong and Liu, 2001), and computes the relevance score between each sentence and the document (e. [sent-68, score-0.77]
</p><p>26 A natural extension is to represent each document or each sentence vector in a latent semantic space (Gong and Liu, 2001), instead of simply using the literal term information as that done by VSM. [sent-72, score-0.253]
</p><p>27 Document summarization thus relies on the global structural information conveyed by such conceptualized  network, rather than merely considering the local features of each node (sentence). [sent-74, score-0.586]
</p><p>28 However, due to the lack of documentsummary reference pairs, the performance of the unsupervised summarizers is usually worse than that of the supervised summarizers. [sent-75, score-0.313]
</p><p>29 Moreover, most of the unsupervised summarizers are constructed solely on the basis of the lexical information without considering other sources of information cues like discourse features, acoustic features, and so forth. [sent-76, score-0.348]
</p><p>30 3  A risk minimization framework extractive summarization  for  Extractive summarization can be viewed as a decision making process in which the summarizer attempts to select a representative subset of sentences or paragraphs from the original documents. [sent-77, score-1.702]
</p><p>31 The expected risk (or conditional risk) associated with taking decision ai is given by Rai | O θLai,θp θ|Odθ,  (1)  where pθ|O is the posterior probability of the state of nature being  given the observation O . [sent-80, score-0.25]
</p><p>32 Bayes decision theory states that the optimum decision can be made by contemplating each action ai , and then choosing the action for which the expected risk is minimum: a*  argminRai | O. [sent-81, score-0.397]
</p><p>33 Following the same spirit, we formulate the extractive summarization task as a Bayes risk minimization problem. [sent-83, score-0.951]
</p><p>34 Without loss of generality, let us denote   Π as one of possible selection strategies (or state of nature) which comprises a set of indicators used to address the importance of each sentence Si in a document D to be summarized. [sent-84, score-0.429]
</p><p>35 For example, it could be a set of binary indicators denoting whether a sentence should be selected as part of summary or not. [sent-86, score-0.271]
</p><p>36 Moreover, we refer to the k -th action ak as choosing the k -th selection strategy k , and the observation O as the document D to be summarized. [sent-88, score-0.292]
</p><p>37 As a result, the expected risk of a certain selection strategy k is given by Rk | D Lk,p  | Dd. [sent-89, score-0.212]
</p><p>38 Consequently,  (3)  the ultimate goal of extractive  summarization could be stated as the search of the best selection strategy from the space of all possible selection strategies that minimizes the expected risk defined as follows: *  |  argminRk D argminLk,p  | Dd. [sent-90, score-1.014]
</p><p>39 More concretely, we assume that the summary 81  sentences of a given document can be iteratively chosen (i. [sent-92, score-0.418]
</p><p>40 , one at each iteration) from the document until the aggregated summary reaches a predefined target summarization ratio. [sent-94, score-0.843]
</p><p>41 Therefore, the risk minimization framework can be reduced to S*  argm~inRSi | D~  aSrgSi  mDD~inSjD~LSi,SjPSj|D~,  (5)  where D~ denotes the remaining sentences that have not been selected into the sum~mary yet (i. [sent-97, score-0.345]
</p><p>42 (7)  By substituting (6) and (7) into (5), we obtain the following final selection strategy for extractive summarization:  S*arSgimD~inSjD~LSi,SjSmPD~PD~D~|S|SjmPPSSjm. [sent-104, score-0.324]
</p><p>43 As we will soon see, such a framework can be regarded as a generalization of several existing summarization methods. [sent-106, score-0.547]
</p><p>44 1  Sentence generative model  In order to estimate the sentence generative probability, we explore the language modeling (LM) approach, which has been introduced to a wide spectrum of IR tasks and demonstrated with good empirical success, to predict the sentence generative probability. [sent-113, score-0.306]
</p><p>45 In the LM approach, each sentence in a document can be simply regarded as a probabilistic generative model consisting of a unigram distribution (the so-called “bag-ofwords” assumption) for generating the document (Chen et al. [sent-114, score-0.468]
</p><p>46 2 Sentence prior model The sentence prior probability PSj  can be regarded as the likelihood of a sentence being important without seeing the whole document. [sent-122, score-0.168]
</p><p>47 It could be assumed uniformly distributed over sentences or estimated from a wide variety of factors, such as the lexical information, the structural  information or the inherent prosodic properties of a spoken sentence. [sent-123, score-0.278]
</p><p>48 Specifically, the prior probability PSj  can be approximated by:  PSjPXj|SpPXS j|SPPXSj |SPS , PXj  (10)  where PXj | S and | S are the likelihoods that a sentence Sj with features Xj are generated by the summary class S and the non-  sbreuivslmet ayr caPlhryp. [sent-127, score-0.303]
</p><p>49 3  Loss function  The loss function introduced in the proposed summarization framework is to measure the relationship between any pair of sentences. [sent-130, score-0.697]
</p><p>50 Intuitively, when a given sentence is more dissimilar from most of the other sentences, it may incur higher loss as it is taken as the representative sentence (or summary sentence) to represent the main theme embedded in the other ones. [sent-131, score-0.491]
</p><p>51 , the product of the term frequency (TF) and inverse document frequency (IDF) scores, associated with an index term wt in sentence Si . [sent-136, score-0.253]
</p><p>52 (11)  thioenOsneLcneStein,hSce jspehrnaitovern mcbeo degnel peProaStpijevralynmdoetdsheiml oPastDe~df|,uSntjhce-,  summary sentences can be selected iteratively by (8) according to a predefined target summarization ratio. [sent-139, score-0.736]
</p><p>53 However, as can be seen from (8), a new summary sentence is selected without considering the redundant information that is also contained in the already selected summary sentences. [sent-140, score-0.488]
</p><p>54 4  Relation to other summarization models  In this subsection, we briefly illustrate the relationship between our proposed summarization framework and a few existing summarization approaches. [sent-143, score-1.569]
</p><p>55 We start by considering a special case where a 0-1 loss function is used in (8), namely, the loss function will take value 0 if the two sentences are identical, and 1 otherwise. [sent-144, score-0.296]
</p><p>56 Then, (8) can be alternatively represented by  S*arSgimD~inSjD~,SjSiP~DP~D|~S|jSPmSPjSm  argSimD~axSmDP~PD~D|~S|SiSmmPDSPiSm,  (13)  which actually provides a natural integration of the supervised and unsupervised summarizers (Lin et al. [sent-145, score-0.313]
</p><p>57 If we further assume the prior probability PSj  is uniformly distributed, the important (or summary) sentence selection problem has now been reduced to the problem of measuring the document-likelihood or the between the document and the sentence. [sent-147, score-0.331]
</p><p>58 1  Experimental setup Data  The summarization dataset used in this research is a widely used broadcast news corpus collected by the Academia Sinica and the Public Television Service Foundation of Taiwan between November 2001 and April 2003 (Wang et al. [sent-150, score-0.606]
</p><p>59 uments compiled between November 2001 and August 2002 was reserved for the summarization experiments. [sent-158, score-0.487]
</p><p>60 Three subjects were asked to create summaries  of the 205 spoken documents for the summarization experiments as references (the gold standard) for evaluation. [sent-159, score-0.726]
</p><p>61 The summaries were generated by ranking the sentences in the reference transcript of a spoken document by importance without assigning a score to each sentence. [sent-160, score-0.447]
</p><p>62 The average Chinese character error rate (CER) obtained for the 205 spoken documents was about 35%. [sent-161, score-0.2]
</p><p>63 2  Performance evaluation  For the assessment of summarization performance, we adopted the widely used ROUGE measure (Lin, 2004) because of its higher corre-  lation with human judgments. [sent-164, score-0.487]
</p><p>64 It evaluates the quality of the summarization by counting the number of overlapping units, such as N-grams, longest common subsequences or skip-bigram, between the automatic summary and a set of reference summaries. [sent-165, score-0.674]
</p><p>65 The summarization ratio, defined as the ratio of the number of words in the automatic (or manual) summary to that in the reference transcript of a spoken document, was set to 10% in this research. [sent-168, score-0.851]
</p><p>66 3  Features for supervised summarizers  We take BC as the representative supervised summarizer to study in this paper. [sent-173, score-0.401]
</p><p>67 The input to BC consists of a set of 28 indicative features used to characterize a spoken sentence, including the structural features, the lexical features, the acoustic features and the relevance feature. [sent-174, score-0.445]
</p><p>68 For each kind of acoustic features, the minimum, maximum, mean, difference value and mean difference value of a spoken sentence are extracted. [sent-175, score-0.295]
</p><p>69 The difference value is defined as the difference between the minimum and maximum values of the spoken sentence, while the mean difference value is defined as the mean difference between a sentence and its previous sentence. [sent-176, score-0.23]
</p><p>70 Finally, the relevance feature (VSM score) is use to measure the degree of relevance for a sentence to the whole document (Gong and Liu, 2001). [sent-177, score-0.443]
</p><p>71 1  Baseline experiments  In the first set of experiments,  we evaluate the  baseline performance of the LM and BC summarizers  (cf. [sent-180, score-0.206]
</p><p>72 338 Table 4: The results achieved by several methods derived from the proposed summarization framework. [sent-251, score-0.487]
</p><p>73 It is also worth  that TD denotes  the summarization  results obtained based on manual transcripts the spoken documents  of  while SD denotes the re-  sults using the speech recognition transcripts which may contain speech recognition errors and sentence boundary detection errors. [sent-253, score-1.259]
</p><p>74 In this research, sentence boundaries were determined by speech pauses. [sent-254, score-0.202]
</p><p>75 For the TD case, the acoustic features were obtained by aligning the manual transcripts to their spoken documents counterpart by performing word-level forced alignment. [sent-255, score-0.385]
</p><p>76 Consequently, we can align the ASR transcripts of the summary sentences to their respective audio segments to  obtain the correct (manual) transcripts for the summarization performance evaluation (i. [sent-258, score-0.957]
</p><p>77 First, there are significant performance gaps between summarization using the manual transcripts and the erroneous speech recognition transcripts. [sent-262, score-0.731]
</p><p>78 One possible explanation is that the erroneous speech recognition transcripts of spoken sentences would probably carry wrong information and thus deviate somewhat from representing the true theme of the spoken document. [sent-264, score-0.632]
</p><p>79 One is that BC is trained with the handcrafted document-summary sentence labels in the development set while LM is instead conducted in a purely unsupervised manner. [sent-271, score-0.195]
</p><p>80 Another is that BC utilizes a rich set of features to characterize a given spoken sentence while LM is constructed solely on the basis of the lexical (uni-  gram) information. [sent-272, score-0.262]
</p><p>81 2  Experiments on the proposed methods  We then turn our attention to investigate the utility of several methods deduced from our proposed summarization framework. [sent-274, score-0.531]
</p><p>82 It can be found that 85  MMR delivers higher summarization performance than SIM (especially for the SD case), which in turn verifies the merit of incorporating the MMR concept into the proposed framework  for extractive summarization. [sent-280, score-0.788]
</p><p>83 On the other hand, the performance gap between the TD and SD cases are reduced to a good extent by using the proposed summarization framework. [sent-283, score-0.487]
</p><p>84 The importance of a given sentence is thus considered from two angles: 1) the relationship between a sentence and the whole document, and 2) the relationship between the sentence and the other individual sentences. [sent-285, score-0.348]
</p><p>85 We can see that the additional consideration of the sentence-sentence relationship appears to be beneficial as compared to that only considering the document-sentence relevance information (cf. [sent-287, score-0.173]
</p><p>86 It should be noted that the LEAD-based method simply extracts the first few sentences in a document as the summary. [sent-293, score-0.231]
</p><p>87 To our surprise, CRF does not provide superior results as compared to the other summarization methods. [sent-294, score-0.487]
</p><p>88 One possible explanation is that the structural evidence of the spoken documents in the test set is not strong enough for CRF to show its advantage of modeling the local structural information among sen-  tences. [sent-295, score-0.274]
</p><p>89 291  Table 5: The results achieved by four conventional summarization methods. [sent-320, score-0.487]
</p><p>90 This somewhat reflects the importance of capturing the global relationship for the sentences in the spoken document to be summarized. [sent-322, score-0.425]
</p><p>91 As compared to the results shown in the “BC” part of Table 4, we can see that our proposed methods significantly outperform all the conventional summarization methods compared in this paper, especially for the SD case. [sent-323, score-0.487]
</p><p>92 7  Conclusions and future work  We have proposed a risk minimization framework for extractive speech summarization, which enjoys several advantages. [sent-324, score-0.642]
</p><p>93 We have also presented a simple yet effective implementation that selects the summary sentences in an iterative manner. [sent-325, score-0.249]
</p><p>94 Experimental results demonstrate that the methods deduced from such a framework can yield substantial improvements over several popular summarization methods compared in this paper. [sent-326, score-0.591]
</p><p>95 Word topic models for spoken  document retrieval and transcription. [sent-335, score-0.351]
</p><p>96 A probabilistic generative framework for extractive broadcast news speech summarization. [sent-344, score-0.584]
</p><p>97 Generic text summarization using relevance measure and latent semantic analysis. [sent-379, score-0.582]
</p><p>98 Improvement of speech summarization using prosodic information, In Proc. [sent-384, score-0.605]
</p><p>99 A comparative study of probabilistic ranking models for Chinese spoken document summarization. [sent-413, score-0.315]
</p><p>100 Hybrids of supervised and unsupervised models for extractive speech summarization. [sent-417, score-0.466]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('summarization', 0.487), ('sj', 0.264), ('extractive', 0.241), ('bc', 0.206), ('summarizers', 0.206), ('summary', 0.187), ('document', 0.169), ('spoken', 0.146), ('sd', 0.146), ('risk', 0.129), ('td', 0.125), ('mmr', 0.124), ('speech', 0.118), ('loss', 0.102), ('gong', 0.1), ('relevance', 0.095), ('minimization', 0.094), ('lm', 0.088), ('transcripts', 0.088), ('si', 0.087), ('sentence', 0.084), ('maybury', 0.083), ('broadcast', 0.082), ('summarizer', 0.075), ('kupiec', 0.074), ('lexrank', 0.074), ('mani', 0.074), ('rouge', 0.073), ('decision', 0.067), ('acoustic', 0.065), ('chen', 0.065), ('abstractive', 0.062), ('sentences', 0.062), ('supervised', 0.06), ('crf', 0.06), ('framework', 0.06), ('vsm', 0.059), ('berlin', 0.057), ('bayes', 0.055), ('documents', 0.054), ('ai', 0.054), ('sm', 0.053), ('xj', 0.052), ('sim', 0.051), ('furui', 0.05), ('zhai', 0.049), ('relationship', 0.048), ('unsupervised', 0.047), ('lin', 0.046), ('generative', 0.046), ('selection', 0.045), ('audio', 0.045), ('deduced', 0.044), ('radev', 0.043), ('argminr', 0.041), ('arsgi', 0.041), ('fattah', 0.041), ('ferrier', 0.041), ('gmm', 0.041), ('goel', 0.041), ('inoue', 0.041), ('kolcz', 0.041), ('koumpis', 0.041), ('rogue', 0.041), ('erkan', 0.04), ('taiwan', 0.04), ('action', 0.04), ('summaries', 0.039), ('indicative', 0.038), ('recognition', 0.038), ('strategy', 0.038), ('news', 0.037), ('structural', 0.037), ('singledocument', 0.036), ('maskey', 0.036), ('tarau', 0.036), ('retrieval', 0.036), ('handcrafted', 0.035), ('theme', 0.034), ('berger', 0.033), ('textrank', 0.033), ('inherit', 0.033), ('sigir', 0.033), ('uniformly', 0.033), ('features', 0.032), ('md', 0.031), ('residual', 0.031), ('conroy', 0.031), ('byrne', 0.031), ('november', 0.031), ('transcript', 0.031), ('liu', 0.031), ('shen', 0.03), ('considering', 0.03), ('mihalcea', 0.03), ('sp', 0.03), ('mckeown', 0.03), ('conducted', 0.029), ('strategies', 0.029), ('js', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="14-tfidf-1" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<p>Author: Shih-Hsiang Lin ; Berlin Chen</p><p>Abstract: In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. 1</p><p>2 0.36868665 <a title="14-tfidf-2" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>Author: Xiaojun Wan ; Huiying Li ; Jianguo Xiao</p><p>Abstract: Cross-language document summarization is a task of producing a summary in one language for a document set in a different language. Existing methods simply use machine translation for document translation or summary translation. However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in readability and content. In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process. First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process. Finally, the English sentences with high translation quality and high informativeness are selected and translated to form the Chinese summary. Experimental results demonstrate the effectiveness and usefulness of the proposed approach. 1</p><p>3 0.3090266 <a title="14-tfidf-3" href="./acl-2010-A_New_Approach_to_Improving_Multilingual_Summarization_Using_a_Genetic_Algorithm.html">11 acl-2010-A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm</a></p>
<p>Author: Marina Litvak ; Mark Last ; Menahem Friedman</p><p>Abstract: Automated summarization methods can be defined as “language-independent,” if they are not based on any languagespecific knowledge. Such methods can be used for multilingual summarization defined by Mani (2001) as “processing several languages, with summary in the same language as input.” In this paper, we introduce MUSE, a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages—English and Hebrew—and evaluated its performance with ROUGE-1 Recall vs. state- of-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank1) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.</p><p>4 0.22970699 <a title="14-tfidf-4" href="./acl-2010-Wrapping_up_a_Summary%3A_From_Representation_to_Generation.html">264 acl-2010-Wrapping up a Summary: From Representation to Generation</a></p>
<p>Author: Josef Steinberger ; Marco Turchi ; Mijail Kabadjov ; Ralf Steinberger ; Nello Cristianini</p><p>Abstract: The main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries. This is motivated by empirical evidence from TAC 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries. We report encouraging preliminary results comparable to those attained by participating systems at TAC 2009.</p><p>5 0.22408526 <a title="14-tfidf-5" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>6 0.18828186 <a title="14-tfidf-6" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>7 0.18741615 <a title="14-tfidf-7" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>8 0.17448471 <a title="14-tfidf-8" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>9 0.15446056 <a title="14-tfidf-9" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>10 0.12681915 <a title="14-tfidf-10" href="./acl-2010-Generating_Image_Descriptions_Using_Dependency_Relational_Patterns.html">124 acl-2010-Generating Image Descriptions Using Dependency Relational Patterns</a></p>
<p>11 0.11762536 <a title="14-tfidf-11" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>12 0.11185275 <a title="14-tfidf-12" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>13 0.098190956 <a title="14-tfidf-13" href="./acl-2010-Hunting_for_the_Black_Swan%3A_Risk_Mining_from_Text.html">138 acl-2010-Hunting for the Black Swan: Risk Mining from Text</a></p>
<p>14 0.087099545 <a title="14-tfidf-14" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>15 0.084283292 <a title="14-tfidf-15" href="./acl-2010-Domain_Adaptation_of_Maximum_Entropy_Language_Models.html">91 acl-2010-Domain Adaptation of Maximum Entropy Language Models</a></p>
<p>16 0.083365962 <a title="14-tfidf-16" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>17 0.083200842 <a title="14-tfidf-17" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>18 0.081780165 <a title="14-tfidf-18" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>19 0.080259144 <a title="14-tfidf-19" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>20 0.079028539 <a title="14-tfidf-20" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.247), (1, 0.066), (2, -0.185), (3, -0.009), (4, -0.01), (5, -0.04), (6, -0.021), (7, -0.48), (8, -0.03), (9, -0.049), (10, 0.023), (11, -0.084), (12, -0.099), (13, 0.016), (14, -0.085), (15, -0.096), (16, -0.046), (17, -0.013), (18, -0.011), (19, 0.047), (20, 0.056), (21, 0.013), (22, -0.029), (23, 0.015), (24, 0.006), (25, -0.005), (26, -0.02), (27, -0.003), (28, 0.08), (29, 0.047), (30, -0.052), (31, 0.045), (32, 0.01), (33, -0.018), (34, -0.007), (35, 0.04), (36, -0.093), (37, 0.083), (38, -0.023), (39, 0.029), (40, 0.009), (41, -0.112), (42, 0.083), (43, 0.006), (44, 0.046), (45, 0.048), (46, -0.049), (47, 0.065), (48, -0.056), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97708744 <a title="14-lsi-1" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<p>Author: Shih-Hsiang Lin ; Berlin Chen</p><p>Abstract: In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. 1</p><p>2 0.89211816 <a title="14-lsi-2" href="./acl-2010-A_New_Approach_to_Improving_Multilingual_Summarization_Using_a_Genetic_Algorithm.html">11 acl-2010-A New Approach to Improving Multilingual Summarization Using a Genetic Algorithm</a></p>
<p>Author: Marina Litvak ; Mark Last ; Menahem Friedman</p><p>Abstract: Automated summarization methods can be defined as “language-independent,” if they are not based on any languagespecific knowledge. Such methods can be used for multilingual summarization defined by Mani (2001) as “processing several languages, with summary in the same language as input.” In this paper, we introduce MUSE, a languageindependent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. We tested our methodology on two languages—English and Hebrew—and evaluated its performance with ROUGE-1 Recall vs. state- of-the-art extractive summarization approaches. Our results show that MUSE performs better than the best known multilingual approach (TextRank1) in both languages. Moreover, our experimental results on a bilingual (English and Hebrew) document collection suggest that MUSE does not need to be retrained on each language and the same model can be used across at least two different languages.</p><p>3 0.87403828 <a title="14-lsi-3" href="./acl-2010-Wrapping_up_a_Summary%3A_From_Representation_to_Generation.html">264 acl-2010-Wrapping up a Summary: From Representation to Generation</a></p>
<p>Author: Josef Steinberger ; Marco Turchi ; Mijail Kabadjov ; Ralf Steinberger ; Nello Cristianini</p><p>Abstract: The main focus of this work is to investigate robust ways for generating summaries from summary representations without recurring to simple sentence extraction and aiming at more human-like summaries. This is motivated by empirical evidence from TAC 2009 data showing that human summaries contain on average more and shorter sentences than the system summaries. We report encouraging preliminary results comparable to those attained by participating systems at TAC 2009.</p><p>4 0.82047659 <a title="14-lsi-4" href="./acl-2010-Cross-Language_Document_Summarization_Based_on_Machine_Translation_Quality_Prediction.html">77 acl-2010-Cross-Language Document Summarization Based on Machine Translation Quality Prediction</a></p>
<p>Author: Xiaojun Wan ; Huiying Li ; Jianguo Xiao</p><p>Abstract: Cross-language document summarization is a task of producing a summary in one language for a document set in a different language. Existing methods simply use machine translation for document translation or summary translation. However, current machine translation services are far from satisfactory, which results in that the quality of the cross-language summary is usually very poor, both in readability and content. In this paper, we propose to consider the translation quality of each sentence in the English-to-Chinese cross-language summarization process. First, the translation quality of each English sentence in the document set is predicted with the SVM regression method, and then the quality score of each sentence is incorporated into the summarization process. Finally, the English sentences with high translation quality and high informativeness are selected and translated to form the Chinese summary. Experimental results demonstrate the effectiveness and usefulness of the proposed approach. 1</p><p>5 0.7854833 <a title="14-lsi-5" href="./acl-2010-A_Hybrid_Hierarchical_Model_for_Multi-Document_Summarization.html">8 acl-2010-A Hybrid Hierarchical Model for Multi-Document Summarization</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur</p><p>Abstract: Scoring sentences in documents given abstract summaries created by humans is important in extractive multi-document summarization. In this paper, we formulate extractive summarization as a two step learning problem building a generative model for pattern discovery and a regression model for inference. We calculate scores for sentences in document clusters based on their latent characteristics using a hierarchical topic model. Then, using these scores, we train a regression model based on the lexical and structural characteristics of the sentences, and use the model to score sentences of new documents to form a summary. Our system advances current state-of-the-art improving ROUGE scores by ∼7%. Generated summaries are less rbeydu ∼n7d%an.t a Gnedn more dc sohuemremnatr bieasse adre upon manual quality evaluations.</p><p>6 0.73947978 <a title="14-lsi-6" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>7 0.68862313 <a title="14-lsi-7" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>8 0.64044023 <a title="14-lsi-8" href="./acl-2010-Identifying_Non-Explicit_Citing_Sentences_for_Citation-Based_Summarization..html">140 acl-2010-Identifying Non-Explicit Citing Sentences for Citation-Based Summarization.</a></p>
<p>9 0.61807287 <a title="14-lsi-9" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>10 0.56897801 <a title="14-lsi-10" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>11 0.55564201 <a title="14-lsi-11" href="./acl-2010-Generating_Image_Descriptions_Using_Dependency_Relational_Patterns.html">124 acl-2010-Generating Image Descriptions Using Dependency Relational Patterns</a></p>
<p>12 0.52699018 <a title="14-lsi-12" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>13 0.47253895 <a title="14-lsi-13" href="./acl-2010-Plot_Induction_and_Evolutionary_Search_for_Story_Generation.html">196 acl-2010-Plot Induction and Evolutionary Search for Story Generation</a></p>
<p>14 0.4647457 <a title="14-lsi-14" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>15 0.45603278 <a title="14-lsi-15" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>16 0.4253062 <a title="14-lsi-16" href="./acl-2010-Learning_Word-Class_Lattices_for_Definition_and_Hypernym_Extraction.html">166 acl-2010-Learning Word-Class Lattices for Definition and Hypernym Extraction</a></p>
<p>17 0.38961795 <a title="14-lsi-17" href="./acl-2010-How_Spoken_Language_Corpora_Can_Refine_Current_Speech_Motor_Training_Methodologies.html">137 acl-2010-How Spoken Language Corpora Can Refine Current Speech Motor Training Methodologies</a></p>
<p>18 0.3868627 <a title="14-lsi-18" href="./acl-2010-Correcting_Errors_in_Speech_Recognition_with_Articulatory_Dynamics.html">74 acl-2010-Correcting Errors in Speech Recognition with Articulatory Dynamics</a></p>
<p>19 0.38319743 <a title="14-lsi-19" href="./acl-2010-Intelligent_Selection_of_Language_Model_Training_Data.html">151 acl-2010-Intelligent Selection of Language Model Training Data</a></p>
<p>20 0.37899923 <a title="14-lsi-20" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.012), (14, 0.028), (25, 0.05), (33, 0.271), (39, 0.01), (42, 0.014), (44, 0.01), (59, 0.084), (71, 0.026), (73, 0.052), (76, 0.013), (78, 0.022), (80, 0.012), (83, 0.088), (84, 0.034), (97, 0.011), (98, 0.189)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95419908 <a title="14-lda-1" href="./acl-2010-String_Extension_Learning.html">217 acl-2010-String Extension Learning</a></p>
<p>Author: Jeffrey Heinz</p><p>Abstract: This paper provides a unified, learningtheoretic analysis of several learnable classes of languages discussed previously in the literature. The analysis shows that for these classes an incremental, globally consistent, locally conservative, set-driven learner always exists. Additionally, the analysis provides a recipe for constructing new learnable classes. Potential applications include learnable models for aspects of natural language and cognition.</p><p>2 0.87500489 <a title="14-lda-2" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>Author: Yoshihide Kato ; Shigeki Matsubara</p><p>Abstract: This paper proposes a method of correcting annotation errors in a treebank. By using a synchronous grammar, the method transforms parse trees containing annotation errors into the ones whose errors are corrected. The synchronous grammar is automatically induced from the treebank. We report an experimental result of applying our method to the Penn Treebank. The result demonstrates that our method corrects syntactic annotation errors with high precision.</p><p>same-paper 3 0.85492641 <a title="14-lda-3" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<p>Author: Shih-Hsiang Lin ; Berlin Chen</p><p>Abstract: In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches. 1</p><p>4 0.84677017 <a title="14-lda-4" href="./acl-2010-Open_Information_Extraction_Using_Wikipedia.html">185 acl-2010-Open Information Extraction Using Wikipedia</a></p>
<p>Author: Fei Wu ; Daniel S. Weld</p><p>Abstract: Information-extraction (IE) systems seek to distill semantic relations from naturallanguage text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? This paper presents WOE, an open IE system which improves dramatically on TextRunner’s precision and recall. The key to WOE’s performance is a novel form of self-supervised learning for open extractors using heuris— tic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE’s extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.</p><p>5 0.67217755 <a title="14-lda-5" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>Author: Peter Prettenhofer ; Benno Stein</p><p>Abstract: We present a new approach to crosslanguage text classification that builds on structural correspondence learning, a recently proposed theory for domain adaptation. The approach uses unlabeled documents, along with a simple word translation oracle, in order to induce taskspecific, cross-lingual word correspondences. We report on analyses that reveal quantitative insights about the use of unlabeled data and the complexity of interlanguage correspondence modeling. We conduct experiments in the field of cross-language sentiment classification, employing English as source language, and German, French, and Japanese as target languages. The results are convincing; they demonstrate both the robustness and the competitiveness of the presented ideas.</p><p>6 0.66918063 <a title="14-lda-6" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>7 0.66824508 <a title="14-lda-7" href="./acl-2010-Automatic_Collocation_Suggestion_in_Academic_Writing.html">36 acl-2010-Automatic Collocation Suggestion in Academic Writing</a></p>
<p>8 0.66800433 <a title="14-lda-8" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>9 0.66567034 <a title="14-lda-9" href="./acl-2010-Optimal_Rank_Reduction_for_Linear_Context-Free_Rewriting_Systems_with_Fan-Out_Two.html">186 acl-2010-Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two</a></p>
<p>10 0.66449887 <a title="14-lda-10" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>11 0.66384327 <a title="14-lda-11" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>12 0.66328096 <a title="14-lda-12" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>13 0.66097331 <a title="14-lda-13" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>14 0.6604954 <a title="14-lda-14" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>15 0.65984094 <a title="14-lda-15" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>16 0.65728879 <a title="14-lda-16" href="./acl-2010-How_Many_Words_Is_a_Picture_Worth%3F_Automatic_Caption_Generation_for_News_Images.html">136 acl-2010-How Many Words Is a Picture Worth? Automatic Caption Generation for News Images</a></p>
<p>17 0.65503347 <a title="14-lda-17" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>18 0.65450901 <a title="14-lda-18" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>19 0.65264034 <a title="14-lda-19" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>20 0.65182006 <a title="14-lda-20" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
