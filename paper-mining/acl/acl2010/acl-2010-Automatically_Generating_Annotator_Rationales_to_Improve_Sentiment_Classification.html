<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-42" href="#">acl2010-42</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</h1>
<br/><p>Source: <a title="acl-2010-42-pdf" href="http://aclweb.org/anthology//P/P10/P10-2062.pdf">pdf</a></p><p>Author: Ainur Yessenalina ; Yejin Choi ; Claire Cardie</p><p>Abstract: One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</p><p>Reference: <a title="acl-2010-42-reference" href="../acl2010_reference/acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Automatically generating annotator rationales to improve sentiment classification Ainur Yessenalina Yejin Choi Claire Cardie Department of Computer Science, Cornell University, Ithaca NY, 14853 USA {ainur , ychoi , cardie} @ c s . [sent-1, score-1.235]
</p><p>2 edu l  Abstract One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. [sent-3, score-0.309]
</p><p>3 Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al. [sent-4, score-0.297]
</p><p>4 We explore methods to automatically generate annotator rationales for document-level sentiment classification. [sent-6, score-1.232]
</p><p>5 Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales. [sent-7, score-0.936]
</p><p>6 1 Introduction  One of the central challenges in sentiment-based text categorization is that not every portion of a given document is equally informative for inferring its overall sentiment (e. [sent-8, score-0.309]
</p><p>7 (2007) address this problem by asking human annotators to mark (at least some of) the relevant text spans that support each document-level sentiment decision. [sent-12, score-0.313]
</p><p>8 The text spans of these “rationales” are then used to construct additional training examples that can guide the learning algorithm toward better categorization models. [sent-13, score-0.104]
</p><p>9 But could we perhaps enjoy the performance gains ofrationale-enhanced learning models without any additional human effort whatsoever (beyond the document-level sentiment label)? [sent-14, score-0.268]
</p><p>10 In this paper, we explore a number of methods  to automatically generate rationales for documentlevel sentiment classification. [sent-16, score-1.145]
</p><p>11 In particular, we investigate the use of off-the-shelf sentiment analysis components and lexicons for this purpose. [sent-17, score-0.229]
</p><p>12 Our approaches for generating annotatorrationales can be viewed as mostly unsupervisedin that we do not require manually annotated rationales for training. [sent-18, score-0.863]
</p><p>13 Rather unexpectedly, our empirical results show that automatically generated rationales (91. [sent-19, score-0.897]
</p><p>14 In addition, complementing the human annotator rationales with automatic rationales boosts the performance even further for this domain, achieving 92. [sent-22, score-1.899]
</p><p>15 (2007) that allows the incorporation of rationales (Section 2). [sent-27, score-0.863]
</p><p>16 We next introduce three methods for the automatic generation of rationales (Section 3). [sent-28, score-0.876]
</p><p>17 (2007) first introduced the notion of annotator rationales text spans highlighted by human annotators as support or evidence for each document-level sentiment decision. [sent-31, score-1.297]
</p><p>18 These rationales, of course, are only useful if the sentiment categorization algorithm can be extended to exploit the rationales effectively. [sent-32, score-1.121]
</p><p>19 Let xi be movie review i, and let { r~ij} be the  µ  set of annotator rationales that support the positive or negative sentiment decision for xi. [sent-37, score-1.45]
</p><p>20 For each such rationale rij in the set, construct a contrastive training example vij, by removing the text span associated with the rationale rij from the original review xi. [sent-38, score-0.245]
</p><p>21 Intuitively, the contrastive example vij should not be as informative to the learning algorithm as the original review xi, since one of the supporting regions identified by the human annotator has been deleted. [sent-39, score-0.417]
</p><p>22 That is, the correct learned model should be less confident of its classification ofa contrastive example vs. [sent-40, score-0.1]
</p><p>23 ’s (2007) contrastive learning method to incorporate rationales for documentlevel sentiment categorization. [sent-50, score-1.204]
</p><p>24 3 Automatically Generating Rationales Our goal in the current work, is to generate annotator rationales automatically. [sent-51, score-0.984]
</p><p>25 For this, we rely on the following two assumptions: (1) Regions marked as annotator rationales are more subjective than unmarked regions. [sent-52, score-1.061]
</p><p>26 (2) The sentiment of each annotatorrationale coincides with the document-level sentiment. [sent-53, score-0.258]
</p><p>27 (2007) work: annotators were asked only to mark a few rationales, leaving other (also subjective) rationale sections unmarked. [sent-55, score-0.057]
</p><p>28 But it is important to include as there can be subjective regions with seemingly conflicting sentiment in the same document (Pang et al. [sent-57, score-0.402]
</p><p>29 For instance, an author for a movie re-  view might express a positive sentiment toward the movie, while also discussing a negative sentiment toward one of the fictional characters appearing in the movie. [sent-59, score-0.659]
</p><p>30 This implies that not all subjective regions will be relevant for the documentlevel sentiment classification rather only those regions whose polarity matches that of the document should be considered. [sent-60, score-0.563]
</p><p>31 In order to extract regions that satisfy the above assumptions, we first look for subjective regions in each document, then filter out those regions that exhibit a sentiment value (i. [sent-61, score-0.469]
</p><p>32 Assumption 2 is important as there can be subjective regions with seemingly conflicting sentiment in the same document (Pang et al. [sent-64, score-0.402]
</p><p>33 Because our ultimate goal is to reduce human annotation effort as much as possible, we do not employ supervised learning methods to directly learn to identify good rationales from humanannotated rationales. [sent-66, score-0.937]
</p><p>34 Instead, we opt for methods that make use of only the document-level sentiment and off-the-shelf utilities that were trained —  for slightly different sentiment classification tasks using a corpus from a different domain and of a different genre. [sent-67, score-0.497]
</p><p>35 Although such utilities might not be optimal for our task, we hoped that these basic resources from the research community would constitute an adequate source of sentiment information for our purposes. [sent-68, score-0.246]
</p><p>36 1 In particular, OpinionFinder identifies phrases expressing positive or negative opinions. [sent-73, score-0.057]
</p><p>37 Because OpinionFinder models the task as a word-based classification problem rather than a sequence tagging task, most of the identified opinion phrases consist of a single word. [sent-74, score-0.048]
</p><p>38 In general, such short text spans cannot fully incorporate the contextual information relevant to the detection of subjective language (Wilson et al. [sent-75, score-0.092]
</p><p>39 There-  fore, we conjecture that good rationales should extend beyond short phrases. [sent-77, score-0.891]
</p><p>40 In addition, to be consistent with our second operating assumption, we keep only those sentences whose polarity coincides with the document-level polarity. [sent-79, score-0.118]
</p><p>41 In sentences where OpinionFinder marks multiple opinion words with opposite polarities we perform a simple voting if words with positive (or negative) polarity dominate, then we consider the entire sentence as positive (or negative). [sent-80, score-0.178]
</p><p>42 3 Therefore, we next consider an approach that does not rely on supervised learning techniques but instead explores the use of a manually constructed polarity lexicon. [sent-85, score-0.076]
</p><p>43 Each entry is assigned one of three polarity values: positive, negative, neutral. [sent-88, score-0.076]
</p><p>44 We construct rationales from the polarity lexicon for every instance of positive and negative words in the lexicon that appear in the training corpus. [sent-89, score-1.007]
</p><p>45 2This conjecture is indirectly confirmed by the fact that human-annotated rationales are rarely a single word. [sent-94, score-0.878]
</p><p>46 3It is worthwhile to note that OpinionFinderis trained on a newswire corpus whose prevailing sentiment is known to be negative (Wiebe et al. [sent-95, score-0.273]
</p><p>47 Furthermore, OpinionFinder is trained for a task (word-level sentiment classification) that is different from marking annotator rationales (sequence tagging or text segmentation). [sent-97, score-1.213]
</p><p>48 We retain as rationales only those sentences whose polarity coincides with the document-level polarity as determined via the voting scheme of Section 3. [sent-99, score-1.073]
</p><p>49 3  Random Selection  Finally,  we generate  annotator  rationales  ran-  domly, selecting 25% of the sentences from each document4 and treating each as a separate rationale. [sent-102, score-0.984]
</p><p>50 Human-annotated Rationales  Before evaluating the performance of the automatically generated rationales, we summarize in Table 1 the differences between automatic vs. [sent-105, score-0.047]
</p><p>51 All computations were performed on the same movie review dataset of Pang and Lee (2004) used in Zaidan et al. [sent-107, score-0.175]
</p><p>52 (2007) annotation guidelines did not insist that annotators mark all rationales, only that some were marked for each document. [sent-110, score-0.044]
</p><p>53 Nevertheless, we report precision, recall, and F-score based on overlap with the human-annotated rationales of Zaidan et al. [sent-111, score-0.863]
</p><p>54 As shown in Table 1, the annotator rationales  found by OpinionFinder (F-score 49. [sent-115, score-0.984]
</p><p>55 6%) match the human rationales much better than those found by random selection (F-score 27. [sent-117, score-0.902]
</p><p>56 As expected, OpinionFinder’s positive rationales match the human rationales at a significantly lower level (F-score 3 1. [sent-119, score-1.802]
</p><p>57 This is due to the fact that OpinionFinder is trained on a dataset biased toward negative sentiment (see Section 3. [sent-122, score-0.301]
</p><p>58 In contrast, all other approaches show a balanced performance for positive and negative rationales vs. [sent-125, score-0.92]
</p><p>59 4  Experiments  For our contrastive learning experiments we use SV Mlight (Joachims, 1999). [sent-127, score-0.078]
</p><p>60 We evaluate the usefulness of automatically generated rationales on 4We chose the value of 25% to match the percentage of sentences per document, on average, that contain humanannotated rationales in our dataset (24. [sent-128, score-1.801]
</p><p>61 The first is the movie review data of Pang and Lee (2004), which was manually annotated with rationales by Zaidan et al. [sent-143, score-1.019]
</p><p>62 (2007)5; the remaining are four product review datasets from Blitzer et al. [sent-144, score-0.088]
</p><p>63 6 Only the movie review dataset contains human annotator rationales. [sent-146, score-0.335]
</p><p>64 7 The contrastive learning method introduced in Zaidan et al. [sent-149, score-0.078]
</p><p>65 The top half of Table 2 shows the performance of a system trained with no anno-  tator rationales vs. [sent-157, score-0.863]
</p><p>66 HUMANR treats each rationale in the same way as Zaidan et al. [sent-159, score-0.047]
</p><p>67 HUMANR@ SENTENCE extends the human annotator rationales to sentence boundaries, and then treats each such sentence as a separate rationale. [sent-161, score-1.037]
</p><p>68 61 This result demonstrates that locking rationales to sentence boundaries was a reasonable  %). [sent-164, score-0.887]
</p><p>69 7We use binary unigram features corresponding to the unstemmed words or punctuation marks with count greater or equal to 4 in the full 2000 documents, then we normalize the examples to the unit length. [sent-173, score-0.048]
</p><p>70 When computing the pseudo examples xij = we first compute ( x~i −~ v ij) using the binary representation. [sent-174, score-0.107]
</p><p>71 results  for the movie  – The numbers marked with • (or ∗) are statistically significantly better than NORATIONALES according to a paired t-test with p < 0. [sent-189, score-0.186]
</p><p>72 – The numbers marked with 4 are statistically significantly better than HUMANR according to a paired t-test with p < 0. [sent-192, score-0.082]
</p><p>73 – The numbers marked with † are not statistically significantly worse than HUMANR according to a paired t-test with p > 0. [sent-194, score-0.082]
</p><p>74 Among the approaches  that make use of only  automatic rationales (bottom half of Table 2), the best is OPINIONFINDER,  reaching 91. [sent-197, score-0.876]
</p><p>75 This result is slightly better than results exploiting human rationales (91. [sent-199, score-0.902]
</p><p>76 This result demonstrates that automatically generated rationales are just as good as human rationales in improving document-level sentiment classification. [sent-202, score-2.041]
</p><p>77 However, notice that the performance of RANDOM is statistically significantly lower than those based on human rationales (91. [sent-208, score-0.944]
</p><p>78 In our experiments so far, we observed that some of the automatic rationales are just as good as human rationales in improving the document-level sentiment classification. [sent-211, score-2.02]
</p><p>79 Could we perhaps achieve an even better result if we combine the automatic rationales with human 339  rationales? [sent-212, score-0.915]
</p><p>80 In other words, not only can our automatically generated rationales replace human rationales, but they can also improve upon human rationales when they are available. [sent-217, score-1.838]
</p><p>81 2 Experiments with the Product Reviews We next evaluate our approaches on datasets for which human annotator rationales do not exist. [sent-219, score-1.035]
</p><p>82 For this, we use some of the product review data from Blitzer et al. [sent-220, score-0.076]
</p><p>83 Each dataset contains 1000 positive and 1000 negative reviews. [sent-222, score-0.076]
</p><p>84 The reviews, however, are substantially shorter than those in the movie review dataset: the average number of sentences in each review is 9. [sent-223, score-0.208]
</p><p>85 An interesting trend in product review datasets is that RANDOM rationales are just as good as other more sophisticated rationales. [sent-233, score-0.964]
</p><p>86 We suspect that this is because product reviews are generally shorter and more focused than the movie reviews, thereby any randomly selected sentence is likely to be a good rationale. [sent-234, score-0.17]
</p><p>87 Quantitatively, subjective sentences in the product reviews amount to 78% (McDonald et al. [sent-235, score-0.11]
</p><p>88 , 2007), while subjective sentences in the movie review dataset are only about 25% (Mao and Lebanon, 2006). [sent-236, score-0.232]
</p><p>89 3 Examples of Annotator Rationales In this section, we examine an example to compare the automatically generated rationales (using OPINIONFINDER) with human annotator rationales for the movie review data. [sent-238, score-2.076]
</p><p>90 In the following  positive document snippet, automatic rationales  are underlined, while human-annotated rationales are in bold face. [sent-239, score-1.786]
</p><p>91 0 940  Table 3: Experimental Product Review data  results for subset of  – The numbers marked with • (or ∗) are statistically significantly better than NORATIONALES according to a paired t-test with p < 0. [sent-257, score-0.082]
</p><p>92 Notice that, although OPINIONFINDER misses some human rationales, it avoids the inclusion of “impossible to hate”, which contains only negative terms and is likely to be confusing for the contrastive learner. [sent-260, score-0.15]
</p><p>93 5  Related Work  In broad terms, constructing annotator rationales automatically and using them to formulate contrastive examples can be viewed as learning with prior knowledge (e. [sent-261, score-1.104]
</p><p>94 Our automatically generated rationales can be potentially combined with other learning frameworks that can exploit annotator rationales, such as Zaidan and Eisner (2008). [sent-270, score-1.018]
</p><p>95 6  Conclusions  In this paper, we explore methods to automatically generate annotator rationales for document-level sentiment classification. [sent-271, score-1.232]
</p><p>96 Our study is motivated by the desire to retain the performance gains of rationale-enhanced learning models while eliminating the need for additional human annotation effort. [sent-272, score-0.052]
</p><p>97 By employing existing resources for sen-  timent analysis, we can create automatic annotator rationales that are as good as human annotator rationales in improving document-level sentiment classification. [sent-273, score-2.262]
</p><p>98 Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. [sent-278, score-0.229]
</p><p>99 A sentimental education: sentiment analysis using subjectivity summarizationbased on minimum cuts. [sent-296, score-0.253]
</p><p>100 Modeling annotators: a generative approach to learning from annotator rationales. [sent-346, score-0.121]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rationales', 0.863), ('sentiment', 0.229), ('zaidan', 0.202), ('opinionfinder', 0.168), ('annotator', 0.121), ('movie', 0.104), ('humanr', 0.102), ('ij', 0.079), ('contrastive', 0.078), ('polarity', 0.076), ('pang', 0.073), ('regions', 0.061), ('subjective', 0.057), ('xij', 0.056), ('review', 0.052), ('vij', 0.051), ('norationales', 0.051), ('wilson', 0.043), ('human', 0.039), ('ccontrast', 0.038), ('polaritylexicon', 0.038), ('documentlevel', 0.034), ('rationale', 0.033), ('negative', 0.033), ('statistically', 0.029), ('coincides', 0.029), ('unexpectedly', 0.029), ('reviews', 0.029), ('categorization', 0.029), ('pseudo', 0.028), ('wiebe', 0.026), ('opinion', 0.026), ('taboada', 0.026), ('annotators', 0.024), ('xi', 0.024), ('positive', 0.024), ('product', 0.024), ('yi', 0.023), ('document', 0.023), ('examples', 0.023), ('theresa', 0.023), ('claire', 0.023), ('humanannotated', 0.022), ('slack', 0.022), ('janyce', 0.022), ('classification', 0.022), ('spans', 0.021), ('lee', 0.021), ('blitzer', 0.021), ('ainur', 0.021), ('mao', 0.021), ('marked', 0.02), ('toward', 0.02), ('paired', 0.02), ('morristown', 0.02), ('cardie', 0.019), ('omar', 0.019), ('rij', 0.019), ('automatically', 0.019), ('assumptions', 0.019), ('dataset', 0.019), ('lillian', 0.018), ('seemingly', 0.018), ('yejin', 0.018), ('bo', 0.018), ('utilities', 0.017), ('kitchen', 0.017), ('choi', 0.017), ('nj', 0.017), ('schapire', 0.016), ('voting', 0.016), ('generated', 0.015), ('conjecture', 0.015), ('informative', 0.015), ('jason', 0.015), ('conflicting', 0.014), ('treats', 0.014), ('association', 0.014), ('contextual', 0.014), ('inferring', 0.013), ('subjectivity', 0.013), ('boundaries', 0.013), ('significantly', 0.013), ('operating', 0.013), ('thing', 0.013), ('retain', 0.013), ('good', 0.013), ('automatic', 0.013), ('svm', 0.013), ('normalize', 0.013), ('datasets', 0.012), ('marks', 0.012), ('construct', 0.011), ('republic', 0.011), ('impossible', 0.011), ('prevailing', 0.011), ('locking', 0.011), ('sentimental', 0.011), ('mlight', 0.011), ('corne', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="42-tfidf-1" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>Author: Ainur Yessenalina ; Yejin Choi ; Claire Cardie</p><p>Abstract: One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</p><p>2 0.17361091 <a title="42-tfidf-2" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>Author: Georgios Paltoglou ; Mike Thelwall</p><p>Abstract: Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.</p><p>3 0.1474953 <a title="42-tfidf-3" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>Author: Wei Wei ; Jon Atle Gulla</p><p>Abstract: Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products.</p><p>4 0.14183889 <a title="42-tfidf-4" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>Author: Christian Scheible</p><p>Abstract: The translation of sentiment information is a task from which sentiment analysis systems can benefit. We present a novel, graph-based approach using SimRank, a well-established vertex similarity algorithm to transfer sentiment information between a source language and a target language graph. We evaluate this method in comparison with SO-PMI.</p><p>5 0.13255887 <a title="42-tfidf-5" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>Author: Valentin Jijkoun ; Maarten de Rijke ; Wouter Weerkamp</p><p>Abstract: We present a method for automatically generating focused and accurate topicspecific subjectivity lexicons from a general purpose polarity lexicon that allow users to pin-point subjective on-topic information in a set of relevant documents. We motivate the need for such lexicons in the field of media analysis, describe a bootstrapping method for generating a topic-specific lexicon from a general purpose polarity lexicon, and evaluate the quality of the generated lexicons both manually and using a TREC Blog track test set for opinionated blog post retrieval. Although the generated lexicons can be an order of magnitude more selective than the general purpose lexicon, they maintain, or even improve, the performance of an opin- ion retrieval system.</p><p>6 0.10530096 <a title="42-tfidf-6" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>7 0.095480114 <a title="42-tfidf-7" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>8 0.087887801 <a title="42-tfidf-8" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>9 0.083152413 <a title="42-tfidf-9" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>10 0.082424238 <a title="42-tfidf-10" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>11 0.067993291 <a title="42-tfidf-11" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>12 0.065143161 <a title="42-tfidf-12" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>13 0.064671285 <a title="42-tfidf-13" href="./acl-2010-Cross_Lingual_Adaptation%3A_An_Experiment_on_Sentiment_Classifications.html">80 acl-2010-Cross Lingual Adaptation: An Experiment on Sentiment Classifications</a></p>
<p>14 0.056101806 <a title="42-tfidf-14" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>15 0.053982608 <a title="42-tfidf-15" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>16 0.039802566 <a title="42-tfidf-16" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>17 0.034365822 <a title="42-tfidf-17" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>18 0.031580314 <a title="42-tfidf-18" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>19 0.028488193 <a title="42-tfidf-19" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>20 0.026638843 <a title="42-tfidf-20" href="./acl-2010-Mood_Patterns_and_Affective_Lexicon_Access_in_Weblogs.html">176 acl-2010-Mood Patterns and Affective Lexicon Access in Weblogs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.087), (1, 0.076), (2, -0.138), (3, 0.171), (4, -0.111), (5, 0.021), (6, -0.024), (7, 0.06), (8, 0.005), (9, 0.008), (10, -0.001), (11, 0.009), (12, 0.002), (13, 0.01), (14, -0.057), (15, -0.025), (16, 0.131), (17, -0.033), (18, -0.012), (19, -0.024), (20, -0.023), (21, 0.056), (22, -0.025), (23, -0.089), (24, 0.016), (25, 0.036), (26, 0.057), (27, -0.044), (28, -0.078), (29, -0.041), (30, 0.027), (31, -0.004), (32, -0.091), (33, 0.076), (34, -0.018), (35, -0.09), (36, 0.022), (37, 0.077), (38, -0.085), (39, 0.056), (40, 0.001), (41, 0.091), (42, 0.003), (43, 0.012), (44, -0.003), (45, 0.086), (46, 0.007), (47, 0.031), (48, -0.039), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94400597 <a title="42-lsi-1" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>Author: Ainur Yessenalina ; Yejin Choi ; Claire Cardie</p><p>Abstract: One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</p><p>2 0.83354193 <a title="42-lsi-2" href="./acl-2010-Sentiment_Learning_on_Product_Reviews_via_Sentiment_Ontology_Tree.html">209 acl-2010-Sentiment Learning on Product Reviews via Sentiment Ontology Tree</a></p>
<p>Author: Wei Wei ; Jon Atle Gulla</p><p>Abstract: Existing works on sentiment analysis on product reviews suffer from the following limitations: (1) The knowledge of hierarchical relationships of products attributes is not fully utilized. (2) Reviews or sentences mentioning several attributes associated with complicated sentiments are not dealt with very well. In this paper, we propose a novel HL-SOT approach to labeling a product’s attributes and their associated sentiments in product reviews by a Hierarchical Learning (HL) process with a defined Sentiment Ontology Tree (SOT). The empirical analysis against a humanlabeled data set demonstrates promising and reasonable performance of the proposed HL-SOT approach. While this paper is mainly on sentiment analysis on reviews of one product, our proposed HLSOT approach is easily generalized to labeling a mix of reviews of more than one products.</p><p>3 0.74820471 <a title="42-lsi-3" href="./acl-2010-Sentiment_Translation_through_Lexicon_Induction.html">210 acl-2010-Sentiment Translation through Lexicon Induction</a></p>
<p>Author: Christian Scheible</p><p>Abstract: The translation of sentiment information is a task from which sentiment analysis systems can benefit. We present a novel, graph-based approach using SimRank, a well-established vertex similarity algorithm to transfer sentiment information between a source language and a target language graph. We evaluate this method in comparison with SO-PMI.</p><p>4 0.73466033 <a title="42-lsi-4" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>Author: Georgios Paltoglou ; Mike Thelwall</p><p>Abstract: Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge.</p><p>5 0.65797299 <a title="42-lsi-5" href="./acl-2010-Optimizing_Informativeness_and_Readability_for_Sentiment_Summarization.html">188 acl-2010-Optimizing Informativeness and Readability for Sentiment Summarization</a></p>
<p>Author: Hitoshi Nishikawa ; Takaaki Hasegawa ; Yoshihiro Matsuo ; Genichiro Kikui</p><p>Abstract: We propose a novel algorithm for sentiment summarization that takes account of informativeness and readability, simultaneously. Our algorithm generates a summary by selecting and ordering sentences taken from multiple review texts according to two scores that represent the informativeness and readability of the sentence order. The informativeness score is defined by the number of sentiment expressions and the readability score is learned from the target corpus. We evaluate our method by summarizing reviews on restaurants. Our method outperforms an existing algorithm as indicated by its ROUGE score and human readability experiments.</p><p>6 0.60842806 <a title="42-lsi-6" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>7 0.56247282 <a title="42-lsi-7" href="./acl-2010-Mood_Patterns_and_Affective_Lexicon_Access_in_Weblogs.html">176 acl-2010-Mood Patterns and Affective Lexicon Access in Weblogs</a></p>
<p>8 0.48322767 <a title="42-lsi-8" href="./acl-2010-Evaluating_Multilanguage-Comparability_of_Subjectivity_Analysis_Systems.html">105 acl-2010-Evaluating Multilanguage-Comparability of Subjectivity Analysis Systems</a></p>
<p>9 0.46395415 <a title="42-lsi-9" href="./acl-2010-Last_but_Definitely_Not_Least%3A_On_the_Role_of_the_Last_Sentence_in_Automatic_Polarity-Classification.html">157 acl-2010-Last but Definitely Not Least: On the Role of the Last Sentence in Automatic Polarity-Classification</a></p>
<p>10 0.41861674 <a title="42-lsi-10" href="./acl-2010-Identifying_Text_Polarity_Using_Random_Walks.html">141 acl-2010-Identifying Text Polarity Using Random Walks</a></p>
<p>11 0.36705169 <a title="42-lsi-11" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>12 0.30696347 <a title="42-lsi-12" href="./acl-2010-Cross-Language_Text_Classification_Using_Structural_Correspondence_Learning.html">78 acl-2010-Cross-Language Text Classification Using Structural Correspondence Learning</a></p>
<p>13 0.30049956 <a title="42-lsi-13" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>14 0.27619651 <a title="42-lsi-14" href="./acl-2010-%22Was_It_Good%3F_It_Was_Provocative.%22_Learning_the_Meaning_of_Scalar_Adjectives.html">2 acl-2010-"Was It Good? It Was Provocative." Learning the Meaning of Scalar Adjectives</a></p>
<p>15 0.26252228 <a title="42-lsi-15" href="./acl-2010-Cross_Lingual_Adaptation%3A_An_Experiment_on_Sentiment_Classifications.html">80 acl-2010-Cross Lingual Adaptation: An Experiment on Sentiment Classifications</a></p>
<p>16 0.24913555 <a title="42-lsi-16" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>17 0.24366674 <a title="42-lsi-17" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>18 0.24093704 <a title="42-lsi-18" href="./acl-2010-Vocabulary_Choice_as_an_Indicator_of_Perspective.html">256 acl-2010-Vocabulary Choice as an Indicator of Perspective</a></p>
<p>19 0.22000742 <a title="42-lsi-19" href="./acl-2010-A_Taxonomy%2C_Dataset%2C_and_Classifier_for_Automatic_Noun_Compound_Interpretation.html">19 acl-2010-A Taxonomy, Dataset, and Classifier for Automatic Noun Compound Interpretation</a></p>
<p>20 0.21898545 <a title="42-lsi-20" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.021), (25, 0.039), (42, 0.082), (52, 0.264), (59, 0.075), (71, 0.024), (73, 0.053), (78, 0.024), (83, 0.125), (84, 0.037), (98, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86078006 <a title="42-lda-1" href="./acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</a></p>
<p>Author: Mikko Kurimo ; William Byrne ; John Dines ; Philip N. Garner ; Matthew Gibson ; Yong Guan ; Teemu Hirsimaki ; Reima Karhila ; Simon King ; Hui Liang ; Keiichiro Oura ; Lakshmi Saheer ; Matt Shannon ; Sayaki Shiota ; Jilei Tian</p><p>Abstract: In the EMIME project we have studied unsupervised cross-lingual speaker adaptation. We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms to adapt the synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition). An important application for this research is personalised speech-to-speech translation that will use the voice of the speaker in the input language to utter the translated sentences in the output language. In mobile environments this enhances the users’ interaction across language barriers by making the output speech sound more like the original speaker’s way of speaking, even if she or he could not speak the output language.</p><p>same-paper 2 0.77106512 <a title="42-lda-2" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>Author: Ainur Yessenalina ; Yejin Choi ; Claire Cardie</p><p>Abstract: One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</p><p>3 0.73107934 <a title="42-lda-3" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>Author: Dong Yang ; Paul Dixon ; Sadaoki Furui</p><p>Abstract: This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outper- forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</p><p>4 0.70089155 <a title="42-lda-4" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly.</p><p>5 0.60298729 <a title="42-lda-5" href="./acl-2010-Using_Anaphora_Resolution_to_Improve_Opinion_Target_Identification_in_Movie_Reviews.html">251 acl-2010-Using Anaphora Resolution to Improve Opinion Target Identification in Movie Reviews</a></p>
<p>Author: Niklas Jakob ; Iryna Gurevych</p><p>Abstract: unkown-abstract</p><p>6 0.60055006 <a title="42-lda-6" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>7 0.5991236 <a title="42-lda-7" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>8 0.5980047 <a title="42-lda-8" href="./acl-2010-Sentence_and_Expression_Level_Annotation_of_Opinions_in_User-Generated_Discourse.html">208 acl-2010-Sentence and Expression Level Annotation of Opinions in User-Generated Discourse</a></p>
<p>9 0.58940005 <a title="42-lda-9" href="./acl-2010-Inducing_Domain-Specific_Semantic_Class_Taggers_from_%28Almost%29_Nothing.html">150 acl-2010-Inducing Domain-Specific Semantic Class Taggers from (Almost) Nothing</a></p>
<p>10 0.5810383 <a title="42-lda-10" href="./acl-2010-Generating_Focused_Topic-Specific_Sentiment_Lexicons.html">123 acl-2010-Generating Focused Topic-Specific Sentiment Lexicons</a></p>
<p>11 0.58103788 <a title="42-lda-11" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>12 0.57938778 <a title="42-lda-12" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>13 0.57833779 <a title="42-lda-13" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>14 0.57752603 <a title="42-lda-14" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>15 0.57475394 <a title="42-lda-15" href="./acl-2010-The_Prevalence_of_Descriptive_Referring_Expressions_in_News_and_Narrative.html">231 acl-2010-The Prevalence of Descriptive Referring Expressions in News and Narrative</a></p>
<p>16 0.57321864 <a title="42-lda-16" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<p>17 0.57283109 <a title="42-lda-17" href="./acl-2010-A_Study_of_Information_Retrieval_Weighting_Schemes_for_Sentiment_Analysis.html">18 acl-2010-A Study of Information Retrieval Weighting Schemes for Sentiment Analysis</a></p>
<p>18 0.57172239 <a title="42-lda-18" href="./acl-2010-A_Unified_Graph_Model_for_Sentence-Based_Opinion_Retrieval.html">22 acl-2010-A Unified Graph Model for Sentence-Based Opinion Retrieval</a></p>
<p>19 0.56694734 <a title="42-lda-19" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>20 0.56646019 <a title="42-lda-20" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
