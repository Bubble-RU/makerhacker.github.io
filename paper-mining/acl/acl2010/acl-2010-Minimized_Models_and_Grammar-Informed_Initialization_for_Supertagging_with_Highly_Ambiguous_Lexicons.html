<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-172" href="#">acl2010-172</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</h1>
<br/><p>Source: <a title="acl-2010-172-pdf" href="http://aclweb.org/anthology//P/P10/P10-1051.pdf">pdf</a></p><p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>Reference: <a title="acl-2010-172-reference" href="../acl2010_reference/acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Minimized models and grammar-informed initialization for supertagging with highly ambiguous lexicons Sujith Ravi1 Jason Baldridge2 Kevin Knight1  1University of Southern California Information Sciences Institute Marina del Rey, California 90292 {s ravi knight }@ i i s . [sent-1, score-0.641]
</p><p>2 edu  ,  Abstract We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. [sent-2, score-0.743]
</p><p>3 We describe a new two-stage integer programming strategy that efficiently deals with  the high degree of ambiguity on these datasets while obtaining the full effect of model minimization. [sent-5, score-0.199]
</p><p>4 1 Introduction Creating accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. [sent-6, score-0.256]
</p><p>5 It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. [sent-7, score-0.199]
</p><p>6 Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. [sent-8, score-0.409]
</p><p>7 The most ambiguous word has 7 different POS tags associated with it. [sent-13, score-0.145]
</p><p>8 3% word token accuracy) via a Minimum Description Length approach using an integer program (IP) that finds a minimal bigram grammar that obeys the tag dictionary constraints and covers the observed data. [sent-18, score-0.783]
</p><p>9 A more challenging task is learning supertaggers for lexicalized grammar formalisms such as Combinatory Categorial Grammar (CCG) (Steedman, 2000). [sent-19, score-0.236]
</p><p>10 For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. [sent-20, score-0.302]
</p><p>11 Baldridge (2008) uses grammar-informed initialization for HMM tag transitions based on the universal combinatory rules of the CCG formalism to obtain 56. [sent-23, score-0.388]
</p><p>12 The former reduces the model size globally given a data set, while the latter biases bitag transitions toward those which are more likely based on a universal grammar without reference to any data. [sent-27, score-0.346]
</p><p>13 In this paper, we show how these strategies may be combined straightforwardly to produce improvements on the task of learning supertaggers from lexicons that have not been filtered in any way. [sent-28, score-0.204]
</p><p>14 Applying the approach of Ravi and Knight (2009) naively to CCG supertagging is intractable due to the high level of ambiguity. [sent-35, score-0.18]
</p><p>15 We deal with this by defining a new two-stage integer programming formulation that identifies minimal grammars efficiently and effectively. [sent-36, score-0.229]
</p><p>16 71 for supertags is indicative of the (challenging) fact that supertag ambiguity is greatest for the most frequent words. [sent-54, score-0.248]
</p><p>17 3  Grammar informed initialization for supertagging Part-of-speech tags are atomic labels that in and of themselves encode no internal structure. [sent-55, score-0.308]
</p><p>18 Distinct: # of distinct lexical categories; Max: # of categories for the most ambiguous word; Type ambig: per word type category ambiguity; Tok ambig: per word token category ambiguity. [sent-59, score-0.255]
</p><p>19 trast, supertags are detailed, structured labels; a universal set of grammatical rules defines how categories may combine with one another to project syntactic structure. [sent-60, score-0.334]
</p><p>20 2 Because of this, properties of the CCG formalism itself can be used to constrain learning—prior to considering any particular language, grammar or data set. [sent-61, score-0.153]
</p><p>21 Baldridge (2008) uses this observation to create grammar-informed tag transitions for a bitag HMM supertagger based on two main properties. [sent-62, score-0.395]
</p><p>22 First, categories differ in their complexity and less complex categories tend to be used more frequently. [sent-63, score-0.198]
</p><p>23 Second, categories indicate the form of categories found adjacent to them; for example, the category for sentential complement verbs ((S\NP)/S) expects an NtePn ttioa lits c olmeftp laenmde an S v etrob ists ( right. [sent-65, score-0.198]
</p><p>24 Given a lexicon containing the categories  for each word, these allow derivations like: Ed might see a  cat  NP (S\NP)/(S\NP) (S\NP)/NP NP/N N (S\NP)/NP>B NP> S\NP> SS\NP> Other derivations are possible. [sent-67, score-0.317]
</p><p>25 Baldridge uses these properties to define tag 2Note that supertags can be lexical categories of CCG (Steedman, 2000), elementary trees of Tree-adjoining Grammar (Joshi, 1988), or types in a feature hierarchy as in Headdriven Phrase Structure Grammar (Pollard and Sag, 1994). [sent-70, score-0.473]
</p><p>26 By starting EM with these tag transition distributions and an unfiltered lexicon (word-tosupertag dictionary), Baldridge obtains a tagging accuracy of 56. [sent-75, score-0.486]
</p><p>27 4  Minimized models for supertagging  The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al. [sent-79, score-0.291]
</p><p>28 There are many challenges involved in using IP minimization for supertagging. [sent-84, score-0.216]
</p><p>29 The 1241 distinct supertags in the tagset result in 1. [sent-85, score-0.207]
</p><p>30 5 million tag bigram entries in the model and the dictionary contains almost 3. [sent-86, score-0.398]
</p><p>31 The set of 45 POS tags for the same data yields 2025 tag bigrams and 8910 dictionary entries. [sent-88, score-0.399]
</p><p>32 Our objective is to find the smallest supertag grammar (of tag bigram types) that explains the entire text while obeying the lexicon’s constraints. [sent-90, score-0.527]
</p><p>33 1 IP method for supertagging Our goal for supertagging is to build a minimized model with the following objective:  ImParor (igi. [sent-93, score-0.471]
</p><p>34 Using the full grammar and lexicon to perform model minimization results in a very large, difficult to solve integer program involving billions of variables and constraints. [sent-96, score-0.687]
</p><p>35 One way of combating this is to use a reduced grammar and lexicon as input to the integer program. [sent-98, score-0.391]
</p><p>36 We do this without further supervision by using the HMM model trained using basic EM: entries are pruned based on the tag sequence it predicts on the test data. [sent-99, score-0.292]
</p><p>37 This produces an observed grammar of distinct tag bigrams (Gobs) and lexicon of ob-  served lexical assignments (Lobs). [sent-100, score-0.676]
</p><p>38 For CCGbank, Gobs and Lobs have 12,363 and 18,869 entries, respectively—far less than the millions of entries in the full grammar and lexicon. [sent-101, score-0.218]
</p><p>39 However, even with the EM-reduced grammar and lexicon, the IP-minimization is still very hard to solve. [sent-104, score-0.153]
</p><p>40 We begin with a simpler minimization problem than the original one (IPoriginal), with the following objective:  WeItgGfaPorm gin 1utygl:ap⊂tseFiontGghbdi,smethruvaecnshdtampnitohaslntheibs ltghdesafrtoe p. [sent-108, score-0.216]
</p><p>41 risofvgeatr ygmlweb,aoigsrctdaeombntise-  ing binary variables gvari for every tag bigram gi = tjtk in G. [sent-109, score-0.552]
</p><p>42 Binary link variables connect tag bigrams with word bigrams; these are restricted 497  Figure 1: Two-stage IP method for selecting minimized models for supertagging. [sent-110, score-0.461]
</p><p>43 , there exists a link variable linkjklm connecting tag bigram tjtk with word bigram wlwm only if the word/tag pairs (wl , tj) and (wm, tk) are present in L. [sent-113, score-0.438]
</p><p>44 The entire integer programming formulation is shown Figure 2. [sent-114, score-0.169]
</p><p>45 The IP solver3 solves the above integer program and we extract the set of tag bigrams Gmin1 based on the activated grammar variables. [sent-115, score-0.629]
</p><p>46 For the CCGbank test data, MIN 1 yields 2530 tag bigrams. [sent-116, score-0.199]
</p><p>47 However, a second stage is needed since there is no guarantee that Gmin1 can explain the test data: it contains tags for all word bigram types, but it cannot necessarily tag the full word sequence. [sent-117, score-0.354]
</p><p>48 Using only tag bigrams from MIN1 (shown in blue), there is no fully-linked tag path through the network. [sent-119, score-0.491]
</p><p>49 This stage uses the original minimization formulation for the supertagging problem IPoriginal, again using an integer programming method similar to that proposed by Ravi and Knight (2009). [sent-123, score-0.593]
</p><p>50 If applied to the observed grammar Gobs, the resulting integer program is hard to solve. [sent-124, score-0.364]
</p><p>51 We implement this by fixing the values of all binary grammar variables present in Gmin1 to 1 before optimization. [sent-126, score-0.183]
</p><p>52 We instantiate binary variables gvari and lvari  for every tag bigram (in G) and lexicon entry (in L). [sent-131, score-0.486]
</p><p>53 , linkcjk corresponds to tag bigram tjtk in column c). [sent-139, score-0.371]
</p><p>54 Next, we formulate the integer program given in Figure 3. [sent-140, score-0.184]
</p><p>55 Figure 1 illustrates how MIN2 augments the grammar Gmin1 (links shown in blue) with addi498  tional tag bigrams (shown in red) to form a complete tag path through the network. [sent-141, score-0.644]
</p><p>56 The minimized grammar set in the final solution Gmin2 contains only 2810 entries, significantly fewer than the original grammar Gobs’s 12,363 tag bigrams. [sent-142, score-0.616]
</p><p>57 We note that the two-stage minimization procedure proposed here is not guaranteed to yield the optimal solution to our original objective IPoriginal. [sent-143, score-0.271]
</p><p>58 We build the transition model using only entries from the minimized grammar set Gmin2, and instantiate an emission model using the word/tag pairs seen in L (provided as input to the minimization procedure). [sent-148, score-0.576]
</p><p>59 The trained model is used to find the Viterbi tag sequence for the corpus. [sent-150, score-0.199]
</p><p>60 The quality of the observed grammar and lexicon improves considerably at the end of a single EM+IP run. [sent-153, score-0.284]
</p><p>61 Ravi and Knight (2009) exploited this to iteratively improve their POS tag model: since the first mini-  mization procedure is seeded with a noisy grammar and tag dictionary, iterating the IP procedure with progressively better grammars further improves the model. [sent-154, score-0.582]
</p><p>62 We do likewise, bootstrapping a new EM+IP run using as input, the observed grammar Gobs and lexicon Lobs from the last tagging output of the previous iteration. [sent-155, score-0.345]
</p><p>63 We run this until the chosen grammar set Gmin2 does not change. [sent-156, score-0.153]
</p><p>64 2  Minimization with grammar-informed initialization There are two complementary ways to use grammar-informed initialization with the IPminimization approach: (1) using EMGI output as the starting grammar/lexicon and (2) using the tag transitions directly in the IP objective function. [sent-158, score-0.498]
</p><p>65 The first takes advantage of the earlier observation that the quality of the grammar and lexicon provided as initial input to the minimization procedure can affect the quality ofthe final supertagging output. [sent-159, score-0.681]
</p><p>66 6Other numeric weights associated with the tag bigrams could be considered, such as 0/1 for uncombin499  the integer program including the constraints remain unchanged, and, we acquire a final tagger in the same manner as described in the previous section. [sent-162, score-0.476]
</p><p>67 In this way, we combine the minimization and GI strategies into a single objective function that finds a minimal grammar set while keeping the more likely tag bigrams in the chosen solution. [sent-163, score-0.849]
</p><p>68 EM+IP IP minimization using initial grammar provided by EM. [sent-166, score-0.397]
</p><p>69 EMGI+IPGI IP minimization using initial grammar/lexicon provided by EMGI and additional grammar-informed IP objective. [sent-168, score-0.244]
</p><p>70 For EM+IP and EMGI+IPGI, the minimization and EM training processes are iterated until the resulting grammar and lexicon remain unchanged. [sent-169, score-0.473]
</p><p>71 We also include a baseline which randomly chooses a tag from those associated with each word in the lexicon, averaged over three runs. [sent-171, score-0.199]
</p><p>72 We  evaluate the performance in terms of tagging accuracy with respect to gold tags for ambiguous words in held-out test sets for English and Italian. [sent-173, score-0.248]
</p><p>73 We consider results with and without Recall that unlike much previous work, we do not collect the lexicon (tag dictionary) from the test set: this means the model must handle unknown words and the possibility of having missing lexical entries for covering the test set. [sent-174, score-0.169]
</p><p>74 As such, these supertags are outside of the categorial system: their use in derivations requires phrase structure rules that are not derivable from the CCG combinatory rules. [sent-181, score-0.293]
</p><p>75 Accuracies are reported for four settings—(1) ambiguous word tokens in the test corpus, (2) ambiguous word tokens, ignoring punctuation, (3) all word tokens, and (4) all word tokens except punctuation. [sent-183, score-0.307]
</p><p>76 recall for each model on the observed bitag grammar and observed lexicon on the test set. [sent-184, score-0.461]
</p><p>77 We calculate them as follows, for an observed grammar or lexicon X:  Precision =|{X} ∩ {O|{bXse}r|vedgold}|  Recall =|{X|}{O ∩b {sOerbvseedrvgoelddg}o|ld}| This provides a measure of model performance on bitag types for the grammar and lexical entry types for the lexicon, rather than tokens. [sent-185, score-0.551]
</p><p>78 4)—this is unsurprising because, as noted already, punctuation supertags are not actual cate-  TaLGberlRxPimceo3ain:lsorCm25E0687Mp. [sent-200, score-0.212]
</p><p>79 gold tagging in terms of precision and recall measures for supertagging on CCGbank data. [sent-205, score-0.321]
</p><p>80 Thus, the better starting point provided by EMGI has more impact than the integer program that includes GI in its objective function. [sent-224, score-0.267]
</p><p>81 However, we note that it should be possible to exploit the GI  information more effectively in the integer program than we have here. [sent-225, score-0.184]
</p><p>82 We can obtain a more-fine grained understanding of how the models differ by considering the precision and recall values for the grammars and lexicons of the different models, given in Table 3. [sent-230, score-0.189]
</p><p>83 EM+IP prunes that set of bitags considerably, leading to better precision at the cost of recall. [sent-234, score-0.151]
</p><p>84 EMGI’s higher recall and precision indicate the tag transition distributions do capture general patterns of linkage between adjacent CCG categories, while EM ensures that the data filters out combinable, but un-  necessary, bitags. [sent-235, score-0.31]
</p><p>85 IP-minimization’s pruning of inappropriate taggings means more common words are not assigned highly infrequent supertags (boosting precision) while unknown words are generally assigned more sensible supertags (boosting recall). [sent-238, score-0.396]
</p><p>86 EMGI again focuses taggings on combinable contexts, boosting precision and recall similarly to EM+IP, but in greater measure. [sent-239, score-0.212]
</p><p>87 Table 4 compares gold tags to tags generated by all four methods for the frequent and highly ambiguous words the and in. [sent-242, score-0.195]
</p><p>88 IP-minimization identifies a smaller set of tags that better matches the gold tags; this emerges  because other determiners and prepositions evoke similar, but not identical, supertags, and the grammar minimization pushes (but does not force) them to rely on the same supertags wherever possible. [sent-244, score-0.594]
</p><p>89 However, the proportions are incorrect; for example, the tag assigned most frequently to in is ((S\NP)\(S\NP))/NP though (NP\NP)/NP i sn more frequent iNn t)h)/eN Ptest th souetg. [sent-245, score-0.199]
</p><p>90 EMGI’s tags correct that balance and find better proportions, but also some less common categories, such as (((N/N)\(N/N))\((N/N)\(N/N)))/N, sneak in because they c/No)m)b\(i(nNe wNi)t\h( frequent categories li bkeeN/N and N. [sent-246, score-0.149]
</p><p>91 We wanted to evaluate performance out-of-the-box because 501  The table shows tag assignments (and their counts for each method) for the and in in the CCGbank test sections. [sent-250, score-0.228]
</p><p>92 Accuracies are for ambiguous word tokens in the test corpus, ignoring punctuation. [sent-258, score-0.166]
</p><p>93 Table 6 gives precision and recall for the grammars and lexicons for CCG-TUT—the values are lower than for CCGbank (in line with the lower baseline), but exhibit the same trends. [sent-263, score-0.189]
</p><p>94 6  Conclusion  We have shown how two complementary strategies—grammar-informed tag transitions and IP-minimization—for learning of supertaggers from highly ambiguous lexicons can be straight-  TabGLlrexPRaecrim c6om:anils ronCm45E1283pM. [sent-264, score-0.543]
</p><p>95 gold tagging in terms of precision and recall measures for supertagging on CCG-TUT. [sent-269, score-0.321]
</p><p>96 We also provide a new two-stage integer programming setup that allows model minimization to be tractable for supertagging without sacrificing the quality of the search for minimal bitag grammars. [sent-272, score-0.708]
</p><p>97 This brings further challenges; in particular, it will be necessary to identify novel entries con-  sisting of seen word and seen category and to predict unseen, but valid, categories which are needed to explain the data. [sent-274, score-0.164]
</p><p>98 Because the lexicon is the grammar in CCG, learning new wordcategory associations is grammar generalization and is of interest for grammar acquisition. [sent-277, score-0.563]
</p><p>99 The improvements we show here for learning supertaggers from lexicons without labeled data may be  able to help create annotated resources more efficiently, or enable CCG parsers to be learned with less human-coded knowledge. [sent-282, score-0.161]
</p><p>100 Converting a dependency treebank to a categorial grammar treebank for Italian. [sent-311, score-0.182]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('emgi', 0.417), ('ccgbank', 0.256), ('ip', 0.24), ('minimization', 0.216), ('em', 0.215), ('tag', 0.199), ('ipgi', 0.19), ('supertagging', 0.18), ('ccg', 0.179), ('supertags', 0.175), ('grammar', 0.153), ('integer', 0.134), ('ravi', 0.134), ('gobs', 0.133), ('npaper', 0.133), ('np', 0.128), ('bitag', 0.114), ('gi', 0.113), ('minimized', 0.111), ('lexicon', 0.104), ('categories', 0.099), ('ambiguous', 0.095), ('lobs', 0.095), ('bigrams', 0.093), ('baldridge', 0.089), ('supertaggers', 0.083), ('lexicons', 0.078), ('initialization', 0.078), ('bigram', 0.077), ('civil', 0.076), ('gvari', 0.076), ('iporiginal', 0.076), ('knight', 0.076), ('hmm', 0.071), ('entries', 0.065), ('tagging', 0.061), ('derivations', 0.057), ('dictionary', 0.057), ('bitags', 0.057), ('ipminimization', 0.057), ('tjtk', 0.057), ('objective', 0.055), ('italian', 0.053), ('transitions', 0.052), ('program', 0.05), ('prunes', 0.05), ('tags', 0.05), ('obtains', 0.049), ('steedman', 0.049), ('boosting', 0.048), ('tokens', 0.046), ('taggings', 0.046), ('ti', 0.045), ('precision', 0.044), ('strategies', 0.043), ('supertag', 0.043), ('accuracy', 0.042), ('bos', 0.042), ('served', 0.039), ('banko', 0.038), ('ambig', 0.038), ('combinable', 0.038), ('linkcjk', 0.038), ('punctuation', 0.037), ('complementary', 0.036), ('recall', 0.036), ('programming', 0.035), ('hockenmaier', 0.033), ('combine', 0.033), ('cplex', 0.033), ('barron', 0.033), ('goldwater', 0.032), ('distinct', 0.032), ('combinatory', 0.032), ('grammars', 0.031), ('transition', 0.031), ('semiautomatically', 0.03), ('supertagger', 0.03), ('ambiguity', 0.03), ('variables', 0.03), ('assignments', 0.029), ('token', 0.029), ('minimal', 0.029), ('categorial', 0.029), ('finds', 0.028), ('basic', 0.028), ('provided', 0.028), ('initialized', 0.028), ('link', 0.028), ('stage', 0.028), ('universal', 0.027), ('pollard', 0.027), ('observed', 0.027), ('converting', 0.027), ('clark', 0.027), ('penn', 0.026), ('creutz', 0.026), ('pos', 0.026), ('ignoring', 0.025), ('moore', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="172-tfidf-1" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>2 0.29050893 <a title="172-tfidf-2" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>Author: Timothy A. D. Fowler ; Gerald Penn</p><p>Abstract: The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.</p><p>3 0.24311623 <a title="172-tfidf-3" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>4 0.23868561 <a title="172-tfidf-4" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>Author: Matthew Honnibal ; James R. Curran ; Johan Bos</p><p>Abstract: Once released, treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of specific phenomena. Instead, separate resources are created to address such problems. In this paper we show how to improve the quality of a treebank, by integrating resources and implementing improved analyses for specific constructions. We demonstrate this rebanking process by creating an updated version of CCGbank that includes the predicate-argument structure of both verbs and nouns, baseNP brackets, verb-particle constructions, and restrictive and non-restrictive nominal modifiers; and evaluate the impact of these changes on a statistical parser.</p><p>5 0.22640799 <a title="172-tfidf-5" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>Author: Jonathan K. Kummerfeld ; Jessika Roesner ; Tim Dawborn ; James Haggerty ; James R. Curran ; Stephen Clark</p><p>Abstract: We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highestscoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtain- ing significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.</p><p>6 0.19560002 <a title="172-tfidf-6" href="./acl-2010-The_Importance_of_Rule_Restrictions_in_CCG.html">228 acl-2010-The Importance of Rule Restrictions in CCG</a></p>
<p>7 0.10966939 <a title="172-tfidf-7" href="./acl-2010-Wide-Coverage_NLP_with_Linguistically_Expressive_Grammars.html">260 acl-2010-Wide-Coverage NLP with Linguistically Expressive Grammars</a></p>
<p>8 0.10736671 <a title="172-tfidf-8" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>9 0.10318824 <a title="172-tfidf-9" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>10 0.10281092 <a title="172-tfidf-10" href="./acl-2010-Optimising_Information_Presentation_for_Spoken_Dialogue_Systems.html">187 acl-2010-Optimising Information Presentation for Spoken Dialogue Systems</a></p>
<p>11 0.098970972 <a title="172-tfidf-11" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>12 0.098520793 <a title="172-tfidf-12" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>13 0.096560605 <a title="172-tfidf-13" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>14 0.09635713 <a title="172-tfidf-14" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>15 0.086599134 <a title="172-tfidf-15" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>16 0.074950263 <a title="172-tfidf-16" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>17 0.073902935 <a title="172-tfidf-17" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>18 0.071714684 <a title="172-tfidf-18" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>19 0.07062345 <a title="172-tfidf-19" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>20 0.069336534 <a title="172-tfidf-20" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.203), (1, -0.027), (2, 0.086), (3, -0.031), (4, -0.097), (5, -0.133), (6, 0.203), (7, 0.056), (8, 0.301), (9, 0.001), (10, 0.171), (11, 0.079), (12, -0.22), (13, -0.009), (14, -0.094), (15, -0.089), (16, -0.019), (17, -0.045), (18, -0.047), (19, -0.052), (20, 0.077), (21, 0.008), (22, -0.034), (23, 0.106), (24, -0.075), (25, 0.042), (26, 0.051), (27, -0.117), (28, -0.027), (29, 0.057), (30, 0.032), (31, 0.015), (32, 0.009), (33, -0.032), (34, -0.012), (35, -0.084), (36, 0.082), (37, 0.022), (38, 0.054), (39, -0.023), (40, 0.075), (41, -0.08), (42, 0.008), (43, 0.053), (44, 0.005), (45, -0.054), (46, -0.075), (47, 0.026), (48, 0.015), (49, 0.157)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94654483 <a title="172-lsi-1" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>2 0.76261914 <a title="172-lsi-2" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>Author: Timothy A. D. Fowler ; Gerald Penn</p><p>Abstract: The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.</p><p>3 0.72510302 <a title="172-lsi-3" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>Author: Jonathan K. Kummerfeld ; Jessika Roesner ; Tim Dawborn ; James Haggerty ; James R. Curran ; Stephen Clark</p><p>Abstract: We propose a novel self-training method for a parser which uses a lexicalised grammar and supertagger, focusing on increasing the speed of the parser rather than its accuracy. The idea is to train the supertagger on large amounts of parser output, so that the supertagger can learn to supply the supertags that the parser will eventually choose as part of the highestscoring derivation. Since the supertagger supplies fewer supertags overall, the parsing speed is increased. We demonstrate the effectiveness of the method using a CCG supertagger and parser, obtain- ing significant speed increases on newspaper text with no loss in accuracy. We also show that the method can be used to adapt the CCG parser to new domains, obtaining accuracy and speed improvements for Wikipedia and biomedical text.</p><p>4 0.66514617 <a title="172-lsi-4" href="./acl-2010-The_Importance_of_Rule_Restrictions_in_CCG.html">228 acl-2010-The Importance of Rule Restrictions in CCG</a></p>
<p>Author: Marco Kuhlmann ; Alexander Koller ; Giorgio Satta</p><p>Abstract: Combinatory Categorial Grammar (CCG) is generally construed as a fully lexicalized formalism, where all grammars use one and the same universal set of rules, and crosslinguistic variation is isolated in the lexicon. In this paper, we show that the weak generative capacity of this ‘pure’ form of CCG is strictly smaller than that of CCG with grammar-specific rules, and of other mildly context-sensitive grammar formalisms, including Tree Adjoining Grammar (TAG). Our result also carries over to a multi-modal extension of CCG.</p><p>5 0.63462251 <a title="172-lsi-5" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>Author: Matthew Honnibal ; James R. Curran ; Johan Bos</p><p>Abstract: Once released, treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of specific phenomena. Instead, separate resources are created to address such problems. In this paper we show how to improve the quality of a treebank, by integrating resources and implementing improved analyses for specific constructions. We demonstrate this rebanking process by creating an updated version of CCGbank that includes the predicate-argument structure of both verbs and nouns, baseNP brackets, verb-particle constructions, and restrictive and non-restrictive nominal modifiers; and evaluate the impact of these changes on a statistical parser.</p><p>6 0.6256699 <a title="172-lsi-6" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>7 0.52609825 <a title="172-lsi-7" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>8 0.45252141 <a title="172-lsi-8" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>9 0.42447639 <a title="172-lsi-9" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>10 0.40854287 <a title="172-lsi-10" href="./acl-2010-Wide-Coverage_NLP_with_Linguistically_Expressive_Grammars.html">260 acl-2010-Wide-Coverage NLP with Linguistically Expressive Grammars</a></p>
<p>11 0.392766 <a title="172-lsi-11" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>12 0.3796646 <a title="172-lsi-12" href="./acl-2010-A_Probabilistic_Generative_Model_for_an_Intermediate_Constituency-Dependency_Representation.html">12 acl-2010-A Probabilistic Generative Model for an Intermediate Constituency-Dependency Representation</a></p>
<p>13 0.36260214 <a title="172-lsi-13" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>14 0.36249122 <a title="172-lsi-14" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>15 0.35938242 <a title="172-lsi-15" href="./acl-2010-Simultaneous_Tokenization_and_Part-Of-Speech_Tagging_for_Arabic_without_a_Morphological_Analyzer.html">213 acl-2010-Simultaneous Tokenization and Part-Of-Speech Tagging for Arabic without a Morphological Analyzer</a></p>
<p>16 0.35203877 <a title="172-lsi-16" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>17 0.34658965 <a title="172-lsi-17" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>18 0.33360574 <a title="172-lsi-18" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>19 0.3148399 <a title="172-lsi-19" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>20 0.3110297 <a title="172-lsi-20" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(1, 0.028), (14, 0.023), (21, 0.193), (25, 0.113), (33, 0.011), (39, 0.024), (42, 0.045), (59, 0.118), (73, 0.052), (78, 0.063), (83, 0.065), (84, 0.024), (98, 0.135)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93562198 <a title="172-lda-1" href="./acl-2010-On_the_Computational_Complexity_of_Dominance_Links_in_Grammatical_Formalisms.html">182 acl-2010-On the Computational Complexity of Dominance Links in Grammatical Formalisms</a></p>
<p>Author: Sylvain Schmitz</p><p>Abstract: Dominance links were introduced in grammars to model long distance scrambling phenomena, motivating the definition of multiset-valued linear indexed grammars (MLIGs) by Rambow (1994b), and inspiring quite a few recent formalisms. It turns out that MLIGs have since been rediscovered and reused in a variety of contexts, and that the complexity of their emptiness problem has become the key to several open questions in computer science. We survey complexity results and open issues on MLIGs and related formalisms, and provide new complexity bounds for some linguistically motivated restrictions.</p><p>same-paper 2 0.83591807 <a title="172-lda-2" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>3 0.74885917 <a title="172-lda-3" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>4 0.74073434 <a title="172-lda-4" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>Author: Trevor Cohn ; Phil Blunsom</p><p>Abstract: Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently con- verges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy.</p><p>5 0.73730344 <a title="172-lda-5" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>Author: Timothy A. D. Fowler ; Gerald Penn</p><p>Abstract: The definition of combinatory categorial grammar (CCG) in the literature varies quite a bit from author to author. However, the differences between the definitions are important in terms of the language classes of each CCG. We prove that a wide range of CCGs are strongly context-free, including the CCG of CCGbank and of the parser of Clark and Curran (2007). In light of these new results, we train the PCFG parser of Petrov and Klein (2007) on CCGbank and achieve state of the art results in supertagging accuracy, PARSEVAL measures and dependency accuracy.</p><p>6 0.73432255 <a title="172-lda-6" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>7 0.73314232 <a title="172-lda-7" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>8 0.7312569 <a title="172-lda-8" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>9 0.73047018 <a title="172-lda-9" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>10 0.72937059 <a title="172-lda-10" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>11 0.72810686 <a title="172-lda-11" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>12 0.72793996 <a title="172-lda-12" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>13 0.72746396 <a title="172-lda-13" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>14 0.72453284 <a title="172-lda-14" href="./acl-2010-Faster_Parsing_by_Supertagger_Adaptation.html">114 acl-2010-Faster Parsing by Supertagger Adaptation</a></p>
<p>15 0.72421861 <a title="172-lda-15" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>16 0.723436 <a title="172-lda-16" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>17 0.72328401 <a title="172-lda-17" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>18 0.72213435 <a title="172-lda-18" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>19 0.72031152 <a title="172-lda-19" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>20 0.71697479 <a title="172-lda-20" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
