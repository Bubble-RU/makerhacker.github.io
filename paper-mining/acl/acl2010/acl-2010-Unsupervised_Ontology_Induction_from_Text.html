<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>248 acl-2010-Unsupervised Ontology Induction from Text</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-248" href="#">acl2010-248</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>248 acl-2010-Unsupervised Ontology Induction from Text</h1>
<br/><p>Source: <a title="acl-2010-248-pdf" href="http://aclweb.org/anthology//P/P10/P10-1031.pdf">pdf</a></p><p>Author: Hoifung Poon ; Pedro Domingos</p><p>Abstract: Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate On- toUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches.</p><p>Reference: <a title="acl-2010-248-reference" href="../acl2010_reference/acl-2010-Unsupervised_Ontology_Induction_from_Text_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. [sent-7, score-0.211]
</p><p>2 The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. [sent-9, score-0.122]
</p><p>3 We evaluate On-  toUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. [sent-10, score-0.078]
</p><p>4 We would like computers to be able to read text and express the knowledge it contains in a formal representation, suitable for answering questions and solving problems. [sent-13, score-0.081]
</p><p>5 Also, the knowledge extracted is simply a large set of formulas without ontological structure, and the latter is essential for compact representation and efficient reasoning (Staab and Studer, 2004). [sent-26, score-0.087]
</p><p>6 We propose OntoUSP (Ontological USP), a sys-  tem that learns an ISA hierarchy over clusters of logical expressions, and populates it by translating sentences to logical form. [sent-27, score-0.279]
</p><p>7 OntoUSP is encoded in a few formulas of higher-order Markov logic (Domingos and Lowd, 2009), and can be viewed as extending USP with the capability to perform hierarchical (as opposed to flat) clustering. [sent-28, score-0.126]
</p><p>8 Finally, experiments on a biomedical knowledge acquisition and question answering task show that OntoUSP can greatly outperform USP and previous systems. [sent-37, score-0.11]
</p><p>9 1 Ontology Learning In general, ontology induction (constructing an ontology) and ontology population (mapping textual expressions to concepts and relations in the ontology) remain difficult open problems (Staab and Studer, 2004). [sent-39, score-0.375]
</p><p>10 Recently, ontology learning has attracted increasing interest in both NLP and semantic Web communities (Cimiano, 2006; Maedche, 2002), and a number of machine learning approaches have been developed (e. [sent-40, score-0.184]
</p><p>11 Moreover, they often focus on inducing ontology over individual words rather than arbitrarily large  meaning units (e. [sent-52, score-0.148]
</p><p>12 Most importantly, existing approaches typically separate ontology induction from population and knowledge extraction, and pursue each task in a standalone fashion. [sent-56, score-0.254]
</p><p>13 The resulted ontology is disconnected from text and requires additional effort to map between the two (Tsujii, 2004). [sent-58, score-0.168]
</p><p>14 Our approach differs from existing ones in two main aspects: we induce a probabilistic ontology from text, and we do so by jointly conducting ontology induction, population, and knowledge extraction. [sent-60, score-0.389]
</p><p>15 Furthermore, we leverage the ontology for hierar-  chical smoothing and incorporate this smoothing into the induction process. [sent-65, score-0.288]
</p><p>16 Our approach can also leverage existing ontologies and knowledge bases to conduct semisupervised ontology induction (e. [sent-67, score-0.243]
</p><p>17 Markov logic is a probabilistic extension of first-order logic and can compactly specify probability distributions over complex relational domains. [sent-77, score-0.097]
</p><p>18 It can be viewed as a structured prediction problem, where a semantic parse is formed by partitioning the input sentence (or a syntactic analysis such as a dependency tree) into meaning units and assigning each unit to the logical form representing an entity or relation (Figure 1). [sent-85, score-0.167]
</p><p>19 Bottom: a semantic parse consists of a partition of the dependency tree and an assignment of its parts. [sent-88, score-0.076]
</p><p>20 parser extracts knowledge from input text and converts them into logical form (the semantic parse), which can then be used in logical and probabilistic inference and support end tasks such as question  answering. [sent-89, score-0.183]
</p><p>21 2 USP inputs dependency trees of sentences and first transforms them into quasi-logical forms (QLFs) by converting each node to a unary atom and each dependency edge to a binary atom (e. [sent-94, score-0.121]
</p><p>22 3 For each sentence, a semantic parse comprises of  a partition of its QLF into subexpressions, each of which has a naturally corresponding lambda 2In this paper, we use a slightly different formulation of USP and its MLN to facilitate the exposition of OntoUSP. [sent-98, score-0.113]
</p><p>23 81…  Figure 2: An example of object/property clusters: INDUCE contains the core-form property cluster and others, such as the agent argument INDUCER. [sent-105, score-0.28]
</p><p>24 The lambda-form clusters naturally form an ISPART hierarchy (Figure 2). [sent-107, score-0.181]
</p><p>25 An object cluster cor-  responds to semantic concepts or relations such as INDUCE, and contains a variable number of property clusters. [sent-108, score-0.281]
</p><p>26 A special property cluster of core forms maintains a distribution over variations in lambda forms for expressing this concept or relation. [sent-109, score-0.341]
</p><p>27 Other property clusters correspond to modifiers or arguments such as INDUCER (the agent argument of INDUCE), each of which in turn contains three subclusters of property values: the argument-object subcluster maintains a distribution over object clusters that may occur in this argument (e. [sent-110, score-0.593]
</p><p>28 , nsubj in active voice and agent in passive voice), and the argument-number subcluster maintains a distribution over total numbers of this argument that may occur in a sentence (e. [sent-116, score-0.15]
</p><p>29 Effectively, USP simultaneously discovers the lambda-form clusters and an IS-PART hierarchy among them. [sent-119, score-0.181]
</p><p>30 This novel form of relational clustering is governed by a joint probability distribution P(T, L) defined in higher-order5 Markov logic, where T are the input dependency trees, and L the semantic parses. [sent-122, score-0.081]
</p><p>31 The 4The lambda form is derived by replacing every Skolem constant ei that does not appear in any unary atom in the subexpression with a lambda variable xi that is uniquely indexed by the corresponding node i. [sent-123, score-0.196]
</p><p>32 For example, the lambda form for nsubj (e1, e2) is λx1λx2 . [sent-124, score-0.079]
</p><p>33 298  main predicates are: e ∈ c: expression e is assigned to cluster c; SubExpr(s, e) : s is a subexpression of e; HasValue(s, v) : s is of value v; IsPart(c, i,p) : p is the property cluster in object cluster c uniquely indexed by i. [sent-127, score-0.557]
</p><p>34 In USP, property clusters in different object clusters use distinct index i’s. [sent-128, score-0.358]
</p><p>35 As we will see later,  in OntoUSP, property clusters with ISA relation share the same index i, which corresponds to a generic semantic frame such as agent and patient. [sent-129, score-0.282]
</p><p>36 The first formula is the core of the model and represents the mixture of property values given the cluster. [sent-134, score-0.158]
</p><p>37 The second formula ensures that a property cluster must be a part in the corresponding object cluster; it is a hard constraint, as signified by the period at the end. [sent-135, score-0.3]
</p><p>38 To parse a new sentence, USP starts by partitioning the QLF into atomic forms, and then hill-  climbs on the probability using a search operator based on lambda reduction until it finds the maximum a posteriori (MAP) parse. [sent-137, score-0.077]
</p><p>39 COMPOSE(c1 , c2) creates a new lambda-form cluster c formed by composing the lambda forms in c1, c2 into larger ones. [sent-142, score-0.237]
</p><p>40 The output contains the optimal lambda-form clusters and parameters, as well as the MAP semantic parses of input sentences. [sent-147, score-0.224]
</p><p>41 3  Unsupervised Ontology Induction with Markov Logic  A major limitation of USP is that it either merges two object clusters into one, or leaves them separate. [sent-148, score-0.176]
</p><p>42 This is suboptimal, because different object clusters may still possess substantial commonalities. [sent-149, score-0.156]
</p><p>43 The best way to capture such commonalities is by forming an ISA hierarchy among the clusters. [sent-151, score-0.08]
</p><p>44 Learning these ISA relations helps answer questions like “What regulates CD1 1b? [sent-153, score-0.082]
</p><p>45 Without the hierarchical structure, each cluster estimates its parameters solely based on its own observations, which can be extremely sparse. [sent-156, score-0.162]
</p><p>46 The better solution is to leverage the hierarchical structure for smoothing (a. [sent-157, score-0.088]
</p><p>47 OntoUSP overcomes the limitations of USP by replacing the flat clustering process with a hierarchical clustering one, and learns an ISA hierarchy of lambda-form clusters in addition to the IS-PART one. [sent-163, score-0.212]
</p><p>48 In effect, OntoUSP conducts ontology induction, population, and knowledge extraction in a single integrated process. [sent-165, score-0.175]
</p><p>49 Specifically, given clusters c1, c2, in addition to merge vs. [sent-166, score-0.154]
</p><p>50 separate, OntoUSP evaluates a third option called abstraction, in which a new object cluster c is created, and ISA links are added from ci to c; the argument clusters in c are formed by merging that of ci ’s. [sent-167, score-0.364]
</p><p>51 Finally, we describe the learning algorithm and how OntoUSP induces the ontology while learning the semantic parser. [sent-171, score-0.227]
</p><p>52 First, we introduce a new predicate IsA(c1 , c2), which is true if cluster c1 is a subconcept of c2. [sent-174, score-0.131]
</p><p>53 The second formula states that if  the ISA relation holds for a pair of object clusters, it also holds between their corresponding property clusters. [sent-181, score-0.172]
</p><p>54 The effect of this change is that now the value v of a subexpression x is not solely determined by its property cluster p1, but is also smoothed by statistics of all p2 that are super clusters of p1. [sent-184, score-0.383]
</p><p>55 In particular, if the weights for some property cluster p are all zero, it means that values in p are completely predicted by p’s ancestors. [sent-186, score-0.211]
</p><p>56 2 Inference Given the dependency tree T of a sentence, the conditional probability of a semantic parse L is given by Pr(L|T) ∝ exp (Pi wini(T, L)). [sent-189, score-0.076]
</p><p>57 OntoUSP uses the same inference algorithm as USP by hill-climbing  on the probability of L; in each step, OntoUSP evaluates the alternative semantic parses that can be formed by lambda-reducing a current subexpression with one of its arguments. [sent-192, score-0.202]
</p><p>58 The only difference is that OntoUSP uses a different MLN and so the probabilities and resulting semantic parses may be different. [sent-193, score-0.102]
</p><p>59 , to find parameters θ that maximizes the loglikelihood of observing the dependency trees T, summing out the unobserved semantic parses L: Lθ(T)  =  log Pθ(L)  =  logPL Pθ(T,  L)  However, the learning problem in OntoUSP is distinct in two important aspects. [sent-198, score-0.142]
</p><p>60 Second and more importantly, OntoUSP leverages this hierarchy during  learning to smooth the parameter estimation of individual clusters, as embodied by the new ISA mixture formula in the OntoUSP MLN. [sent-200, score-0.119]
</p><p>61 Like USP, OntoUSP approximates the sum over all semantic parses with the most probable parse, and searches for both θ and the MAP semantic parses L that maximize Pθ (T, L). [sent-208, score-0.204]
</p><p>62 Align property clusters of c1 and c2; for each aligned pair p1 and p2, either merge them into a single property cluster, or create an abstract property cluster p in c and create ISA links from pi to p, so as to maximize loglikelihood. [sent-212, score-0.542]
</p><p>63 To add a child cluster c2 to an existing abstract cluster c1, OntoUSP also uses an operator ADDCHILD(c1 , c2) that does the following: 1. [sent-214, score-0.262]
</p><p>64 For each property cluster of c2, maximize the log-likelihood by doing one of the following: merge it with a property cluster in an existing child of c1; create ISA link from it to an abstract property cluster in c; leave it unchanged. [sent-216, score-0.665]
</p><p>65 For efficiency, in both operators, the best option is chosen greedily for each property cluster in c2, in descending order of cluster size. [sent-217, score-0.342]
</p><p>66 Notice that once an abstract cluster is created, it could be merged with an existing cluster using MERGE. [sent-218, score-0.262]
</p><p>67 (Of course, the ISA hierarchy it actually induces depends on the data. [sent-220, score-0.102]
</p><p>68 In contrast, OntoUSP has to both induce the hierarchy and populate it, with potentially many levels in the induced hierarchy, starting from raw text with little user supervision. [sent-229, score-0.142]
</p><p>69 Namely, the weights for cluster c are set by counting its observations plus m fractional samples from its parent distribution. [sent-231, score-0.157]
</p><p>70 On one hand, it should encourage creating abstract clusters to summarize intrinsic commonalities among the children. [sent-235, score-0.143]
</p><p>71 A child cluster inherits its parent’s  slots (and thus avoids the penalty on them). [sent-240, score-0.131]
</p><p>72 On301  toUSP also stipulates that, in an ABSTRACT operation, a new property cluster can be created either as a concrete cluster with full parameterization, or as an abstract cluster that merely serves for smoothing purposes. [sent-241, score-0.509]
</p><p>73 To discourage overproposing clusters and ISA links, OntoUSP imposes a large exponential prior γ on the number of concrete clusters created by ABSTRACT. [sent-242, score-0.27]
</p><p>74 These priors and cut-off values can be tuned to control the granularity of the induced ontology and clusters. [sent-245, score-0.165]
</p><p>75 Concretely, given semantic parses L, OntoUSP computes the optimal parameters and evaluates the regularized log-likelihood as follows. [sent-246, score-0.102]
</p><p>76 For convenience, )fo∧r xea ∈ch p pair aofs property +cluv)s-. [sent-248, score-0.08]
</p><p>77 1 Methodology Evaluating unsupervised ontology induction is difficult, because there is no gold ontology for comparison. [sent-255, score-0.362]
</p><p>78 Moreover, our ultimate goal is to aid knowledge acquisition, rather than just inducing an ontology for its own sake. [sent-256, score-0.175]
</p><p>79 2  Systems  OntoUSP is the first unsupervised approach that synergistically conducts ontology induction, population, and knowledge extraction. [sent-274, score-0.194]
</p><p>80 It inputs text and outputs relational triples in the form (R, A1, A2), where R is the relation string, and A1, A2 the argument strings. [sent-284, score-0.09]
</p><p>81 To answer questions, each triple-question pair is considered in turn by first matching their relation strings,  and then the available argument strings. [sent-285, score-0.094]
</p><p>82 USP (Poon and Domingos, 2009) parses the input text using the Stanford dependency parser (Klein and Manning, 2003; de Marneffe et al. [sent-295, score-0.086]
</p><p>83 , 2006), learns an MLN for semantic parsing from the dependency trees, and outputs this MLN and the MAP semantic parses of the input sentences. [sent-296, score-0.158]
</p><p>84 These MAP parses formed the knowledge base (KB). [sent-297, score-0.125]
</p><p>85 To answer questions, USP first parses the questions (with the question slot replaced by a dummy word), and then matches the question parse to parses in the KB by testing subsumption. [sent-298, score-0.265]
</p><p>86 Second, when OntoUSP matches a question to its KB, it not only considers the lambda-form cluster of the question relation, but also all its sub-clusters. [sent-301, score-0.179]
</p><p>87 Manual inspection shows that the induced ISA hierarchy is the key for the recall gain. [sent-312, score-0.076]
</p><p>88 This allows OntoUSP to answer many more questions that are asked about general regulation events, even though the text states them with specific regulation directions like “induce” or “inhibit”. [sent-314, score-0.105]
</p><p>89 For example, both USP and OntoUSP formed a singleton cluster with core form “activate”. [sent-320, score-0.181]
</p><p>90 Although this cluster may appear similar to the INDUCE cluster, the data in GENIA does not support merging the two. [sent-321, score-0.131]
</p><p>91 In fact, they are so similar that OntoUSP merges them into a single property cluster. [sent-323, score-0.1]
</p><p>92 In turn, OntoUSP formed ISA links from these three object clusters to REGULATE, as well as among their property clusters. [sent-325, score-0.268]
</p><p>93 OntoUSP, on the other hand, formed a highlevel cluster with two abstract property clusters, corresponding to general agent argument and patient argument. [sent-338, score-0.33]
</p><p>94 4  Discussion  OntoUSP is a first step towards joint ontology induction and knowledge extraction. [sent-342, score-0.222]
</p><p>95 While OntoUSP induced meaningful ISA relations among relation clusters like REGULATE, INDUCE, etc. [sent-345, score-0.159]
</p><p>96 , it was less successful in inducing ISA relations among entity clusters such as specific genes and proteins. [sent-346, score-0.139]
</p><p>97 5  Conclusion  This paper introduced OntoUSP, the first unsupervised end-to-end system for ontology induction and knowledge extraction from text. [sent-355, score-0.241]
</p><p>98 OntoUSP builds on the USP semantic parser by adding the capability to form hierarchical clusterings of logical expressions, linked by ISA relations, and using them for hierarchical smoothing. [sent-356, score-0.157]
</p><p>99 Improving text classification by  shrinkage in a hierarchy of classes. [sent-438, score-0.109]
</p><p>100 Yago - a large ontology from Wikipedia and WordNet. [sent-462, score-0.148]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ontousp', 0.717), ('usp', 0.418), ('isa', 0.32), ('ontology', 0.148), ('cluster', 0.131), ('clusters', 0.122), ('mln', 0.104), ('property', 0.08), ('domingos', 0.07), ('parses', 0.066), ('induce', 0.066), ('poon', 0.064), ('inhibit', 0.061), ('hierarchy', 0.059), ('lambda', 0.057), ('hasvalue', 0.05), ('regulate', 0.05), ('shrinkage', 0.05), ('subexpression', 0.05), ('genia', 0.048), ('induction', 0.047), ('argument', 0.045), ('induces', 0.043), ('textrunner', 0.041), ('ispart', 0.04), ('subexpressions', 0.04), ('logical', 0.039), ('formulas', 0.039), ('formula', 0.038), ('semantic', 0.036), ('smoothing', 0.036), ('questions', 0.036), ('logic', 0.036), ('object', 0.034), ('agenda', 0.032), ('population', 0.032), ('formed', 0.032), ('atom', 0.032), ('merge', 0.032), ('hierarchical', 0.031), ('markov', 0.03), ('dudik', 0.03), ('qlf', 0.03), ('qlfs', 0.03), ('staab', 0.03), ('tousp', 0.03), ('wini', 0.03), ('answer', 0.029), ('activate', 0.028), ('knowledge', 0.027), ('parent', 0.026), ('imposes', 0.026), ('relational', 0.025), ('gelman', 0.024), ('agent', 0.024), ('question', 0.024), ('pedro', 0.023), ('nsubj', 0.022), ('mixture', 0.022), ('biomedical', 0.022), ('ontological', 0.021), ('commonalities', 0.021), ('maintains', 0.021), ('leverage', 0.021), ('relation', 0.02), ('parse', 0.02), ('dirt', 0.02), ('merges', 0.02), ('kb', 0.02), ('loglikelihood', 0.02), ('bakir', 0.02), ('ikappabalpha', 0.02), ('lowd', 0.02), ('populates', 0.02), ('regulation', 0.02), ('skolem', 0.02), ('studer', 0.02), ('subcluster', 0.02), ('subexpr', 0.02), ('map', 0.02), ('dependency', 0.02), ('capability', 0.02), ('suchanek', 0.019), ('unsupervised', 0.019), ('greatly', 0.019), ('compose', 0.019), ('taskar', 0.019), ('inference', 0.018), ('passive', 0.018), ('hoifung', 0.018), ('answering', 0.018), ('core', 0.018), ('patient', 0.018), ('pi', 0.017), ('regulates', 0.017), ('getoor', 0.017), ('signified', 0.017), ('genes', 0.017), ('induced', 0.017), ('forms', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="248-tfidf-1" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>Author: Hoifung Poon ; Pedro Domingos</p><p>Abstract: Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate On- toUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches.</p><p>2 0.089676313 <a title="248-tfidf-2" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>3 0.062913559 <a title="248-tfidf-3" href="./acl-2010-Optimizing_Question_Answering_Accuracy_by_Maximizing_Log-Likelihood.html">189 acl-2010-Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</a></p>
<p>Author: Matthias H. Heie ; Edward W. D. Whittaker ; Sadaoki Furui</p><p>Abstract: In this paper we demonstrate that there is a strong correlation between the Question Answering (QA) accuracy and the log-likelihood of the answer typing component of our statistical QA model. We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question-and-answer pairs. Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters.</p><p>4 0.053029232 <a title="248-tfidf-4" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>Author: Zornitsa Kozareva ; Eduard Hovy</p><p>Abstract: A challenging problem in open information extraction and text mining is the learning of the selectional restrictions of semantic relations. We propose a minimally supervised bootstrapping algorithm that uses a single seed and a recursive lexico-syntactic pattern to learn the arguments and the supertypes of a diverse set of semantic relations from the Web. We evaluate the performance of our algorithm on multiple semantic relations expressed using “verb”, “noun”, and “verb prep” lexico-syntactic patterns. Humanbased evaluation shows that the accuracy of the harvested information is about 90%. We also compare our results with existing knowledge base to outline the similarities and differences of the granularity and diversity of the harvested knowledge.</p><p>5 0.04918889 <a title="248-tfidf-5" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>Author: Peng Li ; Jing Jiang ; Yinglin Wang</p><p>Abstract: In this paper, we propose a novel approach to automatic generation of summary templates from given collections of summary articles. This kind of summary templates can be useful in various applications. We first develop an entity-aspect LDA model to simultaneously cluster both sentences and words into aspects. We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects. Key features of our method include automatic grouping of semantically related sentence patterns and automatic identification of template slots that need to be filled in. We apply our method on five Wikipedia entity categories and compare our method with two baseline methods. Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method.</p><p>6 0.046135351 <a title="248-tfidf-6" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>7 0.046111837 <a title="248-tfidf-7" href="./acl-2010-Open_Information_Extraction_Using_Wikipedia.html">185 acl-2010-Open Information Extraction Using Wikipedia</a></p>
<p>8 0.045587324 <a title="248-tfidf-8" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>9 0.044954639 <a title="248-tfidf-9" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>10 0.04404125 <a title="248-tfidf-10" href="./acl-2010-Starting_from_Scratch_in_Semantic_Role_Labeling.html">216 acl-2010-Starting from Scratch in Semantic Role Labeling</a></p>
<p>11 0.043952186 <a title="248-tfidf-11" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>12 0.042170931 <a title="248-tfidf-12" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>13 0.042154778 <a title="248-tfidf-13" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>14 0.039964307 <a title="248-tfidf-14" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>15 0.039371118 <a title="248-tfidf-15" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>16 0.038297039 <a title="248-tfidf-16" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>17 0.037200708 <a title="248-tfidf-17" href="./acl-2010-The_S-Space_Package%3A_An_Open_Source_Package_for_Word_Space_Models.html">232 acl-2010-The S-Space Package: An Open Source Package for Word Space Models</a></p>
<p>18 0.036526922 <a title="248-tfidf-18" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>19 0.035926938 <a title="248-tfidf-19" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>20 0.035866678 <a title="248-tfidf-20" href="./acl-2010-Identifying_Generic_Noun_Phrases.html">139 acl-2010-Identifying Generic Noun Phrases</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.116), (1, 0.053), (2, 0.018), (3, -0.017), (4, 0.02), (5, -0.01), (6, 0.02), (7, 0.004), (8, 0.014), (9, -0.014), (10, -0.031), (11, 0.003), (12, -0.015), (13, -0.048), (14, 0.043), (15, 0.041), (16, -0.046), (17, 0.045), (18, 0.018), (19, -0.017), (20, 0.0), (21, 0.042), (22, -0.023), (23, -0.037), (24, -0.031), (25, 0.068), (26, -0.039), (27, -0.01), (28, -0.007), (29, -0.062), (30, 0.018), (31, -0.065), (32, 0.022), (33, 0.051), (34, 0.034), (35, 0.037), (36, 0.065), (37, 0.025), (38, -0.012), (39, -0.005), (40, -0.126), (41, -0.002), (42, -0.037), (43, 0.009), (44, 0.053), (45, 0.069), (46, 0.007), (47, 0.021), (48, -0.029), (49, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89176118 <a title="248-lsi-1" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>Author: Hoifung Poon ; Pedro Domingos</p><p>Abstract: Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate On- toUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches.</p><p>2 0.57940137 <a title="248-lsi-2" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>Author: Dmitry Davidov ; Ari Rappoport</p><p>Abstract: We present a novel framework for automated extraction and approximation of numerical object attributes such as height and weight from the Web. Given an object-attribute pair, we discover and analyze attribute information for a set of comparable objects in order to infer the desired value. This allows us to approximate the desired numerical values even when no exact values can be found in the text. Our framework makes use of relation defining patterns and WordNet similarity information. First, we obtain from the Web and WordNet a list of terms similar to the given object. Then we retrieve attribute values for each term in this list, and information that allows us to compare different objects in the list and to infer the attribute value range. Finally, we combine the retrieved data for all terms from the list to select or approximate the requested value. We evaluate our method using automated question answering, WordNet enrichment, and comparison with answers given in Wikipedia and by leading search engines. In all of these, our framework provides a significant improvement.</p><p>3 0.57502478 <a title="248-lsi-3" href="./acl-2010-Optimizing_Question_Answering_Accuracy_by_Maximizing_Log-Likelihood.html">189 acl-2010-Optimizing Question Answering Accuracy by Maximizing Log-Likelihood</a></p>
<p>Author: Matthias H. Heie ; Edward W. D. Whittaker ; Sadaoki Furui</p><p>Abstract: In this paper we demonstrate that there is a strong correlation between the Question Answering (QA) accuracy and the log-likelihood of the answer typing component of our statistical QA model. We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question-and-answer pairs. Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters.</p><p>4 0.5714469 <a title="248-lsi-4" href="./acl-2010-Complexity_Assumptions_in_Ontology_Verbalisation.html">64 acl-2010-Complexity Assumptions in Ontology Verbalisation</a></p>
<p>Author: Richard Power</p><p>Abstract: We describe the strategy currently pursued for verbalising OWL ontologies by sentences in Controlled Natural Language (i.e., combining generic rules for realising logical patterns with ontology-specific lexicons for realising atomic terms for individuals, classes, and properties) and argue that its success depends on assumptions about the complexity of terms and axioms in the ontology. We then show, through analysis of a corpus of ontologies, that although these assumptions could in principle be violated, they are overwhelmingly respected in practice by ontology developers.</p><p>5 0.53331333 <a title="248-lsi-5" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>Author: Omri Abend ; Roi Reichart ; Ari Rappoport</p><p>Abstract: We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of the induced POS categories. The rest of the words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task.</p><p>6 0.50761843 <a title="248-lsi-6" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>7 0.48953268 <a title="248-lsi-7" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>8 0.48842636 <a title="248-lsi-8" href="./acl-2010-On_Learning_Subtypes_of_the_Part-Whole_Relation%3A_Do_Not_Mix_Your_Seeds.html">181 acl-2010-On Learning Subtypes of the Part-Whole Relation: Do Not Mix Your Seeds</a></p>
<p>9 0.48729524 <a title="248-lsi-9" href="./acl-2010-Open_Information_Extraction_Using_Wikipedia.html">185 acl-2010-Open Information Extraction Using Wikipedia</a></p>
<p>10 0.4790073 <a title="248-lsi-10" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>11 0.43919003 <a title="248-lsi-11" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>12 0.43737653 <a title="248-lsi-12" href="./acl-2010-Automatically_Generating_Term_Frequency_Induced_Taxonomies.html">43 acl-2010-Automatically Generating Term Frequency Induced Taxonomies</a></p>
<p>13 0.43604508 <a title="248-lsi-13" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>14 0.4269543 <a title="248-lsi-14" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>15 0.42592964 <a title="248-lsi-15" href="./acl-2010-Compositional_Matrix-Space_Models_of_Language.html">66 acl-2010-Compositional Matrix-Space Models of Language</a></p>
<p>16 0.42585757 <a title="248-lsi-16" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>17 0.41161907 <a title="248-lsi-17" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>18 0.40859854 <a title="248-lsi-18" href="./acl-2010-Modeling_Semantic_Relevance_for_Question-Answer_Pairs_in_Web_Social_Communities.html">174 acl-2010-Modeling Semantic Relevance for Question-Answer Pairs in Web Social Communities</a></p>
<p>19 0.40558404 <a title="248-lsi-19" href="./acl-2010-Metadata-Aware_Measures_for_Answer_Summarization_in_Community_Question_Answering.html">171 acl-2010-Metadata-Aware Measures for Answer Summarization in Community Question Answering</a></p>
<p>20 0.39728323 <a title="248-lsi-20" href="./acl-2010-Mood_Patterns_and_Affective_Lexicon_Access_in_Weblogs.html">176 acl-2010-Mood Patterns and Affective Lexicon Access in Weblogs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.02), (25, 0.082), (33, 0.019), (39, 0.018), (42, 0.031), (44, 0.012), (59, 0.088), (72, 0.019), (73, 0.042), (76, 0.013), (78, 0.061), (80, 0.014), (83, 0.079), (84, 0.029), (96, 0.269), (98, 0.091)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78357178 <a title="248-lda-1" href="./acl-2010-Extracting_Sequences_from_the_Web.html">111 acl-2010-Extracting Sequences from the Web</a></p>
<p>Author: Anthony Fader ; Stephen Soderland ; Oren Etzioni</p><p>Abstract: Classical Information Extraction (IE) systems fill slots in domain-specific frames. This paper reports on SEQ, a novel open IE system that leverages a domainindependent frame to extract ordered sequences such as presidents of the United States or the most common causes of death in the U.S. SEQ leverages regularities about sequences to extract a coherent set of sequences from Web text. SEQ nearly doubles the area under the precision-recall curve compared to an extractor that does not exploit these regularities.</p><p>same-paper 2 0.75851488 <a title="248-lda-2" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>Author: Hoifung Poon ; Pedro Domingos</p><p>Abstract: Extracting knowledge from unstructured text is a long-standing goal of NLP. Although learning approaches to many of its subtasks have been developed (e.g., parsing, taxonomy induction, information extraction), all end-to-end solutions to date require heavy supervision and/or manual engineering, limiting their scope and scalability. We present OntoUSP, a system that induces and populates a probabilistic ontology using only dependency-parsed text as input. OntoUSP builds on the USP unsupervised semantic parser by jointly forming ISA and IS-PART hierarchies of lambda-form clusters. The ISA hierarchy allows more general knowledge to be learned, and the use of smoothing for parameter estimation. We evaluate On- toUSP by using it to extract a knowledge base from biomedical abstracts and answer questions. OntoUSP improves on the recall of USP by 47% and greatly outperforms previous state-of-the-art approaches.</p><p>3 0.55457789 <a title="248-lda-3" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>4 0.55295575 <a title="248-lda-4" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and treesubstitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.</p><p>5 0.55191886 <a title="248-lda-5" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>Author: Diarmuid O Seaghdha</p><p>Abstract: This paper describes the application of so-called topic models to selectional preference induction. Three models related to Latent Dirichlet Allocation, a proven method for modelling document-word cooccurrences, are presented and evaluated on datasets of human plausibility judgements. Compared to previously proposed techniques, these models perform very competitively, especially for infrequent predicate-argument combinations where they exceed the quality of Web-scale predictions while using relatively little data.</p><p>6 0.55131781 <a title="248-lda-6" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>7 0.550955 <a title="248-lda-7" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>8 0.54991657 <a title="248-lda-8" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>9 0.54799557 <a title="248-lda-9" href="./acl-2010-Contextualizing_Semantic_Representations_Using_Syntactically_Enriched_Vector_Models.html">70 acl-2010-Contextualizing Semantic Representations Using Syntactically Enriched Vector Models</a></p>
<p>10 0.54689533 <a title="248-lda-10" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>11 0.54588324 <a title="248-lda-11" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>12 0.5458293 <a title="248-lda-12" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>13 0.54478854 <a title="248-lda-13" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>14 0.54449964 <a title="248-lda-14" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>15 0.54211211 <a title="248-lda-15" href="./acl-2010-Learning_Arguments_and_Supertypes_of_Semantic_Relations_Using_Recursive_Patterns.html">160 acl-2010-Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns</a></p>
<p>16 0.54101747 <a title="248-lda-16" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>17 0.5407837 <a title="248-lda-17" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>18 0.54002476 <a title="248-lda-18" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>19 0.53992897 <a title="248-lda-19" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>20 0.53936315 <a title="248-lda-20" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
