<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-255" href="#">acl2010-255</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</h1>
<br/><p>Source: <a title="acl-2010-255-pdf" href="http://aclweb.org/anthology//P/P10/P10-1152.pdf">pdf</a></p><p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><p>Reference: <a title="acl-2010-255-reference" href="../acl2010_reference/acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training. [sent-5, score-0.467]
</p><p>2 ” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. [sent-6, score-0.109]
</p><p>3 We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. [sent-7, score-0.278]
</p><p>4 Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. [sent-13, score-0.202]
</p><p>5 Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training. [sent-17, score-0.097]
</p><p>6 We first show that under the assumption that P NP, solving wan tdha even approximating tnh teh aVti Pter6 =bi N training problem is hard. [sent-19, score-0.177]
</p><p>7 We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training. [sent-21, score-0.199]
</p><p>8 We then describe a “competitiveness” result for uniform initialization of Viterbi EM: we show that initialization of the trees in an E-step which uses uniform distributions over the trees is optimal with respect to a certain approximate bound. [sent-22, score-0.383]
</p><p>9 =  2  Background and Notation  We assume familiarity with probabilistic contextfree grammars (PCFGs). [sent-32, score-0.082]
</p><p>10 A PCFG G consists of: • • •  •  A finite set of nonterminal symbols N; A finite set of terminal symbols Σ; For each A ∈ N, a set of rewrite rules R(A) of tFhoer feoarcmh AA → α, wsethe orfe α ∈ (N e∪s Σ)∗, a)n odf tRh = ∪A∈NR(A); For each rule A → α, a probability θA→α. [sent-33, score-0.154]
</p><p>11 fA(z) wleil lA similarly adpepneoaters sth ien number of times that nonterminal A appears in z. [sent-38, score-0.133]
</p><p>12 , zni, lGeti:v Xn  FA→α(z)  =  XfA→α(zi)  (2)  Xi= X1 XXn  FA(z)  = XfA(zi)  (3)  Xi= X1  We use the following notation for G: •  L(G) is the set of all strings (sentences) x that can b)e i generated using nthges grammar G) x (the “language of G”). [sent-42, score-0.096]
</p><p>13 •  •  3  D(G) is the set of all possible derivations z that can G be) generated using othssei grammar tGio. [sent-43, score-0.173]
</p><p>14 n D(G, x) is the set of all possible derivations z tDha(tG can )b ies generated using tshseib grammar Gio nasn dz have the yield x. [sent-44, score-0.173]
</p><p>15 With PCFGs, the E-step requires running an algorithm such as (probabilistic) CKY or Earley’s  1Note that x = yield(z) ; if the derivation is known, the string is also known. [sent-54, score-0.103]
</p><p>16 On the other hand, there may be many derivations with the same yield, perhaps even infinitely many. [sent-55, score-0.077]
</p><p>17 We can understand Viterbi EM as a coordinate ascent procedure that approximates the solution to the following declarative problem: Problem 1. [sent-57, score-0.097]
</p><p>18 We wzil∈l D re(Gfe,rx to Yn  L(θ,z)  =  Yp(xi,zi  | θ)  (5)  iY= Y1  as “the objective function of ViterbiTrain. [sent-72, score-0.076]
</p><p>19 consider selftraining to be round of Viterbi EM” with supervised initialization using labeled seed data. [sent-78, score-0.102]
</p><p>20 “one  4  Hardness of Viterbi Training  We now describe hardness results for Problem 1. [sent-80, score-0.164]
</p><p>21 The extracted assignment would∨ ∨ beY Y∨1 1, Y4 = 0. [sent-83, score-0.134]
</p><p>22 Output: 1if there is a satisfying assignment for φ and 0 otherwise. [sent-87, score-0.249]
</p><p>23 Given an instance of the 3-SAT problem, the reduction will, in polynomial time, create a  grammar and a single string such that solving the ViterbiTrain problem for this grammar and string will yield a solution for the instance of the 3-SAT problem. [sent-89, score-0.454]
</p><p>24 Let φ = Vim=1 (ai ∨ bi ∨ ci) be an instance of the 3-SAT prVoblem, w∨h bere∨ ai, bi and ci are literals over the sVet of variables {Y1, . [sent-90, score-0.12]
</p><p>25 aL liette Cj be the jth clause in φ, such that Cj = aj ∨ bj ∨ cj. [sent-94, score-0.361]
</p><p>26 We define the following context-free grammar Gφ and string to parse sφ: 1. [sent-95, score-0.242]
</p><p>27 Let Y (aj) be the ∨va briab∨le c that aj mentions. [sent-123, score-0.221]
</p><p>28 Let (y1, y2, y3) be a satisfying assignment for Cj where yk ∈ {0, 1} and is the value of Y (aj), Y (bj) and Y (cj) respectively efo vra lku ∈ {1, 2, 3}. [sent-124, score-0.249]
</p><p>29 For each such clause-satisfying assignment, we add the rule:  Aj →  UY (aj),y1UY (bj),y2UY (cj),y3  (6)  For each Aj, we would have at most 7 rules of that form, since one rule will be logically inconsistent with aj ∨ bj ∨ cj. [sent-125, score-0.425]
</p><p>30 A parse of the string sφ using Gφ will be used to get an assignment by setting Yr = 0 if the rule VYr → 0 or VY¯r → 1are used in the derivation of the parse tree, an→d →1 1 1o athreer uwsiesde. [sent-132, score-0.456]
</p><p>31 i nN tohteic dee trhivaat iaot nth oisf point we do not exclude “contradictions” coming from the parse tree, such as VY3 → 0 used in the  grammar’s  tree together with VY3 → 1or VY¯3 → u0. [sent-133, score-0.131]
</p><p>32 s eTdh ien f tohlelowing lemma gives a →con 1di otiro Vn un→der 0 . [sent-134, score-0.078]
</p><p>33 w Thihceh ftohleassignment is consistent (so contradictions do not occur in the parse tree): Lemma 1. [sent-135, score-0.12]
</p><p>34 Let φ be an instance of the 3-SAT problem, and let Gφ be a probabilistic CFG based on the above grammar with weights θφ. [sent-136, score-0.172]
</p><p>35 If the (multiplicative) weight of the Viterbi parse of sφ is 1, then the assignment extracted from the parse tree is consistent. [sent-137, score-0.352]
</p><p>36 Since the probability of the Viterbi parse is 1, all rules of the form {VYr , VY¯r } → {0, 1} iwsh 1ic,h a appear i onf ft thhee parse t {reVe have probability 1} as well. [sent-139, score-0.225]
</p><p>37 For any r, an appearance of both rules of the form VYr → 0 and VYr → 1cannot occur because all r→ules 0 t ahnadt appear i n1 tchaen nVoitte orbcci parse tree have probability 1. [sent-142, score-0.227]
</p><p>38 There exists θφ such that the Viterbi parse of sφ is 1if and only if φ is satisfiable. [sent-150, score-0.087]
</p><p>39 Moreover, the satisfying assignment is the one extracted from the parse tree with weight 1of sφ under θφ. [sent-151, score-0.38]
</p><p>40 ⇒Ea)c Ah scsluamusee Cj = aj i∨s bj ∨at cj yiisn sgat aiss-fied using a tuple (y1, y2, y3) wh∨ich b assigns value for Y (aj), Y (bj) and Y (cj). [sent-154, score-0.489]
</p><p>41 This assignment corresponds the following rule  Aj → UY (aj),y1UY (bj),y2UY (cj),y3  (7)  Set its probability to 1, and set all other rules of Aj to 0. [sent-155, score-0.23]
</p><p>42 t This assignment of rule probabilities results in a Viterbi parse of weight 1. [sent-158, score-0.266]
</p><p>43 (⇐=) Assume that the Viterbi parse has probability 1) . [sent-159, score-0.087]
</p><p>44 Fsrsoumm eL ethmatm tah 1, we bkin powar teha hta we can extract a consistent assignment from the Viterbi parse. [sent-160, score-0.134]
</p><p>45 In addition, for each clause Cj we have a rule  Aj → UY (aj),y1UY (bj),y2UY (cj),y3  (8)  that is assigned probability 1, for some (y1, y2, y3). [sent-161, score-0.077]
</p><p>46 , xn and α ≥ 0,  is the optimized value of the objective f αunc ≥tio 0n, L (θ, z) ≥ α? [sent-166, score-0.134]
</p><p>47 5  Hardness of Approximation  A natural path of exploration following the hardness result we showed is determining whether an approximation of ViterbiTrain is also hard. [sent-171, score-0.221]
</p><p>48 Perhaps there is an efficient approximation algorithm for ViterbiTrain we could use instead of coordinate ascent algorithms such as Viterbi EM. [sent-172, score-0.154]
</p><p>49 We next show that approximating the objective function of ViterbiTrain with a constant factor of ρ is hard for any ρ ∈ (21, 1] (i. [sent-174, score-0.184]
</p><p>50 h Terhei sis m no enfsfi tchiaetn,t u uanl--  =  gorithm that, given a grammar rGe asn ndo a sample aol-f sentences x1, . [sent-180, score-0.096]
</p><p>51 We first note that if p(sφ, z | θ) < 1 (implying that there is no satisfying assignment), then there must be a nonterminal which appears along with two different rules in z. [sent-186, score-0.26]
</p><p>52 This means that we have a nonterminal B ∈ N wiTthh some nrusl eth aBt → α t aha nto appears kl B times, wwhitihle othme en ronutleer mBin→a l appears i anp tpheea parse r ≥ wk + e1 thtieme nso. [sent-187, score-0.217]
</p><p>53 tGerimveinn tlhe a tpreeea z, tnh teh θe ptharats em rax ≥imizes the objective function is the maximum likelihood estimate (MLE) for z (counting and noraspreon tdhieng va vlauerisab olfes th ien a cslsaiugsnem Cenj,t a fonrd th theat co thr eeys aatti s f yied th bisy c tlhaeu ases. [sent-188, score-0.351]
</p><p>54 k≤21  (10)  This means that if the value of the objective function of ViterbiTrain is not 1 using the reduction from §4, then it is at most 21. [sent-203, score-0.14]
</p><p>55 If we had an efficient approximate algorithm wstith approximation coefficient ρ > (Eq. [sent-204, score-0.09]
</p><p>56 9 holds), then in order to solve 3-SAT for formula φ, we could run the algorithm on Gφ and sφ and check whether the assignment to (θ, z) that the algorithm returns satisfies φ or not, and return our response accordingly. [sent-205, score-0.173]
</p><p>57 z owuoludld re thuarvne tθo, correspond tto L a satisfying assignment, and in fact p(z | θ) = 1, because iinn any iogtnhmere case, dth ien probability o)f = a 1d,e brievcaatiuosne which does not represent a satisfying assignment is smaller than 21. [sent-207, score-0.403]
</p><p>58 If φ were not satisfiable, then the approximation algorithm would never return a (θ, z) that results in a satisfying assignment (because such a (θ, z) does not exist). [sent-208, score-0.306]
</p><p>59 The conclusion is that an efficient algorithm for approximating the objective function of ViterbiTrain (Eq. [sent-209, score-0.14]
</p><p>60 Note that there is a hidden assumption in this problem definition, that xi can be parsed using the grammar G. [sent-224, score-0.26]
</p><p>61 We can extend Cond|itiθo n,axlViterbiTrain to return ⊥ in the case of not having a parse fTorra one o ref ttuhren xi—this can e be o efficiently gch ae pcakresed using a run of a cubic-time parser on each of the strings xi with the grammar G. [sent-226, score-0.274]
</p><p>62 Our hardness result for ViterbiTrain applies to ConditionalViterbiTrain as well. [sent-229, score-0.164]
</p><p>63 The reason is  that if p(z, sφ | θφ) = 1for a φ with a satisfying assignment, the|n θ L(G) = {sφ} and D(G) = {z}. [sent-230, score-0.115]
</p><p>64 isfiable, ptlhieesn tfhoart th p(ez optimal θ of ViterbiTrain we have z and z0 such that 0 < p(z, sφ | θφ) < 1 and 0 < p(z0, sφ | θφ) < 1, and ther|efo θre p(z | θφ, sφ) < 1, whic|h means t1h,e a cnodn tdhietrioenfoarle objective function will not obtain the value 1. [sent-233, score-0.108]
</p><p>65 e Inf we actually do have a satisfying assignment from 1506  Lemma 1. [sent-245, score-0.249]
</p><p>66 Otherwise (more than a single derivation), the optimal θ would have to give fractional probabilities to rules of the form VYr → {0, 1} (or VY¯r → {0, 1}). [sent-246, score-0.083]
</p><p>67 12 can be maximized approximately using algorithms like EM, so this gives a hardness result for optimizing the objective function of EM for PCFGs. [sent-250, score-0.24]
</p><p>68 Day (1983) previously showed that maximizing the marginalized likelihood for hidden Markov models is NP-hard. [sent-251, score-0.093]
</p><p>69 We note that the grammar we use for all of our results is not recursive. [sent-252, score-0.096]
</p><p>70 Therefore, we can encode this grammar as a hidden Markov model, strengthening our result from PCFGs to HMMs. [sent-253, score-0.134]
</p><p>71 3 7  Uniform-at-Random Initialization  In the previous sections, we showed that solving Viterbi training is hard, and therefore requires an approximation algorithm. [sent-254, score-0.102]
</p><p>72 Viterbi EM, which is an example of such algorithm, is dependent on an initialization of either θ to start with an E-step or z to start with an M-step. [sent-255, score-0.102]
</p><p>73 In the absence of a betterinformed initializer, it is reasonable to initialize z using a uniform distribution over D(G, xi) for each i. [sent-256, score-0.102]
</p><p>74 We turn next to an analysis of this initialization technique that suggests it is well-motivated. [sent-259, score-0.102]
</p><p>75 The sketch of our result is as follows: we first give an asymptotic upper bound for the log-  ×  likelihood of derivations and sentences. [sent-260, score-0.178]
</p><p>76 This bound, which has an information-theoretic interpretation, depends on a parameter λ, which depends on the distribution from which the derivations were chosen. [sent-261, score-0.122]
</p><p>77 We then show that this bound is minimized when we pick λ such that this distribution is (conditioned on the sentence) a uniform distribution over derivations. [sent-262, score-0.193]
</p><p>78 Let q(x) be any distribution over L(G) and θ some parameters for G. [sent-263, score-0.078]
</p><p>79 hat all derivations of x will h1a/|vRe t(Ahe) same enu gmetb tehra otf a arlul e dse rainvda hioennsce o tfhe x same probability. [sent-270, score-0.077]
</p><p>80 This condition does not hold for grammars with unary cycles because |D(G, x) | may be imnafirnsit we ftohr u some ydecrlievsa btieocnasu. [sent-271, score-0.117]
</p><p>81 Let us assume that some “correct” parameters θ∗ exist, and that our data were drawn from a distribution parametrized by θ∗. [sent-273, score-0.078]
</p><p>82 The goal of this section is to motivate the following initialization for θ, which we call UniformInit: 1. [sent-274, score-0.102]
</p><p>83 Initialize z by sampling from the uniform distribution over D(G, xi) for each xi. [sent-275, score-0.102]
</p><p>84 Then, if we changed the representation of the objective function of the ViterbiTrain problem to loglikelihood, for θ0 that maximizes Eq. [sent-327, score-0.111]
</p><p>85 ePquired to encode a tree in our sample using 1508  θ∗, while removing dependence among all rules and assuming that each node at the tree is chosen uniformly. [sent-337, score-0.139]
</p><p>86 We next show that the uniform distribution optimizes λ in that sense. [sent-345, score-0.102]
</p><p>87 2 Optimizing λ Note that the optimal choice of λ, for a single x and for candidate initializer θ0, is  λopt(x,θ∗;θ0)  =  z∈Dsu(Gp,x)pp((z | θ θ∗0,,x ) (30)  In order to avoid degenerate cases, we will add another condition on the true model, θ∗ :  Condition 2. [sent-347, score-0.164]
</p><p>88 Since we re-  λ,  quire a zuni |fo θr,mx posterior distribution, cthee w number of derivations of a fixed length is finite. [sent-362, score-0.118]
</p><p>89 8  Related Work  Viterbi training is closely related to the k-means clustering problem, where the objective is to find k centroids for a given set of d-dimensional points such that the sum of distances between the points and the closest centroid is minimized. [sent-364, score-0.119]
</p><p>90 The analog for Viterbi EM for the k-means problem is the k-means clustering algorithm (Lloyd, 1982), a coordinate ascent algorithm for solving the k-means problem. [sent-365, score-0.177]
</p><p>91 They show that their initialization is O(log k)-competitive; i. [sent-373, score-0.102]
</p><p>92 , it approximates the optimal clusters assignment by a factor of O(log k). [sent-375, score-0.166]
</p><p>93 i1o,n iws approximately O(|N|Lλ2/n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and λ is a measure for distance between the true  distribution and the uniform distribution. [sent-378, score-0.102]
</p><p>94 Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. [sent-380, score-0.225]
</p><p>95 We gave motivation for uniform-at-random initialization for deriva-  tions in the Viterbi EM algorithm. [sent-384, score-0.102]
</p><p>96 On the computational complexity of approximating distributions by prob5Making the assumption that the grammar is in CNF permits us to use L instead of B, since there is a linear relationship between them in that case. [sent-391, score-0.196]
</p><p>97 Com-  putational complexity of problems on probabilistic grammars and transducers. [sent-418, score-0.118]
</p><p>98 Statistical parsing with a contextfree grammar and word statistics. [sent-424, score-0.096]
</p><p>99 Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction. [sent-433, score-0.096]
</p><p>100 The consensus string problem and the complexity of comparing hidden Markov models. [sent-523, score-0.168]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('viterbitrain', 0.346), ('viterbi', 0.34), ('vy', 0.291), ('vyr', 0.265), ('aj', 0.221), ('fa', 0.204), ('em', 0.181), ('hardness', 0.164), ('cj', 0.16), ('assignment', 0.134), ('conditionalviterbitrain', 0.122), ('satisfying', 0.115), ('bj', 0.108), ('initialization', 0.102), ('zi', 0.101), ('grammar', 0.096), ('pcfgs', 0.093), ('xi', 0.091), ('parse', 0.087), ('yn', 0.084), ('satisfiable', 0.081), ('derivations', 0.077), ('objective', 0.076), ('fb', 0.072), ('condition', 0.067), ('initializer', 0.065), ('mcclosky', 0.064), ('approximating', 0.064), ('reduction', 0.064), ('faf', 0.061), ('uniforminit', 0.061), ('string', 0.059), ('eq', 0.058), ('xn', 0.058), ('nonterminal', 0.058), ('uniform', 0.057), ('approximation', 0.057), ('ascent', 0.055), ('likelihood', 0.055), ('johnson', 0.054), ('rules', 0.051), ('grammars', 0.05), ('denero', 0.049), ('spitkovsky', 0.049), ('quantity', 0.048), ('opt', 0.048), ('uy', 0.046), ('bound', 0.046), ('distribution', 0.045), ('appearance', 0.045), ('solving', 0.045), ('rule', 0.045), ('tree', 0.044), ('derivation', 0.044), ('hard', 0.044), ('let', 0.044), ('yejin', 0.043), ('neal', 0.043), ('yr', 0.043), ('centroids', 0.043), ('bn', 0.043), ('coordinate', 0.042), ('bi', 0.042), ('posterior', 0.041), ('afa', 0.041), ('aloise', 0.041), ('eeeeeeeeeeeeeeeeeeeyyyyyyyyyyyyyyyyyyy', 0.041), ('epe', 0.041), ('grenander', 0.041), ('limn', 0.041), ('lyngs', 0.041), ('udupa', 0.041), ('unsatisfiable', 0.041), ('vim', 0.041), ('xfa', 0.041), ('lemma', 0.039), ('ien', 0.039), ('formula', 0.039), ('hidden', 0.038), ('complexity', 0.036), ('ci', 0.036), ('cnf', 0.036), ('abilistic', 0.036), ('mahajan', 0.036), ('appears', 0.036), ('problem', 0.035), ('cohen', 0.035), ('cfg', 0.035), ('goldwater', 0.035), ('smith', 0.035), ('parameters', 0.033), ('contradictions', 0.033), ('alia', 0.033), ('competitiveness', 0.033), ('tdha', 0.033), ('hinton', 0.033), ('approximate', 0.033), ('clause', 0.032), ('probabilistic', 0.032), ('optimal', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999851 <a title="255-tfidf-1" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><p>2 0.16285986 <a title="255-tfidf-2" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>Author: Nobuhiro Kaji ; Yasuhiro Fujiwara ; Naoki Yoshinaga ; Masaru Kitsuregawa</p><p>Abstract: The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algo- rithm, CARPEDIEM (Esposito and Radicioni, 2009).</p><p>3 0.13865432 <a title="255-tfidf-3" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>4 0.11431765 <a title="255-tfidf-4" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>Author: Tomoharu Iwata ; Daichi Mochihashi ; Hiroshi Sawada</p><p>Abstract: We propose a corpus-based probabilistic framework to extract hidden common syntax across languages from non-parallel multilingual corpora in an unsupervised fashion. For this purpose, we assume a generative model for multilingual corpora, where each sentence is generated from a language dependent probabilistic contextfree grammar (PCFG), and these PCFGs are generated from a prior grammar that is common across languages. We also develop a variational method for efficient inference. Experiments on a non-parallel multilingual corpus of eleven languages demonstrate the feasibility of the proposed method.</p><p>5 0.1094358 <a title="255-tfidf-5" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>6 0.10155421 <a title="255-tfidf-6" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>7 0.097710416 <a title="255-tfidf-7" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>8 0.09635713 <a title="255-tfidf-8" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>9 0.095976606 <a title="255-tfidf-9" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>10 0.092870079 <a title="255-tfidf-10" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>11 0.082607903 <a title="255-tfidf-11" href="./acl-2010-Top-Down_K-Best_A%2A_Parsing.html">236 acl-2010-Top-Down K-Best A* Parsing</a></p>
<p>12 0.079175085 <a title="255-tfidf-12" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>13 0.078185037 <a title="255-tfidf-13" href="./acl-2010-String_Extension_Learning.html">217 acl-2010-String Extension Learning</a></p>
<p>14 0.070723258 <a title="255-tfidf-14" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>15 0.069231138 <a title="255-tfidf-15" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>16 0.069002368 <a title="255-tfidf-16" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>17 0.064780787 <a title="255-tfidf-17" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>18 0.063409828 <a title="255-tfidf-18" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>19 0.062036492 <a title="255-tfidf-19" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>20 0.06177295 <a title="255-tfidf-20" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.181), (1, -0.058), (2, 0.047), (3, -0.023), (4, -0.069), (5, -0.078), (6, 0.128), (7, 0.017), (8, 0.141), (9, -0.069), (10, -0.068), (11, -0.041), (12, 0.055), (13, -0.112), (14, -0.042), (15, -0.099), (16, -0.079), (17, -0.005), (18, -0.048), (19, 0.059), (20, -0.053), (21, 0.062), (22, -0.052), (23, -0.053), (24, -0.053), (25, 0.019), (26, 0.079), (27, 0.032), (28, 0.013), (29, 0.077), (30, 0.045), (31, -0.004), (32, -0.133), (33, -0.04), (34, 0.001), (35, -0.056), (36, 0.035), (37, -0.006), (38, 0.069), (39, -0.067), (40, 0.094), (41, -0.075), (42, 0.022), (43, 0.034), (44, 0.012), (45, -0.098), (46, -0.111), (47, -0.029), (48, 0.068), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95586389 <a title="255-lsi-1" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><p>2 0.73553008 <a title="255-lsi-2" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>Author: Nobuhiro Kaji ; Yasuhiro Fujiwara ; Naoki Yoshinaga ; Masaru Kitsuregawa</p><p>Abstract: The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algo- rithm, CARPEDIEM (Esposito and Radicioni, 2009).</p><p>3 0.7299118 <a title="255-lsi-3" href="./acl-2010-Efficient_Optimization_of_an_MDL-Inspired_Objective_Function_for_Unsupervised_Part-Of-Speech_Tagging.html">96 acl-2010-Efficient Optimization of an MDL-Inspired Objective Function for Unsupervised Part-Of-Speech Tagging</a></p>
<p>Author: Ashish Vaswani ; Adam Pauls ; David Chiang</p><p>Abstract: The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also develop a efficient general search algorithm based on the MAP-EM framework to optimize this function. Since recent work has shown that minimizing the model size in a Hidden Markov Model for part-of-speech (POS) tagging leads to higher accuracies, we test our approach by applying it to this problem. The search algorithm involves a simple change to EM and achieves high POS tagging accuracies on both English and Italian data sets.</p><p>4 0.60738844 <a title="255-lsi-4" href="./acl-2010-Sparsity_in_Dependency_Grammar_Induction.html">214 acl-2010-Sparsity in Dependency Grammar Induction</a></p>
<p>Author: Jennifer Gillenwater ; Kuzman Ganchev ; Joao Graca ; Fernando Pereira ; Ben Taskar</p><p>Abstract: A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graça et al. (2007). In ex- periments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques.</p><p>5 0.6012907 <a title="255-lsi-5" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>Author: Sujith Ravi ; Jason Baldridge ; Kevin Knight</p><p>Abstract: We combine two complementary ideas for learning supertaggers from highly ambiguous lexicons: grammar-informed tag transitions and models minimized via integer programming. Each strategy on its own greatly improves performance over basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization.</p><p>6 0.49964419 <a title="255-lsi-6" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>7 0.49630818 <a title="255-lsi-7" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>8 0.49573052 <a title="255-lsi-8" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>9 0.49521762 <a title="255-lsi-9" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>10 0.47561616 <a title="255-lsi-10" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>11 0.46864343 <a title="255-lsi-11" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>12 0.4298363 <a title="255-lsi-12" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>13 0.41912201 <a title="255-lsi-13" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>14 0.41390237 <a title="255-lsi-14" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>15 0.40714613 <a title="255-lsi-15" href="./acl-2010-Phylogenetic_Grammar_Induction.html">195 acl-2010-Phylogenetic Grammar Induction</a></p>
<p>16 0.40407845 <a title="255-lsi-16" href="./acl-2010-SVD_and_Clustering_for_Unsupervised_POS_Tagging.html">205 acl-2010-SVD and Clustering for Unsupervised POS Tagging</a></p>
<p>17 0.3932054 <a title="255-lsi-17" href="./acl-2010-Simple_Semi-Supervised_Training_of_Part-Of-Speech_Taggers.html">212 acl-2010-Simple Semi-Supervised Training of Part-Of-Speech Taggers</a></p>
<p>18 0.39236596 <a title="255-lsi-18" href="./acl-2010-Computing_Weakest_Readings.html">67 acl-2010-Computing Weakest Readings</a></p>
<p>19 0.38854861 <a title="255-lsi-19" href="./acl-2010-String_Extension_Learning.html">217 acl-2010-String Extension Learning</a></p>
<p>20 0.38817829 <a title="255-lsi-20" href="./acl-2010-Optimal_Rank_Reduction_for_Linear_Context-Free_Rewriting_Systems_with_Fan-Out_Two.html">186 acl-2010-Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.017), (9, 0.265), (14, 0.02), (25, 0.087), (33, 0.02), (39, 0.018), (42, 0.026), (44, 0.01), (59, 0.108), (69, 0.018), (71, 0.013), (73, 0.029), (76, 0.014), (78, 0.039), (80, 0.014), (83, 0.081), (84, 0.032), (98, 0.119)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80174285 <a title="255-lda-1" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>Author: Shay Cohen ; Noah A Smith</p><p>Abstract: We consider the search for a maximum likelihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood.</p><p>2 0.79599595 <a title="255-lda-2" href="./acl-2010-Recommendation_in_Internet_Forums_and_Blogs.html">204 acl-2010-Recommendation in Internet Forums and Blogs</a></p>
<p>Author: Jia Wang ; Qing Li ; Yuanzhu Peter Chen ; Zhangxi Lin</p><p>Abstract: The variety of engaging interactions among users in social medial distinguishes it from traditional Web media. Such a feature should be utilized while attempting to provide intelligent services to social media participants. In this article, we present a framework to recommend relevant information in Internet forums and blogs using user comments, one of the most representative of user behaviors in online discussion. When incorporating user comments, we consider structural, semantic, and authority information carried by them. One of the most important observation from this work is that semantic contents of user comments can play a fairly different role in a different form of social media. When designing a recommendation system for this purpose, such a difference must be considered with caution.</p><p>3 0.77962077 <a title="255-lda-3" href="./acl-2010-Fine-Grained_Genre_Classification_Using_Structural_Learning_Algorithms.html">117 acl-2010-Fine-Grained Genre Classification Using Structural Learning Algorithms</a></p>
<p>Author: Zhili Wu ; Katja Markert ; Serge Sharoff</p><p>Abstract: Prior use of machine learning in genre classification used a list of labels as classification categories. However, genre classes are often organised into hierarchies, e.g., covering the subgenres of fiction. In this paper we present a method of using the hierarchy of labels to improve the classification accuracy. As a testbed for this approach we use the Brown Corpus as well as a range of other corpora, including the BNC, HGC and Syracuse. The results are not encouraging: apart from the Brown corpus, the improvements of our structural classifier over the flat one are not statistically significant. We discuss the relation between structural learning performance and the visual and distributional balance of the label hierarchy, suggesting that only balanced hierarchies might profit from structural learning.</p><p>4 0.61117703 <a title="255-lda-4" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>Author: Mohit Bansal ; Dan Klein</p><p>Abstract: We present a simple but accurate parser which exploits both large tree fragments and symbol refinement. We parse with all fragments of the training set, in contrast to much recent work on tree selection in data-oriented parsing and treesubstitution grammar learning. We require only simple, deterministic grammar symbol refinement, in contrast to recent work on latent symbol refinement. Moreover, our parser requires no explicit lexicon machinery, instead parsing input sentences as character streams. Despite its simplicity, our parser achieves accuracies of over 88% F1 on the standard English WSJ task, which is competitive with substantially more complicated state-of-theart lexicalized and latent-variable parsers. Additional specific contributions center on making implicit all-fragments parsing efficient, including a coarse-to-fine inference scheme and a new graph encoding.</p><p>5 0.60859102 <a title="255-lda-5" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>Author: David Chiang</p><p>Abstract: Statistical translation models that try to capture the recursive structure of language have been widely adopted over the last few years. These models make use of varying amounts of information from linguistic theory: some use none at all, some use information about the grammar of the target language, some use information about the grammar of the source language. But progress has been slower on translation models that are able to learn the relationship between the grammars of both the source and target language. We discuss the reasons why this has been a challenge, review existing attempts to meet this challenge, and show how some old and new ideas can be combined into a sim- ple approach that uses both source and target syntax for significant improvements in translation accuracy.</p><p>6 0.60160983 <a title="255-lda-6" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>7 0.60039926 <a title="255-lda-7" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>8 0.59921622 <a title="255-lda-8" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>9 0.59895402 <a title="255-lda-9" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>10 0.59875822 <a title="255-lda-10" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>11 0.5975576 <a title="255-lda-11" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>12 0.59587723 <a title="255-lda-12" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>13 0.59516364 <a title="255-lda-13" href="./acl-2010-Fully_Unsupervised_Core-Adjunct_Argument_Classification.html">120 acl-2010-Fully Unsupervised Core-Adjunct Argument Classification</a></p>
<p>14 0.59330386 <a title="255-lda-14" href="./acl-2010-Unsupervised_Ontology_Induction_from_Text.html">248 acl-2010-Unsupervised Ontology Induction from Text</a></p>
<p>15 0.59313035 <a title="255-lda-15" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>16 0.59229422 <a title="255-lda-16" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>17 0.59206033 <a title="255-lda-17" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>18 0.59205115 <a title="255-lda-18" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>19 0.59108293 <a title="255-lda-19" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>20 0.59051323 <a title="255-lda-20" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
