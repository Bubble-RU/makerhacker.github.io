<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 acl-2010-Constituency to Dependency Translation with Forests</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-69" href="#">acl2010-69</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>69 acl-2010-Constituency to Dependency Translation with Forests</h1>
<br/><p>Source: <a title="acl-2010-69-pdf" href="http://aclweb.org/anthology//P/P10/P10-1145.pdf">pdf</a></p><p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>Reference: <a title="acl-2010-69-reference" href="../acl2010_reference/acl-2010-Constituency_to_Dependency_Translation_with_Forests_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. [sent-6, score-1.206]
</p><p>2 However, they have a major limitation that they do not have a  principled mechanism to guarantee grammaticality on the target side, since there is no linguistic tree structure of the output. [sent-29, score-0.257]
</p><p>3 1 Our new constituency-to-dependency model (Section 2) extracts rules from word-aligned pairs of source constituency forests and target dependency trees (Section 3), and translates source constituency forests into target dependency trees with a set of features (Section 4). [sent-52, score-1.875]
</p><p>4 7 BLEU points over a stateof-the-art forest-based tree-to-string system even with less translation rules, this is also the first time that a tree-to-tree model can surpass tree-to-string  counterparts. [sent-54, score-0.235]
</p><p>5 1 Constituency Forests on the Source Side A constituency forest (in Figure 1 left) is a compact representation of all the derivations (i. [sent-61, score-0.646]
</p><p>6 More formally, following Huang (2008), such a constituency forest is a pair Fc = Gf = hVf, Hfi, where Vf is the set of nodes, and Hf hthVe set oif, hyperedges. [sent-64, score-0.646]
</p><p>7 cm, each node vf ∈ Vf is in the form of Xi,j, which denotes the recognition of nonterminal X spanning the substring from positions ithrough j (that is, ci+1 . [sent-68, score-0.424]
</p><p>8 Each hyperedge hf ∈ Hf is a pair htails(hf), head(hf)i, where head∈(hf H) ∈ sV af p iasi trh het consequent node )iin, the deductive step, Vand tails(hf) ∈ (Vf)∗ is the list of antecedent nodes. [sent-72, score-0.423]
</p><p>9 Note that common sub-derivations like those for the verb VPB3,5 are shared, which allows the forest to represent exponentially many parses in a compact structure. [sent-76, score-0.31]
</p><p>10 2 Dependency Trees on the Target Side A dependency tree for a sentence represents each word and its syntactic dependents through directed arcs, as shown in the following examples. [sent-82, score-0.333]
</p><p>11 The main advantage of a dependency tree is that it can explore the long distance dependency. [sent-83, score-0.333]
</p><p>12 1434  1:  2:  atalbk Bushblahbellkdtalkwith a Sharon  We use the lexicon dependency grammar (Hellwig, 2006) to express a projective dependency tree. [sent-84, score-0.45]
</p><p>13 Take the dependency trees above for exam-  ple, they will be expressed: 1: ( a ) talk 2: ( Bush ) held ( ( a ) talk ) ( with ( Sharon ) ) where the lexicons in brackets represent the dependencies, while the lexicon out the brackets is the head. [sent-85, score-0.48]
</p><p>14 More formally, a dependency tree is also a pair De = Gd = hVd, Hdi. [sent-86, score-0.333]
</p><p>15 e Facorh ano gdive vnd t ∈ tV sde nisa word ei (1 6 i 6 n), each hyperedge Vhd ∈ Hd is a directed arc hvdi, vjdi from node vid to its head node vjd. [sent-91, score-0.499]
</p><p>16 Following the formalization of the constituency forest scenario, we denote a pair htails(hd) , head(hd)i to be a hyperedge hd, where hhteaaidls((hhd) is the hea)di node, t hayilpse(rhedd)g ies h the node where hd leaves from. [sent-92, score-0.998]
</p><p>17 3  Hypergraph  Actually, both the constituency forest and the dependency tree can be formalized as a hypergraph G, a pair hV, Hi . [sent-96, score-1.01]
</p><p>18 p aFiorr h simplicity, we a Glso use Fc and De to denote a constituency forest and a dependency tree respectively. [sent-98, score-0.979]
</p><p>19 Specifically, the size of tails(hd) of a hyperedge hd in a dependency tree is a constant one. [sent-99, score-0.572]
</p><p>20 3  Rule Extraction  We extract constituency to dependency rules from word-aligned source constituency forest and target dependency tree pairs (Figure 1). [sent-102, score-1.773]
</p><p>21 In this section, we first formalize the constituency to string translation rule (Section 3. [sent-104, score-0.656]
</p><p>22 Then we present the restrictions for dependency structures as well formed fragments (Section 3. [sent-106, score-0.41]
</p><p>23 }; rhs(r) is expressed ifrno tmhe a target language dependency s)tr iusc etxuprere swsiethd words ej (like “with”) and variables from the set X; and φ(r) is a mapping from X to nontermiXnal;s . [sent-115, score-0.307]
</p><p>24 (2008), we also restrict rhs(r) to be well formed dependency fragment. [sent-120, score-0.292]
</p><p>25 Given a dependency 1435  Figure 1: Forest-based constituency to dependency rule extraction. [sent-122, score-0.931]
</p><p>26 fragment di:j composed by the words from ito j, two kinds of well formed structures are defined as follows: Fixed on one node vodne, fixed for short, if it meets the following conditions: •  •  the head of vodne is out of [i, j] , i. [sent-123, score-0.444]
</p><p>27 =  Floating with multi nodes M, floating for short, if it meets the following conditions:  •  •  all nodes in M have a same head node, ia. [sent-131, score-0.23]
</p><p>28 Take the “ (Bush) held ((a) talk))(with (Sharon)) ” for example: partial fixed examples are “ (Bush) held ” and “ held ((a) talk)”; while the partial floating examples are “ (talk) (with (Sharon)) ” and “ ((a) talk) (with (Sharon)) ”. [sent-141, score-0.254]
</p><p>29 The dependency structure “ held ((a))” is not a well formed structure, since the head of word “a” is out of scope of this structure. [sent-144, score-0.412]
</p><p>30 We extract rules from word-aligned source constituency for-  est and target dependency tree pairs (see Figure 1) in three steps: (1) frontier set computation, (2) fragmentation, (3) composition. [sent-147, score-1.038]
</p><p>31 , 2004) is the potential points to “cut” the forest and dependency tree pair into fragments, each of which will form a minimal rule (Galley et al. [sent-149, score-0.86]
</p><p>32 However, not every fragment can be used for rule extraction, since it may or may not respect to the restrictions, such as word alignments and well formed dependency structures. [sent-151, score-0.573]
</p><p>33 The root node of every extractable tree fragment corresponds to a faithful structure on the target side, in which case there is a “translational equivalence” between the subtree rooted at the node and the corresponding target structure. [sent-153, score-0.714]
</p><p>34 For example, in Figure 1, every node in the forest is annotated with its corresponding English struc-  ture. [sent-154, score-0.423]
</p><p>35 1436 Algorithm 1 Forest-based constituency to dependency rule extraction. [sent-156, score-0.706]
</p><p>36 Input: Source constituency forest Fc, target dependency tree De, and alignment a  1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13:  Output: Minimal rule set R fs ← FRONTIER(Fc, De, a) ffosr ← ←eac Fh vf ∈ fs do open ← {h∅, {ovf}i } owpheinle open {∅v d}oi hhs, expsi ← open. [sent-157, score-1.619]
</p><p>37 pop() hifh exps = ∅ ← th oepne generate a rule r using fragment hs R. [sent-158, score-0.395]
</p><p>38 append(hhs ∪ ∪ { (thafi}ls, newexpsi)  ⊲  compute frontier set initial queue of growing fragments  =  ⊲  fragment ⊲ nothing to expand? [sent-162, score-0.381]
</p><p>39 ⊲ generate a rule ⊲ extract a  ⊲ incomplete: further expand ⊲ a non-frontier node ⊲ expand open. [sent-163, score-0.258]
</p><p>40 For each span(vf), we also denote dep(vf) to be its corresponding dependency structure, which represents the dependency structure of all the words in span(vf). [sent-166, score-0.45]
</p><p>41 (  For example, node VV3,4 has a non-faithful structure (crossed out in Figure 1), since its dep(VV3,4 = held ((a) *)” is not a well formed structure, where the head of word “a” lies in the outside of its words covered. [sent-173, score-0.3]
</p><p>42 Nodes with faithful structure form the frontier set (shaded nodes in Figure 1) which serve as potential cut points for rule extraction. [sent-174, score-0.422]
</p><p>43 Given the frontier set, fragmentation step is to “cut” the forest at all frontier nodes and form tree fragments, each of which forms a rule with variables matching the frontier descendant nodes. [sent-175, score-1.061]
</p><p>44 For example, the forest in Figure 1 is cut into 10 pieces, each of which corresponds to a minimal rule listed on the right. [sent-176, score-0.532]
</p><p>45 Our rule extraction algorithm is formalized in  “  Algorithm 1. [sent-177, score-0.176]
</p><p>46 We visit each frontier node vf ∈ fs on the source constituency forest Fc, and keep a queue open ofgrowing fragments rooted at vf. [sent-179, score-1.426]
</p><p>47 We keep expanding incomplete fragments from open, and extract a rule if a complete fragment is found (line 7). [sent-180, score-0.353]
</p><p>48 Each fragment hs in open is associated with a list of expansion sites (exps in line 5) being the subset of leaf nodes of the current fragment that are not in the frontier set. [sent-181, score-0.558]
</p><p>49 So each fragment along hyperedge h is associated with exps =  tails(hf)  \ fs. [sent-182, score-0.354]
</p><p>50 A fragment is complete if its expansion sites is empty (line 6), otherwise we pop one expansion node v′ to grow and spin-off new fragments by following hyperedges of v′, adding new expansion sites (lines 11-13), until all active fragments are complete and open queue is empty (line 4). [sent-183, score-0.471]
</p><p>51 For example, the composed rule r1  in Figure 2 is glued by the following two minimal rules: 1437  IP (NP(x1:NPB x→2:C (xC1) x3 x4:N(PxB2)(x x34):V )PB)  r2  CC (yˇ u)  r3  →  with  where x2:CC in r2 is replaced with 3. [sent-186, score-0.178]
</p><p>52 αβ(lhs(r)) =α(root(r))  ·  Y  P(hf)  hf ∈Y Ylhs(r)  ·  Y  (2)  β(vf)  vf ∈ leaYves(lhs(r)) where root(r) is the root of the rule r, α(v) and β(v) are the outside and inside probabilities of node v, and leaves(lhs(r)) returns the leaf nodes of a tree fragment lhs(r). [sent-189, score-1.18]
</p><p>53 The decoding algorithm works in a bottom-up search fashion by traversing each node in forest Fc. [sent-197, score-0.481]
</p><p>54 (2008) to convert Fc into a translation forest, each hyperedge of which is associated with a constituency to dependency translation rule. [sent-199, score-0.97]
</p><p>55 However, pattern-matching failure2 at a node vf will  terdagne2sPlacat intoebrn er-umcloeanct stahrniuncbget mfdaia lutcvrehfead. [sent-200, score-0.424]
</p><p>56 ta t vnfodoervnfotmraenasnlsat ihoenrehyipsenro1438  cut the derivation path and lead to translation failure. [sent-201, score-0.229]
</p><p>57 To tackle this problem, we construct a pseudo translation rule for each parse hyperedge hf ∈ IN(vf) by mapping the CFG rule into a target de∈pendency tree using the head rules of Magerman (1995). [sent-202, score-1.14]
</p><p>58 Take the hyperedge h0f in Figure1 for example, the corresponding pseudo translation rule is:  NP(x1:NPB x2:CC x3:NPB) → (x1) (x2) x3, since the x3:NPB is the head word of the CFG rule: NP → NPB CC NPB. [sent-203, score-0.508]
</p><p>59 est is constructed, we traverse each node in translation forest also in bottom-up fashion. [sent-205, score-0.555]
</p><p>60 For each node, we use the cube pruning technique (Chiang, 2007; Huang and Chiang, 2007) to produce partial hypotheses and compute all the feature scores including the dependency language model score (Section 4. [sent-206, score-0.225]
</p><p>61 If all the nodes are visited, we trace back along the 1-best derivation at goal item S0,m and build a target side dependency tree. [sent-208, score-0.509]
</p><p>62 1 Dependency Language Model Computing We compute the score of a dependency language model for a dependency tree De in the same way proposed by Shen et al. [sent-211, score-0.558]
</p><p>63 For each nonter-  minal node vhd = eh in De and its children sequences Ll = el1, el2. [sent-213, score-0.234]
</p><p>64 We use the suffix “§” to distinguish the head wWoerd u asned t chheil dsu wffioxrds “ §in” t thoe dependency language model. [sent-223, score-0.28]
</p><p>65 In order to alleviate the problem of data sparse, we also compute a dependency language model for POS tages over a dependency tree. [sent-224, score-0.45]
</p><p>66 So we will also generate a POS taged dependency tree simulta-  neously at the decoding time. [sent-226, score-0.391]
</p><p>67 We calculate this dependency language model by simply replacing each ei in equation 9 with its tag t(ei). [sent-227, score-0.298]
</p><p>68 We also parse the English sentences using the parser of Charniak (2000) into 1-best constituency trees, which will be converted into dependency trees using Magerman (1995)’s head rules. [sent-235, score-0.713]
</p><p>69 We also store the POS tag information for each word in dependency trees, and compute two different dependency language models for words and POS tags in dependency tree separately. [sent-236, score-0.783]
</p><p>70 Finally, we apply translation rule extraction  algorithm described in Section 3. [sent-237, score-0.277]
</p><p>71 At the decoding step, we again parse the input sentences into forests and prune them with a threshold 10, which will direct the translation (Section 4). [sent-239, score-0.364]
</p><p>72 , 2008), or  forest c2s for short, which translates a source forest into a target string by pattern-matching the 1439  constituency-to-string (c2s) rules and the bilingual phrases (s2s). [sent-248, score-0.896]
</p><p>73 We first restrict the target side of translation rules to be well-formed structures, and we extract 13. [sent-254, score-0.396]
</p><p>74 Then we convert c2d and s2d rules to c2s and s2s rules separately by removing the target-dependency structures and feed them into the baseline system. [sent-259, score-0.182]
</p><p>75 7 BLEU points over baseline system due to the poorer rule coverage. [sent-261, score-0.184]
</p><p>76 However, when we further use all s2s rules instead of s2d  rules in our next experiment, it achieves a BLEU score of 34. [sent-262, score-0.182]
</p><p>77 Those results suggest that restrictions on c2s rules won’t hurt the performance, but restrictions on s2s will hurt the translation quality badly. [sent-264, score-0.315]
</p><p>78 So we should utilize all the s2s rules in order to preserve a good coverage of translation rule set. [sent-265, score-0.368]
</p><p>79 This suggests that using dependency language model really improves the translation quality by less than 1BLEU point. [sent-270, score-0.357]
</p><p>80 In order to utilize all the s2s rules and increase the rule coverage, we parse the target strings of the s2s rules into dependency fragments, and construct the pseudo s2d rules (s2s-dep). [sent-271, score-0.797]
</p><p>81 With the help of the dependency language model, our new model achieves a significant improvement of +0. [sent-273, score-0.225]
</p><p>82 7 BLEU points over the forest c2s baseline system (p  < 0. [sent-274, score-0.349]
</p><p>83 6  Related Work  The concept of packed forest has been used in machine translation for several years. [sent-291, score-0.477]
</p><p>84 For example, Huang and Chiang (2007) use forest to characterize the search space of decoding with integrated language models. [sent-292, score-0.368]
</p><p>85 (2008) and Mi and Huang (2008) use forest to direct translation and extract rules rather than 1-best tree in order to weaken the influence of parsing errors, this is also the first time to use forest directly in machine translation. [sent-294, score-0.951]
</p><p>86 (2009), we apply forest into a new constituency tree to dependency tree translation model rather than constituency tree-to-tree model. [sent-301, score-1.555]
</p><p>87 They define the well-formed dependency structures to reduce the size of translation rule set, and integrate a dependency language model in decoding step to exploit long distance word relations. [sent-304, score-0.785]
</p><p>88 (2009) propose a forest-based constituency-to-constituency model, they put more emphasize on how to utilize parse forest to increase the tree-to-tree rule coverage. [sent-309, score-0.496]
</p><p>89 By contrast, we only use 1-best dependency trees on the target side to explore long distance relations and extract translation rules. [sent-310, score-0.586]
</p><p>90 Theoretically, we can extract more rules since dependency tree has the best inter-lingual phrasal cohesion properties (Fox, 2002). [sent-311, score-0.424]
</p><p>91 7  Conclusion and Future Work  In this paper, we presented a novel forest-based constituency-to-dependency translation model, which combines the advantages of both tree-tostring and string-to-tree systems, runs fast and guarantees grammaticality of the output. [sent-312, score-0.199]
</p><p>92 To learn the constituency-to-dependency translation rules, we first identify the frontier set for all the  nodes in the constituency forest on the source side. [sent-313, score-1.032]
</p><p>93 At the decoding step, we first parse the input sentence into a constituency forest. [sent-316, score-0.435]
</p><p>94 Then we convert it into a translation forest by patter-matching the constituency to string rules. [sent-317, score-0.821]
</p><p>95 Finally, we traverse the translation forest in a bottom-up fashion and translate it into a target dependency tree by incorporating string-based and dependency-based language models. [sent-318, score-0.857]
</p><p>96 Using all constituency-to-dependency translation rules and bilingual phrases, our model achieves +0. [sent-319, score-0.223]
</p><p>97 Furthermore, we will replace 1-best dependency trees on the target side with dependency forests to further increase the rule coverage. [sent-323, score-0.957]
</p><p>98 Spmt: Statistical machine translation with syntactified target language phrases. [sent-419, score-0.214]
</p><p>99 A new string-to-dependency machine translation algorithm with a target dependency language model. [sent-449, score-0.439]
</p><p>100 A  dependency treelet string correspondence model for statistical machine translation. [sent-461, score-0.297]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('constituency', 0.336), ('vf', 0.311), ('forest', 0.31), ('dependency', 0.225), ('hf', 0.165), ('rhs', 0.165), ('lhs', 0.152), ('rule', 0.145), ('huang', 0.145), ('hyperedge', 0.145), ('npb', 0.145), ('fc', 0.14), ('frontier', 0.136), ('fragment', 0.136), ('forests', 0.133), ('translation', 0.132), ('node', 0.113), ('bleu', 0.11), ('tree', 0.108), ('liu', 0.099), ('hd', 0.094), ('mi', 0.093), ('side', 0.091), ('rules', 0.091), ('sharon', 0.087), ('eh', 0.084), ('target', 0.082), ('cc', 0.081), ('tails', 0.08), ('exps', 0.073), ('vodne', 0.073), ('ei', 0.073), ('fragments', 0.072), ('dep', 0.069), ('grammaticality', 0.067), ('talk', 0.067), ('formed', 0.067), ('held', 0.065), ('qun', 0.065), ('surpass', 0.064), ('ll', 0.063), ('tc', 0.061), ('source', 0.06), ('xiong', 0.06), ('bush', 0.06), ('floating', 0.059), ('decoding', 0.058), ('nodes', 0.058), ('lr', 0.058), ('zhang', 0.056), ('liang', 0.056), ('cj', 0.056), ('trees', 0.056), ('cxc', 0.055), ('hhs', 0.055), ('head', 0.055), ('shen', 0.054), ('derivation', 0.053), ('galley', 0.052), ('leaf', 0.051), ('fs', 0.051), ('root', 0.051), ('span', 0.05), ('koehn', 0.049), ('forestbased', 0.048), ('fractional', 0.046), ('restrictions', 0.046), ('chiang', 0.044), ('cut', 0.044), ('string', 0.043), ('probabilities', 0.042), ('shouxun', 0.042), ('hyperedges', 0.041), ('parse', 0.041), ('hs', 0.041), ('glue', 0.039), ('points', 0.039), ('haitao', 0.037), ('queue', 0.037), ('kevin', 0.037), ('daensd', 0.037), ('eln', 0.037), ('htails', 0.037), ('stringbased', 0.037), ('vhd', 0.037), ('vkd', 0.037), ('vpb', 0.037), ('de', 0.035), ('vd', 0.035), ('packed', 0.035), ('minimal', 0.033), ('synchronous', 0.032), ('fragmentation', 0.032), ('formalized', 0.031), ('moses', 0.031), ('pseudo', 0.031), ('kf', 0.029), ('treelet', 0.029), ('billot', 0.029), ('extractable', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="69-tfidf-1" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>2 0.28456387 <a title="69-tfidf-2" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>3 0.28440502 <a title="69-tfidf-3" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>Author: Yang Liu ; Liang Huang</p><p>Abstract: unkown-abstract</p><p>4 0.27824113 <a title="69-tfidf-4" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>Author: Min Zhang ; Hui Zhang ; Haizhou Li</p><p>Abstract: This paper proposes a convolution forest kernel to effectively explore rich structured features embedded in a packed parse forest. As opposed to the convolution tree kernel, the proposed forest kernel does not have to commit to a single best parse tree, is thus able to explore very large object spaces and much more structured features embedded in a forest. This makes the proposed kernel more robust against parsing errors and data sparseness issues than the convolution tree kernel. The paper presents the formal definition of convolution forest kernel and also illustrates the computing algorithm to fast compute the proposed convolution forest kernel. Experimental results on two NLP applications, relation extraction and semantic role labeling, show that the proposed forest kernel significantly outperforms the baseline of the convolution tree kernel. 1</p><p>5 0.23914061 <a title="69-tfidf-5" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>Author: Zhiyang Wang ; Yajuan Lv ; Qun Liu ; Young-Sook Hwang</p><p>Abstract: This paper presents a novel filtration criterion to restrict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed wellformed dependency restriction is used to filter out bad rules. Furthermore, a new feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed. Experimental results show that, the new criteria weeds out about 40% rules while with translation performance improvement, and the new feature brings an- other improvement to the baseline system, especially on larger corpus.</p><p>6 0.22009732 <a title="69-tfidf-6" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>7 0.21415423 <a title="69-tfidf-7" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>8 0.19191895 <a title="69-tfidf-8" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>9 0.19152574 <a title="69-tfidf-9" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>10 0.1891803 <a title="69-tfidf-10" href="./acl-2010-Hierarchical_Search_for_Word_Alignment.html">133 acl-2010-Hierarchical Search for Word Alignment</a></p>
<p>11 0.15898733 <a title="69-tfidf-11" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>12 0.15320382 <a title="69-tfidf-12" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>13 0.1509247 <a title="69-tfidf-13" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>14 0.13944374 <a title="69-tfidf-14" href="./acl-2010-A_Transition-Based_Parser_for_2-Planar_Dependency_Structures.html">20 acl-2010-A Transition-Based Parser for 2-Planar Dependency Structures</a></p>
<p>15 0.13108009 <a title="69-tfidf-15" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>16 0.1304376 <a title="69-tfidf-16" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>17 0.12955655 <a title="69-tfidf-17" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>18 0.12311491 <a title="69-tfidf-18" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>19 0.12209618 <a title="69-tfidf-19" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<p>20 0.1113181 <a title="69-tfidf-20" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.285), (1, -0.31), (2, 0.069), (3, 0.038), (4, -0.151), (5, -0.05), (6, 0.159), (7, 0.024), (8, -0.262), (9, 0.046), (10, 0.042), (11, -0.091), (12, 0.105), (13, -0.002), (14, 0.141), (15, 0.043), (16, -0.022), (17, 0.016), (18, -0.07), (19, 0.024), (20, 0.087), (21, 0.011), (22, -0.051), (23, -0.027), (24, -0.051), (25, 0.133), (26, 0.004), (27, -0.038), (28, 0.035), (29, -0.026), (30, -0.037), (31, -0.089), (32, 0.037), (33, 0.002), (34, 0.01), (35, -0.183), (36, 0.009), (37, 0.068), (38, -0.005), (39, 0.02), (40, 0.012), (41, 0.001), (42, -0.08), (43, -0.124), (44, 0.019), (45, 0.027), (46, 0.014), (47, -0.033), (48, 0.074), (49, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9554711 <a title="69-lsi-1" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>2 0.87172365 <a title="69-lsi-2" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>3 0.8375119 <a title="69-lsi-3" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>Author: Yang Liu ; Liang Huang</p><p>Abstract: unkown-abstract</p><p>4 0.6951791 <a title="69-lsi-4" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>Author: Chris Dyer ; Adam Lopez ; Juri Ganitkevitch ; Jonathan Weese ; Ferhan Ture ; Phil Blunsom ; Hendra Setiawan ; Vladimir Eidelman ; Philip Resnik</p><p>Abstract: Adam Lopez University of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phraseWe present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly separates model-specific translation logic from general rescoring, pruning, and inference algorithms. From this unified representation, the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders.</p><p>5 0.69353139 <a title="69-lsi-5" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>Author: Zhiyang Wang ; Yajuan Lv ; Qun Liu ; Young-Sook Hwang</p><p>Abstract: This paper presents a novel filtration criterion to restrict the rule extraction for the hierarchical phrase-based translation model, where a bilingual but relaxed wellformed dependency restriction is used to filter out bad rules. Furthermore, a new feature which describes the regularity that the source/target dependency edge triggers the target/source word is also proposed. Experimental results show that, the new criteria weeds out about 40% rules while with translation performance improvement, and the new feature brings an- other improvement to the baseline system, especially on larger corpus.</p><p>6 0.68710709 <a title="69-lsi-6" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>7 0.66842514 <a title="69-lsi-7" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<p>8 0.66453797 <a title="69-lsi-8" href="./acl-2010-Filtering_Syntactic_Constraints_for_Statistical_Machine_Translation.html">115 acl-2010-Filtering Syntactic Constraints for Statistical Machine Translation</a></p>
<p>9 0.60862815 <a title="69-lsi-9" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>10 0.54454428 <a title="69-lsi-10" href="./acl-2010-Bitext_Dependency_Parsing_with_Bilingual_Subtree_Constraints.html">52 acl-2010-Bitext Dependency Parsing with Bilingual Subtree Constraints</a></p>
<p>11 0.538454 <a title="69-lsi-11" href="./acl-2010-Detecting_Errors_in_Automatically-Parsed_Dependency_Relations.html">84 acl-2010-Detecting Errors in Automatically-Parsed Dependency Relations</a></p>
<p>12 0.51999098 <a title="69-lsi-12" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>13 0.5102213 <a title="69-lsi-13" href="./acl-2010-Boosting-Based_System_Combination_for_Machine_Translation.html">54 acl-2010-Boosting-Based System Combination for Machine Translation</a></p>
<p>14 0.48582923 <a title="69-lsi-14" href="./acl-2010-Dynamic_Programming_for_Linear-Time_Incremental_Parsing.html">93 acl-2010-Dynamic Programming for Linear-Time Incremental Parsing</a></p>
<p>15 0.45596045 <a title="69-lsi-15" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>16 0.44742247 <a title="69-lsi-16" href="./acl-2010-Dependency_Parsing_and_Projection_Based_on_Word-Pair_Classification.html">83 acl-2010-Dependency Parsing and Projection Based on Word-Pair Classification</a></p>
<p>17 0.44091395 <a title="69-lsi-17" href="./acl-2010-Exploring_Syntactic_Structural_Features_for_Sub-Tree_Alignment_Using_Bilingual_Tree_Kernels.html">110 acl-2010-Exploring Syntactic Structural Features for Sub-Tree Alignment Using Bilingual Tree Kernels</a></p>
<p>18 0.434432 <a title="69-lsi-18" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>19 0.42954037 <a title="69-lsi-19" href="./acl-2010-Efficient_Third-Order_Dependency_Parsers.html">99 acl-2010-Efficient Third-Order Dependency Parsers</a></p>
<p>20 0.42015719 <a title="69-lsi-20" href="./acl-2010-Bilingual_Sense_Similarity_for_Statistical_Machine_Translation.html">51 acl-2010-Bilingual Sense Similarity for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(14, 0.013), (16, 0.021), (20, 0.011), (25, 0.475), (42, 0.01), (44, 0.022), (59, 0.087), (73, 0.032), (78, 0.04), (83, 0.058), (84, 0.012), (95, 0.013), (98, 0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98165083 <a title="69-lda-1" href="./acl-2010-Talking_NPCs_in_a_Virtual_Game_World.html">224 acl-2010-Talking NPCs in a Virtual Game World</a></p>
<p>Author: Tina Kluwer ; Peter Adolphs ; Feiyu Xu ; Hans Uszkoreit ; Xiwen Cheng</p><p>Abstract: This paper describes the KomParse system, a natural-language dialog system in the three-dimensional virtual world Twinity. In order to fulfill the various communication demands between nonplayer characters (NPCs) and users in such an online virtual world, the system realizes a flexible and hybrid approach combining knowledge-intensive domainspecific question answering, task-specific and domain-specific dialog with robust chatbot-like chitchat.</p><p>2 0.96925461 <a title="69-lda-2" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>Author: Matthew Honnibal ; James R. Curran ; Johan Bos</p><p>Abstract: Once released, treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of specific phenomena. Instead, separate resources are created to address such problems. In this paper we show how to improve the quality of a treebank, by integrating resources and implementing improved analyses for specific constructions. We demonstrate this rebanking process by creating an updated version of CCGbank that includes the predicate-argument structure of both verbs and nouns, baseNP brackets, verb-particle constructions, and restrictive and non-restrictive nominal modifiers; and evaluate the impact of these changes on a statistical parser.</p><p>3 0.95724368 <a title="69-lda-3" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>Author: Aria Haghighi ; Dan Klein</p><p>Abstract: We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.</p><p>same-paper 4 0.90837836 <a title="69-lda-4" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>5 0.84400123 <a title="69-lda-5" href="./acl-2010-Distributional_Similarity_vs._PU_Learning_for_Entity_Set_Expansion.html">89 acl-2010-Distributional Similarity vs. PU Learning for Entity Set Expansion</a></p>
<p>Author: Xiao-Li Li ; Lei Zhang ; Bing Liu ; See-Kiong Ng</p><p>Abstract: Distributional similarity is a classic technique for entity set expansion, where the system is given a set of seed entities of a particular class, and is asked to expand the set using a corpus to obtain more entities of the same class as represented by the seeds. This paper shows that a machine learning model called positive and unlabeled learning (PU learning) can model the set expansion problem better. Based on the test results of 10 corpora, we show that a PU learning technique outperformed distributional similarity significantly. 1</p><p>6 0.8436321 <a title="69-lda-6" href="./acl-2010-Topic_Models_for_Word_Sense_Disambiguation_and_Token-Based_Idiom_Detection.html">237 acl-2010-Topic Models for Word Sense Disambiguation and Token-Based Idiom Detection</a></p>
<p>7 0.83376026 <a title="69-lda-7" href="./acl-2010-Accurate_Context-Free_Parsing_with_Combinatory_Categorial_Grammar.html">23 acl-2010-Accurate Context-Free Parsing with Combinatory Categorial Grammar</a></p>
<p>8 0.67505789 <a title="69-lda-8" href="./acl-2010-Blocked_Inference_in_Bayesian_Tree_Substitution_Grammars.html">53 acl-2010-Blocked Inference in Bayesian Tree Substitution Grammars</a></p>
<p>9 0.67152137 <a title="69-lda-9" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>10 0.66749674 <a title="69-lda-10" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>11 0.65957624 <a title="69-lda-11" href="./acl-2010-WSD_as_a_Distributed_Constraint_Optimization_Problem.html">257 acl-2010-WSD as a Distributed Constraint Optimization Problem</a></p>
<p>12 0.65575087 <a title="69-lda-12" href="./acl-2010-Grammar_Prototyping_and_Testing_with_the_LinGO_Grammar_Matrix_Customization_System.html">128 acl-2010-Grammar Prototyping and Testing with the LinGO Grammar Matrix Customization System</a></p>
<p>13 0.65515399 <a title="69-lda-13" href="./acl-2010-Correcting_Errors_in_a_Treebank_Based_on_Synchronous_Tree_Substitution_Grammar.html">75 acl-2010-Correcting Errors in a Treebank Based on Synchronous Tree Substitution Grammar</a></p>
<p>14 0.64350653 <a title="69-lda-14" href="./acl-2010-Simple%2C_Accurate_Parsing_with_an_All-Fragments_Grammar.html">211 acl-2010-Simple, Accurate Parsing with an All-Fragments Grammar</a></p>
<p>15 0.64038157 <a title="69-lda-15" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>16 0.63968217 <a title="69-lda-16" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>17 0.63929361 <a title="69-lda-17" href="./acl-2010-Bayesian_Synchronous_Tree-Substitution_Grammar_Induction_and_Its_Application_to_Sentence_Compression.html">46 acl-2010-Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression</a></p>
<p>18 0.62817842 <a title="69-lda-18" href="./acl-2010-PCFGs%2C_Topic_Models%2C_Adaptor_Grammars_and_Learning_Topical_Collocations_and_the_Structure_of_Proper_Names.html">191 acl-2010-PCFGs, Topic Models, Adaptor Grammars and Learning Topical Collocations and the Structure of Proper Names</a></p>
<p>19 0.61625779 <a title="69-lda-19" href="./acl-2010-Generating_Entailment_Rules_from_FrameNet.html">121 acl-2010-Generating Entailment Rules from FrameNet</a></p>
<p>20 0.61292922 <a title="69-lda-20" href="./acl-2010-A_Joint_Rule_Selection_Model_for_Hierarchical_Phrase-Based_Translation.html">9 acl-2010-A Joint Rule Selection Model for Hierarchical Phrase-Based Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
