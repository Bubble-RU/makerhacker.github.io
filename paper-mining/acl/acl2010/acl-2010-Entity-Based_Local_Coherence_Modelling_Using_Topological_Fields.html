<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-101" href="#">acl2010-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</h1>
<br/><p>Source: <a title="acl-2010-101-pdf" href="http://aclweb.org/anthology//P/P10/P10-1020.pdf">pdf</a></p><p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly.</p><p>Reference: <a title="acl-2010-101-reference" href="../acl2010_reference/acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Entity-based local coherence modelling using topological fields Jackie Chi Kit Cheung and Gerald Penn Department of Computer Science University of Toronto Toronto, ON, M5S 3G4, Canada { j cheung ,gpenn} @ cs . [sent-1, score-1.208]
</p><p>2 In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. [sent-4, score-1.112]
</p><p>3 First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. [sent-5, score-1.532]
</p><p>4 Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent  orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly. [sent-6, score-1.206]
</p><p>5 1 Introduction One type of coherence modelling that has captured recent research interest is local coherence modelling, which measures the coherence of a document by examining the similarity between neighbouring text spans. [sent-7, score-1.022]
</p><p>6 The entity-based approach, in particular, considers the occurrences of noun phrase entities in a document (Barzilay and Lapata, 2008). [sent-8, score-0.238]
</p><p>7 Local coherence modelling has been shown to be useful for tasks like natural language generation and summarization, (Barzilay and Lee, 2004) and genre classification (Barzilay and Lapata, 2008). [sent-9, score-0.36]
</p><p>8 Previous work on English, a language with relatively fixed word order, has identified factors that contribute to local coherence, such as the grammatical roles associated with the entities. [sent-10, score-0.27]
</p><p>9 For instance, freerword-order languages exhibit word order patterns which are dependent on discourse factors relating to information structure, in addition to the grammatical roles of nominal arguments of the main verb. [sent-12, score-0.221]
</p><p>10 We thus expect word order information to be particularly important in these languages in discourse analysis, which includes coherence modelling. [sent-13, score-0.311]
</p><p>11 We instead use topological fields, a model of clausal structure which is indicative ofinformation structure in German, but shallow enough to be automatically parsed at high accuracy. [sent-19, score-0.752]
</p><p>12 We test the hypothesis that they would provide a good complement or alternative to grammatical roles in local coherence modelling. [sent-20, score-0.542]
</p><p>13 We show that they are superior to grammatical roles in a sentence ordering experiment, and in fact outperforms simple word-order information as well. [sent-21, score-0.307]
</p><p>14 We further show that these differences are particularly large when manual syntactic and grammatical role an186  Proce dinUgsp osfa tlhae, 4S8wthed Aen n,u 1a1l-1 M6e Jeutilnyg 2 o0f1 t0h. [sent-22, score-0.226]
</p><p>15 ” Figure 1: The clausal and topological field structure of a German sentence. [sent-26, score-0.866]
</p><p>16 We then embed these topological field annotations into a natural language generation system to show the utility of local coherence information in an applied setting. [sent-29, score-1.18]
</p><p>17 We add contextual features using topological field transitions to the model of Filippova and Strube (2007b) and achieve a slight improvement over their model in a constituent ordering task, though not statistically significantly. [sent-30, score-1.054]
</p><p>18 We conclude by discussing possible reasons for the utility of topological fields in local coherence modelling. [sent-31, score-1.108]
</p><p>19 1 German Topological Field Parsing Topological fields are sequences of one or more contiguous phrases found in an enclosing syntactic region, which is the clause in the case of the German topological field model (H¨ ohle, 1983). [sent-33, score-1.008]
</p><p>20 Topological fields are a useful abstraction of word order, because while Germanic word order is relatively free with respect to grammatical functions, the order of the topological fields is strict and unvarying. [sent-36, score-1.033]
</p><p>21 The other topological fields are defined in relation to these two brackets, and contain all other parts of the clause such as verbal arguments, adjuncts, and discourse cues. [sent-39, score-0.892]
</p><p>22 The NF (Nachfeld or “post-field”) contains prosodically heavy elements such as postposed prepositional phrases or relative clauses, and occasionally postposed noun phrases. [sent-44, score-0.319]
</p><p>23 2 The Role of the Vorfeld One of the reasons that we use topological fields for local coherence modelling is the role that the VF plays in signalling the information structure of German clauses, as it often contains the topic of the sentence. [sent-46, score-1.261]
</p><p>24 h1 oth Table 1: a) An example of a document from T ¨uBa-D/Z, b) an abbreviated entity grid representation of it, and c) the feature vector representation of the abbreviated entity grid for transitions of length two. [sent-78, score-0.666]
</p><p>25 nom:  and other arguments Filippova and Strube (2007c) also examine the role of the VF in local coherence and natural language generation, focusing on the correlation between VFs and sentential topics. [sent-80, score-0.444]
</p><p>26 3  Using Entity Grids to Model Local Coherence Barzilay and Lapata (2008) introduce the entity grid as a method ofrepresenting the coherence of a document. [sent-84, score-0.485]
</p><p>27 Entity grids indicate the location of the occurrences of an entity in a document, which is nominative, acc: accusative, oth: dative, oblique,  important for coherence modelling because mentions of an entity tend to appear in clusters of neighbouring or nearby sentences in coherent documents. [sent-85, score-0.675]
</p><p>28 In Barzilay and Lapata (2008), an entity grid is constructed for each document, and is represented as a matrix in which each row represents a sentence, and each column represents an entity. [sent-87, score-0.213]
</p><p>29 The cell is marked by the presence or absence of the entity, and can also be augmented with other information about the entity in this sentence, such as the grammatical role of the noun phrase representing that entity in that sentence, or the topological field in which the noun phrase appears. [sent-89, score-1.501]
</p><p>30 An entity grid representation which incorporates the syntactic role of the noun phrase in which the entity ap188  pears is also shown (not all entities are listed for  brevity). [sent-91, score-0.643]
</p><p>31 We tabulate the transitions of entities between different syntactic positions (or their nonoccurrence) in sentences, and convert the frequencies of transitions into a feature vector representation of transition probabilities in the document. [sent-92, score-0.273]
</p><p>32 This model of local coherence was investigated for German by Filippova and Strube (2007a). [sent-94, score-0.36]
</p><p>33 In contrast, our work focuses on improving performance by annotating entities with additional linguistic information, such as topological fields, and is geared towards natural language generation systems where perfect information is available. [sent-98, score-0.74]
</p><p>34 Similar models of local coherence include various Centering Theory accounts of local coherence  ((Kibble and Power, 2004; Poesio et al. [sent-99, score-0.72]
</p><p>35 1 Method We test a version of the entity grid representation augmented with topological fields in a sentence ordering experiment corresponding to Experiment 1 of Barzilay and Lapata (2008). [sent-104, score-1.15]
</p><p>36 , 2004), which contains manual coreference, grammatical role and topological field information. [sent-109, score-0.944]
</p><p>37 Representation when marking the presence of an entity in a sentence, what information about the entity is marked (topological field, grammatical role, or none). [sent-118, score-0.46]
</p><p>38 2 Entity Representations The main goal of this study is to compare word order, grammatical role and topological field information, which is encoded into the entity grid at each occurrence of an entity. [sent-128, score-1.129]
</p><p>39 Here, we describe the variants of the entity representations that we compare. [sent-129, score-0.207]
</p><p>40 189  Baseline Representations We implement several baseline representations against which we test our topological field-enhanced model. [sent-130, score-0.667]
</p><p>41 The simplest baseline representation marks the mere appearance of an entity without any additional information, which we refer to as de fault . [sent-131, score-0.235]
</p><p>42 The two versions of clausal order we tried are order 1/ 2 / 3+, which marks a noun phrase as the first, the sec-  ond, or the third or later to appear in a clause, and order 1/ 2 +, which marks a noun phrase as the first, or the second or later to appear in a clause. [sent-135, score-0.553]
</p><p>43 Since noun phrases can be embedded in other noun phrases, overlaps can occur. [sent-136, score-0.233]
</p><p>44 In this case, the dominating noun phrase takes the smallest order number among its dominated noun phrases. [sent-137, score-0.229]
</p><p>45 The third class of baseline representations we employ mark an entity by its grammatical role in the clause. [sent-138, score-0.405]
</p><p>46 Because German distinguishes more grammatical roles morphologically than English, we experiment with various granularities of role labelling. [sent-140, score-0.314]
</p><p>47 case s distinguishes five types of entities corresponding to the four morphological cases of German in addition to another category for noun phrases which are not complements of the main verb. [sent-142, score-0.272]
</p><p>48 Topological Field-Based  These representations  mark the topological field in which an entity appears. [sent-143, score-0.925]
</p><p>49 vf marks the noun phrase as belonging to a VF (and not in a PP) or not. [sent-146, score-0.433]
</p><p>50 t opf/pp distinguishes entities in the topological fields VF, MF, and NF, contains a separate category for PP, and a category for all other noun phrases. [sent-148, score-0.951]
</p><p>51 Combined We tried a representation which combines grammatical role and topological field into a single representation, sub j / ob j ×vf, winthoich a ata skinegs ltehe eCparretesseniatna product obfj s/uobb j /×ovbf j and vf above. [sent-151, score-1.231]
</p><p>52 Thus, we devised additional entity representations to account for these aspects of German. [sent-155, score-0.207]
</p><p>53 A noun phrase is marked as TOPIC if it is in VF as in vfpp, or if it is the first noun phrase in MF and also the first NP in the clause. [sent-157, score-0.297]
</p><p>54 While this representation may appear to be very similar to simply distinguishing the first entity in a clause as for order 1/ 2 + in that TOPIC would correspond to the first entity in the clause, they are in fact distinct. [sent-160, score-0.421]
</p><p>55 Due to issues related to coordination, appositive constructions, and fragments which do not receive a topology of fields, the first entity in a clause is labelled the TOPIC only 80. [sent-161, score-0.249]
</p><p>56 The following set of decisions represents how a noun phrase is marked: If the first NP in the clause is a pronoun in an MF field and is the subject, we mark it as TOPIC. [sent-165, score-0.352]
</p><p>57 Thus, we test the robustness of the entity repre190  tection experiment with various entity representations using manual and automatic annotations of topological fields and grammatical roles. [sent-170, score-1.37]
</p><p>58 We employ the following two systems for extracting topological fields and grammatical roles. [sent-178, score-0.889]
</p><p>59 To parse topological fields, we use the Berkeley parser of Petrov and Klein (2007), which has been shown to perform well at this task (Cheung and Penn, 2009). [sent-179, score-0.604]
</p><p>60 35% F1 on topological fields and clausal nodes without gold POS tags on the section of T ¨uBa-D/Z it was tested on. [sent-181, score-0.896]
</p><p>61 First, we tried extracting grammatical roles from the parse trees which we obtained from the Berkeley parser, as this information is present in the edge labels that can be recovered from the parse. [sent-183, score-0.207]
</p><p>62 Morphological case is distinct from grammatical role, as noun phrases can function as adjuncts in possessive constructions and preposiAnnotation Accuracy (%)  Grammatical role83. [sent-185, score-0.278]
</p><p>63 +PP means that prepositional objects are treated as a separate category from topological fields. [sent-190, score-0.712]
</p><p>64 However, we can approximate the grammatical role of an entity using the morphological case. [sent-193, score-0.37]
</p><p>65 We follow the annotation conventions of T ¨uBa-D/Z in not assigning a grammatical role when the noun phrase is a prepositional object. [sent-194, score-0.384]
</p><p>66 We also do not assign a grammatical role when the noun phrase is in the genitive case, as genitive objects are very rare in German and are  far outnumbered by the possessive genitive construction. [sent-195, score-0.474]
</p><p>67 The top four performing entity representations are all topological field-based, and they outperform grammatical role-based and simple clausal order-based models. [sent-198, score-1.1]
</p><p>68 These results indicate that the information that topological fields provide about clause structure, appositives, right dislocation, etc. [sent-199, score-0.853]
</p><p>69 which is not captured by simple clausal order is important for coherence modelling. [sent-200, score-0.42]
</p><p>70 The representations incorporating linguistics-based heuristics do not outperform purely topological field-based models. [sent-201, score-0.667]
</p><p>71 Surprisingly, the VF-based models fare quite poorly, performing worse than not adding any annotations, despite the fact that topological fieldbased models in general perform well. [sent-202, score-0.604]
</p><p>72 The automatic topological field annotations are more accurate than the automatic grammatical role  annotations (Table 3), which may partly explain why grammatical role-based models suffer more when using automatic annotations. [sent-204, score-1.189]
</p><p>73 Note, however, that the models based on automatic topological field annotations outperform even the grammatical role-based models using manual annotation (at marginal significance, p < 0. [sent-205, score-0.953]
</p><p>74 The topo191  logical field annotations are accurate enough that automatic annotations produce no decrease in performance. [sent-207, score-0.246]
</p><p>75 These results show the upper bound of entitybased local coherence modelling with perfect coreference information. [sent-208, score-0.555]
</p><p>76 In our experiments, we create an entity for every single  noun phrase node that we encounter, then merge the entities that are linked by coreference. [sent-224, score-0.345]
</p><p>77 Filip-  pova and Strube (2007a) convert the annotations of T ¨uBa-D/Z  into a dependency  format, then ex-  tract entities from the noun phrases found there. [sent-225, score-0.271]
</p><p>78 They may thus annotate fewer entities, as there 1Barzilay and Lapata (2008) use the coreference system of Ng and Cardie (2002) to obtain coreference annotations. [sent-226, score-0.222]
</p><p>79 experiment with various entity representations using manual and automatic annotations of topological fields and grammatical roles on subset of corpus used by Filippova and Strube (2007a). [sent-230, score-1.267]
</p><p>80 The relative rankings of different entity representations in this experiment are similar to the rankings ofthe previous experiment, with topological field-based models outperforming grammatical role and clausal order models. [sent-233, score-1.193]
</p><p>81 Various coherence models have been tested in corpus-based NLG settings. [sent-237, score-0.272]
</p><p>82 (2009) compare several versions of Centering Theory-based metrics of coherence on corpora by examining how highly the original ordering found in the corpus  is ranked compared to other possible orderings of propositions. [sent-239, score-0.479]
</p><p>83 We embed entity topological field transitions into their probabilistic model, and show that the added coherence component slightly improves the performance of the baseline NLG system in generating constituent orderings in a German corpus, though not to a statistically significant degree. [sent-242, score-1.389]
</p><p>84 The baseline generation system already incorporates topological field information into the constituent ordering process. [sent-248, score-0.999]
</p><p>85 In the first VF selection step, MAXENT simply produces a probability of each constituent being a VF, and the constituent with the highest probability is selected. [sent-259, score-0.24]
</p><p>86 The final ordering is achieved by first randomizing the order of the constituents in a clause (besides the first one, which is selected to be the VF), then sorting them according to the precedence probabilities. [sent-261, score-0.309]
</p><p>87 Specifically, a constituent A is put before a constituent B if MAXENT2(A,B) > 0. [sent-262, score-0.24]
</p><p>88 We incorporate local coherence information into the model by adding entity transition features which  we found to be useful in the sentence ordering experiment in Section 3 above. [sent-273, score-0.72]
</p><p>89 Specifically, we add features indicating the topological fields in which entities occur in the previous sentences. [sent-274, score-0.816]
</p><p>90 Because this corpus does not come with general coreference information except for the coreference chain of the biographee, we use the semantic classes instead. [sent-276, score-0.222]
</p><p>91 An example of a feature may be biog-last2, which takes on a value such as ‘v−’, meaning that this constituent refers to thhe a biographee, ainngd thhaet biographee occurs rins the VF two clauses ago (v), but does not appear in the previous clause (−). [sent-278, score-0.379]
</p><p>92 72  Table 5: Results of adding coherence features into a natural language generation system. [sent-286, score-0.308]
</p><p>93 We suggest that the lack of coreference information for all entities in the article may have reduced the benefit of the coherence component. [sent-295, score-0.451]
</p><p>94 5  Conclusions  We have shown that topological fields are a useful source of information for local coherence modelling. [sent-301, score-1.108]
</p><p>95 In a sentence-order permutation detection task, models which use topological field information outperform both grammatical role-based models and models based on simple clausal order, with the best performing model achieving a relative error reduction of 40. [sent-302, score-1.007]
</p><p>96 Applying our local coherence model in another setting, we have embedded topological field transitions of entities into an NLG system which orders constituents in German clauses. [sent-304, score-1.252]
</p><p>97 We suggest that the utility of topological fields in local coherence modelling comes from the interaction between word order and information structure in freer-word-order languages. [sent-306, score-1.16]
</p><p>98 Crucially, topological fields take into account issues such as coordination, appositives, sentential fragments and differences in clause types, which word order alone does not. [sent-307, score-0.88]
</p><p>99 Further refinement of the topological field annotations to take advantage of the fact that they do not correspond neatly to any single information status such as topic or focus could provide additional performance gains. [sent-309, score-0.828]
</p><p>100 Extending the entity-grid coherence model to semantically related entities. [sent-367, score-0.272]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('topological', 0.604), ('coherence', 0.272), ('vf', 0.262), ('filippova', 0.191), ('clausal', 0.148), ('fields', 0.144), ('entity', 0.144), ('grammatical', 0.141), ('barzilay', 0.127), ('ordering', 0.125), ('strube', 0.122), ('constituent', 0.12), ('field', 0.114), ('coreference', 0.111), ('german', 0.111), ('clause', 0.105), ('biographee', 0.101), ('vorfeld', 0.101), ('centering', 0.096), ('noun', 0.096), ('local', 0.088), ('mf', 0.084), ('lapata', 0.08), ('grid', 0.069), ('entities', 0.068), ('frauen', 0.067), ('annotations', 0.066), ('representations', 0.063), ('nlg', 0.061), ('transitions', 0.061), ('salience', 0.059), ('role', 0.057), ('nf', 0.057), ('transition', 0.055), ('clauses', 0.053), ('prepositional', 0.053), ('modelling', 0.052), ('auch', 0.05), ('postposed', 0.05), ('vfs', 0.05), ('cheung', 0.048), ('constituents', 0.045), ('kendall', 0.044), ('orderings', 0.044), ('topic', 0.044), ('roles', 0.041), ('phrases', 0.041), ('discourse', 0.039), ('distinguishes', 0.039), ('marks', 0.038), ('genitive', 0.038), ('versions', 0.038), ('phrase', 0.037), ('poesio', 0.037), ('document', 0.037), ('generation', 0.036), ('experiment', 0.036), ('pp', 0.036), ('oth', 0.036), ('acc', 0.034), ('precedence', 0.034), ('accidents', 0.034), ('addressation', 0.034), ('earthquakes', 0.034), ('grids', 0.034), ('oftopological', 0.034), ('opic', 0.034), ('vfpp', 0.034), ('werden', 0.034), ('perfect', 0.032), ('marked', 0.031), ('statistically', 0.03), ('passage', 0.029), ('prosodically', 0.029), ('neighbouring', 0.029), ('bracket', 0.029), ('karamanis', 0.029), ('telljohann', 0.029), ('objects', 0.029), ('anaphora', 0.029), ('structuring', 0.029), ('manual', 0.028), ('representation', 0.028), ('morphological', 0.028), ('sentential', 0.027), ('permuted', 0.027), ('versley', 0.027), ('subordinating', 0.027), ('die', 0.027), ('dipper', 0.027), ('sgall', 0.027), ('tau', 0.027), ('resolution', 0.026), ('treated', 0.026), ('tried', 0.025), ('abbreviated', 0.025), ('kibble', 0.025), ('appositives', 0.025), ('women', 0.025), ('fault', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="101-tfidf-1" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly.</p><p>2 0.1893803 <a title="101-tfidf-2" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>Author: Emily Pitler ; Annie Louis ; Ani Nenkova</p><p>Abstract: To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference informa- tion, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.</p><p>3 0.14232129 <a title="101-tfidf-3" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>4 0.10174 <a title="101-tfidf-4" href="./acl-2010-Constituency_to_Dependency_Translation_with_Forests.html">69 acl-2010-Constituency to Dependency Translation with Forests</a></p>
<p>Author: Haitao Mi ; Qun Liu</p><p>Abstract: Tree-to-string systems (and their forestbased extensions) have gained steady popularity thanks to their simplicity and efficiency, but there is a major limitation: they are unable to guarantee the grammaticality of the output, which is explicitly modeled in string-to-tree systems via targetside syntax. We thus propose to combine the advantages of both, and present a novel constituency-to-dependency translation model, which uses constituency forests on the source side to direct the translation, and dependency trees on the target side (as a language model) to ensure grammaticality. Medium-scale experiments show an absolute and statistically significant improvement of +0.7 BLEU points over a state-of-the-art forest-based tree-to-string system even with fewer rules. This is also the first time that a treeto-tree model can surpass tree-to-string counterparts.</p><p>5 0.10018893 <a title="101-tfidf-5" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>Author: Aria Haghighi ; Dan Klein</p><p>Abstract: We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.</p><p>6 0.096357629 <a title="101-tfidf-6" href="./acl-2010-Hard_Constraints_for_Grammatical_Function_Labelling.html">130 acl-2010-Hard Constraints for Grammatical Function Labelling</a></p>
<p>7 0.096048027 <a title="101-tfidf-7" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>8 0.094155967 <a title="101-tfidf-8" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>9 0.094004601 <a title="101-tfidf-9" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>10 0.085131928 <a title="101-tfidf-10" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>11 0.083288819 <a title="101-tfidf-11" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>12 0.082871281 <a title="101-tfidf-12" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>13 0.081186257 <a title="101-tfidf-13" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>14 0.069619291 <a title="101-tfidf-14" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>15 0.068801023 <a title="101-tfidf-15" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>16 0.068477087 <a title="101-tfidf-16" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>17 0.067107067 <a title="101-tfidf-17" href="./acl-2010-Rebanking_CCGbank_for_Improved_NP_Interpretation.html">203 acl-2010-Rebanking CCGbank for Improved NP Interpretation</a></p>
<p>18 0.067101181 <a title="101-tfidf-18" href="./acl-2010-Beyond_NomBank%3A_A_Study_of_Implicit_Arguments_for_Nominal_Predicates.html">49 acl-2010-Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</a></p>
<p>19 0.066497475 <a title="101-tfidf-19" href="./acl-2010-A_Cognitive_Cost_Model_of_Annotations_Based_on_Eye-Tracking_Data.html">4 acl-2010-A Cognitive Cost Model of Annotations Based on Eye-Tracking Data</a></p>
<p>20 0.065132678 <a title="101-tfidf-20" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.192), (1, 0.071), (2, 0.009), (3, -0.104), (4, -0.077), (5, 0.137), (6, 0.02), (7, -0.076), (8, 0.022), (9, 0.058), (10, 0.035), (11, -0.024), (12, 0.011), (13, 0.018), (14, 0.043), (15, 0.024), (16, 0.076), (17, 0.024), (18, 0.031), (19, 0.004), (20, 0.045), (21, -0.017), (22, -0.04), (23, 0.012), (24, 0.013), (25, 0.029), (26, -0.007), (27, 0.018), (28, -0.03), (29, -0.024), (30, 0.033), (31, -0.054), (32, -0.017), (33, -0.017), (34, 0.048), (35, -0.02), (36, 0.002), (37, -0.051), (38, -0.137), (39, 0.019), (40, 0.011), (41, 0.067), (42, -0.139), (43, -0.128), (44, -0.041), (45, -0.03), (46, 0.059), (47, -0.019), (48, 0.057), (49, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93360847 <a title="101-lsi-1" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly.</p><p>2 0.60502023 <a title="101-lsi-2" href="./acl-2010-Automatic_Evaluation_of_Linguistic_Quality_in_Multi-Document_Summarization.html">38 acl-2010-Automatic Evaluation of Linguistic Quality in Multi-Document Summarization</a></p>
<p>Author: Emily Pitler ; Annie Louis ; Ani Nenkova</p><p>Abstract: To date, few attempts have been made to develop and validate methods for automatic evaluation of linguistic quality in text summarization. We present the first systematic assessment of several diverse classes of metrics designed to capture various aspects of well-written text. We train and test linguistic quality models on consecutive years of NIST evaluation data in order to show the generality of results. For grammaticality, the best results come from a set of syntactic features. Focus, coherence and referential clarity are best evaluated by a class of features measuring local coherence on the basis of cosine similarity between sentences, coreference informa- tion, and summarization specific features. Our best results are 90% accuracy for pairwise comparisons of competing systems over a test set of several inputs and 70% for ranking summaries of a specific input.</p><p>3 0.5995878 <a title="101-lsi-3" href="./acl-2010-An_Entity-Level_Approach_to_Information_Extraction.html">28 acl-2010-An Entity-Level Approach to Information Extraction</a></p>
<p>Author: Aria Haghighi ; Dan Klein</p><p>Abstract: We present a generative model of template-filling in which coreference resolution and role assignment are jointly determined. Underlying template roles first generate abstract entities, which in turn generate concrete textual mentions. On the standard corporate acquisitions dataset, joint resolution in our entity-level model reduces error over a mention-level discriminative approach by up to 20%.</p><p>4 0.57044953 <a title="101-lsi-4" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>Author: Micha Elsner ; Eugene Charniak</p><p>Abstract: We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent– but this is not always true, especially if realistic mention detection is used. We describe the distribution of noncoreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some samehead NPs using syntactic features, improving precision.</p><p>5 0.56760162 <a title="101-lsi-5" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>Author: Vincent Ng</p><p>Abstract: The research focus of computational coreference resolution has exhibited a shift from heuristic approaches to machine learning approaches in the past decade. This paper surveys the major milestones in supervised coreference research since its inception fifteen years ago.</p><p>6 0.55265641 <a title="101-lsi-6" href="./acl-2010-Coreference_Resolution_across_Corpora%3A_Languages%2C_Coding_Schemes%2C_and_Preprocessing_Information.html">72 acl-2010-Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information</a></p>
<p>7 0.51365656 <a title="101-lsi-7" href="./acl-2010-Incorporating_Extra-Linguistic_Information_into_Reference_Resolution_in_Collaborative_Task_Dialogue.html">149 acl-2010-Incorporating Extra-Linguistic Information into Reference Resolution in Collaborative Task Dialogue</a></p>
<p>8 0.51000208 <a title="101-lsi-8" href="./acl-2010-Automatic_Generation_of_Story_Highlights.html">39 acl-2010-Automatic Generation of Story Highlights</a></p>
<p>9 0.50958323 <a title="101-lsi-9" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>10 0.5093531 <a title="101-lsi-10" href="./acl-2010-Plot_Induction_and_Evolutionary_Search_for_Story_Generation.html">196 acl-2010-Plot Induction and Evolutionary Search for Story Generation</a></p>
<p>11 0.49990281 <a title="101-lsi-11" href="./acl-2010-Generating_Fine-Grained_Reviews_of_Songs_from_Album_Reviews.html">122 acl-2010-Generating Fine-Grained Reviews of Songs from Album Reviews</a></p>
<p>12 0.47533333 <a title="101-lsi-12" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>13 0.47320566 <a title="101-lsi-13" href="./acl-2010-Generating_Templates_of_Entity_Summaries_with_an_Entity-Aspect_Model_and_Pattern_Mining.html">125 acl-2010-Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining</a></p>
<p>14 0.4719511 <a title="101-lsi-14" href="./acl-2010-Profiting_from_Mark-Up%3A_Hyper-Text_Annotations_for_Guided_Parsing.html">200 acl-2010-Profiting from Mark-Up: Hyper-Text Annotations for Guided Parsing</a></p>
<p>15 0.47114331 <a title="101-lsi-15" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>16 0.47067538 <a title="101-lsi-16" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>17 0.46625328 <a title="101-lsi-17" href="./acl-2010-The_Influence_of_Discourse_on_Syntax%3A_A_Psycholinguistic_Model_of_Sentence_Processing.html">229 acl-2010-The Influence of Discourse on Syntax: A Psycholinguistic Model of Sentence Processing</a></p>
<p>18 0.46044007 <a title="101-lsi-18" href="./acl-2010-Creating_Robust_Supervised_Classifiers_via_Web-Scale_N-Gram_Data.html">76 acl-2010-Creating Robust Supervised Classifiers via Web-Scale N-Gram Data</a></p>
<p>19 0.45549282 <a title="101-lsi-19" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>20 0.45391929 <a title="101-lsi-20" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.01), (14, 0.015), (25, 0.108), (39, 0.015), (42, 0.036), (44, 0.014), (52, 0.204), (59, 0.073), (73, 0.049), (78, 0.053), (80, 0.017), (83, 0.166), (84, 0.033), (98, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88028026 <a title="101-lda-1" href="./acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</a></p>
<p>Author: Mikko Kurimo ; William Byrne ; John Dines ; Philip N. Garner ; Matthew Gibson ; Yong Guan ; Teemu Hirsimaki ; Reima Karhila ; Simon King ; Hui Liang ; Keiichiro Oura ; Lakshmi Saheer ; Matt Shannon ; Sayaki Shiota ; Jilei Tian</p><p>Abstract: In the EMIME project we have studied unsupervised cross-lingual speaker adaptation. We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms to adapt the synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition). An important application for this research is personalised speech-to-speech translation that will use the voice of the speaker in the input language to utter the translated sentences in the output language. In mobile environments this enhances the users’ interaction across language barriers by making the output speech sound more like the original speaker’s way of speaking, even if she or he could not speak the output language.</p><p>same-paper 2 0.84820271 <a title="101-lda-2" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly.</p><p>3 0.84307349 <a title="101-lda-3" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>Author: Ainur Yessenalina ; Yejin Choi ; Claire Cardie</p><p>Abstract: One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</p><p>4 0.79799354 <a title="101-lda-4" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>Author: Dong Yang ; Paul Dixon ; Sadaoki Furui</p><p>Abstract: This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outper- forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</p><p>5 0.74855971 <a title="101-lda-5" href="./acl-2010-%22Ask_Not_What_Textual_Entailment_Can_Do_for_You...%22.html">1 acl-2010-"Ask Not What Textual Entailment Can Do for You..."</a></p>
<p>Author: Mark Sammons ; V.G.Vinod Vydiswaran ; Dan Roth</p><p>Abstract: We challenge the NLP community to participate in a large-scale, distributed effort to design and build resources for developing and evaluating solutions to new and existing NLP tasks in the context of Recognizing Textual Entailment. We argue that the single global label with which RTE examples are annotated is insufficient to effectively evaluate RTE system performance; to promote research on smaller, related NLP tasks, we believe more detailed annotation and evaluation are needed, and that this effort will benefit not just RTE researchers, but the NLP community as a whole. We use insights from successful RTE systems to propose a model for identifying and annotating textual infer- ence phenomena in textual entailment examples, and we present the results of a pilot annotation study that show this model is feasible and the results immediately useful.</p><p>6 0.74223459 <a title="101-lda-6" href="./acl-2010-Convolution_Kernel_over_Packed_Parse_Forest.html">71 acl-2010-Convolution Kernel over Packed Parse Forest</a></p>
<p>7 0.74202448 <a title="101-lda-7" href="./acl-2010-Joint_Syntactic_and_Semantic_Parsing_of_Chinese.html">153 acl-2010-Joint Syntactic and Semantic Parsing of Chinese</a></p>
<p>8 0.72796863 <a title="101-lda-8" href="./acl-2010-Assessing_the_Role_of_Discourse_References_in_Entailment_Inference.html">33 acl-2010-Assessing the Role of Discourse References in Entailment Inference</a></p>
<p>9 0.72705907 <a title="101-lda-9" href="./acl-2010-Coreference_Resolution_with_Reconcile.html">73 acl-2010-Coreference Resolution with Reconcile</a></p>
<p>10 0.72436273 <a title="101-lda-10" href="./acl-2010-Using_Parse_Features_for_Preposition_Selection_and_Error_Detection.html">252 acl-2010-Using Parse Features for Preposition Selection and Error Detection</a></p>
<p>11 0.71986449 <a title="101-lda-11" href="./acl-2010-Kernel_Based_Discourse_Relation_Recognition_with_Temporal_Ordering_Information.html">155 acl-2010-Kernel Based Discourse Relation Recognition with Temporal Ordering Information</a></p>
<p>12 0.71945661 <a title="101-lda-12" href="./acl-2010-Unsupervised_Event_Coreference_Resolution_with_Rich_Linguistic_Features.html">247 acl-2010-Unsupervised Event Coreference Resolution with Rich Linguistic Features</a></p>
<p>13 0.71482867 <a title="101-lda-13" href="./acl-2010-The_Same-Head_Heuristic_for_Coreference.html">233 acl-2010-The Same-Head Heuristic for Coreference</a></p>
<p>14 0.7143873 <a title="101-lda-14" href="./acl-2010-Latent_Variable_Models_of_Selectional_Preference.html">158 acl-2010-Latent Variable Models of Selectional Preference</a></p>
<p>15 0.71367371 <a title="101-lda-15" href="./acl-2010-Hierarchical_Sequential_Learning_for_Extracting_Opinions_and_Their_Attributes.html">134 acl-2010-Hierarchical Sequential Learning for Extracting Opinions and Their Attributes</a></p>
<p>16 0.71359742 <a title="101-lda-16" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<p>17 0.71284992 <a title="101-lda-17" href="./acl-2010-Supervised_Noun_Phrase_Coreference_Research%3A_The_First_Fifteen_Years.html">219 acl-2010-Supervised Noun Phrase Coreference Research: The First Fifteen Years</a></p>
<p>18 0.71124029 <a title="101-lda-18" href="./acl-2010-A_Structured_Model_for_Joint_Learning_of_Argument_Roles_and_Predicate_Senses.html">17 acl-2010-A Structured Model for Joint Learning of Argument Roles and Predicate Senses</a></p>
<p>19 0.70966822 <a title="101-lda-19" href="./acl-2010-The_Manually_Annotated_Sub-Corpus%3A_A_Community_Resource_for_and_by_the_People.html">230 acl-2010-The Manually Annotated Sub-Corpus: A Community Resource for and by the People</a></p>
<p>20 0.70940667 <a title="101-lda-20" href="./acl-2010-Extracting_Social_Networks_from_Literary_Fiction.html">112 acl-2010-Extracting Social Networks from Literary Fiction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
