<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2010" href="../home/acl2010_home.html">acl2010</a> <a title="acl-2010-154" href="#">acl2010-154</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</h1>
<br/><p>Source: <a title="acl-2010-154-pdf" href="http://aclweb.org/anthology//P/P10/P10-2051.pdf">pdf</a></p><p>Author: Dong Yang ; Paul Dixon ; Sadaoki Furui</p><p>Abstract: This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outper- forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</p><p>Reference: <a title="acl-2010-154-reference" href="../acl2010_reference/acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Jointly optimizing a two-step conditional random field model for machine transliteration and its fast decoding algorithm Dong Yang, Paul Dixon and Sadaoki Furui Department of Computer Science Tokyo Institute of Technology Tokyo 152-8552 Japan {raymond, dixonp ,furui} @ furui . [sent-1, score-0.837]
</p><p>2 j p  Abstract This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. [sent-5, score-0.887]
</p><p>3 Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. [sent-6, score-0.167]
</p><p>4 In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. [sent-7, score-0.159]
</p><p>5 In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. [sent-8, score-0.201]
</p><p>6 Our experiments show that the proposed method outper-  forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. [sent-9, score-0.548]
</p><p>7 Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy. [sent-10, score-0.031]
</p><p>8 The translation of named entities from alphabetic to syllabary language is usually performed through transliteration, which tries to preserve the pronunciation in the original language. [sent-13, score-0.081]
</p><p>9 For example, in Chinese, foreign words are written with Chinese characters; in Japanese, foreign words are usually written with special char-  Source Name  Target Name  Note  G o o g l e ? [sent-14, score-0.18]
</p><p>10 An intuitive transliteration method (Knight and Graehl, 1998; Oh et al. [sent-20, score-0.388]
</p><p>11 , 2006) is to firstly convert a source word into phonemes, then find the corresponding phonemes in the target language, and finally convert them to the target language’s written system. [sent-21, score-0.272]
</p><p>12 The source channel and joint source channel models (JSCMs) (Li et al. [sent-26, score-0.253]
</p><p>13 , 2004) have been proposed for DOM, which try to model P(T|S) and P(T, S) respectively, hwh treyre to oT m aondde lS P d(eTn|oSt)e atnhed words in the target and source languages. [sent-27, score-0.09]
</p><p>14 (2006) modified the JSCM to incorporate different context information into the model for 275  UppsalaP,r Sowce ed ein ,g 1s1 o-f16 th Jeu AlyC 2L0 210 1. [sent-29, score-0.025]
</p><p>15 In the “NEWS 2009 Machine Transliteration Shared Task”, a new two-step CRF model for transliteration task has been proposed  (Yang et al. [sent-32, score-0.357]
</p><p>16 , 2009), in which the first step is to segment a word in the source language into character chunks and the second step is to perform a context-dependent mapping from each chunk into one written unit in the target language. [sent-33, score-0.334]
</p><p>17 In this paper, we propose to jointly optimize a two-step CRF model. [sent-34, score-0.068]
</p><p>18 We also propose a fast decoding algorithm to speed up the joint search. [sent-35, score-0.412]
</p><p>19 Although our method is language independent, we use an English-to-Chinese transliteration task in all the explanations and experiments. [sent-37, score-0.388]
</p><p>20 1  Two-step CRF method CRF introduction  A chain-CRF (Lafferty et al. [sent-39, score-0.031]
</p><p>21 , 2001) is an undirected graphical model which assigns a probability to a label sequence L = l1l2 . [sent-40, score-0.034]
</p><p>22 CRF training is usually performed through the L-BFGS algorithm (Wallach, 2002) and decoding is performed by the Viterbi algorithm. [sent-47, score-0.293]
</p><p>23 We formalize machine transliteration as a CRF tagging problem, as shown in Figure 2. [sent-48, score-0.357]
</p><p>24 Figure 2: An pictorial description of a CRF segmenter and a CRF converter 2. [sent-55, score-0.142]
</p><p>25 2 CRF segmenter In the CRF, a feature function describes a cooccurrence relation, and it is usually a binary function, taking the value 1 when both an observation and a label transition are observed. [sent-56, score-0.081]
</p><p>26 One limitation of their work is that only top-1 segmentation is output to the following CRF converter. [sent-59, score-0.113]
</p><p>27 3 CRF converter Similar to the CRF segmenter, the CRF converter has the format shown in Figure 2. [sent-61, score-0.174]
</p><p>28 (2009) used the following features: •  •  Single unit features: CK−1 , CK0, CK1 Combination CK0CK1  features:  CK−1CK0,  where CK represents the source language chunk, and the subscript notation is the same as the CRF segmenter. [sent-63, score-0.084]
</p><p>29 3  Joint optimization and its fast decoding algorithm  3. [sent-64, score-0.41]
</p><p>30 1 Joint optimization We denote a word in the source language by S, a segmentation of S by A, and a word in the target langauge by T. [sent-65, score-0.287]
</p><p>31 Our goal is to find the best word in the target language which maximizes the probability P(T|S). [sent-66, score-0.074]
</p><p>32 (2009) used only the best segmentation in the first CRF and the best output in the second CRF, which is equivalent to  Tˆ  Aˆ Tˆ  =  argAmaxP(A|S)  =  argTmaxP(T|S,Aˆ),  (1)  where P(A|S) and P(T|S, A) represent two wChReFres respectively. [sent-68, score-0.113]
</p><p>33 nTdhi sP m(Te|thSo,dA c)on rseidpreersse tnhte seg-  mentation and the conversion as two independent steps. [sent-69, score-0.115]
</p><p>34 A major limitation is that, if the segmentation from the first step is wrong, the error propagates to the second step, and the error is very difficult to recover. [sent-70, score-0.139]
</p><p>35 It considers the segmentation and conversion in a unified framework and is robust to  segmentation errors. [sent-72, score-0.341]
</p><p>36 2 N-best approximation In the process of finding the best output using Equation 2, a dynamic programming algorithm for joint decoding of the segmentation and conversion is possible, but the implementation becomes very complicated. [sent-74, score-0.538]
</p><p>37 Another direction is to divide the decoding into two steps of segmentation and conversion, which is this paper’s method. [sent-75, score-0.404]
</p><p>38 However, exact inference by listing all possible candidates explicitly and summing over all possible segmentations is intractable, because of the exponential computation complexity with the source word’s increasing length. [sent-76, score-0.228]
</p><p>39 In the segmentation step, the number of possible segmentations is 2N, where N is the length of the source word and 2 is the size of the tagging set. [sent-77, score-0.261]
</p><p>40 In the conversion step, the number of possible candidates is MN′, where N′ is the number of chunks from the 1st step and M is the size of the tagging set. [sent-78, score-0.262]
</p><p>41 Our analysis shows that beyond the 10th candidate, almost all the probabilities of the candidates in both steps drop below 0. [sent-82, score-0.143]
</p><p>42 Therefore we decided to generate top-10 results for both steps to approximate the Equation 2. [sent-84, score-0.024]
</p><p>43 3 Fast decoding algorithm As introduced in the previous subsection, in the whole decoding process we have to perform n-best CRF decoding in the segmentation step and 10 nbest CRF decoding in the second CRF. [sent-86, score-1.207]
</p><p>44 The answer is “No” for candidates with low probabilities. [sent-88, score-0.08]
</p><p>45 Here we propose a no-loss fast decoding algorithm for deciding when to stop performing the second CRF decoding. [sent-89, score-0.435]
</p><p>46 Suppose we have a list of segmentation candidates which are generated by the 1st CRF, ranked by probabilities P(A|S) in descending order A : A1, A2 , . [sent-90, score-0.274]
</p><p>47 , AN a Pn(dA we are performing rtdheer 2And : CRF decoding starting from A1. [sent-93, score-0.299]
</p><p>48 Up to Ak, we get a list of candidates T : T1, T2 , . [sent-94, score-0.08]
</p><p>49 If  we can guarantee that, even performing the 2nd CRF decoding for all the remaining segmentations Ak+1 , Ak+2, . [sent-98, score-0.397]
</p><p>50 , AN, the top 1 candidate does not change, then we can stop decoding. [sent-101, score-0.069]
</p><p>51 We can show that the following formula is the stop condition: Xk  Pk(T1|S) − Pk(T2|S)  > 1  −XP(Aj|S). [sent-102, score-0.034]
</p><p>52 The stop condition here has no approximation nor pre-defined assumption, and it is a no-loss fast decoding algorithm. [sent-105, score-0.43]
</p><p>53 4 Rapid development of a JSCM system The JSCM represents how the source words and target names are generated simultaneously (Li et  al. [sent-106, score-0.09]
</p><p>54 , sk) is a word in the source langauge and T = (t1, t2 , . [sent-119, score-0.093]
</p><p>55 The decoding problem in JSCM can be written as: Tˆ  =  argTmaxP(S,T). [sent-125, score-0.315]
</p><p>56 (5)  277  After the alignments are generated, we use the MITLM toolkit (Hsu and Glass, 2008) to build a trigram model with modified Kneser-Ney smoothing. [sent-126, score-0.025]
</p><p>57 We then convert the n-gram to a WFST  M (Sproat et al. [sent-127, score-0.031]
</p><p>58 To allow transliteration from a sequence of characters, a second WFST T is constructed. [sent-130, score-0.357]
</p><p>59 The input word is converted to an acceptor I, and it is then combined with T and M according to O = I T ◦ M ◦ bwinheedre ◦ dhe Tno atensd Mthe composition operator. [sent-131, score-0.024]
</p><p>60 e oTuht-e put, removing the epsilon labels and applying the n-shortest paths algorithm with determinization in the OpenFst Toolkit (Allauzen et al. [sent-133, score-0.024]
</p><p>61 Top-1 ACC: word accuracy of the top-1 candidate 2. [sent-138, score-0.035]
</p><p>62 Mean F-score: fuzziness in the top-1 candidate, how close the top-1 candidate is to the reference 3. [sent-139, score-0.035]
</p><p>63 3Tr1a9i6n1ing data2D8e9v6elopment dataT2e89st6 data  Table 1: Corpus size (number of word pairs) We compare the proposed decoding method with the baseline which uses only the best candidates in both CRF steps, and also with the well known JSCM. [sent-143, score-0.378]
</p><p>64 As we can see in Table 2, the proposed method improves the baseline top-1 ACC from 0. [sent-144, score-0.031]
</p><p>65 Our experiments also show that the decoding time can be reduced significantly via using our fast decoding algorithm. [sent-147, score-0.636]
</p><p>66 As we have explained, without fast decoding, we need 11 CRF n-best decoding for each word; the number can be reduced to 3. [sent-148, score-0.369]
</p><p>67 We should notice that the decoding time is significantly shorter than the training time. [sent-151, score-0.267]
</p><p>68 While  testing takes minutes on a normal PC, the training of the CRF converter takes up to 13 hours on an 8-core (8*3G Hz) server. [sent-152, score-0.087]
</p><p>69 c7Ro85d90ing  method with the previous method and the JSCM 5. [sent-156, score-0.062]
</p><p>70 From the two-step CRF model we get the conditional probability PCRF(T|S) and from the JSCM we get the joint probability P(S, T). [sent-158, score-0.157]
</p><p>71 The conditional probability of PJSCM(T|S) can be calculuated as follows:  PJSCM(T|S) =PP(T(S,S))=PPT(PT(,TS,)S). [sent-159, score-0.08]
</p><p>72 They are used in our combinatioPn method as:  (6)  P(T|S) = λPCRF(T|S) + (1  λ)PJSCM(T|S) (7) where λ denotes the interpolation weight (λ is set by development data in this paper). [sent-160, score-0.031]
</p><p>73 731 used target language phoneme information, which requires a monolingual dictionary; as a result it is not a standard run. [sent-166, score-0.04]
</p><p>74 ) −  6  Conclusions and future work  In this paper we have presented our new joint optimization method for a two-step CRF model and its fast decoding algorithm. [sent-167, score-0.484]
</p><p>75 The proposed 278  method improved the system significantly and outperformed the JSCM. [sent-168, score-0.031]
</p><p>76 Combining the proposed method with JSCM, the performance was further improved. [sent-169, score-0.031]
</p><p>77 We are currently investigating discriminative training as a method to further improve the JSCM. [sent-172, score-0.031]
</p><p>78 Another issue of our two-step CRF method is that the training complexity increases quadratically according to the size of the label set, and how to reduce the training time needs more research. [sent-173, score-0.031]
</p><p>79 Proof of Equation 3  The CRF segmentation provides a list of segmentations: A : A1, A2, . [sent-175, score-0.113]
</p><p>80 , AN, with conditional probabilities P(A1 |S) , P(A2 |S) , . [sent-178, score-0.085]
</p><p>81 The CRF conversion, given a segmentation Ai, provides a list of transliteration output T1, T2 , . [sent-183, score-0.47]
</p><p>82 , TM, with conditional probabilities P(T1 |S, Ai), P(T2 |S, Ai), . [sent-186, score-0.085]
</p><p>83 In our fast decoding algorithm, we start performing the CRF conversion from A1, then A2, and then A3, etc. [sent-190, score-0.516]
</p><p>84 Up to Ak, we get a list of candidates T : T1, T2, . [sent-191, score-0.08]
</p><p>85 , TL, ranked by probabilities Pk (T|S) in descending order. [sent-194, score-0.081]
</p><p>86 A modified joint source-channel model for transliteration, Proceedings of the COLING/ACL, pages 191-198. [sent-213, score-0.068]
</p><p>87 A joint source-channel model for machine transliteration, Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. [sent-225, score-0.043]
</p><p>88 A comparison of different machine transliteration models , Journal of Artificial Intelligence Research, 27, pages 119-151 . [sent-230, score-0.357]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('crf', 0.561), ('transliteration', 0.357), ('jscm', 0.325), ('decoding', 0.267), ('pk', 0.194), ('tl', 0.176), ('aj', 0.139), ('conversion', 0.115), ('segmentation', 0.113), ('xk', 0.111), ('fast', 0.102), ('segmentations', 0.098), ('xj', 0.095), ('xp', 0.093), ('converter', 0.087), ('argtmaxp', 0.081), ('pjscm', 0.081), ('candidates', 0.08), ('ak', 0.075), ('furui', 0.065), ('ti', 0.064), ('wfst', 0.061), ('dom', 0.061), ('segmenter', 0.055), ('acc', 0.055), ('channel', 0.055), ('pronunciation', 0.055), ('argtmaxxap', 0.054), ('caseiro', 0.054), ('dixon', 0.054), ('pcrf', 0.054), ('li', 0.052), ('xn', 0.051), ('yang', 0.05), ('source', 0.05), ('written', 0.048), ('ck', 0.048), ('ekbal', 0.047), ('glass', 0.047), ('conditional', 0.046), ('chunk', 0.044), ('orthographical', 0.043), ('sadaoki', 0.043), ('langauge', 0.043), ('dong', 0.043), ('hsu', 0.043), ('joint', 0.043), ('descending', 0.042), ('characters', 0.042), ('optimization', 0.041), ('chunks', 0.041), ('wallach', 0.041), ('phonemic', 0.041), ('target', 0.04), ('probabilities', 0.039), ('sproat', 0.039), ('jointly', 0.037), ('ai', 0.035), ('appendix', 0.035), ('yk', 0.035), ('news', 0.035), ('candidate', 0.035), ('probability', 0.034), ('unit', 0.034), ('stop', 0.034), ('tm', 0.034), ('performing', 0.032), ('sk', 0.032), ('phonemes', 0.032), ('oh', 0.032), ('tokyo', 0.032), ('optimize', 0.031), ('convert', 0.031), ('viterbi', 0.031), ('method', 0.031), ('haizhou', 0.03), ('tk', 0.03), ('foreign', 0.029), ('proof', 0.028), ('condition', 0.027), ('rapid', 0.027), ('intermediate', 0.027), ('lafferty', 0.027), ('usually', 0.026), ('step', 0.026), ('equation', 0.025), ('mapping', 0.025), ('modified', 0.025), ('steps', 0.024), ('thejoint', 0.024), ('hz', 0.024), ('mthe', 0.024), ('nisg', 0.024), ('oliveira', 0.024), ('determinization', 0.024), ('convolutional', 0.024), ('dhe', 0.024), ('kumaran', 0.024), ('nakamura', 0.024), ('asif', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="154-tfidf-1" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>Author: Dong Yang ; Paul Dixon ; Sadaoki Furui</p><p>Abstract: This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outper- forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</p><p>2 0.18868953 <a title="154-tfidf-2" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>Author: Nadir Durrani ; Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain final BLEU scores of 19.35 (conditional prob- ability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu.</p><p>3 0.16312546 <a title="154-tfidf-3" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>Author: Nikolaos Trogkanis ; Charles Elkan</p><p>Abstract: Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random fields. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages.</p><p>4 0.12637231 <a title="154-tfidf-4" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>Author: Jenny Rose Finkel ; Christopher D. Manning</p><p>Abstract: One of the main obstacles to producing high quality joint models is the lack of jointly annotated data. Joint modeling of multiple natural language processing tasks outperforms single-task models learned from the same data, but still underperforms compared to single-task models learned on the more abundant quantities of available single-task annotated data. In this paper we present a novel model which makes use of additional single-task annotated data to improve the performance of a joint model. Our model utilizes a hierarchical prior to link the feature weights for shared features in several single-task models and the joint model. Experiments on joint parsing and named entity recog- nition, using the OntoNotes corpus, show that our hierarchical joint model can produce substantial gains over a joint model trained on only the jointly annotated data.</p><p>5 0.11369187 <a title="154-tfidf-5" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>Author: Yang Liu ; Liang Huang</p><p>Abstract: unkown-abstract</p><p>6 0.095294274 <a title="154-tfidf-6" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>7 0.088154167 <a title="154-tfidf-7" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>8 0.082579531 <a title="154-tfidf-8" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>9 0.077105984 <a title="154-tfidf-9" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>10 0.070723258 <a title="154-tfidf-10" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>11 0.068628274 <a title="154-tfidf-11" href="./acl-2010-A_Risk_Minimization_Framework_for_Extractive_Speech_Summarization.html">14 acl-2010-A Risk Minimization Framework for Extractive Speech Summarization</a></p>
<p>12 0.067744717 <a title="154-tfidf-12" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>13 0.064943135 <a title="154-tfidf-13" href="./acl-2010-Training_Phrase_Translation_Models_with_Leaving-One-Out.html">240 acl-2010-Training Phrase Translation Models with Leaving-One-Out</a></p>
<p>14 0.062235542 <a title="154-tfidf-14" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>15 0.058179401 <a title="154-tfidf-15" href="./acl-2010-Unsupervised_Discourse_Segmentation_of_Documents_with_Inherently_Parallel_Structure.html">246 acl-2010-Unsupervised Discourse Segmentation of Documents with Inherently Parallel Structure</a></p>
<p>16 0.056763388 <a title="154-tfidf-16" href="./acl-2010-Understanding_the_Semantic_Structure_of_Noun_Phrase_Queries.html">245 acl-2010-Understanding the Semantic Structure of Noun Phrase Queries</a></p>
<p>17 0.055442542 <a title="154-tfidf-17" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>18 0.054490045 <a title="154-tfidf-18" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>19 0.051750768 <a title="154-tfidf-19" href="./acl-2010-Open_Information_Extraction_Using_Wikipedia.html">185 acl-2010-Open Information Extraction Using Wikipedia</a></p>
<p>20 0.050332546 <a title="154-tfidf-20" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2010_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.14), (1, -0.065), (2, -0.005), (3, 0.01), (4, 0.005), (5, -0.037), (6, -0.007), (7, -0.014), (8, 0.021), (9, 0.03), (10, -0.031), (11, 0.027), (12, 0.075), (13, -0.129), (14, -0.09), (15, -0.008), (16, -0.118), (17, 0.094), (18, -0.022), (19, 0.028), (20, 0.045), (21, -0.015), (22, -0.193), (23, -0.057), (24, -0.061), (25, -0.096), (26, 0.056), (27, -0.132), (28, -0.081), (29, 0.008), (30, -0.14), (31, 0.045), (32, -0.101), (33, -0.197), (34, -0.225), (35, 0.023), (36, -0.138), (37, 0.101), (38, -0.049), (39, -0.142), (40, -0.003), (41, 0.127), (42, -0.086), (43, -0.041), (44, 0.141), (45, -0.087), (46, 0.01), (47, 0.086), (48, 0.014), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95426005 <a title="154-lsi-1" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>Author: Dong Yang ; Paul Dixon ; Sadaoki Furui</p><p>Abstract: This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outper- forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</p><p>2 0.69065017 <a title="154-lsi-2" href="./acl-2010-Hindi-to-Urdu_Machine_Translation_through_Transliteration.html">135 acl-2010-Hindi-to-Urdu Machine Translation through Transliteration</a></p>
<p>Author: Nadir Durrani ; Hassan Sajjad ; Alexander Fraser ; Helmut Schmid</p><p>Abstract: We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain final BLEU scores of 19.35 (conditional prob- ability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu.</p><p>3 0.67460591 <a title="154-lsi-3" href="./acl-2010-Conditional_Random_Fields_for_Word_Hyphenation.html">68 acl-2010-Conditional Random Fields for Word Hyphenation</a></p>
<p>Author: Nikolaos Trogkanis ; Charles Elkan</p><p>Abstract: Finding allowable places in words to insert hyphens is an important practical problem. The algorithm that is used most often nowadays has remained essentially unchanged for 25 years. This method is the TEX hyphenation algorithm of Knuth and Liang. We present here a hyphenation method that is clearly more accurate. The new method is an application of conditional random fields. We create new training sets for English and Dutch from the CELEX European lexical resource, and achieve error rates for English of less than 0.1% for correctly allowed hyphens, and less than 0.01% for Dutch. Experiments show that both the Knuth/Liang method and a leading current commercial alternative have error rates several times higher for both languages.</p><p>4 0.58131164 <a title="154-lsi-4" href="./acl-2010-Efficient_Staggered_Decoding_for_Sequence_Labeling.html">98 acl-2010-Efficient Staggered Decoding for Sequence Labeling</a></p>
<p>Author: Nobuhiro Kaji ; Yasuhiro Fujiwara ; Naoki Yoshinaga ; Masaru Kitsuregawa</p><p>Abstract: The Viterbi algorithm is the conventional decoding algorithm most widely adopted for sequence labeling. Viterbi decoding is, however, prohibitively slow when the label set is large, because its time complexity is quadratic in the number of labels. This paper proposes an exact decoding algorithm that overcomes this problem. A novel property of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algo- rithm, CARPEDIEM (Esposito and Radicioni, 2009).</p><p>5 0.52880228 <a title="154-lsi-5" href="./acl-2010-An_Exact_A%2A_Method_for_Deciphering_Letter-Substitution_Ciphers.html">29 acl-2010-An Exact A* Method for Deciphering Letter-Substitution Ciphers</a></p>
<p>Author: Eric Corlett ; Gerald Penn</p><p>Abstract: Letter-substitution ciphers encode a document from a known or hypothesized language into an unknown writing system or an unknown encoding of a known writing system. It is a problem that can occur in a number of practical applications, such as in the problem of determining the encodings of electronic documents in which the language is known, but the encoding standard is not. It has also been used in relation to OCR applications. In this paper, we introduce an exact method for deciphering messages using a generalization of the Viterbi algorithm. We test this model on a set of ciphers developed from various web sites, and find that our algorithm has the potential to be a viable, practical method for efficiently solving decipherment prob- lems.</p><p>6 0.48433575 <a title="154-lsi-6" href="./acl-2010-Efficient_Path_Counting_Transducers_for_Minimum_Bayes-Risk_Decoding_of_Statistical_Machine_Translation_Lattices.html">97 acl-2010-Efficient Path Counting Transducers for Minimum Bayes-Risk Decoding of Statistical Machine Translation Lattices</a></p>
<p>7 0.43091798 <a title="154-lsi-7" href="./acl-2010-Hierarchical_Joint_Learning%3A_Improving_Joint_Parsing_and_Named_Entity_Recognition_with_Non-Jointly_Labeled_Data.html">132 acl-2010-Hierarchical Joint Learning: Improving Joint Parsing and Named Entity Recognition with Non-Jointly Labeled Data</a></p>
<p>8 0.40737453 <a title="154-lsi-8" href="./acl-2010-On_Jointly_Recognizing_and_Aligning_Bilingual_Named_Entities.html">180 acl-2010-On Jointly Recognizing and Aligning Bilingual Named Entities</a></p>
<p>9 0.38662189 <a title="154-lsi-9" href="./acl-2010-A_Statistical_Model_for_Lost_Language_Decipherment.html">16 acl-2010-A Statistical Model for Lost Language Decipherment</a></p>
<p>10 0.36990398 <a title="154-lsi-10" href="./acl-2010-Automatic_Sanskrit_Segmentizer_Using_Finite_State_Transducers.html">40 acl-2010-Automatic Sanskrit Segmentizer Using Finite State Transducers</a></p>
<p>11 0.3481535 <a title="154-lsi-11" href="./acl-2010-Extracting_Sequences_from_the_Web.html">111 acl-2010-Extracting Sequences from the Web</a></p>
<p>12 0.34628823 <a title="154-lsi-12" href="./acl-2010-cdec%3A_A_Decoder%2C_Alignment%2C_and_Learning_Framework_for_Finite-State_and_Context-Free_Translation_Models.html">265 acl-2010-cdec: A Decoder, Alignment, and Learning Framework for Finite-State and Context-Free Translation Models</a></p>
<p>13 0.33883038 <a title="154-lsi-13" href="./acl-2010-Practical_Very_Large_Scale_CRFs.html">197 acl-2010-Practical Very Large Scale CRFs</a></p>
<p>14 0.33517092 <a title="154-lsi-14" href="./acl-2010-Tree-Based_and_Forest-Based_Translation.html">243 acl-2010-Tree-Based and Forest-Based Translation</a></p>
<p>15 0.30589744 <a title="154-lsi-15" href="./acl-2010-A_Generalized-Zero-Preserving_Method_for_Compact_Encoding_of_Concept_Lattices.html">7 acl-2010-A Generalized-Zero-Preserving Method for Compact Encoding of Concept Lattices</a></p>
<p>16 0.29560459 <a title="154-lsi-16" href="./acl-2010-Paraphrase_Lattice_for_Statistical_Machine_Translation.html">192 acl-2010-Paraphrase Lattice for Statistical Machine Translation</a></p>
<p>17 0.29476818 <a title="154-lsi-17" href="./acl-2010-Word_Representations%3A_A_Simple_and_General_Method_for_Semi-Supervised_Learning.html">263 acl-2010-Word Representations: A Simple and General Method for Semi-Supervised Learning</a></p>
<p>18 0.29288861 <a title="154-lsi-18" href="./acl-2010-Viterbi_Training_for_PCFGs%3A_Hardness_Results_and_Competitiveness_of_Uniform_Initialization.html">255 acl-2010-Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization</a></p>
<p>19 0.28873077 <a title="154-lsi-19" href="./acl-2010-Unsupervised_Search_for_the_Optimal_Segmentation_for_Statistical_Machine_Translation.html">249 acl-2010-Unsupervised Search for the Optimal Segmentation for Statistical Machine Translation</a></p>
<p>20 0.28636974 <a title="154-lsi-20" href="./acl-2010-Arabic_Named_Entity_Recognition%3A_Using_Features_Extracted_from_Noisy_Data.html">32 acl-2010-Arabic Named Entity Recognition: Using Features Extracted from Noisy Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2010_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(7, 0.022), (14, 0.026), (20, 0.012), (25, 0.055), (42, 0.015), (44, 0.015), (52, 0.292), (59, 0.112), (73, 0.087), (76, 0.013), (78, 0.027), (83, 0.072), (84, 0.023), (98, 0.125)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87092137 <a title="154-lda-1" href="./acl-2010-Personalising_Speech-To-Speech_Translation_in_the_EMIME_Project.html">193 acl-2010-Personalising Speech-To-Speech Translation in the EMIME Project</a></p>
<p>Author: Mikko Kurimo ; William Byrne ; John Dines ; Philip N. Garner ; Matthew Gibson ; Yong Guan ; Teemu Hirsimaki ; Reima Karhila ; Simon King ; Hui Liang ; Keiichiro Oura ; Lakshmi Saheer ; Matt Shannon ; Sayaki Shiota ; Jilei Tian</p><p>Abstract: In the EMIME project we have studied unsupervised cross-lingual speaker adaptation. We have employed an HMM statistical framework for both speech recognition and synthesis which provides transformation mechanisms to adapt the synthesized voice in TTS (text-to-speech) using the recognized voice in ASR (automatic speech recognition). An important application for this research is personalised speech-to-speech translation that will use the voice of the speaker in the input language to utter the translated sentences in the output language. In mobile environments this enhances the users’ interaction across language barriers by making the output speech sound more like the original speaker’s way of speaking, even if she or he could not speak the output language.</p><p>same-paper 2 0.77210855 <a title="154-lda-2" href="./acl-2010-Jointly_Optimizing_a_Two-Step_Conditional_Random_Field_Model_for_Machine_Transliteration_and_Its_Fast_Decoding_Algorithm.html">154 acl-2010-Jointly Optimizing a Two-Step Conditional Random Field Model for Machine Transliteration and Its Fast Decoding Algorithm</a></p>
<p>Author: Dong Yang ; Paul Dixon ; Sadaoki Furui</p><p>Abstract: This paper presents a joint optimization method of a two-step conditional random field (CRF) model for machine transliteration and a fast decoding algorithm for the proposed method. Our method lies in the category of direct orthographical mapping (DOM) between two languages without using any intermediate phonemic mapping. In the two-step CRF model, the first CRF segments an input word into chunks and the second one converts each chunk into one unit in the target language. In this paper, we propose a method to jointly optimize the two-step CRFs and also a fast algorithm to realize it. Our experiments show that the proposed method outper- forms the well-known joint source channel model (JSCM) and our proposed fast algorithm decreases the decoding time significantly. Furthermore, combination of the proposed method and the JSCM gives further improvement, which outperforms state-of-the-art results in terms of top-1 accuracy.</p><p>3 0.73130238 <a title="154-lda-3" href="./acl-2010-Automatically_Generating_Annotator_Rationales_to_Improve_Sentiment_Classification.html">42 acl-2010-Automatically Generating Annotator Rationales to Improve Sentiment Classification</a></p>
<p>Author: Ainur Yessenalina ; Yejin Choi ; Claire Cardie</p><p>Abstract: One ofthe central challenges in sentimentbased text categorization is that not every portion of a document is equally informative for inferring the overall sentiment of the document. Previous research has shown that enriching the sentiment labels with human annotators’ “rationales” can produce substantial improvements in categorization performance (Zaidan et al., 2007). We explore methods to automatically generate annotator rationales for document-level sentiment classification. Rather unexpectedly, we find the automatically generated rationales just as helpful as human rationales.</p><p>4 0.67424095 <a title="154-lda-4" href="./acl-2010-Entity-Based_Local_Coherence_Modelling_Using_Topological_Fields.html">101 acl-2010-Entity-Based Local Coherence Modelling Using Topological Fields</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: One goal of natural language generation is to produce coherent text that presents information in a logical order. In this paper, we show that topological fields, which model high-level clausal structure, are an important component of local coherence in German. First, we show in a sentence ordering experiment that topological field information improves the entity grid model of Barzilay and Lapata (2008) more than grammatical role and simple clausal order information do, particularly when manual annotations of this information are not available. Then, we incorporate the model enhanced with topological fields into a natural language generation system that generates constituent orders for German text, and show that the added coherence component improves performance slightly, though not statistically significantly.</p><p>5 0.5682258 <a title="154-lda-5" href="./acl-2010-Fine-Grained_Tree-to-String_Translation_Rule_Extraction.html">118 acl-2010-Fine-Grained Tree-to-String Translation Rule Extraction</a></p>
<p>Author: Xianchao Wu ; Takuya Matsuzaki ; Jun'ichi Tsujii</p><p>Abstract: Tree-to-string translation rules are widely used in linguistically syntax-based statistical machine translation systems. In this paper, we propose to use deep syntactic information for obtaining fine-grained translation rules. A head-driven phrase structure grammar (HPSG) parser is used to obtain the deep syntactic information, which includes a fine-grained description of the syntactic property and a semantic representation of a sentence. We extract fine-grained rules from aligned HPSG tree/forest-string pairs and use them in our tree-to-string and string-to-tree systems. Extensive experiments on largescale bidirectional Japanese-English trans- lations testified the effectiveness of our approach.</p><p>6 0.56803906 <a title="154-lda-6" href="./acl-2010-Towards_Open-Domain_Semantic_Role_Labeling.html">238 acl-2010-Towards Open-Domain Semantic Role Labeling</a></p>
<p>7 0.56417871 <a title="154-lda-7" href="./acl-2010-Structural_Semantic_Relatedness%3A_A_Knowledge-Based_Method_to_Named_Entity_Disambiguation.html">218 acl-2010-Structural Semantic Relatedness: A Knowledge-Based Method to Named Entity Disambiguation</a></p>
<p>8 0.56310964 <a title="154-lda-8" href="./acl-2010-Extraction_and_Approximation_of_Numerical_Attributes_from_the_Web.html">113 acl-2010-Extraction and Approximation of Numerical Attributes from the Web</a></p>
<p>9 0.56281209 <a title="154-lda-9" href="./acl-2010-Bootstrapping_Semantic_Analyzers_from_Non-Contradictory_Texts.html">55 acl-2010-Bootstrapping Semantic Analyzers from Non-Contradictory Texts</a></p>
<p>10 0.56245506 <a title="154-lda-10" href="./acl-2010-Learning_Common_Grammar_from_Multilingual_Corpus.html">162 acl-2010-Learning Common Grammar from Multilingual Corpus</a></p>
<p>11 0.56170553 <a title="154-lda-11" href="./acl-2010-A_Semi-Supervised_Key_Phrase_Extraction_Approach%3A_Learning_from_Title_Phrases_through_a_Document_Semantic_Network.html">15 acl-2010-A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network</a></p>
<p>12 0.56148559 <a title="154-lda-12" href="./acl-2010-Open-Domain_Semantic_Role_Labeling_by_Modeling_Word_Spans.html">184 acl-2010-Open-Domain Semantic Role Labeling by Modeling Word Spans</a></p>
<p>13 0.55961323 <a title="154-lda-13" href="./acl-2010-Minimized_Models_and_Grammar-Informed_Initialization_for_Supertagging_with_Highly_Ambiguous_Lexicons.html">172 acl-2010-Minimized Models and Grammar-Informed Initialization for Supertagging with Highly Ambiguous Lexicons</a></p>
<p>14 0.55895191 <a title="154-lda-14" href="./acl-2010-Better_Filtration_and_Augmentation_for_Hierarchical_Phrase-Based_Translation_Rules.html">48 acl-2010-Better Filtration and Augmentation for Hierarchical Phrase-Based Translation Rules</a></p>
<p>15 0.55861169 <a title="154-lda-15" href="./acl-2010-Wikipedia_as_Sense_Inventory_to_Improve_Diversity_in_Web_Search_Results.html">261 acl-2010-Wikipedia as Sense Inventory to Improve Diversity in Web Search Results</a></p>
<p>16 0.55816853 <a title="154-lda-16" href="./acl-2010-Improved_Unsupervised_POS_Induction_through_Prototype_Discovery.html">144 acl-2010-Improved Unsupervised POS Induction through Prototype Discovery</a></p>
<p>17 0.55811453 <a title="154-lda-17" href="./acl-2010-Learning_to_Translate_with_Source_and_Target_Syntax.html">169 acl-2010-Learning to Translate with Source and Target Syntax</a></p>
<p>18 0.55785316 <a title="154-lda-18" href="./acl-2010-Letter-Phoneme_Alignment%3A_An_Exploration.html">170 acl-2010-Letter-Phoneme Alignment: An Exploration</a></p>
<p>19 0.55775124 <a title="154-lda-19" href="./acl-2010-Improving_the_Use_of_Pseudo-Words_for_Evaluating_Selectional_Preferences.html">148 acl-2010-Improving the Use of Pseudo-Words for Evaluating Selectional Preferences</a></p>
<p>20 0.557639 <a title="154-lda-20" href="./acl-2010-Experiments_in_Graph-Based_Semi-Supervised_Learning_Methods_for_Class-Instance_Acquisition.html">109 acl-2010-Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
