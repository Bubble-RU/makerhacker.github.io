<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-374" href="#">acl2013-374</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</h1>
<br/><p>Source: <a title="acl-2013-374-pdf" href="http://aclweb.org/anthology//P/P13/P13-2057.pdf">pdf</a></p><p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>Reference: <a title="acl-2013-374-reference" href="../acl2013_reference/acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 i r  Abstract Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. [sent-3, score-0.439]
</p><p>2 Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. [sent-4, score-0.648]
</p><p>3 We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. [sent-5, score-1.786]
</p><p>4 We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. [sent-6, score-1.098]
</p><p>5 1  Introduction  Parallel corpora as an important component of a statistical machine translation system are unfortunately unavailable for all pairs of languages, particularly in low resource languages and also producing it consumes time and cost. [sent-9, score-0.35]
</p><p>6 So, new ideas have been developed about how to make a MT system which has lower dependency on parallel data like using comparable corpora for improving performance of a MT system with small parallel corpora or making a MT system without parallel corpora. [sent-10, score-0.774]
</p><p>7 These segments might be in the form of words, phrases or sentences. [sent-12, score-0.222]
</p><p>8 So, this extracted information can be added to the parallel corpus or might be used for adaption of the language model or translation model. [sent-13, score-0.36]
</p><p>9 Another idea for solving the scarce resource problem is to use a high resource language as a pivot to bridge between source and target languages. [sent-16, score-1.017]
</p><p>10 In this paper we use the bridge  technique to make a source-target system and we will prune the phrase table of this system. [sent-17, score-0.579]
</p><p>11 In Section 2, the related works of the bridge approach are considered, in Section 3 the proposed approach will be explained and it will be shown how to prune the phrase table using context vectors, and experiments on GermanEnglish-Farsi systems will be presented in Section 4. [sent-18, score-0.656]
</p><p>12 The simplest way is to build two MT systems in two sides: one system is source-pivot and the other is pivottarget system, then in the translation stage the output of the first system is given to the second system as an input and the output of the second system is the final result. [sent-20, score-0.489]
</p><p>13 The disadvantage of this method is its time consuming translation process, since until the first system’s output is not ready; the second system cannot start the translation process. [sent-21, score-0.388]
</p><p>14 This method is called cascading of two translation systems. [sent-22, score-0.264]
</p><p>15 In the other approach the target side of the training corpus of the source-pivot system is given to the pivot-target system as its input. [sent-23, score-0.292]
</p><p>16 The output of the pivot-target system is parallel with the source side of the training corpus of the source-pivot system. [sent-24, score-0.341]
</p><p>17 A source-to-target system can be built by using this noisy parallel corpus which in it each source sentence is directly translated to a target sentence. [sent-25, score-0.475]
</p><p>18 Another way is combining the phrase tables of the source-pivot and pivot-target systems to directly make a source-target phrase table. [sent-27, score-0.648]
</p><p>19 This combination is done if the pivot phrase is 318  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-28, score-0.913]
</p><p>20 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 318–32 , identical in both phrase tables. [sent-30, score-0.294]
</p><p>21 Since one phrase has many translations in the other language, a large phrase table will be produced. [sent-31, score-0.831]
</p><p>22 This method is called combination of phrase tables approach. [sent-32, score-0.422]
</p><p>23 Since in the bridge language approach two translation systems are used to make a final translation system, the errors of these two translation systems will affect the final output. [sent-33, score-0.57]
</p><p>24 Therefore in order to decrease the propagation of these errors, a language should be chosen as pivot which its structure is similar to the source and target languages. [sent-34, score-0.831]
</p><p>25 But even by choosing a good language as pivot there are some other errors that should be handled or decreased such as the errors of ploysemous words and etc. [sent-35, score-0.83]
</p><p>26 For making a MT system using pivot language several ideas have been proposed. [sent-36, score-0.704]
</p><p>27 Bertoldi (2008) proposed his method in bridging at translation time and bridging at training time by using the cascading method and the combination of phrase tables. [sent-38, score-0.796]
</p><p>28 Bakhshaei (2010) used the combination of phrase tables of source-pivot and pivot-target systems and produced a phrase table for the source-target system. [sent-39, score-0.735]
</p><p>29 Paul (2009) did several experiments to show the effect of pivot language in the final translation system. [sent-40, score-0.725]
</p><p>30 He showed that in some cases if training data is small the pivot should be more similar to the source language, and if training data is large the pivot should be more similar to  the target language. [sent-41, score-1.396]
</p><p>31 In Addition, it is more suitable to use a pivot language that its structure is similar to both of source and target languages. [sent-42, score-0.804]
</p><p>32 So many of the translations produced in the final phrase table might be wrong. [sent-44, score-0.614]
</p><p>33 Therefore for pruning wrong and weak phrases in the phrase table two methods have been used. [sent-45, score-0.804]
</p><p>34 Rapp (1995) suggested his idea about the usage of context vectors in order to find the words that are the translation of each other in comparable corpora. [sent-47, score-0.495]
</p><p>35 In this paper the combination of phrase tables approach is used to make a source-target system. [sent-48, score-0.381]
</p><p>36 But the contribution of our work compared to other works is that here we decrease the size of the produced phrase table and improve the performance of the system. [sent-50, score-0.411]
</p><p>37 Our pruning method is different from the method that Saralegi (201 1) has used. [sent-51, score-0.276]
</p><p>38 He has pruned the phrase table by computing distributional  similarity from comparable corpora or by the structure of source dictionaries. [sent-52, score-0.632]
</p><p>39 Here we use context vectors to determine the concept of phrases and we use the pivot language to compare source and target vectors. [sent-53, score-1.296]
</p><p>40 3  Approach  For the purpose of showing how to create a pruned phrase table, in Section 3. [sent-54, score-0.379]
</p><p>41 2 we will explain how to remove wrong and weak translations in the pruning step. [sent-57, score-0.58]
</p><p>42 In the following we have used these abbreviations: f, e stands for source and target phrases. [sent-59, score-0.212]
</p><p>43 pl, src-pl, pl-trg, src-trg respectively stand for pivot phrase, source-pivot phrase table, pivot-target phrase table and source-target phrase table. [sent-60, score-1.505]
</p><p>44 1 Creating source-to-target system At first, we assume that there is transitive property between three languages in order to make a base system, and then we will show in  different ways that there is not transitive property between three languages. [sent-62, score-0.346]
</p><p>45 ×  319  For each phrase f in src-pl phrase table, all the phrases pl which are translations of f, are considered. [sent-63, score-1.187]
</p><p>46 Then for each of these pls every phrase e from the pl-trg phrase table that are translations of pl, are found. [sent-64, score-0.863]
</p><p>47 Finally f is mapped to all of these es in the new src-trg phrase table. [sent-65, score-0.32]
</p><p>48 p( e e| f) = p( pl | f) p( e e| pl) (1) A simple src-trg phrase table is created by this approach. [sent-67, score-0.57]
</p><p>49 Pl phrases might be ploysemous and produce target phrases that have different meaning in comparison to each other. [sent-68, score-0.545]
</p><p>50 The concept of some of these target phrases are similar to the corresponding source phrase and the concept of others are irrelevant to the source phrase. [sent-69, score-0.948]
</p><p>51 But it cannot ignore these translations if they have high probability. [sent-71, score-0.238]
</p><p>52 Since the probability of translations is  calculated using equation (1), therefore wrong translations have high probability in three cases: first when p(pl|f) is high, second when p(e|pl) is high and third when p(pl|f) and p(e|pl) are high. [sent-72, score-0.539]
</p><p>53 In the first case pl might be a good translation for f and refers to concept c, but pl and e refer to concept ? [sent-73, score-0.797]
</p><p>54 The third case is also similar to the first case, but pl is a good translation for both f and e. [sent-76, score-0.378]
</p><p>55 The pruning method that is explained in Section 3. [sent-77, score-0.287]
</p><p>56 2, tries to find these translations and delete them from the src-trg phrase table. [sent-78, score-0.506]
</p><p>57 2 Pruning method To determine the concept of each phrase (p) in language L at first a vector (V) with length N is created. [sent-80, score-0.477]
</p><p>58 Each element of V is set to zero and N is the number of unique phrases in language L. [sent-81, score-0.246]
</p><p>59 This way of calculating context  vectors is similar to Rapp (1999), but here the window length of phrase co-occurrence is considered a sentence. [sent-87, score-0.646]
</p><p>60 For each source (target) phrase its context vector should be calculated within the source (target) corpus as shown in figure 1. [sent-93, score-0.709]
</p><p>61 The number of unique phrases in the source (target) language is equal to the number of unique source (target) phrases in the src-trg phrase table that are created in the last Section. [sent-94, score-0.915]
</p><p>62 So, the length of source context vectors is m and the length of target context vectors is n. [sent-95, score-0.866]
</p><p>63 In addition to this, source vectors and target vectors are in two different languages, so they are not comparable. [sent-97, score-0.574]
</p><p>64 One method to translate source context vectors to target context vectors is using an additional source-target dictionary. [sent-98, score-0.852]
</p><p>65 But instead here, source  and target context vectors are translated to pivot context vectors. [sent-99, score-1.228]
</p><p>66 In other words if source context vectors have length m and target context vectors have length n, they are converted to pivot context vectors with length z. [sent-100, score-1.785]
</p><p>67 The variable z is the number of unique pivot phrases in src-pl or pl-trg phrase tables. [sent-101, score-1.072]
</p><p>68 ) to the pivot context vector, we use a fixed size vector ? [sent-107, score-0.767]
</p><p>69 ) are the unique phrases extracted from src-pl or pl-trg phrase tables. [sent-116, score-0.48]
</p><p>70 These phrases are the output of k-best translations of ? [sent-131, score-0.354]
</p><p>71 Using K-best translations as middle phrases is for reducing the effect of translation errors that cause wrong concepts. [sent-169, score-0.611]
</p><p>72 This work is done for each target context vector. [sent-170, score-0.209]
</p><p>73 Source and target context vectors will be mapped to identical length vectors and are also in the same language (pivot language). [sent-171, score-0.637]
</p><p>74 Now source and target context vectors are comparable, so with a simple similarity metric their similarity can be calculated. [sent-172, score-0.547]
</p><p>75 The similarity between each source context vector and each 320  target context vector that are translations of the source phrase in src-trg, are calculated. [sent-174, score-1.141]
</p><p>76 For each source phrase, the N-most similar target phrases are kept as translations of the source phrase. [sent-175, score-0.705]
</p><p>77 Therefore this pruning method deletes irrelevant translations form the src-trg phrase table. [sent-177, score-0.838]
</p><p>78 The size of the phrase table is decreased very much and the system performance is increased. [sent-178, score-0.496]
</p><p>79 Reduction of the phrase table size is  considerable while its performance is increased. [sent-179, score-0.355]
</p><p>80 We use English language as a bridge between German and Farsi languages because English language is a high resource language and parallel corpora of German-English and English-Farsi are available. [sent-181, score-0.369]
</p><p>81 With the German-English parallel corpus and an additional German-English dictionary with 118480 entries we have made a German-English (De-En) system and with English-Farsi parallel corpus we have made a German-Farsi (En-Fa)  system. [sent-189, score-0.383]
</p><p>82 Now, we create a translation system by combining phrase tables of De-En and En-Fa systems. [sent-191, score-0.568]
</p><p>83 The size of this phrase table is very large because of ploysemous and some weak translations. [sent-194, score-0.518]
</p><p>84 Then, we apply the pruning method that we 1Available under the LGPL from http://sourceforge. [sent-200, score-0.235]
</p><p>85 With this method only the phrases are kept that their context vectors are similar to each other. [sent-205, score-0.5]
</p><p>86 For each source phrase the  35-most similar target translations are kept. [sent-206, score-0.718]
</p><p>87 The number of phrases in the phrase table is decreased dramatically while the performance of the system is increased by 2. [sent-207, score-0.642]
</p><p>88 We observe that the pruning method has gain better results compared to the system trained on the parallel corpus. [sent-211, score-0.44]
</p><p>89 This is maybe because of some translations that are made in the parallel system and do not have enough training data and their probabilities are not precise. [sent-212, score-0.465]
</p><p>90 But when we use context vectors to measure the contextual similarity of phrases and their translations, the impact of these training samples are decreased. [sent-213, score-0.453]
</p><p>91 In Table 3, two wrong phrase pairs that pruning method has removed them are shown. [sent-214, score-0.619]
</p><p>92 961Ue base # 352syo640fs8,9tp,e651hm631ra24, tshes  pruned system and the parallel system. [sent-217, score-0.34]
</p><p>93 We see that the proposed method obtains competitive result with the pseudo parallel method. [sent-221, score-0.333]
</p><p>94 In the first significance test, we set the pruned system as our base system and we compare the result of the pseudo parallel corpus system with it, the significance level is 72%. [sent-227, score-0.824]
</p><p>95 For another significance test we set the combined phrase table system without pruning as our base system and we compare the result of the pruned system with it, the significance level is 100%. [sent-228, score-1.053]
</p><p>96 In the last significance test we set the combined phrase table system without pruning as our base system and we compare the result of the pseudo system with it, the significance level is 99%. [sent-229, score-1.107]
</p><p>97 Therefore, we can conclude the proposed method obtains the best results and its difference with pseudo parallel corpus method is not significant. [sent-230, score-0.401]
</p><p>98 5  Conclusion and future work  With increasing the size of the phrase table, the MT system performance will not necessarily increase. [sent-231, score-0.405]
</p><p>99 Maybe there are wrong translations with high probability which the language model  cannot remove them from the best translations. [sent-232, score-0.333]
</p><p>100 By removing these translation pairs, the produced phrase table will be more consistent, and irrelevant words or phrases are much less. [sent-233, score-0.694]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pivot', 0.592), ('phrase', 0.294), ('pl', 0.245), ('translations', 0.212), ('pruning', 0.194), ('vectors', 0.181), ('phrases', 0.142), ('pseudo', 0.139), ('bridge', 0.137), ('translation', 0.133), ('parallel', 0.124), ('bakhshaei', 0.11), ('ploysemous', 0.11), ('saralegi', 0.11), ('source', 0.109), ('context', 0.106), ('target', 0.103), ('wrong', 0.09), ('cascading', 0.09), ('pruned', 0.085), ('bridging', 0.085), ('system', 0.081), ('significance', 0.078), ('germanfarsi', 0.073), ('verbmobil', 0.073), ('khadivi', 0.065), ('shahram', 0.065), ('somayeh', 0.065), ('lgpl', 0.065), ('irrelevant', 0.065), ('concept', 0.063), ('mt', 0.063), ('decreased', 0.06), ('tables', 0.06), ('element', 0.06), ('transitive', 0.057), ('farsi', 0.056), ('weak', 0.053), ('explained', 0.052), ('base', 0.05), ('comparable', 0.05), ('maybe', 0.048), ('might', 0.048), ('rapp', 0.045), ('unique', 0.044), ('bertoldi', 0.044), ('method', 0.041), ('length', 0.04), ('reinhard', 0.04), ('vector', 0.039), ('corpora', 0.039), ('resource', 0.038), ('prune', 0.036), ('percent', 0.036), ('property', 0.035), ('increased', 0.034), ('errors', 0.034), ('hirofumi', 0.032), ('deletes', 0.032), ('tehran', 0.032), ('iker', 0.032), ('englishgerman', 0.032), ('vicente', 0.032), ('barbaiani', 0.032), ('pivottarget', 0.032), ('roldano', 0.032), ('pls', 0.032), ('iran', 0.032), ('segments', 0.032), ('translated', 0.031), ('table', 0.031), ('remove', 0.031), ('ideas', 0.031), ('languages', 0.031), ('madalina', 0.03), ('kept', 0.03), ('size', 0.03), ('obtains', 0.029), ('nicola', 0.029), ('marcello', 0.029), ('produced', 0.029), ('alexandra', 0.028), ('telecommunications', 0.028), ('consumes', 0.028), ('adaption', 0.028), ('combination', 0.027), ('federico', 0.027), ('decrease', 0.027), ('ami', 0.027), ('corpus', 0.027), ('mapped', 0.026), ('ignore', 0.026), ('suggested', 0.025), ('recognize', 0.025), ('translate', 0.025), ('calculated', 0.025), ('masao', 0.025), ('utiyama', 0.025), ('considered', 0.025), ('similarity', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="374-tfidf-1" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>2 0.39659992 <a title="374-tfidf-2" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>Author: Ahmed El Kholy ; Nizar Habash ; Gregor Leusch ; Evgeny Matusov ; Hassan Sawaf</p><p>Abstract: An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.</p><p>3 0.24942037 <a title="374-tfidf-3" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>4 0.1992811 <a title="374-tfidf-4" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>Author: Wenduan Xu ; Yue Zhang ; Philip Williams ; Philipp Koehn</p><p>Abstract: We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.</p><p>5 0.18094189 <a title="374-tfidf-5" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>6 0.16781868 <a title="374-tfidf-6" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>7 0.14271282 <a title="374-tfidf-7" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>8 0.13395981 <a title="374-tfidf-8" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>9 0.12713888 <a title="374-tfidf-9" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>10 0.11809402 <a title="374-tfidf-10" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>11 0.11707742 <a title="374-tfidf-11" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>12 0.11205105 <a title="374-tfidf-12" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>13 0.11119638 <a title="374-tfidf-13" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>14 0.11060601 <a title="374-tfidf-14" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>15 0.11017385 <a title="374-tfidf-15" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>16 0.10606834 <a title="374-tfidf-16" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>17 0.10477147 <a title="374-tfidf-17" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>18 0.10059348 <a title="374-tfidf-18" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>19 0.097183317 <a title="374-tfidf-19" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>20 0.096117876 <a title="374-tfidf-20" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.219), (1, -0.141), (2, 0.234), (3, 0.077), (4, -0.019), (5, -0.009), (6, -0.052), (7, 0.043), (8, 0.053), (9, -0.0), (10, -0.005), (11, 0.054), (12, 0.023), (13, 0.078), (14, 0.022), (15, -0.001), (16, -0.084), (17, -0.127), (18, -0.117), (19, 0.156), (20, -0.005), (21, -0.043), (22, 0.069), (23, 0.108), (24, -0.037), (25, -0.055), (26, -0.019), (27, 0.034), (28, 0.093), (29, 0.029), (30, -0.024), (31, 0.024), (32, 0.043), (33, 0.024), (34, 0.045), (35, 0.085), (36, 0.103), (37, -0.201), (38, 0.024), (39, 0.05), (40, 0.097), (41, -0.059), (42, -0.03), (43, 0.014), (44, 0.006), (45, 0.083), (46, 0.004), (47, 0.043), (48, 0.004), (49, -0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95603424 <a title="374-lsi-1" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>2 0.88590169 <a title="374-lsi-2" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>Author: Ahmed El Kholy ; Nizar Habash ; Gregor Leusch ; Evgeny Matusov ; Hassan Sawaf</p><p>Abstract: An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.</p><p>3 0.81177247 <a title="374-lsi-3" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>4 0.75673962 <a title="374-lsi-4" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>Author: Kun Wang ; Chengqing Zong ; Keh-Yih Su</p><p>Abstract: Since statistical machine translation (SMT) and translation memory (TM) complement each other in matched and unmatched regions, integrated models are proposed in this paper to incorporate TM information into phrase-based SMT. Unlike previous multi-stage pipeline approaches, which directly merge TM result into the final output, the proposed models refer to the corresponding TM information associated with each phrase at SMT decoding. On a Chinese–English TM database, our experiments show that the proposed integrated Model-III is significantly better than either the SMT or the TM systems when the fuzzy match score is above 0.4. Furthermore, integrated Model-III achieves overall 3.48 BLEU points improvement and 2.62 TER points reduction in comparison with the pure SMT system. Be- . sides, the proposed models also outperform previous approaches significantly.</p><p>5 0.75658423 <a title="374-lsi-5" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>Author: Conghui Zhu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.</p><p>6 0.7325713 <a title="374-lsi-6" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>7 0.71601689 <a title="374-lsi-7" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>8 0.6844914 <a title="374-lsi-8" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>9 0.68058372 <a title="374-lsi-9" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>10 0.6403631 <a title="374-lsi-10" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>11 0.63290477 <a title="374-lsi-11" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>12 0.62468523 <a title="374-lsi-12" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>13 0.60495943 <a title="374-lsi-13" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>14 0.59868091 <a title="374-lsi-14" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>15 0.59229505 <a title="374-lsi-15" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>16 0.58836234 <a title="374-lsi-16" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>17 0.57321137 <a title="374-lsi-17" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>18 0.56864679 <a title="374-lsi-18" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>19 0.56222069 <a title="374-lsi-19" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>20 0.56001639 <a title="374-lsi-20" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.045), (6, 0.027), (11, 0.051), (24, 0.029), (26, 0.04), (31, 0.232), (35, 0.071), (42, 0.093), (48, 0.062), (70, 0.03), (88, 0.049), (90, 0.049), (95, 0.14)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90023869 <a title="374-lda-1" href="./acl-2013-An_Open_Source_Toolkit_for_Quantitative_Historical_Linguistics.html">48 acl-2013-An Open Source Toolkit for Quantitative Historical Linguistics</a></p>
<p>Author: Johann-Mattis List ; Steven Moran</p><p>Abstract: Given the increasing interest and development of computational and quantitative methods in historical linguistics, it is important that scholars have a basis for documenting, testing, evaluating, and sharing complex workflows. We present a novel open-source toolkit for quantitative tasks in historical linguistics that offers these features. This toolkit also serves as an interface between existing software packages and frequently used data formats, and it provides implementations of new and existing algorithms within a homogeneous framework. We illustrate the toolkit’s functionality with an exemplary workflow that starts with raw language data and ends with automatically calculated phonetic alignments, cognates and borrowings. We then illustrate evaluation metrics on gold standard datasets that are provided with the toolkit.</p><p>2 0.84111923 <a title="374-lda-2" href="./acl-2013-Semantic_Parsing_with_Combinatory_Categorial_Grammars.html">313 acl-2013-Semantic Parsing with Combinatory Categorial Grammars</a></p>
<p>Author: Yoav Artzi ; Nicholas FitzGerald ; Luke Zettlemoyer</p><p>Abstract: unkown-abstract</p><p>3 0.82945979 <a title="374-lda-3" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>Author: Francis Bond ; Ryan Foster</p><p>Abstract: We create an open multilingual wordnet with large wordnets for over 26 languages and smaller ones for 57 languages. It is made by combining wordnets with open licences, data from Wiktionary and the Unicode Common Locale Data Repository. Overall there are over 2 million senses for over 100 thousand concepts, linking over 1.4 million words in hundreds of languages.</p><p>4 0.80491352 <a title="374-lda-4" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<p>Author: Omri Abend ; Ari Rappoport</p><p>Abstract: Syntactic structures, by their nature, reflect first and foremost the formal constructions used for expressing meanings. This renders them sensitive to formal variation both within and across languages, and limits their value to semantic applications. We present UCCA, a novel multi-layered framework for semantic representation that aims to accommodate the semantic distinctions expressed through linguistic utterances. We demonstrate UCCA’s portability across domains and languages, and its relative insensitivity to meaning-preserving syntactic variation. We also show that UCCA can be effectively and quickly learned by annotators with no linguistic background, and describe the compilation of a UCCAannotated corpus.</p><p>same-paper 5 0.77424139 <a title="374-lda-5" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>6 0.75469291 <a title="374-lda-6" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>7 0.67601448 <a title="374-lda-7" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>8 0.669909 <a title="374-lda-8" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>9 0.6692422 <a title="374-lda-9" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>10 0.66923821 <a title="374-lda-10" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>11 0.6655277 <a title="374-lda-11" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>12 0.66303176 <a title="374-lda-12" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>13 0.6618917 <a title="374-lda-13" href="./acl-2013-Online_Relative_Margin_Maximization_for_Statistical_Machine_Translation.html">264 acl-2013-Online Relative Margin Maximization for Statistical Machine Translation</a></p>
<p>14 0.659657 <a title="374-lda-14" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>15 0.65927047 <a title="374-lda-15" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>16 0.65926129 <a title="374-lda-16" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.6591109 <a title="374-lda-17" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>18 0.65667731 <a title="374-lda-18" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>19 0.65490681 <a title="374-lda-19" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>20 0.65485162 <a title="374-lda-20" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
