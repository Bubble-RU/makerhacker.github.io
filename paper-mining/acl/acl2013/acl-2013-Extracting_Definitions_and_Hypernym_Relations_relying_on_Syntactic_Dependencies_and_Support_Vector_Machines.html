<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-152" href="#">acl2013-152</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</h1>
<br/><p>Source: <a title="acl-2013-152-pdf" href="http://aclweb.org/anthology//P/P13/P13-2095.pdf">pdf</a></p><p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>Reference: <a title="acl-2013-152-reference" href="../acl2013_reference/acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines  Guido Boella University of Turin Department of Computer Science boe l a @ di . [sent-1, score-0.086]
</p><p>2 it l Abstract In this paper we present a technique to reveal definitions and hypernym relations from text. [sent-3, score-0.729]
</p><p>3 Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. [sent-4, score-0.354]
</p><p>4 The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. [sent-5, score-0.222]
</p><p>5 Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. [sent-6, score-0.164]
</p><p>6 The results on  an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques. [sent-7, score-0.569]
</p><p>7 Ontology Learning is a task that permits to automatically (or semiautomatically) extract structured knowledge from plain text. [sent-12, score-0.108]
</p><p>8 Manual construction of ontologies usually requires strong efforts from domain experts, and it thus needs an automatization in such sense. [sent-13, score-0.108]
</p><p>9 Luigi Di Caro University of Turin Department of Computer Science dicaro @ di . [sent-17, score-0.051]
</p><p>10 it In this paper, we focus on the extraction of hypernym relations. [sent-19, score-0.588]
</p><p>11 The first step of such task relies on the identification of what (Navigli and Velardi, 2010) called definitional sentences, i. [sent-20, score-0.485]
</p><p>12 , 2007), construction of glossaries (Klavans and Muresan, 2001), extraction of taxonomic and non-taxonomic relations (Navigli, 2009; Snow et al. [sent-24, score-0.17]
</p><p>13 Hypernym relation extraction involves two aspects: linguistic knowlege, and model learning. [sent-28, score-0.14]
</p><p>14 First, patterns have limited expressivity; then, linguistic knowledge inside patterns is learned from small corpora, so it is likely to have low coverage. [sent-30, score-0.278]
</p><p>15 Instead, we use a syntactic parser for the first aspect (with all its native and domain-independent knowledge on language expressivity), and a state-of-the-art approach to learn models with the use of Support Vector Machine classifiers. [sent-32, score-0.083]
</p><p>16 Our assumption is that syntax is less dependent than learned patterns from the length and the complexity of textual expressions. [sent-33, score-0.139]
</p><p>17 In some way, patterns grasp syntactic relationships, but they actually do not use them as input knowledge. [sent-34, score-0.222]
</p><p>18 2  Related Work  In this section we present the current state of the art concerning the automatic extraction of definitions and hypernym relations from plain text. [sent-35, score-0.819]
</p><p>19 We will use the term definitional sentence referring to  the more general meaning given by (Navigli and Velardi, 2010): A sentence that provides a for532  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-36, score-0.513]
</p><p>20 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 532–537, mal explanation for the term of interest, and more specifically as a sentence containing at least one hypernym relation. [sent-38, score-0.537]
</p><p>21 So far, most of the proposed techniques rely on lexico-syntactic patterns, either manually or semiautomatically produced (Hovy et al. [sent-39, score-0.055]
</p><p>22 Such patterns are sequences of words like “is a” or “refers to”, rather than more complex sequences including part-of-speech tags. [sent-41, score-0.139]
</p><p>23 In the work of (Westerhout, 2009), after a manual identification oftypes ofdefinitions and related patterns contained in a corpus, he successively applied Machine Learning techniques on syntactic and location features to improve the results. [sent-42, score-0.276]
</p><p>24 , 2009), where the authors applied genetic algorithms to the extraction ofEnglish def-  initions containing the keyword “is”. [sent-44, score-0.079]
</p><p>25 In detail, they assign weights to a set of features for the classification of definitional sentences, reaching a precision of 62% and a recall of 52%. [sent-45, score-0.638]
</p><p>26 , probabilistic lexicosemantic patterns that are able to generalize over rigid patterns enabling partial matching by calculating a generative degree-of-match probability between a test instance and the set of training instances. [sent-49, score-0.308]
</p><p>27 Similarly to our approach, (Fahmi and Bouma, 2006) used three different Machine Learning algorithms to distinguish actual definitions from other sentences also relying on syntactic features, reaching high accuracy levels. [sent-50, score-0.324]
</p><p>28 The work of (Klavans and Muresan, 2001) relies on a rule-based system that makes use of “cue phrases” and structural indicators that frequently introduce definitions, reaching 87% of precision and 75% of recall on a small and domain-specific corpus. [sent-51, score-0.108]
</p><p>29 The seminal work of (Hearst, 1992) represents the main approach based on fixed patterns like “NPx is a/an NPy” and “NPx such as NPy”, that usually imply < x IS-A y >. [sent-53, score-0.139]
</p><p>30 The main drawback of such technique is that it does not face the high variability of how a relation can be expressed in natural language. [sent-54, score-0.115]
</p><p>31 (Berland and Charniak, 1999) proposed similar lexico-syntactic patterns to extract part-whole relationships. [sent-56, score-0.182]
</p><p>32 (Del Gaudio and Branco, 2007) proposed a rulebased approach to the extraction of hypernyms that, however, leads to very low accuracy values in terms of Precision. [sent-57, score-0.179]
</p><p>33 (Ponzetto and Strube, 2007) proposed a technique to extract hypernym relations from Wikipedia by means of methods based on the connectivity of the network and classical lexicosyntactic patterns. [sent-58, score-0.664]
</p><p>34 , 2009) extended  their work by combining extracted Wikipedia entries with new terms contained in additional web documents, using a distributional similarity-based approach. [sent-60, score-0.091]
</p><p>35 Finally, pure statistical approaches present techniques for the extraction of hierarchies of terms based on words frequency as well as cooccurrence values, relying on clustering procedures (Candan et al. [sent-61, score-0.195]
</p><p>36 Despite this, they are defined by (Biemann, 2005) as prototype-based ontologies rather than formal terminological ontologies, and they usually suffer from the problem of data sparsity in case of small corpora. [sent-65, score-0.075]
</p><p>37 3  Approach  In this section we present our approach to identify hypernym relations within plain text. [sent-66, score-0.632]
</p><p>38 Given a relation rel(x, y) contained in a sentence, the task becomes to find 1)  a possible x, and 2) a possible y. [sent-68, score-0.115]
</p><p>39 By seeing the problem as two different classification problems, there is no need to create abstract patterns between the target terms. [sent-70, score-0.184]
</p><p>40 In addition to this, the general problem of identifying definitional sentences can be seen as to find at least one x and one y in a sentence. [sent-71, score-0.536]
</p><p>41 1 Local Syntactic Information Dependency parsing is a procedure that extracts syntactic dependencies among the terms contained in a sentence. [sent-73, score-0.241]
</p><p>42 The idea is that, given a hypernym relation, hyponyms and hypernyms may be 533  characterized by specific sets of syntactic contexts. [sent-74, score-0.716]
</p><p>43 According to this assumption, the task can be seen as a classification problem where each term in a sentence has to be classified as hyponym, hypernym, or neither of the two. [sent-75, score-0.141]
</p><p>44 For each noun, we construct a textual representation containing its syntactic dependencies (i. [sent-76, score-0.15]
</p><p>45 In particular, for each syntactic dependency dep(a, b) (or dep(b, a)) of a target noun a, we create an abstract token3 dep(or where becomes the generic string “noun” in case it is another noun; otherwise it is equal to b. [sent-79, score-0.181]
</p><p>46 This way, the nouns are transformed into abstract strings; on the contrary, no abstraction is done for verbs. [sent-80, score-0.075]
</p><p>47 After the PartOf-Speech annotation, the parser will extract a series of syntactic dependencies like “det(Albedo, The)”, “nsubj(extent, Albedo)”, “prepof(Albedo, object)”, where det identifies a determiner, nsubj represents a noun phrase which is the syntactic subject of a clause, and so forth4. [sent-82, score-0.448]
</p><p>48 Then, such dependencies will be transformed in abstract terms like “det-target-the”, “nsubj-noun-target”, and “prepof-target-noun”. [sent-83, score-0.14]
</p><p>49 These triples represent the feature space on which the Support Vector Machine classifiers will construct the models. [sent-84, score-0.055]
</p><p>50 2 Learning phase Our model assumes a transformation of the local syntactic information into labelled numeric vectors. [sent-86, score-0.148]
</p><p>51 More in detail, given a sentence S annotated with the terms linked by the hypernym relation, the system produces as many input instances as the number of nouns contained in S. [sent-87, score-0.669]
</p><p>52 For each noun n in S, the method produces two instances Sxn and Syn, associated to the label positive or negative depending on their presence in the target relation (i. [sent-88, score-0.189]
</p><p>53 If a noun is not involved in a hypernym relation, both the two instances will have the label negative. [sent-91, score-0.637]
</p><p>54 , one for each relation argument, namely the x-set and the y-set. [sent-94, score-0.061]
</p><p>55 All the instances of both the datasets are then transformed into numeric vectors according 3We make use of the term “abstract” to indicate that some words are replaced with more general entity identifiers. [sent-95, score-0.129]
</p><p>56 4A complete overview of the Stanford dependencies is available at http://nlp. [sent-96, score-0.067]
</p><p>57 , 1975), and are finally fed into a Support Vector Machine classifier5 (Cortes and Vapnik, 1995). [sent-101, score-0.081]
</p><p>58 These models are binary classifiers that, given the local syntactic information of a noun, estimate if it can be respectively an x or a y in a hypernym relation. [sent-103, score-0.677]
</p><p>59 Once the x-model and the y-model are built, we can both classify definitional sentences and extract hypernym relations. [sent-104, score-1.126]
</p><p>60 The whole set of instances of all the sentences are fed into two Support Vector Machine classifiers, one for each target label (i. [sent-106, score-0.162]
</p><p>61 At this point, it is possible to classify each term as possible x or y by querying the respective classifiers with its local syntactic information. [sent-109, score-0.234]
</p><p>62 4  Setting of the Tasks  In this section we present how our proposed technique is able to classify definitional sentences un-  raveling hypernym relations. [sent-110, score-1.137]
</p><p>63 1 Classification of definitional sentences As already mentioned in previous sections, we label as definitional all the sentences that contain at least one noun n classified as x, and one noun m classified as y (where n m). [sent-112, score-1.404]
</p><p>64 we extract all the syntactic dependencies of the nouns (dependency parsing), 3. [sent-117, score-0.232]
</p><p>65 we check if there exist at least one noun classified as x and one noun classified as y: in this case, we classify the sentences as definitional. [sent-121, score-0.421]
</p><p>66 2  Extraction of hypernym relations  Our method for extracting hypernym relations makes use of both the x-model and the y-model as for the the task of classifying definitional sentences. [sent-123, score-1.701]
</p><p>67 534  in the same sentence, they are directly connected and the relation is extracted. [sent-126, score-0.061]
</p><p>68 Now, considering our target relation hyp(x, y), in case the sentence contains more than one noun that is classified as x (or y), there are two possible scenarios: 1. [sent-128, score-0.227]
</p><p>69 4% of the sentences are classified with multiple x and y. [sent-137, score-0.119]
</p><p>70 Finally, since our method is able to extract single nouns that can be involved in a hypernym relation, we included modifiers preceded by preposition “of”, while the other modifiers are removed. [sent-138, score-0.651]
</p><p>71 For example, considering the sentence “An Archipelago is a chain of islands”, the whole chunk “chain of islands” is extracted from the single triggered noun chain. [sent-139, score-0.135]
</p><p>72 5  Evaluation  In this section we present the evaluation of our approach, that we carried out on an annotated dataset of definitional sentences (Navigli et al. [sent-140, score-0.536]
</p><p>73 The corpus contains 4,619 sentences extracted from Wikipedia, and only 1,908 are annotated as definitional. [sent-142, score-0.051]
</p><p>74 On a first instance, we test the  classifiers on the extraction of hyponyms (x) and hypernyms (y) from the definitional sentences, independently. [sent-143, score-0.743]
</p><p>75 Finally, we evaluate the ability of our technique when extracting whole hypernym relations. [sent-145, score-0.608]
</p><p>76 With the used dataset, the constructed training sets for the two classifiers (x-set and y-set) resulted to have approximately 1,500 features. [sent-146, score-0.055]
</p><p>77 7Notice that this is different from the case in which a single noun is labeled as both x and y. [sent-148, score-0.098]
</p><p>78 684 % % Table 1: Evaluation results for the classification of definitional sentences, in terms of Precision (P), Recall (R), F-Measure (F), and Accuracy (Acc),  using 10-folds cross validation. [sent-155, score-0.567]
</p><p>79 1566%% Table 2: Evaluation results for the hypernym relation extraction, in terms of Precision (P), Recall (R), and F-Measure (F). [sent-164, score-0.607]
</p><p>80 These results are obtained using 10-folds cross validation (* Recall has been inherited from the definition classification task, since no indication has been reported in their contribution). [sent-166, score-0.091]
</p><p>81 1 Results In this section we present the evaluation of our technique on both the tasks of classifying definitional sentences and extracting hypernym relations. [sent-168, score-1.181]
</p><p>82 Notice that our approach is susceptible from the errors given by the POS-tagger8 and the  syntactic parser9 . [sent-169, score-0.083]
</p><p>83 The goal of our evaluation is twofold: first, we evaluate the ability of classifying definitional sentences; finally, we measure the accuracy of the hypernym relation extraction. [sent-172, score-1.092]
</p><p>84 A definitional sentences is extracted only if at least one x and one y are found in the same sentence. [sent-173, score-0.536]
</p><p>85 shtml 535  tern matching approach proposed by (Navigli and Velardi, 2010), our Recall is higher, leading to an higher overall F-Measure. [sent-182, score-0.058]
</p><p>86 Table 2 shows the results of the extraction of the whole hypernym relations. [sent-183, score-0.588]
</p><p>87 In particular, even in this task, our system outperforms the pattern matching algorithm proposed by (Navigli and Velardi, 2010) in terms of Precision and Recall. [sent-185, score-0.067]
</p><p>88 6  Conclusion and Future Work  We presented an approach to reveal definitions and extract underlying hypernym relations from plain text, making use of local syntactic information fed into a Support Vector Machine classifier. [sent-186, score-0.977]
</p><p>89 Our first results on this method highlight the validity of the approach by significantly improving current state-of-the-art techniques in the classification of definitional sentences as well as in the extraction of hypernym relations from text. [sent-188, score-1.26]
</p><p>90 In future works, we aim at using larger syntactic con-  texts. [sent-189, score-0.083]
</p><p>91 In fact, currently, the detection does not surpass the sentence level, while taxonomical information can be even contained in different sentences or paragraphs. [sent-190, score-0.105]
</p><p>92 We also aim at evaluating our approach on the construction of entire taxonomies starting from domain-specific text corpora, as in (Navigli et al. [sent-191, score-0.071]
</p><p>93 Finally, the desired result of the task of extracting hypernym relations from text (as for any semantic relationships in general) depends on the domain and the specific later application. [sent-194, score-0.612]
</p><p>94 Both results can be valid, and a further discrimination can only be done if a specific application or use of this knowlege is taken into consideration. [sent-197, score-0.063]
</p><p>95 Creating tag hierarchies for effective navigation in social media. [sent-225, score-0.084]
</p><p>96 Automatic extraction of definitions in portuguese: A rule-based approach. [sent-250, score-0.187]
</p><p>97 The ontowordnet project: Extension and axiomatization of conceptual relations in wordnet. [sent-267, score-0.058]
</p><p>98 Extending metadata definitions by automatically extracting and organizing glossary definitions. [sent-294, score-0.204]
</p><p>99 Evaluation of the definder system for fully automatic glossary construction. [sent-302, score-0.051]
</p><p>100 An annotated dataset for extracting definitions and hypernyms from the web. [sent-312, score-0.216]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hypernym', 0.509), ('definitional', 0.485), ('velardi', 0.228), ('navigli', 0.218), ('patterns', 0.139), ('albedo', 0.125), ('definitions', 0.108), ('noun', 0.098), ('candan', 0.094), ('klavans', 0.091), ('syntactic', 0.083), ('fed', 0.081), ('paola', 0.079), ('extraction', 0.079), ('ontologies', 0.075), ('digital', 0.071), ('cui', 0.07), ('dep', 0.069), ('classified', 0.068), ('dependencies', 0.067), ('plain', 0.065), ('hypernyms', 0.063), ('caro', 0.063), ('cataldi', 0.063), ('fahmi', 0.063), ('gaudio', 0.063), ('knowlege', 0.063), ('luigi', 0.063), ('npx', 0.063), ('npy', 0.063), ('turin', 0.063), ('relation', 0.061), ('hyponyms', 0.061), ('relations', 0.058), ('westerhout', 0.055), ('fortuna', 0.055), ('berland', 0.055), ('gangemi', 0.055), ('semiautomatically', 0.055), ('classifiers', 0.055), ('technique', 0.054), ('contained', 0.054), ('roberto', 0.052), ('di', 0.051), ('borg', 0.051), ('glossary', 0.051), ('cortes', 0.051), ('sentences', 0.051), ('government', 0.049), ('reaching', 0.047), ('definition', 0.046), ('islands', 0.046), ('muresan', 0.046), ('instructions', 0.045), ('extracting', 0.045), ('classification', 0.045), ('expressivity', 0.044), ('hierarchies', 0.044), ('extract', 0.043), ('ontology', 0.043), ('del', 0.041), ('navigation', 0.04), ('computationallinguistics', 0.04), ('nouns', 0.039), ('taxonomies', 0.038), ('det', 0.038), ('salton', 0.038), ('classify', 0.038), ('classifying', 0.037), ('terms', 0.037), ('chunk', 0.037), ('transformed', 0.036), ('ponzetto', 0.036), ('nsubj', 0.036), ('relying', 0.035), ('numeric', 0.035), ('validity', 0.033), ('construction', 0.033), ('weka', 0.032), ('precision', 0.032), ('vector', 0.03), ('snow', 0.03), ('matching', 0.03), ('modifiers', 0.03), ('local', 0.03), ('instances', 0.03), ('taxonomy', 0.029), ('recall', 0.029), ('term', 0.028), ('yamada', 0.028), ('rosner', 0.028), ('mara', 0.028), ('hyp', 0.028), ('aesr', 0.028), ('aldo', 0.028), ('branco', 0.028), ('coopis', 0.028), ('doa', 0.028), ('odbase', 0.028), ('tern', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="152-tfidf-1" href="./acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines.html">152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</a></p>
<p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>2 0.52061415 <a title="152-tfidf-2" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.</p><p>3 0.17049129 <a title="152-tfidf-3" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>Author: Tiziano Flati ; Roberto Navigli</p><p>Abstract: We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best f∗it) tahned ∗ argument. Taon idco this, we extract failtl thhee ∗ occurrences ion Wikipedia ewxthraiccht match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach.</p><p>4 0.14979433 <a title="152-tfidf-4" href="./acl-2013-GlossBoot%3A_Bootstrapping_Multilingual_Domain_Glossaries_from_the_Web.html">170 acl-2013-GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web</a></p>
<p>Author: Flavio De Benedictis ; Stefano Faralli ; Roberto Navigli</p><p>Abstract: We present GlossBoot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages. For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns. Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages.</p><p>5 0.082454145 <a title="152-tfidf-5" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>Author: Aurelie Herbelot ; Mohan Ganesalingam</p><p>Abstract: Some words are more contentful than others: for instance, make is intuitively more general than produce and fifteen is more ‘precise’ than a group. In this paper, we propose to measure the ‘semantic content’ of lexical items, as modelled by distributional representations. We investigate the hypothesis that semantic content can be computed using the KullbackLeibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL diver- gence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions.</p><p>6 0.08056891 <a title="152-tfidf-6" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>7 0.068373419 <a title="152-tfidf-7" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>8 0.067852169 <a title="152-tfidf-8" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>9 0.064653702 <a title="152-tfidf-9" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>10 0.060006633 <a title="152-tfidf-10" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>11 0.059277005 <a title="152-tfidf-11" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>12 0.059070423 <a title="152-tfidf-12" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>13 0.058697578 <a title="152-tfidf-13" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>14 0.057610244 <a title="152-tfidf-14" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>15 0.052736536 <a title="152-tfidf-15" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>16 0.052049309 <a title="152-tfidf-16" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>17 0.051472113 <a title="152-tfidf-17" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>18 0.051236559 <a title="152-tfidf-18" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>19 0.050239142 <a title="152-tfidf-19" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>20 0.049037911 <a title="152-tfidf-20" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, 0.066), (2, -0.022), (3, -0.096), (4, -0.042), (5, 0.012), (6, -0.079), (7, -0.005), (8, 0.051), (9, -0.021), (10, -0.029), (11, 0.025), (12, -0.047), (13, 0.018), (14, 0.026), (15, -0.033), (16, 0.031), (17, -0.005), (18, -0.039), (19, -0.014), (20, -0.038), (21, 0.059), (22, -0.054), (23, 0.087), (24, 0.225), (25, 0.092), (26, 0.0), (27, -0.157), (28, 0.04), (29, 0.237), (30, -0.122), (31, 0.32), (32, 0.094), (33, 0.062), (34, 0.312), (35, 0.022), (36, 0.008), (37, 0.235), (38, 0.007), (39, -0.012), (40, 0.049), (41, 0.025), (42, -0.009), (43, -0.04), (44, -0.034), (45, -0.064), (46, -0.083), (47, 0.134), (48, 0.026), (49, 0.0)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93533468 <a title="152-lsi-1" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.</p><p>same-paper 2 0.91793066 <a title="152-lsi-2" href="./acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines.html">152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</a></p>
<p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>3 0.81521189 <a title="152-lsi-3" href="./acl-2013-GlossBoot%3A_Bootstrapping_Multilingual_Domain_Glossaries_from_the_Web.html">170 acl-2013-GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web</a></p>
<p>Author: Flavio De Benedictis ; Stefano Faralli ; Roberto Navigli</p><p>Abstract: We present GlossBoot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages. For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns. Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages.</p><p>4 0.52432978 <a title="152-lsi-4" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>Author: Tiziano Flati ; Roberto Navigli</p><p>Abstract: We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best f∗it) tahned ∗ argument. Taon idco this, we extract failtl thhee ∗ occurrences ion Wikipedia ewxthraiccht match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach.</p><p>5 0.47199622 <a title="152-lsi-5" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The English ’s possessive construction occurs frequently in text and can encode several different semantic relations; however, it has received limited attention from the computational linguistics community. This paper describes the creation of a semantic relation inventory covering the use of ’s, an inter-annotator agreement study to calculate how well humans can agree on the relations, a large collection of possessives annotated according to the relations, and an accurate automatic annotation system for labeling new examples. Our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of, and both our automatic classification system, which achieves 87.4% accuracy in our classification experiment, and our annotation data are publicly available.</p><p>6 0.39831221 <a title="152-lsi-6" href="./acl-2013-IndoNet%3A_A_Multilingual_Lexical_Knowledge_Network_for_Indian_Languages.html">198 acl-2013-IndoNet: A Multilingual Lexical Knowledge Network for Indian Languages</a></p>
<p>7 0.39448676 <a title="152-lsi-7" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>8 0.39438891 <a title="152-lsi-8" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>9 0.37620142 <a title="152-lsi-9" href="./acl-2013-Transfer_Learning_Based_Cross-lingual_Knowledge_Extraction_for_Wikipedia.html">356 acl-2013-Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia</a></p>
<p>10 0.37314156 <a title="152-lsi-10" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>11 0.37312302 <a title="152-lsi-11" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>12 0.35489798 <a title="152-lsi-12" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>13 0.34663519 <a title="152-lsi-13" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>14 0.34615532 <a title="152-lsi-14" href="./acl-2013-Understanding_Tables_in_Context_Using_Standard_NLP_Toolkits.html">365 acl-2013-Understanding Tables in Context Using Standard NLP Toolkits</a></p>
<p>15 0.34610564 <a title="152-lsi-15" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>16 0.34300655 <a title="152-lsi-16" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>17 0.33830076 <a title="152-lsi-17" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>18 0.33537203 <a title="152-lsi-18" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<p>19 0.31136107 <a title="152-lsi-19" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>20 0.30754963 <a title="152-lsi-20" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.043), (6, 0.025), (11, 0.077), (24, 0.06), (26, 0.053), (35, 0.079), (42, 0.055), (48, 0.061), (64, 0.322), (70, 0.031), (88, 0.025), (90, 0.02), (95, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88701367 <a title="152-lda-1" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>Author: Eneko Agirre ; Nikolaos Aletras ; Paul Clough ; Samuel Fernando ; Paula Goodale ; Mark Hall ; Aitor Soroa ; Mark Stevenson</p><p>Abstract: This paper describes a system for navigating large collections of information about cultural heritage which is applied to Europeana, the European Library. Europeana contains over 20 million artefacts with meta-data in a wide range of European languages. The system currently provides access to Europeana content with meta-data in English and Spanish. The paper describes how Natural Language Processing is used to enrich and organise this meta-data to assist navigation through Europeana and shows how this information is used within the system.</p><p>2 0.78016919 <a title="152-lda-2" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>same-paper 3 0.75907439 <a title="152-lda-3" href="./acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines.html">152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</a></p>
<p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>4 0.7515108 <a title="152-lda-4" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Dan Roth</p><p>Abstract: Semantic parsing is a domain-dependent process by nature, as its output is defined over a set of domain symbols. Motivated by the observation that interpretation can be decomposed into domain-dependent and independent components, we suggest a novel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains.</p><p>5 0.74752957 <a title="152-lda-5" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>Author: Anoop Kunchukuttan ; Rajen Chatterjee ; Shourya Roy ; Abhijit Mishra ; Pushpak Bhattacharyya</p><p>Abstract: Large amount of parallel corpora is required for building Statistical Machine Translation (SMT) systems. We describe the TransDoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation ofconstituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence translations from phrase translations. Trans- Doop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work.</p><p>6 0.70532328 <a title="152-lda-6" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>7 0.60543132 <a title="152-lda-7" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>8 0.56803596 <a title="152-lda-8" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>9 0.53830731 <a title="152-lda-9" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>10 0.53481477 <a title="152-lda-10" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>11 0.5231573 <a title="152-lda-11" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>12 0.52135301 <a title="152-lda-12" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>13 0.51855505 <a title="152-lda-13" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>14 0.50940174 <a title="152-lda-14" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>15 0.5024246 <a title="152-lda-15" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>16 0.49574494 <a title="152-lda-16" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>17 0.49333042 <a title="152-lda-17" href="./acl-2013-PAL%3A_A_Chatterbot_System_for_Answering_Domain-specific_Questions.html">266 acl-2013-PAL: A Chatterbot System for Answering Domain-specific Questions</a></p>
<p>18 0.48685005 <a title="152-lda-18" href="./acl-2013-Predicting_and_Eliciting_Addressee%27s_Emotion_in_Online_Dialogue.html">282 acl-2013-Predicting and Eliciting Addressee's Emotion in Online Dialogue</a></p>
<p>19 0.48393425 <a title="152-lda-19" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>20 0.48046756 <a title="152-lda-20" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
