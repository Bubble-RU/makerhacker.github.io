<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-192" href="#">acl2013-192</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</h1>
<br/><p>Source: <a title="acl-2013-192-pdf" href="http://aclweb.org/anthology//P/P13/P13-1085.pdf">pdf</a></p><p>Author: Roi Reichart ; Anna Korhonen</p><p>Abstract: Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs. We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1.</p><p>Reference: <a title="acl-2013-192-reference" href="../acl2013_reference/acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk Abstract Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. [sent-3, score-0.425]
</p><p>2 We show how to utilize Determinantal Point Processes (DPPs), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. [sent-5, score-0.148]
</p><p>3 Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of  DPPs to cluster together verbs with similar SCFs and SPs. [sent-6, score-0.667]
</p><p>4 We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1. [sent-7, score-0.156]
</p><p>5 1 Introduction Verb classes (VCs), subcategorization frames (SCFs) and selectional preferences (SPs) capture different aspects of predicate-argument structure. [sent-8, score-0.366]
</p><p>6 These three of types of information have proved useful for Natural Language Processing (NLP) 1The source code of the clustering algorithms and evaluation is submitted with this paper and will be made publicly available upon acceptance of the paper. [sent-10, score-0.138]
</p><p>7 The task of SCF induction involves identifying the arguments of a verb lemma and generalizing about the frames (i. [sent-24, score-0.378]
</p><p>8 For example, in (1), the verb ”show” takes the frame SUBJ-DOBJCCOMP (subject, direct object, and clausal complement). [sent-27, score-0.213]
</p><p>9 (1) [A number of SCF acquisition papers]SUBJ [show]VERB [their readers]DOBJ [which features are most valuable for the acquisition process]CCOMP. [sent-28, score-0.224]
</p><p>10 In sentence (2), for example, the verb ”show” takes the frame SUBJ-DOBJ. [sent-30, score-0.213]
</p><p>11 Finally, VC induction involves clustering together verbs with similar meaning, reflected in similar SCFs and SPs. [sent-35, score-0.274]
</p><p>12 O´ S ´eaghdha (2010) presented a “dual-topic” model for SPs that induces also verb clusters. [sent-41, score-0.192]
</p><p>13 (2012) presented a joint model for inducing simple syntactic frames and VCs. [sent-44, score-0.155]
</p><p>14 Our framework is based on Determinantal Point Processes (DPPs, (Kulesza, 2012; Kulesza and Taskar, 2012c)), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets. [sent-50, score-0.148]
</p><p>15 We first show how individual-task DPP kernel matrices can be naturally combined to construct a joint kernel. [sent-51, score-0.207]
</p><p>16 We then introduce a novel clustering algorithm based on iterative DPP sampling which can (contrary to other probabilistic frameworks such as Markov random fields) be performed both accurately and efficiently. [sent-53, score-0.22]
</p><p>17 We also contribute by evaluating the value of the clusters induced by our model for the acquisition of the three information types. [sent-55, score-0.268]
</p><p>18 Our evaluation against a well-known VC gold standard shows that  our clustering model outperforms the state-of-theart verb clustering algorithm of Sun and Korhonen (2009), in our setup where no manually created SCF or SP data is available. [sent-56, score-0.499]
</p><p>19 2  Previous Work  SCF acquisition Most current works induce SCFs from the output of an unlexicalized parser (i. [sent-58, score-0.139]
</p><p>20 (2012) presented a Bayesian network model for syntactic frame induction that identifies SPs on argument types. [sent-68, score-0.141]
</p><p>21 (2012) do not capture sets of arguments for verbs so are far simpler than traditional SCFs. [sent-70, score-0.145]
</p><p>22 Current approaches to SCF acquisition suffer from lack of semantic information which is needed to guide the purely syntax-driven acquisition process. [sent-71, score-0.224]
</p><p>23 SP acquisition Considerable research has been conducted on SP acquisition, with a variety of unsupervised models proposed for this task that use no hand-crafted information during training. [sent-74, score-0.145]
</p><p>24 Previous unsupervised VC acquisition approaches clustered a variety of linguistic features using different (e. [sent-86, score-0.145]
</p><p>25 The linguistic features included SCFs and SPs, but these were induced separately and then feeded as features to the clustering algorithm. [sent-93, score-0.192]
</p><p>26 Our framework combines together SCF-motivated and SP-motivated kernel matrices , and uses the joint kernel to induce verb clusters which are likely to be highly relevant for both tasks. [sent-94, score-0.618]
</p><p>27 Importantly, no manual or automatic system for SCF or SP acquisition has been utilized when constructing the kernel matrices, we only consider features extracted from the output of an unlexicalized parser. [sent-95, score-0.236]
</p><p>28 (2012) presented a joint unsupervised model of SCF and SP acquisition based on non-negative tensor factorization. [sent-104, score-0.225]
</p><p>29 Our idea is to utilize DPPs for verb clustering that informs both SCF and SP acquisition. [sent-110, score-0.296]
</p><p>30 Individual task DPP kernels represent (i) the quality of a data point (verb) as its average featurebased similarity with the other points in the data set and (ii) the divergence between a pair of points as the inverse similarity between them. [sent-114, score-0.166]
</p><p>31 The high quality and diverse subsets sampled from the DPP model are considered good cluster seeds as they are likely to be relatively uniformly spread and to provide good coverage of the data set. [sent-116, score-0.26]
</p><p>32 The algorithm induces an  hierarchical clustering, which is particularly suitable for semantic tasks, where a set of clusters that share a parent consists of pure members (i. [sent-117, score-0.164]
</p><p>33 most of the points in each cluster member belong to the same gold cluster) and together provide good coverage of the verb space. [sent-119, score-0.369]
</p><p>34 1), we discuss the construction of the joint DPP kernel, given a kernel for each individual task, In section 3. [sent-121, score-0.162]
</p><p>35 2 Constructing a Joint Kernel Matrix DPPs are particularly suitable for joint modeling as they come with various simple and intuitive ways to combine individual model kernel matrices into a joint kernel. [sent-145, score-0.245]
</p><p>36 This stems from the fact that every positive-semidefinite matrix forms a legal DPP kernel (equation 1). [sent-146, score-0.174]
</p><p>37 2 In practice we construct the joint kernel in the following way. [sent-149, score-0.162]
</p><p>38 d PL2 nd etwfinoe Dd by L, P2 = A2TA2, we construct the joint kernel L12:  L12 = L1L2L2L1 Where C A1TA1AT2A2. [sent-152, score-0.162]
</p><p>39 Features Our algorithm builds two DPP kernel matrices (the GenKernelMatrix function), in which the rows and columns correspond to the verbs in the data set, such that the (i, j)-th entry corresponds to verbs number iand j. [sent-156, score-0.375]
</p><p>40 Following equations 2 and 3 one matrix is built for SCF and one for SP, and they are then combined into the  2Note that we do not take a product of the individual models but only of their kernel matrices. [sent-157, score-0.174]
</p><p>41 865  joint kernel matrix (the GenJointMat function) following equation 5. [sent-161, score-0.212]
</p><p>42 Each kernel matrix requires a proper feature representation φ and quality score  q. [sent-162, score-0.21]
</p><p>43 In both kernels we represent a verb by the counts of the grammatical relations (GRs) it participates in. [sent-163, score-0.192]
</p><p>44 In the SCF kernel a GR is represented by the GR type and the POS tags of the verb and its arguments. [sent-164, score-0.282]
</p><p>45 In the SP kernels the GRs are represented by the POS tags of the verb and its arguments as well as by the argument head word. [sent-165, score-0.287]
</p><p>46 The quality score qi of the i-th verb is the average similarity of this verb with the other verbs in the dataset. [sent-167, score-0.471]
</p><p>47 Cluster set construction In its while loop, the algorithm iteratively generates fixed-size cluster sets such that each data point belongs to exactly one cluster in one set. [sent-168, score-0.36]
</p><p>48 These cluster sets form the leaf level of the tree in Figure (1). [sent-169, score-0.146]
</p><p>49 It does so by extracting the T highest probability K-point samples from a set of M subsets, each of which sampled from the joint DPP model, and clustering them by the cluster procedure. [sent-170, score-0.356]
</p><p>50 The cluster procedure first seeds a K-cluster set with the highest probability sample. [sent-172, score-0.146]
</p><p>51 Then, it gradually extends the clusters by iteratively mapping the samples, in decreasing order of probability, to the existing clusters (the m1Mapping function). [sent-173, score-0.204]
</p><p>52 Mapping is done by attaching every point in the mapped subset to its closet cluster, where the distance between a point and the cluster is the maximum over the distances between the point  and each of the points in the cluster. [sent-174, score-0.294]
</p><p>53 Based on the DPP properties, the higher the probability of a sampled subset, the more likely it is to consist of distinct points that provide a good coverage of the verb set. [sent-176, score-0.186]
</p><p>54 By iteratively extending the clusters with high probability subsets, we thus expect each cluster set to consist of clusters that demonstrate these properties. [sent-177, score-0.35]
</p><p>55 The iterative DPP-samples clustering (the While loop) generates the lowest level of the tree, by dividing the data set into cluster sets, each of which consists of K clusters. [sent-185, score-0.284]
</p><p>56 Each point in the data set belongs to exactly one cluster in exactly one set. [sent-186, score-0.186]
</p><p>57 The agglomerative clustering then iteratively combines cluster sets such that in each iteration two sets are combined to one set with K clusters. [sent-187, score-0.388]
</p><p>58 Agglomerative Clustering Finally, the AgglomerativeClustering function builds a hierarchy of cluster sets, by iteratively combining cluster set pairs. [sent-188, score-0.292]
</p><p>59 In each iteration it computes the similarity between any such pair, defined to be the lowest similarity between their cluster members, which is in turn defined to be the lowest cosine similarity between their point members. [sent-189, score-0.231]
</p><p>60 The most similar cluster sets are combined such that each of the clusters in one set is mapped to its most similar cluster in the other set. [sent-190, score-0.394]
</p><p>61 In this step the algorithm generates data partitions at different granularity levels from finest (from the iterative  sampling step) to the coarsest set (generated by the last agglomerative clustering iteration and consisting of exactly K clusters). [sent-191, score-0.324]
</p><p>62 4  Evaluation  Data sets and gold standards We evaluated the SCFs and verb clusters on gold standard datasets. [sent-193, score-0.334]
</p><p>63 3 SCF types per verb) obtained by annotating 250 corpus occurrences per verb with the SCF types of (de Cruys et al. [sent-197, score-0.158]
</p><p>64 Where a verb has more than one VerbNet class, we assign it to the one supported by the highest number of member verbs. [sent-200, score-0.158]
</p><p>65 R915 R5B  Table 2: Performance of the Corpus Statistics SP baseline (non-filtered, NF) as well as for three filtering methods: frequency based (filter-baseline, B), DPP-cluster based (DPP) and AC cluster based (AC). [sent-238, score-0.19]
</p><p>66 ficient representation of each class, we collected from VerbNet the verbs for which at least one of the possible classes is represented in the 183 verbs set by at least one and at most seven verbs. [sent-243, score-0.178]
</p><p>67 Since 176 out of the 183 initial verbs are represented in this corpus, our final gold standard consists of 34 classes containing 277 verbs, ofwhich 176 have SCF gold standard and has been evaluated for this task. [sent-247, score-0.163]
</p><p>68 Clustering Evaluation We first evaluate the quality of the clusters induced by our algorithm (DPP-cluster) compared to the gold standard VCs (table 1). [sent-249, score-0.257]
</p><p>69 We also compare to the state-of-the-art spectral clustering method of Sun and Korhonen (2009) where our 4Importantly, the kernel matrix L used in the agglomerative clustering process is also used by AC. [sent-251, score-0.539]
</p><p>70 kernel matrix is used for the distance between data points (SC) 5. [sent-252, score-0.202]
</p><p>71 We evaluated the unified cluster set induced in each iteration of our algorithm and of the AC baseline and induced the same number of clusters as in each iteration of our algorithm using the SC baseline. [sent-253, score-0.552]
</p><p>72 Since the number of clusters in each iteration  is not an argument for our algorithm or for the AC baseline, the number of clusters slightly differ between the two. [sent-254, score-0.316]
</p><p>73 We do this by gathering the GR combinations for each of the verbs in our gold standard, assuming they are frames and gathering their frequencies. [sent-275, score-0.243]
</p><p>74 To remedy this imbalance, we apply a cluster based filtering method on top of the maximum-  recall frequency filter. [sent-286, score-0.226]
</p><p>75 This filter excludes a candidate frame from a verb’s lexicon only if it meets the frequency filter criterion and appears in no more than N other members of the cluster of the verb in question. [sent-287, score-0.391]
</p><p>76 The filter utilizes the clustering produced by the seventh to last iteration of DPPcluster that contains seven clusters with approximately 30 members each. [sent-288, score-0.285]
</p><p>77 Table 3 clearly demonstrates that cluster based filtering (DPP-cluster and AC) is the only method that provides a good balance between the recall and the precision of the SCF lexicon. [sent-291, score-0.258]
</p><p>78 Moreover, the lexicon induced by this method includes a substantially higher number of frames per verb compared to the other filtering methods. [sent-292, score-0.405]
</p><p>79 This clearly demonstrates that the clustering serves to provide SCF acquisition with semantic information needed for improved performance. [sent-294, score-0.25]
</p><p>80 All methods except from cluster based filtering (DPP-cluster and AC) induce lexicons with strong recall/precision imbalance. [sent-324, score-0.217]
</p><p>81 Cluster based filtering keeps a larger number of frames in the lexicon compared to the frequency thresholding baseline, while keeping similar F-score levels. [sent-325, score-0.193]
</p><p>82 We therefore propose to measure both aspects of the SP task by computing both the recall and the precision between the list of possible arguments a verb can take according to the model and the corresponding test corpus list 9. [sent-330, score-0.282]
</p><p>83 We evaluate the value of our clustering for SP acquisition in the particularly challenging scenario of domain adaptation. [sent-331, score-0.25]
</p><p>84 Using the verb clusters we create a filtered version of the BNC argument lexicon which includes in the noun argument list of  a verb only those nouns that appear in the BNC as arguments of that verb and of one of its cluster members. [sent-337, score-0.888]
</p><p>85 Table10 02− presents the results for verbs with up to 200, 600 and 1000 noun arguments in the training data. [sent-345, score-0.145]
</p><p>86 In all cases, the relative error reduction of the DPP cluster filter is substantially higher than that  of the frequency baseline. [sent-346, score-0.174]
</p><p>87 Note that for this task the baseline AC clusters are of low quality which is reflects by an error reduction ratio of up to 0. [sent-347, score-0.166]
</p><p>88 5  Conclusions and Future Work  In this paper we have presented the first unified framework for the induction of verb clusters, subcategorization frames and selectional preferences from corpus data. [sent-349, score-0.621]
</p><p>89 Our key idea is to cluster together verbs with similar SCFs and SPs and to use the resulting clusters for SCF and SP induction. [sent-350, score-0.337]
</p><p>90 To implement our idea we presented a novel method which involves constructing a product DPP model for SCFs and SPs and introduced a new algorithm that utilizes the efficient DPP sampling algorithms to cluster together verbs with similar SCFs and SPs. [sent-351, score-0.317]
</p><p>91 The induced clusters performed well in evaluation against a VerbNet -based gold standard and proved useful in improving the quality of SCFs and SPs over strong baselines. [sent-352, score-0.229]
</p><p>92 Not only the acquisition of different types information (syntactic and semantic) can support and inform each other, but also a unified framework can be useful for NLP tasks and applications which require rich information about predicate-argument structure. [sent-355, score-0.162]
</p><p>93 IRASubcat, a highly customizable, language independent tool for the acquisition of verbal subcategorization information from corpus. [sent-363, score-0.24]
</p><p>94 Inferring semantic roles using subcategorization frames and maximum entropy model. [sent-382, score-0.245]
</p><p>95 Unsupervised acquisition of verb subcategorization frames from shallow-parsed corpora. [sent-505, score-0.515]
</p><p>96 Which are the best features for automatic verb classification. [sent-513, score-0.158]
</p><p>97 A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora. [sent-550, score-0.357]
</p><p>98 Combining EM training and the MDL principle for an automatic verb classification incorporating selectional preferences. [sent-576, score-0.239]
</p><p>99 Experiments on the automatic induction of german semantic verb classes. [sent-580, score-0.205]
</p><p>100 Unsupervised and constrained dirichlet process mixture models for verb clustering. [sent-612, score-0.158]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('scf', 0.442), ('dpp', 0.344), ('scfs', 0.289), ('sps', 0.229), ('sp', 0.179), ('korhonen', 0.172), ('verb', 0.158), ('kulesza', 0.152), ('cluster', 0.146), ('vcs', 0.14), ('cruys', 0.14), ('clustering', 0.138), ('subcategorization', 0.128), ('determinantal', 0.127), ('kernel', 0.124), ('frames', 0.117), ('acquisition', 0.112), ('dpps', 0.11), ('clusters', 0.102), ('lippincott', 0.093), ('verbs', 0.089), ('anna', 0.081), ('selectional', 0.081), ('subsets', 0.078), ('schulte', 0.072), ('taskar', 0.071), ('verbnet', 0.071), ('bnc', 0.069), ('walde', 0.063), ('topsample', 0.062), ('topsamples', 0.062), ('agglomerative', 0.059), ('eaghdha', 0.057), ('arguments', 0.056), ('donovan', 0.055), ('frame', 0.055), ('induced', 0.054), ('sampling', 0.054), ('vc', 0.052), ('sun', 0.051), ('matrix', 0.05), ('unified', 0.05), ('rooth', 0.048), ('briscoe', 0.048), ('induction', 0.047), ('dppcluster', 0.047), ('genkernelmatrix', 0.047), ('im', 0.046), ('carroll', 0.045), ('matrices', 0.045), ('iteration', 0.045), ('filtering', 0.044), ('tensor', 0.042), ('ac', 0.041), ('preferences', 0.04), ('point', 0.04), ('gr', 0.039), ('argument', 0.039), ('joint', 0.038), ('gold', 0.037), ('levin', 0.036), ('quality', 0.036), ('preiss', 0.036), ('recall', 0.036), ('kernels', 0.034), ('rasp', 0.034), ('elegant', 0.034), ('sy', 0.034), ('induces', 0.034), ('samples', 0.034), ('unsupervised', 0.033), ('diarmuid', 0.033), ('processes', 0.032), ('lexicon', 0.032), ('precision', 0.032), ('basili', 0.031), ('agglomerativeclustering', 0.031), ('altamirano', 0.031), ('chesley', 0.031), ('cholakov', 0.031), ('grs', 0.031), ('ienco', 0.031), ('messiant', 0.031), ('minors', 0.031), ('nant', 0.031), ('swier', 0.031), ('zapirain', 0.031), ('van', 0.031), ('reisinger', 0.03), ('spectral', 0.03), ('qi', 0.03), ('sc', 0.03), ('subj', 0.029), ('predicateargument', 0.029), ('reduction', 0.028), ('de', 0.028), ('algorithm', 0.028), ('points', 0.028), ('joanis', 0.028), ('induce', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="192-tfidf-1" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>Author: Roi Reichart ; Anna Korhonen</p><p>Abstract: Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs. We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1.</p><p>2 0.2743215 <a title="192-tfidf-2" href="./acl-2013-Diathesis_alternation_approximation_for_verb_clustering.html">119 acl-2013-Diathesis alternation approximation for verb clustering</a></p>
<p>Author: Lin Sun ; Diana McCarthy ; Anna Korhonen</p><p>Abstract: Although diathesis alternations have been used as features for manual verb classification, and there is recent work on incorporating such features in computational models of human language acquisition, work on large scale verb classification has yet to examine the potential for using diathesis alternations as input features to the clustering process. This paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategorization frames. Our alternation-based approach is particularly adept at leveraging information from less frequent data.</p><p>3 0.13490492 <a title="192-tfidf-3" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>Author: Zhenhua Tian ; Hengheng Xiang ; Ziqi Liu ; Qinghua Zheng</p><p>Abstract: This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate’s smoothed preferences. Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements.</p><p>4 0.10665427 <a title="192-tfidf-4" href="./acl-2013-Language_Acquisition_and_Probabilistic_Models%3A_keeping_it_simple.html">213 acl-2013-Language Acquisition and Probabilistic Models: keeping it simple</a></p>
<p>Author: Aline Villavicencio ; Marco Idiart ; Robert Berwick ; Igor Malioutov</p><p>Abstract: Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed patterns of under- and overgeneralization in child language acquisition. However, as is well known, HBMs are “ideal”learningsystems, assumingaccess to unlimited computational resources that may not be available to child language learners. Consequently, it remains crucial to carefully assess the use of HBMs along with alternative, possibly simpler, candidate models. This paper presents such an evaluation for a language acquisi- tion domain where explicit HBMs have been proposed: the acquisition of English dative constructions. In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs.</p><p>5 0.10416804 <a title="192-tfidf-5" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>Author: Jesse Dunietz ; Lori Levin ; Jaime Carbonell</p><p>Abstract: Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks, as are annotated corpora such as treebanks. Often, the resources are used as-is, without question or examination. This practice risks missing significant performance gains and even entire techniques. This paper addresses the importance of resource quality through the lens of a challenging NLP task: detecting selectional preference violations. We present DAVID, a simple, lexical resource-based preference violation detector. With asis lexical resources, DAVID achieves an F1-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the F1-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements.</p><p>6 0.10275497 <a title="192-tfidf-6" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>7 0.094040476 <a title="192-tfidf-7" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>8 0.092192978 <a title="192-tfidf-8" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>9 0.089678138 <a title="192-tfidf-9" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>10 0.089199089 <a title="192-tfidf-10" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>11 0.089105032 <a title="192-tfidf-11" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>12 0.085179791 <a title="192-tfidf-12" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>13 0.082831427 <a title="192-tfidf-13" href="./acl-2013-Using_subcategorization_knowledge_to_improve_case_prediction_for_translation_to_German.html">378 acl-2013-Using subcategorization knowledge to improve case prediction for translation to German</a></p>
<p>14 0.081333451 <a title="192-tfidf-14" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>15 0.080709733 <a title="192-tfidf-15" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>16 0.078122579 <a title="192-tfidf-16" href="./acl-2013-A_Visual_Analytics_System_for_Cluster_Exploration.html">29 acl-2013-A Visual Analytics System for Cluster Exploration</a></p>
<p>17 0.076070175 <a title="192-tfidf-17" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>18 0.073416084 <a title="192-tfidf-18" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>19 0.070563905 <a title="192-tfidf-19" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>20 0.070391744 <a title="192-tfidf-20" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, 0.034), (2, -0.002), (3, -0.103), (4, -0.066), (5, -0.062), (6, -0.055), (7, 0.079), (8, -0.013), (9, -0.016), (10, 0.009), (11, -0.028), (12, 0.004), (13, 0.024), (14, -0.068), (15, -0.031), (16, -0.009), (17, 0.025), (18, 0.143), (19, 0.057), (20, 0.116), (21, 0.014), (22, 0.058), (23, -0.076), (24, 0.148), (25, -0.063), (26, -0.04), (27, -0.007), (28, 0.056), (29, 0.126), (30, 0.026), (31, 0.02), (32, -0.057), (33, -0.1), (34, -0.166), (35, 0.035), (36, -0.177), (37, -0.14), (38, -0.074), (39, 0.173), (40, 0.016), (41, -0.008), (42, -0.027), (43, 0.046), (44, -0.066), (45, -0.069), (46, 0.009), (47, 0.006), (48, 0.068), (49, -0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93552476 <a title="192-lsi-1" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>Author: Roi Reichart ; Anna Korhonen</p><p>Abstract: Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs. We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1.</p><p>2 0.91008437 <a title="192-lsi-2" href="./acl-2013-Diathesis_alternation_approximation_for_verb_clustering.html">119 acl-2013-Diathesis alternation approximation for verb clustering</a></p>
<p>Author: Lin Sun ; Diana McCarthy ; Anna Korhonen</p><p>Abstract: Although diathesis alternations have been used as features for manual verb classification, and there is recent work on incorporating such features in computational models of human language acquisition, work on large scale verb classification has yet to examine the potential for using diathesis alternations as input features to the clustering process. This paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategorization frames. Our alternation-based approach is particularly adept at leveraging information from less frequent data.</p><p>3 0.76237714 <a title="192-lsi-3" href="./acl-2013-Language_Acquisition_and_Probabilistic_Models%3A_keeping_it_simple.html">213 acl-2013-Language Acquisition and Probabilistic Models: keeping it simple</a></p>
<p>Author: Aline Villavicencio ; Marco Idiart ; Robert Berwick ; Igor Malioutov</p><p>Abstract: Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed patterns of under- and overgeneralization in child language acquisition. However, as is well known, HBMs are “ideal”learningsystems, assumingaccess to unlimited computational resources that may not be available to child language learners. Consequently, it remains crucial to carefully assess the use of HBMs along with alternative, possibly simpler, candidate models. This paper presents such an evaluation for a language acquisi- tion domain where explicit HBMs have been proposed: the acquisition of English dative constructions. In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs.</p><p>4 0.60385168 <a title="192-lsi-4" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>Author: Jesse Dunietz ; Lori Levin ; Jaime Carbonell</p><p>Abstract: Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks, as are annotated corpora such as treebanks. Often, the resources are used as-is, without question or examination. This practice risks missing significant performance gains and even entire techniques. This paper addresses the importance of resource quality through the lens of a challenging NLP task: detecting selectional preference violations. We present DAVID, a simple, lexical resource-based preference violation detector. With asis lexical resources, DAVID achieves an F1-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the F1-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements.</p><p>5 0.54710186 <a title="192-lsi-5" href="./acl-2013-Semantic_Frames_to_Predict_Stock_Price_Movement.html">310 acl-2013-Semantic Frames to Predict Stock Price Movement</a></p>
<p>Author: Boyi Xie ; Rebecca J. Passonneau ; Leon Wu ; German G. Creamer</p><p>Abstract: Semantic frames are a rich linguistic resource. There has been much work on semantic frame parsers, but less that applies them to general NLP problems. We address a task to predict change in stock price from financial news. Semantic frames help to generalize from specific sentences to scenarios, and to detect the (positive or negative) roles of specific companies. We introduce a novel tree representation, and use it to train predictive models with tree kernels using support vector machines. Our experiments test multiple text representations on two binary classification tasks, change of price and polarity. Experiments show that features derived from semantic frame parsing have significantly better performance across years on the polarity task.</p><p>6 0.53381175 <a title="192-lsi-6" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>7 0.52029598 <a title="192-lsi-7" href="./acl-2013-A_Visual_Analytics_System_for_Cluster_Exploration.html">29 acl-2013-A Visual Analytics System for Cluster Exploration</a></p>
<p>8 0.51841468 <a title="192-lsi-8" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>9 0.48493579 <a title="192-lsi-9" href="./acl-2013-Identifying_English_and_Hungarian_Light_Verb_Constructions%3A_A_Contrastive_Approach.html">186 acl-2013-Identifying English and Hungarian Light Verb Constructions: A Contrastive Approach</a></p>
<p>10 0.48107931 <a title="192-lsi-10" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>11 0.47370207 <a title="192-lsi-11" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>12 0.4625845 <a title="192-lsi-12" href="./acl-2013-Learning_Latent_Personas_of_Film_Characters.html">220 acl-2013-Learning Latent Personas of Film Characters</a></p>
<p>13 0.44803649 <a title="192-lsi-13" href="./acl-2013-Computerized_Analysis_of_a_Verbal_Fluency_Test.html">89 acl-2013-Computerized Analysis of a Verbal Fluency Test</a></p>
<p>14 0.43932012 <a title="192-lsi-14" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>15 0.43539351 <a title="192-lsi-15" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>16 0.43277675 <a title="192-lsi-16" href="./acl-2013-Psycholinguistically_Motivated_Computational_Models_on_the_Organization_and_Processing_of_Morphologically_Complex_Words.html">286 acl-2013-Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words</a></p>
<p>17 0.42648107 <a title="192-lsi-17" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>18 0.40055171 <a title="192-lsi-18" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>19 0.39948499 <a title="192-lsi-19" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>20 0.39650947 <a title="192-lsi-20" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.055), (6, 0.029), (11, 0.084), (14, 0.018), (15, 0.012), (17, 0.252), (24, 0.039), (26, 0.046), (28, 0.017), (35, 0.068), (42, 0.07), (48, 0.054), (70, 0.056), (88, 0.026), (90, 0.026), (95, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78470671 <a title="192-lda-1" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>Author: Roi Reichart ; Anna Korhonen</p><p>Abstract: Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs. We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1.</p><p>2 0.75826001 <a title="192-lda-2" href="./acl-2013-Generalizing_Image_Captions_for_Image-Text_Parallel_Corpus.html">167 acl-2013-Generalizing Image Captions for Image-Text Parallel Corpus</a></p>
<p>Author: Polina Kuznetsova ; Vicente Ordonez ; Alexander Berg ; Tamara Berg ; Yejin Choi</p><p>Abstract: The ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision. However, the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text. We address this challenge with contributions in two folds: first, we introduce the new task of image caption generalization, formulated as visually-guided sentence compression, and present an efficient algorithm based on dynamic beam search with dependency-based constraints. Second, we release a new large-scale corpus with 1 million image-caption pairs achieving tighter content alignment between images and text. Evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new imagetext parallel corpus with respect to a concrete application of image caption transfer.</p><p>3 0.59588957 <a title="192-lda-3" href="./acl-2013-Diathesis_alternation_approximation_for_verb_clustering.html">119 acl-2013-Diathesis alternation approximation for verb clustering</a></p>
<p>Author: Lin Sun ; Diana McCarthy ; Anna Korhonen</p><p>Abstract: Although diathesis alternations have been used as features for manual verb classification, and there is recent work on incorporating such features in computational models of human language acquisition, work on large scale verb classification has yet to examine the potential for using diathesis alternations as input features to the clustering process. This paper proposes a method for approximating diathesis alternation behaviour in corpus data and shows, using a state-of-the-art verb clustering system, that features based on alternation approximation outperform those based on independent subcategorization frames. Our alternation-based approach is particularly adept at leveraging information from less frequent data.</p><p>4 0.5878644 <a title="192-lda-4" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>5 0.58581686 <a title="192-lda-5" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>6 0.58361554 <a title="192-lda-6" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>7 0.58169276 <a title="192-lda-7" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>8 0.58045626 <a title="192-lda-8" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>9 0.57977998 <a title="192-lda-9" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>10 0.57822675 <a title="192-lda-10" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>11 0.57669973 <a title="192-lda-11" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>12 0.57520604 <a title="192-lda-12" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>13 0.57499576 <a title="192-lda-13" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>14 0.57419515 <a title="192-lda-14" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>15 0.57332265 <a title="192-lda-15" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>16 0.5732789 <a title="192-lda-16" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>17 0.57314301 <a title="192-lda-17" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>18 0.57313776 <a title="192-lda-18" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>19 0.57148755 <a title="192-lda-19" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>20 0.57059389 <a title="192-lda-20" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
