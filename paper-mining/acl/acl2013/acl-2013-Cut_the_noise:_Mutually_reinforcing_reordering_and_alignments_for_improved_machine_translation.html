<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-101" href="#">acl2013-101</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</h1>
<br/><p>Source: <a title="acl-2013-101-pdf" href="http://aclweb.org/anthology//P/P13/P13-1125.pdf">pdf</a></p><p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>Reference: <a title="acl-2013-101-reference" href="../acl2013_reference/acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation Karthik Visweswariah Mitesh M. [sent-1, score-1.072]
</p><p>2 Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. [sent-8, score-1.266]
</p><p>3 In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. [sent-9, score-1.7]
</p><p>4 The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align-  ments being noisy. [sent-10, score-0.728]
</p><p>5 To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. [sent-11, score-1.906]
</p><p>6 The data generated allows us to train a reordering model that gives an improvement of 1. [sent-14, score-0.663]
</p><p>7 8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5. [sent-15, score-0.849]
</p><p>8 These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. [sent-22, score-0.99]
</p><p>9 These methods  use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. [sent-31, score-0.971]
</p><p>10 , 2011) which uses manual word alignments for learning a reordering model. [sent-33, score-1.126]
</p><p>11 Specifically, we show that we can significantly improve reordering performance by using a large number of sentence pairs for which manual word alignments are not available. [sent-34, score-1.153]
</p><p>12 The motivation for going beyond manual word alignments is clear: the reordering model can have millions of features and estimating weights for the features on thousands of sentences of manual word alignments is 1275 ProceedingsS ooffita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-35, score-1.798]
</p><p>13 The challenge in going beyond manual word alignments and using machine alignments is the noise in the machine alignments which affects the performance of the reordering model (see Section 5). [sent-42, score-2.029]
</p><p>14 Deriving reference reorderings from these wrong alignments would give us an incorrect reordering. [sent-49, score-0.866]
</p><p>15 A reordering model trained on such incorrect reorderings would obviously perform poorly. [sent-50, score-1.052]
</p><p>16 Our task is thus two-fold (i) improve the quality of machine alignments (ii) use these less noisy alignments to derive cleaner training data for a reordering model. [sent-51, score-1.525]
</p><p>17 , reordering and word alignment are related: Having perfect reordering makes the alignment task easier while having perfect alignments in turn makes the task of finding reorderings trivial. [sent-53, score-2.232]
</p><p>18 Motivated by this fact, we introduce models that allow us to connect the source/target reordering and the word alignments and show that these models help in mutually improving the  performance of word alignments and reordering. [sent-54, score-1.474]
</p><p>19 Specifically, we build two models: the first scores reorderings given the source sentence and noisy alignments, the second scores alignments given the noisy source and target reorderings and the source and target sentences themselves. [sent-55, score-1.687]
</p><p>20 The second model helps produce better alignments, while we use the first model to help generate better reference reordering given noisy alignments. [sent-56, score-0.797]
</p><p>21 These improved reference reorderings will then be used to train a reordering model. [sent-57, score-1.121]
</p><p>22 Our experiments show that reordering models trained using these improved machine alignments perform significantly better than models trained only on manual word alignments. [sent-58, score-1.27]
</p><p>23 Section 2 describes the main reordering issues in  Urdu-English translation. [sent-64, score-0.586]
</p><p>24 Section 3 introduces the reordering modeling framework that forms the basis for our work. [sent-65, score-0.586]
</p><p>25 Section 4 describes the two models we use to tie together reordering and alignments and how we use these models to generate training data for training our reordering model. [sent-66, score-1.681]
</p><p>26 3  Reordering model  In this section we briefly describe the reordering model (Visweswariah et al. [sent-78, score-0.687]
</p><p>27 πi denotes the index ofthe word in the source sentence that maps to position iin the candidate reordering, thus reordering with this candidate permutation π we will reorder the sentence w to wπ1 , wπ2, . [sent-83, score-0.854]
</p><p>28 The reordering model we use assigns costs to candidate permutations as: C(π|w) = Xc(πi−1,πi). [sent-87, score-0.725]
</p><p>29 The costs in the reordering model c(m, n) are parameterized by a linear model: c(m, n) =  θTΦ(w,  m, n)  where θ is a learned vector of weights and Φ is a vector of binary feature functions that inspect the words and POS tags of the source sentence at and around positions m and n. [sent-92, score-0.808]
</p><p>30 (201 1) used high quality manual word alignments to derive the desired reorderings π∗ as follows. [sent-98, score-0.99]
</p><p>31 In the equation above, πˆ = arg minπ C(π|w) is the best reordering based on the currentC parameter value θi and L is a loss function. [sent-107, score-0.622]
</p><p>32 This allows us  to train the reordering model with roughly 150K sentences in about two hours. [sent-111, score-0.716]
</p><p>33 4  Generating reference reordering from parallel sentences  The main aim of our work is to improve the reordering model by using parallel sentences for which manual word alignments are not available. [sent-121, score-1.956]
</p><p>34 In other words, we want to generate relatively clean reference reorderings from parallel sentences and use them for training a reordering model. [sent-122, score-1.174]
</p><p>35 A straightforward approach for this is to use a supervised aligner to align the words in the sentences and then derive the reference reordering as we do for manual word alignments. [sent-123, score-0.951]
</p><p>36 However, as we will see in the experimental results, the quality of a reordering model trained from automatic alignments is very sensitive to the quality of alignments. [sent-124, score-1.09]
</p><p>37 This motivated us to explore if we can further improve our aligner and the method for generating reference reorderings given alignments. [sent-125, score-0.572]
</p><p>38 We improve upon the above mentioned ba-  sic approach by coupling the tasks of reordering and word alignment. [sent-126, score-0.616]
</p><p>39 We do this by building a reordering model (C(πs |ws, wt , a)) that scores reorderings πs given the source sentence target sentence wt and machine alignments a. [sent-127, score-1.836]
</p><p>40 Complementing this model, we build an alignment model (P(a|ws, wt ,πs, πt)) that scores alignments a given the source and target sentences and their predicted reorderings according to source and target reordering models. [sent-128, score-1.918]
</p><p>41 The model (C(πs |ws, wt ,a)) helps to produce better reference reorderings for training our final reordering model given fixed machine alignments and the alignment model (P(a|ws, wt , πs, πt)) helps improve theent tm maocdheilne (P alignments taking into account information from reordering models. [sent-129, score-2.89]
</p><p>42 , reordering model and alignment model, as illustrated in Figure 2. [sent-135, score-0.721]
</p><p>43 The steps involved are as  described below: Step 1: First, we use manual word alignments (H) to train source and target reordering models as described in (Visweswariah et al. [sent-136, score-1.327]
</p><p>44 Step 2: Next, we use the hand alignments to train an alignment model P(a|ws, wt , πs , πt). [sent-138, score-0.658]
</p><p>45 In aadnd aitiloignn tmo etnhet original source and target sentence, we also feed the predictions of the reordering model trained in Step 1 to this alignment model (see section 4. [sent-139, score-0.923]
</p><p>46 Step 3: Finally, we use the predictions of the alignment model trained in Step 2 to train reordering models C(πs |ws, wt , a) (see section 4. [sent-141, score-0.9]
</p><p>47 After building the sequence of models shown in Figure 2, we apply them in sequence on the unaligned parallel data U, starting with the reordering models C(πs |ws) and C(πt |wt). [sent-143, score-0.673]
</p><p>48 The reorderings obtained| wfor the source s|iwde in U (after applying the final model C(πs |ws, a)) are used along with reference reorderings obtained from the manual word alignments to train our reorder-  ing model. [sent-144, score-1.576]
</p><p>49 Also, since we are interested only in the source side reorderings produced by the model C(πs |ws, a), the target reordering model C(πt |wt , a) iws needed only if we iterate over steps 2 a|nwd 3. [sent-146, score-1.21]
</p><p>50 Consider the case when we are training an alignment model conditioned on reorderings (P(a|ws, wt , πs , πt)). [sent-148, score-0.66]
</p><p>51 2 Modeling alignments given reordering In this section we describe how we fuse information from source and target reordering models to improve word alignments. [sent-155, score-1.777]
</p><p>52 (201 1) and include features encoding the Model 1 probabilities between pairs of words linked in the alignment a, features that inspect source and target POS tags and parses (if available) and features that inspect the alignments of adjacent words in the source and target sentence. [sent-165, score-0.896]
</p><p>53 To incorporate information from the reordering model, we add features that use the predicted source πs and target permutations πt. [sent-166, score-0.804]
</p><p>54 If the reorderings πs were perfect we would learn to only allow alignments where wπsis and wπsis+1 were aligned to adjacent words in the target sentence. [sent-173, score-0.935]
</p><p>55 Although the reordering model is not perfect, preferring alignments consistent with the reordering models improves the aligner. [sent-174, score-1.626]
</p><p>56 For the feature functions Φ, in addition to the features that only depend on m, n (that we  ws,  1279 use in our standard reordering model) we add binary indicator features based on msd(Sm, Sn) and msd(Sm, Sn) conjoined with POS(wms) and POS(wns). [sent-180, score-0.628]
</p><p>57 This model allows us to combine features from the original reordering model along with information coming from the alignments to find source reorderings given a parallel corpus and alignments. [sent-187, score-1.599]
</p><p>58 All experiments were done on UrduEnglish and we evaluate reordering in two ways:  Firstly, we evaluate reordering performance directly by comparing the reordered source sentence in Urdu with a reference reordering obtained from the manual word alignments using BLEU (Papineni et al. [sent-190, score-2.476]
</p><p>59 Additionally, we evaluate the effect of reordering on our final systems for machine translation measured using BLEU. [sent-193, score-0.66]
</p><p>60 We use about 10K sentences (180K words) of manual word alignments which were created in house using part of the NIST MT-08 training data3 to train our baseline reordering model and to train our supervised machine aligners. [sent-194, score-1.322]
</p><p>61 The parallel corpus is used for building our phrased based machine translation system and to add training data for our reordering model. [sent-202, score-0.724]
</p><p>62 To extract phrases we use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al. [sent-206, score-0.902]
</p><p>63 Column 2 of Table 1 shows mBLEU as a function  of the number of sentences with manual word alignments that are used to train the reordering model. [sent-212, score-1.19]
</p><p>64 4K sentences with manual word alignments our model benefits from lexical identities of more than the 1000 most frequent words. [sent-217, score-0.607]
</p><p>65 l082exonly  Table 1: mBLEU scores for Urdu to English reordering using different number of sentences of manually word aligned training data with all features and with lexical features instantiated only for the 1000 most frequent words. [sent-225, score-0.767]
</p><p>66 machine alignments to train the reordering model and see the effect of aligner quality on the reordering model generated using this data. [sent-226, score-1.831]
</p><p>67 We experimented with two different supervised aligners : a maximum entropy aligner (Ittycheriah and Roukos, 2005) and an improved correction model that corrects the maximum entropy alignments (McCarley et al. [sent-228, score-0.624]
</p><p>68 Table 2 shows mBLEU scores when the reordering model is trained on reordering references created from aligners with different quality. [sent-234, score-1.257]
</p><p>69 We see that the quality of the alignments matter a great deal to the reordering model; using MaxEnt alignments cause a degradation in performance over just using a small set of manual word alignments. [sent-235, score-1.542]
</p><p>70 (201 1) are of much better quality and hence give higher reordering performance. [sent-237, score-0.612]
</p><p>71 Note that this reordering performance is much better than that obtained using manual word alignments because the size of machine  alignments is much larger (3. [sent-238, score-1.546]
</p><p>72 Improvements in reordering performance using the proposed models: Table 3 shows improvements in the reordering model when using the models proposed in this paper. [sent-240, score-1.236]
</p><p>73 We use H to refer to the manually word aligned data and U to refer to the additional sentence pairs for which manual word alignments are not available. [sent-241, score-0.654]
</p><p>74 Corresponding to this, we also report the baseline for our reordering experiments in the third column. [sent-247, score-0.586]
</p><p>75 We then combine these reference reorderings with the reference reorderings derived from H and use this combined data to train a reordering model which serves as the baseline (mBLEU = 55. [sent-250, score-1.615]
</p><p>76 However, instead of using the basic approach of extracting reference reorderings, we use our improved model C(π|a) to generate reference reorderings from U. [sent-255, score-0.631]
</p><p>77 with the reference reorderings derived from H and used to train a reordering model (mBLEU = 56. [sent-257, score-1.139]
</p><p>78 eT ehnet alignment model P(a|π) is first improved by using premdicetniotn mso dfreolm P tah|eπ reordering mroovdeedl. [sent-261, score-0.743]
</p><p>79 b T uhsiensge pimre-proved alignments are then used to extract better reference reorderings from U using C(π|a). [sent-262, score-0.866]
</p><p>80 Improvements come roughly in equal parts from the two techniques we proposed in this paper : (i) using a model to generate reference reorderings from noisy alignments and (ii) using reordering information to improve the aligner. [sent-264, score-1.577]
</p><p>81 614EU  Table 3: mBLEU with different methods to generate reordering model training data from a machine aligned parallel corpus in addition to manual word alignments. [sent-267, score-0.948]
</p><p>82 For results including a reordering model, we simply reorder the source side Urdu data both while training and at test time. [sent-269, score-0.728]
</p><p>83 8 BLEU points in machine translation by going beyond manual word alignments using the best reordering model reported in Table 3. [sent-274, score-1.301]
</p><p>84 , 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in ad-  T+abRmleoHBcMrh4iadsn:e lrMacSgehi:yT(snatlmeoigp uhnrmtayesflop, dmrtisebgoanmeldc)pbnatswih21Wo809ue. [sent-282, score-0.728]
</p><p>85 lato)riSvdksenltriy-  (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). [sent-288, score-0.664]
</p><p>86 The first set of approaches handle the reordering problem as part of the decoding process. [sent-291, score-0.586]
</p><p>87 The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. [sent-296, score-0.845]
</p><p>88 (2010) proposed the use of function word reordering to improve alignments. [sent-303, score-0.616]
</p><p>89 While this work is similar to one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). [sent-304, score-1.286]
</p><p>90 The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al. [sent-305, score-0.626]
</p><p>91 Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). [sent-309, score-1.071]
</p><p>92 8  Conclusion  In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. [sent-310, score-0.776]
</p><p>93 We proposed a model that scores reorderings given alignments and the source sentence that we use to gener-  ate cleaner training data from noisy alignments. [sent-311, score-1.023]
</p><p>94 We also proposed a model that scores alignments given source and target sentence reorderings that improves a supervised alignment model by 2. [sent-312, score-1.136]
</p><p>95 While the improvement in alignment performance is modest, the improvement does result in improved reordering models. [sent-314, score-0.703]
</p><p>96 8 BLEU points over a baseline reordering model that only uses manual word alignments, a gain of 2. [sent-316, score-0.849]
</p><p>97 Another avenue of future work we would like to explore is the use of monolingual source and target data to further assist the reordering model. [sent-320, score-0.726]
</p><p>98 Automatically learning sourceside reordering rules for large scale machine translation. [sent-368, score-0.616]
</p><p>99 Discriminative word alignment with a function word reordering model. [sent-432, score-0.741]
</p><p>100 Syntax based reordering with automatically derived rules for improved statistical machine translation. [sent-450, score-0.638]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reordering', 0.586), ('reorderings', 0.404), ('alignments', 0.39), ('visweswariah', 0.165), ('mccarley', 0.165), ('ws', 0.153), ('mbleu', 0.14), ('urdu', 0.131), ('preordering', 0.125), ('manual', 0.12), ('wt', 0.096), ('aligner', 0.096), ('alignment', 0.095), ('source', 0.079), ('reference', 0.072), ('ramanathan', 0.069), ('permutation', 0.067), ('target', 0.061), ('ittycheriah', 0.057), ('msd', 0.057), ('permutations', 0.057), ('bleu', 0.057), ('aligned', 0.057), ('correction', 0.053), ('chained', 0.051), ('tsp', 0.051), ('wms', 0.047), ('wns', 0.047), ('translation', 0.044), ('stroudsburg', 0.043), ('costs', 0.042), ('model', 0.04), ('neubig', 0.039), ('parallel', 0.039), ('reorder', 0.038), ('points', 0.038), ('ananthakrishnan', 0.038), ('noisy', 0.038), ('sn', 0.038), ('sm', 0.038), ('train', 0.037), ('arg', 0.036), ('gain', 0.035), ('pa', 0.034), ('inspect', 0.034), ('denero', 0.034), ('phrase', 0.032), ('applegate', 0.031), ('linkernighan', 0.031), ('indices', 0.031), ('karthik', 0.03), ('word', 0.03), ('machine', 0.03), ('sis', 0.03), ('xc', 0.029), ('movements', 0.029), ('uszkoreit', 0.028), ('genzel', 0.028), ('tromble', 0.028), ('signed', 0.027), ('roukos', 0.027), ('ibm', 0.027), ('sentence', 0.027), ('sentences', 0.027), ('quality', 0.026), ('roughly', 0.026), ('nist', 0.026), ('jiri', 0.025), ('salesman', 0.025), ('training', 0.025), ('mccord', 0.024), ('models', 0.024), ('going', 0.023), ('crammer', 0.023), ('heuristic', 0.023), ('india', 0.023), ('abraham', 0.023), ('aligners', 0.023), ('perfect', 0.023), ('trained', 0.022), ('maxent', 0.022), ('improved', 0.022), ('papineni', 0.022), ('distortion', 0.022), ('xia', 0.022), ('hierarchical', 0.022), ('approximation', 0.021), ('features', 0.021), ('zollmann', 0.021), ('generate', 0.021), ('mcdonald', 0.021), ('describe', 0.021), ('collins', 0.02), ('min', 0.02), ('ganitkevitch', 0.02), ('cleaner', 0.02), ('xavier', 0.02), ('derive', 0.02), ('parser', 0.019), ('hypothesized', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="101-tfidf-1" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>2 0.39389288 <a title="101-tfidf-2" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>Author: Fei Huang ; Cezar Pendus</p><p>Abstract: We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation (SMT). Instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage. Furthermore, , we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules. Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach: we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric. 1</p><p>3 0.31390634 <a title="101-tfidf-3" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>Author: Minwei Feng ; Jan-Thorsten Peter ; Hermann Ney</p><p>Abstract: In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. Results on five Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average. Results of comparative study with other seven widely used reordering models will also be reported.</p><p>4 0.24609074 <a title="101-tfidf-4" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>5 0.17608009 <a title="101-tfidf-5" href="./acl-2013-Two-Neighbor_Orientation_Model_with_Cross-Boundary_Global_Contexts.html">363 acl-2013-Two-Neighbor Orientation Model with Cross-Boundary Global Contexts</a></p>
<p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang ; Libin Shen</p><p>Abstract: Long distance reordering remains one of the greatest challenges in statistical machine translation research as the key contextual information may well be beyond the confine of translation units. In this paper, we propose Two-Neighbor Orientation (TNO) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule boundaries. We explicitly model the longest span of such chunks, referred to as Maximal Orientation Span, to serve as a global parameter that constrains underlying local decisions. We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our proposal in a large-scale Chinese-to-English translation task. On NIST MT08 set, our most advanced model brings around +2.0 BLEU and -1.0 TER improvement.</p><p>6 0.15314162 <a title="101-tfidf-6" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>7 0.14836884 <a title="101-tfidf-7" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>8 0.14726138 <a title="101-tfidf-8" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>9 0.13776031 <a title="101-tfidf-9" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>10 0.12325139 <a title="101-tfidf-10" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>11 0.120777 <a title="101-tfidf-11" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>12 0.11446539 <a title="101-tfidf-12" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>13 0.112822 <a title="101-tfidf-13" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>14 0.10500002 <a title="101-tfidf-14" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>15 0.10335389 <a title="101-tfidf-15" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>16 0.10134617 <a title="101-tfidf-16" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>17 0.096646033 <a title="101-tfidf-17" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>18 0.092098467 <a title="101-tfidf-18" href="./acl-2013-Fast_and_Adaptive_Online_Training_of_Feature-Rich_Translation_Models.html">156 acl-2013-Fast and Adaptive Online Training of Feature-Rich Translation Models</a></p>
<p>19 0.085017242 <a title="101-tfidf-19" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>20 0.082487285 <a title="101-tfidf-20" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, -0.187), (2, 0.163), (3, 0.132), (4, -0.044), (5, 0.053), (6, -0.007), (7, 0.009), (8, 0.006), (9, 0.012), (10, -0.004), (11, -0.044), (12, -0.001), (13, -0.008), (14, 0.058), (15, 0.105), (16, 0.202), (17, 0.067), (18, 0.062), (19, -0.043), (20, -0.169), (21, -0.012), (22, 0.007), (23, -0.217), (24, 0.102), (25, 0.045), (26, -0.053), (27, -0.137), (28, -0.291), (29, -0.075), (30, -0.079), (31, 0.037), (32, 0.001), (33, 0.053), (34, -0.072), (35, 0.001), (36, 0.053), (37, 0.022), (38, -0.044), (39, 0.102), (40, 0.031), (41, -0.175), (42, -0.011), (43, -0.06), (44, -0.016), (45, -0.044), (46, 0.026), (47, -0.003), (48, 0.013), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93779439 <a title="101-lsi-1" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>2 0.88712579 <a title="101-lsi-2" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>Author: Fei Huang ; Cezar Pendus</p><p>Abstract: We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation (SMT). Instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage. Furthermore, , we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules. Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach: we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric. 1</p><p>3 0.79349267 <a title="101-lsi-3" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>Author: Minwei Feng ; Jan-Thorsten Peter ; Hermann Ney</p><p>Abstract: In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. Results on five Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average. Results of comparative study with other seven widely used reordering models will also be reported.</p><p>4 0.7788769 <a title="101-lsi-4" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>Author: Isao Goto ; Masao Utiyama ; Eiichiro Sumita ; Akihiro Tamura ; Sadao Kurohashi</p><p>Abstract: This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.</p><p>5 0.77357113 <a title="101-lsi-5" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>6 0.76845598 <a title="101-lsi-6" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>7 0.75442898 <a title="101-lsi-7" href="./acl-2013-Two-Neighbor_Orientation_Model_with_Cross-Boundary_Global_Contexts.html">363 acl-2013-Two-Neighbor Orientation Model with Cross-Boundary Global Contexts</a></p>
<p>8 0.48505333 <a title="101-lsi-8" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>9 0.48095652 <a title="101-lsi-9" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>10 0.47543558 <a title="101-lsi-10" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>11 0.43507299 <a title="101-lsi-11" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>12 0.42103082 <a title="101-lsi-12" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>13 0.3996844 <a title="101-lsi-13" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>14 0.39799857 <a title="101-lsi-14" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>15 0.39638007 <a title="101-lsi-15" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>16 0.39419758 <a title="101-lsi-16" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>17 0.39271632 <a title="101-lsi-17" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>18 0.36256525 <a title="101-lsi-18" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>19 0.36106384 <a title="101-lsi-19" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>20 0.35851592 <a title="101-lsi-20" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.035), (6, 0.122), (11, 0.05), (14, 0.011), (15, 0.014), (24, 0.04), (26, 0.043), (28, 0.022), (35, 0.074), (40, 0.141), (42, 0.106), (48, 0.047), (70, 0.036), (77, 0.013), (88, 0.028), (90, 0.039), (95, 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.87239629 <a title="101-lda-1" href="./acl-2013-Coordination_Structures_in_Dependency_Treebanks.html">94 acl-2013-Coordination Structures in Dependency Treebanks</a></p>
<p>Author: Martin Popel ; David Marecek ; Jan StÄłpanek ; Daniel Zeman ; ZdÄłnÄłk Zabokrtsky</p><p>Abstract: Paratactic syntactic structures are notoriously difficult to represent in dependency formalisms. This has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending problem in dependency analysis of natural languages. This paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical observations on convertibility between selected styles of representations are shown too.</p><p>2 0.86125934 <a title="101-lda-2" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>Author: Kenneth Heafield ; Ivan Pouzyrevsky ; Jonathan H. Clark ; Philipp Koehn</p><p>Abstract: We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM.</p><p>same-paper 3 0.85717285 <a title="101-lda-3" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>4 0.84349453 <a title="101-lda-4" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>Author: Yuki Arase ; Ming Zhou</p><p>Abstract: We propose a method for automatically detecting low-quality Web-text translated by statistical machine translation (SMT) systems. We focus on the phrase salad phenomenon that is observed in existing SMT results and propose a set of computationally inexpensive features to effectively detect such machine-translated sentences from a large-scale Web-mined text. Unlike previous approaches that require bilingual data, our method uses only monolingual text as input; therefore it is applicable for refining data produced by a variety of Web-mining activities. Evaluation results show that the proposed method achieves an accuracy of 95.8% for sentences and 80.6% for text in noisy Web pages.</p><p>5 0.84254712 <a title="101-lda-5" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>Author: Matthew R. Gormley ; Jason Eisner</p><p>Abstract: Many models in NLP involve latent variables, such as unknown parses, tags, or alignments. Finding the optimal model parameters is then usually a difficult nonconvex optimization problem. The usual practice is to settle for local optimization methods such as EM or gradient ascent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ?) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Linearization Technique to produce convex relaxations during branch-and-bound. Although these techniques do not yet provide a practical solution to our instance of this NP-hard problem, they sometimes find better solutions than Viterbi EM with random restarts, in the same time.</p><p>6 0.82899475 <a title="101-lda-6" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>7 0.81346565 <a title="101-lda-7" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>8 0.8079325 <a title="101-lda-8" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>9 0.79813325 <a title="101-lda-9" href="./acl-2013-Modeling_Thesis_Clarity_in_Student_Essays.html">246 acl-2013-Modeling Thesis Clarity in Student Essays</a></p>
<p>10 0.79112202 <a title="101-lda-10" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>11 0.78784066 <a title="101-lda-11" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>12 0.78770477 <a title="101-lda-12" href="./acl-2013-From_Natural_Language_Specifications_to_Program_Input_Parsers.html">163 acl-2013-From Natural Language Specifications to Program Input Parsers</a></p>
<p>13 0.77869612 <a title="101-lda-13" href="./acl-2013-Reducing_Annotation_Effort_for_Quality_Estimation_via_Active_Learning.html">300 acl-2013-Reducing Annotation Effort for Quality Estimation via Active Learning</a></p>
<p>14 0.77734524 <a title="101-lda-14" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>15 0.77420306 <a title="101-lda-15" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>16 0.77127892 <a title="101-lda-16" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>17 0.76929736 <a title="101-lda-17" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>18 0.76665998 <a title="101-lda-18" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>19 0.76521873 <a title="101-lda-19" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>20 0.76520276 <a title="101-lda-20" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
