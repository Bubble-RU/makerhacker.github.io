<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-15" href="#">acl2013-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</h1>
<br/><p>Source: <a title="acl-2013-15-pdf" href="http://aclweb.org/anthology//P/P13/P13-2064.pdf">pdf</a></p><p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>Reference: <a title="acl-2013-15-reference" href="../acl2013_reference/acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ie Abstract In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. [sent-3, score-0.868]
</p><p>2 However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. [sent-4, score-0.295]
</p><p>3 Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. [sent-5, score-0.552]
</p><p>4 1 Introduction  Word alignment is the task of identifying translational relations between words in parallel corpora, in which a word at one language is usually translated into several words at the other language (fertility model) (Brown et al. [sent-7, score-0.287]
</p><p>5 Given that many-to-many links are common in natural languages (Moore, 2005), it is necessary to pay attention to the relations among alignment links. [sent-9, score-0.331]
</p><p>6 In this paper, we have proposed a novel graphbased compact representation of word alignment, which takes into account the joint distribution of alignment links. [sent-10, score-0.402]
</p><p>7 We first transform each alignment to a bigraph that can be decomposed into a set of subgraphs, where all interrelated links are in the same subgraph (§ 2. [sent-11, score-0.834]
</p><p>8 gTrahep ms (a§in 2 challenge of this research is to efficiently calculate the fractional counts for rules extracted from hypergraphs. [sent-16, score-0.26]
</p><p>9 Observing that most alignments are not connected, we propose a divide-and-conquer  strategy by decomposing a hypergraph into a set Lab. [sent-18, score-0.764]
</p><p>10 Processing Institute of Computing Technology, CAS {tu zhaopeng sxl in} @ i . [sent-20, score-0.223]
</p><p>11 cn ct  ‡Key  ,  Figure 1: A bigraph constructed from an alignment (a), and its disjoint MCSs (b). [sent-22, score-0.672]
</p><p>12 of independent subhypergraphs, which is computationally feasible in practice (§ 3. [sent-23, score-0.041]
</p><p>13 Experimenttaatl orensaullltys sfheaoswib tlhea tin our approach significantly iemn-proves translation performance by up to 1. [sent-25, score-0.039]
</p><p>14 1 Word Alignment as a Bigraph Each alignment of a sentence pair can be transformed to a bigraph, in which the two disjoint vertex sets S and T are the source and target words respectively, and the edges are word-by-word links. [sent-29, score-0.332]
</p><p>15 For example, Figure 1(a) shows the corresponding bigraph of an alignment. [sent-30, score-0.372]
</p><p>16 A graph is called connected if there is a path between every pair of distinct vertices. [sent-32, score-0.12]
</p><p>17 In an alignment, words in a specific portion at the source side (i. [sent-33, score-0.036]
</p><p>18 a verb phrase) usually align to those in the corresponding portion (i. [sent-35, score-0.036]
</p><p>19 Therefore, there is no edge that connects the words in the portion to those outside the portion. [sent-38, score-0.036]
</p><p>20 Therefore, a bigraph can be decomposed into a unique set of minimum connected subgraphs (MCSs), where each subgraph is connected and does not contain any other MCSs. [sent-39, score-0.676]
</p><p>21 For example, the bigraph in Figure 1(a) can be decomposed into 358  ProceedingSsof oifa, th Beu 5l1gsarti Aan,An uuaglu Mste 4e-ti9n2g 0 o1f3 t. [sent-40, score-0.424]
</p><p>22 14e53btdoihsen sok Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c) the resulting hypergraph that takes the two alignments as samples. [sent-49, score-1.188]
</p><p>23 We can see that all interrelated links are in the same MCS. [sent-51, score-0.105]
</p><p>24 These MCSs work as fundamental units in our approach to take advantage of the relations among the links. [sent-52, score-0.029]
</p><p>25 Hereinafter, we use bigraph to denote the alignment of a sentence pair. [sent-53, score-0.63]
</p><p>26 2 Weighted Bipartite Hypergraph We believe that offering more alternatives to extracting translation rules could help improve translation quality. [sent-55, score-0.251]
</p><p>27 We propose a new structure called weighted bipartite hypergraph that compactly encodes multiple alignments. [sent-56, score-0.718]
</p><p>28 Figures 2(a) and 2(b) show two bigraphs of the same sentence pair. [sent-58, score-0.186]
</p><p>29 Intuitively, we can encode the union set of subgraphs in a bipartite hypergraph, in which each MCS serves as a hyperedge, as in  Figure 2(c). [sent-59, score-0.197]
</p><p>30 Accordingly, we can calculate how well a hyperedge is by calculating its relative frequency, which is the probability sum of bigraphs in which the corresponding MCS occurs divided by the probability sum of all possible bigraphs. [sent-60, score-0.515]
</p><p>31 Suppose that the probabilities of the two bigraphs in Figures 2(a) and 2(b) are 0. [sent-61, score-0.251]
</p><p>32 Therefore, each hyperedge is associated with a weight to indicate how well it is. [sent-67, score-0.081]
</p><p>33 Formally, a weighted bipartite hypergraph H is a triple hS, T, Ei where S and T are two sets of vae trrtiipceles on tTh,eE source raen dS target sides, wanod s Eets are hyperedges associated with weights. [sent-68, score-0.813]
</p><p>34 Currently, we estimate the weights of hyperedges from an nbest list by calculating relative frequencies:  w(ei) = PBG∈PNp(BG)p ×(B δG(B)G,gi) Here N is an n-best bigPraph (i. [sent-69, score-0.283]
</p><p>35 , alignment) list, p(BG) is the probability of a bigraph BG in the nbest list, gi is the MCS that corresponds to ei, and δ(BG, gi) is an indicator function which equals 1  when gi occurs in BG, and 0 otherwise. [sent-71, score-0.583]
</p><p>36 It is worthy mentioning that a hypergraph encodes much more alignments than the input n-best list. [sent-72, score-0.744]
</p><p>37 For example, we can construct a new alignment by using hyperedges from different bigraphs that cover all vertices. [sent-73, score-0.608]
</p><p>38 3  Graph-based Rule Extraction  In this section we describe how to extract translation rules from a hypergraph (§ 3. [sent-74, score-0.615]
</p><p>39 1) and how to teisotinm rautlee sth feriorm probabilities (§ 3. [sent-75, score-0.065]
</p><p>40 1 Extraction Algorithm We extract translation rules from a hypergraph for the hierarchical phrase-based system (Chiang, 2007). [sent-78, score-0.658]
</p><p>41 Chiang (2007) describes a rule extraction algorithm that involves two steps: (1) extract phrases from 1-best alignments; (2) obtain variable rules by replacing sub-phrase pairs with nonterminals. [sent-79, score-0.198]
</p><p>42 Our extraction algorithm differs at the first step, in which we extract phrases from hypergraphs instead of 1-best alignments. [sent-80, score-0.135]
</p><p>43 Rather than restricting ourselves by the alignment consistency  in the traditional algorithm, we extract all possible candidate target phrases for each source phrase. [sent-81, score-0.289]
</p><p>44 To maintain a reasonable rule table size, we filter out less promising candidates that have a fractional count lower than a threshold. [sent-82, score-0.193]
</p><p>45 2 Calculating Fractional Counts The fractional count of a phrase pair is the probability sum of the alignments with which the phrase pair is consistent (§3. [sent-84, score-0.469]
</p><p>46 2), divided by the probability sum oonfs saisl t alignments ,e dnicvoiddeedd biny a hypergraph (§3. [sent-86, score-0.82]
</p><p>47 How to calculate the probability sum of all alignments encoded in a hypergraph (§3. [sent-91, score-0.792]
</p><p>48 How to efficiently calculate the probability sum of all consistent alignments for each phrase pair (§3. [sent-95, score-0.364]
</p><p>49 1 Enumerating All Alignments In theory, a hypergraph can encode all possible alignments if there are enough hyperedges. [sent-100, score-0.672]
</p><p>50 How-  ever, since a hypergraph is constructed from an nbest list, it can only represent partial space of all alignments (p(A|H) < 1) because of the limiting asilizgen mofe hyperedges )le  < 0. [sent-101, score-0.939]
</p><p>51 4If a subhypergraph has more than 5 hyperedges, we forcibly partition it into small subhypergraphs by iteratively removing lowest-probability hyperedges. [sent-104, score-0.107]
</p><p>52 E61U2 Table 2: Comparison of rule tables learned from n-best lists and hypergraphs. [sent-112, score-0.203]
</p><p>53 table, “Shared” denotes the intersection of two tables, and “Non-shared” that the probabilities  “All” denotes the full rule  denotes the complement. [sent-113, score-0.333]
</p><p>54 Note  of “Shared” rules are different for the two approaches. [sent-114, score-0.085]
</p><p>55 In theory, the rule table extracted from n-best lists is a subset of that from hypergraphs. [sent-116, score-0.155]
</p><p>56 In practice, however, this is not true because we pruned the rules that have fractional counts lower than a threshold. [sent-117, score-0.222]
</p><p>57 Therefore, the question arises as to how many rules are shared by n-best and hypergraphbased extractions. [sent-118, score-0.128]
</p><p>58 We try to answer this question by comparing the different rule tables (filtered on the test sets) learned from n-best lists and hypergraphs. [sent-119, score-0.203]
</p><p>59 “All” denotes the full rule table, “Shared” denotes the intersection of two tables, and “Non-shared” denotes the complement. [sent-121, score-0.268]
</p><p>60 Note that the probabil-  ities of “Shared” rules are different for the two approaches. [sent-122, score-0.085]
</p><p>61 5  Related Work  Our research builds on previous work in the field of graph models and compact representations. [sent-124, score-0.158]
</p><p>62 Graph models have been used before in word alignment: the search space ofword alignment can be structured as a graph and the search problem can be reformulated as finding the optimal path though this graph (e. [sent-125, score-0.399]
</p><p>63 In addition, Kumar and Byrne (2002) define a graph distance as a loss function for minimum Bayes-risk word alignment, Riesa and Marcu (2010) open up the word alignment task to  advances in hypergraph algorithms currently used in parsing. [sent-129, score-0.801]
</p><p>64 As opposed to the search problem, we propose a graph-based compact representation that encodes multiple alignments for machine translation. [sent-130, score-0.399]
</p><p>65 Previous research has demonstrated that compact representations can produce improved results by offering more alternatives, e. [sent-131, score-0.164]
</p><p>66 , 2012a), word lattices over 1-best segmentations (Dyer et al. [sent-135, score-0.026]
</p><p>67 , 2008), and weighted alignment matrices over 1-best word alignments (Liu et al. [sent-136, score-0.571]
</p><p>68 , (2009) estimate the link probabilities from n-best lists, while Gispert et al. [sent-141, score-0.065]
</p><p>69 , (2010) learn the alignment posterior probabilities directly from IBM models. [sent-142, score-0.353]
</p><p>70 However, both of them ignore the relations among alignment links. [sent-143, score-0.287]
</p><p>71 By contrast, our approach takes into account the joint distribution of alignment links and explores the fertility model past the link level. [sent-144, score-0.377]
</p><p>72 6  Conclusion  We have presented a novel compact representation of word alignment, named weighted bipartite hypergraph, to exploit the relations among alignment links. [sent-145, score-0.62]
</p><p>73 Since estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, we propose a computationally tractable divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. [sent-146, score-0.847]
</p><p>74 In Proceedings ofthe 43rdAnnualMeeting on Association for Computational Linguistics, pages 53 1–540. [sent-172, score-0.028]
</p><p>75 Hierarchical phrase-based translation grammars extracted from alignment posterior probabilities. [sent-175, score-0.327]
</p><p>76 In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 545–554. [sent-176, score-0.028]
</p><p>77 In Proceedings of the International Conference on Acoustics, Speech, andSignal Processing, volume 1, pages 181–184. [sent-184, score-0.028]
</p><p>78 In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics, pages 48–54. [sent-188, score-0.028]
</p><p>79 In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 140–147. [sent-192, score-0.028]
</p><p>80 In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1017–1026. [sent-196, score-0.028]
</p><p>81 In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 206–214. [sent-204, score-0.028]
</p><p>82 In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 8 1–88, October. [sent-209, score-0.028]
</p><p>83 In Proceedings of the 41st AnnualMeeting of the Associationfor Computational Linguistics, pages 160–167. [sent-218, score-0.028]
</p><p>84 In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 3 11–3 18. [sent-222, score-0.028]
</p><p>85 In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 157–166. [sent-226, score-0.028]
</p><p>86 In Proceedings of Seventh International Conference on Spoken Language Processing, volume 3, pages 901–904. [sent-230, score-0.028]
</p><p>87 In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1092–1 100. [sent-235, score-0.028]
</p><p>88 In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1294–1303. [sent-239, score-0.028]
</p><p>89 In Proceedings ofthe 24th International Conference on Computational Linguistics, pages 1249–1260. [sent-247, score-0.028]
</p><p>90 Wider pipelines: n-best alignments and parses in mt training. [sent-255, score-0.212]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hypergraph', 0.46), ('bigraph', 0.372), ('alignment', 0.258), ('zhaopeng', 0.223), ('alignments', 0.212), ('bigraphs', 0.186), ('tu', 0.176), ('bg', 0.164), ('hyperedges', 0.164), ('mcss', 0.149), ('shouxun', 0.133), ('qun', 0.133), ('bipartite', 0.119), ('liu', 0.119), ('compact', 0.114), ('fractional', 0.111), ('hypergraphs', 0.104), ('mcs', 0.091), ('rules', 0.085), ('rule', 0.082), ('hyperedge', 0.081), ('subgraphs', 0.078), ('fertility', 0.075), ('subhypergraphs', 0.074), ('nbest', 0.073), ('lists', 0.073), ('weighted', 0.07), ('riesa', 0.066), ('probabilities', 0.065), ('decomposing', 0.065), ('interrelated', 0.061), ('gispert', 0.057), ('yifan', 0.057), ('gi', 0.054), ('decomposed', 0.052), ('sum', 0.052), ('yang', 0.051), ('denotes', 0.051), ('offering', 0.05), ('tables', 0.048), ('subgraph', 0.047), ('calculating', 0.046), ('graph', 0.044), ('connected', 0.044), ('links', 0.044), ('dublin', 0.043), ('encodes', 0.043), ('hierarchical', 0.043), ('shared', 0.043), ('disjoint', 0.042), ('computationally', 0.041), ('ei', 0.04), ('minimum', 0.039), ('translation', 0.039), ('alternatives', 0.038), ('calculate', 0.038), ('portion', 0.036), ('franz', 0.035), ('della', 0.035), ('forest', 0.035), ('och', 0.034), ('intersection', 0.033), ('biny', 0.033), ('forcibly', 0.033), ('oonfs', 0.033), ('partite', 0.033), ('pino', 0.033), ('kumar', 0.033), ('pietra', 0.033), ('pair', 0.032), ('extract', 0.031), ('matrices', 0.031), ('posterior', 0.03), ('adri', 0.03), ('cerov', 0.03), ('mofe', 0.03), ('pnp', 0.03), ('probability', 0.03), ('representation', 0.03), ('relations', 0.029), ('smaranda', 0.029), ('worthy', 0.029), ('dyer', 0.028), ('pages', 0.028), ('andreas', 0.027), ('ofword', 0.027), ('hereinafter', 0.027), ('leslie', 0.027), ('muresan', 0.027), ('restructuring', 0.027), ('strategy', 0.027), ('counts', 0.026), ('intuitively', 0.026), ('compactly', 0.026), ('lattices', 0.026), ('contracts', 0.026), ('hwang', 0.026), ('juan', 0.026), ('reformulated', 0.026), ('tian', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="15-tfidf-1" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>2 0.15776066 <a title="15-tfidf-2" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>3 0.15151854 <a title="15-tfidf-3" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>Author: Yang Liu</p><p>Abstract: We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n-gram and dependency language models. To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-todependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets.</p><p>4 0.13776031 <a title="15-tfidf-4" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>5 0.12996063 <a title="15-tfidf-5" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>Author: Graham Neubig</p><p>Abstract: In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http : / /phont ron . com/t ravat ar</p><p>6 0.12443121 <a title="15-tfidf-6" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>7 0.12239814 <a title="15-tfidf-7" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>8 0.10727608 <a title="15-tfidf-8" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>9 0.10683675 <a title="15-tfidf-9" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>10 0.10453836 <a title="15-tfidf-10" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>11 0.10038443 <a title="15-tfidf-11" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>12 0.096633837 <a title="15-tfidf-12" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>13 0.0948608 <a title="15-tfidf-13" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>14 0.091917388 <a title="15-tfidf-14" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>15 0.089209996 <a title="15-tfidf-15" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>16 0.087587126 <a title="15-tfidf-16" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>17 0.082693666 <a title="15-tfidf-17" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>18 0.082636751 <a title="15-tfidf-18" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>19 0.082515128 <a title="15-tfidf-19" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>20 0.081359133 <a title="15-tfidf-20" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, -0.103), (2, 0.093), (3, 0.067), (4, -0.025), (5, 0.046), (6, -0.026), (7, -0.02), (8, -0.017), (9, -0.036), (10, 0.03), (11, -0.085), (12, 0.037), (13, -0.057), (14, 0.071), (15, 0.007), (16, 0.133), (17, 0.061), (18, 0.034), (19, -0.091), (20, -0.117), (21, 0.039), (22, 0.013), (23, -0.023), (24, -0.02), (25, 0.048), (26, -0.08), (27, 0.039), (28, 0.003), (29, 0.026), (30, 0.011), (31, 0.018), (32, -0.035), (33, -0.02), (34, -0.036), (35, 0.001), (36, -0.072), (37, 0.039), (38, 0.016), (39, 0.056), (40, -0.008), (41, 0.085), (42, 0.046), (43, 0.108), (44, 0.195), (45, -0.049), (46, -0.063), (47, -0.009), (48, -0.024), (49, 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93809867 <a title="15-lsi-1" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>2 0.71546674 <a title="15-lsi-2" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>Author: Thomas Schoenemann</p><p>Abstract: We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores.</p><p>3 0.70346993 <a title="15-lsi-3" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>4 0.67168361 <a title="15-lsi-4" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>5 0.64221418 <a title="15-lsi-5" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>Author: Tingting Li ; Tiejun Zhao ; Andrew Finch ; Chunyue Zhang</p><p>Abstract: Machine Transliteration is an essential task for many NLP applications. However, names and loan words typically originate from various languages, obey different transliteration rules, and therefore may benefit from being modeled independently. Recently, transliteration models based on Bayesian learning have overcome issues with over-fitting allowing for many-to-many alignment in the training of transliteration models. We propose a novel coupled Dirichlet process mixture model (cDPMM) that simultaneously clusters and bilingually aligns transliteration data within a single unified model. The unified model decomposes into two classes of non-parametric Bayesian component models: a Dirichlet process mixture model for clustering, and a set of multinomial Dirichlet process models that perform bilingual alignment independently for each cluster. The experimental results show that our method considerably outperforms conventional alignment models.</p><p>6 0.61402595 <a title="15-lsi-6" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>7 0.60576749 <a title="15-lsi-7" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>8 0.57615507 <a title="15-lsi-8" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>9 0.57079029 <a title="15-lsi-9" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>10 0.56201422 <a title="15-lsi-10" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>11 0.54477954 <a title="15-lsi-11" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>12 0.52383435 <a title="15-lsi-12" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>13 0.52122039 <a title="15-lsi-13" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>14 0.51817667 <a title="15-lsi-14" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>15 0.49293116 <a title="15-lsi-15" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>16 0.47745812 <a title="15-lsi-16" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>17 0.47478366 <a title="15-lsi-17" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>18 0.47104108 <a title="15-lsi-18" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>19 0.46837533 <a title="15-lsi-19" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>20 0.46398586 <a title="15-lsi-20" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (6, 0.071), (11, 0.055), (15, 0.011), (24, 0.048), (26, 0.037), (35, 0.068), (42, 0.056), (48, 0.02), (64, 0.324), (70, 0.045), (88, 0.02), (90, 0.061), (95, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84412718 <a title="15-lda-1" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>Author: Eneko Agirre ; Nikolaos Aletras ; Paul Clough ; Samuel Fernando ; Paula Goodale ; Mark Hall ; Aitor Soroa ; Mark Stevenson</p><p>Abstract: This paper describes a system for navigating large collections of information about cultural heritage which is applied to Europeana, the European Library. Europeana contains over 20 million artefacts with meta-data in a wide range of European languages. The system currently provides access to Europeana content with meta-data in English and Spanish. The paper describes how Natural Language Processing is used to enrich and organise this meta-data to assist navigation through Europeana and shows how this information is used within the system.</p><p>same-paper 2 0.77919096 <a title="15-lda-2" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>3 0.71980327 <a title="15-lda-3" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>Author: Anoop Kunchukuttan ; Rajen Chatterjee ; Shourya Roy ; Abhijit Mishra ; Pushpak Bhattacharyya</p><p>Abstract: Large amount of parallel corpora is required for building Statistical Machine Translation (SMT) systems. We describe the TransDoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation ofconstituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence translations from phrase translations. Trans- Doop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowd’s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work.</p><p>4 0.70948571 <a title="15-lda-4" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Dan Roth</p><p>Abstract: Semantic parsing is a domain-dependent process by nature, as its output is defined over a set of domain symbols. Motivated by the observation that interpretation can be decomposed into domain-dependent and independent components, we suggest a novel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains.</p><p>5 0.70306695 <a title="15-lda-5" href="./acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines.html">152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</a></p>
<p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>6 0.65456009 <a title="15-lda-6" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>7 0.53952575 <a title="15-lda-7" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>8 0.53636307 <a title="15-lda-8" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>9 0.50699705 <a title="15-lda-9" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>10 0.50349975 <a title="15-lda-10" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>11 0.4906154 <a title="15-lda-11" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>12 0.48099613 <a title="15-lda-12" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>13 0.47987881 <a title="15-lda-13" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>14 0.47714859 <a title="15-lda-14" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>15 0.47612035 <a title="15-lda-15" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>16 0.45668051 <a title="15-lda-16" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>17 0.45572296 <a title="15-lda-17" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>18 0.44859222 <a title="15-lda-18" href="./acl-2013-Predicting_and_Eliciting_Addressee%27s_Emotion_in_Online_Dialogue.html">282 acl-2013-Predicting and Eliciting Addressee's Emotion in Online Dialogue</a></p>
<p>19 0.44839537 <a title="15-lda-19" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>20 0.4449392 <a title="15-lda-20" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
