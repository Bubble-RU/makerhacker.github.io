<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-111" href="#">acl2013-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</h1>
<br/><p>Source: <a title="acl-2013-111-pdf" href="http://aclweb.org/anthology//P/P13/P13-1087.pdf">pdf</a></p><p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>Reference: <a title="acl-2013-111-reference" href="../acl2013_reference/acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. [sent-2, score-0.412]
</p><p>2 Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting  sense hypotheses within the same word. [sent-3, score-0.96]
</p><p>3 1 Introduction Word Sense Disambiguation (WSD) is a task to identify the intended sense of a word based on its context. [sent-5, score-0.351]
</p><p>4 They compute dictionary-based sense similarity to find the most related senses among the words within a certain range of text. [sent-12, score-0.501]
</p><p>5 However, those methods mainly focus on modeling sense distribution and have less attention to contextual smoothing/generalization beyond immediate context. [sent-17, score-0.372]
</p><p>6 (2004) proposed a method to combine sense similarity with distributional similarity and configured predominant sense score. [sent-20, score-0.87]
</p><p>7 (2009) used a k-nearest words on distributional similarity as context words. [sent-24, score-0.246]
</p><p>8 They apply a LKB graph-based WSD to a target word together with the distributional context words, and showed that it yields better results on a domain dataset than just using immediate context words. [sent-25, score-0.423]
</p><p>9 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 8 4–893, studies are word-by-word WSD for target words, they demonstrated the effectiveness to enrich immediate context by corpus statistics. [sent-28, score-0.195]
</p><p>10 This paper proposes a smoothing model that integrates dictionary-based semantic similarity and corpus-based context statistics, where a combinatorial optimization scheme is employed to deal with sense interdependency of the all-words WSD task. [sent-29, score-0.778]
</p><p>11 A specific implementation of these metrics is described later in this paper, for now the context metric is generalized with a distance function dx(·, ·) and the sense metric with ds(·, ·). [sent-40, score-0.604]
</p><p>12 For each xi, the intended sense of the word is to be found in a set of sense candidates Si = ⊆ S, where Mi is the number of sense c{ asnd}idates⊆ ⊆fo Sr t,h we hie-rthe word, S is the whole set of sense inventories in a dictionary. [sent-45, score-1.278]
</p><p>13 Let the two-tuple hij = (xi, sij) be the hypothesis that the intended sense in xi is sij. [sent-46, score-0.88]
</p><p>14 As (X, dx) eanntd o (S, ds) iereaccth composes a =m Xetri ×c space, H is also a metric space, provided a proper distance definition with dx and ds. [sent-48, score-0.197]
</p><p>15 Here, we treat the space H as a continuous one, which means that we assume the relationship between context and sense can be generalized in continuous fashion. [sent-49, score-0.435]
</p><p>16 According to the nature of continuity, once given a hypothesis hij for a certain word, we can extrapolate the hypothesis for another word of another sense hi′j′ = (xi′ , si′j′) sufficiently close to hij. [sent-53, score-0.931]
</p><p>17 They control the∈ smoothing intensity in context and in sense, respectively. [sent-55, score-0.282]
</p><p>18 Our objective is to determine the optimal sense for all the target words simultaneously. [sent-56, score-0.402]
</p><p>19 We relax the  integer constraints by introducing a sense probability parameter πij corresponding to each hij. [sent-58, score-0.354]
</p><p>20 πij denotes the probability by which hij is true. [sent-59, score-0.486]
</p><p>21 The probabilit∑y density extrapolated a ≤t hi′j′ by a probabilistic hypothesis hij is given as follows: Qij (hi′j′)  ∝  πij  K(hij , hi′j′) . [sent-61, score-0.694]
</p><p>22 Due to the limitation of drawing, both the context metric space and the sense metric space are drawn schematically as 1-dimensional spaces (axes), actually arbitrary metric spaces similarity-based or feature-based are applicable. [sent-63, score-0.713]
</p><p>23 The product metric space of the context metric space and the sense metric space composes a hypothesis space. [sent-64, score-0.817]
</p><p>24 In the hypothesis space, n sense hypotheses for a certain word is represented as n points on the hyperplane that spreads across the sense metric space. [sent-65, score-0.9]
</p><p>25 The two small circles in the middle of the figure represent the two sense hypotheses for a single word. [sent-66, score-0.445]
</p><p>26 The position of a hypothesis represents  which sense is assigned to the current word in 885  "Invasive, exotic plants cause particular problems for wildlife. [sent-67, score-0.44]
</p><p>27 In  accordance with geometric intuition, intensity of extrapolation is affected by the distance from a hypothesis, and by the probability of the hypothesis by itself. [sent-73, score-0.277]
</p><p>28 Extrapolated probability density is represented by shadow thickness and surface height. [sent-74, score-0.259]
</p><p>29 If there is another word in nearby context, the kernels can validate the sense of that word. [sent-75, score-0.505]
</p><p>30 In the figure, there are two kernels in the context “Invasive, exotic . [sent-76, score-0.229]
</p><p>31 They are two competing hypothesis for the senses decoy and flora of the word plants. [sent-80, score-0.201]
</p><p>32 These kernels affect the senses of another ambiguous word tree in nearby context “Exotic . [sent-81, score-0.419]
</p><p>33 ”, and extrapolate the most at the sense t ree nearby flora. [sent-84, score-0.486]
</p><p>34 It affects little to the word far away in context or in sense as is the case for the background word in the figure. [sent-86, score-0.397]
</p><p>35 Wider bandwidths bring stronger effect of generalization to further hypotheses, but too wide bandwidths smooth out detailed structure. [sent-88, score-0.43]
</p><p>36 The bandwidths are the key for disambiguation, therefore they are to be optimized on a dataset together with sense probabilities. [sent-89, score-0.563]
</p><p>37 3  Simultaneous Optimization of All-words WSD  Given the smoothing model to extrapolate the senses of other words, we now make its instances interact to obtain the optimal combination of senses for all the words. [sent-90, score-0.527]
</p><p>38 The parameters consist of a context bandwidth σx, a sense bandwidth σs, and sense probabilities πij for all i and j. [sent-93, score-0.874]
</p><p>39 For convenience of description, the sense probabilities are all together denoted as a vector π = (. [sent-94, score-0.347]
</p><p>40 We consider all the mappings from context to sense are latent, and find the optimal parameters by maximizing marginal pseudo likelihood based on probability density. [sent-102, score-0.592]
</p><p>41 The likelihood is defined as follows:  L(π,σx,σs;X)  ≡  ln∏∑πijQ(hij), ∏i  (3)  ∑j  where ∏i denotes the product over xi ∈ X, ∑j denotes∏ the summation over all possible sen∑ses sij ∈ Si for the current i-th context. [sent-103, score-0.266]
</p><p>42 W∑e take as the unit of LOOCV not a wor∈d i Snstance∑ but a word type, because the instances of the same word type invariably have the same sense candidates, which still  cause over-fitting when optimizing the sense bandwidth. [sent-107, score-0.618]
</p><p>43 (6)  When we optimize the parameters, the first term of Equation (6) in the right-hand side acts to reinforce nearby hypotheses among different words, whereas the second term acts to suppress conflicting hypotheses of the same word. [sent-113, score-0.48]
</p><p>44 (10)  Qi′j′ (hij) denotes the∑ probability density at hij extrapolated by hi′j′ alone, defined as follows: Qi′j′(hij)  ≡  N −1 Niπi′j′K(hij,hi′j′). [sent-116, score-0.727]
</p><p>45 The right term requires πij to agree with the ratio of responsibility of hij to the whole. [sent-120, score-0.45]
</p><p>46 As for sense probabilities, we set the uniform probability in accordance with the number of sense candidates, thereby πij ← |Si |−1, where |Si | denotes the size of Si. [sent-128, score-0.717]
</p><p>47 |ASs for, bandwidths, we set the mean squared distance in each metric; thereby σx2 ← N−1 ∑i,i′ dx2(xi, xi′) for context ban∑dwidth, and σs2 ←  (∑i|Si|)−1 ∑i,i′ ∑j,j′ ds2(sij,si′j′)  s∑ense ba|)ndw∑idth. [sent-129, score-0.183]
</p><p>48 The sense hypotheses are depicted by twelve upward arrows. [sent-141, score-0.555]
</p><p>49 Through the iterative parameter update, sense probabilities and kernel bandwidths were optimized to the dataset. [sent-144, score-0.562]
</p><p>50 Figure 2(a) illustrates the initial status, where all the sense hypothesis are equivalently probable, thus they are in the most ambiguous status. [sent-145, score-0.375]
</p><p>51 Initial bandwidths are set to the  mean squared distance of all the hypotheses pairs, 887  Figure  2:  Pseudo 2D data simulation to visualize the dynamics of the proposed simultaneous  WSD with ambiguous  five words and twelve sense hypotheses. [sent-146, score-0.902]
</p><p>52 This is because our method is aiming at modeling not the disambiguation of clustermemberships but the disambiguation of senses for each word. [sent-159, score-0.393]
</p><p>53 The context of word instances are tied to the distributional context of the word type in a large corpus. [sent-164, score-0.277]
</p><p>54 To calculate sense similarities, we used the WordNet similarity package by Pedersen et al. [sent-165, score-0.366]
</p><p>55 1, to obtain grammatical relations for the distributional similarity, as well as to obtain lemmata and part-of-speech (POS) tags which are required to look up the sense inventory of WordNet. [sent-174, score-0.41]
</p><p>56 Based on the distributional similarity, we just used k-nearest neighbor words as the context of each target word. [sent-175, score-0.272]
</p><p>57 To treat the above similarity functions of context and of sense as distance functions, we use the conversion: d(·, ·) ≡ −α ln(f(·, ·)/fmax), where cdo dnveneorsteiosn t:he d objective αdilstna(nfc(e· function, i. [sent-180, score-0.501]
</p><p>58 , dx for context and ds for sense, while f and fmax denote the original similarity function and its maximum, respectively. [sent-182, score-0.215]
</p><p>59 5  Evaluation  To confirm the effect of the proposed smoothing model and its combinatorial optimization scheme, we conducted WSD evaluations. [sent-185, score-0.326]
</p><p>60 For this reason, we evaluated all the sense probabilities as they were. [sent-200, score-0.347]
</p><p>61 The context metric space was composed by knearest neighbor words of distributional similarity (Lin, 1998), as is described in Section 4. [sent-203, score-0.403]
</p><p>62 Aluas efodr sense m 3,e 5tr,ic 10 space, we 0e,v 1a0lu0,a2t0e0d, t 3w0o0 measures i. [sent-205, score-0.309]
</p><p>63 In every condition, stopping criterion of iteration is always the number of iteration (500 times), irrespective of the convergence in likelihood. [sent-208, score-0.198]
</p><p>64 (2004), which determines the word sense based on sense similarity and distributional similarity to the k-nearest neighbor words of a target word by distributional similarity. [sent-212, score-1.017]
</p><p>65 Our major advantage is the combinatorial optimization framework, while the conventional one employs word-by-word scheme. [sent-213, score-0.214]
</p><p>66 (2007), which determines the word sense by maximizing the sum of sense similarity to the k immediate neighbor words of a target word. [sent-215, score-0.821]
</p><p>67 Our major advantages are the combinatorial optimization scheme and the smoothing model to integrate distributional similarity. [sent-219, score-0.425]
</p><p>68 Thus we can conclude that, though significance depends on metrics, our smoothing model and the optimization scheme are effective to improve accuracies. [sent-293, score-0.248]
</p><p>69 Let us start by looking at the upper half of Figure 5, which shows the change of sense probabilities through iteration. [sent-314, score-0.387]
</p><p>70 As iteration proceeded, the probabilities gradually spread out to either side of 1 or 0, and finally at iteration  500, we can observe that almost all the words were clearly disambiguated. [sent-316, score-0.236]
</p><p>71 Vertical axis on the left is for the sense bandwidth, and on the right is for the context bandwidth. [sent-318, score-0.397]
</p><p>72 We can observe those bandwidths became narrower as iteration proceeded. [sent-319, score-0.314]
</p><p>73 Intensity of smoothing was dynamically adjusted by the whole disambiguation status. [sent-320, score-0.28]
</p><p>74 6  Discussion  This section discusses the validity of the proposed method as to i) sense-interdependent disambiguation and ii) reliability of data smoothing. [sent-322, score-0.197]
</p><p>75 This means that the ranks of sense candidates for each word were frequently altered through iteration, which further means that some new information not obtained earlier was delivered one after another to sense disambiguation for each word. [sent-331, score-0.747]
</p><p>76 From these results, we could confirm the expected sense-interdependency effect that a sense disambiguation of certain word affected to other words. [sent-332, score-0.483]
</p><p>77 2 Reliability of Smoothing as Supervision Let us now discuss the reliability of our smoothing model. [sent-334, score-0.219]
</p><p>78 In our method, sense disambiguation of a word is guided by its nearby words’ extrapolation  (smoothing). [sent-335, score-0.634]
</p><p>79 If we take sufficient number of random words as nearby words, the sense distribution comes close to the true distribution, and then we expect the statistically true sense distribution should find out the true sense of the target word, according to the distributional hypotheses (Harris, 1954). [sent-338, score-1.328]
</p><p>80 On the contrary, if we take nearby words that are biased to particular words, the sense distribution also becomes biased, and the extrapolation becomes less reliable. [sent-339, score-0.505]
</p><p>81 We can compute the randomness of words that affect for sense disambiguation, by word perplexity. [sent-340, score-0.309]
</p><p>82 T Hhe| condit∑ional probability p(w′|w) denotes the probability with which a cert|wai)n dwenoordte sw ′t ∈ oVb \ {w} determines the sense of w, whi∈ch can {bew }def dienteedrm as sthe t d∑ensity r oaftio: w p(w′|w) ∝  ∑i:wi=w ∑i′:wi′=w′ ∑j,j′ Qi′j′(hij). [sent-344, score-0.488]
</p><p>83 The rela∑tion betwee∑n worQd perplexity and probability change for ground-truth senses of nouns (JCN/k = 30) is shown in Figure 7. [sent-345, score-0.274]
</p><p>84 The upper histogram shows the change in iteration 1-100, and the lower shows that of iteration 101-500. [sent-346, score-0.238]
</p><p>85 We divide the analysis at iteration 100, because roughly until the 100th iteration, the change in bandwidths converged, and the number of words to interact settled, as can be seen in Figure 5. [sent-347, score-0.354]
</p><p>86 I onc contrast, argte tlhye ( l7o9w%er) tloef tth oef c tohrefigure, where perplexity is small (< 30) and bandwidths has been narrowed at iteration 101-500, correct change occupied only 32% of the whole. [sent-353, score-0.408]
</p><p>87 Therefore, we can conclude that if sufficiently random samples of nearby words are provided, our smoothing model is reliable, though it is trained in an unsupervised fashion. [sent-354, score-0.353]
</p><p>88 knowledge-based approaches typically regard senses as vertices (see Section 1), and corpusbased approaches such as (V´ eronis, 2004) regard words as vertices or (Niu et al. [sent-358, score-0.275]
</p><p>89 Mihalcea (2005) proposed graph-based methods, whose vertices are sense label hypotheses on word sequence. [sent-361, score-0.515]
</p><p>90 They disambiguated each target word using its distributionally similar words instead of its immediate context words. [sent-374, score-0.195]
</p><p>91 Second, we extend the definition of density from Euclidean distance to general metric, which makes the proposed method applicable to a wide variety of corpus-based context similarities and dictionarybased sense similarities. [sent-379, score-0.577]
</p><p>92 8  Conclusions  We proposed a novel smoothing model with a combinatorial optimization scheme for all-words WSD from untagged corpora. [sent-380, score-0.384]
</p><p>93 Moreover, our smoothing model, though unsupervised, provides reliable supervision when sufficiently random samples of words are available as nearby words. [sent-383, score-0.317]
</p><p>94 Thus it was confirmed that this method is valid for finding the optimal combination of word senses with large untagged corpora. [sent-384, score-0.244]
</p><p>95 Knowledgebased wsd on specific domains: performing better than generic supervised wsd. [sent-396, score-0.188]
</p><p>96 Semeval-2010 task 17: All-words word sense disambiguation on a specific domain. [sent-400, score-0.438]
</p><p>97 Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. [sent-428, score-0.438]
</p><p>98 Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. [sent-444, score-0.438]
</p><p>99 Word sense disambiguation using label propagation based semi-supervised learning. [sent-456, score-0.438]
</p><p>100 UMND1 : Unsupervised word sense disambiguation using contextual semantic relatedness. [sent-464, score-0.438]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hij', 0.387), ('sense', 0.309), ('bandwidths', 0.215), ('wsd', 0.188), ('ij', 0.188), ('smoothing', 0.151), ('jcn', 0.151), ('rii', 0.151), ('agirre', 0.144), ('hypotheses', 0.136), ('senses', 0.135), ('density', 0.133), ('disambiguation', 0.129), ('nearby', 0.12), ('recalls', 0.116), ('extrapolated', 0.108), ('lesk', 0.101), ('distributional', 0.101), ('iteration', 0.099), ('context', 0.088), ('hi', 0.087), ('dynamics', 0.084), ('conventional', 0.084), ('metric', 0.08), ('combinatorial', 0.076), ('xi', 0.076), ('extrapolation', 0.076), ('kernels', 0.076), ('soroa', 0.075), ('dx', 0.07), ('tran', 0.07), ('vertices', 0.07), ('reliability', 0.068), ('mccarthy', 0.068), ('hypothesis', 0.066), ('bandwidth', 0.065), ('exotic', 0.065), ('mfs', 0.065), ('pknn', 0.065), ('immediate', 0.063), ('twelve', 0.063), ('responsibility', 0.063), ('wi', 0.062), ('untagged', 0.06), ('eneko', 0.058), ('similarity', 0.057), ('lacalle', 0.057), ('oier', 0.057), ('continuity', 0.057), ('extrapolate', 0.057), ('knowledgebased', 0.057), ('arrow', 0.056), ('denotes', 0.054), ('optimization', 0.054), ('pseudo', 0.054), ('si', 0.054), ('perplexity', 0.054), ('status', 0.053), ('gaussian', 0.05), ('optimal', 0.049), ('squared', 0.048), ('equation', 0.048), ('likelihood', 0.047), ('summation', 0.047), ('upward', 0.047), ('distance', 0.047), ('sufficiently', 0.046), ('conflicting', 0.045), ('probability', 0.045), ('confirm', 0.045), ('target', 0.044), ('reinforce', 0.043), ('intensity', 0.043), ('scheme', 0.043), ('invasive', 0.043), ('loocv', 0.043), ('parzen', 0.043), ('pknnrii', 0.043), ('thickness', 0.043), ('sij', 0.042), ('kulkarni', 0.042), ('intended', 0.042), ('change', 0.04), ('dataset', 0.039), ('neighbor', 0.039), ('space', 0.038), ('shadow', 0.038), ('responsibilities', 0.038), ('cwor', 0.038), ('lkb', 0.038), ('probabilities', 0.038), ('predominant', 0.037), ('unsupervised', 0.036), ('aitor', 0.036), ('lopez', 0.036), ('patwardhan', 0.035), ('poss', 0.035), ('sthe', 0.035), ('official', 0.035), ('navigli', 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="111-tfidf-1" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>2 0.27855733 <a title="111-tfidf-2" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>3 0.26304674 <a title="111-tfidf-3" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>Author: Tristan Miller ; Nicolai Erbs ; Hans-Peter Zorn ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</p><p>4 0.21892568 <a title="111-tfidf-4" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>5 0.16620719 <a title="111-tfidf-5" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>Author: Marine Carpuat ; Hal Daume III ; Katharine Henry ; Ann Irvine ; Jagadeesh Jagarlamudi ; Rachel Rudinger</p><p>Abstract: Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</p><p>6 0.12301037 <a title="111-tfidf-6" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>7 0.11508482 <a title="111-tfidf-7" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>8 0.11012921 <a title="111-tfidf-8" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>9 0.11009402 <a title="111-tfidf-9" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>10 0.10458881 <a title="111-tfidf-10" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>11 0.10385488 <a title="111-tfidf-11" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>12 0.099779323 <a title="111-tfidf-12" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>13 0.094884656 <a title="111-tfidf-13" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>14 0.08849749 <a title="111-tfidf-14" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>15 0.084745616 <a title="111-tfidf-15" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>16 0.083998412 <a title="111-tfidf-16" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>17 0.083977029 <a title="111-tfidf-17" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>18 0.082207121 <a title="111-tfidf-18" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>19 0.082084298 <a title="111-tfidf-19" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>20 0.078188457 <a title="111-tfidf-20" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, 0.051), (2, 0.053), (3, -0.156), (4, -0.04), (5, -0.19), (6, -0.136), (7, 0.096), (8, -0.016), (9, -0.05), (10, 0.016), (11, 0.088), (12, -0.111), (13, -0.133), (14, 0.137), (15, 0.045), (16, 0.003), (17, 0.09), (18, -0.042), (19, -0.008), (20, 0.074), (21, -0.065), (22, 0.071), (23, -0.023), (24, -0.044), (25, -0.138), (26, 0.074), (27, -0.009), (28, -0.019), (29, -0.069), (30, 0.103), (31, 0.051), (32, -0.023), (33, 0.092), (34, -0.044), (35, -0.055), (36, 0.046), (37, -0.0), (38, -0.069), (39, -0.015), (40, -0.026), (41, -0.044), (42, -0.027), (43, -0.088), (44, 0.067), (45, -0.01), (46, 0.023), (47, 0.08), (48, -0.075), (49, -0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96453673 <a title="111-lsi-1" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>2 0.89593685 <a title="111-lsi-2" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>3 0.86256671 <a title="111-lsi-3" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>Author: Tristan Miller ; Nicolai Erbs ; Hans-Peter Zorn ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</p><p>4 0.81436294 <a title="111-lsi-4" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>5 0.76988095 <a title="111-lsi-5" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>6 0.75028539 <a title="111-lsi-6" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>7 0.65866947 <a title="111-lsi-7" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>8 0.62066817 <a title="111-lsi-8" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>9 0.61226761 <a title="111-lsi-9" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>10 0.58607203 <a title="111-lsi-10" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>11 0.55319262 <a title="111-lsi-11" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>12 0.52975786 <a title="111-lsi-12" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>13 0.52921349 <a title="111-lsi-13" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>14 0.51802599 <a title="111-lsi-14" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>15 0.50948292 <a title="111-lsi-15" href="./acl-2013-Offspring_from_Reproduction_Problems%3A_What_Replication_Failure_Teaches_Us.html">262 acl-2013-Offspring from Reproduction Problems: What Replication Failure Teaches Us</a></p>
<p>16 0.50350338 <a title="111-lsi-16" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>17 0.49469891 <a title="111-lsi-17" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>18 0.47212148 <a title="111-lsi-18" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>19 0.46803024 <a title="111-lsi-19" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>20 0.46493846 <a title="111-lsi-20" href="./acl-2013-DKPro_Similarity%3A_An_Open_Source_Framework_for_Text_Similarity.html">104 acl-2013-DKPro Similarity: An Open Source Framework for Text Similarity</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.085), (6, 0.031), (11, 0.07), (17, 0.06), (24, 0.029), (26, 0.027), (28, 0.014), (35, 0.074), (42, 0.03), (48, 0.058), (56, 0.013), (64, 0.018), (70, 0.046), (88, 0.218), (90, 0.038), (95, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94686753 <a title="111-lda-1" href="./acl-2013-Sorani_Kurdish_versus_Kurmanji_Kurdish%3A_An_Empirical_Comparison.html">327 acl-2013-Sorani Kurdish versus Kurmanji Kurdish: An Empirical Comparison</a></p>
<p>Author: Kyumars Sheykh Esmaili ; Shahin Salavati</p><p>Abstract: Resource scarcity along with diversity– both in dialect and script–are the two primary challenges in Kurdish language processing. In this paper we aim at addressing these two problems by (i) building a text corpus for Sorani and Kurmanji, the two main dialects of Kurdish, and (ii) highlighting some of the orthographic, phonological, and morphological differences between these two dialects from statistical and rule-based perspectives.</p><p>2 0.93780816 <a title="111-lda-2" href="./acl-2013-Evaluating_a_City_Exploration_Dialogue_System_with_Integrated_Question-Answering_and_Pedestrian_Navigation.html">141 acl-2013-Evaluating a City Exploration Dialogue System with Integrated Question-Answering and Pedestrian Navigation</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon ; Phil Bartie ; Tiphaine Dalmas ; Anna Dickinson ; Xingkun Liu ; William Mackaness ; Bonnie Webber</p><p>Abstract: We present a city navigation and tourist information mobile dialogue app with integrated question-answering (QA) and geographic information system (GIS) modules that helps pedestrian users to navigate in and learn about urban environments. In contrast to existing mobile apps which treat these problems independently, our Android app addresses the problem of navigation and touristic questionanswering in an integrated fashion using a shared dialogue context. We evaluated our system in comparison with Samsung S-Voice (which interfaces to Google navigation and Google search) with 17 users and found that users judged our system to be significantly more interesting to interact with and learn from. They also rated our system above Google search (with the Samsung S-Voice interface) for tourist information tasks.</p><p>3 0.93371451 <a title="111-lda-3" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>Author: Greg Durrett ; David Hall ; Dan Klein</p><p>Abstract: Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.</p><p>4 0.91820848 <a title="111-lda-4" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>Author: Or Biran ; Kathleen McKeown</p><p>Abstract: We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation.</p><p>5 0.9171865 <a title="111-lda-5" href="./acl-2013-Reconstructing_an_Indo-European_Family_Tree_from_Non-native_English_Texts.html">299 acl-2013-Reconstructing an Indo-European Family Tree from Non-native English Texts</a></p>
<p>Author: Ryo Nagata ; Edward Whittaker</p><p>Abstract: Mother tongue interference is the phenomenon where linguistic systems of a mother tongue are transferred to another language. Although there has been plenty of work on mother tongue interference, very little is known about how strongly it is transferred to another language and about what relation there is across mother tongues. To address these questions, this paper explores and visualizes mother tongue interference preserved in English texts written by Indo-European language speakers. This paper further explores linguistic features that explain why certain relations are preserved in English writing, and which contribute to related tasks such as native language identification.</p><p>6 0.90831047 <a title="111-lda-6" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>same-paper 7 0.89320922 <a title="111-lda-7" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>8 0.88563341 <a title="111-lda-8" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>9 0.8174237 <a title="111-lda-9" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>10 0.79722261 <a title="111-lda-10" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>11 0.76658732 <a title="111-lda-11" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>12 0.74517196 <a title="111-lda-12" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>13 0.74248713 <a title="111-lda-13" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>14 0.7407918 <a title="111-lda-14" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>15 0.73621154 <a title="111-lda-15" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>16 0.73063326 <a title="111-lda-16" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>17 0.7251209 <a title="111-lda-17" href="./acl-2013-Question_Classification_Transfer.html">292 acl-2013-Question Classification Transfer</a></p>
<p>18 0.71799272 <a title="111-lda-18" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>19 0.71727318 <a title="111-lda-19" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>20 0.71503526 <a title="111-lda-20" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
