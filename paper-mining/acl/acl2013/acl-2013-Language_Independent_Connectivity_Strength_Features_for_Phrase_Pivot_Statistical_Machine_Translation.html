<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-214" href="#">acl2013-214</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2013-214-pdf" href="http://aclweb.org/anthology//P/P13/P13-2073.pdf">pdf</a></p><p>Author: Ahmed El Kholy ; Nizar Habash ; Gregor Leusch ; Evgeny Matusov ; Hassan Sawaf</p><p>Abstract: An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.</p><p>Reference: <a title="acl-2013-214-reference" href="../acl2013_reference/acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. [sent-8, score-0.181]
</p><p>2 One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. [sent-9, score-0.591]
</p><p>3 Although pivoting is a robust technique, it introduces some low quality translations. [sent-10, score-0.443]
</p><p>4 In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. [sent-11, score-0.049]
</p><p>5 The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between  the source and target phrases in the pivot phrase table. [sent-12, score-1.469]
</p><p>6 1 Introduction One of the main issues in statistical machine translation (SMT) is the scarcity of parallel data for many language pairs especially when the source and target languages are morphologically rich. [sent-15, score-0.348]
</p><p>7 A common SMT solution to the lack of parallel data is to pivot the translation through a third language (called pivot or bridge language) for which there exist abundant parallel corpora with the source and target languages. [sent-16, score-1.204]
</p><p>8 One of the best performing techniques, phrase pivoting (Utiyama and Isahara, 2007), builds an induced new phrase table between the source and target. [sent-18, score-0.907]
</p><p>9 One of the main issues of this technique is that the size of the newly created pivot phrase table is very large (Utiyama and Isahara, 2007). [sent-19, score-0.686]
</p><p>10 Moreover, many of the produced  phrase pairs are of low quality which affects the translation choices during decoding and the overall translation quality. [sent-20, score-0.446]
</p><p>11 In this paper, we introduce language independent features to determine the quality of the pivot phrase pairs between source and target. [sent-21, score-0.748]
</p><p>12 We then review two common pivoting strategies and how we use them in Section 3. [sent-25, score-0.452]
</p><p>13 This is followed by our approach to using connectivity strength features in Section 4. [sent-26, score-0.326]
</p><p>14 2  Related Work  Many researchers have investigated the use of pivoting (or bridging) approaches to solve the data scarcity issue (Utiyama and Isahara, 2007; Wu and Wang, 2009; Khalilov et al. [sent-28, score-0.444]
</p><p>15 The main idea is to introduce a pivot language, for which there exist large source-pivot and pivot-target bilingual cor-  pora. [sent-31, score-0.461]
</p><p>16 Many different pivot strategies have been presented in the literature. [sent-35, score-0.496]
</p><p>17 The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language, and then translate the pivot language sentence to the target language 412  ProceedingSsof oifa, th Beu 5l1gsarti Aan,An uuaglu Mste 4e-ti9n2g 0 o1f3 t. [sent-37, score-1.227]
</p><p>18 The second strategy is based on phrase pivoting (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). [sent-41, score-0.613]
</p><p>19 In phrase pivoting, a new source-target phrase table (translation model) is induced from source-pivot and pivottarget phrase tables. [sent-42, score-0.644]
</p><p>20 Lexical weights and translation probabilities are computed from the two translation models. [sent-43, score-0.278]
</p><p>21 The third strategy is to create a synthetic source-  target corpus by translating the pivot side of source-pivot corpus to the target language using an existing pivot-target model (Bertoldi et al. [sent-44, score-0.557]
</p><p>22 In this paper, we build on the phrase pivoting approach, which has been shown to be the best with comparable settings (Utiyama and Isahara, 2007). [sent-46, score-0.613]
</p><p>23 We extend phrase table scores with two other features that are language independent. [sent-47, score-0.219]
</p><p>24 Since both Persian and Arabic are morphologically rich, we should mention that there has been a lot of work on translation to and from morphologically rich languages (Yeniterzi and Oflazer, 2010; Elming and Habash, 2009; El Kholy and Habash, 2010a; Habash and Sadat, 2006; Kathol and Zheng, 2008). [sent-48, score-0.212]
</p><p>25 Most of these efforts are focused on syntactic and morphological processing to improve the quality of translation. [sent-49, score-0.072]
</p><p>26 The only effort that we are aware of is based on improving the reordering models for PersianArabic SMT (Matusov and K ¨opr¨ u, 2010). [sent-51, score-0.054]
</p><p>27 3 Pivoting Strategies In this section, we review the two pivoting strategies that are our baselines. [sent-52, score-0.452]
</p><p>28 We also discuss how we overcome the large expansion of source-totarget phrase pairs in the process of creating a pivot phrase table. [sent-53, score-0.853]
</p><p>29 Given a Persian sentence, we first translate the Persian sentence from Persian to English, and then from English to Arabic. [sent-56, score-0.037]
</p><p>30 2 Phrase Pivoting In phrase pivoting (sometimes called triangulation or phrase table multiplication), we train a Persianto-Arabic and an English-Arabic translation models, such as those used in the sentence pivoting technique. [sent-58, score-1.382]
</p><p>31 Based on these two models, we induce a new Persian-Arabic translation model. [sent-59, score-0.112]
</p><p>32 , 2007), we need to provide the same set of phrase translation probability distributions. [sent-61, score-0.308]
</p><p>33 The following are the set of equations used to compute the lexical probabilities (φ) and the phrase probabilities (pw)  φ(f|a) = Pφ(f|e)φ(e|a) φ(a|f) = PPeφ(a|e)φ(e|f) pw(f|a) =PPepw(f|e)pw(e|a) pw(a|f) =PPepw(a|e)pw(e|f) Pe  where f is the PePrsian source phrase. [sent-63, score-0.282]
</p><p>34 e is the English pivot phrase that is common in both Persian-English translation model and EnglishArabic translation model. [sent-64, score-0.881]
</p><p>35 We also build a Persian-Arabic reordering table using the same technique but we compute the reordering weights in a similar manner to Henriquez  et al. [sent-66, score-0.169]
</p><p>36 As discussed earlier, the induced PersianArabic phrase and reordering tables are very large. [sent-68, score-0.328]
</p><p>37 Table 1 shows the amount of parallel corpora used to train the Persian-English and the EnglishArabic and the equivalent phrase table sizes compared to the induced Persian-Arabic phrase table. [sent-69, score-0.488]
</p><p>38 2 We introduce a basic filtering technique discussed next to address this issue and present some baseline experiments to test its performance in Section 5. [sent-70, score-0.107]
</p><p>39 2The size of the induced phrase table size is computed but  not created. [sent-74, score-0.252]
</p><p>40 To select the top candidates, we first rank all the candidates based on the log linear scores computed from the phrase translation probabilities and lexical weights multiplied by the optimized decoding weights then we pick the top [n] pairs. [sent-79, score-0.422]
</p><p>41 We compare the different pivoting strategies and various filtering thresholds in Section 5. [sent-80, score-0.508]
</p><p>42 4  Approach  One of the main challenges in phrase pivoting is the very large size of the induced phrase table. [sent-82, score-0.865]
</p><p>43 It becomes even more challenging if either the source or target language is morphologically rich. [sent-83, score-0.14]
</p><p>44 The number of translation candidates (fanout) increases due to ambiguity and richness (discussed  in more details in Section 5. [sent-84, score-0.14]
</p><p>45 2) which in return increases the number of combinations between source and target phrases. [sent-85, score-0.09]
</p><p>46 Since the only criteria of matching between the source and target phrase is through a pivot phrase, many of the induced phrase pairs are of low quality. [sent-86, score-0.999]
</p><p>47 These phrase pairs unnecessarily increase the search space and hurt the overall quality of translation. [sent-87, score-0.222]
</p><p>48 To solve this problem, we introduce two language-independent features which are added to the log linear space of features in order to determine the quality of the pivot phrase pairs. [sent-88, score-0.729]
</p><p>49 They depend on the number of alignment links between words in the source phrase to words of the target phrase. [sent-92, score-0.31]
</p><p>50 SCS and TSC are defined in equations 1 and 2 where S = {i : 1 ≤ i ≤ S} is the set osf 1 source wwoherdrse iSn a given phrase pair i ins tthhee  pivot phrase table and T = {j : 1 ≤ j ≤ T} ipsi othte set oef athblee equivalent target 1w ≤ord js. [sent-93, score-0.943]
</p><p>51 T The} word alignment between S and T is defined as A = {(i, j) : i∈ S and j ∈ T }. [sent-94, score-0.024]
</p><p>52 SCS =||AS||  (1)  TCS =||AT ||  (2)  We get the alignment links by projecting the alignments of source-pivot to the pivot-target phrase pairs used in pivoting. [sent-95, score-0.22]
</p><p>53 If the source-target phrase pair are connected through more than one pivot phrase, we take the union of the alignments. [sent-96, score-0.689]
</p><p>54 In contrast to the aggregated values represented in the lexical weights and the phrase probabilities, connectivity strength features provide additional information by counting the actual links between the source and target phrases. [sent-97, score-0.644]
</p><p>55 They provide an independent and direct approach to measure how good or bad a given phrase pair are connected. [sent-98, score-0.219]
</p><p>56 Figure 1and 2 are two examples (one good, one bad) Persian-Arabic phrase pairs in a pivot phrase table induced by pivoting through English. [sent-99, score-1.326]
</p><p>57 The meaning is preserved in both phrases which is reflected in the SCS and TCS scores. [sent-101, score-0.024]
</p><p>58 In the second example, only one Persian word in aligned to one Arabic word in the equivalent phrase and the two phrases conveys two different meanings. [sent-102, score-0.196]
</p><p>59 The English phrase is not a good translation for either, which leads to this bad pairing. [sent-103, score-0.331]
</p><p>60 5 Experiments In this section, we present a set of baseline experiments including a simple filtering technique to overcome the huge expansion of the pivot phrase table. [sent-105, score-0.742]
</p><p>61 Then we present our results in using connectivity strength features to improve Persian-Arabic pivot translation quality. [sent-106, score-0.899]
</p><p>62 414  Persian:  AςtmAd myAn dw kšwr  " ""  ""  English:  Alθqħ byn Aldwltyn  ‫دو‬  ‫’ ا%$#"د )("ن‬  ‘trust"between"the"two"countries’  trust between the two countries  Arabic:"  ‘‫. [sent-109, score-0.074]
</p><p>63 -,ر‬  ‘3$2‫ا25و‬  34  /012‫’ا‬  ‘the trust between the two countries’  Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. [sent-110, score-0.263]
</p><p>64 All Persian words are connected to one or more Arabic words. [sent-111, score-0.032]
</p><p>65 +ک‬  ‘&<=>‫’321 ",+5"ت ا98"و6ت;: ا‬ ‘Some construcBon companies in the country’  Figure 2: An example of weakly connected Persian-Arabic phrase pairs through English. [sent-116, score-0.254]
</p><p>66 Only one Persian word is connected to an Arabic word. [sent-117, score-0.032]
</p><p>67 we  build  two SMT  One model to translate from Persian to  English and another model to translate from English to Arabic. [sent-123, score-0.074]
</p><p>68 aWileuse an in-house Persian-English parallel corpus of about 170K sentences and 4M words. [sent-127, score-0.04]
</p><p>69 Word alignment is done using GIZA++ (Och and Ney, 2003). [sent-128, score-0.024]
</p><p>70 We use 5-grams  for all language models (LMs) implemented using the SRILM toolkit (Stolcke, 2002). [sent-130, score-0.024]
</p><p>71 For English language modeling, we use English Gigaword Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011). [sent-131, score-0.024]
</p><p>72 For Persian-English translation model, weights are optimized using a set 1000 sentences randomly sampled from the parallel corpus while the English-Arabic translation model weights are optimized using a set of 500 sentences from the 2004 NIST MT evaluation test set (MT04). [sent-137, score-0.384]
</p><p>73 The optimized weights are used for ranking and filtering (discussed in Section 3. [sent-138, score-0.116]
</p><p>74 We use a maximum phrase length of size 8  across all models. [sent-140, score-0.196]
</p><p>75 2 Linguistic Preprocessing In this section we present our motivation and choice for preprocessing Arabic, Persian, English data. [sent-145, score-0.027]
</p><p>76 Both Arabic and Persian are morphologically complex languages but they belong to two different language families. [sent-146, score-0.05]
</p><p>77 They both express richness and linguistic complexities in different ways. [sent-147, score-0.028]
</p><p>78 One aspect of Arabic’s complexity is its various attachable clitics and numerous morphological features (Habash, 2010). [sent-148, score-0.069]
</p><p>79 We follow El Kholy and Habash (2010a) and use the PATB tokenization scheme (Maamouri et al. [sent-149, score-0.036]
</p><p>80 Unlike Arabic, Persian shows only two values for number, just singular and plural (no dual), which are usually marked by either the suffix Aº+ +hA and sometimes ? [sent-157, score-0.024]
</p><p>81 A verb in Persian inflects for 14 different tense, mood, aspect, person, number and voice combination values (Rasooli et al. [sent-162, score-0.039]
</p><p>82 English, our pivot language, is quite different from both Arabic and Persian. [sent-166, score-0.461]
</p><p>83 English is poor in morphology and barely inflects for number and tense, and for person in a limited context. [sent-167, score-0.066]
</p><p>84 English  preprocessing simply includes down-casing, separating punctuation and splitting off “’s”. [sent-168, score-0.027]
</p><p>85 3  Baseline Evaluation  We compare the performance of sentence pivoting against phrase pivoting with different filtering thresholds. [sent-170, score-1.086]
</p><p>86 In general, the phrase pivoting outperforms the sentence pivoting even when we use a small filtering threshold of size 100. [sent-172, score-1.086]
</p><p>87 4 Connectivity Strength Features Evaluation In this experiment, we test the performance of adding the connectivity strength features (+Conn) to the best performing phrase pivoting model (Phrase Pivot F1K). [sent-179, score-0.939]
</p><p>88 5 (BLEU/METEOR) points by adding othfe ≈ connectivity strength OfeRa-) tures. [sent-185, score-0.303]
</p><p>89 6  Conclusion and Future Work  We presented an experiment showing the effect of using two language independent features, source connectivity score and target connectivity score, to improve the quality of pivot-based SMT. [sent-188, score-0.522]
</p><p>90 We showed that these features help improving the overall translation quality. [sent-189, score-0.135]
</p><p>91 , the number of the pivot phases used in connecting the source and target phrase pair and the similarity between these pivot phrases. [sent-192, score-1.208]
</p><p>92 We also plan to explore language specific features which could be extracted from some seed parallel data, e. [sent-193, score-0.063]
</p><p>93 , syntactic and morphological compatibility of the source and target phrase pairs. [sent-195, score-0.332]
</p><p>94 Machine translation by triangulation: Making effective use of multi-parallel corpora. [sent-207, score-0.112]
</p><p>95 MADA+TOKAN: A toolkit for Arabic tokenization, diacritization, morphological disambiguation, POS tagging, stemming and lemmatization. [sent-248, score-0.07]
</p><p>96 Learning reordering models for statistical machine translation with a pivot language. [sent-267, score-0.656]
</p><p>97 Strategies for building a Farsi-English smt system from limited resources. [sent-275, score-0.067]
</p><p>98 A comparison of pivot methods for phrase-based statistical machine translation. [sent-341, score-0.49]
</p><p>99 Syntax-tomorphology mapping in factored phrase-based statistical machine translation from english to turkish. [sent-351, score-0.17]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pivot', 0.461), ('pivoting', 0.417), ('persian', 0.302), ('arabic', 0.29), ('habash', 0.227), ('connectivity', 0.203), ('phrase', 0.196), ('scs', 0.137), ('nizar', 0.136), ('tcs', 0.132), ('kholy', 0.117), ('translation', 0.112), ('englisharabic', 0.11), ('utiyama', 0.104), ('isahara', 0.102), ('strength', 0.1), ('smt', 0.067), ('evgeny', 0.066), ('induced', 0.056), ('filtering', 0.056), ('pw', 0.055), ('reordering', 0.054), ('khalilov', 0.054), ('rasooli', 0.054), ('matusov', 0.054), ('morphologically', 0.05), ('el', 0.05), ('target', 0.048), ('morphological', 0.046), ('elming', 0.044), ('henriquez', 0.044), ('jadidinejad', 0.044), ('persianarabic', 0.044), ('perstem', 0.044), ('ppepw', 0.044), ('soudi', 0.044), ('triangulation', 0.044), ('source', 0.042), ('parallel', 0.04), ('kathol', 0.039), ('inflects', 0.039), ('ebay', 0.039), ('sawaf', 0.039), ('yeniterzi', 0.039), ('countries', 0.039), ('translate', 0.037), ('koehn', 0.037), ('jos', 0.037), ('tokenization', 0.036), ('sadegh', 0.036), ('maamouri', 0.036), ('opr', 0.036), ('bertoldi', 0.035), ('strategies', 0.035), ('trust', 0.035), ('gregor', 0.034), ('ahmed', 0.034), ('connected', 0.032), ('weights', 0.032), ('mada', 0.032), ('moses', 0.032), ('iwslt', 0.032), ('banchs', 0.031), ('meteor', 0.03), ('kenlm', 0.03), ('catalog', 0.029), ('technique', 0.029), ('english', 0.029), ('statistical', 0.029), ('optimized', 0.028), ('denver', 0.028), ('richness', 0.028), ('preprocessing', 0.027), ('scarcity', 0.027), ('morphology', 0.027), ('bleu', 0.027), ('companies', 0.026), ('rafael', 0.026), ('corporation', 0.026), ('quality', 0.026), ('alexandra', 0.026), ('gigaword', 0.025), ('lavie', 0.025), ('carlos', 0.025), ('reflected', 0.024), ('plural', 0.024), ('haji', 0.024), ('alignment', 0.024), ('toolkit', 0.024), ('bad', 0.023), ('greece', 0.023), ('features', 0.023), ('owen', 0.023), ('rambow', 0.023), ('tense', 0.022), ('discussed', 0.022), ('companion', 0.022), ('orthographic', 0.022), ('jon', 0.022), ('probabilities', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="214-tfidf-1" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>Author: Ahmed El Kholy ; Nizar Habash ; Gregor Leusch ; Evgeny Matusov ; Hassan Sawaf</p><p>Abstract: An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.</p><p>2 0.39659992 <a title="214-tfidf-2" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>3 0.16571678 <a title="214-tfidf-3" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>4 0.16446467 <a title="214-tfidf-4" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>Author: Kareem Darwish</p><p>Abstract: Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and mi- croblogs test sets respectively.</p><p>5 0.14395323 <a title="214-tfidf-5" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<p>Author: Heba Elfardy ; Mona Diab</p><p>Abstract: This paper introduces a supervised approach for performing sentence level dialect identification between Modern Standard Arabic and Egyptian Dialectal Arabic. We use token level labels to derive sentence-level features. These features are then used with other core and meta features to train a generative classifier that predicts the correct label for each sentence in the given input text. The system achieves an accuracy of 85.5% on an Arabic online-commentary dataset outperforming a previously proposed approach achieving 80.9% and reflecting a significant gain over a majority baseline of 5 1.9% and two strong baseline systems of 78.5% and 80.4%, respectively.</p><p>6 0.13961697 <a title="214-tfidf-6" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>7 0.12753811 <a title="214-tfidf-7" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>8 0.11986456 <a title="214-tfidf-8" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>9 0.10355912 <a title="214-tfidf-9" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>10 0.10117523 <a title="214-tfidf-10" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>11 0.094065331 <a title="214-tfidf-11" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>12 0.093618736 <a title="214-tfidf-12" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>13 0.091690697 <a title="214-tfidf-13" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>14 0.091050796 <a title="214-tfidf-14" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>15 0.090601847 <a title="214-tfidf-15" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>16 0.086405501 <a title="214-tfidf-16" href="./acl-2013-ParaQuery%3A_Making_Sense_of_Paraphrase_Collections.html">271 acl-2013-ParaQuery: Making Sense of Paraphrase Collections</a></p>
<p>17 0.085020229 <a title="214-tfidf-17" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>18 0.084147453 <a title="214-tfidf-18" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>19 0.083943538 <a title="214-tfidf-19" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>20 0.081591584 <a title="214-tfidf-20" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, -0.113), (2, 0.187), (3, 0.121), (4, -0.015), (5, 0.023), (6, -0.055), (7, 0.021), (8, 0.08), (9, 0.012), (10, -0.023), (11, 0.038), (12, 0.005), (13, 0.103), (14, -0.049), (15, 0.046), (16, -0.077), (17, -0.128), (18, -0.085), (19, 0.153), (20, -0.038), (21, 0.012), (22, 0.096), (23, 0.118), (24, 0.029), (25, -0.046), (26, -0.022), (27, -0.056), (28, 0.102), (29, -0.144), (30, -0.045), (31, 0.02), (32, 0.028), (33, -0.029), (34, 0.077), (35, -0.002), (36, 0.09), (37, -0.118), (38, 0.077), (39, 0.173), (40, 0.136), (41, -0.092), (42, -0.041), (43, 0.01), (44, 0.031), (45, 0.025), (46, 0.006), (47, 0.019), (48, -0.032), (49, -0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90959042 <a title="214-lsi-1" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>Author: Ahmed El Kholy ; Nizar Habash ; Gregor Leusch ; Evgeny Matusov ; Hassan Sawaf</p><p>Abstract: An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.</p><p>2 0.81507134 <a title="214-lsi-2" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>Author: Hassan Sajjad ; Kareem Darwish ; Yonatan Belinkov</p><p>Abstract: We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation. In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic characterlevel transformational model that changes Egyptian to EG0, which looks similar to MSA. The transformations include morphological, phonological and spelling changes. The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points. Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.</p><p>3 0.75621063 <a title="214-lsi-3" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>4 0.6839326 <a title="214-lsi-4" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<p>Author: Heba Elfardy ; Mona Diab</p><p>Abstract: This paper introduces a supervised approach for performing sentence level dialect identification between Modern Standard Arabic and Egyptian Dialectal Arabic. We use token level labels to derive sentence-level features. These features are then used with other core and meta features to train a generative classifier that predicts the correct label for each sentence in the given input text. The system achieves an accuracy of 85.5% on an Arabic online-commentary dataset outperforming a previously proposed approach achieving 80.9% and reflecting a significant gain over a majority baseline of 5 1.9% and two strong baseline systems of 78.5% and 80.4%, respectively.</p><p>5 0.62312001 <a title="214-lsi-5" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>Author: Kareem Darwish</p><p>Abstract: Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and mi- croblogs test sets respectively.</p><p>6 0.59498245 <a title="214-lsi-6" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>7 0.58993524 <a title="214-lsi-7" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>8 0.58823109 <a title="214-lsi-8" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>9 0.5648061 <a title="214-lsi-9" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>10 0.55434775 <a title="214-lsi-10" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>11 0.51648951 <a title="214-lsi-11" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>12 0.51295257 <a title="214-lsi-12" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>13 0.51079249 <a title="214-lsi-13" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>14 0.51015472 <a title="214-lsi-14" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>15 0.4893463 <a title="214-lsi-15" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>16 0.48087034 <a title="214-lsi-16" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>17 0.48035803 <a title="214-lsi-17" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>18 0.46051818 <a title="214-lsi-18" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>19 0.43645924 <a title="214-lsi-19" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>20 0.43019137 <a title="214-lsi-20" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.04), (6, 0.028), (11, 0.043), (24, 0.028), (26, 0.036), (31, 0.012), (35, 0.055), (42, 0.078), (48, 0.04), (66, 0.27), (70, 0.024), (88, 0.017), (90, 0.048), (95, 0.18)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8113839 <a title="214-lda-1" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>Author: Ahmed El Kholy ; Nizar Habash ; Gregor Leusch ; Evgeny Matusov ; Hassan Sawaf</p><p>Abstract: An important challenge to statistical machine translation (SMT) is the lack of parallel data for many language pairs. One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages. Although pivoting is a robust technique, it introduces some low quality translations. In this paper, we present two language-independent features to improve the quality of phrase-pivot based SMT. The features, source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table. We show positive results (0.6 BLEU points) on Persian-Arabic SMT as a case study.</p><p>2 0.70495468 <a title="214-lda-2" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: We study subjective language media and create Twitter-specific lexicons via bootstrapping sentiment-bearing terms from multilingual Twitter streams. Starting with a domain-independent, highprecision sentiment lexicon and a large pool of unlabeled data, we bootstrap Twitter-specific sentiment lexicons, using a small amount of labeled data to guide the process. Our experiments on English, Spanish and Russian show that the resulting lexicons are effective for sentiment classification for many underexplored languages in social media.</p><p>3 0.67567819 <a title="214-lda-3" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>Author: K. Tamsin Maxwell ; Jon Oberlander ; W. Bruce Croft</p><p>Abstract: Techniques that compare short text segments using dependency paths (or simply, paths) appear in a wide range of automated language processing applications including question answering (QA). However, few models in ad hoc information retrieval (IR) use paths for document ranking due to the prohibitive cost of parsing a retrieval collection. In this paper, we introduce a flexible notion of paths that describe chains of words on a dependency path. These chains, or catenae, are readily applied in standard IR models. Informative catenae are selected using supervised machine learning with linguistically informed features and compared to both non-linguistic terms and catenae selected heuristically with filters derived from work on paths. Automatically selected catenae of 1-2 words deliver significant performance gains on three TREC collections.</p><p>4 0.6362859 <a title="214-lda-4" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>5 0.63050818 <a title="214-lda-5" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Julian Schamper ; Hermann Ney</p><p>Abstract: In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet- ter (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.</p><p>6 0.62392819 <a title="214-lda-6" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>7 0.62088442 <a title="214-lda-7" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>8 0.6194191 <a title="214-lda-8" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>9 0.61859709 <a title="214-lda-9" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>10 0.6166473 <a title="214-lda-10" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>11 0.61496383 <a title="214-lda-11" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>12 0.61461574 <a title="214-lda-12" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>13 0.61425811 <a title="214-lda-13" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>14 0.61379457 <a title="214-lda-14" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>15 0.61277723 <a title="214-lda-15" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>16 0.61255854 <a title="214-lda-16" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>17 0.61160576 <a title="214-lda-17" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>18 0.60923082 <a title="214-lda-18" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>19 0.60904086 <a title="214-lda-19" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>20 0.60615325 <a title="214-lda-20" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
