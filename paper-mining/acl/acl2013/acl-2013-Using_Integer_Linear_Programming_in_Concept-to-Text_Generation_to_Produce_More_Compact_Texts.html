<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-375" href="#">acl2013-375</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</h1>
<br/><p>Source: <a title="acl-2013-375-pdf" href="http://aclweb.org/anthology//P/P13/P13-2100.pdf">pdf</a></p><p>Author: Gerasimos Lampouras ; Ion Androutsopoulos</p><p>Abstract: We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.</p><p>Reference: <a title="acl-2013-375-reference" href="../acl2013_reference/acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts. [sent-2, score-0.567]
</p><p>2 1 Introduction Concept-to-text natural language generation (NLG) generates texts from formal knowledge representations (Reiter and Dale, 2000). [sent-3, score-0.158]
</p><p>3 With the emergence of the Semantic Web (Antoniou and van Harmelen, 2008), interest in concept-to-text NLG has been revived and several methods have been proposed to express axioms of OWL ontologies (Grau et al. [sent-4, score-0.203]
</p><p>4 They usually start by selecting the logical facts to express. [sent-10, score-0.408]
</p><p>5 The next stage, text planning, ranges from simply ordering the selected facts to complex decisions about the rhetorical structure of the text. [sent-11, score-0.533]
</p><p>6 Lexicalization then selects the words and syntactic structures that will realize each fact, specifying how each fact can be expressed as a single sentence. [sent-12, score-0.077]
</p><p>7 Another component generates appropriate referring expressions, and surface realization produces the final text. [sent-14, score-0.132]
</p><p>8 Each stage of the pipeline is treated as a local optimization problem, where the decisions of the previous stages cannot be modified. [sent-15, score-0.321]
</p><p>9 This arrangement produces texts that may not be optimal, since the decisions of the stages have been shown to be co-dependent (Danlos, 1984; Marciniak and Strube, 2005; Belz, 2008). [sent-16, score-0.186]
</p><p>10 gr / selection and lexicalization may lead to more or fewer sentence aggregation opportunities. [sent-20, score-0.481]
</p><p>11 We present an Integer Linear Programming (ILP) model that combines content selection, lexicalization, and sentence aggregation. [sent-21, score-0.114]
</p><p>12 Our model does not consider text planning, nor referring expression generation, which we hope to include in future work, but it is combined with an external simple text planner and a referring expression generation component; we also do not discuss surface realization. [sent-22, score-0.326]
</p><p>13 Unlike pipeline architectures, our model jointly examines the possible choices in the three NLG stages it considers, to avoid greedy local decisions. [sent-23, score-0.307]
</p><p>14 Given an individual (entity) or class of an OWL ontology and a set of facts (OWL axioms) about the individual or class, we aim to produce a text that expresses as many of the facts in as few words as possible. [sent-24, score-0.861]
</p><p>15 2  Related work  Marciniak and Strube (2005) propose a general ILP approach for language processing applications where the decisions of classifiers that consider particular, but co-dependent, subtasks need to be combined. [sent-30, score-0.053]
</p><p>16 They also show how their approach can be used to generate multi-sentence route directions, in a setting with very different inputs and processing stages than the ones we consider. [sent-31, score-0.047]
</p><p>17 Barzilay and Lapata (2005) treat content selection as an optimization problem. [sent-32, score-0.125]
</p><p>18 Given a pool of facts and scores indicating the importance of each 561  ProceedingSsof oifa, th Beu 5l1gsarti Aan,An uuaglu Mste 4e-ti9n2g 0 o1f3 t. [sent-33, score-0.408]
</p><p>19 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioinngauli Lsitnicgsu,i psatgices 561–566, fact or pair of facts, they select the facts to express by formulating an optimization problem similar  to energy minimization. [sent-35, score-0.493]
</p><p>20 In other work, Barzilay and Lapata (2006) consider sentence aggregation. [sent-36, score-0.068]
</p><p>21 Given a set of facts that a content selection stage has produced, aggregation is viewed as the problem of partitioning the facts into optimal subsets. [sent-37, score-1.16]
</p><p>22 Sentences expressing facts that are placed in the same subset are aggregated to form a longer sentence. [sent-38, score-0.64]
</p><p>23 An ILP model is used to find the partitioning that maximizes the pairwise similarity of the facts in each subset, subject to constraints limiting the number of subsets and the facts in each subset. [sent-39, score-0.912]
</p><p>24 They also show how an ILP solver can be used in practice. [sent-42, score-0.045]
</p><p>25 Joint optimization ILP models have also been used in multi-document text summarization and sentence compression (McDonald, 2007; Clarke and Lapata, 2008; Berg-Kirkpatrick et al. [sent-43, score-0.14]
</p><p>26 Statistical methods to jointly perform  content selection, lexicalization, and surface realization have also been proposed in NLG (Liang et al. [sent-46, score-0.162]
</p><p>27 To the best of our knowledge, this article is the first one to consider content selection, lexicalization, and sentence aggregation as an ILP joint optimization problem in the context of multi-sentence concept-to-text generation. [sent-48, score-0.356]
</p><p>28 , fn} be the set of all the facts fi (OWL axioms) about }th bee i tnhdeiv seidtu oafl or tchleas fsa cttos b fe described. [sent-53, score-0.468]
</p><p>29 OWL axioms can be represented as sets of RDF triples of the form hS, R, Oi, where S is an individutarilp or class, O fo rism a nhSot,hRer, individual, class, or datatype value, and R is a relation (property) that connects S to O. [sent-54, score-0.165]
</p><p>30 Hence, we can assume that each fact fi is a triple hSi, Ri, Oii . [sent-55, score-0.06]
</p><p>31 }  of alternative sentence plans is =ava {iplable. [sent-59, score-0.318]
</p><p>32 }h  1We actually convert the RDF triples to simpler message triples, so that each message triple can be easily expressed by a simple sentence, but we do not discuss this conversion here. [sent-63, score-0.037]
</p><p>33 sentence plan pik specifies how to express fi = hSi, Ri, Oii as an alternative single sentence. [sent-64, score-0.337]
</p><p>34 In our work, a sentence plan vise a sequence onfc slots, along with instructions specifying how to fill the slots in; and each sentence plan is associated with the relations it can express. [sent-65, score-0.425]
</p><p>35 In our example, the sentence plan would lead to a sentence like “Exhibit 12 was found in Athens”. [sent-67, score-0.208]
</p><p>36 We call elements the slots with their instructions, but with “S” and “O” accompanied by the individuals, classes, or datatype values they refer to; in our exam-  ple, the elements are “[ref (S: exhibit 12 )]”, “[findpast]”, “[in]”, “[ref (O: athens)]”. [sent-68, score-0.31]
</p><p>37 Different sentence plans may lead to more or fewer aggregation opportunities; for example, sentences with the same verb are easier to aggregate. [sent-69, score-0.526]
</p><p>38 We use aggregation rules (Dalianis, 1999) that operate on sentence plans and usually lead to shorter texts. [sent-70, score-0.565]
</p><p>39 , sm be disjoint subsets of F, each containing 0 to n facts, with m < n. [sent-74, score-0.051]
</p><p>40 A single sentence is generated for each subset sj by aggregating the sentences (more precisely, the sentence plans) expressing the facts of sj . [sent-75, score-1.09]
</p><p>41 10,,  likj=01,,  iofth fearcwt fisieis selected  (1)  ofiafth sctee rfnwtie,in ascneed p flai ns p inik siusb usseetd s tjo express (2)  btj=? [sent-80, score-0.093]
</p><p>42 10,,  oift heleerwmeisnet etis used in subset sj  (3)  and let B be the set of all the distinct elements (no  duplicates) from all the available sentence plans that can express the facts of F. [sent-81, score-1.185]
</p><p>43 If multiple aggregation rules apply, we use the one that leads to a shorter text. [sent-83, score-0.247]
</p><p>44 Our objective function (4) maximizes the number of selected facts fi and minimizes the number of distinct elements in each subset sj, i. [sent-85, score-0.716]
</p><p>45 , the approximate length of the corresponding aggregated sentence; an alternative explanation is that by minimizing the number of distinct elements in each sj, we favor subsets that aggregate well. [sent-87, score-0.351]
</p><p>46 By a and b we jointly denote all the ai and btj variables. [sent-88, score-0.126]
</p><p>47 The two parts (sums) of the objective function are nor-  malized to [0, 1] by dividing by the total number of available facts |F| and the number of subsets m otifm aevsa tihlaeb ltoet afalc cntus |mFb|er a nodf dthiesti nnucmt beleerm oefn sutsb |B| . [sent-89, score-0.459]
</p><p>48 The parameters λ1 and λ2 are used to tune the priority given to expressing many facts vs. [sent-92, score-0.464]
</p><p>49 ),n  (8)  (9) Constraint 5 ensures that for each selected fact, only one sentence plan in only one subset is selected; if a fact is not selected, no sentence plan for the fact is selected either. [sent-112, score-0.453]
</p><p>50 n |tσ 6, Bik tise sth teh set aor-f distinct elements et of the sentence plan pik. [sent-115, score-0.29]
</p><p>51 This constraint ensures that if pik is selected in a subset sj, then all the elements of pik are also present in sj. [sent-116, score-0.428]
</p><p>52 If pik is not selected in sj, then some of its elements may still be present in sj, if they appear in another selected sentence plan of sj. [sent-117, score-0.406]
</p><p>53 In constraint 7, P(et) is the set of sentence plans that contain element et. [sent-118, score-0.347]
</p><p>54 If et is used in a subset sj, then at least one of the sentence plans of P(et) must also be selected in sj. [sent-119, score-0.416]
</p><p>55 If et is not used in sj, then no sentence plan of P(et) may be selected in sj. [sent-120, score-0.182]
</p><p>56 Lastly, constraint 8 limits the number of ele-  ments that a subset sj can contain to a maximum allowed number Bmax, in effect limiting the maximum length of an aggregated sentence. [sent-121, score-0.453]
</p><p>57 We assume that each relation R has been manually mapped to a single topical section; e. [sent-122, score-0.032]
</p><p>58 , relations expressing the color, body, and flavor of a wine may be grouped in one section, and relations about the wine’s producer in another. [sent-124, score-0.204]
</p><p>59 The section of a fact fi = hSi, Ri, Oii is the section of its relation Ri. [sent-125, score-0.06]
</p><p>60 C=ons htSraint 9 ensures etha ste cfaticotns from different sections will not be placed in the same subset sj, to avoid unnatural aggregations. [sent-126, score-0.131]
</p><p>61 , 2013), an NLG system for OWL ontologies that relies on a pipeline of content selection, text planning, lexicalization, aggregation, referring expression generation, and surface realization. [sent-129, score-0.431]
</p><p>62 3 We modified content selection, lexicalization, and aggregation to use our ILP model, maintaining the aggregation rules of the original system. [sent-130, score-0.462]
</p><p>63 4 For re-  ferring expression generation and surface realization, the new system, called ILPNLG, invokes the corresponding components of NaturalOWL. [sent-131, score-0.137]
</p><p>64 It also assumes that a manually specified order of the sections and the relations of each section is available, which is used by the text planner to order the selected facts (by their relations). [sent-133, score-0.532]
</p><p>65 The subsequent components ofthe pipeline are not allowed to change the order of the facts, and aggregation operates only on sentence plans of adjacent facts from the same section. [sent-134, score-1.167]
</p><p>66 In ILPNLG, the manually specified order of sections and relations is used to order the sentences of each subset sj (before aggregating them), the aggregated sentences in each section (each aggregated sentence inherits the minimum order of its constituents), and the sections (with their sentences). [sent-135, score-0.68]
</p><p>67 5 We kept the 2 topical sections, the ordering of sections and relations, and the sentence plans that had been used in the previous experiments, but we added more sentence plans to ensure that 3 sentence plans were available per fact. [sent-145, score-1.058]
</p><p>68 We generated texts for the 52 wine individuals of the ontology; we did not experiment with texts describing classes of wines, because we could not think of multiple alternative sentence plans for many of their axioms. [sent-146, score-0.651]
</p><p>69 For each individual, there were 5 facts on average and a maximum of 6 facts. [sent-147, score-0.408]
</p><p>70 PIPELINE has a parameter M specifying the maximum number of facts it is allowed to report per text. [sent-148, score-0.495]
</p><p>71 When M is smaller than the number of available facts |F| and all the facts are treated as equally important, as i anl our experiments, eitd selects randomly M of the available facts. [sent-149, score-0.852]
</p><p>72 We repeated the generation of PIPELINE’s texts for the 52 individuals for M = 2, 3, 4, 5, 6. [sent-150, score-0.227]
</p><p>73 For each M, the texts of PIPELINE for the 52 individuals were  generated three times, each time using one of the different alternative sentence plans of each relation. [sent-151, score-0.473]
</p><p>74 We also generated the texts using a variant of PIPELINE, dubbed PIPELINESHORT, which always selects the shortest (in elements) sentence plan among the available ones. [sent-152, score-0.262]
</p><p>75 With ILPNLG, we repeated the generation of the texts of the 52 individuals using different values of λ1 (λ2 = 1 λ1), which led to texts expressing from zero t o− a λll of the available facts. [sent-154, score-0.369]
</p><p>76 We set the maximum number of fact subsets to m = 3, which was the maximum number of aggregated sentences observed in the texts of PIPELINE and PIPELINESHORT. [sent-155, score-0.257]
</p><p>77 We compared ILPNLG to PIPELINE and PIPELINESHORT by measuring the average number of facts they reported divided by the average text −  length (in words). [sent-157, score-0.408]
</p><p>78 Figure 1 shows this ratio as a function of the average number of reported facts, along with 95% confidence intervals (of sample means). [sent-158, score-0.034]
</p><p>79 Figure 1: Facts/words ratio of the generated texts. [sent-165, score-0.034]
</p><p>80 since it focuses on minimizing the number of distinct elements of each text. [sent-166, score-0.15]
</p><p>81 3, it obtains the highest fact/words ratio by s≈ele 0c. [sent-171, score-0.034]
</p><p>82 3t-,  ing the facts and sentence plans that lead to the most compressive aggregations. [sent-172, score-0.769]
</p><p>83 For greater values of λ1, it selects additional facts whose sentence plans do not aggregate that well, which is why the ratio declines. [sent-173, score-0.826]
</p><p>84 For small numbers of facts, the two pipeline systems select facts and sentence plans that offer very few aggregation opportunities; as the number of selected facts increases, some more aggregation opportunities arise, which is why the facts/words ratio of the two systems improves. [sent-174, score-1.884]
</p><p>85 In all the experiments, the ILP solver was very fast (average: 0. [sent-175, score-0.045]
</p><p>86 Experiments with human judges also showed that the texts of ILPNLG cannot be distinguished from those of PIPELINESHORT in terms of fluency and text clarity. [sent-178, score-0.086]
</p><p>87 Hence, the highest compactness of the texts of ILPNLG does not come at the expense of lower text quality. [sent-179, score-0.086]
</p><p>88 We show below texts produced by PIPELINE (M = 4) and ILPNLG (λ1 = 0. [sent-181, score-0.086]
</p><p>89 It is made from Semillon grapes and it is produced by Chateau D’ychem. [sent-184, score-0.073]
</p><p>90 It is made from Semillon  grapes by Chateau D’ychem. [sent-186, score-0.073]
</p><p>91 In the first pair, PIPELINE uses different verbs for the grapes and producer, whereas ILPNLG uses the same verb, which leads to a more compressive aggregation; both texts describe the same wine and report 4 facts. [sent-190, score-0.294]
</p><p>92 In the second pair, ILPNLG has chosen to express the sweetness instead of the producer, and uses the same verb (“be”) for all the facts, leading to a shorter sentence; again both texts describe the same wine and report 4 facts. [sent-191, score-0.268]
</p><p>93 564  In both examples, some facts are not aggregated because they belong in different sections. [sent-192, score-0.528]
</p><p>94 5  Conclusions  We presented an ILP model for NLG that jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy local decisions and produce more compact texts. [sent-193, score-0.38]
</p><p>95 Exper-  iments verified that our model can express more facts per word, compared to a pipeline, which is important when space is scarce. [sent-194, score-0.459]
</p><p>96 We plan to extend our model to include text planning and referring expressions generation. [sent-197, score-0.188]
</p><p>97 Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. [sent-235, score-0.158]
</p><p>98 Global inference for  sentence compression: An integer linear programming approach. [sent-254, score-0.114]
</p><p>99 An open-source natural language generator for OWL ontologies and its use in Prot e´g e´ and Second Life. [sent-280, score-0.073]
</p><p>100 The Semantic Web as a linguistic resource: opportunities for nat. [sent-352, score-0.071]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('facts', 0.408), ('ilpnlg', 0.292), ('owl', 0.278), ('plans', 0.25), ('aggregation', 0.208), ('sj', 0.202), ('ilp', 0.191), ('pipeline', 0.187), ('nlg', 0.164), ('lexicalization', 0.16), ('galanis', 0.151), ('pipelineshort', 0.122), ('aggregated', 0.12), ('elements', 0.096), ('androutsopoulos', 0.093), ('ref', 0.092), ('wine', 0.092), ('texts', 0.086), ('pik', 0.086), ('lampouras', 0.086), ('axioms', 0.079), ('mellish', 0.075), ('bmax', 0.073), ('findpast', 0.073), ('grapes', 0.073), ('likj', 0.073), ('naturalowl', 0.073), ('schwitter', 0.073), ('ontologies', 0.073), ('plan', 0.072), ('generation', 0.072), ('konstas', 0.071), ('opportunities', 0.071), ('individuals', 0.069), ('slots', 0.069), ('sentence', 0.068), ('oii', 0.065), ('athens', 0.064), ('referring', 0.06), ('fi', 0.06), ('marciniak', 0.06), ('subset', 0.056), ('producer', 0.056), ('expressing', 0.056), ('planning', 0.056), ('distinct', 0.054), ('sec', 0.053), ('decisions', 0.053), ('express', 0.051), ('subsets', 0.051), ('lapata', 0.049), ('hsi', 0.049), ('althaus', 0.049), ('antoniou', 0.049), ('btj', 0.049), ('chateau', 0.049), ('datatype', 0.049), ('semillon', 0.049), ('stages', 0.047), ('allowed', 0.046), ('content', 0.046), ('integer', 0.046), ('selection', 0.045), ('partitioning', 0.045), ('solver', 0.045), ('ontology', 0.045), ('jointly', 0.044), ('compressive', 0.043), ('sections', 0.042), ('selected', 0.042), ('barzilay', 0.041), ('specifying', 0.041), ('planner', 0.04), ('bik', 0.04), ('grau', 0.04), ('shorter', 0.039), ('compression', 0.038), ('triples', 0.037), ('surface', 0.036), ('realization', 0.036), ('selects', 0.036), ('instructions', 0.035), ('ratio', 0.034), ('rdf', 0.034), ('optimization', 0.034), ('liang', 0.034), ('european', 0.033), ('ai', 0.033), ('woodsend', 0.033), ('ensures', 0.033), ('topical', 0.032), ('economics', 0.032), ('reiter', 0.031), ('aggregate', 0.03), ('ri', 0.03), ('aggregating', 0.03), ('ordering', 0.03), ('expression', 0.029), ('constraint', 0.029), ('choices', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000013 <a title="375-tfidf-1" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>Author: Gerasimos Lampouras ; Ion Androutsopoulos</p><p>Abstract: We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.</p><p>2 0.13967554 <a title="375-tfidf-2" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>3 0.11340209 <a title="375-tfidf-3" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>Author: Sina Zarriess ; Jonas Kuhn</p><p>Abstract: We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</p><p>4 0.10816932 <a title="375-tfidf-4" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>Author: Xingxing Zhang ; Jianwen Zhang ; Junyu Zeng ; Jun Yan ; Zheng Chen ; Zhifang Sui</p><p>Abstract: Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.</p><p>5 0.094938792 <a title="375-tfidf-5" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>Author: Miguel Almeida ; Andre Martins</p><p>Abstract: We present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear programming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers.</p><p>6 0.089492925 <a title="375-tfidf-6" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>7 0.089215748 <a title="375-tfidf-7" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>8 0.071429461 <a title="375-tfidf-8" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>9 0.067187235 <a title="375-tfidf-9" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>10 0.066060744 <a title="375-tfidf-10" href="./acl-2013-Tag2Blog%3A_Narrative_Generation_from_Satellite_Tag_Data.html">337 acl-2013-Tag2Blog: Narrative Generation from Satellite Tag Data</a></p>
<p>11 0.064649865 <a title="375-tfidf-11" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>12 0.063919738 <a title="375-tfidf-12" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>13 0.061221179 <a title="375-tfidf-13" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>14 0.054290317 <a title="375-tfidf-14" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<p>15 0.053576816 <a title="375-tfidf-15" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>16 0.05204165 <a title="375-tfidf-16" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>17 0.049977407 <a title="375-tfidf-17" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>18 0.048763629 <a title="375-tfidf-18" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>19 0.048322104 <a title="375-tfidf-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.047911145 <a title="375-tfidf-20" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, 0.027), (2, -0.012), (3, -0.047), (4, -0.005), (5, 0.036), (6, 0.034), (7, -0.02), (8, -0.06), (9, -0.014), (10, -0.058), (11, -0.002), (12, -0.105), (13, -0.033), (14, -0.06), (15, 0.031), (16, 0.073), (17, -0.051), (18, -0.026), (19, -0.029), (20, -0.042), (21, -0.029), (22, -0.032), (23, 0.086), (24, 0.01), (25, 0.015), (26, 0.058), (27, -0.019), (28, -0.045), (29, 0.0), (30, 0.025), (31, -0.034), (32, 0.008), (33, -0.021), (34, 0.012), (35, 0.106), (36, 0.018), (37, -0.008), (38, -0.014), (39, -0.024), (40, 0.092), (41, 0.075), (42, 0.007), (43, 0.042), (44, -0.056), (45, 0.049), (46, 0.068), (47, -0.062), (48, -0.147), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92585629 <a title="375-lsi-1" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>Author: Gerasimos Lampouras ; Ion Androutsopoulos</p><p>Abstract: We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.</p><p>2 0.75536257 <a title="375-lsi-2" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>Author: Sina Zarriess ; Jonas Kuhn</p><p>Abstract: We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</p><p>3 0.73566955 <a title="375-lsi-3" href="./acl-2013-Tag2Blog%3A_Narrative_Generation_from_Satellite_Tag_Data.html">337 acl-2013-Tag2Blog: Narrative Generation from Satellite Tag Data</a></p>
<p>Author: Kapila Ponnamperuma ; Advaith Siddharthan ; Cheng Zeng ; Chris Mellish ; Rene van der Wal</p><p>Abstract: The aim of the Tag2Blog system is to bring satellite tagged wild animals “to life” through narratives that place their movements in an ecological context. Our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. We are working with one of the largest nature conservation charities in Europe in this regard, focusing on a single species, the red kite. We describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual, and constructs a story around its use of the landscape.</p><p>4 0.72089833 <a title="375-lsi-4" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>Author: Ravi Kondadadi ; Blake Howald ; Frank Schilder</p><p>Abstract: We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated weather and biography texts fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. – – *∗Ravi Kondadadi is now affiliated with Nuance Communications, Inc.</p><p>5 0.63722032 <a title="375-lsi-5" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>Author: Lu Wang ; Claire Cardie</p><p>Abstract: We address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion. We apply Multiple-Sequence Alignment to induce abstract generation templates that can be used for different domains. An Overgenerateand-Rank strategy is utilized to produce and rank candidate abstracts. Experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches. In addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality.</p><p>6 0.62193018 <a title="375-lsi-6" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>7 0.61496019 <a title="375-lsi-7" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>8 0.58414572 <a title="375-lsi-8" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>9 0.53299326 <a title="375-lsi-9" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>10 0.53199601 <a title="375-lsi-10" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>11 0.52767044 <a title="375-lsi-11" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>12 0.5171333 <a title="375-lsi-12" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>13 0.48598143 <a title="375-lsi-13" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>14 0.47846165 <a title="375-lsi-14" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>15 0.45714062 <a title="375-lsi-15" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>16 0.45070288 <a title="375-lsi-16" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>17 0.43733826 <a title="375-lsi-17" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>18 0.43269521 <a title="375-lsi-18" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>19 0.43231541 <a title="375-lsi-19" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>20 0.42862642 <a title="375-lsi-20" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.06), (6, 0.031), (11, 0.05), (24, 0.03), (26, 0.054), (35, 0.091), (42, 0.067), (48, 0.042), (64, 0.012), (70, 0.041), (88, 0.049), (90, 0.019), (95, 0.05), (99, 0.315)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79160142 <a title="375-lda-1" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>Author: Gerasimos Lampouras ; Ion Androutsopoulos</p><p>Abstract: We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.</p><p>2 0.71145707 <a title="375-lda-2" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>Author: Egoitz Laparra ; German Rigau</p><p>Abstract: This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling. The system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data.</p><p>3 0.62704539 <a title="375-lda-3" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>Author: Trevor Cohn ; Lucia Specia</p><p>Abstract: Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and levels of consistency. We present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour. These techniques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data. Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently outperform strong baselines.</p><p>4 0.60444915 <a title="375-lda-4" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>Author: Greg Coppola ; Mark Steedman</p><p>Abstract: Higher-order dependency features are known to improve dependency parser accuracy. We investigate the incorporation of such features into a cube decoding phrase-structure parser. We find considerable gains in accuracy on the range of standard metrics. What is especially interesting is that we find strong, statistically significant gains on dependency recovery on out-of-domain tests (Brown vs. WSJ). This suggests that higher-order dependency features are not simply overfitting the training material.</p><p>5 0.50599378 <a title="375-lda-5" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>6 0.47586152 <a title="375-lda-6" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>7 0.47345188 <a title="375-lda-7" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>8 0.47250816 <a title="375-lda-8" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>9 0.47166386 <a title="375-lda-9" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>10 0.47153178 <a title="375-lda-10" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>11 0.47048587 <a title="375-lda-11" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>12 0.46856374 <a title="375-lda-12" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>13 0.46781287 <a title="375-lda-13" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>14 0.4672901 <a title="375-lda-14" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>15 0.4672811 <a title="375-lda-15" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>16 0.46606523 <a title="375-lda-16" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>17 0.46531105 <a title="375-lda-17" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>18 0.46507633 <a title="375-lda-18" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>19 0.46506336 <a title="375-lda-19" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>20 0.46415883 <a title="375-lda-20" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
