<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-182" href="#">acl2013-182</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</h1>
<br/><p>Source: <a title="acl-2013-182-pdf" href="http://aclweb.org/anthology//P/P13/P13-3020.pdf">pdf</a></p><p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>Reference: <a title="acl-2013-182-reference" href="../acl2013_reference/acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 j p s Abstract In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. [sent-4, score-0.72]
</p><p>2 Furthermore, it is also important to provide high-quality correct data as training data. [sent-5, score-0.058]
</p><p>3 In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR-  ank algorithm. [sent-6, score-1.02]
</p><p>4 Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization. [sent-7, score-0.244]
</p><p>5 It has been widely used for solving many document categorization problems (Zhu and Ghahramani, 2002; Zhu et al. [sent-9, score-0.216]
</p><p>6 A good accuracy of GBSSL depends on success in dealing with three crucial issues: graph construction, selection of high-quality training data, and categorization algorithm. [sent-11, score-0.455]
</p><p>7 In a graph-based categorization of documents, a graph is constructed based on a certain relation between nodes (i. [sent-13, score-0.439]
</p><p>8 It is similarity that is often used to express the relation between nodes in a graph. [sent-16, score-0.216]
</p><p>9 We think of two types of  similarity: the one is between surface information obtained by document vector (Salton and McGill, Ichiro Kobayashi Ochanomizu University 2-1-1 Otsuka Bunkyo-ku Tokyo, Japan koba @ i . [sent-17, score-0.267]
</p><p>10 j p s 1983) and the other is between latent information obtained by word probabilistic distribution (Latent Dirichlet Allocation (Blei et al. [sent-20, score-0.159]
</p><p>11 We use both surface information and latent information at the ratio of (1 α) : α(0 ≤ α ≤ 1) to construct a similarity graph f)or : GBSSL, αa n≤d we investigate tah sei optimal α for raising the accuracy in GBSSL. [sent-23, score-0.833]
</p><p>12 In selecting high-quality training data, it is important to take two aspects of data into consideration: quantity and quality. [sent-24, score-0.133]
</p><p>13 The more the training data are, the better the accuracy becomes. [sent-25, score-0.098]
</p><p>14 We do not always, however, have a large quantity of training data. [sent-26, score-0.095]
</p><p>15 In such a case, the quality of training data is generally a key for better accuracy. [sent-27, score-0.058]
</p><p>16 It is required to assess the quality of training data ex−  actly. [sent-28, score-0.058]
</p><p>17 We use the PageRank algorithm (Brin and Page, 1998) to select high-quality data, which have a high centrality in a similarity graph of training data (i. [sent-30, score-0.459]
</p><p>18 We apply our methods to solving the problem of a multi-class document categorization. [sent-33, score-0.096]
</p><p>19 We evaluate the results of experiments for each category and for the whole category. [sent-35, score-0.05]
</p><p>20 We confirm that the way of selecting the high-quality training data from data on a similarity graph based on both surface information and latent information is superior to that of selecting from a graph based on just surface information or latent information. [sent-36, score-1.238]
</p><p>21 Dipanjan and Petrov (201 1) have applied a graph-based label propagation method to solve the problem of part-ofspeech tagging. [sent-44, score-0.099]
</p><p>22 Dipanjan and Smith (2012) have also applied GBSSL to construct compact natural language lexicons. [sent-46, score-0.065]
</p><p>23 Whitney and Sarkar (2012) have proposed the bootstrapping learning method in which a graph propagation algorithm is adopted. [sent-48, score-0.265]
</p><p>24 There are two main issues in GBSSL: the one  is the way of constructing a graph to propagate labels, and the other is the way of propagating labels. [sent-49, score-0.205]
</p><p>25 It is essential to construct a good graph in GBSSL (Zhu, 2005). [sent-50, score-0.27]
</p><p>26 On the one hand, graph construction is a key to success of any GBSSL. [sent-51, score-0.205]
</p><p>27 On the other hand, as for semi-supervised learning, it is quite important to select better training data (i. [sent-52, score-0.105]
</p><p>28 labeled data), because the effect of learning will be changed by the data we select as training data. [sent-54, score-0.105]
</p><p>29 Considering the above mentioned, in our study, we focus on the way of selecting training data so as to be well propagated in a graph. [sent-55, score-0.096]
</p><p>30 We use the PageRank algorithm to select high-quality training data and evaluate how our proposed method influences the way of document categorization. [sent-56, score-0.201]
</p><p>31 3  Text classification based on a graph  The details of our proposed GBSSL method in a multi-class document categorization are as follows. [sent-57, score-0.421]
</p><p>32 1 Graph construction In our study, we use a weighted undirected graph  G = (V, E) whose node and edge represent a document and the similarity between nodes, respectively. [sent-59, score-0.403]
</p><p>33 V and E represent nodes and edges in a graph, respectively. [sent-61, score-0.114]
</p><p>34 A graph G can be represented as an adjacency matrix, and wij ∈ W represents the similarity between nodes i∈and W j. [sent-62, score-0.538]
</p><p>35 eIpnr particular, isnim mthilea case eofGBSSL method, the similarity between nodes are formed as wij = sim(xi, xj)δ(j ∈ K(i)). [sent-63, score-0.298]
</p><p>36 2 Similarity in a graph Generally speaking, when we construct a graph to represent some relation among documents, cosine similarity (simcos) of document vectors is adopted as a similarity measure based on surface information. [sent-66, score-0.912]
</p><p>37 In our study, we add the similarity (simJS) based on latent information and the similarity (simcos) based on surface information in the proportion of α : (1 − α) (0 ≤ α ≤ 1). [sent-67, score-0.5]
</p><p>38 We define tphreo sum onf o simJS a −nd α simcos as simnodes (see, Eq. [sent-68, score-0.261]
</p><p>39 (1), P and Q represent the latent topic  distributions of documents S and T, respectively. [sent-71, score-0.199]
</p><p>40 , 2003) to estimate the latent topic distribution of a document, and we use a measure JensenShannon divergence (DJS) for the similarity between topic distributions. [sent-73, score-0.261]
</p><p>41 simnodes (S, T) ≡ α ∗ simJS(P, Q) +(1 − α)  ∗  simcos(tfidf(S), tfidf(T)) (1)  simJS(P, Q) ≡ 1 DJS(P, Q) (2) 3. [sent-76, score-0.077]
</p><p>42 3 Selection of training data We use the graph-based document summarization methods (Erkan and Radev, 2004; Kitajima and Kobayashi, 2012) in order to select high-quality training data. [sent-77, score-0.294]
</p><p>43 Erkan and Radev (2004) proposed a multi-document summarization method using the PageRank algorithm (Brin and Page, 1998) to extract important sentences. [sent-78, score-0.035]
</p><p>44 They showed that it is useful to extract the important sentences which have higher PageRank scores in a similarity graph of sentences. [sent-79, score-0.307]
</p><p>45 In order to get high-quality training data, we first construct a similarity graph of training data in each category, and then compute a TopicRank score for each training datum in every category graph. [sent-84, score-0.63]
</p><p>46 We employ the data with a high TopicRank score as training data in GBSSL. [sent-85, score-0.058]
</p><p>47 In TopicRank method, Kitajima and Kobayashi (2012) regard a sentence as a node in a graph on 137  surface information and latent information. [sent-86, score-0.501]
</p><p>48 We, however, deal with documents, so we replace a sentence with a document (i. [sent-92, score-0.096]
</p><p>49 (3), N indicates total number of documents, adj [u] indicates the adjoining nodes of document u. [sent-96, score-0.253]
</p><p>50 4  (3)  Label propagation  We use the label propagation method (Zhu et al. [sent-98, score-0.159]
</p><p>51 It estimates the value of label based on the assumption that the nodes linked to each other in a graph should belong to the same category. [sent-102, score-0.358]
</p><p>52 l indicates the number of training data among all n nodes in a graph. [sent-104, score-0.172]
</p><p>53 The estimation values f for n nodes are obtained as the solution (Eq. [sent-105, score-0.114]
</p><p>54 (6)) of the following objective function of an optimal problem (Eq. [sent-106, score-0.052]
</p><p>55 (4) expresses the deviation between an estimation value and a correct value of training data. [sent-109, score-0.089]
</p><p>56 (4) expresses the difference between the esti-  mation values of the nodes which are next to another in the adjacency graph. [sent-111, score-0.214]
</p><p>57 λ(> 0) is a parameter balancing both of the terms. [sent-112, score-0.054]
</p><p>58 L(≡ D W) fiso rcmaleledd i tnhteo Laplacian m maetarinxs. [sent-116, score-0.031]
</p><p>59 a diagonal matrix, each diagonal element of which is equal to the sum of elements in W’s each row (or column). [sent-118, score-0.084]
</p><p>60 4, 5, 6, 8, 10, the PRBEPs at α 0 are greater th 4,an 5 ,th 6a,t 8a,t α = 0, although t ahte α αPR6 =B0E P a raet α = 1is less than that at α = 0 in Fig. [sent-123, score-0.106]
</p><p>61 2, 7, the PRBEPs at α 0 are loethsse rth haann dt,h aint Fati α = 0, . [sent-126, score-0.034]
</p><p>62 1, 3, 9, =the 0 PRBEPs at α 0 fluctuate widely or narrowly ParRoBunEdP tsh aatt aαt α = 0 f. [sent-128, score-0.111]
</p><p>63 Indexes at α 0 are greater than or equal to an hin. [sent-139, score-0.106]
</p><p>64 8 at α = 0, though the macro average at α = 1is 43. [sent-153, score-0.365]
</p><p>65 Hence, the maximum macro average is greater than that at α = 1by 3. [sent-155, score-0.471]
</p><p>66 The macro average at α = 1is greater than that at α = 0 by 7. [sent-158, score-0.471]
</p><p>67 2, the macro averages fluctuate within the range from 40. [sent-166, score-0.477]
</p><p>68 1 ≤ α ≤ 1are greater mthaacnr toh aatv at α = l0u. [sent-170, score-0.106]
</p><p>69 1is more important, attheer macro averages at α = 0. [sent-172, score-0.4]
</p><p>70 9 are greater than that at α = 1 and of course greater than that at α = 0. [sent-177, score-0.212]
</p><p>71 1-10, each optimal  α  at  which PRBEP is the maximum is different and not uniform in respective categories. [sent-179, score-0.052]
</p><p>72 So, we cannot simply tell a specific ratio of balancing both information (i. [sent-180, score-0.089]
</p><p>73 surface information and latent information) which gives the best accuracy. [sent-182, score-0.327]
</p><p>74 139 Table 1: the optimal parameters  (k, λ)  for each category  Cmaicgowtnrsehudqpayints-f\xα(12530 , 8. [sent-189, score-0.102]
</p><p>75 e0-)814 Figure 1: earn  Figure 2: acq  Figure 4: grain  Figure 5: crude  Figure 7: interest  Figure 8: ship  Figure 10: corn  Figure 11: Relative value 140  Figure 3: money-fx  Figure 6: trade  Figure 9: wheat  Figure 12: Macro average The macro average of the whole category is shown in Fig. [sent-200, score-0.523]
</p><p>76 Regarding the macro average at α = 0 as a baseline, the macro average at α = 1is greater than that at α = 0 by 7. [sent-202, score-0.836]
</p><p>77 1 ≤ α ≤ 1 are greater tfhivaen mthaactr oat a α = e1s. [sent-207, score-0.106]
</p><p>78 Therefore, we can say tahteatr using latent information gives a higher accuracy than using only surface information and that using both information gives a higher accuracy than us-  ing only latent information. [sent-208, score-0.597]
</p><p>79 6  α  is  Conclusion  We have proposed methods to construct a similarity graph based on both surface information and latent information and to select high-quality training data for GBSSL. [sent-210, score-0.773]
</p><p>80 Through experiments, we have found that using both information gives a better accuracy than using either only surface information or only latent information. [sent-211, score-0.367]
</p><p>81 We used the PageRank algorithm in the selection of highquality training data. [sent-212, score-0.153]
</p><p>82 In this condition, we have confirmed that our proposed methods are useful for raising the accuracy of a multi-class document categorization using GBSSL in the whole category. [sent-213, score-0.364]
</p><p>83 We will verify in other data corpus sets that the selection of highquality training data with both information gives a better accuracy and that the optimal α is around 0. [sent-215, score-0.276]
</p><p>84 We will revise the way of setting a pair of the optimal parameters (k, λ) and use latent informa-  tion in the process of label propagation. [sent-217, score-0.25]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gbssl', 0.422), ('macro', 0.325), ('subramanya', 0.233), ('topicrank', 0.23), ('graph', 0.205), ('simjs', 0.192), ('latent', 0.159), ('kitajima', 0.153), ('prbep', 0.153), ('simcos', 0.153), ('surface', 0.137), ('categorization', 0.12), ('kobayashi', 0.118), ('prbeps', 0.115), ('dipanjan', 0.115), ('nodes', 0.114), ('erkan', 0.112), ('pagerank', 0.107), ('greater', 0.106), ('similarity', 0.102), ('document', 0.096), ('whitney', 0.094), ('bilmes', 0.09), ('brin', 0.084), ('amarnag', 0.084), ('xiaojin', 0.078), ('fluctuate', 0.077), ('simnodes', 0.077), ('zhu', 0.076), ('averages', 0.075), ('raising', 0.073), ('adjacency', 0.069), ('ocha', 0.068), ('otsuka', 0.068), ('construct', 0.065), ('highquality', 0.063), ('ochanomizu', 0.063), ('djs', 0.063), ('propagation', 0.06), ('unes', 0.059), ('ichiro', 0.059), ('training', 0.058), ('radev', 0.056), ('balancing', 0.054), ('indexes', 0.054), ('optimal', 0.052), ('category', 0.05), ('ghahramani', 0.05), ('zoubin', 0.05), ('wij', 0.048), ('centrality', 0.047), ('salton', 0.047), ('select', 0.047), ('sarkar', 0.046), ('tfidf', 0.044), ('petrov', 0.043), ('adj', 0.043), ('tokyo', 0.043), ('blei', 0.043), ('diagonal', 0.042), ('average', 0.04), ('accuracy', 0.04), ('documents', 0.04), ('label', 0.039), ('selecting', 0.038), ('quantity', 0.037), ('jeff', 0.036), ('dirichlet', 0.035), ('summarization', 0.035), ('confirmed', 0.035), ('tell', 0.035), ('prr', 0.034), ('narrowly', 0.034), ('laplacian', 0.034), ('mcgill', 0.034), ('fati', 0.034), ('aint', 0.034), ('proceedigns', 0.034), ('acq', 0.034), ('corn', 0.034), ('datum', 0.034), ('dengyong', 0.034), ('isnim', 0.034), ('jensenshannon', 0.034), ('koba', 0.034), ('maximums', 0.034), ('risa', 0.034), ('softsupervised', 0.034), ('japan', 0.033), ('semisupervised', 0.033), ('mellon', 0.033), ('carnegie', 0.033), ('selection', 0.032), ('slav', 0.032), ('expresses', 0.031), ('gives', 0.031), ('tphreo', 0.031), ('ank', 0.031), ('dv', 0.031), ('fiso', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="182-tfidf-1" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>2 0.2758342 <a title="182-tfidf-2" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>3 0.16990158 <a title="182-tfidf-3" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>4 0.097723544 <a title="182-tfidf-4" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>5 0.097345039 <a title="182-tfidf-5" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>6 0.09268789 <a title="182-tfidf-6" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>7 0.090363063 <a title="182-tfidf-7" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>8 0.069980204 <a title="182-tfidf-8" href="./acl-2013-Modeling_Thesis_Clarity_in_Student_Essays.html">246 acl-2013-Modeling Thesis Clarity in Student Essays</a></p>
<p>9 0.069042534 <a title="182-tfidf-9" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>10 0.068454355 <a title="182-tfidf-10" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>11 0.06705147 <a title="182-tfidf-11" href="./acl-2013-SEMILAR%3A_The_Semantic_Similarity_Toolkit.html">304 acl-2013-SEMILAR: The Semantic Similarity Toolkit</a></p>
<p>12 0.065844432 <a title="182-tfidf-12" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>13 0.062492855 <a title="182-tfidf-13" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>14 0.0615693 <a title="182-tfidf-14" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>15 0.060243011 <a title="182-tfidf-15" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>16 0.059299231 <a title="182-tfidf-16" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>17 0.058413308 <a title="182-tfidf-17" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>18 0.058248907 <a title="182-tfidf-18" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>19 0.056152381 <a title="182-tfidf-19" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>20 0.05565469 <a title="182-tfidf-20" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, 0.053), (2, -0.001), (3, -0.058), (4, 0.076), (5, -0.043), (6, 0.03), (7, 0.004), (8, -0.146), (9, -0.056), (10, 0.078), (11, 0.043), (12, 0.057), (13, 0.015), (14, 0.001), (15, 0.015), (16, -0.026), (17, 0.066), (18, -0.066), (19, 0.014), (20, -0.014), (21, 0.042), (22, 0.043), (23, -0.017), (24, -0.026), (25, 0.034), (26, 0.018), (27, 0.044), (28, -0.072), (29, -0.037), (30, -0.112), (31, -0.05), (32, -0.049), (33, 0.147), (34, 0.062), (35, -0.051), (36, -0.173), (37, -0.012), (38, 0.029), (39, 0.008), (40, 0.043), (41, 0.051), (42, 0.108), (43, 0.066), (44, -0.017), (45, 0.018), (46, 0.006), (47, 0.075), (48, 0.071), (49, -0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9441573 <a title="182-lsi-1" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>2 0.78317618 <a title="182-lsi-2" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>3 0.67658359 <a title="182-lsi-3" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki ; Suguru Matsuyoshi</p><p>Abstract: This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</p><p>4 0.6577667 <a title="182-lsi-4" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>5 0.59805942 <a title="182-lsi-5" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>6 0.54669935 <a title="182-lsi-6" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>7 0.54068863 <a title="182-lsi-7" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>8 0.53244579 <a title="182-lsi-8" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>9 0.52881628 <a title="182-lsi-9" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>10 0.52557176 <a title="182-lsi-10" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>11 0.5242933 <a title="182-lsi-11" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>12 0.5075953 <a title="182-lsi-12" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>13 0.4986279 <a title="182-lsi-13" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>14 0.49533206 <a title="182-lsi-14" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>15 0.49022648 <a title="182-lsi-15" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>16 0.48854735 <a title="182-lsi-16" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>17 0.48160219 <a title="182-lsi-17" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>18 0.46289945 <a title="182-lsi-18" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>19 0.44902447 <a title="182-lsi-19" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>20 0.44391599 <a title="182-lsi-20" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.026), (6, 0.028), (11, 0.056), (14, 0.018), (24, 0.049), (26, 0.04), (35, 0.052), (42, 0.038), (48, 0.04), (70, 0.019), (88, 0.018), (90, 0.521), (95, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88793844 <a title="182-lda-1" href="./acl-2013-On_the_Predictability_of_Human_Assessment%3A_when_Matrix_Completion_Meets_NLP_Evaluation.html">263 acl-2013-On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation</a></p>
<p>Author: Guillaume Wisniewski</p><p>Abstract: This paper tackles the problem of collecting reliable human assessments. We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality. To reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example.</p><p>same-paper 2 0.86667532 <a title="182-lda-2" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>3 0.83695793 <a title="182-lda-3" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>Author: Stefan L. Frank ; Leun J. Otten ; Giulia Galli ; Gabriella Vigliocco</p><p>Abstract: We investigated the effect of word surprisal on the EEG signal during sentence reading. On each word of 205 experimental sentences, surprisal was estimated by three types of language model: Markov models, probabilistic phrasestructure grammars, and recurrent neural networks. Four event-related potential components were extracted from the EEG of 24 readers of the same sentences. Surprisal estimates under each model type formed a significant predictor of the amplitude of the N400 component only, with more surprising words resulting in more negative N400s. This effect was mostly due to content words. These findings provide support for surprisal as a gener- ally applicable measure of processing difficulty during language comprehension.</p><p>4 0.8113035 <a title="182-lda-4" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>Author: Fabienne Braune ; Nina Seemann ; Daniel Quernheim ; Andreas Maletti</p><p>Abstract: We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German tlriannes olnati tohne tWasMk.T TA 2s0 an a Edndgitliisonha →l c Gonetrrmibauntion we make the developed software and complete tool-chain publicly available for further experimentation.</p><p>5 0.79625267 <a title="182-lda-5" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>6 0.76071972 <a title="182-lda-6" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>7 0.7276181 <a title="182-lda-7" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>8 0.46698019 <a title="182-lda-8" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>9 0.46030185 <a title="182-lda-9" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>10 0.44174361 <a title="182-lda-10" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>11 0.43815431 <a title="182-lda-11" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>12 0.43753207 <a title="182-lda-12" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>13 0.43681416 <a title="182-lda-13" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>14 0.41877565 <a title="182-lda-14" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>15 0.41375458 <a title="182-lda-15" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>16 0.39548787 <a title="182-lda-16" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>17 0.38397342 <a title="182-lda-17" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>18 0.38339582 <a title="182-lda-18" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<p>19 0.37685388 <a title="182-lda-19" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>20 0.37590298 <a title="182-lda-20" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
