<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-353" href="#">acl2013-353</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</h1>
<br/><p>Source: <a title="acl-2013-353-pdf" href="http://aclweb.org/anthology//P/P13/P13-1121.pdf">pdf</a></p><p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.</p><p>Reference: <a title="acl-2013-353-reference" href="../acl2013_reference/acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. [sent-4, score-0.256]
</p><p>2 In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. [sent-6, score-0.356]
</p><p>3 We conduct a series of studies comparing human-written model  summaries to system summaries at the semantic level of caseframes. [sent-7, score-0.518]
</p><p>4 We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. [sent-8, score-1.154]
</p><p>5 1 Introduction In automatic summarization, centrality has been one of the guiding principles for content selection in extractive systems. [sent-10, score-0.211]
</p><p>6 We define centrality to be the idea that a summary should contain the parts of the source text that are most similar or representative of the source text. [sent-11, score-0.34]
</p><p>7 This is most transparently illustrated by the Maximal Marginal Relevance (MMR) system of Carbonell and Goldstein  (1998), which defines the summarization objective Gerald Penn University of Toronto 10 King’s College Rd. [sent-12, score-0.203]
</p><p>8 For example, term weighting methods such as the signature term method of Lin and Hovy (2000) pick out salient terms that occur more often than would be expected in the source text based on frequencies in a background corpus. [sent-17, score-0.286]
</p><p>9 This method is a core component of the most successful summarization methods (Conroy et al. [sent-18, score-0.203]
</p><p>10 While extractive methods based on centrality have thus achieved success, there has long been recognition that abstractive methods are ultimately  more desirable. [sent-20, score-0.292]
</p><p>11 A key part of the usefulness of summaries is that they provide some synthesis or analysis of the source text and make a more general statement that is of direct relevance to the user. [sent-23, score-0.324]
</p><p>12 The position of this paper is that centrality is not enough to make substantial progress towards abstractive summarization that is capable of this type of semantic inference. [sent-25, score-0.397]
</p><p>13 Instead, summarization systems need to make more use of domain knowledge. [sent-26, score-0.267]
</p><p>14 We provide evidence for this in a series of studies on the TAC 2010 guided summarization data set that examines how the behaviour of automatic summarizers can or cannot be distinguished from human summarizers. [sent-27, score-0.493]
</p><p>15 Ac s2s0o1ci3a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 123 –1242,  provide a quantitative measure of the degree of sentence aggregation in a summarization system. [sent-30, score-0.305]
</p><p>16 Second, we show that centrality-based measures are unlikely to lead to substantial progress towards abstractive summarization, because current topperforming systems already produce summaries that are more “central” than humans do. [sent-31, score-0.359]
</p><p>17 Third, we consider how domain knowledge may be useful as a resource for an abstractive system, by showing that key parts of model summaries can be reconstructed from the source plus related in-domain documents. [sent-32, score-0.473]
</p><p>18 Instead, our studies reveal useful criteria with which to distinguish human-written and system summaries, helping to guide the development of future summarization systems. [sent-37, score-0.221]
</p><p>19 As noted above, Lin and Hovy’s (2000) signature terms have been successful in discovering terms that are specific to the source text. [sent-42, score-0.206]
</p><p>20 In this paper, we use guided summarization data as an opportunity to reopen the investigation into the effect of domain, because multiple document clusters from the same domain are available. [sent-48, score-0.326]
</p><p>21 They find that the best possible extractive systems score higher or as highly than human summarizers, but it is unclear whether this means the oracle summaries are actually as useful as human ones in an extrinsic setting. [sent-56, score-0.444]
</p><p>22 (1999; 2000) find that lecture transcripts that have been manually highlighted with key points improve students’ quiz scores more than when using automated summarization techniques or when providing only the lecture transcript or slides. [sent-60, score-0.203]
</p><p>23 Saggion and Lapalme (2002) similarly define a list of transformations necessary to convert source text to summary text, and manually analyzed their frequencies. [sent-62, score-0.209]
</p><p>24 Copeck and Szpakowicz (2004) find that at most 55% of vocabulary items found in model summaries occur in the source text, but they do not investigate where the other vocabulary items might be found. [sent-63, score-0.296]
</p><p>25 1234  Sentence: At one point, two bomb squad trucks sped to the school after a backpack scare. [sent-64, score-0.216]
</p><p>26 3  Theoretical basis of our analysis  Many existing summarization evaluation methods rely on word or N-gram overlap measures, but these measures are not appropriate for our analysis. [sent-66, score-0.203]
</p><p>27 Good summaries should certainly contain the salient entities in the source text, but when assessing the effect of the domain, different domain instances (i. [sent-68, score-0.373]
</p><p>28 Based on these criteria, we selected caseframes to be the appropriate unit of analysis. [sent-80, score-0.669]
</p><p>29 A caseframe is a shallow approximation of the semantic role structure of a proposition-bearing unit like a verb, and are derived from the dependency parse of a sentence1 . [sent-81, score-0.234]
</p><p>30 1Note that caseframes  are  distinct from (though directly  Relation Caseframe PairSim. [sent-82, score-0.669]
</p><p>31 81  Figure 1: Sample pairs of similar caseframes by relation type, and the similarity score assigned to them by our distributional model. [sent-86, score-0.693]
</p><p>32 The use of caseframes is well grounded in a variety of NLP tasks relevant to summarization such as coreference resolution (Bean and Riloff, 2004), and information extraction (Chambers and Jurafsky, 2011), where they serve the central unit of semantic analysis. [sent-89, score-0.872]
</p><p>33 We use the following algorithm to extract caseframes from dependency parses. [sent-91, score-0.669]
</p><p>34 1 Caseframe Similarity Direct caseframe matches account for some variation in the expression of slots, such as voicing alternations, but there are other reasons different caseframes may indicate the same slot (Figure 1). [sent-96, score-0.926]
</p><p>35 To account for these issues, we measure caseframe similarity based on their distributional similarity in a large training corpus. [sent-101, score-0.282]
</p><p>36 First, we construct vector representations of each caseframe, where the dimensions of the vector correspond to the lemma of the head word that fills the caseframe in the training corpus. [sent-102, score-0.234]
</p><p>37 For example, kicked the ball would result in a count of 1added to the caseframe (kick, dobj) for the context word ball. [sent-103, score-0.234]
</p><p>38 Similarity between caseframes can then be compared by cosine similarity between the their vector representations. [sent-105, score-0.693]
</p><p>39 For reasons of sparsity, we only considered caseframes that appear at least five times in the guided summarization corpus, and only the 3000 most common lemmata in Gigaword as context words. [sent-109, score-0.926]
</p><p>40 4 Experiments  We conducted our experiments on the data and results of the TAC 2010 summarization workshop. [sent-123, score-0.203]
</p><p>41 Ten are used in an initial guided summarization task, and ten are used in an update summarization task, in which a summary must be produced assuming that the original ten documents had already been read. [sent-125, score-0.598]
</p><p>42 All summaries have a word length limit of 100 words. [sent-126, score-0.24]
</p><p>43 We analyzed the results of the two summarization tasks separately in our experiments. [sent-127, score-0.222]
</p><p>44 In our study, we compared the characteristics of summaries generated by the eight human summarizers with those generated by the peer summaries, which are basically extractive systems. [sent-130, score-0.924]
</p><p>45 There  are 43 peer summarization systems, including two baselines defined by NIST. [sent-131, score-0.572]
</p><p>46 We refer to systems by their ID given by NIST, which are alphabetical for the human summarizers (A to H), and numeric for the peer summarizers (1 to 43). [sent-132, score-0.759]
</p><p>47 We removed two peer systems (systems 29 and 43) which did not generate any summary text in the workshop, presumably due to software problems. [sent-133, score-0.522]
</p><p>48 For each measure that we consider, we compare the average among the human-written summaries to the three individual peer systems, which we chose in order to provide a representative sample of the average and best performance of the automatic systems according to current evaluation methods. [sent-134, score-0.674]
</p><p>49 These systems are all primarily extractive, like most of the systems in the workshop: Peer average The average of the measure among the 41 peer summarizers. [sent-135, score-0.453]
</p><p>50 Peer 16 This system scored the highest in responsiveness scores on the original summarization task and in ROUGE-2, responsiveness, and Pyramid score in the update task. [sent-136, score-0.288]
</p><p>51 Peer 22 This system scored the highest in ROUGE-2 and Pyramid score in the original summarization task. [sent-137, score-0.203]
</p><p>52 1236  (a) Initial guided summarization task  (b) Update summarization task Figure 2: Average sentence cover size: the average number of sentences needed to generate the caseframes in a summary sentence (Study 1). [sent-138, score-1.335]
</p><p>53 09  Table 2: The average number of source text sen-  tences needed to cover a summary sentence. [sent-151, score-0.25]
</p><p>54 1 Study 1: Sentence aggregation We first confirm that human summarizers are more prone to sentence aggregation than system summarizers, showing that abstraction is indeed a desirable goal. [sent-156, score-0.39]
</p><p>55 This is defined to be the minimum number of sentences from the source text needed to cover all of the caseframes found in a summary sentence (for those caseframes that can be found in the source text at all), averaged over all  of the summary sentences. [sent-158, score-1.775]
</p><p>56 Human summarizers would be expected to score higher, if they actually aggregate information from multiple points in the source text. [sent-161, score-0.267]
</p><p>57 To illustrate, suppose we assign arbitrary indices to caseframes, a summary sentence contains caseframes {1, 2, 3, 4, 5}, and the source tteaixnt c ocanstaeifrnasm tehsre e{1 ,s2en,t3e,n4c,e5s} ,w aitnhd caseframes, which can be represented as a nested set {{1, 3, 4}, {2, 5, 6}, {1, 4, 7}}. [sent-162, score-0.851]
</p><p>58 imum set cover problem, in which sentences are sets, and caseframes are set elements. [sent-166, score-0.706]
</p><p>59 Most peer systems have a low average sentence cover size of close to 1, which reflects the fact that they are purely or almost purely extractive. [sent-170, score-0.468]
</p><p>60 Human model summarizers show a higher degree of aggregation in their summaries. [sent-171, score-0.255]
</p><p>61 2 Study 2: Signature caseframe density Study 1 shows that human summarizers are more abstractive in that they aggregate information from multiple sentences in the source text, but how is this aggregation performed? [sent-178, score-0.694]
</p><p>62 One possibility is that human summary writers are able to pack a  greater number of salient caseframes into their summaries. [sent-179, score-0.832]
</p><p>63 That is, humans are fundamentally relying on centrality just as automatic summarizers do, and are simply able to achieve higher compression ratios by being more succinct. [sent-180, score-0.31]
</p><p>64 Unfortunately, we show that this is false and that system summaries are actually more central than model ones. [sent-182, score-0.258]
</p><p>65 To extract topical caseframes, we use Lin and Hovy’s (2000) method of calculating signature terms, but extend the method to apply it at the caseframe rather than the word level. [sent-183, score-0.402]
</p><p>66 084∗  Table 3: Signature caseframe densities for different sets of summarizers, for the initial and update guided summarization tasks (Study 2). [sent-194, score-0.555]
</p><p>67 Figure 4 shows examples of signature caseframes for several topics. [sent-199, score-0.819]
</p><p>68 Then, we calculate the signature caseframe density of each of the summarization systems. [sent-200, score-0.652]
</p><p>69 This is defined to be the number of signature caseframes in the set of summaries divided by the number of words in that set of summaries. [sent-201, score-1.059]
</p><p>70 As can be seen, the human abstractors actually tend to use fewer signature caseframes in their summaries than automatic systems. [sent-203, score-1.158]
</p><p>71 075  Table 4: Density of signature caseframes after merging to various threshold for the initial (Init. [sent-234, score-0.819]
</p><p>72 not merely repeat the caseframes that are indicative of a topic cluster or use minor grammatical  alternations in their summaries. [sent-237, score-0.716]
</p><p>73 Merging Caseframes We next investigate whether simple paraphrasing could account for the above results; it may be the case that human summarizers simply replace words in the source text with synonyms, which can be detected with distributional similarity. [sent-239, score-0.306]
</p><p>74 Thus, we merged similar caseframes into clusters according to the distributional semantic similarity defined in Section 3. [sent-240, score-0.717]
</p><p>75 That is, each caseframe begins as a separate cluster, and the two most similar clusters are merged at each step until the desired similarity threshold is reached. [sent-247, score-0.282]
</p><p>76 Once again, model summaries contain a lower density of signature caseframes. [sent-254, score-0.455]
</p><p>77 This indicates that simple paraphrasing alone cannot account for the difference in the signature caseframe densities, and that some deeper abstraction or semantic inference has occurred. [sent-256, score-0.479]
</p><p>78 Note that we are not claiming that a lower density of signature caseframes necessarily correlates with a more informative summary. [sent-257, score-0.884]
</p><p>79 3 Study 3: Summary Reconstruction The above studies show that the higher degree of abstraction in model summaries cannot be explained by better compression of topically salient caseframes alone. [sent-261, score-1.076]
</p><p>80 We now switch perspectives to ask how model summaries might be automatically generated at all. [sent-262, score-0.24]
</p><p>81 Our measure of whether a model summary can be reconstructed is caseframe coverage. [sent-265, score-0.391]
</p><p>82 We define this to be the proportion of caseframes in a summary that is contained  by some reference set. [sent-266, score-0.775]
</p><p>83 Results We first calculated caseframe coverage with respect to the source text alone (Figure 5). [sent-269, score-0.361]
</p><p>84 As expected, automatic systems show close to perfect coverage, because of their basically extractive nature, while model summaries show much lower coverage. [sent-270, score-0.376]
</p><p>85 These results present a fundamental limit to extractive systems, and also text simplification and sentence fusion methods based solely on the source text. [sent-272, score-0.274]
</p><p>86 The Impact of Domain Knowledge How might automatic summarizers be able to acquire these 1239  (a) Initial guided summarization task  (b) Update summarization task Figure 5: Coverage of summary text caseframes in source text (Study 3). [sent-273, score-1.52]
</p><p>87 00  Table 5: Coverage of caseframes in summaries with respect to the source text. [sent-284, score-0.965]
</p><p>88 Traditional systems that perform semantic inference do so from a set of known facts about the domain in the form of a knowledge base, but as we have seen, most extractive summarization systems do not make much use of in-domain corpora. [sent-287, score-0.403]
</p><p>89 As shown in Table 6, the effect of adding more in-domain text on caseframe coverage is substantial, and noticeably more than using out-of-domain text. [sent-292, score-0.286]
</p><p>90 In fact, nearly all caseframes can be found in the expanded set of articles. [sent-293, score-0.669]
</p><p>91 The implication of this result is that it may be possible to generate better summaries by mining in-domain text for relevant caseframes. [sent-294, score-0.268]
</p><p>92 97 Table 6: The effect on caseframe coverage of  adding in-domain and out-of-domain documents. [sent-301, score-0.258]
</p><p>93 5  Conclusion  We have presented a series of studies to distinguish human-written informative summaries from the summaries produced by current systems. [sent-303, score-0.518]
</p><p>94 First, we confirm that model summaries are more abstractive and aggregate information from multiple source text sentences. [sent-305, score-0.405]
</p><p>95 Then, we show that this is not simply due to summary writers packing together source text sentences containing topical caseframes to achieve a higher compression ratio, even if paraphrasing is taken into account. [sent-306, score-0.944]
</p><p>96 Indeed, model summaries cannot be reconstructed from the source text alone. [sent-307, score-0.375]
</p><p>97 However, our results are also positive in that we find that nearly all model summary caseframes can be found in the source text together with some indomain documents. [sent-308, score-0.885]
</p><p>98 Current summarization systems have been heavily optimized towards centrality and lexicalsemantical reasoning, but we are nearing the bottom of the barrel. [sent-309, score-0.316]
</p><p>99 Mining useful caseframes 1240  for a sentence fusion-based approach has the potential, as our experiments have shown, to deliver results in just the areas where current approaches are weakest. [sent-311, score-0.689]
</p><p>100 Automatically evaluating content selection in summarization without human models. [sent-417, score-0.228]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('caseframes', 0.669), ('peer', 0.369), ('summaries', 0.24), ('caseframe', 0.234), ('summarization', 0.203), ('summarizers', 0.173), ('signature', 0.15), ('extractive', 0.117), ('summary', 0.106), ('prep', 0.103), ('centrality', 0.094), ('dobj', 0.087), ('abstractive', 0.081), ('trucks', 0.07), ('density', 0.065), ('sped', 0.062), ('nenkova', 0.061), ('aggregation', 0.06), ('source', 0.056), ('abstractors', 0.056), ('nsubj', 0.056), ('guided', 0.054), ('responsiveness', 0.053), ('abstraction', 0.052), ('reconstructed', 0.051), ('domain', 0.045), ('compression', 0.043), ('pyramid', 0.043), ('conditioninitialupdate', 0.042), ('plead', 0.042), ('unabomber', 0.042), ('kathleen', 0.038), ('hovy', 0.038), ('conroy', 0.038), ('toronto', 0.038), ('ani', 0.037), ('cover', 0.037), ('copeck', 0.037), ('tac', 0.037), ('mckeown', 0.036), ('saggion', 0.036), ('fusion', 0.034), ('gov', 0.034), ('densities', 0.032), ('update', 0.032), ('slots', 0.032), ('salient', 0.032), ('scare', 0.031), ('bomb', 0.031), ('mmr', 0.031), ('text', 0.028), ('indictment', 0.028), ('liwei', 0.028), ('sanocki', 0.028), ('squad', 0.028), ('szpakowicz', 0.026), ('indomain', 0.026), ('trial', 0.026), ('human', 0.025), ('lin', 0.025), ('cplex', 0.025), ('bean', 0.025), ('backpack', 0.025), ('genest', 0.025), ('lapalme', 0.025), ('rouge', 0.024), ('paraphrasing', 0.024), ('similarity', 0.024), ('study', 0.024), ('clusters', 0.024), ('alternations', 0.024), ('coverage', 0.024), ('average', 0.023), ('kill', 0.023), ('cluster', 0.023), ('conditions', 0.023), ('voicing', 0.023), ('oront', 0.023), ('ilog', 0.023), ('cheung', 0.023), ('degree', 0.022), ('sentence', 0.02), ('series', 0.02), ('expected', 0.02), ('attacks', 0.019), ('alone', 0.019), ('systems', 0.019), ('solely', 0.019), ('analyzed', 0.019), ('substantial', 0.019), ('actually', 0.018), ('topical', 0.018), ('victim', 0.018), ('gupta', 0.018), ('guy', 0.018), ('studies', 0.018), ('horacio', 0.018), ('agglomerative', 0.018), ('carbonell', 0.018), ('barzilay', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="353-tfidf-1" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.</p><p>2 0.18199691 <a title="353-tfidf-2" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>3 0.16187538 <a title="353-tfidf-3" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>Author: Lu Wang ; Claire Cardie</p><p>Abstract: We address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion. We apply Multiple-Sequence Alignment to induce abstract generation templates that can be used for different domains. An Overgenerateand-Rank strategy is utilized to produce and rank candidate abstracts. Experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches. In addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality.</p><p>4 0.14611532 <a title="353-tfidf-4" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>Author: Rebecca J. Passonneau ; Emily Chen ; Weiwei Guo ; Dolores Perin</p><p>Abstract: The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students’ summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics.</p><p>5 0.13677043 <a title="353-tfidf-5" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>Author: Miguel Almeida ; Andre Martins</p><p>Abstract: We present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear programming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers.</p><p>6 0.1330916 <a title="353-tfidf-6" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>7 0.1329446 <a title="353-tfidf-7" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>8 0.12081838 <a title="353-tfidf-8" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>9 0.11984926 <a title="353-tfidf-9" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>10 0.11903234 <a title="353-tfidf-10" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>11 0.075852007 <a title="353-tfidf-11" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>12 0.072836488 <a title="353-tfidf-12" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>13 0.070155688 <a title="353-tfidf-13" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>14 0.06579946 <a title="353-tfidf-14" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>15 0.05439394 <a title="353-tfidf-15" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>16 0.053975381 <a title="353-tfidf-16" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>17 0.052062787 <a title="353-tfidf-17" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>18 0.042451341 <a title="353-tfidf-18" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>19 0.041800648 <a title="353-tfidf-19" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>20 0.041758712 <a title="353-tfidf-20" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, 0.043), (2, 0.008), (3, -0.084), (4, -0.001), (5, 0.011), (6, 0.11), (7, 0.024), (8, -0.171), (9, -0.092), (10, -0.031), (11, 0.058), (12, -0.143), (13, -0.003), (14, -0.084), (15, 0.152), (16, 0.159), (17, -0.143), (18, -0.003), (19, 0.073), (20, -0.028), (21, -0.108), (22, -0.011), (23, -0.03), (24, 0.015), (25, -0.05), (26, 0.017), (27, 0.005), (28, 0.042), (29, 0.022), (30, -0.049), (31, 0.024), (32, 0.052), (33, -0.056), (34, -0.07), (35, 0.012), (36, -0.068), (37, -0.023), (38, -0.029), (39, -0.021), (40, 0.018), (41, 0.046), (42, 0.02), (43, 0.032), (44, 0.046), (45, -0.003), (46, 0.01), (47, 0.019), (48, 0.037), (49, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91375548 <a title="353-lsi-1" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.</p><p>2 0.8508755 <a title="353-lsi-2" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>3 0.8062498 <a title="353-lsi-3" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>Author: Anirban Dasgupta ; Ravi Kumar ; Sujith Ravi</p><p>Abstract: We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</p><p>4 0.77980465 <a title="353-lsi-4" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>Author: Rebecca J. Passonneau ; Emily Chen ; Weiwei Guo ; Dolores Perin</p><p>Abstract: The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students’ summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics.</p><p>5 0.77647638 <a title="353-lsi-5" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>6 0.74599165 <a title="353-lsi-6" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>7 0.73698288 <a title="353-lsi-7" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>8 0.64661354 <a title="353-lsi-8" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>9 0.6417442 <a title="353-lsi-9" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>10 0.63117319 <a title="353-lsi-10" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>11 0.58545423 <a title="353-lsi-11" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>12 0.51587516 <a title="353-lsi-12" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>13 0.50717622 <a title="353-lsi-13" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>14 0.49358293 <a title="353-lsi-14" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>15 0.45203498 <a title="353-lsi-15" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>16 0.42670542 <a title="353-lsi-16" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>17 0.41848013 <a title="353-lsi-17" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>18 0.394761 <a title="353-lsi-18" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>19 0.38511705 <a title="353-lsi-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.34146723 <a title="353-lsi-20" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.041), (6, 0.087), (11, 0.069), (24, 0.041), (26, 0.062), (28, 0.012), (35, 0.067), (42, 0.054), (48, 0.034), (60, 0.024), (64, 0.017), (70, 0.035), (86, 0.235), (88, 0.055), (90, 0.029), (95, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74112684 <a title="353-lda-1" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.</p><p>2 0.65289944 <a title="353-lda-2" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>Author: Amjad Abu-Jbara ; Ben King ; Mona Diab ; Dragomir Radev</p><p>Abstract: In this paper, we use Arabic natural language processing techniques to analyze Arabic debates. The goal is to identify how the participants in a discussion split into subgroups with contrasting opinions. The members of each subgroup share the same opinion with respect to the discussion topic and an opposing opinion to the members of other subgroups. We use opinion mining techniques to identify opinion expressions and determine their polarities and their targets. We opinion predictions to represent the discussion in one of two formal representations: signed attitude network or a space of attitude vectors. We identify opinion subgroups by partitioning the signed network representation or by clustering the vector space representation. We evaluate the system using a data set of labeled discussions and show that it achieves good results.</p><p>3 0.61723709 <a title="353-lda-3" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. Then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. The agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data. The segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature.</p><p>4 0.58441716 <a title="353-lda-4" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>Author: Joohyun Kim ; Raymond Mooney</p><p>Abstract: We adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees.</p><p>5 0.57120734 <a title="353-lda-5" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>6 0.57073826 <a title="353-lda-6" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>7 0.56926757 <a title="353-lda-7" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>8 0.56374311 <a title="353-lda-8" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>9 0.56141114 <a title="353-lda-9" href="./acl-2013-Modeling_Thesis_Clarity_in_Student_Essays.html">246 acl-2013-Modeling Thesis Clarity in Student Essays</a></p>
<p>10 0.55981916 <a title="353-lda-10" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>11 0.55818832 <a title="353-lda-11" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>12 0.55717361 <a title="353-lda-12" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>13 0.55646223 <a title="353-lda-13" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>14 0.55529761 <a title="353-lda-14" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>15 0.5525707 <a title="353-lda-15" href="./acl-2013-Automatic_detection_of_deception_in_child-produced_speech_using_syntactic_complexity_features.html">63 acl-2013-Automatic detection of deception in child-produced speech using syntactic complexity features</a></p>
<p>16 0.55084026 <a title="353-lda-16" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>17 0.55048466 <a title="353-lda-17" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>18 0.54879057 <a title="353-lda-18" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>19 0.54874718 <a title="353-lda-19" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>20 0.54842514 <a title="353-lda-20" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
