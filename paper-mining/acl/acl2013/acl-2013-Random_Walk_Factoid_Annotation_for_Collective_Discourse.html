<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-293" href="#">acl2013-293</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</h1>
<br/><p>Source: <a title="acl-2013-293-pdf" href="http://aclweb.org/anthology//P/P13/P13-2045.pdf">pdf</a></p><p>Author: Ben King ; Rahul Jha ; Dragomir Radev ; Robert Mankoff</p><p>Abstract: In this paper, we study the problem of automatically annotating the factoids present in collective discourse. Factoids are information units that are shared between instances of collective discourse and may have many different ways ofbeing realized in words. Our approach divides this problem into two steps, using a graph-based approach for each step: (1) factoid discovery, finding groups of words that correspond to the same factoid, and (2) factoid assignment, using these groups of words to mark collective discourse units that contain the respective factoids. We study this on two novel data sets: the New Yorker caption contest data set, and the crossword clues data set.</p><p>Reference: <a title="acl-2013-293-reference" href="../acl2013_reference/acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Random Walk Factoid Annotation for Collective Discourse Ben King Rahul Jha Department of EECS University of Michigan Ann Arbor, MI benking@umi ch . [sent-1, score-0.022]
</p><p>2 Radev Department of EECS School of Information University of Michigan Ann Arbor, MI radev@umi ch . [sent-4, score-0.022]
</p><p>3 com  Abstract In this paper, we study the problem of automatically annotating the factoids present in collective discourse. [sent-6, score-0.634]
</p><p>4 Factoids are information units that are shared between instances of collective discourse and may have many different ways ofbeing realized in words. [sent-7, score-0.215]
</p><p>5 We study this on two novel data sets: the New Yorker caption contest data set, and the crossword clues data set. [sent-9, score-0.919]
</p><p>6 1 Introduction Collective discourse tends to contain relatively few factoids, or information units about which the author speaks, but many nuggets, different ways to speak about or refer to a factoid (Qazvinian and Radev, 2011). [sent-10, score-0.421]
</p><p>7 Many natural language applications could be improved with good factoid annotation. [sent-11, score-0.274]
</p><p>8 Our approach in this paper divides this problem into two subtasks: discovery of factoids, and assignment of factoids. [sent-12, score-0.051]
</p><p>9 We take a graph-based approach to the problem, clustering a word graph to discover factoids and using random walks to assign factoids to discourse units. [sent-13, score-1.383]
</p><p>10 We also introduce two new datasets in this paper, covered in more detail in section 3. [sent-14, score-0.023]
</p><p>11 The New Yorker cartoon caption dataset, provided by Robert Mankoff, the cartoon editor at The New Yorker magazine, is composed of readersubmitted captions for a cartoon published in the magazine. [sent-15, score-1.571]
</p><p>12 The crossword clue dataset consists ∗ Cartoon Editor, The New Yorker magazine  Figure 1: The cartoon used for the New Yorker caption contest #33 1. [sent-16, score-1.163]
</p><p>13 of word-clue pairs used in major American crossword puzzles, with most words having several hundred different clues published for it. [sent-17, score-0.622]
</p><p>14 The term “factoid” is used as in (Van Halteren and Teufel, 2003), but in a slightly more abstract sense in this paper, denoting a set of related words that should ideally refer to a real-world entity, but  may not for some of the less coherent factoids. [sent-18, score-0.017]
</p><p>15 The factoids discovered using this method don’t necessarily correspond to the factoids that might be chosen by annotators. [sent-19, score-1.084]
</p><p>16 ” The automatic methods however, might say that these captions share factoid3, which is identified by the words “mother,” “in-laws,” “family,” “house,” etc. [sent-21, score-0.315]
</p><p>17 The layout ofthis paper is as follows: we review related work in section 2, we introduce the datasets 249  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-22, score-0.023]
</p><p>18 2  Related Work  The distribution of factoids present in text collections is important for several NLP tasks such as summarization. [sent-25, score-0.533]
</p><p>19 The Pyramid Evaluation method (Nenkova and Passonneau, 2004) for automatic summary evaluation depends on finding and annotating factoids in input sentences. [sent-26, score-0.583]
</p><p>20 Qazvinian and Radev (201 1) also studied the properties of factoids present in collective human datasets and used it to create a summarization system. [sent-27, score-0.652]
</p><p>21 (2010) describe an approach for automatically learning factoids for pyramid evaluation using a topic modeling approach. [sent-29, score-0.584]
</p><p>22 Das and Petrov (201 1) also introduced a graph-based method for part-of-speech tagging in which edge weights are based on feature vectors similarity, which is like the corpus-based lexical similarity graph that we construct. [sent-31, score-0.147]
</p><p>23 3  Data Sets  We introduce two new data sets in this paper, the New Yorker caption contest data set, and the crossword clues data set. [sent-32, score-0.919]
</p><p>24 Though these two data sets are quite different, they share a few important characteristics. [sent-33, score-0.028]
</p><p>25 First, the discourse units tend to be short, approximately ten words for cartoon captions and approximately three words for crossword clues. [sent-34, score-1.181]
</p><p>26 Second, though the authors act independently, they tend to produce surprisingly similar text, making the same sorts of jokes, or referring to words in the same sorts of ways. [sent-35, score-0.048]
</p><p>27 Thirdly, the authors often try to be non-obvious: obvious jokes are often not funny, and obvious crossword clues make a puzzle less challenging. [sent-36, score-0.666]
</p><p>28 1 New Yorker Caption Contest Data Set The New Yorker magazine holds a weekly contest1 in which they publish a cartoon without a caption and solicit caption suggestions from their readers. [sent-38, score-0.823]
</p><p>29 The three funniest captions are selected by the editor and published in the following weeks. [sent-39, score-0.341]
</p><p>30 com/humor/caption I ’t care what planet they are from, they can pass on the don left like everyone else. [sent-43, score-0.105]
</p><p>31 Idon’t care what planet they’re from, they should have the common courtesy to dim their lights. [sent-44, score-0.072]
</p><p>32 If he wants to pass, he use the right lane like everyone else. [sent-46, score-0.024]
</p><p>33 You’d better call your mother and tell her to set a few extra place settings. [sent-49, score-0.07]
</p><p>34 can  you pass on  Which finger do I use for aliens? [sent-51, score-0.058]
</p><p>35 I the middle finger means the same thing to them. [sent-52, score-0.033]
</p><p>36 ”  guess sense  was  Table 1: Captions for contest #33 1. [sent-55, score-0.117]
</p><p>37 this research project, we have acquired five cartoons along with all of the captions submitted in the corresponding contest. [sent-57, score-0.342]
</p><p>38 While the task of automatically identifying the funny captions would be quite useful, it is well beyond the current state of the art in NLP. [sent-58, score-0.312]
</p><p>39 A much more manageable task, and one that is quite important for the contest’s editor is to annotate captions according to their factoids. [sent-59, score-0.316]
</p><p>40 This allows the organizers of the contest to find the most frequently mentioned factoids and select representative captions for each factoid. [sent-60, score-0.92]
</p><p>41 On average, each cartoon has 5,400 submitted captions, but for each of five cartoons, we sampled 500 captions for annotation. [sent-61, score-0.641]
</p><p>42 The annotators were instructed to mark factoids by identifying and grouping events, objects, and themes present in the captions, creating a unique name for each factoid, and marking the captions that contain each factoid. [sent-62, score-0.838]
</p><p>43 For example, in cartoon #33 1, such factoids may be “bad directions”, “police”, “take me  to your leader”, “racism”, or “headlights”. [sent-64, score-0.869]
</p><p>44 After annotating, each set of captions contained about 60 factoids on average. [sent-65, score-0.82]
</p><p>45 90 factoids, with approximately 80% of the discourse units having at least one factoid, 20% having at least two, and only 2% having more than two. [sent-67, score-0.147]
</p><p>46 As van Halteren and Teufel (2003) also found 250  4620000115050000 0  20  40  (a)  60  0  5  10  15  20  (b)  Figure 2: Average factoid frequency distributions for cartoon captions (a) and crossword clues (b). [sent-70, score-1.494]
</p><p>47 25  4620 0 010 20 30 40 50 1050 10 20 30 40 50 (a)  (b)  Figure 3: Growth of the number of unique factoids as the size ofthe corpus grows for cartoon captions (a) and crossword clues (b). [sent-71, score-1.771]
</p><p>48 when examining factoid distributions in humanproduced summaries, we found that the distribution of factoids in the caption set for each cartoon seems to follow a power law. [sent-72, score-1.382]
</p><p>49 Figure 2 shows the average frequencies of factoids, when ordered from most- to least-frequent. [sent-73, score-0.024]
</p><p>50 We also found a Heap’s law-type effect in the number of unique factoids compared to the size of the corpus, as in Figure 3. [sent-74, score-0.551]
</p><p>51 2  Crossword Clues Data Set  Clues in crossword puzzles are typically obscure,  requiring the reader to recognize double meanings or puns, which leads to a great deal of diversity. [sent-76, score-0.467]
</p><p>52 These clues can also refer to one or more of many different senses of the word. [sent-77, score-0.214]
</p><p>53 Table 2 shows examples of many different clues for the word “tea”. [sent-78, score-0.186]
</p><p>54 This table clearly illustrates the difference between factoids (the senses being referred to) and nuggets (the realization of the factoids). [sent-79, score-0.591]
</p><p>55 com collects a large number of clues that appear in different published crossword puzzles and aggregates them according to their answer. [sent-81, score-0.678]
</p><p>56 From this site, we collected 200 sets of clues for common crossword answers. [sent-82, score-0.597]
</p><p>57 We manually annotated 20 sets of crossword clues according to their factoids in the same fashion as described in section 3. [sent-83, score-1.13]
</p><p>58 On average each set of clues contains 283 clues and 15 different factoids. [sent-85, score-0.396]
</p><p>59 Afternoon social 4:00 gathering Sympathy partner Mythical Irish queen Party movement Word with rose or garden  drink event event film person political movement plant and place  Table 2: Examples of crossword clues and their different senses for the word “tea”. [sent-91, score-0.692]
</p><p>60 1 Random Walk Method We take a graph-based approach to the discovery of factoids, clustering a word similarity graph and taking the resulting clusters to be the factoids. [sent-93, score-0.195]
</p><p>61 Two different graphs, a word co-occurrence graph and a lexical similarity graph learned from the corpus, are compared. [sent-94, score-0.194]
</p><p>62 We also compare the graph-based methods against baselines of clustering and topic modeling. [sent-95, score-0.062]
</p><p>63 1 Word Co-occurrence Graph To create the word co-occurrence graph, we create  a link between every pair of words with an edge weight proportional to the number of times they both occur in the same discourse unit. [sent-98, score-0.127]
</p><p>64 2 Corpus-based Lexical Similarity Graph To build the lexical similarity graph, a lexical similarity function is learned from the corpus, that is, from one set of captions or clues. [sent-101, score-0.351]
</p><p>65 We do this by computing feature vectors for each lemma and using the cosine similarity between these feature vectors as a lexical similarity function. [sent-102, score-0.098]
</p><p>66 We construct a word graph with edge weights proportional to the learned similarity of the respective word pairs. [sent-103, score-0.147]
</p><p>67 Context part-of-speech features are the part-of-speech labels given by the Stanford POS tagger (Toutanova et al. [sent-106, score-0.023]
</p><p>68 Table 3 shows examples of similar word pairs from the set of crossword clues for “tea”. [sent-109, score-0.597]
</p><p>69 From 251  Figure 4: Example of natural clusters in a subsection of the word  co-occurrence  graph for the crossword  clue “astro”. [sent-110, score-0.58]
</p><p>70 38 Table 3: Examples of similar pairs of words as calculated on the set of crossword clues for “tea”. [sent-122, score-0.597]
</p><p>71 3  Graph Clustering  To cluster the word similarity graph, we use the Louvain graph clustering method (Blondel et al. [sent-128, score-0.189]
</p><p>72 Figure 4 shows an example of clusters found in the word graph for the crossword clue “astro”. [sent-132, score-0.58]
</p><p>73 There are three obvious clusters, one for the Houston Astros baseball team, one for the dog in the Jetsons cartoon, and one for the lexical prefix “astro-”. [sent-133, score-0.046]
</p><p>74 In this example, two of the clusters are connected by a clue that mentions multiple senses, “Houston ballplayer or Jetson dog”. [sent-134, score-0.088]
</p><p>75 4 Random Walk Factoid Assignment After discovering factoids, the remaining task is to annotate captions according to the factoids they contain. [sent-137, score-0.82]
</p><p>76 We approach this problem by taking random walks on the word graph constructed in the previous sections, starting the random walks from words in the caption and measuring the hitting times to different clusters. [sent-138, score-0.606]
</p><p>77 For each discourse unit, we repeatedly sample words from it and take Markov random walks starting from the nodes corresponding to the selected and lasting 10 steps (which is enough to ensure that every node in the graph can be reached). [sent-139, score-0.288]
</p><p>78 After 1000 random walks, we measure the average hitting time to each cluster, where a cluster is considered to be reached by the random walk the first time a node in that cluster is reached. [sent-140, score-0.298]
</p><p>79 Heuristically, 1000 random walks was more than enough to ensure that the factoid distribution had stabilized in development data. [sent-141, score-0.388]
</p><p>80 The labels that are applied to a caption are the labels of the clusters that have a sufficiently low hitting time. [sent-142, score-0.412]
</p><p>81 We perform five-fold cross valida-  tion on each caption or set of clues and tune the threshold on the hitting time such that the average number of labels per unit produced matches the average number of labels per unit in the gold annotation of the held-out portion. [sent-143, score-0.699]
</p><p>82 For example, a certain caption may have the following hitting times to the different factoid clusters: factoid1 factoid2 factoid3 factoid4  0. [sent-144, score-0.603]
</p><p>83 2 factoids per caption, it may be determined that the optimal thresh252  old on the hitting times is 0. [sent-149, score-0.657]
</p><p>84 In this case factoid1 and factoid2 would be marked for this caption, since the hitting times fall below the threshold. [sent-153, score-0.107]
</p><p>85 2 Clustering A simple baseline that can act as a surrogate for factoid annotation is clustering of discourse units, which is equivalent to assigning exactly one factoid (the name of its cluster) to each discourse unit. [sent-155, score-0.779]
</p><p>86 As our clustering method, we use C-Lexrank (Qazvinian and Radev, 2008), a method that has been well-tested on collective discourse. [sent-156, score-0.113]
</p><p>87 3 Topic Model Topic modeling is a natural way to approach the problem of factoid annotation, if we consider the topics to be factoids. [sent-158, score-0.274]
</p><p>88 As with the random walk method, we perform five-fold cross validation, tuning the threshold for the average number of labels per discourse unit to match the average number of labels in the held-out portion. [sent-161, score-0.308]
</p><p>89 We also use the average number of unique factoids in the held-out  portion as the number of LDA topics. [sent-163, score-0.575]
</p><p>90 This is a reasonable evaluation method, since the average number of factoids per discourse unit is close to one. [sent-166, score-0.695]
</p><p>91 Because the factoids discovered by this method don’t necessarily match the factoids chosen by the annotators, it doesn’t make sense to try to measure whether two discourse units share the “correct” factoid. [sent-167, score-1.276]
</p><p>92 Tables 4 and 5 show the results of the various methods on the cartoon captions and crossword clues datasets, respectively. [sent-168, score-1.22]
</p><p>93 On the crossword clues datasets, the random-walk-based methods are clearly superior to the other methods tested, whereas simple clustering is more effective on the  Method Prec. [sent-169, score-0.642]
</p><p>94 162 Table 4: Performance of various methods annotating factoids for cartoon captions. [sent-183, score-0.902]
</p><p>95 447 Table 5: Performance of various methods annotating factoids for crossword clues. [sent-198, score-0.977]
</p><p>96 In some sense, the two datasets in this paper both represent difficult domains, ones in which authors are intentionally obscure. [sent-200, score-0.023]
</p><p>97 The good results acheived on the crossword clues dataset indicate that this obscurity can be overcome when  discourse units are short. [sent-201, score-0.744]
</p><p>98 Learning from collective human behavior to introduce diversity in lexical choice. [sent-238, score-0.068]
</p><p>99 Examining the consensus between human summaries: initial experiments with factoid analysis. [sent-247, score-0.274]
</p><p>100 In Proceedings of the HLT-NAACL 03 on Text summarization workshop-Volume 5, pages 57–64. [sent-248, score-0.028]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('factoids', 0.533), ('crossword', 0.411), ('cartoon', 0.336), ('captions', 0.287), ('factoid', 0.274), ('caption', 0.222), ('clues', 0.186), ('yorker', 0.168), ('hitting', 0.107), ('contest', 0.1), ('discourse', 0.093), ('graph', 0.081), ('leader', 0.072), ('walks', 0.069), ('collective', 0.068), ('qazvinian', 0.063), ('puzzles', 0.056), ('radev', 0.055), ('units', 0.054), ('mother', 0.053), ('tea', 0.052), ('clue', 0.051), ('earl', 0.049), ('lady', 0.049), ('walk', 0.047), ('clustering', 0.045), ('umi', 0.043), ('halteren', 0.043), ('magazine', 0.043), ('astro', 0.037), ('cartoons', 0.037), ('hennig', 0.037), ('mankoff', 0.037), ('clusters', 0.037), ('dragomir', 0.037), ('pyramid', 0.034), ('annotating', 0.033), ('finger', 0.033), ('houston', 0.033), ('similarity', 0.032), ('cluster', 0.031), ('divides', 0.03), ('idon', 0.03), ('planet', 0.03), ('nuggets', 0.03), ('random', 0.029), ('editor', 0.029), ('jokes', 0.029), ('summarization', 0.028), ('unit', 0.028), ('share', 0.028), ('senses', 0.028), ('care', 0.026), ('vahed', 0.026), ('blondel', 0.026), ('dog', 0.026), ('pass', 0.025), ('funny', 0.025), ('drink', 0.025), ('published', 0.025), ('eecs', 0.024), ('sorts', 0.024), ('everyone', 0.024), ('average', 0.024), ('house', 0.024), ('teufel', 0.024), ('datasets', 0.023), ('grey', 0.023), ('labels', 0.023), ('ch', 0.022), ('assignment', 0.021), ('rahul', 0.021), ('movement', 0.021), ('nenkova', 0.02), ('mallet', 0.02), ('indian', 0.02), ('obvious', 0.02), ('lda', 0.02), ('michigan', 0.019), ('submitted', 0.018), ('company', 0.018), ('unique', 0.018), ('hassan', 0.018), ('discovered', 0.018), ('per', 0.017), ('proportional', 0.017), ('spelling', 0.017), ('sense', 0.017), ('summary', 0.017), ('topic', 0.017), ('examining', 0.017), ('tell', 0.017), ('vectors', 0.017), ('edge', 0.017), ('supermarket', 0.016), ('lasting', 0.016), ('dim', 0.016), ('benking', 0.016), ('jerk', 0.016), ('stabilized', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="293-tfidf-1" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>Author: Ben King ; Rahul Jha ; Dragomir Radev ; Robert Mankoff</p><p>Abstract: In this paper, we study the problem of automatically annotating the factoids present in collective discourse. Factoids are information units that are shared between instances of collective discourse and may have many different ways ofbeing realized in words. Our approach divides this problem into two steps, using a graph-based approach for each step: (1) factoid discovery, finding groups of words that correspond to the same factoid, and (2) factoid assignment, using these groups of words to mark collective discourse units that contain the respective factoids. We study this on two novel data sets: the New Yorker caption contest data set, and the crossword clues data set.</p><p>2 0.3266899 <a title="293-tfidf-2" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>Author: Rahul Jha ; Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In this paper, we investigate the problem of automatic generation of scientific surveys starting from keywords provided by a user. We present a system that can take a topic query as input and generate a survey of the topic by first selecting a set of relevant documents, and then selecting relevant sentences from those documents. We discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids, or information units, from 47 gold standard documents (surveys and tutorials) on seven topics in Natural Language Processing. We have manually annotated 2,625 sentences with these factoids (around 375 sentences per topic) to build an evaluation corpus for this task. We present evaluation results for the performance of our system using this annotated data.</p><p>3 0.17745596 <a title="293-tfidf-3" href="./acl-2013-Generalizing_Image_Captions_for_Image-Text_Parallel_Corpus.html">167 acl-2013-Generalizing Image Captions for Image-Text Parallel Corpus</a></p>
<p>Author: Polina Kuznetsova ; Vicente Ordonez ; Alexander Berg ; Tamara Berg ; Yejin Choi</p><p>Abstract: The ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision. However, the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text. We address this challenge with contributions in two folds: first, we introduce the new task of image caption generalization, formulated as visually-guided sentence compression, and present an efficient algorithm based on dynamic beam search with dependency-based constraints. Second, we release a new large-scale corpus with 1 million image-caption pairs achieving tighter content alignment between images and text. Evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new imagetext parallel corpus with respect to a concrete application of image caption transfer.</p><p>4 0.068545707 <a title="293-tfidf-4" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>5 0.066635862 <a title="293-tfidf-5" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>6 0.056937128 <a title="293-tfidf-6" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>7 0.055551872 <a title="293-tfidf-7" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>8 0.049348474 <a title="293-tfidf-8" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>9 0.045253444 <a title="293-tfidf-9" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>10 0.043613084 <a title="293-tfidf-10" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>11 0.040751226 <a title="293-tfidf-11" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>12 0.040615149 <a title="293-tfidf-12" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>13 0.039211415 <a title="293-tfidf-13" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>14 0.037467193 <a title="293-tfidf-14" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>15 0.036914304 <a title="293-tfidf-15" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>16 0.03682768 <a title="293-tfidf-16" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>17 0.036591887 <a title="293-tfidf-17" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>18 0.035833701 <a title="293-tfidf-18" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>19 0.035469417 <a title="293-tfidf-19" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>20 0.035340082 <a title="293-tfidf-20" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.097), (1, 0.057), (2, 0.003), (3, -0.041), (4, 0.023), (5, -0.036), (6, 0.024), (7, 0.023), (8, -0.083), (9, -0.008), (10, 0.003), (11, 0.005), (12, -0.063), (13, 0.071), (14, 0.017), (15, 0.02), (16, 0.067), (17, -0.046), (18, -0.091), (19, -0.041), (20, 0.016), (21, 0.018), (22, 0.026), (23, -0.066), (24, -0.03), (25, -0.056), (26, 0.029), (27, -0.015), (28, 0.052), (29, -0.106), (30, 0.064), (31, -0.05), (32, -0.04), (33, -0.022), (34, 0.008), (35, -0.107), (36, -0.074), (37, 0.006), (38, 0.023), (39, -0.015), (40, 0.074), (41, 0.07), (42, 0.192), (43, -0.068), (44, 0.158), (45, -0.056), (46, -0.054), (47, 0.045), (48, 0.001), (49, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89134216 <a title="293-lsi-1" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>Author: Ben King ; Rahul Jha ; Dragomir Radev ; Robert Mankoff</p><p>Abstract: In this paper, we study the problem of automatically annotating the factoids present in collective discourse. Factoids are information units that are shared between instances of collective discourse and may have many different ways ofbeing realized in words. Our approach divides this problem into two steps, using a graph-based approach for each step: (1) factoid discovery, finding groups of words that correspond to the same factoid, and (2) factoid assignment, using these groups of words to mark collective discourse units that contain the respective factoids. We study this on two novel data sets: the New Yorker caption contest data set, and the crossword clues data set.</p><p>2 0.68233424 <a title="293-lsi-2" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>Author: Rahul Jha ; Amjad Abu-Jbara ; Dragomir Radev</p><p>Abstract: In this paper, we investigate the problem of automatic generation of scientific surveys starting from keywords provided by a user. We present a system that can take a topic query as input and generate a survey of the topic by first selecting a set of relevant documents, and then selecting relevant sentences from those documents. We discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids, or information units, from 47 gold standard documents (surveys and tutorials) on seven topics in Natural Language Processing. We have manually annotated 2,625 sentences with these factoids (around 375 sentences per topic) to build an evaluation corpus for this task. We present evaluation results for the performance of our system using this annotated data.</p><p>3 0.42125136 <a title="293-lsi-3" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>Author: Xiaorui Jiang ; Xiaoping Sun ; Hai Zhuge</p><p>Abstract: School of thought analysis is an important yet not-well-elaborated scientific knowledge discovery task. This paper makes the first attempt at this problem. We focus on one aspect of the problem: do characteristic school-of-thought words exist and whether they are characterizable? To answer these questions, we propose a probabilistic generative School-Of-Thought (SOT) model to simulate the scientific authoring process based on several assumptions. SOT defines a school of thought as a distribution of topics and assumes that authors determine the school of thought for each sentence before choosing words to deliver scientific ideas. SOT distinguishes between two types of school-ofthought words for either the general background of a school of thought or the original ideas each paper contributes to its school of thought. Narrative and quantitative experiments show positive and promising results to the questions raised above. 1</p><p>4 0.41213876 <a title="293-lsi-4" href="./acl-2013-Generalizing_Image_Captions_for_Image-Text_Parallel_Corpus.html">167 acl-2013-Generalizing Image Captions for Image-Text Parallel Corpus</a></p>
<p>Author: Polina Kuznetsova ; Vicente Ordonez ; Alexander Berg ; Tamara Berg ; Yejin Choi</p><p>Abstract: The ever growing amount of web images and their associated texts offers new opportunities for integrative models bridging natural language processing and computer vision. However, the potential benefits of such data are yet to be fully realized due to the complexity and noise in the alignment between image content and text. We address this challenge with contributions in two folds: first, we introduce the new task of image caption generalization, formulated as visually-guided sentence compression, and present an efficient algorithm based on dynamic beam search with dependency-based constraints. Second, we release a new large-scale corpus with 1 million image-caption pairs achieving tighter content alignment between images and text. Evaluation results show the intrinsic quality of the generalized captions and the extrinsic utility of the new imagetext parallel corpus with respect to a concrete application of image caption transfer.</p><p>5 0.3815164 <a title="293-lsi-5" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>Author: Rebecca J. Passonneau ; Emily Chen ; Weiwei Guo ; Dolores Perin</p><p>Abstract: The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students’ summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics.</p><p>6 0.3658444 <a title="293-lsi-6" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>7 0.36222926 <a title="293-lsi-7" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>8 0.35885188 <a title="293-lsi-8" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>9 0.35587829 <a title="293-lsi-9" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>10 0.3514367 <a title="293-lsi-10" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>11 0.35143626 <a title="293-lsi-11" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>12 0.34936076 <a title="293-lsi-12" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>13 0.34922612 <a title="293-lsi-13" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>14 0.34871727 <a title="293-lsi-14" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>15 0.34784403 <a title="293-lsi-15" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>16 0.34385243 <a title="293-lsi-16" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>17 0.33462873 <a title="293-lsi-17" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>18 0.33187655 <a title="293-lsi-18" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>19 0.31883246 <a title="293-lsi-19" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>20 0.31428391 <a title="293-lsi-20" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (6, 0.031), (11, 0.042), (21, 0.371), (24, 0.04), (26, 0.046), (35, 0.066), (42, 0.038), (48, 0.044), (57, 0.018), (70, 0.054), (88, 0.03), (90, 0.037), (95, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7382527 <a title="293-lda-1" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>Author: Ben King ; Rahul Jha ; Dragomir Radev ; Robert Mankoff</p><p>Abstract: In this paper, we study the problem of automatically annotating the factoids present in collective discourse. Factoids are information units that are shared between instances of collective discourse and may have many different ways ofbeing realized in words. Our approach divides this problem into two steps, using a graph-based approach for each step: (1) factoid discovery, finding groups of words that correspond to the same factoid, and (2) factoid assignment, using these groups of words to mark collective discourse units that contain the respective factoids. We study this on two novel data sets: the New Yorker caption contest data set, and the crossword clues data set.</p><p>2 0.6058107 <a title="293-lda-2" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>Author: Xingxing Zhang ; Jianwen Zhang ; Junyu Zeng ; Jun Yan ; Zheng Chen ; Zhifang Sui</p><p>Abstract: Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.</p><p>3 0.6030072 <a title="293-lda-3" href="./acl-2013-Predicting_and_Eliciting_Addressee%27s_Emotion_in_Online_Dialogue.html">282 acl-2013-Predicting and Eliciting Addressee's Emotion in Online Dialogue</a></p>
<p>Author: Takayuki Hasegawa ; Nobuhiro Kaji ; Naoki Yoshinaga ; Masashi Toyoda</p><p>Abstract: While there have been many attempts to estimate the emotion of an addresser from her/his utterance, few studies have explored how her/his utterance affects the emotion of the addressee. This has motivated us to investigate two novel tasks: predicting the emotion of the addressee and generating a response that elicits a specific emotion in the addressee’s mind. We target Japanese Twitter posts as a source of dialogue data and automatically build training data for learning the predictors and generators. The feasibility of our approaches is assessed by using 1099 utterance-response pairs that are built by . five human workers.</p><p>4 0.57786059 <a title="293-lda-4" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>Author: Haonan Yu ; Jeffrey Mark Siskind</p><p>Abstract: We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.</p><p>5 0.50096411 <a title="293-lda-5" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>Author: Akihiro Tamura ; Taro Watanabe ; Eiichiro Sumita ; Hiroya Takamura ; Manabu Okumura</p><p>Abstract: This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forest- to-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model.</p><p>6 0.41774762 <a title="293-lda-6" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>7 0.35552797 <a title="293-lda-7" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>8 0.35143 <a title="293-lda-8" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>9 0.35109764 <a title="293-lda-9" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>10 0.3505913 <a title="293-lda-10" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>11 0.34938836 <a title="293-lda-11" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>12 0.34914702 <a title="293-lda-12" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>13 0.34821588 <a title="293-lda-13" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>14 0.34816334 <a title="293-lda-14" href="./acl-2013-Lightly_Supervised_Learning_of_Procedural_Dialog_Systems.html">230 acl-2013-Lightly Supervised Learning of Procedural Dialog Systems</a></p>
<p>15 0.34714279 <a title="293-lda-15" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>16 0.34714037 <a title="293-lda-16" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>17 0.34636411 <a title="293-lda-17" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>18 0.34554118 <a title="293-lda-18" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>19 0.34522185 <a title="293-lda-19" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>20 0.34506556 <a title="293-lda-20" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
