<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-210" href="#">acl2013-210</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</h1>
<br/><p>Source: <a title="acl-2013-210-pdf" href="http://aclweb.org/anthology//P/P13/P13-1106.pdf">pdf</a></p><p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>Reference: <a title="acl-2013-210-reference" href="../acl2013_reference/acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. [sent-9, score-0.293]
</p><p>2 However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. [sent-10, score-0.272]
</p><p>3 We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. [sent-11, score-1.005]
</p><p>4 We intro-  duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. [sent-12, score-0.523]
</p><p>5 We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. [sent-13, score-0.679]
</p><p>6 Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines. [sent-14, score-0.383]
</p><p>7 1 Introduction We study the problem of Named Entity Recognition (NER) in a bilingual context, where the goal is to annotate parallel bi-texts with named entity tags. [sent-15, score-0.401]
</p><p>8 In this work, we first develop a bilingual NER model (denoted as BI-NER) by embedding two monolingual CRF-based NER models into a larger undirected graphical model, and introduce additional edge factors based on word alignment (WA). [sent-32, score-0.845]
</p><p>9 Because the new bilingual model contains many cyclic cliques, exact inference is intractable. [sent-33, score-0.272]
</p><p>10 We employ a dual decomposition (DD) inference algorithm (Bertsekas, 1999; Rush et al. [sent-34, score-0.327]
</p><p>11 The dashed alignment link between e3 and f2 is an example of  alignment error. [sent-40, score-0.49]
</p><p>12 , 2010; DeNero and Macherey, 2011; Chieu and Teow, 2012), our method decomposes the larger graphical model into many overlapping components where each alignment edge forms a separate factor. [sent-43, score-0.438]
</p><p>13 We design clique potentials over the alignment-based edges to encourage entity tag agreements. [sent-44, score-0.29]
</p><p>14 Our method does not require any manual annotation of word alignments or named entities over the bilingual training data. [sent-45, score-0.302]
</p><p>15 The aforementioned BI-NER model assumes fixed alignment input given by an underlying word aligner. [sent-46, score-0.28]
</p><p>16 But the entity span and type predictions given by the NER models contain complementary information for correcting alignment errors. [sent-47, score-0.354]
</p><p>17 To capture this source of information, we present a novel extension that combines the BI-NER model with two uni-directional HMM-based alignment models, and perform joint decoding of NER and word alignments. [sent-48, score-0.387]
</p><p>18 The new model (denoted as BI-NER-WA) factors over five components: one NER model and one word alignment model for each language, plus a joint NER-alignment model  which not only enforces NER label agreements but also facilitates message passing among the other four components. [sent-49, score-0.516]
</p><p>19 We also assume that a set of word alignments (A = {(i, j) : ei ↔ fj}) is given by a word aligner a{n(id, rje)m :ain e fi↔xed ifn our sm goidveenl. [sent-58, score-0.214]
</p><p>20 For clarity, we assume and yf are binary variables in the description of our algorithms. [sent-59, score-0.326]
</p><p>21 1 Hard Agreement We define a BI-NER model which imposes hard agreement of entity labels over aligned word pairs. [sent-62, score-0.287]
</p><p>22 f) k(ye) + l(yf) 3 yie = yjf ∀(i, j) ∈ A  mye,ayxf log(PCRFe =myea,yxf  ’myea,yxf  We dropped the Ze(e) and Zf(f) terms because they ere dmroaipnp ecdon tshetan Zt (ate )in afenrden Zce time. [sent-71, score-0.417]
</p><p>23 Instead of solving the Lagrangian directly, we can form the dual of this problem and solve it using dual decomposition (Rush et al. [sent-81, score-0.465]
</p><p>24 A thorough introduction to the theoretical foundations of dual decomposition algorithms is beyond the scope of this paper; we encourage unfamiliar readers to read Rush and Collins (2012) for a full tutorial. [sent-89, score-0.325]
</p><p>25 2 Soft Agreement The previously discussed hard agreement model rests on the core assumption that aligned words must have identical entity tags. [sent-91, score-0.287]
</p><p>26 The assumption in the hard agreement model can also be violated if there are word alignment errors. [sent-111, score-0.36]
</p><p>27 In order to model this uncertainty, we extend the two previously independent CRF models into a larger undirected graphical model, by introducing a cross-lingual edge factor φ(i, j) for every pair of word positions (i, j) ∈ A. [sent-112, score-0.261]
</p><p>28 Pˆ(ei,fj)  pmi(yie,yjf)  where is the point-wise mutual information (PMI) of the tag pair, and we raise it to the power of a posterior alignment probability fj). [sent-117, score-0.359]
</p><p>29 The use of the posterior alignment probability facilitates this purpose. [sent-119, score-0.302]
</p><p>30 Pˆ(ei,  1075  Simultaneously, the monolingual models will also be encouraged to agree with the cross-lingual edge factors. [sent-121, score-0.293]
</p><p>31 Since we assume no bilingually annotated NER corpus is available, in order to get an estimate of the PMI scores, we first tag a collection of unannotated bilingual sentence pairs using the monolingual CRF taggers, and collect counts of aligned entity pairs from this auto-generated tagged data. [sent-123, score-0.616]
</p><p>32 , f3 on Chinese side and e4 on English side), and we seek agreement with the Chinese CRF model over tag assignment of fj, and similarly for ei on English side. [sent-128, score-0.317]
</p><p>33 In other words, no direct agreement between the two CRF models is enforced, but they both need to agree with the bilingual edge factors. [sent-129, score-0.429]
</p><p>34 where the notation denotes tag assignment to word ei by the English CRF and denotes as-  yei(h)  yfj(l)  signment to word ei by the bilingual factor; denotes the tag assignment to word fj by the Chi-  yfj(h)  nese CRF and denotes assignment to word fj by the bilingual factor. [sent-140, score-1.117]
</p><p>35 We introduce two separate sets of dual constraints and wf, which range over the set of vertices on their respective half of the graph. [sent-142, score-0.219]
</p><p>36 Decoding the edge factor model h(i,j)(yie, simply involves finding the pair of tag assignments that gives the highest PMI score, subject to the dual constraints. [sent-143, score-0.417]
</p><p>37 Joint Alignment and NER Decoding  In this section we develop an extended model in which NER information can in turn be used to improve alignment accuracy. [sent-146, score-0.28]
</p><p>38 Although we have seen more than a handful of recent papers that apply the dual decomposition method for joint inference problems, all of the past work deals with cases where the various model components have the same inference output space (e. [sent-147, score-0.447]
</p><p>39 In our case the output space is the much more complex joint alignment and NER tagging space. [sent-153, score-0.321]
</p><p>40 We propose a novel dual decomposition variant for performing inference over this joint space. [sent-154, score-0.369]
</p><p>41 Most commonly used alignment models, such as the IBM models and HMM-based aligner are unsupervised learners, and can only capture simple distortion features and lexical translational fea-  tures due to the high complexity of the structure prediction space. [sent-155, score-0.303]
</p><p>42 The entity label predictions made by the NER model can potentially be leveraged to correct alignment mistakes. [sent-157, score-0.389]
</p><p>43 For example, in Figure 1, if the tagger knows that the word “Agency” is tagged I-ORG, and if it also knows that the first comma in the Chinese sentence is not part of any entity, then we can infer it is very unlikely that there exists an alignment link between “Agency” and the comma. [sent-158, score-0.27]
</p><p>44 To capture this intuition, we extend the BI-NER model to jointly perform word alignment and NER decoding, and call the resulting model BI-NERWA. [sent-159, score-0.315]
</p><p>45 In our experiments, we used two HMMbased alignment models. [sent-164, score-0.245]
</p><p>46 But in principle we can adopt any alignment model as long as we can perform efficient inference over it. [sent-165, score-0.323]
</p><p>47 We introduce a cross-lingual edge factor ζ(i, j) in the undirected graphical model for every pair of word indices (i, j), which predicts a binary vari-  Bf  bf  1076  Algorithm 2 DD inference algorithm for joint alignment and NER model. [sent-166, score-0.713]
</p><p>48 able a(i, j) for an alignment link between ei and fj. [sent-200, score-0.357]
</p><p>49 The edge factor also predicts the entity tags for ei and fj. [sent-201, score-0.406]
</p><p>50 + S(yie,  yjf|a(i,j))P(a(i,j)=1)  S(yei,yjf|a(i,j))=(0p,meil(seyie,yjf),if a(i,j) = 1 P(a(i, j) = 1) is the alignment probability assigned by the bilingual edge factor between node ei and fj. [sent-206, score-0.695]
</p><p>51 We include two dual constraints de(i, j) and df(i, j) over alignments for every bilingual edge factor ζ(i, j), which are applied to the English and Chinese sides of the alignment space, respectively. [sent-219, score-0.846]
</p><p>52 One special note is that after each iteration when we consider updates to the dual constraint for entity tags, we only check tag agreements for cross-lingual edge factors that have an alignment assignment value of 1. [sent-221, score-0.828]
</p><p>53 In other  words, cross-lingual edges that are not aligned do not affect bilingual NER tagging. [sent-222, score-0.257]
</p><p>54 But the real power of these cross-language edge cliques is that they act as a liaison between the NER and alignment models on each language side, and encourage these models to indirectly agree with each other by having them all agree with the edge cliques. [sent-224, score-0.659]
</p><p>55 It is also worth noting that since we decode the alignment models with Viterbi inference, additional constraints such as the neighborhood constraint proposed by DeNero and Macherey (201 1) can be easily integrated into our model. [sent-225, score-0.34]
</p><p>56 The neighborhood constraint enforces that if fj is aligned to ei, then fj can only be aligned to ei+1 or ei−1 (with a small penalty), but not any other word position. [sent-226, score-0.475]
</p><p>57 4O00ut oofc utmhee 1nt8s (n∼am39ekd entity types trh Eatn are annotated in OntoNotes, which include person, location, date, money, and so on, we select the four most commonly seen named entity types for evaluation. [sent-246, score-0.282]
</p><p>58 When we consider label agreements over aligned word pairs in all bilingual agreement models, we ignore the distinction between B- and I -  tags. [sent-249, score-0.389]
</p><p>59 For alignment experiments, we train two uni1The exact feature set and the CRF implementation can be found here: http : / /nlp . [sent-252, score-0.245]
</p><p>60 shtml directional HMM models as our baseline and monolingual alignment models. [sent-255, score-0.418]
</p><p>61 Each model is trained for 2 iterations over a parallel corpus of 12 million English words and Chinese words, almost twice as much data as used in previous work that yields state-of-the-art unsupervised alignment results (DeNero and Klein, 2008; Haghighi et al. [sent-258, score-0.34]
</p><p>62 Word alignment evaluation is done over the sections of OntoNotes that have matching gold-  standard word alignment annotations from GALE Y1Q4 dataset. [sent-260, score-0.49]
</p><p>63 In joint NER and alignment experiments, instead of posterior thresholding, we take the direct intersection of the Viterbi-best alignment of the two directional models. [sent-267, score-0.624]
</p><p>64 We report the standard P, R, F1 and Alignment Error Rate (AER) measures for alignment experiments. [sent-268, score-0.245]
</p><p>65 Their method is similar to ours in that they also model bilingual agreement in conjunction with two CRFbased monolingual models. [sent-271, score-0.447]
</p><p>66 But instead of using just the PMI scores of bilingual NE pairs, as in our work, they employed a feature-rich log-linear  model to capture bilingual correlations. [sent-272, score-0.423]
</p><p>67 To counter this problem, they proposed an “up-training” method which simulates a supervised learning environment by pairing a weak classifier with strong classifiers, and train the bilingual model to rank the output of the strong classifier highly among the N-best outputs of the weak classifier. [sent-274, score-0.229]
</p><p>68 32160 Table 1: NER results on bilingual parallel test set. [sent-285, score-0.228]
</p><p>69 By jointly decoding NER with word alignment, our model not only maintains significant improvements in NER performance, but also yields significant improvements to alignment performance. [sent-303, score-0.345]
</p><p>70 In terms of word alignment results, we see great increases in F1 and recall, but precision goes down significantly. [sent-308, score-0.245]
</p><p>71 This is be-  cause the joint decoding algorithm promotes an effect of “soft-union”, by encouraging the two unidirectional aligners to agree more often. [sent-309, score-0.222]
</p><p>72 In this example, a snippet of a longer sentence pair is shown with NER and word alignment results. [sent-312, score-0.245]
</p><p>73 Our bilingual NER model is able to correct this error as expected. [sent-315, score-0.229]
</p><p>74 Similarly, the bilingual model corrects the error over e11. [sent-316, score-0.229]
</p><p>75 Dotted alignment links are the oracle, dashed links are alignments from HMM baseline, and solid links are outputs of our model. [sent-334, score-0.289]
</p><p>76 If we were to take this alignment as fixed input, most likely we would not be able to recover the error over e11, but the  joint decoding method successfully recovered 4 more links, and indirectly resulted in the NER tagging improvement discussed above. [sent-338, score-0.421]
</p><p>77 8  Related Work  The idea of employing bilingual resources to improve over monolingual systems has been explored by much previous work. [sent-339, score-0.332]
</p><p>78 (2009) improved parsing performance using a bilingual parallel corpus. [sent-341, score-0.228]
</p><p>79 The feature-rich CRF formulation of bilingual edge potentials in their model is much more powerful than our simple PMI-based bilingual edge model. [sent-344, score-0.691]
</p><p>80 Adding a richer bilingual edge model might well further improve our results, and this is a possible direction for further experimentation. [sent-345, score-0.343]
</p><p>81 However, a big drawback of this approach is that training such a feature-rich model requires manually annotated bilingual NER data, which can be prohibitively expensive to generate. [sent-346, score-0.229]
</p><p>82 (2010b), which explored an “up-training” mechanism by using the outputs from a strong monolingual model as ground-truth, and simulated a learning environment where a bilingual model is trained to help a “weakened” monolingual model to recover the results of the strong model. [sent-349, score-0.575]
</p><p>83 It is worth mentioning that since our method does not require additional training and can take pretty much any existing model as “black-box” during decoding, the richer and more accurate bilingual model learned from Burkett et al. [sent-350, score-0.264]
</p><p>84 A similar dual decomposition algorithm to ours was proposed by Riedel and McCallum (201 1) for biomedical event detection. [sent-352, score-0.284]
</p><p>85 In their Model 3, the trigger and argument extraction models are reminiscent of the two monolingual CRFs in our model; additional binding agreements are enforced over every protein pair, similar to how we  enforce agreement between every aligned word 1080  pair. [sent-353, score-0.333]
</p><p>86 Our joint alignment and NER decoding approach is inspired by prior work on improving alignment quality through encouraging agreement between bi-directional models (Liang et al. [sent-364, score-0.677]
</p><p>87 Instead of enforcing agreement in the alignment space based on best sequences found by Viterbi, we could opt to encourage agreement between posterior probability distributions, which is related to the posterior regularization work by Gra ¸ca et al. [sent-366, score-0.589]
</p><p>88 This idea is similar to ourjoint alignment and NER approach, but in our case the phrasal constraints are indirectly imposed by entity spans. [sent-369, score-0.427]
</p><p>89 (2010a) presented a supervised learning method for performing joint parsing and word alignment using log-linear models over parse trees and an ITG model over alignment. [sent-372, score-0.322]
</p><p>90 The  model demonstrates performance improvements in both parsing and alignment, but shares the common limitations of other supervised work in that it requires manually annotated bilingual joint parsing and word alignment data. [sent-373, score-0.516]
</p><p>91 (2010) also tackled the problem of joint alignment and NER. [sent-375, score-0.287]
</p><p>92 Their method employs a set of heuristic rules to expand a candidate named entity set generated by monolingual taggers, and then rank those candidates using a bilingual named entity dictionary. [sent-376, score-0.678]
</p><p>93 9  Conclusion  We introduced a graphical model that combines two HMM word aligners and two CRF NER taggers into a joint model, and presented a dual decomposition inference method for performing efficient decoding over this model. [sent-378, score-0.592]
</p><p>94 Results from NER and word alignment experiments suggest that our method gives significant improvements in both NER and word alignment. [sent-379, score-0.245]
</p><p>95 Joint parsing and alignment with weakly synchronized grammars. [sent-414, score-0.245]
</p><p>96 Combining local and non-local information with dual decomposition for named entity recognition from text. [sent-426, score-0.457]
</p><p>97 An alignment algorithm using belief propagation and a structure-based distortion model. [sent-430, score-0.278]
</p><p>98 Improved named entity translation and bilingual named entity extraction. [sent-462, score-0.54]
</p><p>99 A tutorial on dual decomposition and Lagrangian relaxation for inference in natural language processing. [sent-519, score-0.327]
</p><p>100 On dual decomposition and linear programming relaxations for natural language processing. [sent-524, score-0.284]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ner', 0.371), ('yf', 0.326), ('ye', 0.246), ('alignment', 0.245), ('yie', 0.226), ('bilingual', 0.194), ('yfj', 0.191), ('yjf', 0.191), ('dual', 0.181), ('fj', 0.146), ('dd', 0.14), ('yei', 0.138), ('monolingual', 0.138), ('burkett', 0.127), ('bf', 0.122), ('rush', 0.121), ('edge', 0.114), ('ei', 0.112), ('entity', 0.109), ('crf', 0.106), ('denero', 0.104), ('decomposition', 0.103), ('argmax', 0.083), ('agreement', 0.08), ('ontonotes', 0.077), ('chinese', 0.076), ('macherey', 0.076), ('decoding', 0.065), ('pmi', 0.065), ('named', 0.064), ('aligned', 0.063), ('lagrangian', 0.059), ('aligner', 0.058), ('tag', 0.057), ('posterior', 0.057), ('neighborhood', 0.057), ('wie', 0.052), ('wjf', 0.052), ('agreements', 0.052), ('df', 0.052), ('conv', 0.046), ('graphical', 0.044), ('alignments', 0.044), ('au', 0.044), ('inference', 0.043), ('clique', 0.043), ('chieu', 0.043), ('joint', 0.042), ('encourage', 0.041), ('agree', 0.041), ('taggers', 0.041), ('tags', 0.041), ('potentials', 0.04), ('hmm', 0.039), ('constraints', 0.038), ('aligners', 0.038), ('martins', 0.038), ('undirected', 0.038), ('finkel', 0.038), ('factors', 0.037), ('koo', 0.037), ('unidirectional', 0.036), ('directional', 0.035), ('crfbased', 0.035), ('melody', 0.035), ('pcrfe', 0.035), ('pcrff', 0.035), ('teow', 0.035), ('indirectly', 0.035), ('model', 0.035), ('tagging', 0.034), ('parallel', 0.034), ('aer', 0.034), ('vi', 0.033), ('belief', 0.033), ('assignment', 0.033), ('dev', 0.032), ('agency', 0.032), ('stopping', 0.031), ('zf', 0.031), ('cromi', 0.031), ('eres', 0.031), ('tibet', 0.031), ('return', 0.031), ('factor', 0.03), ('consistency', 0.03), ('enforcing', 0.029), ('blitzer', 0.028), ('unannotated', 0.028), ('loopy', 0.028), ('babych', 0.028), ('cliques', 0.028), ('champollion', 0.028), ('soft', 0.027), ('bilingually', 0.027), ('log', 0.026), ('name', 0.026), ('iterations', 0.026), ('tagger', 0.025), ('efron', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="210-tfidf-1" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>2 0.22134933 <a title="210-tfidf-2" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>3 0.20640112 <a title="210-tfidf-3" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>Author: Chris Quirk</p><p>Abstract: The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.</p><p>4 0.19184841 <a title="210-tfidf-4" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>Author: Manaal Faruqui ; Chris Dyer</p><p>Abstract: We present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages. The monolingual component of our objective is the average mutual information of clusters of adjacent words in each language, while the bilingual component is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters.</p><p>5 0.17322887 <a title="210-tfidf-5" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>6 0.16224593 <a title="210-tfidf-6" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>7 0.15384191 <a title="210-tfidf-7" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>8 0.14778295 <a title="210-tfidf-8" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>9 0.14238411 <a title="210-tfidf-9" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>10 0.13273284 <a title="210-tfidf-10" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>11 0.12978448 <a title="210-tfidf-11" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>12 0.12228815 <a title="210-tfidf-12" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>13 0.12046111 <a title="210-tfidf-13" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>14 0.11728106 <a title="210-tfidf-14" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>15 0.11344131 <a title="210-tfidf-15" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>16 0.10999527 <a title="210-tfidf-16" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>17 0.10574722 <a title="210-tfidf-17" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>18 0.10453836 <a title="210-tfidf-18" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>19 0.10262214 <a title="210-tfidf-19" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>20 0.10240468 <a title="210-tfidf-20" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.246), (1, -0.099), (2, 0.024), (3, 0.051), (4, 0.109), (5, 0.046), (6, -0.113), (7, -0.007), (8, -0.058), (9, -0.072), (10, 0.072), (11, -0.272), (12, -0.042), (13, -0.136), (14, 0.052), (15, 0.037), (16, 0.09), (17, 0.044), (18, 0.018), (19, -0.118), (20, -0.11), (21, 0.009), (22, -0.018), (23, 0.126), (24, -0.052), (25, 0.078), (26, -0.156), (27, 0.01), (28, 0.081), (29, 0.015), (30, 0.035), (31, -0.036), (32, -0.031), (33, 0.049), (34, -0.104), (35, 0.047), (36, -0.026), (37, 0.075), (38, 0.171), (39, 0.061), (40, 0.027), (41, -0.038), (42, -0.057), (43, -0.029), (44, 0.021), (45, -0.05), (46, 0.031), (47, -0.054), (48, -0.006), (49, -0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95598853 <a title="210-lsi-1" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>2 0.79782784 <a title="210-lsi-2" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>Author: Chris Quirk</p><p>Abstract: The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.</p><p>3 0.76863468 <a title="210-lsi-3" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>4 0.73556131 <a title="210-lsi-4" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>Author: Thomas Schoenemann</p><p>Abstract: We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores.</p><p>5 0.68218017 <a title="210-lsi-5" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>6 0.64395517 <a title="210-lsi-6" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>7 0.63440442 <a title="210-lsi-7" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>8 0.61493272 <a title="210-lsi-8" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>9 0.60541749 <a title="210-lsi-9" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>10 0.5538944 <a title="210-lsi-10" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>11 0.53669757 <a title="210-lsi-11" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>12 0.53369862 <a title="210-lsi-12" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<p>13 0.51009166 <a title="210-lsi-13" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>14 0.4685896 <a title="210-lsi-14" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>15 0.44648901 <a title="210-lsi-15" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>16 0.43574777 <a title="210-lsi-16" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>17 0.42683861 <a title="210-lsi-17" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>18 0.42446446 <a title="210-lsi-18" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>19 0.42037469 <a title="210-lsi-19" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>20 0.41904899 <a title="210-lsi-20" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.083), (6, 0.355), (11, 0.06), (15, 0.013), (24, 0.028), (26, 0.046), (35, 0.054), (42, 0.049), (48, 0.035), (70, 0.046), (88, 0.027), (90, 0.022), (95, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94896817 <a title="210-lda-1" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>Author: Chris Quirk</p><p>Abstract: The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.</p><p>2 0.93762934 <a title="210-lda-2" href="./acl-2013-Reducing_Annotation_Effort_for_Quality_Estimation_via_Active_Learning.html">300 acl-2013-Reducing Annotation Effort for Quality Estimation via Active Learning</a></p>
<p>Author: Daniel Beck ; Lucia Specia ; Trevor Cohn</p><p>Abstract: Quality estimation models provide feedback on the quality of machine translated texts. They are usually trained on humanannotated datasets, which are very costly due to its task-specific nature. We investigate active learning techniques to reduce the size of these datasets and thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors. ,t .</p><p>3 0.92610413 <a title="210-lda-3" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>Author: Dehong Gao ; Wenjie Li ; Renxian Zhang</p><p>Abstract: The growth of the Web 2.0 technologies has led to an explosion of social networking media sites. Among them, Twitter is the most popular service by far due to its ease for realtime sharing of information. It collects millions of tweets per day and monitors what people are talking about in the trending topics updated timely. Then the question is how users can understand a topic in a short time when they are frustrated with the overwhelming and unorganized tweets. In this paper, this problem is approached by sequential summarization which aims to produce a sequential summary, i.e., a series of chronologically ordered short subsummaries that collectively provide a full story about topic development. Both the number and the content of sub-summaries are automatically identified by the proposed stream-based and semantic-based approaches. These approaches are evaluated in terms of sequence coverage, sequence novelty and sequence correlation and the effectiveness of their combination is demonstrated. 1 Introduction and Background Twitter, as a popular micro-blogging service, collects millions of real-time short text messages (known as tweets) every second. It acts as not only a public platform for posting trifles about users’ daily lives, but also a public reporter for real-time news. Twitter has shown its powerful ability in information delivery in many events, like the wildfires in San Diego and the earthquake in Japan. Nevertheless, the side effect is individual users usually sink deep under millions of flooding-in tweets. To alleviate this problem, the applications like whatthetrend 1 have evolved from Twitter to provide services that encourage users to edit explanatory tweets about a trending topic, which can be regarded as topic summaries. It is to some extent a good way to help users understand trending topics. 1 whatthetrend.com There is also pioneering research in automatic Twitter trending topic summarization. (O'Connor et al., 2010) explained Twitter trending topics by providing a list of significant terms. Users could utilize these terms to drill down to the tweets which are related to the trending topics. (Sharifi et al., 2010) attempted to provide a one-line summary for each trending topic using phrase reinforcement ranking. The relevance model employed by (Harabagiu and Hickl, 2011) generated summaries in larger size, i.e., 250word summaries, by synthesizing multiple high rank tweets. (Duan et al., 2012) incorporate the user influence and content quality information in timeline tweet summarization and employ reinforcement graph to generate summaries for trending topics. Twitter summarization is an emerging research area. Current approaches still followed the traditional summarization route and mainly focused on mining tweets of both significance and representativeness. Though, the summaries generated in such a way can sketch the most important aspects of the topic, they are incapable of providing full descriptions of the changes of the focus of a topic, and the temporal information or freshness of the tweets, especially for those newsworthy trending topics, like earthquake and sports meeting. As the main information producer in Twitter, the massive crowd keeps close pace with the development of trending topics and provide the timely updated information. The information dynamics and timeliness is an important consideration for Twitter summarization. That is why we propose sequential summarization in this work, which aims to produce sequential summaries to capture the temporal changes of mass focus. Our work resembles update summarization promoted by TAC 2 which required creating summaries with new information assuming the reader has already read some previous documents under the same topic. Given two chronologically ordered documents sets about a topic, the systems were asked to generate two 2 www.nist.gov/tac 567 summaries, and the second one should inform the user of new information only. In order to achieve this goal, existing approaches mainly emphasized the novelty of the subsequent summary (Li and Croft, 2006; Varma et al., 2009; Steinberger and Jezek, 2009). Different from update summarization, we focus more on the temporal change of trending topics. In particular, we need to automatically detect the “update points” among a myriad of related tweets. It is the goal of this paper to set up a new practical summarization application tailored for timely updated Twitter messages. With the aim of providing a full description of the focus changes and the records of the timeline of a trending topic, the systems are expected to discover the chronologically ordered sets of information by themselves and they are free to generate any number of update summaries according to the actual situations instead of a fixed number of summaries as specified in DUC/TAC. Our main contributions include novel approaches to sequential summarization and corresponding evaluation criteria for this new application. All of them will be detailed in the following sections. 2 Sequential Summarization Sequential summarization proposed here aims to generate a series of chronologically ordered subsummaries for a given Twitter trending topic. Each sub-summary is supposed to represent one main subtopic or one main aspect of the topic, while a sequential summary, made up by the subsummaries, should retain the order the information is delivered to the public. In such a way, the sequential summary is able to provide a general picture of the entire topic development. 2.1 Subtopic Segmentation One of the keys to sequential summarization is subtopic segmentation. How many subtopics have attracted the public attention, what are they, and how are they developed? It is important to provide the valuable and organized materials for more fine-grained summarization approaches. We proposed the following two approaches to automatically detect and chronologically order the subtopics. 2.1.1 Stream-based Subtopic Detection and Ordering Typically when a subtopic is popular enough, it will create a certain level of surge in the tweet stream. In other words, every surge in the tweet stream can be regarded as an indicator of the appearance of a subtopic that is worthy of being summarized. Our early investigation provides evidence to support this assumption. By examining the correlations between tweet content changes and volume changes in randomly selected topics, we have observed that the changes in tweet volume can really provide the clues of topic development or changes of crowd focus. The stream-based subtopic detection approach employs the offline peak area detection (Opad) algorithm (Shamma et al., 2010) to locate such surges by tracing tweet volume changes. It regards the collection of tweets at each such surge time range as a new subtopic. Offline Peak Area Detection (Opad) Algorithm 1: Input: TS (tweets stream, each twi with timestamp ti); peak interval window ∆? (in hour), and time stepℎ (ℎ ≪ ∆?); 2: Output: Peak Areas PA. 3: Initial: two time slots: ?′ = ? = ?0 + ∆?; Tweet numbers: ?′ = ? = ?????(?) 4: while (?? = ? + ℎ) < ??−1 5: update ?′ = ?? + ∆? and ?′ = ?????(?′) 6: if (?′ < ? And up-hilling) 7: output one peak area ??? 8: state of down-hilling 9: else 10: update ? = ?′ and ? = ?′ 11: state of up-hilling 12: 13: function ?????(?) 14: Count tweets in time interval T The subtopics detected by the Opad algorithm are naturally ordered in the timeline. 2.1.2 Semantic-based Subtopic Detection and Ordering Basically the stream-based approach monitors the changes of the level of user attention. It is easy to implement and intuitively works, but it fails to handle the cases where the posts about the same subtopic are received at different time ranges due to the difference of geographical and time zones. This may make some subtopics scattered into several time slots (peak areas) or one peak area mixed with more than one subtopic. In order to sequentially segment the subtopics from the semantic aspect, the semantic-based subtopic detection approach breaks the time order of tweet stream, and regards each tweet as an individual short document. It takes advantage of Dynamic Topic Modeling (David and Michael, 2006) to explore the tweet content. 568 DTM in nature is a clustering approach which can dynamically generate the subtopic underlying the topic. Any clustering approach requires a pre-specified cluster number. To avoid tuning the cluster number experimentally, the subtopic number required by the semantic-based approach is either calculated according to heuristics or determined by the number of the peak areas detected from the stream-based approach in this work. Unlike the stream-based approach, the subtopics formed by DTM are the sets of distributions of subtopic and word probabilities. They are time independent. Thus, the temporal order among these subtopics is not obvious and needs to be discovered. We use the probabilistic relationships between tweets and topics learned from DTM to assign each tweet to a subtopic that it most likely belongs to. Then the subtopics are ordered temporally according to the mean values of their tweets’ timestamps. 2.2 Sequential Summary Generation Once the subtopics are detected and ordered, the tweets belonging to each subtopic are ranked and the most significant one is extracted to generate the sub-summary regarding that subtopic. Two different ranking strategies are adopted to conform to two different subtopic detection mechanisms. For a tweet in a peak area, the linear combination of two measures is considered to independently. Each sub-summary is up to 140 characters in length to comply with the limit of tweet, but the annotators are free to choose the number of sub-summaries. It ends up with 6.3 and 4.8 sub-summaries on average in a sequential summary written by the two annotators respectively. These two sets of sequential summaries are regarded as reference summaries to evaluate system-generated summaries from the following three aspects. Sequence Coverage Sequence coverage measures the N-gram match between system-generated summaries and human-written summaries (stopword removed first). Considering temporal information is an important factor in sequential summaries, we evaluate its significance to be a sub-summary: (1) subtopic representativeness measured by the  cosine similarity between the tweet and the centroid of all the tweets in the same peak area; (2) crowding endorsement measured by the times that the tweet is re-tweeted normalized by the total number of re-tweeting. With the DTM model, the significance of the tweets is evaluated directly by word distribution per subtopic. MMR (Carbonell and Goldstein, 1998) is used to reduce redundancy in sub-summary generation. 3 Experiments and Evaluations The experiments are conducted on the 24 Twitter trending topics collected using Twitter APIs 3 . The statistics are shown in Table 1. Due to the shortage of gold-standard sequential summaries, we invite two annotators to read the chronologically ordered tweets, and write a series of sub-summaries for each topic 3https://dev.twitter.com/ propose the position-aware coverage measure by accommodating the position information in matching. Let S={s1, s2, sk} denote a … … …, sequential summary and si the ith sub-summary, N-gram coverage is defined as: ???????? =|? 1?|?∑?∈? ?∑? ? ?∈?∙ℎ ?∑ ? ?∈?-?ℎ? ?∑? ∈-? ?,? ? ? ?∈? ? ? ? ? ? ? (ℎ?(?-?-? ? ? ?) where, ??? = |? − ?| + 1, i and j denote the serial numbers of the sub-summaries in the systemgenerated summary ??? and the human-written summary ?ℎ? , respectively. ? serves as a coefficient to discount long-distance matched sub-summaries. We evaluate unigram, bigram, and skipped bigram matches. Like in ROUGE (Lin, 2004), the skip distance is up to four words.  Sequence Novelty Sequence novelty evaluates the average novelty of two successive sub-summaries. Information content (IC) has been used to measure the novelty of update summaries by (Aggarwal et al., 2009). In this paper, the novelty of a system569 generated sequential summary is defined as the average of IC increments of two adjacent subsummaries, ??????? =|?|1 − 1?∑>1(????− ????, ??−1) × where |?| is the number of sub-summaries in the sequential summary. ???? = ∑?∈?? ??? . ????, ??−1 = ∑?∈??∩??−1 ??? is the overlapped information in the two adjacent sub-summaries. ??? = ???? ?????????(?, ???) where w is a word, ???? is the inverse tweet frequency of w, and ??? is all the tweets in the trending topic. The relevance function is introduced to ensure that the information brought by new sub-summaries is not only novel but also related to the topic.  Sequence Correlation Sequence correlation evaluates the sequential matching degree between system-generated and human-written summaries. In statistics, Kendall’s tau coefficient is often used to measure the association between two sequences (Lapata, 2006). The basic idea is to count the concordant and discordant pairs which contain the same elements in two sequences. Borrowing this idea, for each sub-summary in a human-generated summary, we find its most matched subsummary (judged by the cosine similarity measure) in the corresponding system-generated summary and then define the correlation according to the concordance between the two matched sub-summary sequences. ??????????? 2(|#???????????????| |#???????????????|) − = ?(? − 1) where n is the number of human-written subsummaries. Tables 2 and 3 below present the evaluation results. For the stream-based approach, we set ∆t=3 hours experimentally. For the semanticbased approach, we compare three different approaches to defining the sub-topic number K: (1) Semantic-based 1: Following the approach proposed in (Li et al., 2007), we first derive the matrix of tweet cosine similarity. Given the 1norm of eigenvalues ?????? (? = 1, 2, ,?) of the similarity matrix and the ratios ?? = ??????/?2 , the subtopic number ? = ? + 1 if ?? − ??+1 > ? (? 0.4 ). (2) Semantic-based 2: Using the rule of thumb in (Wan and Yang, 2008), ? = √? , where n is the tweet number. (3) Combined: K is defined as the number of the peak areas detected from the Opad algorithm, meanwhile we use the … = tweets within peak areas as the tweets of DTM. This is our new idea. The experiments confirm the superiority of the semantic-based approach over the stream-based approach in summary content coverage and novelty evaluations, showing that the former is better at subtopic content modeling. The subsummaries generated by the stream-based approach have comparative sequence (i.e., order) correlation with the human summaries. Combining the advantages the two approaches leads to the best overall results. SCebomaSCs beonmtdivr1eac( ∆nrdδ-bm(ta=i∆g0-cs3e.t)5=d32U0 n.3ig510r32a7m B0 .i1g 6r3589a46m87 SB0 k.i1 gp8725r69ame173d Table 2. N-Gram Coverage Evaluation Sem CtraeonTmaA tmicapb-nplibentria ec3os-de.abcd N(a∆hs(o1evt∆=(sdetδ=3l2)t 0y).a4n)dCoN0r .o 73e vl071ea96lti783 oy nEvCalo0ur a. 3 tei3792ol3a489nt650io n 4 Concluding Remarks We start a new application for Twitter trending topics, i.e., sequential summarization, to reveal the developing scenario of the trending topics while retaining the order of information presentation. We develop several solutions to automatically detect, segment and order subtopics temporally, and extract the most significant tweets into the sub-summaries to compose sequential summaries. Empirically, the combination of the stream-based approach and the semantic-based approach leads to sequential summaries with high coverage, low redundancy, and good order. Acknowledgments The work described in this paper is supported by a Hong Kong RGC project (PolyU No. 5202/12E) and a National Nature Science Foundation of China (NSFC No. 61272291). References Aggarwal Gaurav, Sumbaly Roshan and Sinha Shakti. 2009. Update Summarization. Stanford: CS224N Final Projects. 570 Blei M. David and Jordan I. Michael. 2006. Dynamic topic models. In Proceedings of the 23rd international conference on Machine learning, 113120. Pittsburgh, Pennsylvania. Carbonell Jaime and Goldstein Jade. 1998. The use of MMR, diversity based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual International Conference on Research and Development in Information Retrieval, 335-336. Melbourne, Australia. Duan Yajuan, Chen Zhimin, Wei Furu, Zhou Ming and Heung-Yeung Shum. 2012. Twitter Topic Summarization by Ranking Tweets using Social Influence and Content Quality. In Proceedings of the 24th International Conference on Computational Linguistics, 763-780. Mumbai, India. Harabagiu Sanda and Hickl Andrew. 2011. Relevance Modeling for Microblog Summarization. In Proceedings of 5th International AAAI Conference on Weblogs and Social Media. Barcelona, Spain. Lapata Mirella. 2006. Automatic evaluation of information ordering: Kendall’s tau. Computational Linguistics, 32(4): 1-14. Li Wenyuan, Ng Wee-Keong, Liu Ying and Ong Kok-Leong. 2007. Enhancing the Effectiveness of Clustering with Spectra Analysis. IEEE Transactions on Knowledge and Data Engineering, 19(7):887-902. Li Xiaoyan and Croft W. Bruce. 2006. Improving novelty detection for general topics using sentence level information patterns. In Proceedings of the 15th ACM International Conference on Information and Knowledge Management, 238-247. New York, USA. Lin Chin-Yew. 2004. ROUGE: a Package for Automatic Evaluation of Summaries. In Proceedings of the ACL Workshop on Text Summarization Branches Out, 74-81 . Barcelona, Spain. Liu Fei, Liu Yang and Weng Fuliang. 2011. Why is “SXSW ” trending? Exploring Multiple Text Sources for Twitter Topic Summarization. In Proceedings of the ACL Workshop on Language in Social Media, 66-75. Portland, Oregon. O'Connor Brendan, Krieger Michel and Ahn David. 2010. TweetMotif: Exploratory Search and Topic Summarization for Twitter. In Proceedings of the 4th International AAAI Conference on Weblogs and Social Media, 384-385. Atlanta, Georgia. Shamma A. David, Kennedy Lyndon and Churchill F. Elizabeth. 2010. Tweetgeist: Can the Twitter Timeline Reveal the Structure of Broadcast Events? In Proceedings of the 2010 ACM Conference on Computer Supported Cooperative Work, 589-593. Savannah, Georgia, USA. Sharifi Beaux, Hutton Mark-Anthony and Kalita Jugal. 2010. Summarizing Microblogs Automatically. In Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 685688. Los Angeles, California. Steinberger Josef and Jezek Karel. 2009. Update summarization based on novel topic distribution. In Proceedings of the 9th ACM Symposium on Document Engineering, 205-213. Munich, Germany. Varma Vasudeva, Bharat Vijay, Kovelamudi Sudheer, Praveen Bysani, Kumar K. N, Kranthi Reddy, Karuna Kumar and Nitin Maganti. 2009. IIIT Hyderabad at TAC 2009. In Proceedings of the 2009 Text Analysis Conference. GaithsBurg, Maryland. Wan Xiaojun and Yang Jianjun. 2008. Multidocument summarization using cluster-based link analysis. In Proceedings of the 3 1st Annual International Conference on Research and Development in Information Retrieval, 299-306. Singapore, Singapore. 571</p><p>4 0.92368078 <a title="210-lda-4" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>Author: Jose G.C. de Souza ; Miquel Espla-Gomis ; Marco Turchi ; Matteo Negri</p><p>Abstract: The use of automatic word alignment to capture sentence-level semantic relations is common to a number of cross-lingual NLP applications. Despite its proved usefulness, however, word alignment information is typically considered from a quantitative point of view (e.g. the number of alignments), disregarding qualitative aspects (the importance of aligned terms). In this paper we demonstrate that integrating qualitative information can bring significant performance improvements with negligible impact on system complexity. Focusing on the cross-lingual textual en- tailment task, we contribute with a novel method that: i) significantly outperforms the state of the art, and ii) is portable, with limited loss in performance, to language pairs where training data are not available.</p><p>same-paper 5 0.88451225 <a title="210-lda-5" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>6 0.88284153 <a title="210-lda-6" href="./acl-2013-Modeling_Thesis_Clarity_in_Student_Essays.html">246 acl-2013-Modeling Thesis Clarity in Student Essays</a></p>
<p>7 0.86334473 <a title="210-lda-7" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>8 0.6673038 <a title="210-lda-8" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>9 0.65872979 <a title="210-lda-9" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>10 0.64997488 <a title="210-lda-10" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>11 0.64815521 <a title="210-lda-11" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>12 0.64327818 <a title="210-lda-12" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>13 0.63838273 <a title="210-lda-13" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>14 0.6378566 <a title="210-lda-14" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>15 0.63726509 <a title="210-lda-15" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>16 0.62987906 <a title="210-lda-16" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>17 0.62191862 <a title="210-lda-17" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>18 0.61470592 <a title="210-lda-18" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>19 0.61135989 <a title="210-lda-19" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>20 0.60665923 <a title="210-lda-20" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
