<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-210" href="#">acl2013-210</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</h1>
<br/><p>Source: <a title="acl-2013-210-pdf" href="http://aclweb.org/anthology//P/P13/P13-1106.pdf">pdf</a></p><p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>Reference: <a title="acl-2013-210-reference" href="../acl2013_reference/acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ner', 0.402), ('yf', 0.337), ('align', 0.296), ('ye', 0.253), ('yfs', 0.21), ('yjf', 0.21), ('dual', 0.196), ('fj', 0.16), ('dd', 0.153), ('bil', 0.151), ('monol', 0.151), ('burket', 0.139), ('bf', 0.133), ('rush', 0.132), ('crf', 0.116), ('denero', 0.115), ('edg', 0.115), ('agr', 0.113), ('decomposit', 0.113), ('ei', 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="210-tfidf-1" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>2 0.23377788 <a title="210-tfidf-2" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>Author: Chris Quirk</p><p>Abstract: The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.</p><p>3 0.23248638 <a title="210-tfidf-3" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>4 0.23234312 <a title="210-tfidf-4" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>5 0.23026989 <a title="210-tfidf-5" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>Author: Manaal Faruqui ; Chris Dyer</p><p>Abstract: We present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages. The monolingual component of our objective is the average mutual information of clusters of adjacent words in each language, while the bilingual component is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters.</p><p>6 0.21025929 <a title="210-tfidf-6" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>7 0.17628939 <a title="210-tfidf-7" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>8 0.17303571 <a title="210-tfidf-8" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>9 0.16970134 <a title="210-tfidf-9" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>10 0.16256547 <a title="210-tfidf-10" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>11 0.15261428 <a title="210-tfidf-11" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>12 0.14302529 <a title="210-tfidf-12" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>13 0.13737544 <a title="210-tfidf-13" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>14 0.13530505 <a title="210-tfidf-14" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>15 0.13460557 <a title="210-tfidf-15" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>16 0.13385627 <a title="210-tfidf-16" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>17 0.12898986 <a title="210-tfidf-17" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>18 0.12545906 <a title="210-tfidf-18" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>19 0.12018543 <a title="210-tfidf-19" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>20 0.11289641 <a title="210-tfidf-20" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.266), (1, 0.109), (2, 0.021), (3, 0.073), (4, -0.049), (5, 0.051), (6, 0.137), (7, -0.063), (8, 0.126), (9, -0.109), (10, -0.233), (11, -0.044), (12, 0.145), (13, 0.027), (14, 0.124), (15, -0.052), (16, 0.077), (17, 0.042), (18, 0.005), (19, 0.136), (20, 0.03), (21, -0.006), (22, 0.107), (23, -0.068), (24, 0.102), (25, 0.035), (26, -0.013), (27, 0.105), (28, -0.023), (29, 0.006), (30, -0.069), (31, -0.157), (32, -0.06), (33, -0.031), (34, 0.049), (35, 0.077), (36, 0.002), (37, 0.015), (38, -0.007), (39, -0.026), (40, -0.002), (41, 0.004), (42, -0.032), (43, -0.013), (44, -0.022), (45, 0.027), (46, 0.026), (47, 0.105), (48, -0.132), (49, -0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94407964 <a title="210-lsi-1" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>2 0.82926339 <a title="210-lsi-2" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>Author: Chris Quirk</p><p>Abstract: The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.</p><p>3 0.80533302 <a title="210-lsi-3" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>Author: Thomas Schoenemann</p><p>Abstract: We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores.</p><p>4 0.76736909 <a title="210-lsi-4" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>5 0.75090861 <a title="210-lsi-5" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>6 0.72453201 <a title="210-lsi-6" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>7 0.68873131 <a title="210-lsi-7" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>8 0.63376158 <a title="210-lsi-8" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>9 0.60338783 <a title="210-lsi-9" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>10 0.5753445 <a title="210-lsi-10" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>11 0.55698109 <a title="210-lsi-11" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>12 0.54147816 <a title="210-lsi-12" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>13 0.53234679 <a title="210-lsi-13" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<p>14 0.52320904 <a title="210-lsi-14" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>15 0.51821995 <a title="210-lsi-15" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>16 0.51710683 <a title="210-lsi-16" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>17 0.51077223 <a title="210-lsi-17" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>18 0.50986785 <a title="210-lsi-18" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>19 0.50666291 <a title="210-lsi-19" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>20 0.48752654 <a title="210-lsi-20" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.061), (9, 0.022), (41, 0.063), (53, 0.286), (55, 0.024), (67, 0.19), (76, 0.01), (82, 0.011), (87, 0.122), (90, 0.08), (95, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88533151 <a title="210-lda-1" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>Author: Abhijit Mishra ; Pushpak Bhattacharyya ; Michael Carl</p><p>Abstract: In this paper we introduce Translation Difficulty Index (TDI), a measure of difficulty in text translation. We first define and quantify translation difficulty in terms of TDI. We realize that any measure of TDI based on direct input by translators is fraught with subjectivity and adhocism. We, rather, rely on cognitive evidences from eye tracking. TDI is measured as the sum of fixation (gaze) and saccade (rapid eye movement) times of the eye. We then establish that TDI is correlated with three properties of the input sentence, viz. length (L), degree of polysemy (DP) and structural complexity (SC). We train a Support Vector Regression (SVR) system to predict TDIs for new sentences using these features as input. The prediction done by our framework is well correlated with the empirical gold standard data, which is a repository of < L, DP, SC > and TDI pairs for a set of sentences. The primary use of our work is a way of “binning” sentences (to be translated) in “easy”, “medium” and “hard” categories as per their predicted TDI. This can decide pricing of any translation task, especially useful in a scenario where parallel corpora for Machine Translation are built through translation crowdsourcing/outsourcing. This can also provide a way of monitoring progress of second language learners.</p><p>2 0.87558496 <a title="210-lda-2" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>Author: Eirini Florou</p><p>Abstract: As one of the most challenging issues in NLP, metaphor identification and its interpretation have seen many models and methods proposed. This paper presents a study on metaphor identification based on the semantic similarity between literal and non literal meanings of words that can appear at the same context.</p><p>same-paper 3 0.87375116 <a title="210-lda-3" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>4 0.87111992 <a title="210-lda-4" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>Author: Yang Liu</p><p>Abstract: We introduce a shift-reduce parsing algorithm for phrase-based string-todependency translation. As the algorithm generates dependency trees for partial translations left-to-right in decoding, it allows for efficient integration of both n-gram and dependency language models. To resolve conflicts in shift-reduce parsing, we propose a maximum entropy model trained on the derivation graph of training data. As our approach combines the merits of phrase-based and string-todependency models, it achieves significant improvements over the two baselines on the NIST Chinese-English datasets.</p><p>5 0.86542672 <a title="210-lda-5" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Julian Schamper ; Hermann Ney</p><p>Abstract: In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet- ter (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.</p><p>6 0.85120374 <a title="210-lda-6" href="./acl-2013-Multilingual_Affect_Polarity_and_Valence_Prediction_in_Metaphor-Rich_Texts.html">253 acl-2013-Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich Texts</a></p>
<p>7 0.84785515 <a title="210-lda-7" href="./acl-2013-Mining_Opinion_Words_and_Opinion_Targets_in_a_Two-Stage_Framework.html">244 acl-2013-Mining Opinion Words and Opinion Targets in a Two-Stage Framework</a></p>
<p>8 0.8440792 <a title="210-lda-8" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>9 0.84402883 <a title="210-lda-9" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>10 0.84341276 <a title="210-lda-10" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>11 0.84168071 <a title="210-lda-11" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>12 0.84095824 <a title="210-lda-12" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>13 0.84002095 <a title="210-lda-13" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>14 0.8380022 <a title="210-lda-14" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>15 0.83672488 <a title="210-lda-15" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>16 0.83662623 <a title="210-lda-16" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>17 0.83656341 <a title="210-lda-17" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>18 0.8364653 <a title="210-lda-18" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>19 0.83609164 <a title="210-lda-19" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>20 0.83570331 <a title="210-lda-20" href="./acl-2013-Implicatures_and_Nested_Beliefs_in_Approximate_Decentralized-POMDPs.html">190 acl-2013-Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
