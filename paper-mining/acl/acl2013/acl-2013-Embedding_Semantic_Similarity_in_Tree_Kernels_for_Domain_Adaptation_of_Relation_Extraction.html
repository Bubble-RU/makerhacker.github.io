<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-134" href="#">acl2013-134</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</h1>
<br/><p>Source: <a title="acl-2013-134-pdf" href="http://aclweb.org/anthology//P/P13/P13-1147.pdf">pdf</a></p><p>Author: Barbara Plank ; Alessandro Moschitti</p><p>Abstract: Relation Extraction (RE) is the task of extracting semantic relationships between entities in text. Recent studies on relation extraction are mostly supervised. The clear drawback of supervised methods is the need of training data: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. This is the problem of domain adaptation. In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic analysis (LSA) and (ii) structured kernels to improve the adaptability of relation extractors to new text genres/domains. The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation.</p><p>Reference: <a title="acl-2013-134-reference" href="../acl2013_reference/acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic analysis (LSA) and (ii) structured kernels to  improve the adaptability of relation extractors to new text genres/domains. [sent-6, score-0.588]
</p><p>2 The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation. [sent-7, score-0.321]
</p><p>3 Recent studies on relation extraction have shown that supervised approaches based on either feature or kernel methods achieve state-of-the-art accuracy (Zelenko et al. [sent-11, score-0.35]
</p><p>4 This is the problem of domain adaptation (DA) or transfer learning (TL). [sent-23, score-0.311]
</p><p>5 Technically, domain adaptation addresses the problem of learning when the assumption of independent and identically distributed (i. [sent-24, score-0.28]
</p><p>6 two shared tasks have been organized on domain adaptation for dependency parsing (Nivre et al. [sent-30, score-0.28]
</p><p>7 , 2009), which are clearly related but different: the former is the problem of how to transfer knowledge from old to new relation types, while distant supervision tries to learn new relations from unlabeled text by exploiting weak-supervision in the form of a knowledge resource (e. [sent-36, score-0.358]
</p><p>8 Weak supervision is a promising approach to improve a relation extraction system, especially to increase its coverage in terms of types of relations covered. [sent-44, score-0.232]
</p><p>9 Even a weakly supervised system is expected to perform well when applied to any kind of text (other domain/genre), thus ideally, we believe that combining domain adaptation with weak supervision is the way to go in the future. [sent-46, score-0.353]
</p><p>10 Moreover, we consider a particular domain adaptation setting: singlesystem DA, i. [sent-51, score-0.28]
</p><p>11 We consider this as a shift in what was considered domain adaptation in the past (adapt from source to a specific target) and what can be considered a somewhat different recent view of DA, that became widespread since 2011/2012. [sent-60, score-0.28]
</p><p>12 In this setup, the domain adaptation problem boils down to finding a more robust system (Søgaard and Johannsen, 2012), i. [sent-62, score-0.317]
</p><p>13 We propose to combine (i) term generalization approaches and (ii) structured kernels to improve the performance of a relation extractor on new domains. [sent-65, score-0.538]
</p><p>14 Given the complexity of feature  engineering, we exploit kernel methods (ShaweTaylor and Cristianini, 2004). [sent-69, score-0.231]
</p><p>15 We encode word clusters or similarity in tree kernels, which, in turn, produce spaces of tree fragments. [sent-70, score-0.358]
</p><p>16 Rather than only matching the surface string of words, lexical similarity enables soft matches between similar words in convolution tree kernels. [sent-72, score-0.284]
</p><p>17 In the empirical evaluation on Automatic Content Extraction (ACE) data, we evaluate the impact of convolution tree kernels embedding lexical semantic similarities. [sent-73, score-0.687]
</p><p>18 Then, we test our RE system on the ACE 2005 data, which exploits kernels, structures and similarities for domain adaptation. [sent-77, score-0.174]
</p><p>19 The results show that combining the huge space of tree fragments generalized at the lexical level provides an effective model for adapt-  ing RE systems to new domains. [sent-78, score-0.174]
</p><p>20 l yiαiφ(oi)φ(o) + b = 0, where oi and o arPe two objects, φ is a mapping from an object to a feature vector x~ i and φ(oi)φ(o) = K(oi, o) is a kernel function implicitly defining such a mapping. [sent-82, score-0.248]
</p><p>21 Commonly used kernels in NLP are string kernels (Lodhi et al. [sent-84, score-0.75]
</p><p>22 Syntactic tree kernels (Collins and Duffy, 2001) compute the similarity between two trees T1 and T2 by counting common sub-trees (cf. [sent-88, score-0.535]
</p><p>23 Semantic syntactic tree kernels (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Croce et al. [sent-93, score-0.524]
</p><p>24 TKσ combines generalized lexical with structural information: it allows matching tree fragments that have the same syntactic structure but differ in their terminals. [sent-98, score-0.251]
</p><p>25 3  Related Work  Semantic syntactic tree kernels have been previously used for question classification (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b; Croce et al. [sent-100, score-0.524]
</p><p>26 These kernels have not yet been studied for either domain adaptation or RE. [sent-102, score-0.655]
</p><p>27 Thus, we present a novel application of semantic syntactic tree kernels and Brown clusters for domain adaptation oftree-kernel based relation extraction. [sent-105, score-1.062]
</p><p>28 , 2006) exploits unlabeled data from both source and target domain to find correspondences among features from different domains. [sent-110, score-0.304]
</p><p>29 The key to SCL is to exploit pivot features to automatically identify feature correspondences, and as such is applicable to feature-based approaches but not in our case since we do not assume availability of target domain data. [sent-112, score-0.349]
</p><p>30 Instead, we apply a similar idea where we exploit an entire unlabeled corpus as pivot, and compare our approach to instance weighting (Jiang and Zhai, 2007). [sent-113, score-0.232]
</p><p>31 Instance weighting is a method for domain adaptation in which instance-dependent weights are assigned to the loss function that is minimized during the training process. [sent-114, score-0.349]
</p><p>32 4  Computational Structures for RE  A common way to represent a constituency-based relation instance is the PET (path-enclosed-tree), the smallest subtree including the two target entities (Zhang et al. [sent-123, score-0.27]
</p><p>33 An alternative kernel that does not use syntactic information is the Bag-of-Words (BOW) kernel, where a single root node is added above the terminals. [sent-131, score-0.237]
</p><p>34 Note that in this BOW kernel we actually mark target entities with E1/E2. [sent-132, score-0.291]
</p><p>35 Therefore, our BOW kernel can be considered an enriched BOW model. [sent-133, score-0.243]
</p><p>36 (2006), including gold-standard information on entity and mention type substantially improves relation extraction performance. [sent-136, score-0.208]
</p><p>37 In the case of porting a system to new domains entity information will be unreliable or missing. [sent-141, score-0.175]
</p><p>38 Therefore, in our domain adaptation experiments on the ACE 2005 data (Section 6. [sent-142, score-0.28]
</p><p>39 3) we will not rely on this gold information but rather train a system using PET (target mentions only marked with E1/E2 and no gold entity label). [sent-143, score-0.223]
</p><p>40 1 Syntactic Semantic Structures Combining syntax with semantics has a clear advantage: it generalizes lexical information encapsulated in syntactic parse trees, while at the same time syntax guides semantics in order to obtain an effective semantic similarity. [sent-145, score-0.261]
</p><p>41 In fact, lexical information is highly affected by data-sparseness, thus tree kernels combined with semantic information created from additional resources should provide a way to obtain a more robust system. [sent-146, score-0.531]
</p><p>42 We argue that whenever gold data is not available, distributional semantics paired with kernels can be useful to improve generalization and complement missing gold info. [sent-148, score-0.535]
</p><p>43 We propose to use an entire unlabeled corpus as pivot: this corpus must be general enough to encapsulate the source and target domains of interest. [sent-151, score-0.221]
</p><p>44 The idea is to (i) learn semantic similarity between words on the pivot corpus and (ii) use tree kernels embedding such a similarity to learn a RE system on the source, which allows to generalize to the new target domain. [sent-152, score-0.883]
</p><p>45 In SCL, a representation shared across domains is learned by exploiting pivot features, where a set –  of pivot features has to be selected (usually a few thousands). [sent-155, score-0.335]
</p><p>46 In our case pivots are words that cooccur with the target words in a large unlabeled corpus and are thus implicitly represented in the similarity matrix. [sent-156, score-0.189]
</p><p>47 Thus, in contrast to SCL, we do not need to select a set of pivot features but rather rely on the distributional hypothesis to infer a semantic similarity from a large unlabeled corpus. [sent-157, score-0.282]
</p><p>48 Then, this similarity is incorporated into the tree kernel that provides the necessary restriction for an effective semantic similarity calculation. [sent-158, score-0.458]
</p><p>49 We study two ways for term generalization in tree kernels: Brown words clusters and Latent Semantic Analysis (LSA), both briefly described next. [sent-162, score-0.245]
</p><p>50 Given two words w1 and w2, the term similarity function σ is estimated as the cosine similarity between the corresponding projections w~ 1 , w~ 2 and used in the kernel as described in Section 2. [sent-174, score-0.302]
</p><p>51 We modified the SVM-light-TK package to include the semantic tree kernels and instance weighting. [sent-177, score-0.588]
</p><p>52 Thus, for instance for the 7 coarse ACE 2004 relations, we build 14 coarse-grained classifiers (two for each coarse ACE 2004 relation type except for PER-SOC, which is symmetric, and one classifier for the none relation). [sent-188, score-0.173]
</p><p>53 For the domain adaptation experiments we use the ACE 2005 corpus. [sent-206, score-0.28]
</p><p>54 More importantly, the ACE 2005 corpus covers additional domains: weblogs, telephone con-  versation, usenet and broadcast conversation. [sent-211, score-0.193]
</p><p>55 In the experiments, we use news (the union of nw and bn) as source domain, and weblogs (wl), telephone conversations (cts) and broadcast conversation (bc) as target domains. [sent-212, score-0.4]
</p><p>56 7 We take half of bc as only target development set, and leave the remaining data and domains for final testing (since they are already small, cf. [sent-213, score-0.205]
</p><p>57 To get a feeling of how these domains differ, Figure 3 depicts the distribution ofrelations in each domain and Table 2 provides the most frequent out-of-vocabulary words together with their percentage. [sent-215, score-0.223]
</p><p>58 We incorporate cluster information by us-  7We did not consider the usenet subpart, since it is among the smaller domains and data-preprocessing was difficult. [sent-222, score-0.178]
</p><p>59 e˜-  Table 2: For each domain the percentage of target domain words (types) that are unseen in the source together with the most frequent OOV words. [sent-231, score-0.34]
</p><p>60 For the domain adaptation experiments, we use ukWaC corpus-induced clusters as bridge between domains. [sent-234, score-0.372]
</p><p>61 , βm as input, where each βi represents the relative importance of example iwith respect to the target domain (Huang et al. [sent-240, score-0.203]
</p><p>62 To estimate the importance weights, we train a binary classifier that distinguishes between source and target domain instances. [sent-242, score-0.203]
</p><p>63 We consider the union of the three target domains as target data. [sent-243, score-0.218]
</p><p>64 Our best system (composite kernel with polynomial expansion) reaches an F1 of 70. [sent-271, score-0.231]
</p><p>65 Since we focus on evaluating the impact of semantic similarity in tree kernels, we think our system is very competitive. [sent-279, score-0.247]
</p><p>66 Thus, in the domain adaptation setup we assume entity boundaries given but not their label. [sent-288, score-0.332]
</p><p>67 2 Tree Kernels with Brown Word Clusters To evaluate the effectiveness of Brown word clusters in tree kernels, we evaluated different instance representations (cf. [sent-294, score-0.255]
</p><p>68 45306  Table 4: Brown clusters in tree kernels (cf. [sent-300, score-0.573]
</p><p>69 replacing or ignoring terminals harms performance; ii) the best way to incorporate Brown clusters is to replace the Pos tag with the cluster bitstring; iii) marking all words is generally better than only mentions; this is in contrast to Sun et al. [sent-304, score-0.185]
</p><p>70 (201 1) who found that in their feature-based system it was better to add cluster information to entity mentions only. [sent-305, score-0.199]
</p><p>71 As we will discuss, the combination of syntax and semantics exploited in this novel kernel avoids the necessity of restricting cluster information to mentions only. [sent-306, score-0.388]
</p><p>72 3 Semantic Tree Kernels for DA To evaluate the effectiveness of the proposed kernels across domains, we use the ACE 2005 data as testbed. [sent-308, score-0.375]
</p><p>73 As we want to build a single system that is able to handle heterogeneous data, we do not assume that there is further unlabeled domain-specific data, but we assume to have a large unlabeled corpus (ukWaC) at our disposal to improve the generalizability of our models. [sent-311, score-0.175]
</p><p>74 In-domain (col 1): when evaluated on the same domain the system was trained on (nw+bn, 5-fold cross-validation). [sent-314, score-0.174]
</p><p>75 Out-of-domain performance (cols 2-4): the system evaluated on the targets, namely broadcast conversation (bc), telephone conversation (cts) and weblogs (wl). [sent-315, score-0.407]
</p><p>76 The BOW kernel that disregards syntax is often less effective (row 2). [sent-321, score-0.245]
</p><p>77 We see also the effect of target entity marking: the BOW kernel without entity marking performs substantially worse (row 3). [sent-322, score-0.401]
</p><p>78 For the remaining experiments we  use the BOW kernel with entity marking. [sent-323, score-0.246]
</p><p>79 Instance weighting shows mixed results: it helps slightly on the weblogs domain, but does not help on broadcast conversation and telephone conversations. [sent-326, score-0.372]
</p><p>80 The Brown cluster kernel applied to PET (P WC) improves performance over the baseline over all target domains. [sent-332, score-0.316]
</p><p>81 The same holds also for the lexical semantic kernel based on LSA (P LSA), however, to only two out of three domains. [sent-333, score-0.244]
</p><p>82 This suggests that the two kernels capture different information and a combined  kernel might be effective. [sent-334, score-0.569]
</p><p>83 Only on the weblogs domain B LSA achieves a minor improvement (from 32. [sent-372, score-0.216]
</p><p>84 We believe that the semantic information does not help the BOW kernel as there is no syntactic information that constrains the application of the noisy source, as opposed to the case with the PET kernel. [sent-380, score-0.287]
</p><p>85 –  –  As the two semantically enriched kernels, PET LSA and PET WC, seem to capture different information we use composite kernels (rows 1011): the baseline kernel (PET) summed with the lexical semantic kernels. [sent-381, score-0.719]
</p><p>86 Adding also PET LSA results in the best performance and our final system (last row): the composite kernel (PET+PET WC+PET LSA) reaches an F1 of 48. [sent-385, score-0.282]
</p><p>87 The table shows that our system is able to improve F1 on all relations for the broadcast and weblogs data. [sent-413, score-0.228]
</p><p>88 As shown in the same figure, the relation distribution of the cts domain is also rather different from the source. [sent-417, score-0.374]
</p><p>89 7  Conclusions and Future Work  We proposed syntactic tree kernels enriched by lexical semantic similarity to tackle the portability of a relation extractor to different domains. [sent-423, score-0.793]
</p><p>90 The results of diverse kernels exploiting (i) Brown clustering and (ii) LSA show that a suitable combination of syntax and lexical generalization is very promising for domain adaptation. [sent-424, score-0.641]
</p><p>91 The proposed system is able to improve performance significantly on two out of three target domains (up to 8% relative improvement). [sent-425, score-0.189]
</p><p>92 In contrast, adding lexical information combined with syntax can help to improve performance: the syntactic structure enriched with lexical information provides a feature space where syntax constrains lexical similar-  ity obtained from unlabeled data. [sent-429, score-0.263]
</p><p>93 Thus, semantic syntactic tree kernels appear to be a suitable mechanism to adequately trade off the two kinds of information. [sent-430, score-0.574]
</p><p>94 Semantic convolution kernels over dependency trees: smoothed partial tree kernel. [sent-475, score-0.605]
</p><p>95 A study on convolution kernels for shallow semantic parsing. [sent-546, score-0.549]
</p><p>96 Efficient convolution kernels for dependency and constituent syntactic trees. [sent-550, score-0.542]
</p><p>97 In Proceedings ’09, pages 1378–1387, Stroudsburg, PA,  kernels on structures of EMNLP USA. [sent-560, score-0.375]
</p><p>98 A re-examination of dependency path kernels for relation extraction. [sent-599, score-0.491]
</p><p>99 Discovering relations between named entities from a large raw corpus using tree similarity-based clustering. [sent-615, score-0.177]
</p><p>100 A composite kernel to extract relations between entities with both flat and structured features. [sent-619, score-0.316]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pet', 0.399), ('kernels', 0.375), ('ace', 0.31), ('bow', 0.23), ('kernel', 0.194), ('lsa', 0.144), ('adaptation', 0.143), ('bloehdorn', 0.142), ('brown', 0.141), ('domain', 0.137), ('convolution', 0.124), ('moschitti', 0.121), ('cts', 0.121), ('relation', 0.116), ('pivot', 0.109), ('tree', 0.106), ('clusters', 0.092), ('wc', 0.087), ('domains', 0.086), ('telephone', 0.085), ('scl', 0.081), ('weblogs', 0.079), ('broadcast', 0.072), ('unlabeled', 0.069), ('weighting', 0.069), ('alessandro', 0.068), ('da', 0.068), ('conversation', 0.067), ('croce', 0.066), ('target', 0.066), ('filmmaker', 0.061), ('zhang', 0.06), ('instance', 0.057), ('gaard', 0.057), ('cluster', 0.056), ('blitzer', 0.055), ('oi', 0.054), ('similarity', 0.054), ('mentions', 0.054), ('sun', 0.053), ('bc', 0.053), ('entity', 0.052), ('syntax', 0.051), ('composite', 0.051), ('zhai', 0.05), ('semantic', 0.05), ('jiang', 0.05), ('enriched', 0.049), ('ii', 0.049), ('chan', 0.048), ('generalization', 0.047), ('re', 0.045), ('ukwac', 0.044), ('aligns', 0.044), ('syntactic', 0.043), ('robustly', 0.042), ('gonve', 0.04), ('ionext', 0.04), ('lodhi', 0.04), ('qjn', 0.04), ('zelenko', 0.04), ('relations', 0.04), ('extraction', 0.04), ('gold', 0.04), ('system', 0.037), ('guodong', 0.037), ('anders', 0.037), ('marking', 0.037), ('exploit', 0.037), ('drops', 0.037), ('supervision', 0.036), ('blindly', 0.036), ('cristianini', 0.036), ('employment', 0.036), ('usenet', 0.036), ('citizen', 0.036), ('golub', 0.036), ('ract', 0.036), ('generalized', 0.035), ('bn', 0.035), ('distant', 0.035), ('structural', 0.034), ('argument', 0.034), ('semantics', 0.033), ('fragments', 0.033), ('min', 0.033), ('iii', 0.032), ('embedding', 0.032), ('correspondences', 0.032), ('ch', 0.032), ('entities', 0.031), ('jian', 0.031), ('tk', 0.031), ('nw', 0.031), ('sys', 0.031), ('nello', 0.031), ('exploiting', 0.031), ('transfer', 0.031), ('rows', 0.031), ('daum', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="134-tfidf-1" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>Author: Barbara Plank ; Alessandro Moschitti</p><p>Abstract: Relation Extraction (RE) is the task of extracting semantic relationships between entities in text. Recent studies on relation extraction are mostly supervised. The clear drawback of supervised methods is the need of training data: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. This is the problem of domain adaptation. In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic analysis (LSA) and (ii) structured kernels to improve the adaptability of relation extractors to new text genres/domains. The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation.</p><p>2 0.29511607 <a title="134-tfidf-2" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>Author: Matt Post ; Shane Bergsma</p><p>Abstract: Syntactic features are useful for many text classification tasks. Among these, tree kernels (Collins and Duffy, 2001) have been perhaps the most robust and effective syntactic tool, appealing for their empirical success, but also because they do not require an answer to the difficult question of which tree features to use for a given task. We compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly avail- able tools) , we suggest they should always be included as baseline comparisons in tree kernel method evaluations.</p><p>3 0.23844834 <a title="134-tfidf-3" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>Author: Aliaksei Severyn ; Massimo Nicosia ; Alessandro Moschitti</p><p>Abstract: Measuring semantic textual similarity (STS) is at the cornerstone of many NLP applications. Different from the majority of approaches, where a large number of pairwise similarity features are used to represent a text pair, our model features the following: (i) it directly encodes input texts into relational syntactic structures; (ii) relies on tree kernels to handle feature engineering automatically; (iii) combines both structural and feature vector representations in a single scoring model, i.e., in Support Vector Regression (SVR); and (iv) delivers significant improvement over the best STS systems.</p><p>4 0.1489872 <a title="134-tfidf-4" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>Author: Goran Glavas ; Jan Snajder</p><p>Abstract: Identifying news stories that discuss the same real-world events is important for news tracking and retrieval. Most existing approaches rely on the traditional vector space model. We propose an approach for recognizing identical real-world events based on a structured, event-oriented document representation. We structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs. Our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events.</p><p>5 0.14594525 <a title="134-tfidf-5" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>Author: Xuezhe Ma ; Fei Xia</p><p>Abstract: In this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets.</p><p>6 0.1431632 <a title="134-tfidf-6" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>7 0.13481498 <a title="134-tfidf-7" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>8 0.10819866 <a title="134-tfidf-8" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>9 0.10675814 <a title="134-tfidf-9" href="./acl-2013-Semantic_Frames_to_Predict_Stock_Price_Movement.html">310 acl-2013-Semantic Frames to Predict Stock Price Movement</a></p>
<p>10 0.10632626 <a title="134-tfidf-10" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>11 0.10278779 <a title="134-tfidf-11" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>12 0.096117876 <a title="134-tfidf-12" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>13 0.094436504 <a title="134-tfidf-13" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>14 0.094358742 <a title="134-tfidf-14" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>15 0.093337096 <a title="134-tfidf-15" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>16 0.092705861 <a title="134-tfidf-16" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>17 0.092192978 <a title="134-tfidf-17" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>18 0.084933363 <a title="134-tfidf-18" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>19 0.084845334 <a title="134-tfidf-19" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>20 0.083353199 <a title="134-tfidf-20" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.254), (1, 0.012), (2, -0.042), (3, -0.078), (4, -0.025), (5, 0.07), (6, -0.031), (7, 0.037), (8, -0.013), (9, -0.005), (10, 0.072), (11, -0.005), (12, 0.017), (13, 0.063), (14, -0.028), (15, 0.067), (16, -0.089), (17, 0.088), (18, -0.048), (19, 0.105), (20, 0.165), (21, 0.052), (22, 0.141), (23, 0.033), (24, -0.084), (25, -0.03), (26, 0.06), (27, -0.031), (28, -0.073), (29, 0.03), (30, -0.115), (31, 0.237), (32, 0.006), (33, 0.013), (34, -0.146), (35, 0.156), (36, 0.055), (37, -0.141), (38, 0.008), (39, 0.166), (40, -0.008), (41, 0.175), (42, -0.023), (43, -0.035), (44, 0.036), (45, -0.128), (46, -0.008), (47, -0.065), (48, -0.044), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94033045 <a title="134-lsi-1" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>Author: Barbara Plank ; Alessandro Moschitti</p><p>Abstract: Relation Extraction (RE) is the task of extracting semantic relationships between entities in text. Recent studies on relation extraction are mostly supervised. The clear drawback of supervised methods is the need of training data: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. This is the problem of domain adaptation. In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic analysis (LSA) and (ii) structured kernels to improve the adaptability of relation extractors to new text genres/domains. The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation.</p><p>2 0.81295776 <a title="134-lsi-2" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>Author: Aliaksei Severyn ; Massimo Nicosia ; Alessandro Moschitti</p><p>Abstract: Measuring semantic textual similarity (STS) is at the cornerstone of many NLP applications. Different from the majority of approaches, where a large number of pairwise similarity features are used to represent a text pair, our model features the following: (i) it directly encodes input texts into relational syntactic structures; (ii) relies on tree kernels to handle feature engineering automatically; (iii) combines both structural and feature vector representations in a single scoring model, i.e., in Support Vector Regression (SVR); and (iv) delivers significant improvement over the best STS systems.</p><p>3 0.77383763 <a title="134-lsi-3" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>Author: Matt Post ; Shane Bergsma</p><p>Abstract: Syntactic features are useful for many text classification tasks. Among these, tree kernels (Collins and Duffy, 2001) have been perhaps the most robust and effective syntactic tool, appealing for their empirical success, but also because they do not require an answer to the difficult question of which tree features to use for a given task. We compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly avail- able tools) , we suggest they should always be included as baseline comparisons in tree kernel method evaluations.</p><p>4 0.58967257 <a title="134-lsi-4" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>Author: Xuezhe Ma ; Fei Xia</p><p>Abstract: In this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets.</p><p>5 0.55176198 <a title="134-lsi-5" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>Author: Boxing Chen ; Roland Kuhn ; George Foster</p><p>Abstract: This paper proposes a new approach to domain adaptation in statistical machine translation (SMT) based on a vector space model (VSM). The general idea is first to create a vector profile for the in-domain development (“dev”) set. This profile might, for instance, be a vector with a dimensionality equal to the number of training subcorpora; each entry in the vector reflects the contribution of a particular subcorpus to all the phrase pairs that can be extracted from the dev set. Then, for each phrase pair extracted from the training data, we create a vector with features defined in the same way, and calculate its similarity score with the vector representing the dev set. Thus, we obtain a de- coding feature whose value represents the phrase pair’s closeness to the dev. This is a simple, computationally cheap form of instance weighting for phrase pairs. Experiments on large scale NIST evaluation data show improvements over strong baselines: +1.8 BLEU on Arabic to English and +1.4 BLEU on Chinese to English over a non-adapted baseline, and significant improvements in most circumstances over baselines with linear mixture model adaptation. An informal analysis suggests that VSM adaptation may help in making a good choice among words with the same meaning, on the basis of style and genre.</p><p>6 0.50879687 <a title="134-lsi-6" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>7 0.50185436 <a title="134-lsi-7" href="./acl-2013-Understanding_Tables_in_Context_Using_Standard_NLP_Toolkits.html">365 acl-2013-Understanding Tables in Context Using Standard NLP Toolkits</a></p>
<p>8 0.49498239 <a title="134-lsi-8" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>9 0.49109864 <a title="134-lsi-9" href="./acl-2013-Semantic_Frames_to_Predict_Stock_Price_Movement.html">310 acl-2013-Semantic Frames to Predict Stock Price Movement</a></p>
<p>10 0.48148715 <a title="134-lsi-10" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>11 0.47941098 <a title="134-lsi-11" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>12 0.4714337 <a title="134-lsi-12" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>13 0.47063544 <a title="134-lsi-13" href="./acl-2013-Transfer_Learning_Based_Cross-lingual_Knowledge_Extraction_for_Wikipedia.html">356 acl-2013-Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia</a></p>
<p>14 0.45174205 <a title="134-lsi-14" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>15 0.44799483 <a title="134-lsi-15" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>16 0.44762501 <a title="134-lsi-16" href="./acl-2013-A_Novel_Classifier_Based_on_Quantum_Computation.html">14 acl-2013-A Novel Classifier Based on Quantum Computation</a></p>
<p>17 0.4457089 <a title="134-lsi-17" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>18 0.43913653 <a title="134-lsi-18" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>19 0.43207911 <a title="134-lsi-19" href="./acl-2013-Joint_Apposition_Extraction_with_Syntactic_and_Semantic_Constraints.html">205 acl-2013-Joint Apposition Extraction with Syntactic and Semantic Constraints</a></p>
<p>20 0.43048677 <a title="134-lsi-20" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.066), (6, 0.056), (11, 0.076), (19, 0.017), (24, 0.048), (26, 0.059), (28, 0.019), (31, 0.011), (35, 0.071), (42, 0.081), (44, 0.127), (48, 0.042), (70, 0.089), (88, 0.027), (90, 0.022), (95, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94751763 <a title="134-lda-1" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>Author: Mehdi Manshadi ; Daniel Gildea ; James Allen</p><p>Abstract: Recent work on statistical quantifier scope disambiguation (QSD) has improved upon earlier work by scoping an arbitrary number and type of noun phrases. No corpusbased method, however, has yet addressed QSD when incorporating the implicit universal of plurals and/or operators such as negation. In this paper we report early, though promising, results for automatic QSD when handling both phenomena. We also present a general model for learning to build partial orders from a set of pairwise preferences. We give an n log n algorithm for finding a guaranteed approximation of the optimal solution, which works very well in practice. Finally, we significantly improve the performance of the pre- vious model using a rich set of automatically generated features.</p><p>2 0.91070569 <a title="134-lda-2" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>Author: Shane Bergsma ; Benjamin Van Durme</p><p>Abstract: We describe a novel approach for automatically predicting the hidden demographic properties of social media users. Building on prior work in common-sense knowledge acquisition from third-person text, we first learn the distinguishing attributes of certain classes of people. For example, we learn that people in the Female class tend to have maiden names and engagement rings. We then show that this knowledge can be used in the analysis of first-person communication; knowledge of distinguishing attributes allows us to both classify users and to bootstrap new training examples. Our novel approach enables substantial improvements on the widelystudied task of user gender prediction, ob- taining a 20% relative error reduction over the current state-of-the-art.</p><p>same-paper 3 0.88692218 <a title="134-lda-3" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>Author: Barbara Plank ; Alessandro Moschitti</p><p>Abstract: Relation Extraction (RE) is the task of extracting semantic relationships between entities in text. Recent studies on relation extraction are mostly supervised. The clear drawback of supervised methods is the need of training data: labeled data is expensive to obtain, and there is often a mismatch between the training data and the data the system will be applied to. This is the problem of domain adaptation. In this paper, we propose to combine (i) term generalization approaches such as word clustering and latent semantic analysis (LSA) and (ii) structured kernels to improve the adaptability of relation extractors to new text genres/domains. The empirical evaluation on ACE 2005 domains shows that a suitable combination of syntax and lexical generalization is very promising for domain adaptation.</p><p>4 0.83769929 <a title="134-lda-4" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>5 0.83625185 <a title="134-lda-5" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>Author: Meishan Zhang ; Yue Zhang ; Wanxiang Che ; Ting Liu</p><p>Abstract: Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing.</p><p>6 0.83384383 <a title="134-lda-6" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>7 0.83170766 <a title="134-lda-7" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>8 0.82182741 <a title="134-lda-8" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>9 0.82034248 <a title="134-lda-9" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>10 0.81889606 <a title="134-lda-10" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>11 0.81819695 <a title="134-lda-11" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>12 0.81616145 <a title="134-lda-12" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>13 0.81574482 <a title="134-lda-13" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>14 0.8144148 <a title="134-lda-14" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>15 0.81324792 <a title="134-lda-15" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>16 0.81294107 <a title="134-lda-16" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>17 0.81274509 <a title="134-lda-17" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>18 0.81191516 <a title="134-lda-18" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>19 0.81039971 <a title="134-lda-19" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>20 0.81024623 <a title="134-lda-20" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
