<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-22" href="#">acl2013-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</h1>
<br/><p>Source: <a title="acl-2013-22-pdf" href="http://aclweb.org/anthology//P/P13/P13-2083.pdf">pdf</a></p><p>Author: Kartik Goyal ; Sujay Kumar Jauhar ; Huiying Li ; Mrinmaya Sachan ; Shashank Srivastava ; Eduard Hovy</p><p>Abstract: In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics pre- viously employed in the literature.</p><p>Reference: <a title="acl-2013-22-reference" href="../acl2013_reference/acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. [sent-3, score-0.358]
</p><p>2 We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. [sent-4, score-0.333]
</p><p>3 DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. [sent-7, score-0.039]
</p><p>4 , 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). [sent-11, score-0.089]
</p><p>5 Composing the ∗*Equally contributing authors distributions for “Lincoln”, “Booth”, and “killed”  gives the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. [sent-13, score-0.151]
</p><p>6 But as suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately yields greater expressive power. [sent-14, score-0.109]
</p><p>7 Thus, to remedy the bag-of-words failing, we extend the generic DSM model to several relation-specific distributions over syntactic neighborhoods. [sent-15, score-0.071]
</p><p>8 In other words, one can think of the Structured DSM (SDSM) representation of a word/phrase as several vectors defined over the same vocabulary, each vector representing the word’s selectional preferences for its various syntactic arguments. [sent-16, score-0.156]
</p><p>9 We argue that this representation not only captures individual word semantics more effectively than the standard DSM, but is also better able to express the semantics of compositional units. [sent-17, score-0.304]
</p><p>10 We prove this on the task ofjudging event coreference. [sent-18, score-0.234]
</p><p>11 Experimental results indicate that our model achieves greater predictive accuracy on the task than models that employ weaker forms of composition, as well as a baseline that relies on state-  of-the-art window based word embeddings. [sent-19, score-0.085]
</p><p>12 This suggests that our formalism holds the potential of greater expressive power in problems that involve underlying semantic compositionality. [sent-20, score-0.077]
</p><p>13 Several works have defined approaches to modelling context-word distributions anchored on a target word, topic, or sentence position. [sent-26, score-0.105]
</p><p>14 While DSMs have been very successful on a variety of tasks, they are not an effective model of  semantics as they lack properties such as compositionality or the ability to handle operators such as negation. [sent-28, score-0.226]
</p><p>15 In order to model a stronger form of semantics, there has been a recent surge in studies that phrase the problem of DSM compositionality as one of vector composition. [sent-29, score-0.194]
</p><p>16 Mitchell and Lapata (2008) propose a framework to define the composition c = f(a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition. [sent-31, score-0.225]
</p><p>17 To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. [sent-33, score-0.313]
</p><p>18 (201 1), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. [sent-36, score-0.039]
</p><p>19 Pantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. [sent-40, score-0.199]
</p><p>20 2 Event Co-reference Resolution While automated resolution of entity coreference  has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al. [sent-49, score-0.22]
</p><p>21 , 2010), there has been relatively little work on event coreference resolution. [sent-51, score-0.387]
</p><p>22 (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. [sent-53, score-0.553]
</p><p>23 3  Structured Distributional Semantics  In this paper, we propose an approach to incorporate structure into distributional semantics (more details in Goyal et al. [sent-55, score-0.238]
</p><p>24 The word distributions drawn from the context defined by a set of relations anchored on the target word (or phrase) form a set of vectors, namely a matrix for the target word. [sent-57, score-0.216]
</p><p>25 One axis of the matrix runs over all the relations and the other axis is over the distributional word vocabulary. [sent-58, score-0.344]
</p><p>26 Note that collapsing the rows of the  matrix provides the standard dependency based distributional representation. [sent-60, score-0.229]
</p><p>27 1 Building Representation: The PropStore To build a lexicon of SDSM matrices for a given vocabulary we first construct a proposition knowledge base (the PropStore) created by parsing the Simple English Wikipedia. [sent-62, score-0.071]
</p><p>28 We also store sentence indices for triples as this allows us to achieve an intuitive technique to achieve compositionality. [sent-64, score-0.044]
</p><p>29 This helps to generalize our representation when surface-form distributions are sparse. [sent-66, score-0.074]
</p><p>30 The PropStore can be used to query for the expectations of words, supersenses, relations, etc. [sent-67, score-0.06]
</p><p>31 “what is consumed” might return expectations [pasta: 1, spaghetti: 1, mice: 1 . [sent-73, score-0.06]
</p><p>32 Relations and  POS tags are obtained using a dependency parser Tratz and Hovy (201 1), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas us468  Figure 1: Sample sentences & triples  ing Wordnet Fellbaum (1998). [sent-77, score-0.088]
</p><p>33 2 Mimicking Compositionality For representing intermediate multi-word phrases, we extend the above word-relation matrix symbolism in a bottom-up fashion using the PropStore. [sent-79, score-0.062]
</p><p>34 The combination hinges on the intuition that when lexical units combine to form a larger syntactically connected phrase, the representation of the phrase is given by its own distributional neighborhood within the embedded parse tree. [sent-80, score-0.278]
</p><p>35 The distributional neighborhood of the net phrase can be computed using the PropStore given syntactic relations anchored on its parts. [sent-81, score-0.39]
</p><p>36 person and Lemma(W1) = eat appearing together with a nsubj relation to obtain expectations around “people eat” yielding [pasta: 1, spaghetti: 1 . [sent-83, score-0.219]
</p><p>37 Larger phrasal queries can be built to answer queries like “What do people in China eat with? [sent-90, score-0.071]
</p><p>38 All of this helps us to account for both relation r and knowledge K obtained from the PropStore within the compositional framework c = f(a, b, r, K). [sent-93, score-0.166]
</p><p>39 The general outline to obtain a composition of two words is given in Algorithm 1, which returns the distributional expectation around the composed unit. [sent-94, score-0.387]
</p><p>40 person nsubj eat”, steps (1) and (2) involve querying the PropStore for the individual tokens, noun. [sent-97, score-0.095]
</p><p>41 Let the resulting matrices be M1 and M2, respectively. [sent-99, score-0.071]
</p><p>42 In step (3), SentIDs (sentences where the two words appear with the specified relation) are obtained by taking the intersection between the nsubj component vectors of the two matrices M1 and M2. [sent-100, score-0.159]
</p><p>43 In step (4), the entries of the original matrices M1 and M2 are intersected with this list of common SentIDs. [sent-101, score-0.118]
</p><p>44 Finally, the resulting matrix for the composition of the two words is simply the union of all the relationwise intersected sentence IDs. [sent-102, score-0.295]
</p><p>45 Intuitively, through this procedure, we have computed the expectation around the words w1 and w2 when they are connected by the relation “r”. [sent-103, score-0.039]
</p><p>46 Similar to the two-word composition process, given a parse subtree T of a phrase, we obtain its matrix representation of empirical counts over word-relation contexts (described in Algorithm 2). [sent-104, score-0.324]
</p><p>47 3 Event Coreferentiality Given the SDSM formulation and assuming no sparsity constraints, it is possible to calculate 469  SDSM matrices for composed concepts. [sent-115, score-0.105]
</p><p>48 Intuitively, if they truly capture semantics, the two SDSM matrix representations for “Booth assassinated Lincoln” and “Booth shot  Lincoln with a gun" should be (almost) the same. [sent-117, score-0.062]
</p><p>49 To test this hypothesis we turn to the task of predicting whether two event mentions are coreferent or not, even if their surface forms differ. [sent-118, score-0.301]
</p><p>50 It may be noted that this task is different from the task of full event coreference and hence is not directly comparable to previous experimental results in the literature. [sent-119, score-0.387]
</p><p>51 Two mentions generally refer to the same event when their respective actions, agents, patients, locations, and times are (almost) the same. [sent-120, score-0.301]
</p><p>52 Given the non-compositional nature of determining equality of locations and times, we represent each event mention by a triple E = (e, a, p) for the event, agent, and patient. [sent-121, score-0.276]
</p><p>53 However, when nominalized events are encountered, we replace them by their verbal forms. [sent-123, score-0.099]
</p><p>54 (201 1) to determine the agent and patient arguments of an event mention. [sent-125, score-0.371]
</p><p>55 When SRL fails to determine either role, its empirical substitutes are obtained by querying the PropStore for the most likely word expectations for the role. [sent-126, score-0.194]
</p><p>56 It may be noted that the SDSM repre-  sentation relies on syntactic dependancy relations. [sent-127, score-0.032]
</p><p>57 Hence, to bridge the gap between these relations and the composition of semantic role participants of event mentions we empirically determine those syntactic relations which most strongly co-occur with the semantic relations connecting events, agents and patients. [sent-128, score-0.778]
</p><p>58 The triple (e, a, p) is thus the composition of the triples (a, relationsetagent, e) and (p, relationsetpatient, e), and hence a complex object. [sent-129, score-0.272]
</p><p>59 To determine equality of this complex composed representation we generate three levels of progressively simplified event constituents for comparison: Level 1: Full Composition: Mfull = ComposePhrase(e, a, p). [sent-130, score-0.303]
</p><p>60 Level 3: No Composition: ME = queryMatrix(e) MA = queryMatrix(a) MP = queryMatrix(p) To judge coreference between  events E1 and E2, we compute pairwise similarities Sim(M1full , M2full), Sim(M1part:EA, M2part:EA), etc. [sent-132, score-0.252]
</p><p>61 4  Experiments  We evaluate our method on two datasets and compare it against four baselines, two of which use window based distributional vectors and two that employ weaker forms of composition. [sent-136, score-0.24]
</p><p>62 , 2013), drawn from 100 news articles  about violent events, contains manually created annotations for 2214 pairs of co-referent and noncoreferent events each. [sent-139, score-0.099]
</p><p>63 Where available, events’ semantic role-fillers for agent and patient are annotated as well. [sent-140, score-0.176]
</p><p>64 When missing, empirical substitutes were obtained by querying the PropStore for the preferred word attachments. [sent-141, score-0.134]
</p><p>65 EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents from Google News is clustered into 45 topics, with event coreference chains annotated over each topic. [sent-142, score-0.387]
</p><p>66 The event mentions are enriched with semantic roles to obtain the canonical event structure described above. [sent-143, score-0.574]
</p><p>67 Positive instances are obtained by taking pairwise event mentions within each chain, and negative instances are generated from pairwise event mentions across chains, but within the same topic. [sent-144, score-0.602]
</p><p>68 We also compare SDSM against the window-based embeddings trained using a recursive neural network (SENNA) (Collobert et al. [sent-149, score-0.136]
</p><p>69 87c39410  SENNA to generate level 3 similarity features for events’ individual words (agent, patient and action). [sent-160, score-0.064]
</p><p>70 As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition. [sent-161, score-0.054]
</p><p>71 We extend it to our matrix representation and build two baselines AVC (element-wise addition) and MVC (element-wise multiplication). [sent-162, score-0.097]
</p><p>72 The IC corpus comprises of domain specific texts, resulting in high lexical overlap between event mentions. [sent-167, score-0.234]
</p><p>73 The improvements over DSM and SENNA embeddings, support our hypothesis that syntax lends greater expressive power to distributional semantics in compositional configurations. [sent-169, score-0.403]
</p><p>74 Furthermore, the increase in predictive accuracy over MVC and AVC shows that our formulation of composition of two words based on the relation binding them yields a stronger form of compositionality than simple additive and multiplicative models. [sent-170, score-0.431]
</p><p>75 Next, we perform an ablation study to determine the most predictive features for the task of  event coreferentiality. [sent-171, score-0.285]
</p><p>76 The forward selection procedure reveals that the most informative attributes are the level 2 compositional features involving the agent and the action, as well as their individual level 3 features. [sent-172, score-0.2]
</p><p>77 This corresponds to the intuition that the agent and the action are the principal determiners for identifying events. [sent-173, score-0.106]
</p><p>78 Features involving the patient and level 1 features are least useful. [sent-174, score-0.064]
</p><p>79 This is probably because features involving full composition are sparse, and not as likely to provide statistically significant evidence. [sent-175, score-0.186]
</p><p>80 5  Conclusion and Future Work  We outlined an approach that introduces structure into distributed semantic representations gives us an ability to compare the identity of two representations derived from supposedly semantically identical phrases with different surface realizations. [sent-177, score-0.039]
</p><p>81 We employed the task of event coreference to validate our representation and achieved significantly higher predictive accuracy than several baselines. [sent-178, score-0.473]
</p><p>82 In the future, we would like to extend our model to other semantic tasks such as paraphrase detection, lexical substitution and recognizing textual entailment. [sent-179, score-0.039]
</p><p>83 We would also like to replace our syntactic relations to semantic relations and explore various ways of dimensionality reduction to solve this problem. [sent-180, score-0.169]
</p><p>84 Nouns  are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space. [sent-191, score-0.039]
</p><p>85 Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. [sent-201, score-0.044]
</p><p>86 A structured distributional semantic model : Integrating structure with semantics. [sent-237, score-0.206]
</p><p>87 Concrete sentence spaces for compositional distributional models of meaning. [sent-241, score-0.294]
</p><p>88 A regression model of adjective-noun compositionality in distributional semantics. [sent-246, score-0.322]
</p><p>89 Simple coreference resolution with rich syntactic and semantic features. [sent-251, score-0.291]
</p><p>90 Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences. [sent-277, score-0.05]
</p><p>91 Conundrums in noun phrase coreference resolution: making sense of the stateof-the-art. [sent-336, score-0.192]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sentids', 0.345), ('propstore', 0.287), ('sdsm', 0.259), ('event', 0.234), ('querymatrix', 0.201), ('dsm', 0.188), ('composition', 0.186), ('distributional', 0.167), ('compositionality', 0.155), ('coreference', 0.153), ('stroudsburg', 0.151), ('senna', 0.127), ('compositional', 0.127), ('pa', 0.12), ('lincoln', 0.117), ('booth', 0.11), ('dsms', 0.105), ('events', 0.099), ('embeddings', 0.097), ('avc', 0.086), ('composepair', 0.086), ('mvc', 0.076), ('collobert', 0.075), ('agent', 0.073), ('matrices', 0.071), ('semantics', 0.071), ('eat', 0.071), ('sst', 0.07), ('mentions', 0.067), ('resolution', 0.067), ('anchored', 0.066), ('goyal', 0.066), ('patient', 0.064), ('matrix', 0.062), ('expectations', 0.06), ('mccarthy', 0.06), ('baroni', 0.058), ('composephrase', 0.057), ('coreferentiality', 0.057), ('huiying', 0.057), ('mpart', 0.057), ('mrinmaya', 0.057), ('pasta', 0.057), ('sachan', 0.057), ('shashank', 0.057), ('weisman', 0.057), ('killed', 0.056), ('ic', 0.055), ('multiplication', 0.054), ('ecb', 0.051), ('aghdha', 0.051), ('rudolph', 0.051), ('spaghetti', 0.051), ('tellex', 0.051), ('predictive', 0.051), ('selectional', 0.05), ('nsubj', 0.049), ('relations', 0.049), ('substitutes', 0.047), ('sujay', 0.047), ('intersected', 0.047), ('querying', 0.046), ('hovy', 0.045), ('association', 0.044), ('jauhar', 0.044), ('srivastava', 0.044), ('stoyanov', 0.044), ('supersense', 0.044), ('triples', 0.044), ('bejan', 0.042), ('triple', 0.042), ('erk', 0.042), ('emnlp', 0.041), ('empirical', 0.041), ('tratz', 0.04), ('raghunathan', 0.04), ('distributions', 0.039), ('phrase', 0.039), ('recursive', 0.039), ('relation', 0.039), ('vectors', 0.039), ('grefenstette', 0.039), ('heeyoung', 0.039), ('semantic', 0.039), ('expressive', 0.038), ('neighborhood', 0.037), ('pantel', 0.036), ('ri', 0.036), ('ciaramita', 0.035), ('dinu', 0.035), ('thater', 0.035), ('representation', 0.035), ('eduard', 0.034), ('agents', 0.034), ('weaker', 0.034), ('composed', 0.034), ('action', 0.033), ('axis', 0.033), ('syntactic', 0.032), ('mitchell', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="22-tfidf-1" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>Author: Kartik Goyal ; Sujay Kumar Jauhar ; Huiying Li ; Mrinmaya Sachan ; Shashank Srivastava ; Eduard Hovy</p><p>Abstract: In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics pre- viously employed in the literature.</p><p>2 0.19107214 <a title="22-tfidf-2" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>Author: Raffaella Bernardi ; Georgiana Dinu ; Marco Marelli ; Marco Baroni</p><p>Abstract: Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</p><p>3 0.18915121 <a title="22-tfidf-3" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>4 0.18609056 <a title="22-tfidf-4" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>Author: Angeliki Lazaridou ; Marco Marelli ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</p><p>5 0.16930905 <a title="22-tfidf-5" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>Author: Karl Moritz Hermann ; Phil Blunsom</p><p>Abstract: Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.</p><p>6 0.16754851 <a title="22-tfidf-6" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>7 0.16686393 <a title="22-tfidf-7" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>8 0.14630191 <a title="22-tfidf-8" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>9 0.14404657 <a title="22-tfidf-9" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>10 0.14345756 <a title="22-tfidf-10" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>11 0.12683964 <a title="22-tfidf-11" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>12 0.12499893 <a title="22-tfidf-12" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>13 0.11644142 <a title="22-tfidf-13" href="./acl-2013-Extracting_Events_with_Informal_Temporal_References_in_Personal_Histories_in_Online_Communities.html">153 acl-2013-Extracting Events with Informal Temporal References in Personal Histories in Online Communities</a></p>
<p>14 0.11586268 <a title="22-tfidf-14" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>15 0.10866798 <a title="22-tfidf-15" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>16 0.10540833 <a title="22-tfidf-16" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>17 0.099507466 <a title="22-tfidf-17" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>18 0.095872261 <a title="22-tfidf-18" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>19 0.089295514 <a title="22-tfidf-19" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>20 0.085179448 <a title="22-tfidf-20" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.213), (1, 0.068), (2, -0.011), (3, -0.186), (4, -0.066), (5, 0.114), (6, -0.012), (7, 0.141), (8, -0.052), (9, 0.112), (10, -0.019), (11, -0.062), (12, 0.181), (13, -0.105), (14, -0.035), (15, 0.131), (16, -0.171), (17, -0.074), (18, 0.011), (19, 0.031), (20, -0.061), (21, -0.054), (22, -0.018), (23, -0.117), (24, -0.012), (25, -0.118), (26, -0.05), (27, 0.058), (28, 0.024), (29, 0.062), (30, -0.015), (31, 0.103), (32, -0.029), (33, -0.017), (34, 0.043), (35, 0.041), (36, 0.069), (37, 0.028), (38, 0.036), (39, 0.021), (40, 0.042), (41, -0.112), (42, 0.03), (43, -0.031), (44, 0.045), (45, 0.023), (46, 0.008), (47, -0.016), (48, -0.005), (49, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92310238 <a title="22-lsi-1" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>Author: Kartik Goyal ; Sujay Kumar Jauhar ; Huiying Li ; Mrinmaya Sachan ; Shashank Srivastava ; Eduard Hovy</p><p>Abstract: In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics pre- viously employed in the literature.</p><p>2 0.74249983 <a title="22-lsi-2" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>Author: Raffaella Bernardi ; Georgiana Dinu ; Marco Marelli ; Marco Baroni</p><p>Abstract: Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</p><p>3 0.71949971 <a title="22-lsi-3" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>Author: Georgiana Dinu ; Nghia The Pham ; Marco Baroni</p><p>Abstract: We introduce DISSECT, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. The toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure.</p><p>4 0.65767753 <a title="22-lsi-4" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: Generative probabilistic models have been used for content modelling and template induction, and are typically trained on small corpora in the target domain. In contrast, vector space models of distributional semantics are trained on large corpora, but are typically applied to domaingeneral lexical disambiguation tasks. We introduce Distributional Semantic Hidden Markov Models, a novel variant of a hidden Markov model that integrates these two approaches by incorporating contextualized distributional semantic vectors into a generative model as observed emissions. Experiments in slot induction show that our approach yields improvements in learning coherent entity clusters in a domain. In a subsequent extrinsic evaluation, we show that these improvements are also reflected in multi-document summarization.</p><p>5 0.65648472 <a title="22-lsi-5" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>Author: Angeliki Lazaridou ; Marco Marelli ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</p><p>6 0.59120768 <a title="22-lsi-6" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>7 0.56126887 <a title="22-lsi-7" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>8 0.55951208 <a title="22-lsi-8" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>9 0.53152984 <a title="22-lsi-9" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>10 0.52601421 <a title="22-lsi-10" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>11 0.5140909 <a title="22-lsi-11" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>12 0.51250398 <a title="22-lsi-12" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>13 0.50576675 <a title="22-lsi-13" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>14 0.50305319 <a title="22-lsi-14" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>15 0.48086151 <a title="22-lsi-15" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>16 0.46773416 <a title="22-lsi-16" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>17 0.46542776 <a title="22-lsi-17" href="./acl-2013-Building_and_Evaluating_a_Distributional_Memory_for_Croatian.html">76 acl-2013-Building and Evaluating a Distributional Memory for Croatian</a></p>
<p>18 0.46112031 <a title="22-lsi-18" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>19 0.44984108 <a title="22-lsi-19" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>20 0.4391447 <a title="22-lsi-20" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.045), (6, 0.029), (11, 0.066), (24, 0.03), (26, 0.032), (28, 0.013), (35, 0.121), (42, 0.047), (48, 0.082), (64, 0.252), (67, 0.019), (70, 0.073), (88, 0.058), (90, 0.017), (95, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9099316 <a title="22-lda-1" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>Author: Eneko Agirre ; Nikolaos Aletras ; Paul Clough ; Samuel Fernando ; Paula Goodale ; Mark Hall ; Aitor Soroa ; Mark Stevenson</p><p>Abstract: This paper describes a system for navigating large collections of information about cultural heritage which is applied to Europeana, the European Library. Europeana contains over 20 million artefacts with meta-data in a wide range of European languages. The system currently provides access to Europeana content with meta-data in English and Spanish. The paper describes how Natural Language Processing is used to enrich and organise this meta-data to assist navigation through Europeana and shows how this information is used within the system.</p><p>same-paper 2 0.82304478 <a title="22-lda-2" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>Author: Kartik Goyal ; Sujay Kumar Jauhar ; Huiying Li ; Mrinmaya Sachan ; Shashank Srivastava ; Eduard Hovy</p><p>Abstract: In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics pre- viously employed in the literature.</p><p>3 0.81666404 <a title="22-lda-3" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>4 0.81291628 <a title="22-lda-4" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Dan Roth</p><p>Abstract: Semantic parsing is a domain-dependent process by nature, as its output is defined over a set of domain symbols. Motivated by the observation that interpretation can be decomposed into domain-dependent and independent components, we suggest a novel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains.</p><p>5 0.80797493 <a title="22-lda-5" href="./acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines.html">152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</a></p>
<p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>6 0.7480458 <a title="22-lda-6" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>7 0.67506254 <a title="22-lda-7" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>8 0.65050817 <a title="22-lda-8" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>9 0.64926785 <a title="22-lda-9" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>10 0.63207531 <a title="22-lda-10" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>11 0.62831765 <a title="22-lda-11" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>12 0.61886829 <a title="22-lda-12" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>13 0.60881758 <a title="22-lda-13" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>14 0.60655379 <a title="22-lda-14" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>15 0.60437727 <a title="22-lda-15" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>16 0.59911668 <a title="22-lda-16" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>17 0.59753138 <a title="22-lda-17" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>18 0.59628582 <a title="22-lda-18" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>19 0.59479374 <a title="22-lda-19" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>20 0.59206724 <a title="22-lda-20" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
