<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-345" href="#">acl2013-345</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</h1>
<br/><p>Source: <a title="acl-2013-345-pdf" href="http://aclweb.org/anthology//P/P13/P13-1041.pdf">pdf</a></p><p>Author: Kashyap Popat ; Balamurali A.R ; Pushpak Bhattacharyya ; Gholamreza Haffari</p><p>Abstract: Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA.</p><p>Reference: <a title="acl-2013-345-reference" href="../acl2013_reference/acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. [sent-6, score-0.4]
</p><p>2 In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. [sent-9, score-0.486]
</p><p>3 Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. [sent-10, score-1.034]
</p><p>4 Similar idea  is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA. [sent-11, score-0.367]
</p><p>5 , word sparsity, can be addressed in one of the two obvious ways: 1) sparsity reduction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. [sent-18, score-0.282]
</p><p>6 Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. [sent-30, score-0.453]
</p><p>7 In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. [sent-41, score-0.375]
</p><p>8 All these factors make the paradigmatic property based cluster features like WordNet senses a less promising pursuit for SA. [sent-50, score-0.595]
</p><p>9 The syntagmatic analysis essentially makes use of distributional similarity and may in many circumstances subsume the paradigmatic analysis. [sent-51, score-0.451]
</p><p>10 In the current work, this particular insight is used to solve the data sparsity problem in the sentiment analysis by leveraging unlabelled monolingual corpora. [sent-52, score-0.7]
</p><p>11 Specifically, experiments are performed to investigate whether features developed from manually crafted clusterings (coming from WordNet) can be replaced by those generated from clustering based on syntagmatic properties. [sent-53, score-0.467]
</p><p>12 Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. [sent-54, score-0.655]
</p><p>13 To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. [sent-61, score-0.217]
</p><p>14 These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. [sent-62, score-0.46]
</p><p>15 Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of unlabelled monolingual corpora. [sent-64, score-0.757]
</p><p>16 Features created from manually built and finer clusters can be replaced by inexpensive cluster based features generated solely from unlabelled corpora. [sent-66, score-0.643]
</p><p>17 , English, Hindi and Marathi1 suggest that cluster based features can considerably boost the performance of an SA system. [sent-68, score-0.343]
</p><p>18 Word clustering is a powerful mechanism to “transfer” a sentiment classifier from one language to another. [sent-72, score-0.509]
</p><p>19 Section 3 explains different word cluster based features employed to reduce data sparsity for monolingual SA. [sent-75, score-0.517]
</p><p>20 There has been research related to clustering and sentiment analysis. [sent-94, score-0.471]
</p><p>21 (201 1), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. [sent-96, score-0.636]
</p><p>22 (201 1) attempts to cluster features of a product to perform sentiment analysis on product reviews. [sent-98, score-0.695]
</p><p>23 In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. [sent-100, score-0.352]
</p><p>24 Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. [sent-104, score-0.312]
</p><p>25 In this section, different clustering approaches are presented for feature engineering in a monolingual setting. [sent-106, score-0.248]
</p><p>26 Each synset can be considered as a word cluster comprising of semantically similar words. [sent-109, score-0.33]
</p><p>27 (201 1) showed that WordNet synsets can act as good features for document level sentiment classification. [sent-111, score-0.399]
</p><p>28 To empirically prove the superiority of sense based features, different variants of a travel review domain corpus were generated by using automatic/manual sense disambiguation techniques. [sent-113, score-0.394]
</p><p>29 In this study, synset identifiers are extracted from manually/automatically sense annotated corpora and used as features for creating sentiment classifiers. [sent-116, score-0.637]
</p><p>30 As the algorithm is based on co-occurrence, one can extract the classes that have the flavour of syntagmatic grouping, depending on the nature of underlying statistics. [sent-121, score-0.255]
</p><p>31 Then, the likelihood (L()) of a sequence of word tokens, w = [wj]jm=1, with wj ∈ V , can be factored as, Ym  L(w; C) = Yp(wj|C(wj))p(C(wj)|C(wj−1)))  Yj=1  (1) Words are assigned to clusters such that the above quantity is maximized. [sent-129, score-0.299]
</p><p>32 For the purpose of sentiment classification, cluster identifiers representing words in the document are used as features for training. [sent-130, score-0.741]
</p><p>33 Given that sentiment analysis is a less resource intensive task compared to machine translation, the use of an MT system is hard to justify for performing 414  CLSA. [sent-136, score-0.4]
</p><p>34 As a viable alternative, cluster linkages could be learned from a bilingual parallel corpus and these linkages can be used to bridge the language gap for CLSA. [sent-137, score-0.588]
</p><p>35 The language whose annotated data is used for training is called the source language (S), while the language whose documents are to be sentiment classified is  referred to as the target language (T). [sent-139, score-0.312]
</p><p>36 An entry in the multidict will have a WordNet sense identifier from S and the corresponding WordNet sense identifier from T. [sent-144, score-0.403]
</p><p>37 For those sense identifiers which do not have a corresponding entry in the Multidict, no projection is performed. [sent-148, score-0.233]
</p><p>38 2  Approach 2: Direct Cluster Linking (DCL) Given a parallel bilingual corpus, word clusters in S can be aligned to clusters in T. [sent-150, score-0.295]
</p><p>39 CTX(w∪Tk α)=t CS(waSk)=l Here, a target side cluster t ∈ CT is linked to a source asi tdaerg cetlu ssitdere lc ∈ eCrS t s∈uc Ch that the total alignment score u bsettewre len ∈ w Cords in l and words in t is maximum. [sent-158, score-0.373]
</p><p>40 CS and CT stands for source and target side cluster list respectively. [sent-159, score-0.29]
</p><p>41 LC(l) gives the target side cluster t to which lis linked. [sent-160, score-0.29]
</p><p>42 3  Approach 3: Cross-Lingual Clustering (XC) Direct cluster linking approach suffers from the size of alignment dataset in the form of parallel corpora. [sent-162, score-0.417]
</p><p>43 Given a list of words and clusters it belongs to, a clustering algorithm tries to obtain word-cluster association which maximizes the joint likelihood of words and clusters. [sent-167, score-0.325]
</p><p>44 Whereas in case of crosslingual clustering, the same clustering can be explained in terms of maximizing the likelihood of monolingual word-cluster pairs of the source, the target and alignments between them. [sent-168, score-0.335]
</p><p>45 For clustering the words, monolingual data of  Indian Languages Corpora Initiative (ILCI)2 was used. [sent-203, score-0.248]
</p><p>46 It should also be noted that sentiment annotated data was also included in the data used for the word clusterings process. [sent-204, score-0.312]
</p><p>47 This dataset was sense annotated using an automatic WSD engine which was trained on tourism domain (Khapra et al. [sent-221, score-0.215]
</p><p>48 Apart from being marked with polarity labels at document level, they are also manually sense annotated using Hindi and Marathi WordNet respectively. [sent-229, score-0.227]
</p><p>49 66  Table 1: Classification accuracy for monolingual sentiment analysis. [sent-261, score-0.448]
</p><p>50 40 Table 2: Classification accuracy (in %) versus cluster size (number of clusters to be used). [sent-279, score-0.514]
</p><p>51 SVM was used since it is known to perform well for sentiment classification (Pang et al. [sent-281, score-0.383]
</p><p>52 It must be noted that accuracies reported for cluster based features are with respect to the best accuracy based on different cluster sizes. [sent-287, score-0.724]
</p><p>53 The improvements in results of cluster features based approach is found to be statistically significant over the word features based approach and sense features based approach at 95% confidence level when tested using a paired t-test (except for Hindi cluster features based approach). [sent-288, score-0.938]
</p><p>54 But in general, their accuracies do not significantly vary after cluster size crosses 1500. [sent-289, score-0.385]
</p><p>55 Table 2 shows the classification accuracy  variation when cluster size is altered. [sent-290, score-0.459]
</p><p>56 For, En-TD and En-PD experiments, the cluster size was varied between 200-3000 with an interval of 500 (after a size of 500). [sent-291, score-0.392]
</p><p>57 In the En-TD experiment, the best accuracy is achieved for cluster size 500, which is lesser than the number of unique-words/unique-senses (6435/6004) present in the data. [sent-292, score-0.465]
</p><p>58 the optimal cluster size of 2500 is also lesser than the number of unique-words/unique-senses (30468/4735) present in the data. [sent-298, score-0.418]
</p><p>59 As in monolingual case, the reported accuracies are for features based on the best cluster size. [sent-309, score-0.476]
</p><p>60 Syntagmatic analysis may be used in lieu of paradigmatic analysis for SA: The results suggest that word cluster based features using syntagmatic analysis is comparatively better than cluster (sense) based features using paradigmatic analysis. [sent-321, score-1.438]
</p><p>61 For English, the gap between classification accuracy based on sense features and cluster features is around 10%. [sent-323, score-0.697]
</p><p>62 However for Hindi, classifier built on features based on syntagmatic analysis trails the one based on paradigmatic analysis. [sent-328, score-0.542]
</p><p>63 However, the same would not have occurred for a classifier developed on sense based  features as it was manually sense tagged. [sent-335, score-0.383]
</p><p>64 However, it must be noted that clustering based on unlabelled corpora is less taxing than manually creating paradigmatic property based clusters like WordNet synsets. [sent-342, score-0.691]
</p><p>65 Barring one instance, both cluster based features outperform word based features. [sent-343, score-0.343]
</p><p>66 The reason for the drop in the accuracy of approach based on sense features for En-PD dataset is the domain specific nature of sentiment analysis (Blitzer et al. [sent-344, score-0.667]
</p><p>67 Domain issues are resolved while using cluster based features: For En-PD, the classifier developed using sense features based on  paradigmatic analysis performs inferior to word based features. [sent-347, score-0.723]
</p><p>68 The sense disambiguation accuracy of the same would have lowered in a cross-domain setting. [sent-351, score-0.226]
</p><p>69 However, it was seen that classifier developed on cluster features based on syntagmatic analysis do not suffer from this. [sent-353, score-0.676]
</p><p>70 In addition, as more unlabelled data is included for clustering, the classification accuracy improves. [sent-355, score-0.292]
</p><p>71 1 million unlabelled  documents, SA accuracy improved by 1%. [sent-358, score-0.221]
</p><p>72 Cluster based features using syntagmatic analysis requires lesser training data: Cluster based features drastically reduces the dimension of the feature vector. [sent-361, score-0.478]
</p><p>73 For instance, the size of sense based features for En-TD dataset was 1/6th of the size of word based features. [sent-362, score-0.334]
</p><p>74 The reduction in the perplexity leads to the reduction of training documents to attain the same classification accuracy without any dimensionality reduction. [sent-364, score-0.23]
</p><p>75 This is evident from Figure 1 where accuracy of the cluster features based on unlabelled corpora are higher even with lesser training data. [sent-365, score-0.675]
</p><p>76 Effect of cluster size: The cluster size (number of clusters employed) has an implication on the purity of each cluster with respect to the application. [sent-367, score-1.047]
</p><p>77 The system performance improved upon increasing the cluster size and converged  after attaining a certain level of accuracy. [sent-368, score-0.341]
</p><p>78 In general, it was found that the best classification accuracy was obtained for a cluster size between 1000 and 2500. [sent-369, score-0.459]
</p><p>79 As evident from Table 2, once the optimal accuracy is obtained, no significant changes were observed by increasing the cluster size. [sent-370, score-0.337]
</p><p>80 Whereas, sentiment classifier using sense (PS) or direct cluster linking (DCL) is not very effective. [sent-374, score-0.786]
</p><p>81 The number of a linkages between sense from English to Hindi is only around 1/3rd the size of Princeton WordNet (Fellbaum, 1998). [sent-376, score-0.289]
</p><p>82 All this is obliterated by the use of a cluster based CLSA approach. [sent-388, score-0.29]
</p><p>83 Moreover, as more monolingual copora is added for clustering, the cross lingual cluster linkages could be refined. [sent-389, score-0.562]
</p><p>84 8  Conclusion and Future Work  This paper explored feasibility of using word cluster based features in lieu of features based on WordNet senses for sentiment analysis to alleviate the problem of data sparsity. [sent-391, score-0.835]
</p><p>85 Abstractly, the motivation was to see if highly effective features based on paradigmatic property based clustering could be replaced with the inexpensive ones based on syntagmatic property for SA. [sent-392, score-0.707]
</p><p>86 It was found that cluster features based on syntagmatic analysis are better than the WordNet sense features based on paradigmatic analysis for SA. [sent-394, score-1.033]
</p><p>87 For CLSA, clusters linked together using  unlabelled parallel corpora do away with the need of translating labelled corpora from one language to another using an intermediary MT system or bilingual dictionary. [sent-397, score-0.507]
</p><p>88 However, the size of the parallel corpora required 419  for CLSA can considerably be much lesser than the size of the parallel corpora required to train an MT system. [sent-402, score-0.333]
</p><p>89 A naive cluster linkage algorithm based on word alignments was used to perform CLSA. [sent-403, score-0.337]
</p><p>90 It would be interesting to see if these could be replaced by clusters based on the syntagmatic property. [sent-407, score-0.381]
</p><p>91 Cross-lingual sentiment analysis for Indian languages using linked wordnets. [sent-418, score-0.393]
</p><p>92 Lost in translation: viability of machine translation for cross language sentiment analysis. [sent-425, score-0.378]
</p><p>93 Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. [sent-437, score-0.312]
</p><p>94 Further experiments in sentiment analysis of french movie reviews. [sent-487, score-0.352]
</p><p>95 Learning to shift the polarity of words for sentiment classification. [sent-499, score-0.359]
</p><p>96 Domain-specific word sense disambiguation combining corpus based and wordnet based parameters. [sent-503, score-0.272]
</p><p>97 Delta TFIDF: An improved feature space for sentiment analysis. [sent-530, score-0.312]
</p><p>98 Topic sentiment mixture: modeling facets and opinions in weblogs. [sent-538, score-0.312]
</p><p>99 Dependency tree-based sentiment classification using crfs with hidden variables. [sent-558, score-0.383]
</p><p>100 An exploration into the use of contextual document clustering for cluster sentiment analysis. [sent-586, score-0.795]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clsa', 0.407), ('sentiment', 0.312), ('cluster', 0.29), ('syntagmatic', 0.255), ('marathi', 0.195), ('unlabelled', 0.174), ('balamurali', 0.166), ('clustering', 0.159), ('paradigmatic', 0.156), ('sense', 0.146), ('sa', 0.144), ('wj', 0.133), ('wordnet', 0.126), ('clusters', 0.126), ('hindi', 0.123), ('multidict', 0.111), ('linkages', 0.092), ('monolingual', 0.089), ('sparsity', 0.085), ('mt', 0.079), ('lesser', 0.077), ('dcl', 0.075), ('classification', 0.071), ('stroudsburg', 0.066), ('travel', 0.066), ('pushpak', 0.066), ('ackstr', 0.063), ('scarce', 0.058), ('lingual', 0.058), ('truly', 0.057), ('xc', 0.057), ('ct', 0.057), ('reduction', 0.056), ('intermediary', 0.055), ('martineau', 0.055), ('wask', 0.055), ('wtk', 0.055), ('mar', 0.055), ('senses', 0.054), ('features', 0.053), ('pa', 0.053), ('identifiers', 0.052), ('size', 0.051), ('khapra', 0.05), ('wkt', 0.049), ('wordnets', 0.048), ('resource', 0.048), ('cs', 0.048), ('accuracy', 0.047), ('polarity', 0.047), ('alignments', 0.047), ('accuracies', 0.044), ('parallel', 0.043), ('lc', 0.042), ('property', 0.042), ('linked', 0.041), ('likelihood', 0.04), ('jm', 0.04), ('banea', 0.04), ('analysis', 0.04), ('synset', 0.04), ('ps', 0.04), ('pang', 0.039), ('classifier', 0.038), ('wsd', 0.038), ('aditya', 0.037), ('mitesh', 0.037), ('gap', 0.037), ('ghorbel', 0.037), ('ilci', 0.037), ('mullen', 0.037), ('rentoumi', 0.037), ('ripe', 0.037), ('rooney', 0.037), ('syntagma', 0.037), ('thereafter', 0.037), ('brown', 0.037), ('domain', 0.036), ('projection', 0.035), ('bridge', 0.034), ('reviews', 0.034), ('corpora', 0.034), ('document', 0.034), ('morphologically', 0.033), ('translation', 0.033), ('dataset', 0.033), ('uszkoreit', 0.033), ('cross', 0.033), ('benamara', 0.033), ('ator', 0.033), ('obliterate', 0.033), ('ikeda', 0.033), ('samuelsson', 0.033), ('lowered', 0.033), ('mohanty', 0.033), ('minkov', 0.033), ('lieu', 0.033), ('wt', 0.033), ('comparatively', 0.032), ('negative', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="345-tfidf-1" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>Author: Kashyap Popat ; Balamurali A.R ; Pushpak Bhattacharyya ; Gholamreza Haffari</p><p>Abstract: Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA.</p><p>2 0.21869482 <a title="345-tfidf-2" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>Author: Hongliang Yu ; Zhi-Hong Deng ; Shiyingxue Li</p><p>Abstract: Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications. Most existing researches exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words.</p><p>3 0.20049952 <a title="345-tfidf-3" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>4 0.19448984 <a title="345-tfidf-4" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>5 0.18001951 <a title="345-tfidf-5" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>Author: Manaal Faruqui ; Chris Dyer</p><p>Abstract: We present an information theoretic objective for bilingual word clustering that incorporates both monolingual distributional evidence as well as cross-lingual evidence from parallel corpora to learn high quality word clusters jointly in any number of languages. The monolingual component of our objective is the average mutual information of clusters of adjacent words in each language, while the bilingual component is the average mutual information of the aligned clusters. To evaluate our method, we use the word clusters in an NER system and demonstrate a statistically significant improvement in F1 score when using bilingual word clusters instead of monolingual clusters.</p><p>6 0.16770864 <a title="345-tfidf-6" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>7 0.15822145 <a title="345-tfidf-7" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>8 0.15127175 <a title="345-tfidf-8" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>9 0.15081762 <a title="345-tfidf-9" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<p>10 0.14724042 <a title="345-tfidf-10" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>11 0.14591391 <a title="345-tfidf-11" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>12 0.13843416 <a title="345-tfidf-12" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<p>13 0.13599537 <a title="345-tfidf-13" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>14 0.13238777 <a title="345-tfidf-14" href="./acl-2013-Probabilistic_Sense_Sentiment_Similarity_through_Hidden_Emotions.html">284 acl-2013-Probabilistic Sense Sentiment Similarity through Hidden Emotions</a></p>
<p>15 0.12919451 <a title="345-tfidf-15" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>16 0.12097979 <a title="345-tfidf-16" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>17 0.11989687 <a title="345-tfidf-17" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>18 0.11691183 <a title="345-tfidf-18" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>19 0.10579256 <a title="345-tfidf-19" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>20 0.10574865 <a title="345-tfidf-20" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.252), (1, 0.175), (2, 0.036), (3, 0.138), (4, -0.085), (5, -0.19), (6, -0.091), (7, 0.103), (8, 0.05), (9, -0.024), (10, 0.173), (11, -0.112), (12, -0.077), (13, 0.018), (14, 0.038), (15, 0.057), (16, 0.004), (17, 0.019), (18, 0.041), (19, 0.087), (20, -0.004), (21, -0.028), (22, 0.008), (23, -0.01), (24, 0.045), (25, -0.145), (26, 0.02), (27, 0.008), (28, 0.012), (29, 0.002), (30, 0.075), (31, 0.085), (32, -0.056), (33, -0.096), (34, -0.057), (35, -0.013), (36, -0.055), (37, 0.073), (38, -0.102), (39, 0.006), (40, -0.064), (41, 0.042), (42, -0.036), (43, -0.012), (44, 0.051), (45, -0.021), (46, 0.136), (47, -0.008), (48, -0.058), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9445129 <a title="345-lsi-1" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>Author: Kashyap Popat ; Balamurali A.R ; Pushpak Bhattacharyya ; Gholamreza Haffari</p><p>Abstract: Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA.</p><p>2 0.70305377 <a title="345-lsi-2" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>3 0.68525207 <a title="345-lsi-3" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>Author: Hongliang Yu ; Zhi-Hong Deng ; Shiyingxue Li</p><p>Abstract: Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications. Most existing researches exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words.</p><p>4 0.67844325 <a title="345-lsi-4" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<p>Author: Eric T. Nalisnick ; Henry S. Baird</p><p>Abstract: We present an automatic method for analyzing sentiment dynamics between characters in plays. This literary format’s structured dialogue allows us to make assumptions about who is participating in a conversation. Once we have an idea of who a character is speaking to, the sentiment in his or her speech can be attributed accordingly, allowing us to generate lists of a character’s enemies and allies as well as pinpoint scenes critical to a character’s emotional development. Results of experiments on Shakespeare’s plays are presented along with discussion of how this work can be extended to unstructured texts (i.e. novels).</p><p>5 0.64803392 <a title="345-lsi-5" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: We study subjective language media and create Twitter-specific lexicons via bootstrapping sentiment-bearing terms from multilingual Twitter streams. Starting with a domain-independent, highprecision sentiment lexicon and a large pool of unlabeled data, we bootstrap Twitter-specific sentiment lexicons, using a small amount of labeled data to guide the process. Our experiments on English, Spanish and Russian show that the resulting lexicons are effective for sentiment classification for many underexplored languages in social media.</p><p>6 0.64654797 <a title="345-lsi-6" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>7 0.60313022 <a title="345-lsi-7" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>8 0.59063518 <a title="345-lsi-8" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>9 0.57695848 <a title="345-lsi-9" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>10 0.56095421 <a title="345-lsi-10" href="./acl-2013-Probabilistic_Sense_Sentiment_Similarity_through_Hidden_Emotions.html">284 acl-2013-Probabilistic Sense Sentiment Similarity through Hidden Emotions</a></p>
<p>11 0.5550189 <a title="345-lsi-11" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>12 0.5460304 <a title="345-lsi-12" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>13 0.53695971 <a title="345-lsi-13" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>14 0.52205384 <a title="345-lsi-14" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>15 0.51823837 <a title="345-lsi-15" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>16 0.51321149 <a title="345-lsi-16" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>17 0.50492483 <a title="345-lsi-17" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>18 0.50402039 <a title="345-lsi-18" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>19 0.48197913 <a title="345-lsi-19" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>20 0.4811005 <a title="345-lsi-20" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (6, 0.032), (11, 0.05), (15, 0.019), (24, 0.035), (26, 0.067), (35, 0.054), (42, 0.052), (48, 0.049), (56, 0.012), (70, 0.025), (88, 0.386), (90, 0.016), (95, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95299643 <a title="345-lda-1" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>Author: Greg Durrett ; David Hall ; Dan Klein</p><p>Abstract: Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.</p><p>2 0.94053572 <a title="345-lda-2" href="./acl-2013-Evaluating_a_City_Exploration_Dialogue_System_with_Integrated_Question-Answering_and_Pedestrian_Navigation.html">141 acl-2013-Evaluating a City Exploration Dialogue System with Integrated Question-Answering and Pedestrian Navigation</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon ; Phil Bartie ; Tiphaine Dalmas ; Anna Dickinson ; Xingkun Liu ; William Mackaness ; Bonnie Webber</p><p>Abstract: We present a city navigation and tourist information mobile dialogue app with integrated question-answering (QA) and geographic information system (GIS) modules that helps pedestrian users to navigate in and learn about urban environments. In contrast to existing mobile apps which treat these problems independently, our Android app addresses the problem of navigation and touristic questionanswering in an integrated fashion using a shared dialogue context. We evaluated our system in comparison with Samsung S-Voice (which interfaces to Google navigation and Google search) with 17 users and found that users judged our system to be significantly more interesting to interact with and learn from. They also rated our system above Google search (with the Samsung S-Voice interface) for tourist information tasks.</p><p>3 0.9261297 <a title="345-lda-3" href="./acl-2013-Sorani_Kurdish_versus_Kurmanji_Kurdish%3A_An_Empirical_Comparison.html">327 acl-2013-Sorani Kurdish versus Kurmanji Kurdish: An Empirical Comparison</a></p>
<p>Author: Kyumars Sheykh Esmaili ; Shahin Salavati</p><p>Abstract: Resource scarcity along with diversity– both in dialect and script–are the two primary challenges in Kurdish language processing. In this paper we aim at addressing these two problems by (i) building a text corpus for Sorani and Kurmanji, the two main dialects of Kurdish, and (ii) highlighting some of the orthographic, phonological, and morphological differences between these two dialects from statistical and rule-based perspectives.</p><p>4 0.89752889 <a title="345-lda-4" href="./acl-2013-Reconstructing_an_Indo-European_Family_Tree_from_Non-native_English_Texts.html">299 acl-2013-Reconstructing an Indo-European Family Tree from Non-native English Texts</a></p>
<p>Author: Ryo Nagata ; Edward Whittaker</p><p>Abstract: Mother tongue interference is the phenomenon where linguistic systems of a mother tongue are transferred to another language. Although there has been plenty of work on mother tongue interference, very little is known about how strongly it is transferred to another language and about what relation there is across mother tongues. To address these questions, this paper explores and visualizes mother tongue interference preserved in English texts written by Indo-European language speakers. This paper further explores linguistic features that explain why certain relations are preserved in English writing, and which contribute to related tasks such as native language identification.</p><p>5 0.86887771 <a title="345-lda-5" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>Author: Ryan Georgi ; Fei Xia ; William D. Lewis</p><p>Abstract: As most of the world’s languages are under-resourced, projection algorithms offer an enticing way to bootstrap the resources available for one resourcepoor language from a resource-rich language by means of parallel text and word alignment. These algorithms, however, make the strong assumption that the language pairs share common structures and that the parse trees will resemble one another. This assumption is useful but often leads to errors in projection. In this paper, we will address this weakness by using trees created from instances of Interlinear Glossed Text (IGT) to discover patterns of divergence between the lan- guages. We will show that this method improves the performance of projection algorithms significantly in some languages by accounting for divergence between languages using only the partial supervision of a few corrected trees.</p><p>6 0.83516121 <a title="345-lda-6" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>same-paper 7 0.81725103 <a title="345-lda-7" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>8 0.7084403 <a title="345-lda-8" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>9 0.66243666 <a title="345-lda-9" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>10 0.62809438 <a title="345-lda-10" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>11 0.56431276 <a title="345-lda-11" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>12 0.56427306 <a title="345-lda-12" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>13 0.55659252 <a title="345-lda-13" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>14 0.55482763 <a title="345-lda-14" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>15 0.5482831 <a title="345-lda-15" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>16 0.54792637 <a title="345-lda-16" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>17 0.54553699 <a title="345-lda-17" href="./acl-2013-Question_Classification_Transfer.html">292 acl-2013-Question Classification Transfer</a></p>
<p>18 0.53389955 <a title="345-lda-18" href="./acl-2013-Multilingual_Affect_Polarity_and_Valence_Prediction_in_Metaphor-Rich_Texts.html">253 acl-2013-Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich Texts</a></p>
<p>19 0.53054309 <a title="345-lda-19" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>20 0.52262074 <a title="345-lda-20" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
