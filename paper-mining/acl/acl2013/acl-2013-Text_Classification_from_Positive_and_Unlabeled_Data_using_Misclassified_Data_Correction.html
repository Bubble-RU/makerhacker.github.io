<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-342" href="#">acl2013-342</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</h1>
<br/><p>Source: <a title="acl-2013-342-pdf" href="http://aclweb.org/anthology//P/P13/P13-2084.pdf">pdf</a></p><p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki ; Suguru Matsuyoshi</p><p>Abstract: This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</p><p>Reference: <a title="acl-2013-342-reference" href="../acl2013_reference/acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Text Classification from Positive and Unlabeled Data using Misclassified Data Correction Fumiyo Fukumoto and Yoshimi Suzuki and Suguru Matsuyoshi Interdisciplinary Graduate School of Medicine and Engineering University of Yamanashi, Kofu, 400-851 1, JAPAN { fukumoto ,ysuzuki ,sugurum}@yamanashi . [sent-1, score-0.062]
</p><p>2 j p  Abstract This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. [sent-3, score-1.192]
</p><p>3 We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). [sent-4, score-1.072]
</p><p>4 The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0. [sent-5, score-0.431]
</p><p>5 1 Introduction  Text classification using machine learning (ML) techniques with a small number oflabeled data has become more important with the rapid increase in volume of online documents. [sent-8, score-0.138]
</p><p>6 , semi-supervised learning, selftraining, and active learning have been proposed. [sent-11, score-0.047]
</p><p>7 proposed a semi-supervised learning approach called the Graph Mincut algorithm which uses a small number of positive and negative examples and assigns values to unlabeled examples in a way that optimizes consistency in a nearest-neighbor sense (Blum et al. [sent-13, score-0.638]
</p><p>8 described a method for self-training text categorization using the Web as the corpus (Cabrera et al. [sent-16, score-0.039]
</p><p>9 The method extracts unlabeled documents automatically from the Web and applies an enriched self-training for constructing the classifier. [sent-18, score-0.577]
</p><p>10 Several authors have attempted to improve classification accuracy using only positive and unlabeled data (Yu et al. [sent-19, score-0.595]
</p><p>11 proposed a method called biased-SVM that uses soft-margin SVM as the underlying classi-  fiers (Liu et al. [sent-23, score-0.07]
</p><p>12 Elkan and Noto proposed a theoretically justified method (Elkan and Noto, 2008). [sent-25, score-0.039]
</p><p>13 They showed that under the assumption that the labeled documents are selected randomly from the positive documents, a classifier trained on positive and unlabeled documents predicts probabilities that differ by only a constant factor from the true conditional probabilities of being positive. [sent-26, score-1.349]
</p><p>14 model a region containing most of the available positive data. [sent-30, score-0.214]
</p><p>15 However, these methods are sensitive to the parameter values, especially the small size of labeled data presents special difficulties in tuning the parameters to produce optimal results. [sent-31, score-0.048]
</p><p>16 In this paper, we propose a method for eliminating the need for manually collecting training documents, especially annotating negative training documents based on supervised ML techniques. [sent-32, score-0.639]
</p><p>17 Our goal is to eliminate the need for manually collecting training documents, and hopefully achieve classification accuracy from positive and  unlabeled data as high as that from labeled positive and labeled negative data. [sent-33, score-1.115]
</p><p>18 Like much previous work on semi-supervised ML, we apply SVM to the positive and unlabeled data, and add the classification results to the training data. [sent-34, score-0.66]
</p><p>19 The difference is that before adding the classification results, we applied the MisClassified data Detection and Correction (MCDC) technique to the results of SVM learning in order to improve classification accuracy obtained by the final classifiers. [sent-35, score-0.305]
</p><p>20 2 Framework of the System The MCDC method involves category error correction, i. [sent-36, score-0.175]
</p><p>21 , correction of misclassified candidates, while there are several strategies for automatically detecting lexical/syntactic errors in corpora (Abney et al. [sent-38, score-0.576]
</p><p>22 As error candidates, we focus on support vectors (SVs) extracted from the training documents by SVM. [sent-47, score-0.469]
</p><p>23 Thus, if some training document reduces  the overall performance of text classification because of an outlier, we can assume that the document is a SV. [sent-49, score-0.175]
</p><p>24 First, we randomly select documents from unlabeled data (U) where the number of documents is equal to that of the initial positive training documents (P1). [sent-51, score-1.381]
</p><p>25 We set these selected documents to negative training documents (N1), and apply SVM to learn classifiers. [sent-52, score-0.712]
</p><p>26 Next, we apply the MCDC technique to the results of SVM learning. [sent-53, score-0.038]
</p><p>27 For the result of correction (RC1)1 , we train SVM classifiers, and classify the remaining unlabeled data (U \ N1). [sent-54, score-0.493]
</p><p>28 For the resreumlt aoinf classification, we randomly select positive (CP1) and negative (CN1) documents classified by SVM and add to the SVM training data (RC1). [sent-55, score-0.753]
</p><p>29 We re-train SVM classifiers with the training documents, and apply the MCDC. [sent-56, score-0.1]
</p><p>30 The procedure is repeated until there are no unlabeled documents judged to be either positive or negative. [sent-57, score-0.815]
</p><p>31 Finally, the test data are classified using the final classifiers. [sent-58, score-0.064]
</p><p>32 In the following subsections, we present the MCDC procedure shown in Figure 2. [sent-59, score-0.034]
</p><p>33 It consists  of three steps: extraction of misclassified candidates, estimation of error reduction, and correction of misclassified candidates. [sent-60, score-0.93]
</p><p>34 Training data D  misclassified candidates  Final results  Figure 2: The MCDC procedure 2. [sent-62, score-0.414]
</p><p>35 1 Extraction of misclassified candidates Let D be a set of training documents and xk ∈ {x1, x2, · · ·, xm} be a SV of negative or positive {dxocume,nt ·s· ·o,b xtain}ed b by SSVVM of. [sent-63, score-1.198]
</p><p>36 Wneeg remove ∪km=1xk  fdroocmu tehen training ddo bcyu SmVeMnts. [sent-64, score-0.124]
</p><p>37 mTohev resulting D \ ∪km=1xk is used for training Naive Bayes (NB) (McCallum, 2001), leading to a classification model. [sent-66, score-0.175]
</p><p>38 This classification model is tested on each xk, and assigns a positive or negative label. [sent-67, score-0.437]
</p><p>39 If the label is different from that assigned to xk, we declare xk an error candidate. [sent-68, score-0.391]
</p><p>40 2 Estimation of error reduction We detect misclassified data from the extracted candidates by estimating error reduction. [sent-70, score-0.668]
</p><p>41 The estimation of error reduction is often used in active learning. [sent-71, score-0.233]
</p><p>42 The earliest work is the method of Roy and McCallum (Roy and McCallum, 2001). [sent-72, score-0.039]
</p><p>43 They proposed a method that directly optimizes expected future error by log-loss or 0-1 loss, using the entropy of the posterior class distribution on a sample of unlabeled documents. [sent-73, score-0.562]
</p><p>44 Specifically, we estimated future error rate by log-loss function. [sent-75, score-0.109]
</p><p>45 It uses the entropy of the posterior class distribution on a sample of the unlabeled documents. [sent-76, score-0.346]
</p><p>46 (1)  Eq (1) denotes the expected error of the learner. [sent-81, score-0.137]
</p><p>47 P(y | x) denotes the true distribution of output ycla |ss xe)s y ∈ eYs given inputs x. [sent-82, score-0.038]
</p><p>48 PˆD2∪(xk,yk)(y | x) shows the learner’s prediction, and D2 den(oyte |s xth)e s htraowinsing documents D except for the error candidates ∪lk=1xk. [sent-84, score-0.474]
</p><p>49 If the value of Eq (1) is sufficiently small, the learner’s prediction is close to the true output distribution. [sent-85, score-0.065]
</p><p>50 yM |o xr)e precisely, from the training documents D, a different training set consisting of positive and negative documents is created2. [sent-87, score-1.018]
</p><p>51 The learner then creates a new classifier from the training documents. [sent-88, score-0.119]
</p><p>52 The procedure is repeated m times3, and the final  class posterior for an instance is taken to be the unweighted average of the class posteriori for each of the classifiers. [sent-89, score-0.172]
</p><p>53 3  Correction of misclassified candidates  For each error candidate xk, we calculated the expected error of the learner, EPˆD2∪(xk,yk old) and EPˆD2∪(xk,yk new) by using Eq (1). [sent-91, score-0.626]
</p><p>54 Here, yk old refers to the original label assigned to xk, and yk new is the resulting category label estimated by NB classifiers. [sent-92, score-0.449]
</p><p>55 If the value of the latter is smaller than that of the former, we declare the document xk to be misclassified, i. [sent-93, score-0.24]
</p><p>56 , the label yk old is an error, and its true label is yk new. [sent-95, score-0.392]
</p><p>57 After eliminating unlabeled documents, we divided these into three. [sent-99, score-0.329]
</p><p>58 The data (20,000 documents) extracted from 20 Aug to 19  Sept is used as training data indicating positive and unlabeled documents. [sent-100, score-0.578]
</p><p>59 9 to create a wide range of scenarios, where δ refers to the ratio of documents from the positive class first selected from a fold as the positive set. [sent-103, score-0.761]
</p><p>60 The rest of the positive and negative documents are used as unlabeled data. [sent-104, score-0.865]
</p><p>61 We used categories assigned to more than 100 documents in the training data as it is necessary to examine a wide range of δ values. [sent-105, score-0.401]
</p><p>62 The data from 20 Sept to 19 Nov is used 2We set the number of negative documents extracted randomly from the unlabeled documents to the same number of positive training documents. [sent-107, score-1.255]
</p><p>63 as a test set X, to estimate true output distribution. [sent-109, score-0.038]
</p><p>64 We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-ofspeech tagger (Schmid, 1995), and stop word removal. [sent-111, score-0.105]
</p><p>65 We chose PEBL because the convergence procedure is very similar to our framework. [sent-119, score-0.064]
</p><p>66 We randomly selected 1,000 positive and 1,000 negative documents classified by SVM and added to the SVM training data in each iteration5. [sent-122, score-0.753]
</p><p>67 For biased-SVM, we used training data and classified test documents directly. [sent-123, score-0.396]
</p><p>68 We empirically selected values of two parameters, “c” (trade-off between training error and margin) and “j”, i. [sent-124, score-0.174]
</p><p>69 , cost (cost-factor, by which training errors on positive examples) that optimized the F-score obtained by classification of test documents. [sent-126, score-0.475]
</p><p>70 The positive training data in SVM are assigned  to the target category. [sent-127, score-0.313]
</p><p>71 The negative training data are the remaining data except for the documents that were assigned to the target category, i. [sent-128, score-0.479]
</p><p>72 , this is the ideal method as we used all the training data with positive/negative labeled documents. [sent-130, score-0.184]
</p><p>73 The number of positive training data in other three methods depends on the value of δ, and the rest of the positive and negative documents were used as unlabeled data. [sent-131, score-1.171]
</p><p>74 2  Text classification  Classification results for 88 categories are shown in Figure 3. [sent-133, score-0.145]
</p><p>75 As expected, the results obtained by SVM were the best among all δ values. [sent-135, score-0.047]
</p><p>76 However, this is the ideal method that requires 20,000 documents labeled positive/negative, while other methods including our 4http://svmlight. [sent-136, score-0.386]
</p><p>77 org 5We set the number of documents 476  up to  1,000. [sent-138, score-0.267]
</p><p>78 Delta Value  Figure 3: F-score against the value of δ method used only positive and unlabeled documents. [sent-139, score-0.551]
</p><p>79 Overall performance obtained by MCDC  was better for those obtained by PEBL and biasedSVM methods in all δ values, especially when the positive set was small, e. [sent-140, score-0.308]
</p><p>80 Table 1 shows the results obtained by each method with a δ value of 0. [sent-144, score-0.113]
</p><p>81 “Iter” in PEBL indicates the number of iterations until the number of negative documents is zero in the convergence procedure. [sent-148, score-0.441]
</p><p>82 Similarly, “Iter” in the MCDC indicates the number of iterations until no unlabeled documents are judged to be either positive or negative. [sent-149, score-0.812]
</p><p>83 As can be seen clearly from Table 1, the results with MCDC were better than those obtained by PEBL in each level of the hierarchy. [sent-150, score-0.047]
</p><p>84 6 F7902  Table 2: Miss-classified data correction results  ter than those of biased-SVM except for the fourth level, “C151 1”(Annual results). [sent-155, score-0.222]
</p><p>85 3 Correction of misclassified candidates Our goal is to achieve classification accuracy from only positive documents and unlabeled data as high as that from labeled positive and negative data. [sent-160, score-1.617]
</p><p>86 We thus applied a miss-classified data detection and correction technique for the classification results obtained by SVM. [sent-161, score-0.462]
</p><p>87 Table 2 shows detection and correction  performance against all categories. [sent-163, score-0.267]
</p><p>88 “SV” shows the total number of SVs in 88 categories in all iterations. [sent-164, score-0.035]
</p><p>89 “Ec” refers to the total number of extracted error candidates. [sent-165, score-0.171]
</p><p>90 “Err” denotes the number of documents classified incorrectly by SVM and added to the training data, i. [sent-166, score-0.396]
</p><p>91 , the number of documents that should be assigned correctly by the correction procedure. [sent-168, score-0.523]
</p><p>92 Table 2 shows that precision was better than recall with both δ values, as the precision obtained by γ value = 0. [sent-170, score-0.074]
</p><p>93 These observations indicated that the error candidates extracted by our method were appropriately 477  corrected. [sent-175, score-0.274]
</p><p>94 In contrast, there were still other documents that were miss-classified but not extracted as error candidates. [sent-176, score-0.404]
</p><p>95 We extracted error candidates using the results of SVM and NB classifiers. [sent-177, score-0.235]
</p><p>96 Ensemble of other techniques such as boosting and kNN for further efficacy gains seems promising to  try with our method. [sent-178, score-0.08]
</p><p>97 4  Conclusion  The research described in this paper involved text classification using positive and unlabeled data. [sent-179, score-0.595]
</p><p>98 Miss-classified data detection and correction technique was incorporated in the existing classification technique. [sent-180, score-0.415]
</p><p>99 The results using the 1996 Reuters corpora showed that the method was comparable to the current state-of-the-art biased-SVM method as the F-score obtained by our method was 0. [sent-181, score-0.164]
</p><p>100 Future work will include feature reduction and investigation of other classification algorithms to obtain further advantages in efficiency and efficacy in manipulating real-world large corpora. [sent-184, score-0.196]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mcdc', 0.418), ('misclassified', 0.282), ('pebl', 0.279), ('unlabeled', 0.271), ('documents', 0.267), ('correction', 0.222), ('positive', 0.214), ('svm', 0.212), ('xk', 0.159), ('elkan', 0.142), ('svs', 0.123), ('yk', 0.122), ('reuters', 0.114), ('negative', 0.113), ('classification', 0.11), ('error', 0.109), ('cabrera', 0.105), ('noto', 0.105), ('candidates', 0.098), ('eq', 0.083), ('dickinson', 0.08), ('blum', 0.076), ('akoglu', 0.07), ('biasedsvm', 0.07), ('sept', 0.07), ('yamanashi', 0.07), ('training', 0.065), ('classified', 0.064), ('fukumoto', 0.062), ('aug', 0.062), ('nb', 0.06), ('eliminating', 0.058), ('roy', 0.058), ('nov', 0.057), ('iter', 0.057), ('anomaly', 0.057), ('ep', 0.055), ('learner', 0.054), ('declare', 0.054), ('abney', 0.054), ('boyd', 0.051), ('sigdat', 0.051), ('ml', 0.05), ('labeled', 0.048), ('obtained', 0.047), ('active', 0.047), ('km', 0.045), ('detection', 0.045), ('efficacy', 0.044), ('posterior', 0.043), ('reduction', 0.042), ('sv', 0.042), ('old', 0.04), ('optimizes', 0.04), ('method', 0.039), ('yu', 0.039), ('errors', 0.039), ('true', 0.038), ('technique', 0.038), ('mccallum', 0.037), ('boosting', 0.036), ('label', 0.035), ('categorical', 0.035), ('ho', 0.035), ('classifiers', 0.035), ('estimation', 0.035), ('liu', 0.035), ('categories', 0.035), ('procedure', 0.034), ('refers', 0.034), ('assigned', 0.034), ('detecting', 0.033), ('collecting', 0.032), ('class', 0.032), ('ideal', 0.032), ('iterations', 0.031), ('gomez', 0.031), ('fumiyo', 0.031), ('posteriori', 0.031), ('wit', 0.031), ('ddo', 0.031), ('fiers', 0.031), ('randomly', 0.03), ('convergence', 0.03), ('judged', 0.029), ('yoshimi', 0.028), ('oflabeled', 0.028), ('err', 0.028), ('anlp', 0.028), ('tehen', 0.028), ('interdisciplinary', 0.028), ('expected', 0.028), ('extracted', 0.028), ('category', 0.027), ('value', 0.027), ('consisting', 0.027), ('prec', 0.027), ('tsai', 0.027), ('oft', 0.027), ('mincut', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="342-tfidf-1" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki ; Suguru Matsuyoshi</p><p>Abstract: This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</p><p>2 0.18548104 <a title="342-tfidf-2" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>3 0.12147482 <a title="342-tfidf-3" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>4 0.11440659 <a title="342-tfidf-4" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>5 0.10111313 <a title="342-tfidf-5" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>6 0.094120927 <a title="342-tfidf-6" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>7 0.085036226 <a title="342-tfidf-7" href="./acl-2013-A_Learner_Corpus-based_Approach_to_Verb_Suggestion_for_ESL.html">8 acl-2013-A Learner Corpus-based Approach to Verb Suggestion for ESL</a></p>
<p>8 0.083389223 <a title="342-tfidf-8" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>9 0.081840426 <a title="342-tfidf-9" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>10 0.081106618 <a title="342-tfidf-10" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>11 0.078522906 <a title="342-tfidf-11" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>12 0.073473088 <a title="342-tfidf-12" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>13 0.066920877 <a title="342-tfidf-13" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>14 0.063053988 <a title="342-tfidf-14" href="./acl-2013-Improving_Chinese_Word_Segmentation_on_Micro-blog_Using_Rich_Punctuations.html">193 acl-2013-Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations</a></p>
<p>15 0.062740169 <a title="342-tfidf-15" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>16 0.059772197 <a title="342-tfidf-16" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>17 0.058039173 <a title="342-tfidf-17" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>18 0.057622787 <a title="342-tfidf-18" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>19 0.057088021 <a title="342-tfidf-19" href="./acl-2013-A_Comparison_of_Techniques_to_Automatically_Identify_Complex_Words..html">3 acl-2013-A Comparison of Techniques to Automatically Identify Complex Words.</a></p>
<p>20 0.056871593 <a title="342-tfidf-20" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.15), (1, 0.058), (2, -0.027), (3, 0.023), (4, 0.063), (5, -0.061), (6, 0.016), (7, -0.005), (8, -0.049), (9, 0.022), (10, 0.07), (11, 0.002), (12, -0.009), (13, 0.011), (14, -0.07), (15, 0.036), (16, -0.059), (17, 0.08), (18, -0.044), (19, 0.008), (20, 0.08), (21, 0.061), (22, 0.021), (23, 0.024), (24, 0.042), (25, 0.051), (26, -0.013), (27, -0.055), (28, -0.096), (29, -0.016), (30, -0.097), (31, 0.008), (32, -0.067), (33, 0.18), (34, 0.011), (35, 0.032), (36, -0.148), (37, -0.012), (38, 0.05), (39, -0.053), (40, -0.042), (41, -0.027), (42, 0.044), (43, 0.067), (44, 0.064), (45, 0.058), (46, 0.041), (47, -0.027), (48, 0.042), (49, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96659988 <a title="342-lsi-1" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki ; Suguru Matsuyoshi</p><p>Abstract: This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</p><p>2 0.87788975 <a title="342-lsi-2" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>3 0.68719167 <a title="342-lsi-3" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>Author: Xiaojun Wan</p><p>Abstract: The task of review rating prediction can be well addressed by using regression algorithms if there is a reliable training set of reviews with human ratings. In this paper, we aim to investigate a more challenging task of crosslanguage review rating prediction, which makes use of only rated reviews in a source language (e.g. English) to predict the rating scores of unrated reviews in a target language (e.g. German). We propose a new coregression algorithm to address this task by leveraging unlabeled reviews. Evaluation results on several datasets show that our proposed co-regression algorithm can consistently improve the prediction results. 1</p><p>4 0.68571615 <a title="342-lsi-4" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>5 0.60928798 <a title="342-lsi-5" href="./acl-2013-Part-of-speech_tagging_with_antagonistic_adversaries.html">277 acl-2013-Part-of-speech tagging with antagonistic adversaries</a></p>
<p>Author: Anders Sgaard</p><p>Abstract: Supervised NLP tools and on-line services are often used on data that is very different from the manually annotated data used during development. The performance loss observed in such cross-domain applications is often attributed to covariate shifts, with out-of-vocabulary effects as an important subclass. Many discriminative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features. Regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts. We present a new perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilin- gual cross-domain part-of-speech tagging datasets. While previous approaches do not improve on our supervised baseline, our approach is better across the board with an average 4% error reduction.</p><p>6 0.59779179 <a title="342-lsi-6" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>7 0.58281958 <a title="342-lsi-7" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>8 0.57913768 <a title="342-lsi-8" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>9 0.57690835 <a title="342-lsi-9" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>10 0.54485166 <a title="342-lsi-10" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>11 0.51879102 <a title="342-lsi-11" href="./acl-2013-A_Novel_Classifier_Based_on_Quantum_Computation.html">14 acl-2013-A Novel Classifier Based on Quantum Computation</a></p>
<p>12 0.48907176 <a title="342-lsi-12" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>13 0.48861855 <a title="342-lsi-13" href="./acl-2013-Transfer_Learning_Based_Cross-lingual_Knowledge_Extraction_for_Wikipedia.html">356 acl-2013-Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia</a></p>
<p>14 0.47046563 <a title="342-lsi-14" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>15 0.46890351 <a title="342-lsi-15" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>16 0.46638328 <a title="342-lsi-16" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>17 0.46499974 <a title="342-lsi-17" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>18 0.46468925 <a title="342-lsi-18" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>19 0.46214843 <a title="342-lsi-19" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>20 0.45765883 <a title="342-lsi-20" href="./acl-2013-Automatic_detection_of_deception_in_child-produced_speech_using_syntactic_complexity_features.html">63 acl-2013-Automatic detection of deception in child-produced speech using syntactic complexity features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.061), (4, 0.029), (6, 0.037), (11, 0.052), (24, 0.061), (26, 0.112), (35, 0.063), (42, 0.031), (48, 0.054), (50, 0.278), (70, 0.028), (88, 0.026), (90, 0.017), (95, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74342275 <a title="342-lda-1" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki ; Suguru Matsuyoshi</p><p>Abstract: This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</p><p>2 0.55959135 <a title="342-lda-2" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>3 0.55409676 <a title="342-lda-3" href="./acl-2013-Mapping_Source_to_Target_Strings_without_Alignment_by_Analogical_Learning%3A_A_Case_Study_with_Transliteration.html">236 acl-2013-Mapping Source to Target Strings without Alignment by Analogical Learning: A Case Study with Transliteration</a></p>
<p>Author: Phillippe Langlais</p><p>Abstract: Analogical learning over strings is a holistic model that has been investigated by a few authors as a means to map forms of a source language to forms of a target language. In this study, we revisit this learning paradigm and apply it to the transliteration task. We show that alone, it performs worse than a statistical phrase-based machine translation engine, but the combination of both approaches outperforms each one taken separately, demonstrating the usefulness of the information captured by a so-called formal analogy.</p><p>4 0.5473538 <a title="342-lda-4" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>Author: Young-Bum Kim ; Benjamin Snyder</p><p>Abstract: In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic distinctions. On a three-way classification task between vowels, nasals, and nonnasal consonants, our model yields unsu- pervised accuracy of 89% across the same set of languages.</p><p>5 0.54509199 <a title="342-lda-5" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>6 0.54455054 <a title="342-lda-6" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>7 0.5442434 <a title="342-lda-7" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>8 0.54348797 <a title="342-lda-8" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>9 0.54211676 <a title="342-lda-9" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>10 0.54195958 <a title="342-lda-10" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>11 0.54175705 <a title="342-lda-11" href="./acl-2013-Crawling_microblogging_services_to_gather_language-classified_URLs._Workflow_and_case_study.html">95 acl-2013-Crawling microblogging services to gather language-classified URLs. Workflow and case study</a></p>
<p>12 0.53952926 <a title="342-lda-12" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>13 0.53908646 <a title="342-lda-13" href="./acl-2013-SORT%3A_An_Interactive_Source-Rewriting_Tool_for_Improved_Translation.html">305 acl-2013-SORT: An Interactive Source-Rewriting Tool for Improved Translation</a></p>
<p>14 0.53884125 <a title="342-lda-14" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>15 0.53811848 <a title="342-lda-15" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>16 0.53586686 <a title="342-lda-16" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>17 0.53443587 <a title="342-lda-17" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>18 0.53105962 <a title="342-lda-18" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>19 0.53102356 <a title="342-lda-19" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>20 0.53095078 <a title="342-lda-20" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
