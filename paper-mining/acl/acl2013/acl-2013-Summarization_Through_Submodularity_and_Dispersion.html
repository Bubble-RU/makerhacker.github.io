<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>333 acl-2013-Summarization Through Submodularity and Dispersion</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-333" href="#">acl2013-333</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>333 acl-2013-Summarization Through Submodularity and Dispersion</h1>
<br/><p>Source: <a title="acl-2013-333-pdf" href="http://aclweb.org/anthology//P/P13/P13-1100.pdf">pdf</a></p><p>Author: Anirban Dasgupta ; Ravi Kumar ; Sujith Ravi</p><p>Abstract: We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</p><p>Reference: <a title="acl-2013-333-reference" href="../acl2013_reference/acl-2013-Summarization_Through_Submodularity_and_Dispersion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). [sent-5, score-0.627]
</p><p>2 In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. [sent-6, score-0.772]
</p><p>3 We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. [sent-7, score-0.843]
</p><p>4 We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on  submodularity. [sent-8, score-0.22]
</p><p>5 Thanks to the omnipresent information overload facing all of us, the importance of summarization is gaining; semiautomatically summarized content is increasingly becoming user-facing: many newspapers equip editors with automated tools to aid them in choosing a subset of user comments to show. [sent-11, score-0.518]
</p><p>6 Each domain throws up its own set of idiosyncrasies and challenges for the summarization task. [sent-13, score-0.339]
</p><p>7 While there have been many approaches to automatic summarization (see Section 2), our work is directly inspired by the recent elegant framework of (Lin and Bilmes, 2011). [sent-20, score-0.367]
</p><p>8 They employed the powerful theory of submodular functions for summarization: submodularity embodies the “diminishing returns” property and hence is a natural vocabulary to express the summarization desider-  ata. [sent-21, score-0.784]
</p><p>9 ) is captured as a submodular function and the objective is to maximize their sum. [sent-23, score-0.349]
</p><p>10 For example, a natural constraint on the summary is that the sum or the minimum ofpairwise dissimilarities between sentences chosen in the summary should be maximized; this, unfortunately, is not a submodular function. [sent-28, score-0.557]
</p><p>11 Ac s2s0o1ci3a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 1014–102 , wise dissimilarities in the summary as dispersion functions. [sent-31, score-0.746]
</p><p>12 Our focus in this work is on significantly furthering the submodularity-based sum-  marization framework to incorporate such dispersion functions. [sent-32, score-0.604]
</p><p>13 We propose a very general graph-based summarization framework that combines a submodular function with a non-submodular dispersion function. [sent-33, score-1.233]
</p><p>14 We consider three natural dispersion functions on the sentences in a summary: sum of all-pair sentence dissimilarities, the weight of the minimum spanning tree on the sentences, and the minimum of all-pair sentence dissimilarities. [sent-34, score-0.844]
</p><p>15 We then show that a greedy algorithm can obtain approximately optimal summary in each of the three cases; the proof exploits some nice combinatorial properties satisfied by the three dispersion functions. [sent-36, score-0.807]
</p><p>16 We then conduct experiments on two corpora: the DUC 2004 corpus and a corpus of user comments on news articles. [sent-37, score-0.189]
</p><p>17 On DUC 2004, we obtain performance that matches (Lin and Bilmes, 2011), without any serious parameter tuning; note that their framework does not have the dispersion function. [sent-38, score-0.604]
</p><p>18 On the comment corpus, we outperform  their method, demonstrating that value of dispersion functions. [sent-39, score-0.679]
</p><p>19 2  Related Work  Automatic summarization is a well-studied problem in the literature. [sent-41, score-0.339]
</p><p>20 Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001 ; Takamura and Okumura, 2009; Shen and Li, 2010). [sent-42, score-0.339]
</p><p>21 Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum ´e and Marcu, 2006), microblog summarization (Sharifi et al. [sent-43, score-0.678]
</p><p>22 , 2010), event summarization (Filatova, 2004), and others (Riedhammer et al. [sent-44, score-0.339]
</p><p>23 Graph-based methods have been used for summarization (Ganesan et al. [sent-48, score-0.339]
</p><p>24 For a detailed  survey on existing automatic summarization techniques and other related topics, see (Kim et al. [sent-50, score-0.339]
</p><p>25 3  Framework  In this section we present the summarization framework. [sent-52, score-0.339]
</p><p>26 We start by describing a generic objective function that can be widely applied to several summarization scenarios. [sent-53, score-0.456]
</p><p>27 This objective function is the sum of a monotone submodular coverage function and a non-submodular dispersion function. [sent-54, score-1.087]
</p><p>28 We then describe a simple greedy algorithm for optimizing this objective function with provable approximation guarantees for three natural dispersion functions. [sent-55, score-0.95]
</p><p>29 Depending on the summarization application, C can refer to the set of documents (e. [sent-58, score-0.369]
</p><p>30 , user-generated content), it is a collection of comments associated with a news article  or a blog post, etc. [sent-62, score-0.232]
</p><p>31 : Othuer sum measure hs(S, T) = P{u,v}∈P d(u, v), the spanning tree measure ht (SP, T{)u given by the cost of the minimum spanning trePe of the set S∪T, and tohfe t hmei nm measure hm(S, T) = min{u,v}∈P d(u, v). [sent-80, score-0.485]
</p><p>32 B 1/ute )ifδ > 0, since the dispersion function h(·) is not submodular, eth teh ec odmispbeirnseiod objective f(·) i ss nnoott ssuubbmmoodduullaarr, as ew celol. [sent-93, score-0.693]
</p><p>33 m Despite this, we s(h·)ow is th noatt a simple greedy algorithm achieves a provable approximation factor for (1). [sent-94, score-0.221]
</p><p>34 This is possible due to some nice structural properties of the dispersion functions we consider. [sent-95, score-0.648]
</p><p>35 Algorithm 2 Greedy algorithm, parametrized by the dispersion function h; here, U, k, g, δ are fixed. [sent-96, score-0.66]
</p><p>36 3 Analysis In this section we obtain a provable approximation for the greedy algorithm. [sent-101, score-0.19]
</p><p>37 First, we show that a greedy choice is well-behaved with respect to the dispersion function h· (·). [sent-102, score-0.719]
</p><p>38 ∅,  =  =  We next show that the tree created by the greedy algorithm for h = ht is not far from the optimum. [sent-128, score-0.425]
</p><p>39 PThe proof follows by noting that we get a spanning tree by connecting each ui to its closest pPoint in Si−1. [sent-136, score-0.215]
</p><p>40 The cost of this spanning tree is P2≤j≤kd(uj,Sj−1) and this tree is also the resPul2t ≤ojf≤ thke greedy algorithm run in an online fashPion on the input sequence {u1, . [sent-137, score-0.28]
</p><p>41 For hs and ht, we run Algorithm 1 using a new dispersion function h0, which is a slightly modified version of h. [sent-147, score-0.712]
</p><p>42 For ht, using the above argument of submodularity and monotonicity of g, and the result from  Lemma 1(ii), we have X g(Si∪ u) − g(Si) + δd(u,Si) u∈XO\Si  ≥  ≥ ≥  g(O) g(Si) + δ(ht (O)/2 ht (Si)) (g(O) + δht(O)/2) (g(Si) + δht (Si)) −  −  −  f(O)/2  −  (g(Si)  + δht(Si)). [sent-162, score-0.433]
</p><p>43 Also, ht (Si) ≤ 2 smt(Si) since this is a metric space. [sent-163, score-0.265]
</p><p>44 Using t2he s monotonicity of the Steiner tree cost, smt(Si) ≤ smt(Sk) ≤ ht (Sk). [sent-164, score-0.365]
</p><p>45 We do not use this algorithm in our experiments, as it is oblivious of the actual dispersion functions used. [sent-182, score-0.679]
</p><p>46 We then use this representation to 1017  generate a graph and instantiate our summarization objective function with specific components that capture the desiderata of a given summarization task. [sent-189, score-0.86]
</p><p>47 1 Structured representation for sentences In order to instantiate the summarization graph (nodes and edges), we first need to model each sentence (in multi-document summarization) or comment (i. [sent-191, score-0.508]
</p><p>48 Sentences have been typically modeled using standard ngrams (unigrams or bigrams) in  previous summarization work. [sent-194, score-0.364]
</p><p>49 Furthermore, the edge weights s(u, v) represent pairwise similarity between sentences or comments (e. [sent-202, score-0.238]
</p><p>50 The edge weights are then used to define the inter-sentence distance metric d(u, v) for the different dispersion functions. [sent-205, score-0.602]
</p><p>51 Using the syntactic structure along with semantic similarity helps us identify useful (valid) nuggets of information within comments (or documents), avoid redundancies, and identify similar views in a semantic space. [sent-216, score-0.216]
</p><p>52 , 50 different summarization tasks) with 10 documents per cluster on average. [sent-253, score-0.369]
</p><p>53 We extracted a set of news articles and corresponding user comments from Yahoo! [sent-256, score-0.189]
</p><p>54 2  Evaluation  For each summarization task, we compare the system output (i. [sent-261, score-0.339]
</p><p>55 We use the following evaluation settings in our experiments for each summarization task: (1) For multi-document summarization, we compute the ROUGE-15 scores that was the main evaluation criterion for DUC 2004 evaluations. [sent-265, score-0.339]
</p><p>56 2 (2) For comment summarization, the collection of user comments associated with a given article is typically much larger. [sent-274, score-0.301]
</p><p>57 Hence for this task, we use a slightly different evaluation criterion that is inspired from the DUC 2005-2007 summarization evaluation tasks. [sent-276, score-0.339]
</p><p>58 We then run our summarization algorithm on the instantiated graph to produce a summary for each news article. [sent-280, score-0.524]
</p><p>59 In addition, each news article and corresponding set of comments were presented to three human annotators. [sent-281, score-0.232]
</p><p>60 They were asked to select a subset of comments (at most 20 comments) that best represented a summary capturing the most popular  as well as diverse set of views and opinions expressed by different users that are relevant to the given news article. [sent-282, score-0.297]
</p><p>61 We then compare the automatically generated comment summaries against the human-generated summaries and compute the ROUGE-1 and ROUGE-2 scores. [sent-283, score-0.209]
</p><p>62 6 This summarization task is particularly hard for even human annotators since user-generated comments are typically noisy and there are several hundreds of comments per article. [sent-284, score-0.649]
</p><p>63 This shows that even though this is a new type of summarization task, humans tend to generate more consistent summaries and hence their annotations are reliable for evaluation purposes as in multi-document sum-  marization. [sent-289, score-0.421]
</p><p>64 5 -t 0 -d -l 150  -a -n  2 -x  -m  -2 4 -u -c 95  1019  of our system that approximates the submodular objective function proposed by (Lin and Bilmes, 2011). [sent-296, score-0.349]
</p><p>65 7 As shown in the results, our best system8 which uses the hs dispersion function achieves a better ROUGE-1 F-score than all other systems. [sent-297, score-0.712]
</p><p>66 (2) We observe that the hm and ht dispersion functions produce slightly lower scores than hs, which may be a characteristic of this particular summarization task. [sent-298, score-1.439]
</p><p>67 We believe that the empirical results achieved by different dispersion functions depend on the nature of the summarization tasks and there  are task settings under which hm or ht perform better than hs. [sent-299, score-1.439]
</p><p>68 For example, we show later how using the ht dispersion function yields the best performance on the comments summarization task. [sent-300, score-1.42]
</p><p>69 (3) We also analyze the contributions of individual components of the new objective function towards summarization performance by selectively setting certain parameters to 0. [sent-302, score-0.48]
</p><p>70 We clearly see that each component (popularity, cluster contribution, dispersion) individually yields a reasonable summarization performance but the best result is achieved by the combined system (row 5 in the table). [sent-304, score-0.366]
</p><p>71 We also contrast the performance of the full system with and without the dispersion component (row 4 versus row 5). [sent-305, score-0.601]
</p><p>72 The results show that optimizing for dispersion yields an improvement in summarization performance. [sent-306, score-0.942]
</p><p>73 (4) To understand the effect of utilizing syntactic structure and semantic similarity for constructing  the summarization graph, we ran the experiments using just the unigrams and bigrams; we obtained a ROUGE-1 F-score the syntactic  structure  of 37. [sent-307, score-0.371]
</p><p>74 This is because their system was tuned for the particular summarization task using the DUC 2003 corpus. [sent-311, score-0.339]
</p><p>75 On the other hand, even without any parameter tuning our method yields good performance, as evidenced by results on the two different summarization tasks. [sent-312, score-0.366]
</p><p>76 8For the full system, we weight certain parameters pertaining to cluster contributions and dispersion higher (α = β = δ = 5) compared to the rest of the objective function (λ = 1). [sent-314, score-0.693]
</p><p>77 If the maximum number of sentences/comments chosen were k, we brought both hs and ht to the same approximate scale as hm by dividing hs by k(k − 1)/2 and ht by k − 1. [sent-316, score-0.873]
</p><p>78 However, while the structured represen-  tation is beneficial, we observed that dispersion (and other individual components) contribute similar performance gains even when using ngrams alone. [sent-319, score-0.625]
</p><p>79 So the improvements obtained from the structured representation and dispersion are complementary. [sent-320, score-0.6]
</p><p>80 , we first pick the longest comment (comprising the most number of characters), then the next longest comment and so on, to create an ordered set of comments. [sent-326, score-0.206]
</p><p>81 The intuition behind this baseline is  that longer comments contain more content and possibly cover more topics than short ones. [sent-327, score-0.21]
</p><p>82 From the table, we observe that the new system (using either dispersion function) outperforms the baseline by a huge margin (+44% relative improvement in ROUGE-1 and much bigger improvements in ROUGE-2 scores). [sent-328, score-0.576]
</p><p>83 Our system models sentences using the syntactic structure and semantics and jointly optimizes for multiple summarization criteria (including dispersion) which helps weed out the noise and identify relevant, useful information within the comments thereby producing better quality summaries. [sent-330, score-0.519]
</p><p>84 (2) Unlike the multi-document summarization, here we observe that the ht dispersion function yields the best empirical performance for this task. [sent-334, score-0.926]
</p><p>85 This observation supports our claim that the  choice of the specific dispersion function depends 1020  TabOhle=j2c:sti,Pvwe(rfSuo)αwnrc=(m Stia)αoβn,c=α e,oβλm w,=p λito,hδ n e=d >nitf0 sferRnOtUp3 a758Gr. [sent-335, score-0.634]
</p><p>86 on the summarization task and that the dispersion functions proposed in this paper have a wider variety of use cases. [sent-337, score-0.987]
</p><p>87 (3) Results showing contributions from individual components of the new summarization objective function are listed in Table 4. [sent-338, score-0.48]
</p><p>88 The table also shows that incorporating dispersion into the  objective function yields an improvement in summarization quality (row 4 versus row 5). [sent-341, score-1.084]
</p><p>89 6  Conclusions  We introduced a new general-purpose graph-based summarization framework that combines a sub-  modular coverage function with a non-submodular dispersion function. [sent-348, score-1.03]
</p><p>90 We presented three natural dispersion functions that represent three different ways of ensuring non-redundancy (using sentence dissimilarities) for summarization and proved that a simple greedy algorithm can obtain an approximately optimal summary in all these cases. [sent-349, score-1.182]
</p><p>91 Experiments on two different summarization tasks show that our algorithm outperforms algorithms that rely only on submodularity. [sent-350, score-0.37]
</p><p>92 Finally, we demonstrated that using a structured representation to model sentences in the graph improves summarization quality. [sent-351, score-0.429]
</p><p>93 Firstly, it would interesting to see if dispersion offers similar improvements over a tuned version of the submodular framework ofLin and Bilmes (201 1). [sent-353, score-0.836]
</p><p>94 In a very recent work, Lin and Bilmes (2012) demonstrate a further improvement in performance for document summarization by using mixtures of  submodular shells. [sent-354, score-0.6]
</p><p>95 This is an interesting extension of their previous submodular framework and while the new formulation permits more complex functions, the resulting function is still submodular and hence can be combined with the dispersion measures proposed in this paper. [sent-355, score-1.155]
</p><p>96 A different body of work uses determinantal point processes (DPP) to model subset selection problems and adapt it for document summarization (Kulesza and Taskar, 2011). [sent-356, score-0.396]
</p><p>97 Opinosis: A graph based approach to abstractive summarization of highly redundant opinions. [sent-404, score-0.405]
</p><p>98 Learning mixtures of submodular shells with application to document summarization. [sent-429, score-0.261]
</p><p>99 An analysis of approximations for maximizing submodular set functions I. [sent-445, score-0.304]
</p><p>100 Text summarization model based on maximum coverage problem and its variant. [sent-486, score-0.368]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dispersion', 0.576), ('summarization', 0.339), ('ht', 0.265), ('submodular', 0.232), ('si', 0.216), ('hm', 0.187), ('sk', 0.173), ('comments', 0.155), ('duc', 0.116), ('submodularity', 0.112), ('comment', 0.103), ('dissimilarities', 0.091), ('bilmes', 0.09), ('ui', 0.086), ('tennis', 0.086), ('greedy', 0.085), ('summary', 0.079), ('hs', 0.078), ('steiner', 0.076), ('functions', 0.072), ('adore', 0.069), ('uj', 0.06), ('og', 0.06), ('objective', 0.059), ('function', 0.058), ('monotonicity', 0.056), ('xo', 0.056), ('approximation', 0.053), ('summaries', 0.053), ('borodin', 0.052), ('ganesan', 0.052), ('provable', 0.052), ('monotone', 0.051), ('pu', 0.05), ('bi', 0.05), ('spanning', 0.049), ('oh', 0.046), ('dpps', 0.046), ('tree', 0.044), ('article', 0.043), ('cov', 0.042), ('smt', 0.042), ('graph', 0.041), ('lemma', 0.038), ('lin', 0.037), ('let', 0.036), ('proof', 0.036), ('guarantees', 0.036), ('curel', 0.034), ('diversification', 0.034), ('halld', 0.034), ('imase', 0.034), ('kavita', 0.034), ('yatani', 0.034), ('news', 0.034), ('iii', 0.033), ('rel', 0.033), ('similarity', 0.032), ('wn', 0.031), ('algorithm', 0.031), ('sg', 0.031), ('cover', 0.031), ('nemhauser', 0.03), ('riedhammer', 0.03), ('hyun', 0.03), ('sharifi', 0.03), ('chandra', 0.03), ('rouge', 0.03), ('documents', 0.03), ('dependency', 0.03), ('chengxiang', 0.029), ('hence', 0.029), ('coverage', 0.029), ('ravi', 0.029), ('views', 0.029), ('document', 0.029), ('framework', 0.028), ('determinantal', 0.028), ('cost', 0.027), ('yields', 0.027), ('minimum', 0.027), ('summarizing', 0.027), ('parametrized', 0.026), ('edge', 0.026), ('sentences', 0.025), ('ngrams', 0.025), ('keyphrase', 0.025), ('abstractive', 0.025), ('sd', 0.025), ('sh', 0.025), ('row', 0.025), ('redundancy', 0.025), ('components', 0.024), ('wordnet', 0.024), ('uai', 0.024), ('mountain', 0.024), ('structured', 0.024), ('sum', 0.024), ('sj', 0.024), ('content', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="333-tfidf-1" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>Author: Anirban Dasgupta ; Ravi Kumar ; Sujith Ravi</p><p>Abstract: We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</p><p>2 0.2232631 <a title="333-tfidf-2" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>Author: Hajime Morita ; Ryohei Sasano ; Hiroya Takamura ; Manabu Okumura</p><p>Abstract: This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 21 (1 − e−1). Our experiments with the NTC(1IR − −A eCLIA test collections show that our approach outperforms a state-of-the-art algorithm.</p><p>3 0.15172468 <a title="333-tfidf-3" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>Author: Miguel Almeida ; Andre Martins</p><p>Abstract: We present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear programming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers.</p><p>4 0.15125659 <a title="333-tfidf-4" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>5 0.12271941 <a title="333-tfidf-5" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>6 0.11984926 <a title="333-tfidf-6" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>7 0.11617365 <a title="333-tfidf-7" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>8 0.11506508 <a title="333-tfidf-8" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>9 0.10222584 <a title="333-tfidf-9" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>10 0.10002912 <a title="333-tfidf-10" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>11 0.096607625 <a title="333-tfidf-11" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>12 0.092073701 <a title="333-tfidf-12" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>13 0.089544214 <a title="333-tfidf-13" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>14 0.074875012 <a title="333-tfidf-14" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>15 0.069156758 <a title="333-tfidf-15" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>16 0.065702006 <a title="333-tfidf-16" href="./acl-2013-Joint_Modeling_of_News_Reader%C3%A2%E2%80%A2%C5%BDs_and_Comment_Writer%C3%A2%E2%80%A2%C5%BDs_Emotions.html">209 acl-2013-Joint Modeling of News Readerâ•Žs and Comment Writerâ•Žs Emotions</a></p>
<p>17 0.061904285 <a title="333-tfidf-17" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>18 0.060555048 <a title="333-tfidf-18" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>19 0.057657216 <a title="333-tfidf-19" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>20 0.056038644 <a title="333-tfidf-20" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.162), (1, 0.029), (2, 0.008), (3, -0.057), (4, -0.0), (5, 0.02), (6, 0.119), (7, -0.007), (8, -0.174), (9, -0.097), (10, -0.024), (11, 0.048), (12, -0.122), (13, -0.015), (14, -0.063), (15, 0.129), (16, 0.148), (17, -0.098), (18, 0.016), (19, 0.075), (20, -0.009), (21, -0.097), (22, 0.015), (23, -0.026), (24, -0.018), (25, -0.034), (26, 0.029), (27, 0.023), (28, 0.028), (29, -0.01), (30, -0.043), (31, 0.055), (32, -0.013), (33, 0.019), (34, -0.002), (35, 0.015), (36, -0.028), (37, -0.039), (38, 0.027), (39, -0.045), (40, -0.052), (41, -0.002), (42, 0.07), (43, 0.022), (44, -0.024), (45, -0.041), (46, -0.059), (47, -0.003), (48, 0.051), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94698709 <a title="333-lsi-1" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>Author: Anirban Dasgupta ; Ravi Kumar ; Sujith Ravi</p><p>Abstract: We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</p><p>2 0.85737628 <a title="333-lsi-2" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.</p><p>3 0.8431865 <a title="333-lsi-3" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>Author: Hajime Morita ; Ryohei Sasano ; Hiroya Takamura ; Manabu Okumura</p><p>Abstract: This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 21 (1 − e−1). Our experiments with the NTC(1IR − −A eCLIA test collections show that our approach outperforms a state-of-the-art algorithm.</p><p>4 0.83006209 <a title="333-lsi-4" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>5 0.80935425 <a title="333-lsi-5" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>6 0.79621625 <a title="333-lsi-6" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>7 0.78244871 <a title="333-lsi-7" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>8 0.69132 <a title="333-lsi-8" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>9 0.68530405 <a title="333-lsi-9" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>10 0.6794157 <a title="333-lsi-10" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>11 0.58515483 <a title="333-lsi-11" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>12 0.52421772 <a title="333-lsi-12" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>13 0.50051785 <a title="333-lsi-13" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>14 0.49557304 <a title="333-lsi-14" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>15 0.48898509 <a title="333-lsi-15" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>16 0.45359632 <a title="333-lsi-16" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>17 0.43124929 <a title="333-lsi-17" href="./acl-2013-An_improved_MDL-based_compression_algorithm_for_unsupervised_word_segmentation.html">50 acl-2013-An improved MDL-based compression algorithm for unsupervised word segmentation</a></p>
<p>18 0.41864756 <a title="333-lsi-18" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>19 0.41392764 <a title="333-lsi-19" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>20 0.38522473 <a title="333-lsi-20" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.054), (4, 0.011), (6, 0.087), (11, 0.075), (15, 0.011), (23, 0.204), (24, 0.038), (26, 0.065), (28, 0.013), (35, 0.055), (42, 0.053), (48, 0.035), (70, 0.051), (88, 0.042), (90, 0.035), (95, 0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8625294 <a title="333-lda-1" href="./acl-2013-Joint_Modeling_of_News_Reader%C3%A2%E2%80%A2%C5%BDs_and_Comment_Writer%C3%A2%E2%80%A2%C5%BDs_Emotions.html">209 acl-2013-Joint Modeling of News Readerâ•Žs and Comment Writerâ•Žs Emotions</a></p>
<p>Author: Huanhuan Liu ; Shoushan Li ; Guodong Zhou ; Chu-ren Huang ; Peifeng Li</p><p>Abstract: Emotion classification can be generally done from both the writer’s and reader’s perspectives. In this study, we find that two foundational tasks in emotion classification, i.e., reader’s emotion classification on the news and writer’s emotion classification on the comments, are strongly related to each other in terms of coarse-grained emotion categories, i.e., negative and positive. On the basis, we propose a respective way to jointly model these two tasks. In particular, a cotraining algorithm is proposed to improve semi-supervised learning of the two tasks. Experimental evaluation shows the effectiveness of our joint modeling approach. . 1</p><p>same-paper 2 0.80921924 <a title="333-lda-2" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>Author: Anirban Dasgupta ; Ravi Kumar ; Sujith Ravi</p><p>Abstract: We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</p><p>3 0.79499108 <a title="333-lda-3" href="./acl-2013-Stacking_for_Statistical_Machine_Translation.html">328 acl-2013-Stacking for Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Anoop Sarkar</p><p>Abstract: We propose the use of stacking, an ensemble learning technique, to the statistical machine translation (SMT) models. A diverse ensemble of weak learners is created using the same SMT engine (a hierarchical phrase-based system) by manipulating the training data and a strong model is created by combining the weak models on-the-fly. Experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model.</p><p>4 0.79049826 <a title="333-lda-4" href="./acl-2013-Understanding_Tables_in_Context_Using_Standard_NLP_Toolkits.html">365 acl-2013-Understanding Tables in Context Using Standard NLP Toolkits</a></p>
<p>Author: Vidhya Govindaraju ; Ce Zhang ; Christopher Re</p><p>Abstract: Tabular information in text documents contains a wealth of information, and so tables are a natural candidate for information extraction. There are many cues buried in both a table and its surrounding text that allow us to understand the meaning of the data in a table. We study how natural-language tools, such as part-of-speech tagging, dependency paths, and named-entity recognition, can be used to improve the quality of relation extraction from tables. In three domains we show that (1) a model that performs joint probabilistic inference across tabular and natural language features achieves an F1 score that is twice as high as either a puretable or pure-text system, and (2) using only shallower features or non-joint inference results in lower quality.</p><p>5 0.66533101 <a title="333-lda-5" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>Author: Joohyun Kim ; Raymond Mooney</p><p>Abstract: We adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees.</p><p>6 0.66112536 <a title="333-lda-6" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>7 0.654562 <a title="333-lda-7" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>8 0.65406752 <a title="333-lda-8" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>9 0.65178967 <a title="333-lda-9" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>10 0.65093774 <a title="333-lda-10" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>11 0.65039188 <a title="333-lda-11" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>12 0.64896899 <a title="333-lda-12" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>13 0.64665318 <a title="333-lda-13" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>14 0.64659119 <a title="333-lda-14" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>15 0.64464009 <a title="333-lda-15" href="./acl-2013-Modeling_Thesis_Clarity_in_Student_Essays.html">246 acl-2013-Modeling Thesis Clarity in Student Essays</a></p>
<p>16 0.64292324 <a title="333-lda-16" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>17 0.64289403 <a title="333-lda-17" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>18 0.64233243 <a title="333-lda-18" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>19 0.64175606 <a title="333-lda-19" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>20 0.64171284 <a title="333-lda-20" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
