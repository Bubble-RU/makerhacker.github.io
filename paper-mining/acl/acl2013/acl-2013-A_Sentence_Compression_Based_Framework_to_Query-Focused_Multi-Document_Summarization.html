<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-18" href="#">acl2013-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</h1>
<br/><p>Source: <a title="acl-2013-18-pdf" href="http://aclweb.org/anthology//P/P13/P13-1136.pdf">pdf</a></p><p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>Reference: <a title="acl-2013-18-reference" href="../acl2013_reference/acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 s,e NdYu 10598, USA  {hraghav,  vitt orio  Abstract We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. [sent-7, score-0.741]
</p><p>2 We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. [sent-8, score-0.609]
</p><p>3 An innovative beam search decoder is proposed to efficiently find highly probable compressions. [sent-9, score-0.197]
</p><p>4 Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. [sent-10, score-1.316]
</p><p>5 4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. [sent-15, score-0.134]
</p><p>6 Query-focused multi-document summarization (MDS) methods have been proposed as one such technique and have attracted significant attention in recent years. [sent-17, score-0.134]
</p><p>7 The goal of query-focused MDS is to synthesize a brief (often fixed-length) and well-organized summary from a set of topicrelated documents that answer a complex question or address a topic statement. [sent-18, score-0.098]
</p><p>8 The resulting summaries, in turn, can support a number of information analysis applications including openended question answering, recommender systems, and summarization of search engine results. [sent-19, score-0.17]
</p><p>9 As further evidence of its importance, the Document Understanding Conference (DUC) has used queryfocused MDS as its main task since 2004 to foster radu f} @ us . [sent-20, score-0.126]
</p><p>10 com new research on automatic summarization in the context of users’ needs. [sent-22, score-0.134]
</p><p>11 First, lengthy sentences that are partly relevant are either excluded from the summary or (if selected) can block the selection of other important sentences, due to summary length constraints. [sent-25, score-0.159]
</p><p>12 In news articles, for example, most sentences are lengthy and contain both potentially useful information for a summary as well as unnecessary details that are better omitted. [sent-27, score-0.095]
</p><p>13 Consider the following DUC query as input for a MDS system:1 “In what ways have stolen artworks been recovered? [sent-28, score-0.105]
</p><p>14 In this example, the compressed sentence is rela1From DUC 2005, query for topic d422g. [sent-31, score-0.231]
</p><p>15 Sentence compression techniques (Knight and Marcu, 2000; Clarke and Lapata, 2008) are the  standard for producing a compact and grammatical version of a sentence while preserving relevance, and prior research (e. [sent-41, score-0.688]
</p><p>16 Similarly, strides have been made to incorporate sentence compression into query-focused MDS systems (Zajic et al. [sent-44, score-0.645]
</p><p>17 Most attempts, however, fail to produce better results than those of the best systems built on pure extraction-based approaches that use no sentence compression. [sent-46, score-0.122]
</p><p>18 In this paper we investigate the role of sentence compression techniques for query-focused MDS. [sent-47, score-0.645]
</p><p>19 We extend existing work in the area first by investigating the role of learning-based sentence compression techniques. [sent-48, score-0.645]
</p><p>20 Our topperforming sentence compression algorithm incorporates measures of query relevance, content importance, redundancy and language qual-  ity, among others. [sent-50, score-0.831]
</p><p>21 We evaluate the summarization models on the standard Document Understanding Conference (DUC) 2006 and 2007 corpora 2 for queryfocused MDS and find that all of our compressionbased summarization models achieve statistically significantly better performance than the best DUC 2006 systems. [sent-52, score-0.415]
</p><p>22 TAC-08’s opinion summarization or TAC-09’s update summarization) or domains (e. [sent-59, score-0.134]
</p><p>23 , 2006)); human annotators furthermore rate our system-generated summaries as having less redundancy and comparable quality w. [sent-75, score-0.104]
</p><p>24 With these results we believe we are the first to successfully show that sentence compression can provide statistically significant improvements over pure extraction-based approaches for queryfocused MDS. [sent-79, score-0.842]
</p><p>25 2  Related Work  Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. [sent-80, score-0.265]
</p><p>26 , 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al. [sent-83, score-0.14]
</p><p>27 Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. [sent-90, score-0.68]
</p><p>28 (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all  sentences in the original document; these then become the candidates for extraction. [sent-92, score-0.094]
</p><p>29 A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385  but limited improvement is observed over extractive baselines with simple compression rules. [sent-94, score-0.659]
</p><p>30 Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al. [sent-95, score-0.573]
</p><p>31 Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely re-  move a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. [sent-98, score-0.733]
</p><p>32 Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. [sent-99, score-0.573]
</p><p>33 First, sentence ranking determines the importance of each sentence given the query. [sent-101, score-0.178]
</p><p>34 Then, a sentence compressor iteratively generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. [sent-102, score-0.119]
</p><p>35 Finally, the postprocessing stage applies coreference resolution and sentence reordering to build the summary. [sent-103, score-0.165]
</p><p>36 Table 1), we describe next the query-relevant features used for  sentence ranking as these are the most important for our summarization setting. [sent-119, score-0.24]
</p><p>37 The goal of this feature subset is to determine the similarity between the query and each candidate sentence. [sent-120, score-0.105]
</p><p>38 Then we conduct simple query expansion based on the title of the topic and cross-document coreference resolution. [sent-123, score-0.16]
</p><p>39 Finally, we compute two versions of the features—one based on the original query and another on the expanded one. [sent-126, score-0.105]
</p><p>40 We also derive the semantic role overlap and relation instance overlap between the query and each sentence. [sent-127, score-0.105]
</p><p>41 As the main focus of this paper, we propose three types of compression methods, described in detail in Section 4 below. [sent-130, score-0.573]
</p><p>42 For sentence ordering, each compressed sentence is assigned  to the most similar (tf-idf) query sentence. [sent-135, score-0.303]
</p><p>43 , 2002) sorts the sentences for each query based first on the time stamp, and then the position in the source document. [sent-137, score-0.105]
</p><p>44 4  Sentence Compression  Sentence compression is typically formulated as the problem of removing secondary information from a sentence while maintaining its grammaticality and semantic structure (Knight and Marcu, 2000; McDonald, 2006; Galley and McKeown, 2007; Clarke and Lapata, 2008). [sent-138, score-0.681]
</p><p>45 Below we describe the sentence compression approaches developed in this research: RULE-BASED COMPRESSION, SEQUENCE-BASED COMPRESSION, and TREEBASED COMPRESSION. [sent-140, score-0.645]
</p><p>46 , 2007) to create the linguistically-motivated compression rules of Table 2. [sent-145, score-0.573]
</p><p>47 To avoid ill-formed output, we disallow compressions of more than 10 words by each rule. [sent-146, score-0.094]
</p><p>48 2 Sequence-based Compression As in McDonald (2006) and Clarke and Lapata (2008), our sequence-based compression model makes a binary “keep-or-delete” decision for each word in the sentence. [sent-148, score-0.573]
</p><p>49 ” view compression as a sequential tagging problem and make use of linear-chain Conditional Random Fields (CRFs) (Lafferty et al. [sent-154, score-0.573]
</p><p>50 3 Tree-based Compression Our tree-based compression methods are in line with syntax-driven approaches (Galley and McKeown, 2007), where operations are carried out on parse tree constituents. [sent-171, score-0.654]
</p><p>51 Formally, given a parse tree T of the sentence to be compressed and a tree traversal algorithm, T can be presented as a list of ordered constituent nodes, T = t0t1 . [sent-174, score-0.313]
</p><p>52 RETAIN (RET) Rand REMOVE (REM) denote} whether the node ti is retained or removed. [sent-182, score-0.13]
</p><p>53 Labels are identified, in order, according to the tree traversal algorithm. [sent-186, score-0.106]
</p><p>54 Every node label needs to be compatible with the labeling history: given a node ti, and a set of labels l0 . [sent-187, score-0.129]
</p><p>55 ti−1, li =RET or li =REM is compatible with the history when all children of ti are labeled as RET or REM, respectively; li =PAR is compatible when ti has at least two descendents  tj and tk (j < i and k < i), one of which is RETained and the other, REMoved. [sent-193, score-0.154]
</p><p>56 As the space of possible compressions is exponential in the number of leaves in the parse tree, instead of looking for the globally optimal solution, we use beam search to find a set of highly likely compressions and employ a language model trained on a large corpus for evaluation. [sent-195, score-0.374]
</p><p>57 The beam search decoder (see Algorithm 1) takes as input the sentence’s parse tree T = t0t1 . [sent-197, score-0.278]
</p><p>58 postorder) as a sequence of nodes in T, the set L of possible node labels, a scoring function S for evaluating each sentence compression hypothesis, and a beam size N. [sent-202, score-0.874]
</p><p>59 In iteration i, all existing sentence compression hypotheses are ex-  panded by one node, tOi , labeling it with all compatible labels. [sent-211, score-0.68]
</p><p>60 Our BASIC Tree-based Compression instantiates the beam search decoder with postorder traversal and a hypothesis scorer that takes a possible sentence compression— a sequence of nodes (e. [sent-214, score-0.479]
</p><p>61 ,  Pjk=1  1388  Figure 2: Example of beam search decoding. [sent-226, score-0.15]
</p><p>62 For postorder traversal, the three nodes are visited in a bottom-up order. [sent-227, score-0.112]
</p><p>63 The associated compression hypotheses (boxed) are ranked based on the scores in parentheses. [sent-228, score-0.573]
</p><p>64 HEAD-driven search modifies the BASIC postorder tree traversal by visiting the head node first at each level, leaving other orders unchanged. [sent-232, score-0.266]
</p><p>65 Given the N-best compressions from the decoder, we evaluate the yield of the trimmed trees using a language model trained on the Gigaword (Graff, 2003) corpus and return the compression with the highest probability. [sent-239, score-0.701]
</p><p>66 Thus, the decoder is quite flexible its learned scoring function allows us to incorporate features salient for sentence compression while its language model guarantees the linguistic quality of the compressed string. [sent-240, score-0.779]
</p><p>67 Towards this goal, we construct a compression scoring function—the multi-scorer (MULTI)—that allows the incorporation of multiple task-specific scorers. [sent-246, score-0.606]
</p><p>68 The query Q is expanded as described in Section 3. [sent-253, score-0.105]
</p><p>69 Relevant documents for each query are provided along with 4 to 9 human MDS abstracts. [sent-271, score-0.105]
</p><p>70 We split DUC 2005 into two parts: 40 topics to train the sentence ranking models, and 10 for ranking algorithm selection and parameter tuning for the multiscorer. [sent-273, score-0.14]
</p><p>71 It includes 82 newswire articles with one manually produced compression aligned to each sentence. [sent-277, score-0.573]
</p><p>72 Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al. [sent-279, score-0.103]
</p><p>73 For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. [sent-286, score-0.573]
</p><p>74 4 Sentence compressions are evaluated by a 5-gram language model trained on Gigaword (Graff, 2003) by SRILM (Stolcke, 2002). [sent-292, score-0.094]
</p><p>75 (2012) that report the best R-2 score on DUC 2006 and 2007 thus far, and to the purely extractive methods of SVR and LambdaMART. [sent-297, score-0.086]
</p><p>76 Our sentence-compression-based systems (marked with †) show statistically significant improvements over pure wex sttraatictsitvicea lsluymm sigarnizifaictiaonnt for both R-2 and R-SU4 (paired t-test, p < 0. [sent-298, score-0.101]
</p><p>77 This means our systems can effectively remove redundancy within the summary through compression. [sent-300, score-0.109]
</p><p>78 Furthermore, our HEAD-driven beam search method with MULTI-scorer beats all systems on DUC 20066 and all systems on DUC 2007 except the best system in terms of R-2 (p < 0. [sent-301, score-0.15]
</p><p>79 01) better than extractive methods, rule-based and sequence-based compression methods on both DUC 2006 and 2007. [sent-304, score-0.659]
</p><p>80 Moreover, our systems with  learning-based compression have considerable compression rates, indicating their capability to remove superfluous words as well as improve summary quality. [sent-305, score-1.21]
</p><p>81 BASIC, CONTEXT and HEAD represent the basic beam search decoder, context-aware and head-driven search extensions respectively. [sent-319, score-0.186]
</p><p>82 Four native speakers who are undergraduate students in computer science (none are authors) per-  formed the task, We compare our system based on HEAD-driven beam search with MULTI-scorer to the best systems in DUC 2006 achieving top ROUGE scores (Jagarlamudi et al. [sent-353, score-0.15]
</p><p>83 (2006), which either uses minimal non-learning-based compression rules or is a pure extractive system. [sent-366, score-0.709]
</p><p>84 However, our compression system sometimes generates less grammatical sentences, and those are mostly due  to parsing errors. [sent-367, score-0.616]
</p><p>85 A sample summary from our multiscorer based system is in Figure 3. [sent-370, score-0.102]
</p><p>86 We also evaluate sentence compression separately on (Clarke and Lapata, 2008), adopting the same partitions as (Martins and Smith, 2009), i. [sent-372, score-0.645]
</p><p>87 Our compression models are compared with Hedge Trimmer (Dorr et al. [sent-375, score-0.573]
</p><p>88 there is no statistically signifi-  cant difference between our models and McDonald (2006) / M & S (2009) with p unigram F1 (Uni-F1) are statistically indistinguishable (p  > 0. [sent-380, score-0.102]
</p><p>89 How and  Figure 3: Part of the summary generated by the multiscorer based summarizer for topic D0626H (DUC 2006). [sent-389, score-0.102]
</p><p>90 In Table 7, our context-aware and head-driven tree-based compression systems show statistically significantly (p < 0. [sent-395, score-0.624]
</p><p>91 For grammatical relation evaluation, our head-driven tree-based system obtains statistically significantly (p < 0. [sent-403, score-0.094]
</p><p>92 7  Conclusion  We have presented a framework for query-focused multi-document summarization based on sentence compression. [sent-405, score-0.206]
</p><p>93 Our tree-based compression method can easily incorporate measures of query relevance, content importance, redundancy and language quality into the compression process. [sent-407, score-1.332]
</p><p>94 Inferring strategies for sentence ordering in multidocument news summarization. [sent-425, score-0.148]
</p><p>95 Global inference for sentence compression an integer linear programming approach. [sent-477, score-0.645]
</p><p>96 In Proceedings of the HLT-NAACL 03 on Text summarization workshop - Volume 5, HLT-NAACLDUC ’03, pages 1 8, Stroudsburg, PA, USA. [sent-527, score-0.134]
</p><p>97 Support vector machines for query-focused summarization trained and evaluated on pyramid data. [sent-544, score-0.24]
</p><p>98 Improving summarization performance by sentence compression: a pilot study. [sent-622, score-0.206]
</p><p>99 A mentionsynchronous coreference resolution algorithm based on the bell tree. [sent-631, score-0.093]
</p><p>100 Sentence compression as a component of a multidocument summarization system. [sent-705, score-0.752]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('compression', 0.573), ('duc', 0.424), ('mds', 0.191), ('jagarlamudi', 0.142), ('summarization', 0.134), ('beam', 0.114), ('pyramid', 0.106), ('query', 0.105), ('rem', 0.102), ('lacatusu', 0.096), ('queryfocused', 0.096), ('compressions', 0.094), ('extractive', 0.086), ('clarke', 0.082), ('rouge', 0.078), ('davis', 0.078), ('lambdamart', 0.077), ('postorder', 0.077), ('ret', 0.074), ('sentence', 0.072), ('dang', 0.069), ('stroudsburg', 0.069), ('luo', 0.067), ('conroy', 0.065), ('summary', 0.064), ('martins', 0.063), ('lq', 0.063), ('traversal', 0.061), ('summaries', 0.059), ('ouyang', 0.058), ('pingali', 0.058), ('zajic', 0.056), ('coreference', 0.055), ('pa', 0.055), ('compressed', 0.054), ('xiaoqiang', 0.054), ('marcu', 0.052), ('statistically', 0.051), ('pure', 0.05), ('decoder', 0.047), ('galley', 0.047), ('compressor', 0.047), ('node', 0.047), ('redundancy', 0.045), ('tree', 0.045), ('lapata', 0.045), ('multidocument', 0.045), ('turner', 0.044), ('grammatical', 0.043), ('erkan', 0.042), ('mckeown', 0.042), ('ti', 0.042), ('retained', 0.041), ('gillick', 0.04), ('knight', 0.039), ('fuentes', 0.038), ('mozer', 0.038), ('multiscorer', 0.038), ('rankers', 0.038), ('scorebasic', 0.038), ('scoreim', 0.038), ('scorelm', 0.038), ('scoreq', 0.038), ('scorered', 0.038), ('sumbasic', 0.038), ('resolution', 0.038), ('scorer', 0.037), ('parse', 0.036), ('grammaticality', 0.036), ('content', 0.036), ('search', 0.036), ('document', 0.035), ('nodes', 0.035), ('compatible', 0.035), ('burges', 0.034), ('lin', 0.034), ('ranking', 0.034), ('aho', 0.034), ('grayed', 0.034), ('kincaid', 0.034), ('synthesize', 0.034), ('toi', 0.034), ('trimmed', 0.034), ('trimmer', 0.034), ('gray', 0.034), ('scoring', 0.033), ('mcdonald', 0.032), ('relevance', 0.032), ('nenkova', 0.031), ('ordering', 0.031), ('vasudeva', 0.031), ('crossdocument', 0.031), ('langauge', 0.031), ('lengthy', 0.031), ('nanda', 0.031), ('otterbacher', 0.031), ('varma', 0.031), ('radu', 0.03), ('retain', 0.03), ('arrested', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="18-tfidf-1" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>2 0.28797209 <a title="18-tfidf-2" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>Author: Miguel Almeida ; Andre Martins</p><p>Abstract: We present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear programming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers.</p><p>3 0.23477224 <a title="18-tfidf-3" href="./acl-2013-An_improved_MDL-based_compression_algorithm_for_unsupervised_word_segmentation.html">50 acl-2013-An improved MDL-based compression algorithm for unsupervised word segmentation</a></p>
<p>Author: Ruey-Cheng Chen</p><p>Abstract: We study the mathematical properties of a recently proposed MDL-based unsupervised word segmentation algorithm, called regularized compression. Our analysis shows that its objective function can be efficiently approximated using the negative empirical pointwise mutual information. The proposed extension improves the baseline performance in both efficiency and accuracy on a standard benchmark.</p><p>4 0.22355177 <a title="18-tfidf-4" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>Author: Hajime Morita ; Ryohei Sasano ; Hiroya Takamura ; Manabu Okumura</p><p>Abstract: This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 21 (1 − e−1). Our experiments with the NTC(1IR − −A eCLIA test collections show that our approach outperforms a state-of-the-art algorithm.</p><p>5 0.1621491 <a title="18-tfidf-5" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>6 0.15125659 <a title="18-tfidf-6" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>7 0.14920481 <a title="18-tfidf-7" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>8 0.14412877 <a title="18-tfidf-8" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>9 0.1330916 <a title="18-tfidf-9" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>10 0.13170682 <a title="18-tfidf-10" href="./acl-2013-Generalizing_Image_Captions_for_Image-Text_Parallel_Corpus.html">167 acl-2013-Generalizing Image Captions for Image-Text Parallel Corpus</a></p>
<p>11 0.12091849 <a title="18-tfidf-11" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>12 0.11074369 <a title="18-tfidf-12" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>13 0.10340738 <a title="18-tfidf-13" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>14 0.092551276 <a title="18-tfidf-14" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>15 0.091592155 <a title="18-tfidf-15" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>16 0.080230907 <a title="18-tfidf-16" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>17 0.078528695 <a title="18-tfidf-17" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>18 0.078148939 <a title="18-tfidf-18" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>19 0.077196151 <a title="18-tfidf-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.075597294 <a title="18-tfidf-20" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.21), (1, 0.004), (2, -0.031), (3, -0.071), (4, 0.009), (5, 0.057), (6, 0.173), (7, -0.036), (8, -0.226), (9, -0.07), (10, -0.068), (11, 0.056), (12, -0.212), (13, -0.023), (14, -0.083), (15, 0.189), (16, 0.183), (17, -0.105), (18, -0.031), (19, 0.112), (20, -0.035), (21, -0.023), (22, -0.032), (23, -0.019), (24, 0.017), (25, -0.085), (26, -0.002), (27, 0.02), (28, 0.072), (29, 0.02), (30, -0.022), (31, 0.029), (32, -0.03), (33, 0.009), (34, 0.047), (35, -0.026), (36, 0.069), (37, -0.027), (38, 0.0), (39, -0.05), (40, -0.06), (41, -0.046), (42, 0.041), (43, 0.107), (44, 0.034), (45, -0.015), (46, -0.033), (47, 0.023), (48, 0.023), (49, -0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9272455 <a title="18-lsi-1" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>2 0.85845071 <a title="18-lsi-2" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>Author: Hajime Morita ; Ryohei Sasano ; Hiroya Takamura ; Manabu Okumura</p><p>Abstract: This study proposes a text summarization model that simultaneously performs sentence extraction and compression. We translate the text summarization task into a problem of extracting a set of dependency subtrees in the document cluster. We also encode obligatory case constraints as must-link dependency constraints in order to guarantee the readability of the generated summary. In order to handle the subtree extraction problem, we investigate a new class of submodular maximization problem, and a new algorithm that has the approximation ratio 21 (1 − e−1). Our experiments with the NTC(1IR − −A eCLIA test collections show that our approach outperforms a state-of-the-art algorithm.</p><p>3 0.85407126 <a title="18-lsi-3" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>Author: Anirban Dasgupta ; Ravi Kumar ; Sujith Ravi</p><p>Abstract: We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</p><p>4 0.82463413 <a title="18-lsi-4" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>Author: Miguel Almeida ; Andre Martins</p><p>Abstract: We present a dual decomposition framework for multi-document summarization, using a model that jointly extracts and compresses sentences. Compared with previous work based on integer linear programming, our approach does not require external solvers, is significantly faster, and is modular in the three qualities a summary should have: conciseness, informativeness, and grammaticality. In addition, we propose a multi-task learning framework to take advantage of existing data for extractive summarization and sentence compression. Experiments in the TAC2008 dataset yield the highest published ROUGE scores to date, with runtimes that rival those of extractive summarizers.</p><p>5 0.797306 <a title="18-lsi-5" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>6 0.79656506 <a title="18-lsi-6" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>7 0.73490173 <a title="18-lsi-7" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>8 0.65757751 <a title="18-lsi-8" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>9 0.6439631 <a title="18-lsi-9" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>10 0.62249458 <a title="18-lsi-10" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>11 0.59157622 <a title="18-lsi-11" href="./acl-2013-An_improved_MDL-based_compression_algorithm_for_unsupervised_word_segmentation.html">50 acl-2013-An improved MDL-based compression algorithm for unsupervised word segmentation</a></p>
<p>12 0.57686162 <a title="18-lsi-12" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>13 0.52240878 <a title="18-lsi-13" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>14 0.51043844 <a title="18-lsi-14" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>15 0.5098871 <a title="18-lsi-15" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>16 0.48695379 <a title="18-lsi-16" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>17 0.46869785 <a title="18-lsi-17" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>18 0.42683959 <a title="18-lsi-18" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>19 0.41627887 <a title="18-lsi-19" href="./acl-2013-Generalizing_Image_Captions_for_Image-Text_Parallel_Corpus.html">167 acl-2013-Generalizing Image Captions for Image-Text Parallel Corpus</a></p>
<p>20 0.3914313 <a title="18-lsi-20" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.051), (6, 0.064), (11, 0.082), (15, 0.015), (16, 0.014), (24, 0.039), (26, 0.056), (34, 0.163), (35, 0.077), (42, 0.067), (48, 0.04), (60, 0.012), (64, 0.015), (70, 0.048), (88, 0.045), (90, 0.041), (95, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8663733 <a title="18-lda-1" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>Author: Ravi Kondadadi ; Blake Howald ; Frank Schilder</p><p>Abstract: We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated weather and biography texts fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. – – *∗Ravi Kondadadi is now affiliated with Nuance Communications, Inc.</p><p>same-paper 2 0.85163337 <a title="18-lda-2" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>3 0.75439799 <a title="18-lda-3" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>Author: Xipeng Qiu ; Qi Zhang ; Xuanjing Huang</p><p>Abstract: The growing need for Chinese natural language processing (NLP) is largely in a range of research and commercial applications. However, most of the currently Chinese NLP tools or components still have a wide range of issues need to be further improved and developed. FudanNLP is an open source toolkit for Chinese natural language processing (NLP) , which uses statistics-based and rule-based methods to deal with Chinese NLP tasks, such as word segmentation, part-ofspeech tagging, named entity recognition, dependency parsing, time phrase recognition, anaphora resolution and so on.</p><p>4 0.74083757 <a title="18-lda-4" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>5 0.73060668 <a title="18-lda-5" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>Author: Anirban Dasgupta ; Ravi Kumar ; Sujith Ravi</p><p>Abstract: We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</p><p>6 0.72857964 <a title="18-lda-6" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>7 0.72665977 <a title="18-lda-7" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>8 0.72592258 <a title="18-lda-8" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>9 0.72555768 <a title="18-lda-9" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>10 0.72501922 <a title="18-lda-10" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>11 0.72474569 <a title="18-lda-11" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>12 0.72466165 <a title="18-lda-12" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>13 0.72448486 <a title="18-lda-13" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>14 0.72230291 <a title="18-lda-14" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>15 0.72087538 <a title="18-lda-15" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>16 0.71917617 <a title="18-lda-16" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>17 0.71836168 <a title="18-lda-17" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>18 0.71732724 <a title="18-lda-18" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>19 0.71699107 <a title="18-lda-19" href="./acl-2013-Automatic_detection_of_deception_in_child-produced_speech_using_syntactic_complexity_features.html">63 acl-2013-Automatic detection of deception in child-produced speech using syntactic complexity features</a></p>
<p>20 0.7153939 <a title="18-lda-20" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
