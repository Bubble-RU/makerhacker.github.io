<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 acl-2013-Automatic Term Ambiguity Detection</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-62" href="#">acl2013-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 acl-2013-Automatic Term Ambiguity Detection</h1>
<br/><p>Source: <a title="acl-2013-62-pdf" href="http://aclweb.org/anthology//P/P13/P13-2140.pdf">pdf</a></p><p>Author: Tyler Baldwin ; Yunyao Li ; Bogdan Alexe ; Ioana R. Stanoi</p><p>Abstract: While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96.</p><p>Reference: <a title="acl-2013-62-reference" href="../acl2013_reference/acl-2013-Automatic_Term_Ambiguity_Detection_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com , rs  Abstract While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. [sent-4, score-1.037]
</p><p>2 To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. [sent-5, score-0.699]
</p><p>3 By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. [sent-6, score-1.0]
</p><p>4 To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. [sent-7, score-1.027]
</p><p>5 It can be particularly problematic for information extraction (IE), as IE systems often wish to extract information about only one sense of polysemous terms. [sent-12, score-0.185]
</p><p>6 If nothing is done to account for this polysemy, frequent mentions of unrelated senses can drastically harm performance. [sent-13, score-0.103]
</p><p>7 Several NLP tasks, such as word sense disambiguation, word sense induction, and named entity disambiguation, address this ambiguity problem to varying degrees. [sent-14, score-0.843]
</p><p>8 While the goals and initial data assumptions vary between these tasks, all of them attempt to map an instance of a term seen in context to an individual sense. [sent-15, score-0.408]
</p><p>9 While making a judgment for every instance may be appropri-  ate for small or medium sized data sets, the cost of applying these ambiguity resolution procedures becomes prohibitively expensive on large data sets of tens to hundreds of million items. [sent-16, score-0.728]
</p><p>10 To combat this, this work zooms out to examine the ambiguity problem at a more general level. [sent-17, score-0.589]
</p><p>11 To do so, we define an IE-centered ambiguity detection problem, which ties the notion of ambiguity to a given topical domain. [sent-18, score-1.254]
</p><p>12 For instance, given that the terms Call of Juarez and A New Beginning can both reference video games, we would like to discover that only the latter case is likely to appear frequently in non-video game contexts. [sent-19, score-0.262]
</p><p>13 The goal is to make a binary decision as to whether, given a term and a domain, we can expect every instance of that term to reference an entity in that domain. [sent-20, score-0.68]
</p><p>14 By doing so, we segregate ambiguous terms from their unambiguous counterparts. [sent-21, score-0.401]
</p><p>15 Using this segregation allows ambiguous and unambiguous instances to be treated differently while saving the processing time that might normally be spent attempting to disambiguate individual instances of unambiguous terms. [sent-22, score-0.67]
</p><p>16 Previous approaches to handling word ambiguity employ a variety of disparate methods, variously relying on structured ontologies, gleaming insight from general word usage patterns via language models, or clustering the contexts in which words appear. [sent-23, score-0.666]
</p><p>17 This work employs an ambiguity detection pipeline that draws inspiration from all of these methods to achieve high performance. [sent-24, score-0.652]
</p><p>18 2  Term Ambiguity Detection (TAD)  A term can be ambiguous in many ways. [sent-25, score-0.429]
</p><p>19 It may have non-referential senses in which it shares a name with a common word or phrase, such as in the films Brave and 2012. [sent-26, score-0.221]
</p><p>20 A term may have referential senses across topical domains, such as The Girl with the Dragon Tattoo, which may reference either the book or the film adaptation. [sent-27, score-0.619]
</p><p>21 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 804â€“809, also be ambiguous within a topical domain. [sent-30, score-0.249]
</p><p>22 For instance, the term Final Fantasy may refer to the video game franchise or one of several individual  games within the franchise. [sent-31, score-0.586]
</p><p>23 In this work we concern ourselves with the first two types of ambiguity, as within topical domain ambiguity tends to pose a less severe problem for IE systems. [sent-32, score-0.643]
</p><p>24 IE systems are often asked to perform extraction over a dictionary of terms centered around a single topic. [sent-33, score-0.082]
</p><p>25 With this use case in mind, we define the term ambiguity detection (TAD) problem as follows: Given a term and a corresponding topic domain, determine whether the term uniquely references a member of that topic domain. [sent-35, score-1.636]
</p><p>26 That is, given a term such as Brave and a category such as film, the task is make a binary decision as to whether all instances of Brave reference a film by that name. [sent-36, score-0.423]
</p><p>27 1 Framework Our TAD framework ing of three modules is primarily designed biguity. [sent-38, score-0.179]
</p><p>28 This module  is a hybrid approach consist(Figure 1). [sent-39, score-0.096]
</p><p>29 The first module to detect non-referential amexamines n-gram data from  a large text collection. [sent-40, score-0.134]
</p><p>30 The rationale behind the n-gram module is based on the understanding that terms appearing in non-named entity contexts are likely to be nonreferential, and terms that can be non-referential are ambiguous. [sent-42, score-0.25]
</p><p>31 Therefore, detecting terms that have non-referential usages can also be used to detect ambiguity. [sent-43, score-0.132]
</p><p>32 Since we wish for the ambiguity detection determination to be fast, we develop our method to make this judgment solely on the n-gram probability, without the need to examine each individual usage context. [sent-44, score-0.947]
</p><p>33 To do so, we assume that an all lowercased version of the term is a reasonable proxy for non-named entity usages in formal text. [sent-45, score-0.369]
</p><p>34 If the probability is above a certain threshold, the term is labeled as ambiguous. [sent-47, score-0.271]
</p><p>35 If the term is below the threshold, it is tentatively labeled as unambiguous and passed to the next module. [sent-48, score-0.464]
</p><p>36 To avoid making  judgments of ambiguity based on very infrequent uses, the ambiguous-unambiguous determination threshold is empirically determined by minimizing error over held out data. [sent-49, score-0.663]
</p><p>37 The second module employs ontologies to detect across domain ambiguity. [sent-50, score-0.32]
</p><p>38 Terms that have multiple senses in Wiktionary were labeled as ambiguous. [sent-53, score-0.103]
</p><p>39 All terms that had a disambiguation page were marked as ambiguous. [sent-55, score-0.222]
</p><p>40 The final module attempts to detect both nonreferential and across domain ambiguity by clustering the contexts in which words appear. [sent-56, score-0.822]
</p><p>41 LDA represents a document as a distribution of topics, and each topic as a distribution of words. [sent-59, score-0.063]
</p><p>42 As our domain of interest is Twitter, we performed clustering over a large collection of tweets. [sent-60, score-0.108]
</p><p>43 For a given term, all tweets that contained  the term were used as a document collection. [sent-61, score-0.378]
</p><p>44 Following standard procedure, stopwords and infrequent words were removed before topic modeling was performed. [sent-62, score-0.134]
</p><p>45 Since the clustering mechanism was designed to make predictions over the already filtered data of the other modules, it adopts a conservative approach to predicting ambiguity. [sent-63, score-0.119]
</p><p>46 , film) or a synonym from the WordNet synset does not appear in the 10 most heavily weighted words for any cluster, the term is marked as ambiguous. [sent-66, score-0.311]
</p><p>47 A term is labeled as ambiguous if any one of the three modules predicts that it is ambiguous, but only labeled as unambiguous if all three modules make this prediction. [sent-67, score-0.98]
</p><p>48 This design allows each module to be relatively conservative in predicting ambiguity, keeping precision of ambiguity prediction high, under the assumption that other modules will compensate for the corresponding drop in recall. [sent-68, score-0.838]
</p><p>49 1 Data Set Initial Term Sets We collected a data set of terms  from four topical domains: books, films, video games, and cameras. [sent-70, score-0.243]
</p><p>50 Terms for the first three domains are lists of books, films, and video games respectively from the years 2000-201 1 from dbpedia (Auer et al. [sent-71, score-0.258]
</p><p>51 neTo,p5worstmovies verA B eaST ueptlirfcumel M indCaft ie l gm moryJudygnemosent Table 1: Example tweet annotations. [sent-78, score-0.231]
</p><p>52 Figure 1: Overview of the ambiguity detection framework. [sent-79, score-0.652]
</p><p>53 for cameras includes all the cameras from the six most popular brands on flickr2. [sent-80, score-0.122]
</p><p>54 Gold Standard A set of 100 terms per domain were chosen at random from the initial term sets. [sent-81, score-0.393]
</p><p>55 Rather than annotating each term directly, ambiguity was determined by examining actual usage. [sent-82, score-0.782]
</p><p>56 Specifically, for each term, usage examples were extracted from large amounts of Twitter data. [sent-83, score-0.047]
</p><p>57 Tweets for the video game andfilm categories were extracted from the TREC Twitter corpus. [sent-84, score-0.174]
</p><p>58 3 The less common book and camera cases were extracted from a subset of all tweets from September 1st-9th, 2012. [sent-85, score-0.187]
</p><p>59 For each term, two annotators were given the term, the corresponding topic domain, and 10 randomly selected tweets containing the term. [sent-86, score-0.17]
</p><p>60 They were then asked to make a binary judgment as to whether the usage of the term in the tweet referred to an instance of the given category. [sent-87, score-0.517]
</p><p>61 The degree of ambiguity is then determined by calculating the percentage of tweets that did not reference a member of the topic domain. [sent-88, score-0.764]
</p><p>62 If all individual tweet judgments for a term were marked as referring to a  2http://www. [sent-90, score-0.519]
</p><p>63 member of the topic domain, the term was marked as fully unambiguous within the data examined. [sent-102, score-0.612]
</p><p>64 Most disagreements on individual tweet judgments had little effect on the final judgment of a term as ambiguous or unambiguous, and those that did were resolved internally. [sent-106, score-0.718]
</p><p>65 2  Evaluation and Results  Effectiveness To understand the contribution of  the n-gram (NG), ontology (ON), and clustering (CL) based modules, we ran each separately, as well as every possible combination. [sent-108, score-0.131]
</p><p>66 Of the three individual modules, the ngram and clustering methods achieve F-measure of around 0. [sent-111, score-0.127]
</p><p>67 9, while the ontology-based module performs only modestly above baseline. [sent-112, score-0.096]
</p><p>68 Unsurprisingly, the ontology method is affected heavily by its coverage, so its poor performance is primarily attributable to low recall. [sent-113, score-0.064]
</p><p>69 Additionally, ontologies may be apt to list cases of strict ambiguity, rather than practical ambiguity. [sent-115, score-0.145]
</p><p>70 That is, an ontology may list a term as ambiguous if there are 4The  annotated data is available at http / / re s earche r . [sent-116, score-0.493]
</p><p>71 Combining any two methods produced substantial performance increases over any of the individual runs. [sent-123, score-0.06]
</p><p>72 The final system that employed all modules produced an F-measure of 0. [sent-124, score-0.179]
</p><p>73 Usefulness To establish that term ambiguity detection is actually helpful for IE, we conducted a preliminary study by integrating our pipeline into a commercially available rule-based IE system (Chiticariu et al. [sent-128, score-0.923]
</p><p>74 The system takes a list of product names as input and outputs tweets associated with each product. [sent-131, score-0.107]
</p><p>75 It utilizes rules that employ more conservative extraction for ambiguous entities. [sent-132, score-0.283]
</p><p>76 Experiments were conducted over several million tweets using the terms from the video game and camera domains. [sent-133, score-0.376]
</p><p>77 When no ambiguity detection was performed, all terms were treated as unambiguous. [sent-134, score-0.702]
</p><p>78 16 when no ambiguity detection was used, due to the extraction of irrelevant instances of ambiguous objects. [sent-136, score-0.875]
</p><p>79 However, the inclusion of disambiguation did reduce the overall recall; the system that employed disambiguation returned only about 57% of the true positives returned by the system that did not employ disambiguation. [sent-139, score-0.367]
</p><p>80 Although this reduction in recall is significant, the overall impact of disambiguation is clearly positive, due to the stark difference in precision. [sent-140, score-0.132]
</p><p>81 Machine translation systems can suffer, as ambiguity in the source language may lead to incorrect translations, and unambiguous sentences in one language may become am-  biguous in another (Carpuat and Wu, 2007; Chan et al. [sent-143, score-0.704]
</p><p>82 The ambiguity detection problem is similar to the well studied problems of named entity disambiguation (NED) and word sense disambiguation (WSD). [sent-146, score-1.127]
</p><p>83 However, these tasks assume that the number of senses a word has is given, essentially assuming that the ambiguity detection problem has already been solved. [sent-147, score-0.755]
</p><p>84 This makes these tasks inapplicable in many IE instances where the amount of ambiguity is not known ahead of time. [sent-148, score-0.544]
</p><p>85 Both named entity and word sense disambiguation are extensively studied, and surveys on each are available (Nadeau and Sekine, 2007; Navigli, 2009). [sent-149, score-0.343]
</p><p>86 Another task that shares similarities with TAD is word sense induction (WSI). [sent-150, score-0.217]
</p><p>87 Like NED and WSD, WSI frames the ambiguity problem as one of determining the sense of each individual instance, rather than the term as a whole. [sent-151, score-0.963]
</p><p>88 Unlike  those approaches, the word sense induction task attempts to both figure out the number of senses a word has, and what they are. [sent-152, score-0.287]
</p><p>89 Pantel and Lin (2002) employ a clustering by committee method that iteratively adds words to clusters based on their similarities. [sent-155, score-0.108]
</p><p>90 5  Conclusion  This paper introduced the term ambiguity detection task, which detects whether a term is ambiguous relative to a topical domain. [sent-160, score-1.443]
</p><p>91 Unlike other ambiguity resolution tasks, the ambiguity detection problem makes general ambiguity judgments about terms, rather than resolving individual instances. [sent-161, score-1.889]
</p><p>92 By doing so, it eliminates the need for ambiguity resolution on unambiguous objects, allowing for increased throughput of IE systems on large data sets. [sent-162, score-0.807]
</p><p>93 Our solution for the term ambiguity detection 807  task is based on a combined model with three distinct modules based on n-grams, ontologies, and clustering. [sent-163, score-1.102]
</p><p>94 Our initial study suggests that the combination of different modules designed for different types of ambiguity used in our solution is effective in determining whether a term is ambiguous for a given domain. [sent-164, score-1.15]
</p><p>95 Although the task as presented here was motivated with information extraction in mind, it is possible that term ambiguity detection could be useful for other tasks. [sent-166, score-0.955]
</p><p>96 For instance, TAD could be used to aid word sense induction more generally, or could be applied as part of other tasks such as coreference resolution. [sent-167, score-0.184]
</p><p>97 Chinese verb sense discrimination using an em clustering model with rich linguistic features. [sent-202, score-0.188]
</p><p>98 Word sense induction & disambiguation using hierarchical random graphs. [sent-218, score-0.316]
</p><p>99 Inducing word senses to improve web search result clustering. [sent-232, score-0.103]
</p><p>100 Query ambiguity revisited: Clickthrough measures for distinguishing informational and ambiguous queries. [sent-253, score-0.669]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ambiguity', 0.511), ('term', 0.271), ('unambiguous', 0.193), ('modules', 0.179), ('tad', 0.173), ('ie', 0.159), ('ambiguous', 0.158), ('ontologies', 0.145), ('detection', 0.141), ('disambiguation', 0.132), ('sense', 0.121), ('tweets', 0.107), ('wsi', 0.107), ('alexe', 0.104), ('senses', 0.103), ('video', 0.102), ('module', 0.096), ('topical', 0.091), ('films', 0.085), ('rajasekar', 0.085), ('brave', 0.085), ('judgment', 0.081), ('games', 0.081), ('film', 0.081), ('judgments', 0.076), ('krishnamurthy', 0.073), ('tweet', 0.072), ('game', 0.072), ('klapaftis', 0.069), ('nonreferential', 0.069), ('clustering', 0.067), ('ontology', 0.064), ('induction', 0.063), ('topic', 0.063), ('ioana', 0.061), ('stanoi', 0.061), ('cameras', 0.061), ('nadeau', 0.061), ('individual', 0.06), ('stroudsburg', 0.058), ('throughput', 0.057), ('chiticariu', 0.057), ('yunyao', 0.057), ('navigli', 0.056), ('polysemy', 0.055), ('entity', 0.054), ('bogdan', 0.053), ('conservative', 0.052), ('shivakumar', 0.051), ('terms', 0.05), ('carpuat', 0.049), ('zhong', 0.049), ('usage', 0.047), ('combat', 0.047), ('lau', 0.047), ('auer', 0.047), ('instance', 0.046), ('resolution', 0.046), ('twitter', 0.046), ('pa', 0.046), ('member', 0.045), ('camera', 0.045), ('usages', 0.044), ('ned', 0.044), ('dbpedia', 0.044), ('prohibitively', 0.044), ('determination', 0.043), ('dirichlet', 0.042), ('chan', 0.041), ('employ', 0.041), ('domain', 0.041), ('contemporary', 0.04), ('marked', 0.04), ('brody', 0.039), ('detect', 0.038), ('reference', 0.038), ('stopwords', 0.038), ('examination', 0.036), ('ibm', 0.036), ('wsd', 0.036), ('named', 0.036), ('book', 0.035), ('books', 0.034), ('infrequent', 0.033), ('shares', 0.033), ('resolving', 0.033), ('instances', 0.033), ('wish', 0.032), ('extraction', 0.032), ('association', 0.032), ('solely', 0.032), ('mind', 0.032), ('asian', 0.032), ('returned', 0.031), ('domains', 0.031), ('initial', 0.031), ('zooms', 0.031), ('almaden', 0.031), ('avialable', 0.031), ('byu', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="62-tfidf-1" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>Author: Tyler Baldwin ; Yunyao Li ; Bogdan Alexe ; Ioana R. Stanoi</p><p>Abstract: While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96.</p><p>2 0.17884426 <a title="62-tfidf-2" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>Author: Ahmet Aker ; Monica Paramita ; Rob Gaizauskas</p><p>Abstract: In this paper we present a method for extracting bilingual terminologies from comparable corpora. In our approach we treat bilingual term extraction as a classification problem. For classification we use an SVM binary classifier and training data taken from the EUROVOC thesaurus. We test our approach on a held-out test set from EUROVOC and perform precision, recall and f-measure evaluations for 20 European language pairs. The performance of our classifier reaches the 100% precision level for many language pairs. We also perform manual evaluation on bilingual terms extracted from English-German term-tagged comparable corpora. The results of this manual evaluation showed 60-83% of the term pairs generated are exact translations and over 90% exact or partial translations.</p><p>3 0.15870254 <a title="62-tfidf-3" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>4 0.15564486 <a title="62-tfidf-4" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>Author: Volkan Cirik</p><p>Abstract: We study substitute vectors to solve the part-of-speech ambiguity problem in an unsupervised setting. Part-of-speech tagging is a crucial preliminary process in many natural language processing applications. Because many words in natural languages have more than one part-of-speech tag, resolving part-of-speech ambiguity is an important task. We claim that partof-speech ambiguity can be solved using substitute vectors. A substitute vector is constructed with possible substitutes of a target word. This study is built on previous work which has proven that word substitutes are very fruitful for part-ofspeech induction. Experiments show that our methodology works for words with high ambiguity.</p><p>5 0.1171542 <a title="62-tfidf-5" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>Author: Marine Carpuat ; Hal Daume III ; Katharine Henry ; Ann Irvine ; Jagadeesh Jagarlamudi ; Rachel Rudinger</p><p>Abstract: Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</p><p>6 0.11012921 <a title="62-tfidf-6" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>7 0.10737292 <a title="62-tfidf-7" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>8 0.10727523 <a title="62-tfidf-8" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>9 0.10190758 <a title="62-tfidf-9" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>10 0.097239435 <a title="62-tfidf-10" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<p>11 0.089493439 <a title="62-tfidf-11" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>12 0.088361405 <a title="62-tfidf-12" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>13 0.08498805 <a title="62-tfidf-13" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>14 0.081312947 <a title="62-tfidf-14" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>15 0.080681764 <a title="62-tfidf-15" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>16 0.080124304 <a title="62-tfidf-16" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>17 0.079534538 <a title="62-tfidf-17" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>18 0.078808986 <a title="62-tfidf-18" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>19 0.077866845 <a title="62-tfidf-19" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>20 0.077067375 <a title="62-tfidf-20" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, 0.111), (2, 0.039), (3, -0.068), (4, 0.076), (5, -0.063), (6, -0.035), (7, 0.092), (8, 0.068), (9, -0.111), (10, -0.011), (11, 0.037), (12, -0.03), (13, -0.008), (14, 0.082), (15, -0.004), (16, 0.016), (17, 0.055), (18, -0.086), (19, -0.051), (20, -0.049), (21, 0.017), (22, 0.015), (23, -0.024), (24, 0.033), (25, -0.067), (26, 0.01), (27, -0.083), (28, 0.049), (29, 0.054), (30, 0.101), (31, 0.039), (32, 0.074), (33, -0.001), (34, -0.002), (35, -0.002), (36, 0.03), (37, 0.036), (38, -0.075), (39, 0.019), (40, -0.003), (41, -0.034), (42, 0.054), (43, -0.058), (44, 0.038), (45, 0.031), (46, 0.059), (47, 0.115), (48, -0.062), (49, -0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95361096 <a title="62-lsi-1" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>Author: Tyler Baldwin ; Yunyao Li ; Bogdan Alexe ; Ioana R. Stanoi</p><p>Abstract: While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96.</p><p>2 0.73460704 <a title="62-lsi-2" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>3 0.67791551 <a title="62-lsi-3" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>4 0.67668587 <a title="62-lsi-4" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>5 0.67522794 <a title="62-lsi-5" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>Author: Marine Carpuat ; Hal Daume III ; Katharine Henry ; Ann Irvine ; Jagadeesh Jagarlamudi ; Rachel Rudinger</p><p>Abstract: Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</p><p>6 0.64016372 <a title="62-lsi-6" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>7 0.59103167 <a title="62-lsi-7" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>8 0.58058482 <a title="62-lsi-8" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>9 0.57266271 <a title="62-lsi-9" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>10 0.56889784 <a title="62-lsi-10" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>11 0.54077435 <a title="62-lsi-11" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>12 0.53383219 <a title="62-lsi-12" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>13 0.53343099 <a title="62-lsi-13" href="./acl-2013-An_Empirical_Study_on_Uncertainty_Identification_in_Social_Media_Context.html">45 acl-2013-An Empirical Study on Uncertainty Identification in Social Media Context</a></p>
<p>14 0.53143978 <a title="62-lsi-14" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>15 0.52629846 <a title="62-lsi-15" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>16 0.52011985 <a title="62-lsi-16" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>17 0.51809806 <a title="62-lsi-17" href="./acl-2013-Aid_is_Out_There%3A_Looking_for_Help_from_Tweets_during_a_Large_Scale_Disaster.html">42 acl-2013-Aid is Out There: Looking for Help from Tweets during a Large Scale Disaster</a></p>
<p>18 0.49933127 <a title="62-lsi-18" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>19 0.49850836 <a title="62-lsi-19" href="./acl-2013-Resolving_Entity_Morphs_in_Censored_Data.html">301 acl-2013-Resolving Entity Morphs in Censored Data</a></p>
<p>20 0.49803925 <a title="62-lsi-20" href="./acl-2013-GlossBoot%3A_Bootstrapping_Multilingual_Domain_Glossaries_from_the_Web.html">170 acl-2013-GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.058), (6, 0.041), (11, 0.071), (24, 0.056), (26, 0.066), (28, 0.01), (29, 0.015), (35, 0.099), (37, 0.136), (42, 0.062), (48, 0.107), (64, 0.011), (70, 0.034), (88, 0.046), (90, 0.015), (95, 0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91263628 <a title="62-lda-1" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>Author: Tyler Baldwin ; Yunyao Li ; Bogdan Alexe ; Ioana R. Stanoi</p><p>Abstract: While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96.</p><p>2 0.83617818 <a title="62-lda-2" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>Author: Olivier Ferret</p><p>Abstract: Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.</p><p>3 0.83459461 <a title="62-lda-3" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>Author: Hongliang Yu ; Zhi-Hong Deng ; Shiyingxue Li</p><p>Abstract: Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications. Most existing researches exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words.</p><p>4 0.83188367 <a title="62-lda-4" href="./acl-2013-Online_Relative_Margin_Maximization_for_Statistical_Machine_Translation.html">264 acl-2013-Online Relative Margin Maximization for Statistical Machine Translation</a></p>
<p>Author: Vladimir Eidelman ; Yuval Marton ; Philip Resnik</p><p>Abstract: Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. However, these solutions are impractical in complex structured prediction problems such as statistical machine translation. We present an online gradient-based algorithm for relative margin maximization, which bounds the spread ofthe projected data while maximizing the margin. We evaluate our optimizer on Chinese-English and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant im- provements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set.</p><p>5 0.8310923 <a title="62-lda-5" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>6 0.83068061 <a title="62-lda-6" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>7 0.82929778 <a title="62-lda-7" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>8 0.8280977 <a title="62-lda-8" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>9 0.82710296 <a title="62-lda-9" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>10 0.82626009 <a title="62-lda-10" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>11 0.82434398 <a title="62-lda-11" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>12 0.82339323 <a title="62-lda-12" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>13 0.82176447 <a title="62-lda-13" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>14 0.82173061 <a title="62-lda-14" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>15 0.82163763 <a title="62-lda-15" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>16 0.82036686 <a title="62-lda-16" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>17 0.81953651 <a title="62-lda-17" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>18 0.81916487 <a title="62-lda-18" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>19 0.818905 <a title="62-lda-19" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>20 0.81692404 <a title="62-lda-20" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
