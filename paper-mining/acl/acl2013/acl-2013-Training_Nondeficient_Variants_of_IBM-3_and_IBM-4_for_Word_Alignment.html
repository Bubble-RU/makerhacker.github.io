<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-354" href="#">acl2013-354</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</h1>
<br/><p>Source: <a title="acl-2013-354-pdf" href="http://aclweb.org/anthology//P/P13/P13-1003.pdf">pdf</a></p><p>Author: Thomas Schoenemann</p><p>Abstract: We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores.</p><p>Reference: <a title="acl-2013-354-reference" href="../acl2013_reference/acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 1 40225 D ¨usseldorf, Germany  Abstract We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. [sent-2, score-0.324]
</p><p>2 Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. [sent-3, score-0.209]
</p><p>3 The arising M-step energies are non-trivial and handled via projected gradient ascent. [sent-4, score-0.278]
</p><p>4 Training the nondeficient IBM-5 in the regular way gives surprisingly good results. [sent-7, score-0.694]
</p><p>5 1 Introduction While most people think of the translation and word alignment models IBM-3 and IBM-4 as inherently deficient models (i. [sent-12, score-0.64]
</p><p>6 models that assign non-zero probability mass to impossible events), in this paper we derive nondeficient variants maintaining their zero order (IBM-3) and first order (IBM-4) parameters. [sent-14, score-0.948]
</p><p>7 likelihoods (the quantity to be minimized) arising in training deficient and nondeficient models (for Europarl German | English, training scheme 15H53545). [sent-18, score-1.106]
</p><p>8 14% of the (HMM) probability mass are covered by the Viterbi alignment and its neighbors. [sent-22, score-0.277]
</p><p>9 With deficient models (and deficient empty words) the final neg-  ative log likelihood is higher than the initial HMM one, with nondeficient models it is lower than for the HMM, as it should be for a better model. [sent-23, score-1.488]
</p><p>10 Having introduced the model variants, we proceed to derive a hillclimbing method to compute a likely alignment (ideally the Viterbi alignment) and its neighbors. [sent-25, score-0.307]
</p><p>11 As for the deficient models, this plays an important role in the E-step of the subsequently derived expectation maximization (EM) training scheme. [sent-26, score-0.389]
</p><p>12 The source code of this project is available in our word alignment software RegAligner1 , version 1. [sent-33, score-0.18]
</p><p>13 Figure 1 gives a first demonstration of how much the proposed variants differ from the standard models by visualizing the resulting negative log likelihoods2, the quantity to be minimized in EM-training. [sent-35, score-0.211]
</p><p>14 The nondeficient IBM-4 derives a lower negative log likelihood than the HMM, the regular deficient variant only a lower one than the IBM-1. [sent-36, score-1.134]
</p><p>15 14% of the probability mass3 are preserved when using the Viterbi alignment and its neighbors instead of all alignments. [sent-38, score-0.244]
</p><p>16 Indeed, it is widely recognized that with proper initialization fertility based models outperform sequence based ones. [sent-39, score-0.208]
</p><p>17 In particular, sequence based models can simply ignore a part of the sentence to be conditioned on, while fertility based models explicitly factor in a probability of words in this sentence to have no aligned words (or any other number of aligned words, called the fertility). [sent-40, score-0.355]
</p><p>18 Hence, it is encouraging to see that the nondeficient IBM-4 indeed derives a higher likelihood than the sequence based HMM. [sent-41, score-0.715]
</p><p>19 –  –  Related Work Today’s most widely used models for word alignment are still the models IBM 1-5 of Brown et al. [sent-42, score-0.248]
</p><p>20 While it is known that fertilitybased models outperform sequence-based ones, the large bulk of word alignment literature following these publications has mostly ignored fertilitybased models. [sent-45, score-0.32]
</p><p>21 One reason for the lack of interest is surely that computing expectations and Viterbi alignments for these models is a hard problem (Udupa and Maji, 2006). [sent-47, score-0.234]
</p><p>22 2Note that the figure slightly favors IBM-1 and HMM as for them the length J of the foreign sequence is assumed to be known whereas IBM-3 and IBM-4 explicitly predict it. [sent-50, score-0.222]
</p><p>23 The number is not entirely fair as alignments where more than half the words align to the empty word are assigned a probability of 0. [sent-54, score-0.231]
</p><p>24 In addition, some rather complex models have been proposed that usually aim to replace the fertility based models (Wang and Waibel, 1998; Fraser and Marcu, 2007a). [sent-60, score-0.205]
</p><p>25 , 1993) who proposed to use the deficient models IBM-3 and IBM-4 to initialize the nondeficient IBM-5. [sent-72, score-1.008]
</p><p>26 The resulting alignment is then written as a vectora1J, whereeach aj takes integral values between 0 and I, with 0 indicating that fj  has no English correspondence. [sent-80, score-0.256]
</p><p>27 The fertility-based models IBM-3, IBM-4 and IBM-5 factor the (conditional) probability p(f1J, aJ1|e1I) of obtaining an alignment and a translatio|ne given an English sentence according to the following generative story: 23  1. [sent-81, score-0.278]
</p><p>28 , I, decide on the number Φi of foreign words aligned to ei. [sent-85, score-0.233]
</p><p>29 Choose the number Φ0 of unaligned words in the (still unknown) foreign sequence. [sent-89, score-0.228]
</p><p>30 Since each foreign word belongs tPo exactly one English position (including 0), Pthe foreignP Psequence is now known to  PiI=1  be of length J  = PiI=0 Φi. [sent-91, score-0.258]
</p><p>31 , Φi decide on (a) the identity fi,k of the next foreign P. [sent-97, score-0.19]
</p><p>32 ,  Choose with probability di,1, di,k−1, fi,k) p(fi,k |ei), where di comprises all di,k for word |ie (see point b) below) and fi,k comprises all foreign words known at that point. [sent-102, score-0.346]
</p><p>33 (b) the position di,k of the just generated foreign word fi,k, with probability ei. [sent-103, score-0.29]
</p><p>34 The remaining Φ0 open positions in the foreign sequence align to position 0. [sent-112, score-0.295]
</p><p>35 Decide on the corresponding foreign words with  pw(ofrdd0,”k. [sent-113, score-0.19]
</p><p>36 |e0), where e0is an artificial “empty To model the probability for the number of unaligned words in step 2, each of the Φi properly aligned foreign words generatePs an unaligned foreign word with probability p0, resulting in  PiI=1  p? [sent-114, score-0.627]
</p><p>37 The main diffQerence between IBM-3, IBM-4 and IBM-5 is the cQhoice of probability model in step 3 b), called a distortion model. [sent-128, score-0.159]
</p><p>38 Since most of the context to be conditioned on is ignored, this allows invalid configurations to occur with non-zero probability: some foreign positions can be chosen several times, while others remain empty. [sent-132, score-0.259]
</p><p>39 To prevent this it is common to make this model deficient as well (Och and Ney, 2003), which improves performance immensely and gives much better results than simply fixing p0 in the original model. [sent-135, score-0.31]
</p><p>40 ways to generate the same alignment a1J (Qwhere the Φi are the fertilities induced by a1J). [sent-139, score-0.18]
</p><p>41 The arising M-step energy eis aarddamreestseerds by projected gradient ascent (see below). [sent-146, score-0.246]
</p><p>42 These parameters are also used for the nondeficient variants. [sent-147, score-0.703]
</p><p>43 2  IBM-4  The distortion model of the IBM-4 is a first order one that generates the di,k of each English position iin ascending order (i. [sent-150, score-0.196]
</p><p>44 is f othr 1en < a kon ≤e-t Φo-one correspondence between alignments a1J and (valid) distortion parameters (di,k)i=1,. [sent-155, score-0.227]
</p><p>45 For position i, let [i] = arg max{i0 | 1≤ i0 < i, Φi0 > 0} pdoesniotitoe4n ith, ele ctl [ois]e=sta preceding 1En≤glish word> t0ha}t dheanso aligned foreign words. [sent-164, score-0.269]
</p><p>46 The aligned foreign positions of [i] are combined into a center position ? [sent-165, score-0.338]
</p><p>47 e distortion probability for the first word (k = 1) is p=1(di,1|? [sent-168, score-0.159]
</p><p>48 [i], A(fi,1), B(e[i]), J)  ,  where A gives the word class of a foreign word awnhde rBe t hAe wgivoresd c thlaess w oofr an English wa foordre (there are typically 5 w0o crdla scsleass per language, dweorirvde (dth by machine learning techniques). [sent-169, score-0.19]
</p><p>49 Reducing Deficiency In this paper, we also investigate the effect of reducing the amount of wasted probability mass by enforcing the dependence on J by proper renormalization, i. [sent-177, score-0.227]
</p><p>50 Therefore, here, too, the probability for Φ0 has to be made deficient to get good performance. [sent-184, score-0.416]
</p><p>51 3 IBM-5 We note in passing that the distortion model of the IBM-5 is nondeficient and has parameters for filling the nth open gap in the foreign sequence given that there are N positions to choose from see the next section for exactly what positions one can choose from. [sent-190, score-1.126]
</p><p>52 There is also a dependence on word classes for the foreign language. [sent-191, score-0.25]
</p><p>53 This motivated us to instead reformulate IBM-3 and IBM-4 as nondeficient models. [sent-194, score-0.664]
</p><p>54 A central concept for the generation of di,k in step 3(b) is the set of positions in the foreign sequence that are still without alignment. [sent-197, score-0.259]
</p><p>55 It is tempting to think that in a nondeficient model all members of Ji,k,J can be chosen for 25  di,k, but this holds only Φi = 1. [sent-202, score-0.74]
</p><p>56 With that, we can state the nondeficient variants of IBM-3 and IBM-4. [sent-206, score-0.752]
</p><p>57 To get a nondeficient variant, it remains to renormalize, resulting in  p(di,k= j|i,JiΦ,ki,J) =PjJq=(1jq|i(,jJ|iiΦ,,kJi,Ji,Φ)ki,J). [sent-210, score-0.706]
</p><p>58 Lastly, here we use the original nondeficient empty word model p(Φ0 | Φi), resulting in a totally nondeficient mode|lP. [sent-213, score-1.368]
</p><p>59 2 Nondeficient IBM-4 With the notation set up, it is rather straightforward to derive a nondeficient variant of the IBM4. [sent-215, score-0.727]
</p><p>60 p>1(j −0 di,k−1|α) eifls je ∈ , Ji,Φki,J from which the nondeficient distribution p>1(di,k=j|di,k−1,α, is again obtained by renormal=izja|tdion. [sent-226, score-0.736]
</p><p>61 When pθ (·) denotes a fertility based model the resulting problem oist a non-concave smedax mimodizealti thoen problem with many local minima and no (known) closed-form solutions. [sent-237, score-0.17]
</p><p>62 As in the training of the deficient IBM-3 and IBM-4 models, we approximate the expectations in the E-step by a set of likely alignments, ideally centered around the Viterbi alignment, but already for the regular deficient variants computing it is NP-hard (Udupa and Maji, 2006). [sent-242, score-0.845]
</p><p>63 This task is also needed for the actual task of word alignment (annotating a given sentence pair with an alignment). [sent-244, score-0.18]
</p><p>64 1 Alignment Computation For computing alignments, we use the common procedure of hillclimbing where we start with an alignment, then iteratively compute the probabilities of all alignments differing by a move or a swap (Brown et al. [sent-246, score-0.346]
</p><p>65 Since we cannot ignore parts of the history and still get a nondeficient model, computing the probabilities of the neighbors cannot be handled incrementally (or rather only partially, for the dictionary and fertility models). [sent-248, score-0.92]
</p><p>66 26  For self-containment, we recall here that for an alignment a1J applying the move a1J[j → i] results in the alignment a1J defined by aj =i[ ja n→d aj0 =aj0 for j0 =j. [sent-250, score-0.534]
</p><p>67 Applying the swap a1J[j1 ↔ j2] results in the alignment ain1Jg d tehfein sewda by aj1 =aj2, aj2 =aj1 and aj0 = aj0 elsewhere. [sent-251, score-0.246]
</p><p>68 If a1J is the alignment produced by hillclimbing, the move matrix m ∈ pI RroJd×uIc+e1d di sb definedby mj,i being theprobability o ∈f a1J[j → i] aslongas aj i,otherwise0. [sent-252, score-0.269]
</p><p>69 The move and swap m jatrices are uased to approximate expectations in EM training (see below). [sent-255, score-0.23]
</p><p>70 the same problem Och and Ney (2003) found for the deficient models. [sent-260, score-0.31]
</p><p>71 Therefore, executing an iteration of EM requires first calculating all expectations (E-step) and then solving the maximization problems (M-step). [sent-286, score-0.204]
</p><p>72 problems for the translation probabilities and the fertility probabilities yield the known standard update rules. [sent-304, score-0.269]
</p><p>73 In the deficient setting, the problem for each iis  {mp(ja|xi)}XJwi,j,Jlog PjJ0p=(1j|pi()j0|i)! [sent-306, score-0.31]
</p><p>74 In the nondeficient setting, wPe now drop the subscripts i, k, J and the superscript Φ from the sets defined in the previous sections, i. [sent-307, score-0.664]
</p><p>75 However, since we approximate expectations from the move and swap matrices, and hence by O((I + J) · J) alignments per sentence pair, biny t Ohe( (eInd + we get a polynomial pneurm sebenrte onfc tee prmaisr,. [sent-314, score-0.365]
</p><p>76 For both the deficient and the nondeficient variants, the M-step problems for the distortion parameters p(j |i) are non-trivial, non-concave and rhaamvee no (known) eclo nsoend- tfroivrmial ,so nlounti-ocnosn. [sent-317, score-1.108]
</p><p>77 aWvee approach them via the method of projected gradient ascent (PGA), where the gradient for the nondeficient problem is  ∂∂p(Eji|i)=JX:j∈J"pw(j ,|Ji)−PPjj0∈0∈JJpw(j 00,|Ji)#. [sent-318, score-0.934]
</p><p>78 For p=1 (·) we have one problem (f·o)r aenadch p foreign Fcolras ps α (an·)d w weaec hha English pcrloasbsβ, of the form  {p=1(mj|ajx0,α,β)}jX,j0,Jwj,j0,J,α,βlog? [sent-335, score-0.19]
</p><p>79 For the IBM-3, the nondeficient variant is clearly best. [sent-359, score-0.698]
</p><p>80 for  the nondeficient variant, with p=1 (j |j0, α, β, J) based on (7). [sent-361, score-0.664]
</p><p>81 F(ojr p>1 (·) we bhaasveed one problem per foreign class α, of t(·h)e fwoerm h  {p>1m(ja|xj0,α)}jX,j0,Jwj,j0,J,αlog? [sent-362, score-0.19]
</p><p>82 for reduced deficiency, with p>1 (j |j0, α, J) based on (4), and for the nondeficient (vja|rjiant it has the form  {p>1m(ja|jx0,α)}j,Xj0,Jwj,j0,J,αlog? [sent-364, score-0.698]
</p><p>83 We evaluate alignment accuracies on gold alignments6 in the form of weighted F-measures with α = 0. [sent-370, score-0.18]
</p><p>84 Therefore, we compare to the deficient models in our own software as well as to those in GIZA++. [sent-380, score-0.344]
</p><p>85 The first iteration of the IBM-3 collects counts from the HMM, and likewise the first iteration of the IBM-4 collects counts from the IBM3 (in both cases the move and swap matrices are filled with probabilities of the former model, then theses matrices are used as in a regular model iteration). [sent-382, score-0.291]
</p><p>86 A nondeficient IBM-4 is always initialized by a nondeficient IBM-3. [sent-383, score-1.328]
</p><p>87 1 Alignment Accuracy The alignment accuracies weighted F-measures with α = 0. [sent-394, score-0.18]
</p><p>88 Clearly, nondeficiency greatly improves the accuracy of the IBM3, both compared to our deficient implementation and that of GIZA++. [sent-396, score-0.365]
</p><p>89 For the IBM-4 we get improvements for the nondeficient variant in roughly half the cases, both with and without word classes. [sent-397, score-0.774]
</p><p>90 Interestingly, also the reduced deficient IBM-4 is not always better than the fully deficient variant. [sent-399, score-0.654]
</p><p>91 There is also quite some surprise regarding the IBM-5: contrary to the findings of (Och and Ney, 2003) the IBM-5 in GIZA++ performs best in three out of four cases - when competing with both deficient and nondeficient variants of IBM-3 and IBM-4. [sent-401, score-1.062]
</p><p>92 2  Effect on Translation Performance  We also check the effect of the various alignments (all produced by RegAligner) on translation performance for phrase-based translation, randomly choosing translation from German to English. [sent-404, score-0.165]
</p><p>93 6  Conclusion  We have shown that the word alignment models IBM-3 and IBM-4 can be turned into nondeficient  HMModMel-#ClassesnDoenfdiceiefinccieynt2B9L. [sent-411, score-0.878]
</p><p>94 Here we have exploited that the models are proper applications of the chain rule of probabilities, where deficiency is only introduced by ignoring parts of the history for the distortion factors in  the factorization. [sent-426, score-0.243]
</p><p>95 By proper renormalization the desired nondeficient variants are obtained. [sent-427, score-0.854]
</p><p>96 In the E-step we use hillclimbing to get a likely alignment (ideally the Viterbi alignment). [sent-429, score-0.32]
</p><p>97 Nevertheless, we think that nondeficiency in fertility based models is an important issue, and that at the very least our paper is of theoretical value. [sent-437, score-0.272]
</p><p>98 An alignment algorithm using Belief Propagation and a structurebased distortion model. [sent-463, score-0.275]
</p><p>99 HMM word and phrase alignment for statistical machine translation. [sent-479, score-0.18]
</p><p>100 Smaller alignment models for better translations: Unsupervised word alignment with the l0-norm. [sent-601, score-0.394]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nondeficient', 0.664), ('deficient', 0.31), ('foreign', 0.19), ('alignment', 0.18), ('fertility', 0.137), ('hmm', 0.116), ('pii', 0.111), ('expectations', 0.107), ('hillclimbing', 0.098), ('distortion', 0.095), ('alignments', 0.093), ('ei', 0.092), ('variants', 0.088), ('ja', 0.085), ('em', 0.082), ('gradient', 0.078), ('deficiency', 0.077), ('pk', 0.072), ('ji', 0.07), ('positions', 0.069), ('swap', 0.066), ('renormalization', 0.065), ('ascending', 0.065), ('projected', 0.064), ('probability', 0.064), ('dependence', 0.06), ('viterbi', 0.059), ('move', 0.057), ('maji', 0.055), ('nondeficiency', 0.055), ('pga', 0.055), ('regaligner', 0.055), ('schoenemann', 0.055), ('xas', 0.055), ('xhxsxp', 0.055), ('arising', 0.054), ('iteration', 0.053), ('udupa', 0.052), ('likelihood', 0.051), ('parametric', 0.05), ('ascent', 0.05), ('fs', 0.048), ('think', 0.046), ('handled', 0.045), ('ilog', 0.045), ('log', 0.045), ('quantity', 0.044), ('maximization', 0.044), ('fj', 0.044), ('aligned', 0.043), ('get', 0.042), ('ney', 0.042), ('giza', 0.041), ('empty', 0.04), ('vaswani', 0.04), ('och', 0.039), ('parameters', 0.039), ('es', 0.039), ('fraser', 0.039), ('je', 0.039), ('unaligned', 0.038), ('proper', 0.037), ('energies', 0.037), ('fertilitybased', 0.037), ('mstep', 0.037), ('qii', 0.037), ('usseldorf', 0.037), ('pi', 0.037), ('brown', 0.037), ('position', 0.036), ('zero', 0.036), ('translation', 0.036), ('expectation', 0.035), ('reduced', 0.034), ('variant', 0.034), ('models', 0.034), ('half', 0.034), ('mp', 0.033), ('xi', 0.033), ('wasted', 0.033), ('eaj', 0.033), ('eifls', 0.033), ('minima', 0.033), ('cromi', 0.033), ('eres', 0.033), ('mass', 0.033), ('probabilities', 0.032), ('aj', 0.032), ('gb', 0.032), ('known', 0.032), ('auxiliary', 0.031), ('comprises', 0.03), ('ithe', 0.03), ('tempting', 0.03), ('ois', 0.03), ('regular', 0.03), ('xs', 0.03), ('europarl', 0.029), ('derive', 0.029), ('marcu', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="354-tfidf-1" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>Author: Thomas Schoenemann</p><p>Abstract: We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores.</p><p>2 0.22970404 <a title="354-tfidf-2" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>Author: Chris Quirk</p><p>Abstract: The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.</p><p>3 0.12695554 <a title="354-tfidf-3" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>4 0.11671099 <a title="354-tfidf-4" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>5 0.10727608 <a title="354-tfidf-5" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>6 0.10574722 <a title="354-tfidf-6" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>7 0.10495114 <a title="354-tfidf-7" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>8 0.096610725 <a title="354-tfidf-8" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>9 0.088437922 <a title="354-tfidf-9" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>10 0.085017242 <a title="354-tfidf-10" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>11 0.085012347 <a title="354-tfidf-11" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>12 0.083216906 <a title="354-tfidf-12" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>13 0.077436082 <a title="354-tfidf-13" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>14 0.07680247 <a title="354-tfidf-14" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>15 0.075297043 <a title="354-tfidf-15" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>16 0.067657024 <a title="354-tfidf-16" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>17 0.067379229 <a title="354-tfidf-17" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>18 0.066701613 <a title="354-tfidf-18" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>19 0.064964078 <a title="354-tfidf-19" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>20 0.061653245 <a title="354-tfidf-20" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, -0.078), (2, 0.078), (3, 0.04), (4, -0.011), (5, -0.006), (6, -0.014), (7, 0.0), (8, -0.058), (9, -0.058), (10, 0.035), (11, -0.161), (12, -0.024), (13, -0.119), (14, 0.0), (15, -0.03), (16, 0.056), (17, 0.011), (18, 0.045), (19, -0.116), (20, -0.054), (21, 0.016), (22, 0.02), (23, -0.008), (24, -0.011), (25, 0.053), (26, -0.091), (27, 0.038), (28, -0.019), (29, -0.005), (30, -0.004), (31, -0.012), (32, -0.001), (33, 0.034), (34, -0.015), (35, -0.013), (36, -0.001), (37, 0.035), (38, 0.037), (39, 0.02), (40, -0.032), (41, -0.003), (42, 0.039), (43, -0.019), (44, 0.088), (45, -0.014), (46, -0.029), (47, -0.036), (48, -0.075), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92994261 <a title="354-lsi-1" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>Author: Thomas Schoenemann</p><p>Abstract: We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores.</p><p>2 0.87014449 <a title="354-lsi-2" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>Author: Chris Quirk</p><p>Abstract: The notion of fertility in word alignment (the number of words emitted by a single state) is useful but difficult to model. Initial attempts at modeling fertility used heuristic search methods. Recent approaches instead use more principled approximate inference techniques such as Gibbs sampling for parameter estimation. Yet in practice we also need the single best alignment, which is difficult to find using Gibbs. Building on recent advances in dual decomposition, this paper introduces an exact algorithm for finding the single best alignment with a fertility HMM. Finding the best alignment appears important, as this model leads to a substantial improvement in alignment quality.</p><p>3 0.77907115 <a title="354-lsi-3" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>4 0.77885181 <a title="354-lsi-4" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>5 0.7644968 <a title="354-lsi-5" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>6 0.7243259 <a title="354-lsi-6" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>7 0.67107421 <a title="354-lsi-7" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>8 0.62283266 <a title="354-lsi-8" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>9 0.60924906 <a title="354-lsi-9" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>10 0.59975624 <a title="354-lsi-10" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>11 0.55671078 <a title="354-lsi-11" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>12 0.5561046 <a title="354-lsi-12" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>13 0.5371545 <a title="354-lsi-13" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>14 0.52746439 <a title="354-lsi-14" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>15 0.52314788 <a title="354-lsi-15" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>16 0.4971073 <a title="354-lsi-16" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>17 0.47972527 <a title="354-lsi-17" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>18 0.47468716 <a title="354-lsi-18" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>19 0.46776035 <a title="354-lsi-19" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>20 0.46046984 <a title="354-lsi-20" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.075), (6, 0.093), (11, 0.053), (24, 0.027), (26, 0.041), (28, 0.01), (35, 0.058), (42, 0.05), (48, 0.369), (70, 0.04), (88, 0.037), (90, 0.022), (95, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98224783 <a title="354-lda-1" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>Author: Jun Suzuki ; Masaaki Nagata</p><p>Abstract: This paper proposes a framework of supervised model learning that realizes feature grouping to obtain lower complexity models. The main idea of our method is to integrate a discrete constraint into model learning with the help of the dual decomposition technique. Experiments on two well-studied NLP tasks, dependency parsing and NER, demonstrate that our method can provide state-of-the-art performance even if the degrees of freedom in trained models are surprisingly small, i.e., 8 or even 2. This significant benefit enables us to provide compact model representation, which is especially useful in actual use.</p><p>2 0.9547956 <a title="354-lda-2" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>Author: Xiaorui Jiang ; Xiaoping Sun ; Hai Zhuge</p><p>Abstract: School of thought analysis is an important yet not-well-elaborated scientific knowledge discovery task. This paper makes the first attempt at this problem. We focus on one aspect of the problem: do characteristic school-of-thought words exist and whether they are characterizable? To answer these questions, we propose a probabilistic generative School-Of-Thought (SOT) model to simulate the scientific authoring process based on several assumptions. SOT defines a school of thought as a distribution of topics and assumes that authors determine the school of thought for each sentence before choosing words to deliver scientific ideas. SOT distinguishes between two types of school-ofthought words for either the general background of a school of thought or the original ideas each paper contributes to its school of thought. Narrative and quantitative experiments show positive and promising results to the questions raised above. 1</p><p>3 0.94678581 <a title="354-lda-3" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>Author: Volkan Cirik</p><p>Abstract: We study substitute vectors to solve the part-of-speech ambiguity problem in an unsupervised setting. Part-of-speech tagging is a crucial preliminary process in many natural language processing applications. Because many words in natural languages have more than one part-of-speech tag, resolving part-of-speech ambiguity is an important task. We claim that partof-speech ambiguity can be solved using substitute vectors. A substitute vector is constructed with possible substitutes of a target word. This study is built on previous work which has proven that word substitutes are very fruitful for part-ofspeech induction. Experiments show that our methodology works for words with high ambiguity.</p><p>same-paper 4 0.93632197 <a title="354-lda-4" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>Author: Thomas Schoenemann</p><p>Abstract: We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores.</p><p>5 0.91959012 <a title="354-lda-5" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>Author: Tiziano Flati ; Roberto Navigli</p><p>Abstract: We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best f∗it) tahned ∗ argument. Taon idco this, we extract failtl thhee ∗ occurrences ion Wikipedia ewxthraiccht match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach.</p><p>6 0.89425611 <a title="354-lda-6" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>7 0.7422424 <a title="354-lda-7" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<p>8 0.71216697 <a title="354-lda-8" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>9 0.711474 <a title="354-lda-9" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>10 0.70003074 <a title="354-lda-10" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>11 0.69615835 <a title="354-lda-11" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>12 0.69262302 <a title="354-lda-12" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>13 0.68805724 <a title="354-lda-13" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>14 0.68463373 <a title="354-lda-14" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>15 0.67090219 <a title="354-lda-15" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>16 0.66811544 <a title="354-lda-16" href="./acl-2013-Decipherment_Complexity_in_1%3A1_Substitution_Ciphers.html">109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</a></p>
<p>17 0.66473275 <a title="354-lda-17" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>18 0.66377389 <a title="354-lda-18" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>19 0.65985721 <a title="354-lda-19" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>20 0.65801871 <a title="354-lda-20" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
