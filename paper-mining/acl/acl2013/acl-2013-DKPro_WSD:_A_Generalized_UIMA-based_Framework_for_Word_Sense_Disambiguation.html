<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-105" href="#">acl2013-105</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</h1>
<br/><p>Source: <a title="acl-2013-105-pdf" href="http://aclweb.org/anthology//P/P13/P13-4007.pdf">pdf</a></p><p>Author: Tristan Miller ; Nicolai Erbs ; Hans-Peter Zorn ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</p><p>Reference: <a title="acl-2013-105-reference" href="../acl2013_reference/acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. [sent-3, score-0.708]
</p><p>2 This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. [sent-4, score-0.086]
</p><p>3 DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. [sent-6, score-0.347]
</p><p>4 Its UIMA-based architecture makes it easy to add support for new  resources and algorithms. [sent-7, score-0.092]
</p><p>5 Related tasks such as word sense induction and entity linking are also supported. [sent-8, score-0.293]
</p><p>6 1 Introduction Word sense disambiguation, or WSD (Agirre and Edmonds, 2006)—the task of determining which of a word’s senses is the one intended in a particular context—has been a core research problem in computational linguistics since the very inception of the field. [sent-9, score-0.295]
</p><p>7 Despite the task’s importance and popularity as a subject of study, tools and resources supporting WSD have seen relatively little generalization and standardization. [sent-10, score-0.028]
</p><p>8 That is, most prior implementations of WSD systems have been hard-coded for particular algorithms, sense inventories, and data sets. [sent-11, score-0.311]
</p><p>9 In this paper we present DKPro WSD, a general-purpose framework for word sense disambiguation which is both modular and extensible. [sent-13, score-0.386]
</p><p>10 Its modularity means that it makes a logical sep-  aration between the data sets (e. [sent-14, score-0.037]
</p><p>11 , the lexical-semantic resources enumerating the senses to which words in the corpora are assigned), and the algorithms (i. [sent-20, score-0.148]
</p><p>12 , code which actually performs the sense assignments and prerequisite linguistic annotations), and provides a standard interface for each of these component types. [sent-22, score-0.318]
</p><p>13 Components which provide the same functionality can be freely swapped, so that one can easily run the same algorithm on different data sets (irrespective of which sense inventory they use), or test several different algorithms on the same data set. [sent-23, score-0.456]
</p><p>14 While DKPro WSD ships with support for a number ofcommon WSD algorithms, sense inventories, and data set formats, its extensibility means that it is easy to adapt to work with new methods and resources. [sent-24, score-0.346]
</p><p>15 The system is written in Java and is based on UIMA (Lally et al. [sent-25, score-0.026]
</p><p>16 , 2009), an industry-standard architecture for analysis of unstructured information. [sent-26, score-0.029]
</p><p>17 Support for new corpus  formats, sense inventories, and WSD algorithms can be added by implementing new UIMA components for them, or more conveniently by writing UIMA wrappers around existing code. [sent-27, score-0.438]
</p><p>18 The framework and all existing components are released under the Apache License 2. [sent-28, score-0.056]
</p><p>19 DKPro WSD was designed primarily to support the needs of WSD researchers, who will appreciate the convenience and flexibility it affords in tuning and comparing algorithms and data sets. [sent-30, score-0.121]
</p><p>20 Its support for interactive visualization of the disambiguation process also makes it a powerful tool for learning or teaching the principles of WSD. [sent-32, score-0.21]
</p><p>21 The remainder of this paper is organized as follows: In §2 we review previous work in WSD file floowrmsa:t Isn a §n2d w implementations. [sent-33, score-0.039]
</p><p>22 c e2 A0s1s3oc Aiastsio cnia fotiron C fo mrp Cuotmatpiounta tlio Lninaglu Li sntgicusi,s ptaicgses 37–42, our system and further explain its capabilities and  advantages. [sent-36, score-0.026]
</p><p>23 It was not until the growing availability of large-scale lexical resources and corpora in the 1990s that the need to establish a common platform for the evaluation of WSD systems was recognized. [sent-39, score-0.028]
</p><p>24 Each competition defined a number of tasks with prescribed evaluation metrics, sense inventories, corpus file formats, and human-annotated test sets. [sent-41, score-0.3]
</p><p>25 For each task it was therefore possible to compare algorithms against each other. [sent-42, score-0.086]
</p><p>26 However, sense inventories and file formats still vary across tasks and competitions. [sent-43, score-0.628]
</p><p>27 There are also a number of increasingly popular resources used outside Senseval and SemEval, each with their own formats and struc-  tures: examples of sense-annotated corpora include SemCor (Miller et al. [sent-44, score-0.117]
</p><p>28 , 2012), and sense inventories include VerbNet (Kipper et al. [sent-47, score-0.5]
</p><p>29 , 2010), DANTE (Kilgarriff, 2010), BabelNet (Navigli and Ponzetto, 2012), and online community-produced resources such as Wiktionary and Wikipedia. [sent-49, score-0.028]
</p><p>30 So despite attempts at standardization, the canon of WSD resources remains quite fragmented. [sent-50, score-0.028]
</p><p>31 The few publically available implementations of individual disambiguation algorithms, such as SenseLearner (Mihalcea and Csomai, 2005), SenseRelate::TargetWord (Patwardhan et al. [sent-51, score-0.15]
</p><p>32 , 2005), UKB (Agirre and Soroa, 2009), and IMS (Zhong and Ng, 2010), are all tied to a particular corpus and/or sense inventory, or define their own custom formats into which existing resources must be converted. [sent-52, score-0.378]
</p><p>33 Furthermore, where the algorithm depends on linguistic annotations such as part-of-speech tags, the users are expected to supply these themselves, or else must use the annotators built into the system (which may not always  be appropriate for the corpus language or domain). [sent-53, score-0.08]
</p><p>34 One alternative to coding WSD algorithms from scratch is to use general-purpose NLP toolkits such as NLTK (Bird, 2006) or DKPro (Gurevych et al. [sent-54, score-0.15]
</p><p>35 Such toolkits provide individual components potentially useful for WSD, such as WordNet-based measures of sense similarity and readers for the odd corpus format. [sent-56, score-0.391]
</p><p>36 However, these toolkits are not specifically geared towards development and evaluation of WSD systems; there is no unified type system or architecture which allows WSD-specific components to be combined or substituted orthogonally. [sent-57, score-0.15]
</p><p>37 The only general-purpose dedicated WSD system we are aware of is I Can Sense It (Joshi et al. [sent-58, score-0.026]
</p><p>38 , 2012), a Web-based interface for running and evaluating various WSD algorithms. [sent-59, score-0.03]
</p><p>39 It includes I/O support for several corpus formats and implementations of a number of baseline and state-of-theart disambiguation algorithms. [sent-60, score-0.274]
</p><p>40 However, as with previous single-algorithm systems, it is not possible to select the sense inventory, and the user is responsible for pre-annotating the input text with  POS tags. [sent-61, score-0.261]
</p><p>41 The usability and extensibility of the system are greatly restricted by the fact that it is a proprietary, closed-source application fully hosted by the developers. [sent-62, score-0.076]
</p><p>42 3  DKPro WSD  Our system, DKPro WSD, is implemented as a framework of UIMA components (type systems, collection readers, annotators, CAS consumers, resources) which the user combines into a data processing pipeline. [sent-63, score-0.056]
</p><p>43 We can best illustrate this with an example: Figure 1 shows a pipeline for running two disambiguation algorithms on the Estonian all-words task from Senseval-2. [sent-64, score-0.252]
</p><p>44 UIMA components are the solid, rounded boxes in the lower half of the diagram, and the data and algorithms they encapsulate are the light grey shapes in the upper half. [sent-65, score-0.17]
</p><p>45 The first component of the pipeline is a collection reader which reads the text of the XML-formatted corpus into a CAS (a UIMA data structure for storing layers of data and stand-off annotations) and marks the words  to be disambiguated (the “instances”) with their IDs. [sent-66, score-0.098]
</p><p>46 The next component is an annotator which reads the answer key—a separate file which associates each instance ID with a sense ID from the Estonian EuroWordNet—and adds the goldstandard sense annotations to their respective instances in the CAS. [sent-67, score-0.68]
</p><p>47 Processing then passes to another annotator—in this case a UIMA wrapper for TreeTagger (Schmid, 1994)—which adds POS and lemma annotations to the instances. [sent-68, score-0.081]
</p><p>48 38  Figure 1: A sample DKPro WSD pipeline for the Estonian all-words data set from Senseval-2. [sent-69, score-0.066]
</p><p>49 Then come the two disambiguation algorithms, also modelled as UIMA annotators wrapping nonUIMA-aware algorithms. [sent-70, score-0.1]
</p><p>50 Each WSD annotator iterates over the instances in the CAS and annotates them with sense IDs from EuroWordNet. [sent-71, score-0.294]
</p><p>51 ) Finally, control passes to a CAS consumer which compares the WSD algorithms’ sense annotations against the gold-standard annotations produced by the answer key annotator, and outputs these sense annotations along with various evaluation metrics (precision, recall, etc. [sent-74, score-0.711]
</p><p>52 A pipeline of this sort can be written with just a few lines of code: one or two to declare each component and if necessary bind it to the appropriate resources, and a final one to string the components together into a pipeline. [sent-76, score-0.122]
</p><p>53 Moreover, once such a pipeline is written it is simple to substitute functionally equivalent components. [sent-77, score-0.101]
</p><p>54 For example, with only a few small changes the same pipeline could be used for Senseval-3’s English lexical sample task, which uses a corpus and sense inventory in a different format and language. [sent-78, score-0.436]
</p><p>55 Crucially, none of the WSD algorithms need to be changed. [sent-82, score-0.086]
</p><p>56 The most important features of our system are as follows: Corpora and data sets. [sent-83, score-0.026]
</p><p>57 DKPro WSD currently has collection readers for all Senseval and SemEval all-words and lexical sample tasks, the AIDA CoNLL-YAGO data set (Hoffart et al. [sent-84, score-0.035]
</p><p>58 , 2011), the TAC KBP entity linking tasks (McNamee and Dang, 2009), and the aforementioned MASC, SemCor, and WebCAGe corpora. [sent-85, score-0.032]
</p><p>59 Our prepackaged corpus analysis modules can compute statistics on monosemous terms, average polysemy, terms absent from the sense inventory, etc. [sent-86, score-0.261]
</p><p>60 Sense inventories are abstracted into a system of types and interfaces according to the sort of lexical-semantic information they provide. [sent-88, score-0.265]
</p><p>61 There is currently support for WordNet (Fellbaum, 1998), WordNet++ (Ponzetto and  Navigli, 2010), EuroWordNet (Vossen, 1998), the Turk Bootstrap Word Sense Inventory (Biemann, 2013), and UBY (Gurevych et al. [sent-89, score-0.035]
</p><p>62 The system can automatically convert between various versions of WordNet using the UPC mappings (Daud ´e et al. [sent-91, score-0.026]
</p><p>63 As with sense inventories, WSD algorithms have a type and interface hierarchy according to what knowledge sources they require. [sent-94, score-0.377]
</p><p>64 Algorithms and baselines already implemented include the analytically calculated random sense baseline; the most frequent sense baseline; the original, simplified, extended, and lexically expanded Lesk variants (Miller et al. [sent-95, score-0.522]
</p><p>65 , 2012); various 39  graph connectivity approaches from Navigli and Lapata (2010); Personalized PageRank (Agirre and Soroa, 2009); the supervised TWSI system (Biemann, 2013); and IMS (Zhong and Ng, 2010). [sent-96, score-0.068]
</p><p>66 Our open API permits users to program support for further knowledge-based and supervised algorithms. [sent-97, score-0.035]
</p><p>67 Many WSD algorithms require linguistic annotations from segmenters, lemmatizers, POS taggers, parsers, etc. [sent-99, score-0.14]
</p><p>68 Off-theshelf UIMA components for producing such annotations, such as those provided by DKPro Core (Gurevych et al. [sent-100, score-0.056]
</p><p>69 , 2007), can be used in a DKPro WSD pipeline with little or no adaptation. [sent-101, score-0.066]
</p><p>70 We have enhanced some families of algorithms with animated, interactive visualizations of the disambiguation process. [sent-103, score-0.263]
</p><p>71 For example, Figure 2 shows part of a screenshot from the interactive running of the degree centrality algorithm (Navigli and Lapata, 2010). [sent-104, score-0.068]
</p><p>72 The system is disambiguating the three content words in the sentence “I drink milk with a straw. [sent-105, score-0.097]
</p><p>73 ” Red, green, and blue nodes represent senses (or more specifically, WordNet sense keys) of the words drink, milk, and straw, respectively; grey nodes are senses of other words discovered by traversing semantic re-  lations (represented by arcs) in the sense inventory. [sent-106, score-0.618]
</p><p>74 We have found such visualizations to be invaluable for understanding and debugging algorithms. [sent-109, score-0.037]
</p><p>75 The behaviour of many components (or entire pipelines) can be altered according to various parameters. [sent-111, score-0.056]
</p><p>76 For example, for the degree centrality algorithm one must specify the maximum search depth, the minimum vertex degree, and the context size. [sent-112, score-0.028]
</p><p>77 DKPro WSD can perform a parameter sweep, automatically running the pipeline once for every possible combination of parameters in user-specified ranges and concatenating the results into a table from which the optimal system configurations can be identified. [sent-113, score-0.092]
</p><p>78 There are several reporting tools to support evaluation and error analysis. [sent-115, score-0.035]
</p><p>79 Raw sense assignments can be output in a variety offormats (XML, HTML, CSV, Senseval answer key,  etc. [sent-116, score-0.261]
</p><p>80 ), some of which support colour-coding  to  Figure 2: DKPro WSD’s interactive visualization of a graph connectivity WSD algorithm. [sent-117, score-0.152]
</p><p>81 Figure 3 shows an example of an HTML report produced by the system—on the left is the sense assignment table, in the upper right is a table of evaluation metrics, and in the lower right is a precision–recall graph. [sent-123, score-0.261]
</p><p>82 DKPro WSD also has support for tasks closely related to word sense disambiguation: Entity linking. [sent-124, score-0.296]
</p><p>83 Entity linking (EL) is the task of linking a named entity in a text (e. [sent-125, score-0.064]
</p><p>84 DKPro WSD supports EL-specific sense inventories such as the list of Wikipedia articles used in the Knowledge Base Population workshop of the Text Analysis Conference (TAC  KBP). [sent-133, score-0.5]
</p><p>85 DKPro WSD contains a reader for the TAC KBP data set, components for mapping other sense inventories to the TAC KBP inventory, and evaluation components for the 40  Figure 3: An HTML report produced by DKPro WSD. [sent-135, score-0.612]
</p><p>86 WSD is usually performed with respect to manually created sense inventories such as WordNet. [sent-139, score-0.5]
</p><p>87 In word sense induction (WSI) a sense inventory for target words is automatically constructed from an unlabelled corpus. [sent-140, score-0.631]
</p><p>88 This can be useful for search result clustering, or for general applications of WSD for languages and domains for which a sense inventory is not yet available. [sent-141, score-0.37]
</p><p>89 DKPro WSD supports WSI by providing state-ofthe art WSD algorithms capable of using arbitrary sense inventories, including induced ones. [sent-143, score-0.347]
</p><p>90 It also includes readers and writers for the SemEval-2007 and -2013 WSI data sets. [sent-144, score-0.035]
</p><p>91 4  Conclusions and future work  In this paper we introduced DKPro WSD, a Javaand UIMA-based framework for word sense disambiguation. [sent-145, score-0.261]
</p><p>92 By segregating and providing layers of abstraction for code, data sets, and sense  inventories, DKPro WSD greatly simplifies the comparison of WSD algorithms in heterogeneous scenarios. [sent-147, score-0.347]
</p><p>93 Support for a wide variety ofcommonly used algorithms, data sets, and sense inventories has already been implemented. [sent-148, score-0.5]
</p><p>94 These include implementations or wrappers for further algorithms and for the DANTE and BabelNet sense inventories. [sent-150, score-0.432]
</p><p>95 A Web interface is in the works and should be operational by the time of publication. [sent-151, score-0.03]
</p><p>96 Creating a system for lexical substitutions from scratch using crowdsourcing. [sent-167, score-0.051]
</p><p>97 SenseLearner: Word sense disambiguation for all words in unrestricted text. [sent-242, score-0.361]
</p><p>98 Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. [sent-254, score-0.261]
</p><p>99 An experimental study of graph connectivity for unsupervised word sense disambiguation. [sent-259, score-0.303]
</p><p>100 It Makes Sense: A wide-coverage word sense disambiguation system for free text. [sent-302, score-0.387]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wsd', 0.614), ('dkpro', 0.387), ('sense', 0.261), ('inventories', 0.239), ('uima', 0.192), ('inventory', 0.109), ('senseval', 0.101), ('disambiguation', 0.1), ('formats', 0.089), ('estonian', 0.088), ('algorithms', 0.086), ('kbp', 0.084), ('eurowordnet', 0.075), ('navigli', 0.074), ('gurevych', 0.074), ('tac', 0.072), ('agirre', 0.071), ('webcage', 0.069), ('pipeline', 0.066), ('demos', 0.061), ('iryna', 0.057), ('components', 0.056), ('cas', 0.055), ('annotations', 0.054), ('wsi', 0.053), ('edmonds', 0.053), ('extensibility', 0.05), ('babelnet', 0.05), ('implementations', 0.05), ('wordnet', 0.049), ('zhong', 0.048), ('dante', 0.046), ('daud', 0.046), ('jmwnl', 0.046), ('masc', 0.046), ('pazienza', 0.046), ('senselearner', 0.046), ('senserelate', 0.046), ('targetword', 0.046), ('biemann', 0.043), ('connectivity', 0.042), ('torsten', 0.041), ('germanet', 0.04), ('semcor', 0.04), ('henrich', 0.04), ('milk', 0.04), ('tristan', 0.04), ('uby', 0.04), ('interactive', 0.04), ('miller', 0.039), ('file', 0.039), ('ponzetto', 0.039), ('toolkits', 0.039), ('framenet', 0.038), ('visualizations', 0.037), ('keys', 0.037), ('modularity', 0.037), ('substitute', 0.035), ('wrappers', 0.035), ('kipper', 0.035), ('readers', 0.035), ('semeval', 0.035), ('visualization', 0.035), ('support', 0.035), ('senses', 0.034), ('annotator', 0.033), ('linking', 0.032), ('soroa', 0.032), ('reads', 0.032), ('darmstadt', 0.032), ('karin', 0.032), ('vossen', 0.032), ('drink', 0.031), ('hoffart', 0.031), ('interface', 0.03), ('treetagger', 0.03), ('lally', 0.03), ('mcnamee', 0.03), ('ruppenhofer', 0.03), ('verbnet', 0.03), ('roberto', 0.029), ('architecture', 0.029), ('centrality', 0.028), ('nltk', 0.028), ('grey', 0.028), ('wiktionary', 0.028), ('resources', 0.028), ('html', 0.027), ('washington', 0.027), ('code', 0.027), ('passes', 0.027), ('ide', 0.026), ('el', 0.026), ('system', 0.026), ('fellbaum', 0.026), ('pagerank', 0.026), ('patwardhan', 0.025), ('scratch', 0.025), ('modular', 0.025), ('eneko', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999779 <a title="105-tfidf-1" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>Author: Tristan Miller ; Nicolai Erbs ; Hans-Peter Zorn ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</p><p>2 0.35290334 <a title="105-tfidf-2" href="./acl-2013-DKPro_Similarity%3A_An_Open_Source_Framework_for_Text_Similarity.html">104 acl-2013-DKPro Similarity: An Open Source Framework for Text Similarity</a></p>
<p>Author: Daniel Bar ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: We present DKPro Similarity, an open source framework for text similarity. Our goal is to provide a comprehensive repository of text similarity measures which are implemented using standardized interfaces. DKPro Similarity comprises a wide variety of measures ranging from ones based on simple n-grams and common subsequences to high-dimensional vector comparisons and structural, stylistic, and phonetic measures. In order to promote the reproducibility of experimental results and to provide reliable, permanent experimental conditions for future studies, DKPro Similarity additionally comes with a set of full-featured experimental setups which can be run out-of-the-box and be used for future systems to built upon.</p><p>3 0.26304674 <a title="105-tfidf-3" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>4 0.23423588 <a title="105-tfidf-4" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>5 0.22055423 <a title="105-tfidf-5" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>6 0.12793033 <a title="105-tfidf-6" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>7 0.11486369 <a title="105-tfidf-7" href="./acl-2013-Development_and_Analysis_of_NLP_Pipelines_in_Argo.html">118 acl-2013-Development and Analysis of NLP Pipelines in Argo</a></p>
<p>8 0.10786251 <a title="105-tfidf-8" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>9 0.10727523 <a title="105-tfidf-9" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>10 0.10605069 <a title="105-tfidf-10" href="./acl-2013-Extending_an_interoperable_platform_to_facilitate_the_creation_of_multilingual_and_multimodal_NLP_applications.html">150 acl-2013-Extending an interoperable platform to facilitate the creation of multilingual and multimodal NLP applications</a></p>
<p>11 0.1044388 <a title="105-tfidf-11" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>12 0.093337983 <a title="105-tfidf-12" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>13 0.091911793 <a title="105-tfidf-13" href="./acl-2013-WebAnno%3A_A_Flexible%2C_Web-based_and_Visually_Supported_System_for_Distributed_Annotations.html">385 acl-2013-WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations</a></p>
<p>14 0.087576136 <a title="105-tfidf-14" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>15 0.086589232 <a title="105-tfidf-15" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>16 0.082700662 <a title="105-tfidf-16" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>17 0.080715455 <a title="105-tfidf-17" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>18 0.066616826 <a title="105-tfidf-18" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>19 0.064019665 <a title="105-tfidf-19" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>20 0.06268087 <a title="105-tfidf-20" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, 0.06), (2, 0.033), (3, -0.161), (4, -0.023), (5, -0.18), (6, -0.166), (7, 0.066), (8, 0.086), (9, -0.046), (10, -0.028), (11, 0.121), (12, -0.187), (13, -0.067), (14, 0.153), (15, 0.013), (16, 0.037), (17, 0.126), (18, -0.071), (19, -0.028), (20, -0.034), (21, -0.063), (22, -0.076), (23, -0.001), (24, -0.129), (25, -0.205), (26, 0.13), (27, -0.054), (28, -0.031), (29, -0.133), (30, 0.08), (31, 0.076), (32, -0.092), (33, 0.006), (34, -0.016), (35, 0.048), (36, -0.078), (37, 0.067), (38, -0.033), (39, -0.004), (40, -0.034), (41, -0.045), (42, -0.025), (43, -0.07), (44, 0.049), (45, 0.049), (46, 0.008), (47, 0.026), (48, -0.002), (49, 0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96715814 <a title="105-lsi-1" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>Author: Tristan Miller ; Nicolai Erbs ; Hans-Peter Zorn ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</p><p>2 0.73057759 <a title="105-lsi-2" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>3 0.72706771 <a title="105-lsi-3" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>4 0.69096768 <a title="105-lsi-4" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>5 0.6767199 <a title="105-lsi-5" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>6 0.58117777 <a title="105-lsi-6" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>7 0.57532221 <a title="105-lsi-7" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>8 0.55167317 <a title="105-lsi-8" href="./acl-2013-DKPro_Similarity%3A_An_Open_Source_Framework_for_Text_Similarity.html">104 acl-2013-DKPro Similarity: An Open Source Framework for Text Similarity</a></p>
<p>9 0.54990947 <a title="105-lsi-9" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>10 0.52719539 <a title="105-lsi-10" href="./acl-2013-Development_and_Analysis_of_NLP_Pipelines_in_Argo.html">118 acl-2013-Development and Analysis of NLP Pipelines in Argo</a></p>
<p>11 0.50610167 <a title="105-lsi-11" href="./acl-2013-Extending_an_interoperable_platform_to_facilitate_the_creation_of_multilingual_and_multimodal_NLP_applications.html">150 acl-2013-Extending an interoperable platform to facilitate the creation of multilingual and multimodal NLP applications</a></p>
<p>12 0.50598419 <a title="105-lsi-12" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>13 0.4424873 <a title="105-lsi-13" href="./acl-2013-IndoNet%3A_A_Multilingual_Lexical_Knowledge_Network_for_Indian_Languages.html">198 acl-2013-IndoNet: A Multilingual Lexical Knowledge Network for Indian Languages</a></p>
<p>14 0.44226679 <a title="105-lsi-14" href="./acl-2013-AnnoMarket%3A_An_Open_Cloud_Platform_for_NLP.html">51 acl-2013-AnnoMarket: An Open Cloud Platform for NLP</a></p>
<p>15 0.42994958 <a title="105-lsi-15" href="./acl-2013-Offspring_from_Reproduction_Problems%3A_What_Replication_Failure_Teaches_Us.html">262 acl-2013-Offspring from Reproduction Problems: What Replication Failure Teaches Us</a></p>
<p>16 0.42594829 <a title="105-lsi-16" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>17 0.42407909 <a title="105-lsi-17" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>18 0.41977525 <a title="105-lsi-18" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>19 0.40982127 <a title="105-lsi-19" href="./acl-2013-WebAnno%3A_A_Flexible%2C_Web-based_and_Visually_Supported_System_for_Distributed_Annotations.html">385 acl-2013-WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations</a></p>
<p>20 0.40835211 <a title="105-lsi-20" href="./acl-2013-SEMILAR%3A_The_Semantic_Similarity_Toolkit.html">304 acl-2013-SEMILAR: The Semantic Similarity Toolkit</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.167), (6, 0.028), (11, 0.057), (24, 0.045), (26, 0.048), (30, 0.204), (35, 0.041), (42, 0.046), (48, 0.051), (56, 0.01), (70, 0.024), (71, 0.022), (88, 0.059), (90, 0.024), (95, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86315465 <a title="105-lda-1" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>Author: Tristan Miller ; Nicolai Erbs ; Hans-Peter Zorn ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</p><p>2 0.85246944 <a title="105-lda-2" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>3 0.78347361 <a title="105-lda-3" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>Author: Jesse Dunietz ; Lori Levin ; Jaime Carbonell</p><p>Abstract: Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks, as are annotated corpora such as treebanks. Often, the resources are used as-is, without question or examination. This practice risks missing significant performance gains and even entire techniques. This paper addresses the importance of resource quality through the lens of a challenging NLP task: detecting selectional preference violations. We present DAVID, a simple, lexical resource-based preference violation detector. With asis lexical resources, DAVID achieves an F1-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the F1-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements.</p><p>4 0.71337163 <a title="105-lda-4" href="./acl-2013-Probabilistic_Sense_Sentiment_Similarity_through_Hidden_Emotions.html">284 acl-2013-Probabilistic Sense Sentiment Similarity through Hidden Emotions</a></p>
<p>Author: Mitra Mohtarami ; Man Lan ; Chew Lim Tan</p><p>Abstract: Sentiment Similarity of word pairs reflects the distance between the words regarding their underlying sentiments. This paper aims to infer the sentiment similarity between word pairs with respect to their senses. To achieve this aim, we propose a probabilistic emotionbased approach that is built on a hidden emotional model. The model aims to predict a vector of basic human emotions for each sense of the words. The resultant emotional vectors are then employed to infer the sentiment similarity of word pairs. We apply the proposed approach to address two main NLP tasks, namely, Indirect yes/no Question Answer Pairs inference and Sentiment Orientation prediction. Extensive experiments demonstrate the effectiveness of the proposed approach.</p><p>5 0.70605052 <a title="105-lda-5" href="./acl-2013-Part-of-speech_tagging_with_antagonistic_adversaries.html">277 acl-2013-Part-of-speech tagging with antagonistic adversaries</a></p>
<p>Author: Anders Sgaard</p><p>Abstract: Supervised NLP tools and on-line services are often used on data that is very different from the manually annotated data used during development. The performance loss observed in such cross-domain applications is often attributed to covariate shifts, with out-of-vocabulary effects as an important subclass. Many discriminative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features. Regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts. We present a new perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilin- gual cross-domain part-of-speech tagging datasets. While previous approaches do not improve on our supervised baseline, our approach is better across the board with an average 4% error reduction.</p><p>6 0.70365906 <a title="105-lda-6" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>7 0.70047075 <a title="105-lda-7" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>8 0.69123858 <a title="105-lda-8" href="./acl-2013-PLIS%3A_a_Probabilistic_Lexical_Inference_System.html">269 acl-2013-PLIS: a Probabilistic Lexical Inference System</a></p>
<p>9 0.68684119 <a title="105-lda-9" href="./acl-2013-Extending_an_interoperable_platform_to_facilitate_the_creation_of_multilingual_and_multimodal_NLP_applications.html">150 acl-2013-Extending an interoperable platform to facilitate the creation of multilingual and multimodal NLP applications</a></p>
<p>10 0.68255067 <a title="105-lda-10" href="./acl-2013-Development_and_Analysis_of_NLP_Pipelines_in_Argo.html">118 acl-2013-Development and Analysis of NLP Pipelines in Argo</a></p>
<p>11 0.68163007 <a title="105-lda-11" href="./acl-2013-DKPro_Similarity%3A_An_Open_Source_Framework_for_Text_Similarity.html">104 acl-2013-DKPro Similarity: An Open Source Framework for Text Similarity</a></p>
<p>12 0.6811747 <a title="105-lda-12" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>13 0.66991949 <a title="105-lda-13" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>14 0.65236324 <a title="105-lda-14" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>15 0.65110505 <a title="105-lda-15" href="./acl-2013-Recognizing_Partial_Textual_Entailment.html">297 acl-2013-Recognizing Partial Textual Entailment</a></p>
<p>16 0.65005487 <a title="105-lda-16" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>17 0.65004277 <a title="105-lda-17" href="./acl-2013-AnnoMarket%3A_An_Open_Cloud_Platform_for_NLP.html">51 acl-2013-AnnoMarket: An Open Cloud Platform for NLP</a></p>
<p>18 0.64906496 <a title="105-lda-18" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>19 0.64872479 <a title="105-lda-19" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>20 0.64768922 <a title="105-lda-20" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
