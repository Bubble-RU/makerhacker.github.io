<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-83" href="#">acl2013-83</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</h1>
<br/><p>Source: <a title="acl-2013-83-pdf" href="http://aclweb.org/anthology//P/P13/P13-1053.pdf">pdf</a></p><p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>Reference: <a title="acl-2013-83-reference" href="../acl2013_reference/acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. [sent-6, score-0.747]
</p><p>2 We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. [sent-7, score-1.022]
</p><p>3 1 Introduction In recent years, the possibility to undertake largescale annotation projects with hundreds or thousands of annotators has become a reality thanks to online crowdsourcing methods such as Amazon’s Mechanical Turk and Games with a Purpose. [sent-9, score-0.462]
</p><p>4 In this paper, we take a different perspective and instead focus on investigating different aggregation methods for deriving a single collective annotation from a diverse set of judgments. [sent-14, score-0.827]
</p><p>5 For this we draw inspiration from the field of social choice theory, a theoretical framework for combining the preferences or choices of several individuals into a collective decision (Arrow et al. [sent-15, score-0.567]
</p><p>6 Our contribution consists in the formulation of a general formal model for collective annotation and, in particular, the introduction of several families of aggregation methods that go beyond the commonly used majority rule. [sent-18, score-1.048]
</p><p>7 In Section 2 we introduce some basic terminology and argue that there are four natural forms of collective annotation. [sent-20, score-0.345]
</p><p>8 Section 4 introduces three families of aggregation methods: bias-correcting majority rules, greedy methods for identifying (near-)consensual coalitions of annotators, and distance-based aggregators. [sent-23, score-0.568]
</p><p>9 We test the former two families of aggregators, as well as the simple majority rule commonly used in similar studies, in a case study on data extracted from a crowdsourcing experiment on textual entailment in Section 5. [sent-24, score-0.431]
</p><p>10 , 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. [sent-32, score-0.429]
</p><p>11 , 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. [sent-37, score-0.366]
</p><p>12 For large-scale annotation projects run over the Internet it is furthermore very likely that an annotator will not be confronted with every single item, and it makes sense to distinguish items not seen by the annotator from items labelled as  “don’t know”. [sent-41, score-0.79]
</p><p>13 , an annotation task where coders have the option to (i) label items with one of the available categories, to (ii) choose “don’t know”, or to (iii) not label an item at all, as plain annotation. [sent-44, score-0.846]
</p><p>14 Plain annotation is the most common form of annotation and it is the one we shall focus on in this paper. [sent-45, score-0.436]
</p><p>15 For instance, we may ask coders to rank the available categories (resulting in, say, a weak or partial order over the categories); we may ask them to provide a qualitative ratings of the available categories for each item (e. [sent-47, score-0.601]
</p><p>16 We want to investigate how to aggregate the information available for each item once annotations by multiple annotators have been collected. [sent-54, score-0.599]
</p><p>17 In line with the terminology used in social choice theory and particularly judgment aggregation (Ar-  1Some authors have combined qualitative and quantitative ratings; e. [sent-55, score-0.599]
</p><p>18 (2009) coders were asked to classify each relevant WordNet sense for a given item on a 5-point scale: 1completely different, 2 mostly different, 3 similar, 4 very similar, 5 identical. [sent-58, score-0.419]
</p><p>19 row, 1963; List and Pettit, 2002), let us call an aggregation method independent if the outcome regarding a given item j only depends on the categories provided by the annotators regarding j itself (but not on, say, the categories assigned to a different item j0). [sent-59, score-1.232]
</p><p>20 For instance, if a particular annotator almost always chooses category c, then we should maybe give less weight to her selecting c for the item j at hand than when some other annotator chooses c for j. [sent-63, score-0.595]
</p><p>21 Note that when studying independent aggregation methods, without loss of generality, we may assume that each annotation task consists of just a single item. [sent-65, score-0.482]
</p><p>22 In view of our discussion above, there are four classes of approaches to collective annotation: (1) Independent aggregation of plain annotations. [sent-66, score-0.685]
</p><p>23 In case there are exactly two categories available, the plurality rule is also called the majority rule. [sent-69, score-0.44]
</p><p>24 , 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (201 1) should be considered. [sent-79, score-0.504]
</p><p>25 , 2008), and to both binary aggregation (Dokow and Holzman, 2010; Grandi and Endriss, 2011) and judgment aggregation (List and Pettit, 2002). [sent-83, score-0.615]
</p><p>26 3  Formal Model  Next we present our model for general aggregation of plain annotations into a collective annotation. [sent-86, score-0.789]
</p><p>27 1 Terminology and Notation An annotation task is defined in terms of m items, with each item j ∈ {1, . [sent-88, score-0.424]
</p><p>28 Acinanteodtwaittohrs a are atsek seedt otof provide an answer f oCr each of the items of the annotation task. [sent-92, score-0.396]
</p><p>29 An annotation is a vector of answers by one annotator, one answer for each item of the annotation task at hand, i. [sent-99, score-0.654]
</p><p>30 Item 1  Item 2  Item 3  Annotator 1 Annotator 2 Annotator 3  B B A  A B B  A B A  Majority  B  B  A  ×  Table 1: A profile with a collective annotation. [sent-116, score-0.449]
</p><p>31 We want to aggregate the information provided by the annotators into a (single) collective annotation. [sent-120, score-0.61]
</p><p>32 , a labelling of the items in the annotation task with corresponding categories (or ? [sent-127, score-0.51]
</p><p>33 ow orn ⊥ as Athen majority rule for binary tasks with |Cj | = 2 for amlla jiotremitys j), ew fhoirc bhi annotates e wacihth it |Cem| w=it h2 tfhoer category chosen most often. [sent-130, score-0.354]
</p><p>34 Note that the collective annotation need not coincide with any of the individual annotations. [sent-131, score-0.58]
</p><p>35 Take, for example, a binary annotation task in which three coders label three items with category  A or B as shown in Table 1. [sent-132, score-0.608]
</p><p>36 Here using the majority rule to aggregate the annotations would result in a collective annotation that does not fully match any annotation by an individual coder. [sent-133, score-1.174]
</p><p>37 2 Basic Properties A typical task in social choice theory is to formulate axioms that formalise specific desirable properties of an aggregator F (Arrow et al. [sent-135, score-0.507]
</p><p>38 Below we adapt three of the most basic axioms that have been considered in the social choice literature to our setting and we briefly discuss their relevance to collective annotation tasks. [sent-137, score-0.816]
</p><p>39 We will require some additional notation: for any profile A, item j,and possible answer a ∈ Aj, laenty NjA:a ldeeAno,te it mthej ,s aetn dofp oasnsnibolteataonrssw werhoa ∈ch Aose answer a for item j under profile A. [sent-138, score-0.74]
</p><p>40 That is, ∈if Athe patterns of individual annotations of j and j0 are the same, then also their collective annotation should coincide. [sent-155, score-0.684]
</p><p>41 In social choice theory, neutrality is also considered a basic fairness requirement (avoiding preferential treatment one candidate in an election). [sent-156, score-0.332]
</p><p>42 In the context of collective annotation there may be good reasons to violate neutrality: e. [sent-157, score-0.539]
</p><p>43 3 F is independent if the collective annotation of any given ietendme j only depends on t ahnen iontdaitvioidnu oalf annotations of j. [sent-160, score-0.643]
</p><p>44 For collective annotation, we strongly believe that it is not a desirable property: by considering how annotators label other items we can learn about their biases and we should try to exploit this information to obtain the best possible annotation for the item at hand. [sent-163, score-1.167]
</p><p>45 Thus, a further desirable property that will play a role for some annotation tasks is collective rationality (Grandi and Endriss, 2011): if all individual annotations respect a given integrity constraint, then so should the collective annotation. [sent-174, score-1.169]
</p><p>46 However, for some annotation tasks, no integrity constraints may be known to us in advance, even  though we may have reasons to believe that the individual annotators do respect some such constraints. [sent-176, score-0.535]
</p><p>47 In that case, selecting one of the individual annotations in the profile as the collective annotation is the only way to ensure that these integrity constraints will be satisfied by the collective annotation (Grandi and Endriss, 2011). [sent-177, score-1.431]
</p><p>48 Each of them is inspired, in part, by standard approaches to desigining aggregation rules developed in social choice theory and, in part, by the specific needs of collective annotation. [sent-180, score-0.941]
</p><p>49 certain items) and we try to integrate the process of aggregation with a process whereby less reliable annotators are either given less weight or are excluded altogether. [sent-184, score-0.526]
</p><p>50 1 Bias-Correcting Majority Rules We first want to explore the following idea: If a given annotator annotates most items with 0, then we might want to assign less significance to that 542  choice for any particular item. [sent-186, score-0.473]
</p><p>51 What follows applies only to annotation tasks where every item is associated with the same set of categories. [sent-188, score-0.424]
</p><p>52 Given profile A, the collective caPtegory for item j will be 1 in  case Pai,j=1 wi1 > Pai,j=0 wi0, and 0 otherwise. [sent-194, score-0.679]
</p><p>53 5 That Pis, we computeP the overall weight for category P1 by adding up Pthe corresponding weights for those coders that chose 1 for item j, and we do accordingly for the overall weight for category 0; finally, we choose as collective category that category with the larger overall weight. [sent-195, score-1.235]
</p><p>54 , we generalise the majority rule underlying the family of BCM rules to the plurality rule). [sent-239, score-0.402]
</p><p>55 We stress that our bias-correcting majority rules do not violate anonymity (nor neutrality for that matter). [sent-240, score-0.329]
</p><p>56 2 Greedy Consensus Rules Now consider the following idea: If for a given item there is almost complete consensus amongst those coders that annotated it with a proper category (i. [sent-243, score-0.611]
</p><p>57 Furthermore, the fact that there is almost full consensus for one item  543  may cast doubts on the reliability of coders who disagree with this near-consensus choice and we might want to disregard their views not only w. [sent-251, score-0.643]
</p><p>58 that item but also as far as the annotation of other items is concerned. [sent-254, score-0.59]
</p><p>59 sTethe Nn iterate the following tiowno steps: (1) Find the item with the strongest majority for either 0 or 1 amongst coders in N? [sent-268, score-0.615]
</p><p>60 who disagree on more eth aalln cto idteermss wroimth Nthe values locked  in for the collective annotation so far. [sent-271, score-0.575]
</p><p>61 with high einedteyr”-a wnnaoyt aotfo ird agreement aconda ithtieonn applying the majority rule to this coalition to obtain the collective annotation. [sent-274, score-0.758]
</p><p>62 To be precise, the above is a description of an entire family of aggregators: Whenever there is more than one item with a majority of maximal strength, we could choose to lock in any one of them. [sent-275, score-0.491]
</p><p>63 Also, when there is a split majority between annotators in N? [sent-276, score-0.357]
</p><p>64 Note that in case t = m, GreedyCRt is simply the majority rule (as no annotator will ever get eliminated). [sent-279, score-0.376]
</p><p>65 In case t = 0, we end up with a coalition of annotators that unanimously agree with all of the categories chosen for the collective annotation. [sent-280, score-0.74]
</p><p>66 However, this coalition of perfectly aligned  6There are some similarities to Tideman’s Ranked Pairs method for preference aggregation (Tideman, 1987), which works by fixing the relative rankings of pairs of alternatives in order of the strength of the supporting majorities. [sent-281, score-0.379]
</p><p>67 In preference aggregation (unlike here), the population of voters is not reduced in the process; instead, decisions against the majority are taken whenever this is necessary to guarantee the transitivity of the resulting collective preference order. [sent-282, score-0.794]
</p><p>68 annotators need not be the largest such coalition (due to the greedy nature of our rule). [sent-283, score-0.346]
</p><p>69 This induces an aggregator that, for a given profile, returns a collective choice that minimises the sum of distances to the individual choices in the proThis opens up a wide range of possibilities; we only sketch some of them here. [sent-289, score-0.62]
</p><p>70 In the presence of integrity constraints excluding some combinations, however, a distance-based rule allows for more so-  phisticated forms of aggregation (by choosing the optimal annotation w. [sent-298, score-0.686]
</p><p>71 Consider the following idea: If a group of annotators is (fairly) reliable, then they should have a 7This idea has been used in voting (Kemeny, 1959), belief merging (Konieczny and Pino P ´erez, 2002), and judgment aggregation (Miller and Osherson, 2009). [sent-303, score-0.627]
</p><p>72 By this reasoning, we should choose a group of annotators ANN ⊆ N that maximises inter-annotator agreement i⊆n A NNN th aantd m awxoirmki swesith in tthere- aggregator argminA∈A Pi∈ANN d(A, Ai). [sent-306, score-0.416]
</p><p>73 , select few a snhnooutladto crhso otoos s bea ase s our lc soeltlective annotation on), as that will allow us to increase the (average) reliability of the annotators taken into account. [sent-311, score-0.39]
</p><p>74 , sre hleacntd many aonunlodt cahtooross eto a blaasrgee our collective annotation on), as that will increase the amount of information exploited. [sent-314, score-0.539]
</p><p>75 10Since the annotation task and dataset used for our case study do not involve any interesting integrity constraints, we have not tested any distance-based aggregation rules. [sent-324, score-0.586]
</p><p>76 A quick examination of the dataset shows that there are a total of 164 annotators who have annotated between 20 items (124 annotators) and 800 items each (only one annotator). [sent-328, score-0.528]
</p><p>77 For each pair we report the observed agreement Ao (proportion of items on which two annotations agree) and, in brackets, Cohen’s kappa κ = with Ae being the expected agreement for 1in−Adependent annotators (Cohen, 1960; Artstein and Poesio, 2008). [sent-333, score-0.588]
</p><p>78 0 is the majority rule that chooses 1in case the number of annotators choosing 1 is equal to the number of annotators choosing 0 (and accordingly for Maj0? [sent-336, score-0.653]
</p><p>79 (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) between the majority rule and the gold standard of 89. [sent-343, score-0.746]
</p><p>80 Observe that all of the bias-correcting majority rules approximate the gold standard better than the ma-  jority rule with uniformly random tie-breaking. [sent-356, score-0.35]
</p><p>81 81)  Table 2: Observed agreement (and κ) between collective annotations and the gold standard. [sent-417, score-0.563]
</p><p>82 Recall that the greedy consensus rule is in fact a family of aggregators: whenever there is more than one item with a maximal majority, we may lock in any one of them. [sent-418, score-0.504]
</p><p>83 The results reported here refer to an implementation that always chooses the lexicographically first item amongst all those with a maximal majority and that breaks ties in favour of 1. [sent-420, score-0.533]
</p><p>84 The coalition found for tolerance 0 consists of 46 annotators who all completely agree with the collective annotation; the coalition found for toler-  ance 15 consists of 156 annotators who all disagree with the collective annotation on at most 15 items. [sent-424, score-1.56]
</p><p>85 This is surprising and suggests, on the one hand, that eliminating only the most extreme outlier annotators is a useful strategy, and on the other hand, that a high-quality collective annotation can be obtained from a group of annotators that disagree substantially. [sent-426, score-0.967]
</p><p>86 Similarly, crowdsourcing via microworking sites like Amazon’s Mechanical Turk has been used in several annotation experiments related to tasks such as affect analysis, event annotation, sense definition and word sense disambiguation (Snow et al. [sent-432, score-0.334]
</p><p>87 12 All these efforts face the problem of how to aggregate the information provided by a group of volunteers into a collective annotation. [sent-435, score-0.38]
</p><p>88 However, by and large, the emphasis so far has been on issues such as experiment design, data quality, and costs, with little attention being paid to the aggregation methods used, which are typically limited to some form of majority vote (or taking averages if the categories are numeric). [sent-436, score-0.557]
</p><p>89 In contrast, our focus has been on investigating different aggregation methods for arriving at a collective annotation. [sent-437, score-0.633]
</p><p>90 Agreement scores such as kappa are used to assess the quality of an annotation but do not play a direct role in constructing one single annotation from the labellings of several The methods we have proposed, in contrast, do precisely that. [sent-439, score-0.388]
</p><p>91 In our discussion of distance-based aggregation, we suggested how agreement can be used to select a subset of annotators whose individual annotations are minimally distant from the resulting collective annotation. [sent-441, score-0.747]
</p><p>92 In our approach, we do not directly rate annotators or recalibrate their annotations—rather, some outlier annotators get to play a marginal role in the resulting collective annotation as a side effect of the aggregation methods themselves. [sent-456, score-1.219]
</p><p>93 Although in our case study we have tested our aggregators by comparing their outcomes to a gold standard, our approach to collective annotation itself does not assume that there is in fact a ground truth. [sent-457, score-0.892]
</p><p>94 Instead, we view collective annotations as reflecting the views of a community of speakers. [sent-458, score-0.449]
</p><p>95 Hence, a collective annotation may be the closest we can get to a representation of the linguistic knowledge/use of a linguistic community. [sent-468, score-0.539]
</p><p>96 Importantly, this does not mean that the concrete aggregation methods developed in social choice theory are immediately applicable or that all the axioms typically studied in social choice theory are necessarily relevant to aggregating linguistic annotations. [sent-476, score-0.949]
</p><p>97 Rather, what we claim is that it is the methodology of social choice theory which is useful: to formally state desirable properties of aggregators as axioms and then to investigate which specific aggregators satisfy them. [sent-477, score-0.869]
</p><p>98 To put it differently: at the moment, researchers in computational linguistics simply use some given aggregation methods (almost always the majority rule) and judge their quality on how they fare in specific experiments—but there is no principled reflection on the methods themselves. [sent-478, score-0.449]
</p><p>99 In future work, the framework we have pre-  sented here should be tested more extensively, not only against a gold standard but also in terms of the usefulness ofthe derived collective annotations for training supervised learning systems. [sent-480, score-0.502]
</p><p>100 On the theoretial side, it would be interesting to study the axiomatic properties of the methods of aggregation we have proposed here in more depth and to define axiomatic properties of aggregators that are specifically tailored to the task of collective annotation of linguistic resources. [sent-481, score-1.08]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('collective', 0.345), ('aggregation', 0.288), ('aggregators', 0.253), ('item', 0.23), ('annotators', 0.196), ('annotation', 0.194), ('items', 0.166), ('majority', 0.161), ('coders', 0.155), ('endriss', 0.135), ('choice', 0.124), ('annotator', 0.115), ('aggregator', 0.11), ('categories', 0.108), ('profile', 0.104), ('voting', 0.104), ('annotations', 0.104), ('integrity', 0.104), ('bcm', 0.101), ('rule', 0.1), ('social', 0.098), ('category', 0.093), ('coalition', 0.091), ('combcm', 0.084), ('freqi', 0.084), ('grandi', 0.084), ('ties', 0.073), ('crowdsourcing', 0.072), ('plurality', 0.071), ('amongst', 0.069), ('anonymity', 0.067), ('diffbcm', 0.067), ('nja', 0.067), ('relbcm', 0.067), ('ulle', 0.067), ('venhuizen', 0.067), ('tolerance', 0.066), ('neutrality', 0.065), ('consensus', 0.064), ('aggregating', 0.062), ('agreement', 0.061), ('families', 0.06), ('greedy', 0.059), ('snow', 0.057), ('axioms', 0.055), ('arrow', 0.055), ('gold', 0.053), ('plain', 0.052), ('lock', 0.051), ('pini', 0.051), ('wix', 0.051), ('theory', 0.05), ('testset', 0.049), ('choose', 0.049), ('shall', 0.048), ('freq', 0.047), ('ground', 0.047), ('rosenthal', 0.045), ('beigman', 0.045), ('fairness', 0.045), ('raykar', 0.045), ('mechanical', 0.044), ('amazon', 0.043), ('weight', 0.042), ('labelling', 0.042), ('truth', 0.042), ('rumshisky', 0.041), ('individual', 0.041), ('artstein', 0.04), ('judgment', 0.039), ('recognising', 0.039), ('aj', 0.039), ('basile', 0.039), ('entailment', 0.038), ('broken', 0.037), ('desirable', 0.036), ('disagree', 0.036), ('answer', 0.036), ('rules', 0.036), ('regarding', 0.036), ('cj', 0.035), ('aggregate', 0.035), ('poesio', 0.035), ('sense', 0.034), ('generalise', 0.034), ('formulate', 0.034), ('want', 0.034), ('balinski', 0.034), ('brams', 0.034), ('chevaleyre', 0.034), ('conitzer', 0.034), ('dawid', 0.034), ('dokow', 0.034), ('jeuxdemots', 0.034), ('konieczny', 0.034), ('oann', 0.034), ('pettit', 0.034), ('peugeot', 0.034), ('rossi', 0.034), ('skewness', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="83-tfidf-1" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>2 0.13146706 <a title="83-tfidf-2" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>Author: Maria Skeppstedt</p><p>Abstract: For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. The task of the annotator is to select the correct version among these two alternatives. To minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classifier.</p><p>3 0.12348527 <a title="83-tfidf-3" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>Author: Trevor Cohn ; Lucia Specia</p><p>Abstract: Annotating linguistic data is often a complex, time consuming and expensive endeavour. Even with strict annotation guidelines, human subjects often deviate in their analyses, each bringing different biases, interpretations of the task and levels of consistency. We present novel techniques for learning from the outputs of multiple annotators while accounting for annotator specific behaviour. These techniques use multi-task Gaussian Processes to learn jointly a series of annotator and metadata specific models, while explicitly representing correlations between models which can be learned directly from data. Our experiments on two machine translation quality estimation datasets show uniform significant accuracy gains from multi-task learning, and consistently outperform strong baselines.</p><p>4 0.11882652 <a title="83-tfidf-4" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Mu Li ; Ming Zhou ; Longkai Zhang ; Houfeng Wang</p><p>Abstract: We propose a novel entity disambiguation model, based on Deep Neural Network (DNN). Instead of utilizing simple similarity measures and their disjoint combinations, our method directly optimizes document and entity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches.</p><p>5 0.10285227 <a title="83-tfidf-5" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>Author: Rohan Ramanath ; Monojit Choudhury ; Kalika Bali ; Rishiraj Saha Roy</p><p>Abstract: Query segmentation, like text chunking, is the first step towards query understanding. In this study, we explore the effectiveness of crowdsourcing for this task. Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees.</p><p>6 0.099665523 <a title="83-tfidf-6" href="./acl-2013-WebAnno%3A_A_Flexible%2C_Web-based_and_Visually_Supported_System_for_Distributed_Annotations.html">385 acl-2013-WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations</a></p>
<p>7 0.09783677 <a title="83-tfidf-7" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>8 0.09382125 <a title="83-tfidf-8" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>9 0.092760496 <a title="83-tfidf-9" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>10 0.089215748 <a title="83-tfidf-10" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>11 0.086717501 <a title="83-tfidf-11" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>12 0.086297892 <a title="83-tfidf-12" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>13 0.082939066 <a title="83-tfidf-13" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>14 0.0812684 <a title="83-tfidf-14" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>15 0.078166045 <a title="83-tfidf-15" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>16 0.075149909 <a title="83-tfidf-16" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>17 0.069815278 <a title="83-tfidf-17" href="./acl-2013-Recognizing_Partial_Textual_Entailment.html">297 acl-2013-Recognizing Partial Textual Entailment</a></p>
<p>18 0.068172753 <a title="83-tfidf-18" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>19 0.065994695 <a title="83-tfidf-19" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>20 0.065669179 <a title="83-tfidf-20" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.187), (1, 0.047), (2, -0.028), (3, -0.074), (4, -0.04), (5, -0.033), (6, 0.018), (7, -0.007), (8, 0.073), (9, 0.043), (10, -0.078), (11, 0.065), (12, -0.067), (13, -0.012), (14, -0.07), (15, -0.061), (16, 0.017), (17, 0.053), (18, 0.063), (19, -0.092), (20, -0.047), (21, 0.025), (22, -0.12), (23, 0.004), (24, -0.076), (25, -0.015), (26, -0.042), (27, -0.019), (28, -0.009), (29, -0.011), (30, -0.01), (31, -0.035), (32, 0.056), (33, -0.013), (34, -0.035), (35, -0.013), (36, 0.087), (37, -0.027), (38, -0.052), (39, -0.053), (40, 0.203), (41, 0.05), (42, 0.122), (43, 0.014), (44, 0.092), (45, -0.071), (46, -0.032), (47, 0.072), (48, -0.014), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96745193 <a title="83-lsi-1" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>2 0.67755026 <a title="83-lsi-2" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>Author: Maria Skeppstedt</p><p>Abstract: For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. The task of the annotator is to select the correct version among these two alternatives. To minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classifier.</p><p>3 0.65339953 <a title="83-lsi-3" href="./acl-2013-WebAnno%3A_A_Flexible%2C_Web-based_and_Visually_Supported_System_for_Distributed_Annotations.html">385 acl-2013-WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations</a></p>
<p>Author: Seid Muhie Yimam ; Iryna Gurevych ; Richard Eckart de Castilho ; Chris Biemann</p><p>Abstract: We present WebAnno, a general purpose web-based annotation tool for a wide range of linguistic annotations. WebAnno offers annotation project management, freely configurable tagsets and the management of users in different roles. WebAnno uses modern web technology for visualizing and editing annotations in a web browser. It supports arbitrarily large documents, pluggable import/export filters, the curation of annotations across various users, and an interface to farming out annotations to a crowdsourcing platform. Currently WebAnno allows part-ofspeech, named entity, dependency parsing and co-reference chain annotations. The architecture design allows adding additional modes of visualization and editing, when new kinds of annotations are to be supported.</p><p>4 0.63217098 <a title="83-lsi-4" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<p>Author: Omri Abend ; Ari Rappoport</p><p>Abstract: Syntactic structures, by their nature, reflect first and foremost the formal constructions used for expressing meanings. This renders them sensitive to formal variation both within and across languages, and limits their value to semantic applications. We present UCCA, a novel multi-layered framework for semantic representation that aims to accommodate the semantic distinctions expressed through linguistic utterances. We demonstrate UCCA’s portability across domains and languages, and its relative insensitivity to meaning-preserving syntactic variation. We also show that UCCA can be effectively and quickly learned by annotators with no linguistic background, and describe the compilation of a UCCAannotated corpus.</p><p>5 0.57730174 <a title="83-lsi-5" href="./acl-2013-A_computational_approach_to_politeness_with_application_to_social_factors.html">30 acl-2013-A computational approach to politeness with application to social factors</a></p>
<p>Author: Cristian Danescu-Niculescu-Mizil ; Moritz Sudhof ; Dan Jurafsky ; Jure Leskovec ; Christopher Potts</p><p>Abstract: We propose a computational framework for identifying linguistic aspects of politeness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. Our classifier achieves close to human performance and is effective across domains. We use our framework to study the relationship between po- liteness and social power, showing that polite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bottom. Finally, we apply our classifier to a preliminary analysis of politeness variation by gender and community.</p><p>6 0.57687455 <a title="83-lsi-6" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>7 0.57657152 <a title="83-lsi-7" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>8 0.54310632 <a title="83-lsi-8" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>9 0.53993297 <a title="83-lsi-9" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>10 0.51465321 <a title="83-lsi-10" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>11 0.51354635 <a title="83-lsi-11" href="./acl-2013-Linguistic_Models_for_Analyzing_and_Detecting_Biased_Language.html">232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</a></p>
<p>12 0.50670397 <a title="83-lsi-12" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>13 0.49076521 <a title="83-lsi-13" href="./acl-2013-Patient_Experience_in_Online_Support_Forums%3A_Modeling_Interpersonal_Interactions_and_Medication_Use.html">278 acl-2013-Patient Experience in Online Support Forums: Modeling Interpersonal Interactions and Medication Use</a></p>
<p>14 0.47527239 <a title="83-lsi-14" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>15 0.47256437 <a title="83-lsi-15" href="./acl-2013-Public_Dialogue%3A_Analysis_of_Tolerance_in_Online_Discussions.html">287 acl-2013-Public Dialogue: Analysis of Tolerance in Online Discussions</a></p>
<p>16 0.46879458 <a title="83-lsi-16" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>17 0.45376027 <a title="83-lsi-17" href="./acl-2013-On_the_Predictability_of_Human_Assessment%3A_when_Matrix_Completion_Meets_NLP_Evaluation.html">263 acl-2013-On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation</a></p>
<p>18 0.45060056 <a title="83-lsi-18" href="./acl-2013-Extra-Linguistic_Constraints_on_Stance_Recognition_in_Ideological_Debates.html">151 acl-2013-Extra-Linguistic Constraints on Stance Recognition in Ideological Debates</a></p>
<p>19 0.44912776 <a title="83-lsi-19" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<p>20 0.44535598 <a title="83-lsi-20" href="./acl-2013-Reducing_Annotation_Effort_for_Quality_Estimation_via_Active_Learning.html">300 acl-2013-Reducing Annotation Effort for Quality Estimation via Active Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.077), (6, 0.055), (11, 0.075), (15, 0.028), (24, 0.043), (26, 0.045), (35, 0.078), (42, 0.053), (48, 0.045), (53, 0.033), (63, 0.014), (64, 0.019), (70, 0.037), (87, 0.134), (88, 0.037), (90, 0.027), (95, 0.049), (99, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88323182 <a title="83-lda-1" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>2 0.75387222 <a title="83-lda-2" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>Author: Zhenhua Tian ; Hengheng Xiang ; Ziqi Liu ; Qinghua Zheng</p><p>Abstract: This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate’s smoothed preferences. Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements.</p><p>3 0.75324768 <a title="83-lda-3" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>Author: Jiwei Tan ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 1</p><p>4 0.75234765 <a title="83-lda-4" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>Author: Wei Xu ; Raphael Hoffmann ; Le Zhao ; Ralph Grishman</p><p>Abstract: Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus. However, previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases. To tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features. We adapt the information retrieval technique of pseudo- relevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments.</p><p>5 0.75224978 <a title="83-lda-5" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>6 0.75060993 <a title="83-lda-6" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>7 0.75060761 <a title="83-lda-7" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>8 0.74861318 <a title="83-lda-8" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>9 0.7480883 <a title="83-lda-9" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>10 0.74785858 <a title="83-lda-10" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>11 0.74673033 <a title="83-lda-11" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>12 0.74654037 <a title="83-lda-12" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>13 0.74546945 <a title="83-lda-13" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>14 0.74527335 <a title="83-lda-14" href="./acl-2013-Word_Association_Profiles_and_their_Use_for_Automated_Scoring_of_Essays.html">389 acl-2013-Word Association Profiles and their Use for Automated Scoring of Essays</a></p>
<p>15 0.74441278 <a title="83-lda-15" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>16 0.74418777 <a title="83-lda-16" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>17 0.74328655 <a title="83-lda-17" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>18 0.7430898 <a title="83-lda-18" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>19 0.74293345 <a title="83-lda-19" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>20 0.74172837 <a title="83-lda-20" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
