<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-109" href="#">acl2013-109</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</h1>
<br/><p>Source: <a title="acl-2013-109-pdf" href="http://aclweb.org/anthology//P/P13/P13-1060.pdf">pdf</a></p><p>Author: Malte Nuhn ; Hermann Ney</p><p>Abstract: In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchanging them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before.</p><p>Reference: <a title="acl-2013-109-reference" href="../acl2013_reference/acl-2013-Decipherment_Complexity_in_1%3A1_Substitution_Ciphers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). [sent-2, score-1.478]
</p><p>2 To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before. [sent-3, score-0.771]
</p><p>3 1 Introduction The decipherment approach for MT has recently gained popularity for training and adapting translation models using only monolingual data. [sent-4, score-0.648]
</p><p>4 In general, the process of translation has a wide range of phenomena like substitution and reordering of words and phrases. [sent-6, score-0.083]
</p><p>5 In this paper we only study models that substitute tokens—i. [sent-7, score-0.031]
</p><p>6 It therefore serves as a very basic case for decipherment and machine translation. [sent-10, score-0.648]
</p><p>7 Multiple techniques like integer linear programming (ILP), A∗ search, genetic algorithms, and Bayesian inference have been used to tackle the decipherment problem for 1:1substitution ciphers. [sent-11, score-0.791]
</p><p>8 The existence of such a variety of different approaches for solving the same problem already shows that there is no obvious way to solve the problem optimally. [sent-12, score-0.189]
</p><p>9 In this paper we show that decipherment of 1:1 substitution ciphers is indeed NP-hard and thus ex. [sent-13, score-0.807]
</p><p>10 The literature on decipherment provides surprisingly little on the analysis of the complexity of the decipherment problem. [sent-16, score-1.296]
</p><p>11 This might be related to the fact that a statistical formulation of the decipherment problem has not been analyzed with respect to n-gram language models: This paper shows the close relationship of the decipherment problem to the quadratic assignment problem. [sent-17, score-2.095]
</p><p>12 To the best of our knowledge the connection between the decipherment problem and the quadratic assignment problem was not known. [sent-18, score-1.494]
</p><p>13 Section 3 introduces the decipherment problem and describes the notation and definitions used throughout this paper. [sent-20, score-0.765]
</p><p>14 In Section 4 we show that decipherment using a unigram language model corresponds to solving a linear sum assignment problem (LSAP). [sent-21, score-1.253]
</p><p>15 Section 5 shows the connection between the quadratic assignment problem and decipherment using a bigram language model. [sent-22, score-1.507]
</p><p>16 Here we also give a reduction of the traveling salesman problem (TSP) to the decipherment problem  to highlight the additional complexity in the decipherment problem. [sent-23, score-1.634]
</p><p>17 2  Related Work  In recent years a large number of publications on the automatic decipherment of substitution ciphers has been published. [sent-24, score-0.828]
</p><p>18 These publications were mostly dominated by rather heuristic methods and did not provide a theoretical analysis of the complexity of the decipherment problem: (Knight and Yamada, 1999) and (Knight et al. [sent-25, score-0.669]
</p><p>19 , 2006) use the EM algorithm for various decipherment problems, like e. [sent-26, score-0.648]
</p><p>20 , 2012), and (Dou and Knight, 2012) treat natural language translation as a deciphering problem including phenomena like reordering, in-  sertion, and deletion and are able to train translation models using only monolingual data. [sent-35, score-0.149]
</p><p>21 In this paper we will show the connection between the decipherment problem and the linear sum assignment problem as well as the quadratic assignment problem: Regarding the linear sum assignment problem we will make use of definitions presented in (Burkard and ela, 1999). [sent-36, score-2.457]
</p><p>22 Concerning the quadratic assignment problem we will use basic definitions from (Beckmann and Koopmans, 1957). [sent-37, score-0.764]
</p><p>23 , 1998) gives a good overview over the quadratic assignment problem, including different formulations, solution methods, and an analysis of computational complexity. [sent-39, score-0.647]
</p><p>24 3  Definitions  In the following we will use the machine translation notation and denote the ciphertext with f1N = f1. [sent-41, score-0.088]
</p><p>25 We define e0  = f0 =  eN+1  = fN+1 = $  (1)  with “$” being a special sentence boundary token. [sent-55, score-0.025]
</p><p>26 A general substitution cipher uses a table s(e|f) which contains for each cipher token f a probability cthha ct othntea tionske fno f eisa shub csitpihtuetred to wkeitnh fthe a plaintext token e. [sent-57, score-0.446]
</p><p>27 Such a table for substituting cipher tokens {A, B, C, D} with plaintext tokens {a, b, c, d} cnosu {ldA ,foBr example loithok p lliakien a b c d A0. [sent-58, score-0.195]
</p><p>28 1  The 1:1 substitution cipher encrypts a given plaintext into a ciphertext by replacing each plain-  text token with a unique substitute: This means that the table s(e|f) contains all zeroes, except for one “1. [sent-74, score-0.393]
</p><p>29 Fonore example trh fe text abadcab would be enciphered to BCBADBC when using the substitution a b c d A0001 B  1 0  C D  0 0  0  0  1 0 0 0 1 0  We formalize the 1:1 substitutions with a bijective function φ : Vf → Ve. [sent-77, score-0.083]
</p><p>30 The general decipherment goal is to o→btai Vn a mapping φ such that the probability of the deciphered text is maximal:  φˆ = argφmaxp(φ(f1)φ(f2)φ(f3). [sent-78, score-0.648]
</p><p>31 Given a ciphertext f1N, we define the unigram count Nf of f ∈ Vf as1  Nf  = NX+1δ(f,fi) Xi=0  (3)  This implies that Nf are integer counts > 0. [sent-86, score-0.291]
</p><p>32 We similarly define the bigram count Nff0 of f,f0 ∈ Vf as Nff0  =  NX+1δ(f,fi−1) · δ(f0,fi)  (4)  Xi= X1  This definition implies that (a) Nff0 are integer counts > 0 of bigrams found in the ciphertext f1N. [sent-87, score-0.311]
</p><p>33 (b) Given the first and last token of the cipher f1 and fN, the bigram counts involving the sentence boundary token $ need to fulfill  =  N$f δ(f, f1) Nf$ = δ(f, fN)  (5) (6)  (c) For all f ∈ Vf  X Nff0 fX0∈Vf  = X Nf0f fX0∈Vf  (7)  must hold. [sent-88, score-0.333]
</p><p>34 616  Similarly, we define language model matrices S for the unigram and the bigram case. [sent-90, score-0.207]
</p><p>35 bacVefCBAcijabN A lo Ag p (bac)N B lo Bg p (bca)N C lo Cg p (cab) Figure 1: Linear sum assignment problem for a cipher with Ve = {a, b, c}, Vf = {A, B, C}, unigram counts Nf, a {nad, unigram probabilities p(e). [sent-93, score-0.823]
</p><p>36 2  The Linear Sum Assignment Problem  The practical problem behind the linear sum assignment problem can be described as follows: Given jobs {j1, . [sent-95, score-0.627]
</p><p>37 , wn}, th jeo btassk { ijs to assign each job ji to a {wworker wj. [sent-101, score-0.025]
</p><p>38 E}a,c thh assignment sinsicgunrs e a hco jsotb cij and  the total cost for assigning all jobs and workers is to be minimized. [sent-102, score-0.527]
</p><p>39 This can be formalized as finding the assignment  φˆ  =  Xn  argφminXi=1ciφ(i)  (18)  The general LSAP can be solved in polynomial time using the Hungarian algorithm (Kuhn, 1955). [sent-103, score-0.393]
</p><p>40 Sorting and then assigning the elements can be done in O(n log n). [sent-105, score-0.036]
</p><p>41 1 Problem Definition When using a 2-gram language model, Equation 2 simplifies to  φˆ = argφmaxNjY=+11p(φ(fj)|φ(fj−1))(19) 617  Assignments  Flows  ( ab))fl 111ffl422flf333lf 244 ff21f11f12f3f4 y  f3 f4  1 1  l3l2l1l4x Figure 2: Hypothetical quadratic assignment prob-  lem with locations l1. [sent-107, score-0.716]
</p><p>42 f4 with all flows being zero except f1 ↔ f2 and f3 ↔ f4. [sent-113, score-0.049]
</p><p>43 l4 is implicitly given by the locations in the plane, implying a euclidean metric. [sent-117, score-0.045]
</p><p>44 However, he does not mention the close connection to the quadratic assignment problem. [sent-120, score-0.694]
</p><p>45 2  The Quadratic Assignment Problem  The quadratic assignment problem was introduced by (Beckmann and Koopmans, 1957) for the following real-life problem: Given a set of facilities {f1, . [sent-122, score-0.796]
</p><p>46 the amount of supplies to be transported between a pair of facilities) the problem is to assign the facilities to locations such that the sum of the distances multiplied by the corresponding flows (which can be interpreted as total transportation costs) is minimized. [sent-130, score-0.284]
</p><p>47 Following (Beckmann and Koopmans, 1957) we can express the quadratic assignment problem as finding  φˆ = argφminXi=n1jX=n1aijbφ(i)φ(j)+Xi=n1ciφ(i) (21) where A = (aij) , B = (bij) , C = (cij) ∈ Nn×n and φ a permutation  φ : {1, . [sent-132, score-0.723]
</p><p>48 The so-called pure or homogeneous QAP  φˆ = argφminXi=n1jX=n1aijbφ(i)φ(j)  (23)  is obtained by setting cij = 0, and is often denoted as QAP(A, B). [sent-140, score-0.076]
</p><p>49 In terms of the real-life problem presented in (Beckmann and Koopmans, 1957) the matrix A can be interpreted as distance matrix for locations {l1 . [sent-141, score-0.261]
</p><p>50 nd Gonzalez, 1976) show that the quadratic assignment problem is strongly NPhard. [sent-146, score-0.723]
</p><p>51 We will now show the relation between the quadratic assignment problem and the decipher-  . [sent-147, score-0.723]
</p><p>52 Quadratic Assignment nPtro Pbrloebmle Every decipherment problem is directly a quadratic assignment problem, since the matrices Nff0 and Sff0 are just special cases of the general matrices A and B required for the quadratic assignment problem. [sent-153, score-2.116]
</p><p>53 Thus a reduction from the decipherment problem to the quadratic assignment problem is trivial. [sent-154, score-1.471]
</p><p>54 This means that all algorithms capable of solving QAPs can directly be used to solve the decipherment problem. [sent-155, score-0.685]
</p><p>55 Decipherment iPgnrombelenmt P Given QAP(A, B) with integer matrices A = (aij), B = (bij) i,j ∈ {1, . [sent-158, score-0.087]
</p><p>56 , n} we construct the count matrix) Nff0 a {nd1 language me coodnesl matrix Sff0 in such a way that the solution for the decipherment problem implies the solution to the 618  quadratic assignment problem, and vice versa. [sent-161, score-1.514]
</p><p>57 Show that Sff0 is a valid bigram language model matrix. [sent-173, score-0.112]
</p><p>58 Show that the decipherment problem and the newly constructed quadratic assignment problem are equivalent. [sent-175, score-1.447]
</p><p>59 We start by showing that Nff0 is a valid count matrix: (a) By construction, Nff0 has integer counts that are greater or equal to 0. [sent-176, score-0.121]
</p><p>60 , n} the count propertFieosr are equivalent to  af∗+X ˜aff0 = ˜a∗f+X a˜f0f+ δ(f,1) Xf0  Xf0  (24)  •  which holds by construction of a∗f and af∗. [sent-180, score-0.126]
</p><p>61 For f = n+1the count property is equivaFloernft t o=  1 +X a˜f0∗ Xf0  = 2 +X a˜∗f0  (25)  Xf0  which follows from Equation (24) by summing over all f ∈ {1, . [sent-181, score-0.037]
</p><p>62 t We now show that Sff0 is a valid bigram language model matrix: (a) By construction, Sff0 ∈ [−∞, 0] holds. [sent-186, score-0.112]
</p><p>63 (c) By the construction of the values Sff0 ful-  b˜f∗,  fill Pf0 exp(Sff0) = 1for all f. [sent-188, score-0.037]
</p><p>64 are chosen to be smaller  We now show the equivalence of the quadratic assignment problem and the newly constructed decipherment problem. [sent-190, score-1.371]
</p><p>65 For this we will use the definitions  A˜ B˜  =  {1, . [sent-191, score-0.041]
</p><p>66 , n}  (26)  =  {n + 1, n + 2, n + 3}  (27)  We first show that solutions of the constructed decipherment problem with score > −∞ fulfill φ(f) = f feonrt f ∈ fA)ll = mappings φ, with φ(f) = f0 for any f ∈ and f0 ∈ will induce a score offo −∞ si fnc ∈e  bB˜le. [sent-194, score-0.752]
</p><p>67 2   b˜ff0= bff0− mf˜af˜0x{bf˜f˜0} − log(n + 2)  b˜f∗= log1 −fX0n=1exp(b˜ff0) −n +2 2 εi  =  −  log(n + i)  Figure 3: Construction of matrices Nff0 and Sff0 of the decipherment problem from matrices A = (aij) and B = (bij) of the quadratic assignment problem QAP(A, B). [sent-214, score-1.545]
</p><p>68 Assuming that calculat-  ing elementary functions can be done in O(1), setting up Nff0 ayn fdu Sff0 can nb e b edo dnoen ien i polynomial time. [sent-216, score-0.04]
</p><p>69 2 Thus we have given a polynomial time reduction from the quadratic assignment problem to 2This is the case if we only require a fixed number of digits precision for the log and exp operations. [sent-217, score-0.823]
</p><p>70 the decipherment problem: Since the quadratic assignment problem is NP-hard, it follows that the decipherment problem is NP-hard, too. [sent-218, score-2.095]
</p><p>71 Decipherment sPmraonble Pmro Using the above construction we can immediately construct a decipherment problem that is equivalent to the traveling salesman problem by using the quadratic assignment problem formulation of the traveling salesman problem. [sent-221, score-1.915]
</p><p>72 Without loss of generality3 we assume that the TSP’s distance matrix fulfills the constraints of a bigram language model matrix Sff0 . [sent-222, score-0.261]
</p><p>73 Then the count matrix Nff0 needs to be chosen as  Nf0=  10 . [sent-223, score-0.107]
</p><p>74   (35)  which fulfills the constraints of a bigram count matrix. [sent-230, score-0.158]
</p><p>75 3The general case can be covered using the reduction shown in Section 5. [sent-231, score-0.024]
</p><p>76 620  This matrix corresponds to a ciphertext of the form $abcd$  (36)  and represents the tour of the traveling salesman in an intuitive way. [sent-232, score-0.32]
</p><p>77 The mapping φ then only decides in which order the cities are visited, and only costs between two successive cities are counted. [sent-233, score-0.056]
</p><p>78 This shows that the TSP is only a special case of the decipherment problem. [sent-234, score-0.648]
</p><p>79 For a bigram language model, the decipherment problem is equivalent to the quadratic assignment problem and is NP-hard. [sent-236, score-1.567]
</p><p>80 We also pointed out that all available algorithms for the quadratic assignment problem can be directly used to solve the decipherment problem. [sent-237, score-1.371]
</p><p>81 To the best of our knowledge, this correspondence between the decipherment problem and the quadratic assignment problem has not been known previous to our work. [sent-238, score-1.447]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('decipherment', 0.648), ('assignment', 0.353), ('quadratic', 0.294), ('vf', 0.188), ('xf', 0.16), ('qap', 0.141), ('cipher', 0.114), ('beckmann', 0.1), ('nf', 0.094), ('bigram', 0.089), ('ciphertext', 0.088), ('fx', 0.086), ('substitution', 0.083), ('plaintext', 0.081), ('salesman', 0.081), ('traveling', 0.081), ('koopmans', 0.08), ('ciphers', 0.076), ('cij', 0.076), ('problem', 0.076), ('fn', 0.073), ('facilities', 0.073), ('deciphering', 0.073), ('burkard', 0.07), ('matrix', 0.07), ('unigram', 0.069), ('xn', 0.068), ('bij', 0.06), ('sf', 0.054), ('af', 0.054), ('arg', 0.054), ('jobs', 0.052), ('equation', 0.05), ('matrices', 0.049), ('tsp', 0.049), ('flows', 0.049), ('fj', 0.048), ('connection', 0.047), ('knight', 0.047), ('workers', 0.046), ('nuhn', 0.046), ('locations', 0.045), ('ravi', 0.044), ('aij', 0.044), ('sum', 0.041), ('definitions', 0.041), ('eranda', 0.04), ('lsap', 0.04), ('sahni', 0.04), ('vfexp', 0.04), ('polynomial', 0.04), ('ve', 0.038), ('integer', 0.038), ('construction', 0.037), ('count', 0.037), ('solving', 0.037), ('implies', 0.036), ('log', 0.036), ('argmax', 0.036), ('corlett', 0.035), ('bz', 0.035), ('rainer', 0.035), ('bb', 0.032), ('fulfills', 0.032), ('logp', 0.032), ('substitute', 0.031), ('equivalent', 0.031), ('ela', 0.031), ('aachen', 0.031), ('malte', 0.031), ('xi', 0.029), ('linear', 0.029), ('az', 0.029), ('kevin', 0.028), ('cities', 0.028), ('dou', 0.028), ('fulfill', 0.028), ('token', 0.027), ('mf', 0.027), ('xz', 0.027), ('lo', 0.026), ('sujith', 0.026), ('kluwer', 0.025), ('hungarian', 0.025), ('ji', 0.025), ('boundary', 0.025), ('bauer', 0.024), ('reduction', 0.024), ('simplifies', 0.024), ('counts', 0.023), ('valid', 0.023), ('lms', 0.023), ('rewritten', 0.023), ('nx', 0.023), ('ln', 0.023), ('bj', 0.022), ('sorting', 0.022), ('max', 0.021), ('holds', 0.021), ('publications', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="109-tfidf-1" href="./acl-2013-Decipherment_Complexity_in_1%3A1_Substitution_Ciphers.html">109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Hermann Ney</p><p>Abstract: In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchanging them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before.</p><p>2 0.45453888 <a title="109-tfidf-2" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Julian Schamper ; Hermann Ney</p><p>Abstract: In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet- ter (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.</p><p>3 0.31208256 <a title="109-tfidf-3" href="./acl-2013-Decipherment.html">108 acl-2013-Decipherment</a></p>
<p>Author: Kevin Knight</p><p>Abstract: The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. This tutorial explores connections between early decipherment research and today’s NLP work. We cover classic military and diplomatic ciphers, automatic decipherment algorithms, unsolved ciphers, language translation as decipherment, and analyzing ancient writing as decipherment. 1 Tutorial Overview The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. Sixty years later, we have many more applications, including web search, question answering, summarization, speech recognition, and language translation. This tutorial explores connections between early decipherment research and today’s NLP work. We find that many ideas from the earlier era have become core to the field, while others still remain to be picked up and developed. We first cover classic military and diplomatic cipher types, including complex substitution ciphers implemented in the first electro-mechanical encryption machines. We look at mathematical tools (language recognition, frequency counting, smoothing) developed to decrypt such ciphers on proto-computers. We show algorithms and extensive empirical results for solving different types of ciphers, and we show the role of algorithms in recent decipherments of historical documents. We then look at how foreign language can be viewed as a code for English, a concept developed by Alan Turing and Warren Weaver. We describe recently published work on building automatic translation systems from non-parallel data. We also demonstrate how some of the same algorithmic tools can be applied to natural language tasks like part-of-speech tagging and word alignment. Turning back to historical ciphers, we explore a number of unsolved ciphers, giving results of initial computer experiments on several of them. Finally, we look briefly at writing as a way to encipher phoneme sequences, covering ancient scripts and modern applications. 2 Outline 1. Classical military/diplomatic ciphers (15 minutes) • 60 cipher types (ACA) • Ciphers vs. codes • Enigma cipher: the mother of natural language processing computer analysis of text language recognition Good-Turing smoothing – – – 2. Foreign language as a code (10 minutes) • • Alan Turing’s ”Thinking Machines” Warren Weaver’s Memorandum 3. Automatic decipherment (55 minutes) • Cipher type detection • Substitution ciphers (simple, homophonic, polyalphabetic, etc) plaintext language recognition ∗ how much plaintext knowledge is – nheowede mdu 3 Proce diSnogfsia, of B thuleg5a r1iast, A Anungu aslt M4-9e t2in01g3 o.f ? tc he20 A1s3so Acsiasoticoinat fio rn C fo rm Cpoumtaptuiotantaioln Lainlg Luinisgtuicis ,tpi casges 3–4, – ∗ index of coincidence, unicity distance, oanf dc oointhceidr measures navigating a difficult search space ∗ frequencies of letters and words ∗ pattern words and cribs ∗ pElMin,g ILP, Bayesian models, sam– recent decipherments ∗ Jefferson cipher, Copiale cipher, cJievfifle war ciphers, n Caovaplia Enigma • • • • Application to part-of-speech tagging, Awopprdli alignment Application to machine translation withoAuptp parallel t teoxtm Parallel development of cryptography aPnarda ltrleanls dlaetvioenlo Recently released NSA internal nReewcselnettlyter (1974-1997) 4. *** Break *** (30 minutes) 5. Unsolved ciphers (40 minutes) • Zodiac 340 (1969), including computatZioodnaial cw 3o4r0k • Voynich Manuscript (early 1400s), including computational ewarolyrk • Beale (1885) • Dorabella (1897) • Taman Shud (1948) • Kryptos (1990), including computatKiorynaplt owsor (k1 • McCormick (1999) • Shoeboxes in attics: DuPonceau jour- nal, Finnerana, SYP, Mopse, diptych 6. Writing as a code (20 minutes) • Does writing encode ideas, or does it encDoodees phonemes? • Ancient script decipherment Egyptian hieroglyphs Linear B Mayan glyphs – – – – wUgoarkritic, including computational Chinese N ¨ushu, including computational work • Automatic phonetic decipherment • Application to transliteration 7. Undeciphered writing systems (15 minutes) • Indus Valley Script (3300BC) • Linear A (1900BC) • Phaistos disc (1700BC?) • Rongorongo (1800s?) – 8. Conclusion and further questions (15 minutes) 3 About the Presenter Kevin Knight is a Senior Research Scientist and Fellow at the Information Sciences Institute of the University of Southern California (USC), and a Research Professor in USC’s Computer Science Department. He received a PhD in computer science from Carnegie Mellon University and a bachelor’s degree from Harvard University. Professor Knight’s research interests include natural language processing, machine translation, automata theory, and decipherment. In 2001, he co-founded Language Weaver, Inc., and in 2011, he served as President of the Association for Computational Linguistics. Dr. Knight has taught computer science courses at USC for more than fifteen years and co-authored the widely adopted textbook Artificial Intelligence. 4</p><p>4 0.30118826 <a title="109-tfidf-4" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>Author: Sujith Ravi</p><p>Abstract: In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</p><p>5 0.13942493 <a title="109-tfidf-5" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>Author: Young-Bum Kim ; Benjamin Snyder</p><p>Abstract: In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic distinctions. On a three-way classification task between vowels, nasals, and nonnasal consonants, our model yields unsu- pervised accuracy of 89% across the same set of languages.</p><p>6 0.085682541 <a title="109-tfidf-6" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>7 0.082204908 <a title="109-tfidf-7" href="./acl-2013-SEMILAR%3A_The_Semantic_Similarity_Toolkit.html">304 acl-2013-SEMILAR: The Semantic Similarity Toolkit</a></p>
<p>8 0.061365202 <a title="109-tfidf-8" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>9 0.059708834 <a title="109-tfidf-9" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>10 0.056196686 <a title="109-tfidf-10" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>11 0.055365007 <a title="109-tfidf-11" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>12 0.048574876 <a title="109-tfidf-12" href="./acl-2013-On_the_Predictability_of_Human_Assessment%3A_when_Matrix_Completion_Meets_NLP_Evaluation.html">263 acl-2013-On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation</a></p>
<p>13 0.046850454 <a title="109-tfidf-13" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<p>14 0.039976198 <a title="109-tfidf-14" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>15 0.039975762 <a title="109-tfidf-15" href="./acl-2013-Unsupervised_Transcription_of_Historical_Documents.html">370 acl-2013-Unsupervised Transcription of Historical Documents</a></p>
<p>16 0.039273556 <a title="109-tfidf-16" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>17 0.039051399 <a title="109-tfidf-17" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>18 0.037901103 <a title="109-tfidf-18" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>19 0.033798415 <a title="109-tfidf-19" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>20 0.033658698 <a title="109-tfidf-20" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.099), (1, -0.037), (2, 0.047), (3, 0.006), (4, -0.002), (5, -0.028), (6, 0.047), (7, -0.003), (8, -0.132), (9, -0.054), (10, -0.022), (11, -0.135), (12, -0.108), (13, -0.306), (14, 0.033), (15, -0.422), (16, -0.06), (17, -0.147), (18, -0.088), (19, 0.272), (20, 0.023), (21, 0.064), (22, -0.171), (23, -0.146), (24, 0.064), (25, 0.048), (26, 0.104), (27, -0.029), (28, -0.123), (29, -0.035), (30, 0.002), (31, 0.049), (32, 0.035), (33, -0.018), (34, -0.027), (35, -0.026), (36, 0.014), (37, -0.041), (38, 0.098), (39, -0.015), (40, 0.038), (41, 0.0), (42, 0.036), (43, 0.003), (44, -0.006), (45, -0.009), (46, -0.009), (47, -0.037), (48, -0.016), (49, -0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97214884 <a title="109-lsi-1" href="./acl-2013-Decipherment_Complexity_in_1%3A1_Substitution_Ciphers.html">109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Hermann Ney</p><p>Abstract: In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchanging them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before.</p><p>2 0.94045103 <a title="109-lsi-2" href="./acl-2013-Decipherment.html">108 acl-2013-Decipherment</a></p>
<p>Author: Kevin Knight</p><p>Abstract: The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. This tutorial explores connections between early decipherment research and today’s NLP work. We cover classic military and diplomatic ciphers, automatic decipherment algorithms, unsolved ciphers, language translation as decipherment, and analyzing ancient writing as decipherment. 1 Tutorial Overview The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. Sixty years later, we have many more applications, including web search, question answering, summarization, speech recognition, and language translation. This tutorial explores connections between early decipherment research and today’s NLP work. We find that many ideas from the earlier era have become core to the field, while others still remain to be picked up and developed. We first cover classic military and diplomatic cipher types, including complex substitution ciphers implemented in the first electro-mechanical encryption machines. We look at mathematical tools (language recognition, frequency counting, smoothing) developed to decrypt such ciphers on proto-computers. We show algorithms and extensive empirical results for solving different types of ciphers, and we show the role of algorithms in recent decipherments of historical documents. We then look at how foreign language can be viewed as a code for English, a concept developed by Alan Turing and Warren Weaver. We describe recently published work on building automatic translation systems from non-parallel data. We also demonstrate how some of the same algorithmic tools can be applied to natural language tasks like part-of-speech tagging and word alignment. Turning back to historical ciphers, we explore a number of unsolved ciphers, giving results of initial computer experiments on several of them. Finally, we look briefly at writing as a way to encipher phoneme sequences, covering ancient scripts and modern applications. 2 Outline 1. Classical military/diplomatic ciphers (15 minutes) • 60 cipher types (ACA) • Ciphers vs. codes • Enigma cipher: the mother of natural language processing computer analysis of text language recognition Good-Turing smoothing – – – 2. Foreign language as a code (10 minutes) • • Alan Turing’s ”Thinking Machines” Warren Weaver’s Memorandum 3. Automatic decipherment (55 minutes) • Cipher type detection • Substitution ciphers (simple, homophonic, polyalphabetic, etc) plaintext language recognition ∗ how much plaintext knowledge is – nheowede mdu 3 Proce diSnogfsia, of B thuleg5a r1iast, A Anungu aslt M4-9e t2in01g3 o.f ? tc he20 A1s3so Acsiasoticoinat fio rn C fo rm Cpoumtaptuiotantaioln Lainlg Luinisgtuicis ,tpi casges 3–4, – ∗ index of coincidence, unicity distance, oanf dc oointhceidr measures navigating a difficult search space ∗ frequencies of letters and words ∗ pattern words and cribs ∗ pElMin,g ILP, Bayesian models, sam– recent decipherments ∗ Jefferson cipher, Copiale cipher, cJievfifle war ciphers, n Caovaplia Enigma • • • • Application to part-of-speech tagging, Awopprdli alignment Application to machine translation withoAuptp parallel t teoxtm Parallel development of cryptography aPnarda ltrleanls dlaetvioenlo Recently released NSA internal nReewcselnettlyter (1974-1997) 4. *** Break *** (30 minutes) 5. Unsolved ciphers (40 minutes) • Zodiac 340 (1969), including computatZioodnaial cw 3o4r0k • Voynich Manuscript (early 1400s), including computational ewarolyrk • Beale (1885) • Dorabella (1897) • Taman Shud (1948) • Kryptos (1990), including computatKiorynaplt owsor (k1 • McCormick (1999) • Shoeboxes in attics: DuPonceau jour- nal, Finnerana, SYP, Mopse, diptych 6. Writing as a code (20 minutes) • Does writing encode ideas, or does it encDoodees phonemes? • Ancient script decipherment Egyptian hieroglyphs Linear B Mayan glyphs – – – – wUgoarkritic, including computational Chinese N ¨ushu, including computational work • Automatic phonetic decipherment • Application to transliteration 7. Undeciphered writing systems (15 minutes) • Indus Valley Script (3300BC) • Linear A (1900BC) • Phaistos disc (1700BC?) • Rongorongo (1800s?) – 8. Conclusion and further questions (15 minutes) 3 About the Presenter Kevin Knight is a Senior Research Scientist and Fellow at the Information Sciences Institute of the University of Southern California (USC), and a Research Professor in USC’s Computer Science Department. He received a PhD in computer science from Carnegie Mellon University and a bachelor’s degree from Harvard University. Professor Knight’s research interests include natural language processing, machine translation, automata theory, and decipherment. In 2001, he co-founded Language Weaver, Inc., and in 2011, he served as President of the Association for Computational Linguistics. Dr. Knight has taught computer science courses at USC for more than fifteen years and co-authored the widely adopted textbook Artificial Intelligence. 4</p><p>3 0.90794593 <a title="109-lsi-3" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Julian Schamper ; Hermann Ney</p><p>Abstract: In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet- ter (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.</p><p>4 0.4282777 <a title="109-lsi-4" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>Author: Sujith Ravi</p><p>Abstract: In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</p><p>5 0.33084983 <a title="109-lsi-5" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>Author: Young-Bum Kim ; Benjamin Snyder</p><p>Abstract: In this paper, we present a solution to one aspect of the decipherment task: the prediction of consonants and vowels for an unknown language and alphabet. Adopting a classical Bayesian perspective, we performs posterior inference over hundreds of languages, leveraging knowledge of known languages and alphabets to uncover general linguistic patterns of typologically coherent language clusters. We achieve average accuracy in the unsupervised consonant/vowel prediction task of 99% across 503 languages. We further show that our methodology can be used to predict more fine-grained phonetic distinctions. On a three-way classification task between vowels, nasals, and nonnasal consonants, our model yields unsu- pervised accuracy of 89% across the same set of languages.</p><p>6 0.28186971 <a title="109-lsi-6" href="./acl-2013-Unsupervised_Transcription_of_Historical_Documents.html">370 acl-2013-Unsupervised Transcription of Historical Documents</a></p>
<p>7 0.22773798 <a title="109-lsi-7" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>8 0.2053792 <a title="109-lsi-8" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>9 0.19990014 <a title="109-lsi-9" href="./acl-2013-Grammatical_Error_Correction_Using_Integer_Linear_Programming.html">171 acl-2013-Grammatical Error Correction Using Integer Linear Programming</a></p>
<p>10 0.19422358 <a title="109-lsi-10" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>11 0.1852854 <a title="109-lsi-11" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>12 0.18140072 <a title="109-lsi-12" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<p>13 0.18065643 <a title="109-lsi-13" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>14 0.17374611 <a title="109-lsi-14" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>15 0.1724041 <a title="109-lsi-15" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>16 0.16639216 <a title="109-lsi-16" href="./acl-2013-PhonMatrix%3A_Visualizing_co-occurrence_constraints_of_sounds.html">279 acl-2013-PhonMatrix: Visualizing co-occurrence constraints of sounds</a></p>
<p>17 0.15587273 <a title="109-lsi-17" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>18 0.1449679 <a title="109-lsi-18" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>19 0.14403147 <a title="109-lsi-19" href="./acl-2013-Variable_Bit_Quantisation_for_LSH.html">381 acl-2013-Variable Bit Quantisation for LSH</a></p>
<p>20 0.14345707 <a title="109-lsi-20" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.087), (6, 0.038), (8, 0.212), (11, 0.037), (15, 0.032), (24, 0.026), (26, 0.035), (28, 0.012), (35, 0.047), (42, 0.039), (48, 0.057), (64, 0.012), (70, 0.037), (88, 0.019), (95, 0.211)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85286438 <a title="109-lda-1" href="./acl-2013-Decipherment_Complexity_in_1%3A1_Substitution_Ciphers.html">109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Hermann Ney</p><p>Abstract: In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchanging them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before.</p><p>2 0.75076509 <a title="109-lda-2" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Julian Schamper ; Hermann Ney</p><p>Abstract: In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet- ter (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.</p><p>3 0.74579298 <a title="109-lda-3" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>Author: Congle Zhang ; Tyler Baldwin ; Howard Ho ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normal- ization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</p><p>4 0.74514562 <a title="109-lda-4" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>Author: Silvana Hartmann ; Iryna Gurevych</p><p>Abstract: We present a new bilingual FrameNet lexicon for English and German. It is created through a simple, but powerful approach to construct a FrameNet in any language using Wiktionary as an interlingual representation. Our approach is based on a sense alignment of FrameNet and Wiktionary, and subsequent translation disambiguation into the target language. We perform a detailed evaluation of the created resource and a discussion of Wiktionary as an interlingual connection for the cross-language transfer of lexicalsemantic resources. The created resource is publicly available at http : / /www . ukp .tu-darmst adt .de / fnwkde / .</p><p>5 0.73917896 <a title="109-lda-5" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>Author: Lucia Specia ; ; ; Kashif Shah ; Jose G.C. de Souza ; Trevor Cohn</p><p>Abstract: We describe QUEST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms.</p><p>6 0.73603982 <a title="109-lda-6" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>7 0.72902131 <a title="109-lda-7" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>8 0.72498322 <a title="109-lda-8" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>9 0.72388899 <a title="109-lda-9" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>10 0.72285587 <a title="109-lda-10" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>11 0.71484035 <a title="109-lda-11" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>12 0.71090227 <a title="109-lda-12" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>13 0.71060616 <a title="109-lda-13" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>14 0.70855534 <a title="109-lda-14" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>15 0.70738846 <a title="109-lda-15" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>16 0.70526773 <a title="109-lda-16" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>17 0.70497751 <a title="109-lda-17" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>18 0.70481277 <a title="109-lda-18" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>19 0.70100623 <a title="109-lda-19" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<p>20 0.6988979 <a title="109-lda-20" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
