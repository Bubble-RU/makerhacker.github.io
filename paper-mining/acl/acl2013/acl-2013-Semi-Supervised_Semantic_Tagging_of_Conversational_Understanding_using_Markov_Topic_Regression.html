<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-315" href="#">acl2013-315</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</h1>
<br/><p>Source: <a title="acl-2013-315-pdf" href="http://aclweb.org/anthology//P/P13/P13-1090.pdf">pdf</a></p><p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur ; Gokhan Tur ; Ruhi Sarikaya</p><p>Abstract: Microsoft Research Microsoft Mountain View, CA, USA Redmond, WA, USA dilek @ ieee .org rus arika@mi cro s o ft . com gokhan .tur @ ieee .org performance (Tur and DeMori, 2011). This requires a tedious and time intensive data collection Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem. To deal with these issues, we describe an efficient semisupervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (concepts). It can efficiently handle semantic ambiguity by extending standard topic models with two new features. First, it encodes word n-gram features from labeled source and unlabeled target data. Second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.</p><p>Reference: <a title="acl-2013-315-reference" href="../acl2013_reference/acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This requires a tedious and time intensive data collection Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. [sent-7, score-0.226]
</p><p>2 To deal with  these issues, we describe an efficient semisupervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (concepts). [sent-9, score-0.191]
</p><p>3 It can efficiently handle semantic ambiguity by extending standard topic models with two new features. [sent-10, score-0.164]
</p><p>4 First, it encodes word n-gram features from labeled source and unlabeled target data. [sent-11, score-0.26]
</p><p>5 Second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. [sent-12, score-0.17]
</p><p>6 (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. [sent-13, score-0.176]
</p><p>7 Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging. [sent-14, score-0.162]
</p><p>8 1 Introduction  Semantic tagging is used in natural language understanding (NLU) to recognize words of semantic importance in an utterance, such as entities. [sent-15, score-0.162]
</p><p>9 Typically, a semantic tagging model require large amount of domain specific data to achieve good and labeling process. [sent-16, score-0.226]
</p><p>10 Using the trained model, it decodes unlabeled dataset from the target domain. [sent-22, score-0.176]
</p><p>11 Our first contribution is a  new probabilistic topic model, Markov Topic Regression (MTR), which uses rich features to capture the degree of association between words and semantic tags. [sent-24, score-0.164]
</p><p>12 First, it encodes the n-gram context features from the labeled source data and the unlabeled target data as prior information to learn semantic classes based on context. [sent-25, score-0.402]
</p><p>13 Thus, each latent semantic class corresponds to one of the semantic tags found in labeled data. [sent-26, score-0.36]
</p><p>14 Our SSL uses MTR to smooth the semantic tag posteriors on the unlabeled target data (decoded using the CRF model) and later obtains the best tag sequences. [sent-29, score-0.692]
</p><p>15 Although our iterative SSL learning model can deal with the training and test data mismatch, it neglects the performance effects caused by adapting the source domain to the target domain. [sent-33, score-0.2]
</p><p>16 While retrospective learning iteratively trains CRF models with the automatically annotated target data (explained above), it keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. [sent-40, score-0.326]
</p><p>17 In short, through a series of experiments we show how MTR clustering provides additional information to SSL on the target domain utterances, and greatly impacts semantic tagging performance. [sent-41, score-0.335]
</p><p>18 Specifically, we analyze MTR’s performance on two different types of semantic tags: named-entities and descriptive tags as shown in Table 1. [sent-42, score-0.313]
</p><p>19 Our experiments show that it is much  harder to detect descriptive tags compared to named-entities. [sent-43, score-0.233]
</p><p>20 To the best of our knowledge, our work is the first to explore the unlabeled data to iteratively adapt the semantic tagging models for target domains, preserving information from the previous iterations. [sent-45, score-0.338]
</p><p>21 , we extend the earlier work on SSL partof-speech (POS) tagging and show in the experiments that our approach is not only useful for semantic tagging but also syntactic tagging. [sent-47, score-0.244]
</p><p>22 Supervised methods for semantic tagging in NLU require a large number of in-domain human-labeled utterances and gazetteers (movie, actor names, etc. [sent-50, score-0.252]
</p><p>23 ), increas-  ances from movie domain, named-entities and descriptive tags. [sent-51, score-0.235]
</p><p>24 They extract semantic lexicons from unlabeled web queries, to use as features. [sent-57, score-0.199]
</p><p>25 Our work differs from these, in that, rather than just detecting named-entities, our utterances include descriptive tags (see Table 1). [sent-58, score-0.323]
</p><p>26 • Typically the source domain has different distrib•u Ttiyopni cthalalny t thhee target domain, d huaes to topic sth difitssin time, newly introduced features (e. [sent-59, score-0.233]
</p><p>27 Adapting the source domain using unlabeled data is the key to achieving good performance across domains. [sent-63, score-0.211]
</p><p>28 , 2010) an efficient iterative SSL method is described for syntactic tagging, using graph-based learning to smooth POS tag posteriors. [sent-67, score-0.25]
</p><p>29 We present a retrospective SSL for CRF, in that, the iterative learner keeps track of the errors of the previous iterations so as to carry the properties of both the source and target domains. [sent-74, score-0.32]
</p><p>30 While MTR has a similar Markovian property, we encode features on words to allow each word in an utterance to sample from  any of the given semantic tags, as in ”what are [scary]genre movies by [Hitchcock]director? [sent-97, score-0.2]
</p><p>31 (I) Semantic Tags (si): Each word wi of a given utterance with Nj words, ∈U, j=1,. [sent-111, score-0.166]
</p><p>32 a l|a,t efrnotm mse am seant otifc tag (state) Uva,r isia absleso si∈S, where S is the set of semantic tags. [sent-114, score-0.249]
</p><p>33 , 2005) described for documents, MTR samples each si from a Markov chain that is specific to its utterance uj. [sent-118, score-0.207]
</p><p>34 MTR allows for sampling of consecutive words from different tag clusters. [sent-120, score-0.213]
</p><p>35 (II) Tag Transition Indicator (ψv): Given utterance uj, the decision to sample a wi from a  uj={wi}iN=j1  new topic is determined by an indicator variable, cj,i, that is sampled from a Binomial(ψv=wi) distribution with a Beta conjugate prior. [sent-122, score-0.311]
</p><p>36 ) cj,i=1 suggests that a new state be sampled from K possible tags for the word wi in uj, and cj,i=0 suggests that the state si of wi should be the same as the previous word’s latent state si−1 . [sent-124, score-0.557]
</p><p>37 The first position of the sequence is sampled from a new state, hence cj,i=1=1 (III) Tag Transition Base Measure (η): Prior probability of a word given a tag should increase the chances of sampling words from the correct semantic tag. [sent-125, score-0.326]
</p><p>38 MTR constrains the generation of a tag si given the previous tag si−1 and the current wi based on cj,i by using a vocabulary specific Beta prior, ψv∼Beta(ηv) 1, on each word in vocabulary wv=1,. [sent-126, score-0.663]
</p><p>39 WBeet inject the prior information on semantic tags to define values of the base measure ηv using external knowledge from two sources: (a) Entity Priors (ηS): Prior probability on named-entities and descriptive tags denoted as  . [sent-129, score-0.486]
</p><p>40 com) and labeled training data to extract entity lists that  correspond to the semantic tags of our domains. [sent-135, score-0.271]
</p><p>41 (1), we assume that the prior on the semantic tags, ηS, is more indicative of the decision for sampling a wi from a new tag compared to language model posteriors on word sequences, ηW. [sent-141, score-0.507]
</p><p>42 Here we represent the base-measure (hyperparameter) of the semantic tag indicator variable, which is not to be confused with a probability measure 2  We update the indicator parameter via mean cri-  ψv=wi=PiK,j=1ηsv=i|swji/(K2). [sent-142, score-0.342]
</p><p>43 Algorithm 1Markov Topic Regression 1:foreach semantic tag topic sk, k ← 1,. [sent-146, score-0.333]
</p><p>44 , V∈ d Ro 5: − draw a tag indiinc avtoocra mbuilxatruyre v ψ ←v 1∼, B. [sent-155, score-0.169]
</p><p>45 We use blocked Gibbs sampling, in which the topic assignments sk and hyper-parameters {βk}kK=1 are alternately sampled at each Gibbs sampling lag period g given all other variables. [sent-187, score-0.222]
</p><p>46 We impose the prior knowledge on naturally related words, such that if two words ”funny” and ”hilarious” indicate the same given ”genre” class, then their latent tag distributions should also be similar. [sent-188, score-0.264]
</p><p>47 We use the word-tag posterior probabilities obtained from a CRF sequence model trained on labeled utterances as features. [sent-192, score-0.214]
</p><p>48 At the start of the Gibbs sampling, we designate t Ahet 917  K latent topics to the K semantic tags of our labeled data. [sent-198, score-0.309]
</p><p>49 This way we use observed scalar counts of each labeled word v associated with its semantic tag k, as the output label of its input vector, xlv; an indication of likelihood of words getting sampled from the corresponding semantic label sk. [sent-200, score-0.452]
</p><p>50 Since the impact of the  nk(gv),  asymmetric prior is equivalent to adding pseudocounts to the sufficient statistics of the semantic tag to which the word belongs, we predict the  βk(gv) using the scalar counts of the labeled data, n(kgv), based on the log-linear model in Eq. [sent-201, score-0.427]
</p><p>51 2  Collapsed Sampler  The goal of MTR is to infer the degree of relationship between a word v and each semantic tag k, φkv. [sent-206, score-0.249]
</p><p>52 Under the Markov assumption, for each word wi=v in a given utterance uj, if cj,i=1, we sample a new tag si=k given the remaining tags and hyper-parameters βk, α, and  ψv,  ηwsii|=si−v1. [sent-211, score-0.362]
</p><p>53 , 2010), a new SSL method is described for adapting syntactic POS tagging of sentences in newswire articles along with search queries to a target domain of natural language (NL) questions. [sent-216, score-0.238]
</p><p>54 They decode unlabeled queries from target domain (t) using a CRF model trained on the POS-labeled newswire data (source domain (o)). [sent-217, score-0.37]
</p><p>55 The unlabeled POS tag posteriors are then smoothed using a graph-based learning algorithm. [sent-218, score-0.356]
</p><p>56 Since CRF tagger only uses local features of the input to score tag pairs, they  try to capture all the context with the graph with additional context features on types. [sent-220, score-0.169]
</p><p>57 Later, using viterbi decoding, they select the 1-best POS tag sequence, sj∗ for each utterance uj. [sent-221, score-0.251]
</p><p>58 The last term is the loss on unlabeled data from target domain with a hyper-parameter τ. [sent-224, score-0.24]
</p><p>59 The posterior probability of a tag sji=k given a word wji in unlabeled utterance uj from target domain (t) pˆn(j, i)= pˆn(sji=k|wji; ), is decoded using the n-th iteration C=RkF| wmodel. [sent-229, score-0.834]
</p><p>60 MTR uses the decoded probabilities as semantic tag prior features on vocabulary items. [sent-230, score-0.438]
</p><p>61 We generate a word-tag matrix of posteriors, x∈(0, 1)V ×K, where K is the number pofo ssetemriaonrsti,c x tags a,1nd) V is the vocabulary size from n-th iteration. [sent-231, score-0.169]
</p><p>62 Each row is a K dimensional vector of tag posterior probabilities xv={xv1,. [sent-232, score-0.237]
</p><p>63 The l=a{bxeled rows x}l oonf the vocabulary matrix, x={xl,xu}, contain only {0,1} values, indicating t=he{ word’}s, oc bosnteraivned o sem{0a,n1t}ic v tags i,n i nthdiec laatibnegled th eda wtao. [sent-236, score-0.169]
</p><p>64 S’sin ocbes a lvaebdel seedterm wv can have different tags (e. [sent-237, score-0.17]
</p><p>65 The  Λ(nt)  PkK  x is used as the input matriPx of the≥ ≥kt1h h log-linear model (corresponding to ktPh semantic tag (topic)) to infer the β hyper-parameter of MTR in Eq. [sent-240, score-0.249]
</p><p>66 MTR generates smoothed conditional probabilities φkv for each vocabulary term v given semantic tag k. [sent-242, score-0.331]
</p><p>67 For each word wji=v in unlabeled utterance uj, we interpolate tag marginals from CRF and MTR for each semantic tag sji = k: CRF posterior  ˆqn(sji|wij;Λ(nt)) = πˆz pn(sji|}w|ij;Λ(nt){) MTR{  +(1  − π)zφ}k|v{  (6)  III. [sent-245, score-0.847]
</p><p>68 Using viterbi decoding over the tag marginals, ˆ qn(sji|wij; and transition probabilities obtained fro|mw the CRF model of n-th 1-best decode  Λ(nt)),  iteration, we get pˆ n(sj∗|uj;Λ(nt)), the  sj∗ of each unlabeled u|tuterance uj∈Unu. [sent-247, score-0.369]
</p><p>69 Each iteration makes predictions on the semantic tags of unlabeled data with varying posterior probabilities. [sent-251, score-0.385]
</p><p>70 Thus, R-S SL encodes the history of the prior pre-  Algorithm 2 Retrospective Semi-Supervised CRF Input: Labeled Ul, and unlabeled Uudata. [sent-254, score-0.181]
</p><p>71 The last term ensures that the predictions of the current model have the same sign as the predictions of the previous models (using labeled and unlabeled data), denoted by a maximum margin  P1n−1 ˆpn(sj∗|uj; Λn(t)). [sent-264, score-0.175]
</p><p>72 MTR provides a separate probability distribution θj over tags for each utterance j,implicitly allowing for the same word v in separate utter-  ances to differ in tag posteriors φkv. [sent-266, score-0.457]
</p><p>73 Although the crowd-sourced data is similar  to target domain, in terms of pre-defined user intentions, the target domain contains more descriptive vocabulary, which is almost twice as large as the source domain. [sent-276, score-0.328]
</p><p>74 In total, our corpus has a 40K semantically tagged utterances from each source and target domains. [sent-278, score-0.208]
</p><p>75 We separated 5K utterances to test the performance of the semantic tagging models. [sent-280, score-0.252]
</p><p>76 ; whereas top descriptive tags are: genre (’feel good’), description  white’, ’pg 13’), review-rate (’epic’,  (’black and ’not for me’), theater-location (’near me’,’city center’), etc. [sent-282, score-0.277]
</p><p>77 Unlabeled utterances similar to the movie domain are pulled from a month old web query logs and extracted over 2 million search queries from well-known sites, e. [sent-283, score-0.275]
</p><p>78 The QuestionBank contains 4000 POS-tagged questions, however it is difficult to tag with WSJ-trained taggers because the word order is different than WSJ and contains a test-set vocabulary that is twice as large as the one in the development set. [sent-299, score-0.227]
</p><p>79 There are 36 different tag sets in the Penn dataset which includes tag labels for verbs, nouns, adjectives, adverbs, modal, determiners, prepositions, etc. [sent-303, score-0.338]
</p><p>80 More information about the Penn Tree-bank tag set can be found here (Marcus et al. [sent-304, score-0.169]
</p><p>81 1 Semantic Clustering Since MTR provides a mixture of properties adapted from earlier models, we present performance benchmarks on tag clustering using: (i) LDA; (ii) Hidden Markov Topic Model HMTM (Gruber et al. [sent-309, score-0.221]
</p><p>82 A CRF model is used to decode the unlabeled data to generate more labeled examples for re-training. [sent-324, score-0.206]
</p><p>83 , 2010) that uses graph-based learning as posterior tag smoother for CRF model using  Eq. [sent-327, score-0.237]
</p><p>84 smooth the semantic tag posteriors of a unlabeled data decoded by the CRF model using Eq. [sent-332, score-0.511]
</p><p>85 R-SSL-Graph: Our second version uses graph-learning to smooth the tag posteriors and retrain a new CRF model using retrospective SSL in Eq. [sent-334, score-0.451]
</p><p>86 R-SSL-MTR: Our full model uses MTR as a Bayesian smoothing model, and retrospective SSL in Eq. [sent-337, score-0.21]
</p><p>87 Here, we want to demonstrate the performance of MTR model for capturing relationships between words and semantic tags against baseline topic models: LDA, HMTM, w-LDA. [sent-361, score-0.275]
</p><p>88 We take the semantically labeled utterances from the movie target domain and use the first half for training and the rest for performance testing. [sent-362, score-0.386]
</p><p>89 We use all the collected unlabeled web queries from the movie domain. [sent-363, score-0.24]
</p><p>90 For fair comparison, each benchmark topic model is provided with prior information on word-semantic tag distributions based on the labeled training data, hence, each K latent topic is assigned to one of K semantic tags at the beginning of Gibbs sampling. [sent-364, score-0.679]
</p><p>91 As for the effect of word features in MTR, we see a 3% absolute performance gain over the second best performing HMTM baseline on named-entity tags, a 1% absolute gain on descriptive tags and a 2% absolute overall gain. [sent-369, score-0.233]
</p><p>92 Each SSL model is built using labeled training data from the source domain and unlabeled training data from target domain. [sent-375, score-0.324]
</p><p>93 Our Bayesian MTR efficiently extracts information from the unlabeled data for the target domain. [sent-382, score-0.176]
</p><p>94 Combined with retrospective training, R-SSL-MTR demonstrates noticeable improvements, ∼2% on descriptive tags, taincde 1bl%e mabpsoroluvteem gains ∼in2 %ove ornal dle sscemripatnivtiec tag921  ging performance over S SL-Graph. [sent-383, score-0.306]
</p><p>95 On syntactic tagging, the two retrospective learning models is comparable, close to 1% improvement over the S SL-Graph and S SL-MTR. [sent-384, score-0.184]
</p><p>96 We investigate words that have more than one observed semantic tag in training data, such as ”are there any [war]genre movies available. [sent-391, score-0.287]
</p><p>97 In Table 3 we show two most frequent descriptive tags; genre and description, and commonly misclassified words by the two models. [sent-400, score-0.166]
</p><p>98 riGf96R4570c8A% aPgtHieonr1pM8096Tr% RfomGa687nd5−0Ace% PsHrinptF1o86-M07mTn% Reasur  for semantically ambiguous words on the most frequently confused descriptive tags in the movie domain. [sent-404, score-0.389]
</p><p>99 Domain adaptation with latent semantic association for named entity recognition. [sent-479, score-0.169]
</p><p>100 Crouching dirichlet, hidden markov model: Unsupervised pos tagging with context local tag generation. [sent-554, score-0.351]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mtr', 0.614), ('ssl', 0.389), ('retrospective', 0.184), ('sji', 0.184), ('uj', 0.182), ('tag', 0.169), ('crf', 0.164), ('si', 0.125), ('descriptive', 0.122), ('unlabeled', 0.119), ('tags', 0.111), ('utterances', 0.09), ('movie', 0.086), ('wi', 0.084), ('topic', 0.084), ('subramanya', 0.083), ('utterance', 0.082), ('tagging', 0.082), ('semantic', 0.08), ('questionbank', 0.077), ('markov', 0.076), ('nt', 0.074), ('posteriors', 0.068), ('domain', 0.064), ('prior', 0.062), ('beta', 0.062), ('hmtm', 0.061), ('wv', 0.059), ('vocabulary', 0.058), ('target', 0.057), ('sj', 0.057), ('labeled', 0.056), ('clustering', 0.052), ('iterative', 0.051), ('kg', 0.05), ('gruber', 0.05), ('gibbs', 0.047), ('lavoie', 0.046), ('petterson', 0.046), ('priors', 0.045), ('decoded', 0.045), ('lda', 0.044), ('genre', 0.044), ('sampling', 0.044), ('posterior', 0.044), ('wji', 0.041), ('nlu', 0.041), ('tur', 0.039), ('movies', 0.038), ('gv', 0.038), ('confused', 0.037), ('lag', 0.035), ('queries', 0.035), ('scalar', 0.034), ('semantically', 0.033), ('latent', 0.033), ('sampled', 0.033), ('binomial', 0.032), ('ji', 0.032), ('adaptation', 0.032), ('mismatch', 0.031), ('xv', 0.031), ('iteration', 0.031), ('demori', 0.031), ('htmm', 0.031), ('kgv', 0.031), ('nyugen', 0.031), ('ogp', 0.031), ('sisi', 0.031), ('unu', 0.031), ('wsii', 0.031), ('xlv', 0.031), ('xvk', 0.031), ('decode', 0.031), ('smooth', 0.03), ('state', 0.029), ('regression', 0.029), ('pn', 0.029), ('xl', 0.029), ('topics', 0.029), ('indicator', 0.028), ('source', 0.028), ('dialog', 0.028), ('moon', 0.027), ('scary', 0.027), ('ances', 0.027), ('argmkin', 0.027), ('sampler', 0.027), ('transition', 0.026), ('asymmetric', 0.026), ('sk', 0.026), ('smoothing', 0.026), ('guo', 0.025), ('markovian', 0.025), ('semi', 0.025), ('sl', 0.024), ('entity', 0.024), ('probabilities', 0.024), ('hidden', 0.024), ('smoother', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="315-tfidf-1" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur ; Gokhan Tur ; Ruhi Sarikaya</p><p>Abstract: Microsoft Research Microsoft Mountain View, CA, USA Redmond, WA, USA dilek @ ieee .org rus arika@mi cro s o ft . com gokhan .tur @ ieee .org performance (Tur and DeMori, 2011). This requires a tedious and time intensive data collection Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem. To deal with these issues, we describe an efficient semisupervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (concepts). It can efficiently handle semantic ambiguity by extending standard topic models with two new features. First, it encodes word n-gram features from labeled source and unlabeled target data. Second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.</p><p>2 0.16883054 <a title="315-tfidf-2" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>3 0.1398198 <a title="315-tfidf-3" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>4 0.08873985 <a title="315-tfidf-4" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>Author: Xuezhe Ma ; Fei Xia</p><p>Abstract: In this paper, we propose a simple and effective approach to domain adaptation for dependency parsing. This is a feature augmentation approach in which the new features are constructed based on subtree information extracted from the autoparsed target domain data. To demonstrate the effectiveness of the proposed approach, we evaluate it on three pairs of source-target data, compared with several common baseline systems and previous approaches. Our approach achieves significant improvement on all the three pairs of data sets.</p><p>5 0.081603192 <a title="315-tfidf-5" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. Then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. The agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data. The segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature.</p><p>6 0.07996092 <a title="315-tfidf-6" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>7 0.079925045 <a title="315-tfidf-7" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>8 0.07989601 <a title="315-tfidf-8" href="./acl-2013-Improving_Chinese_Word_Segmentation_on_Micro-blog_Using_Rich_Punctuations.html">193 acl-2013-Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations</a></p>
<p>9 0.079683021 <a title="315-tfidf-9" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>10 0.077842668 <a title="315-tfidf-10" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>11 0.077138074 <a title="315-tfidf-11" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>12 0.07375323 <a title="315-tfidf-12" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>13 0.073719844 <a title="315-tfidf-13" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>14 0.069663152 <a title="315-tfidf-14" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>15 0.068079256 <a title="315-tfidf-15" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>16 0.067947142 <a title="315-tfidf-16" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>17 0.065529302 <a title="315-tfidf-17" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>18 0.064833529 <a title="315-tfidf-18" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>19 0.063450098 <a title="315-tfidf-19" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>20 0.062725194 <a title="315-tfidf-20" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.182), (1, 0.013), (2, -0.03), (3, 0.005), (4, 0.078), (5, -0.071), (6, 0.02), (7, -0.026), (8, -0.11), (9, -0.017), (10, 0.063), (11, -0.0), (12, 0.054), (13, 0.02), (14, -0.024), (15, -0.039), (16, -0.048), (17, 0.069), (18, 0.008), (19, -0.028), (20, -0.025), (21, -0.047), (22, 0.108), (23, 0.04), (24, 0.07), (25, 0.017), (26, 0.044), (27, -0.004), (28, -0.03), (29, -0.013), (30, -0.012), (31, 0.072), (32, -0.024), (33, 0.153), (34, -0.008), (35, 0.054), (36, 0.016), (37, -0.092), (38, -0.032), (39, -0.034), (40, -0.003), (41, -0.041), (42, 0.015), (43, 0.024), (44, 0.068), (45, 0.072), (46, 0.042), (47, 0.007), (48, 0.106), (49, -0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91157764 <a title="315-lsi-1" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur ; Gokhan Tur ; Ruhi Sarikaya</p><p>Abstract: Microsoft Research Microsoft Mountain View, CA, USA Redmond, WA, USA dilek @ ieee .org rus arika@mi cro s o ft . com gokhan .tur @ ieee .org performance (Tur and DeMori, 2011). This requires a tedious and time intensive data collection Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem. To deal with these issues, we describe an efficient semisupervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (concepts). It can efficiently handle semantic ambiguity by extending standard topic models with two new features. First, it encodes word n-gram features from labeled source and unlabeled target data. Second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.</p><p>2 0.78797776 <a title="315-lsi-2" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>Author: Michael Lucas ; Doug Downey</p><p>Abstract: Semi-supervised learning (SSL) methods augment standard machine learning (ML) techniques to leverage unlabeled data. SSL techniques are often effective in text classification, where labeled data is scarce but large unlabeled corpora are readily available. However, existing SSL techniques typically require multiple passes over the entirety of the unlabeled data, meaning the techniques are not applicable to large corpora being produced today. In this paper, we show that improving marginal word frequency estimates using unlabeled data can enable semi-supervised text classification that scales to massive unlabeled data sets. We present a novel learning algorithm, which optimizes a Naive Bayes model to accord with statistics calculated from the unlabeled corpus. In experiments with text topic classification and sentiment analysis, we show that our method is both more scalable and more accurate than SSL techniques from previous work.</p><p>3 0.66635054 <a title="315-lsi-3" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>4 0.66348702 <a title="315-lsi-4" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>Author: Fumiyo Fukumoto ; Yoshimi Suzuki ; Suguru Matsuyoshi</p><p>Abstract: This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.</p><p>5 0.61554569 <a title="315-lsi-5" href="./acl-2013-Part-of-speech_tagging_with_antagonistic_adversaries.html">277 acl-2013-Part-of-speech tagging with antagonistic adversaries</a></p>
<p>Author: Anders Sgaard</p><p>Abstract: Supervised NLP tools and on-line services are often used on data that is very different from the manually annotated data used during development. The performance loss observed in such cross-domain applications is often attributed to covariate shifts, with out-of-vocabulary effects as an important subclass. Many discriminative learning algorithms are sensitive to such shifts because highly indicative features may swamp other indicative features. Regularized and adversarial learning algorithms have been proposed to be more robust against covariate shifts. We present a new perceptron learning algorithm using antagonistic adversaries and compare it to previous proposals on 12 multilin- gual cross-domain part-of-speech tagging datasets. While previous approaches do not improve on our supervised baseline, our approach is better across the board with an average 4% error reduction.</p><p>6 0.5671041 <a title="315-lsi-6" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>7 0.56431615 <a title="315-lsi-7" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>8 0.55556643 <a title="315-lsi-8" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>9 0.55444491 <a title="315-lsi-9" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>10 0.55406475 <a title="315-lsi-10" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>11 0.53957659 <a title="315-lsi-11" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>12 0.53798515 <a title="315-lsi-12" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>13 0.53697586 <a title="315-lsi-13" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>14 0.52031219 <a title="315-lsi-14" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>15 0.52028799 <a title="315-lsi-15" href="./acl-2013-Transfer_Learning_Based_Cross-lingual_Knowledge_Extraction_for_Wikipedia.html">356 acl-2013-Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia</a></p>
<p>16 0.51668346 <a title="315-lsi-16" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>17 0.51344723 <a title="315-lsi-17" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>18 0.51023877 <a title="315-lsi-18" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>19 0.50882375 <a title="315-lsi-19" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>20 0.50589097 <a title="315-lsi-20" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.053), (4, 0.28), (6, 0.047), (11, 0.061), (24, 0.059), (26, 0.063), (28, 0.014), (35, 0.085), (42, 0.044), (48, 0.032), (70, 0.055), (88, 0.04), (90, 0.021), (95, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75664485 <a title="315-lda-1" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>Author: Asli Celikyilmaz ; Dilek Hakkani-Tur ; Gokhan Tur ; Ruhi Sarikaya</p><p>Abstract: Microsoft Research Microsoft Mountain View, CA, USA Redmond, WA, USA dilek @ ieee .org rus arika@mi cro s o ft . com gokhan .tur @ ieee .org performance (Tur and DeMori, 2011). This requires a tedious and time intensive data collection Finding concepts in natural language utterances is a challenging task, especially given the scarcity of labeled data for learning semantic ambiguity. Furthermore, data mismatch issues, which arise when the expected test (target) data does not exactly match the training data, aggravate this scarcity problem. To deal with these issues, we describe an efficient semisupervised learning (SSL) approach which has two components: (i) Markov Topic Regression is a new probabilistic model to cluster words into semantic tags (concepts). It can efficiently handle semantic ambiguity by extending standard topic models with two new features. First, it encodes word n-gram features from labeled source and unlabeled target data. Second, by going beyond a bag-of-words approach, it takes into account the inherent sequential nature of utterances to learn semantic classes based on context. (ii) Retrospective Learner is a new learning technique that adapts to the unlabeled target data. Our new SSL approach improves semantic tagging performance by 3% absolute over the baseline models, and also compares favorably on semi-supervised syntactic tagging.</p><p>2 0.75176191 <a title="315-lda-2" href="./acl-2013-Paraphrasing_Adaptation_for_Web_Search_Ranking.html">273 acl-2013-Paraphrasing Adaptation for Web Search Ranking</a></p>
<p>Author: Chenguang Wang ; Nan Duan ; Ming Zhou ; Ming Zhang</p><p>Abstract: Mismatch between queries and documents is a key issue for the web search task. In order to narrow down such mismatch, in this paper, we present an in-depth investigation on adapting a paraphrasing technique to web search from three aspects: a search-oriented paraphrasing model; an NDCG-based parameter optimization algorithm; an enhanced ranking model leveraging augmented features computed on paraphrases of original queries. Ex- periments performed on the large scale query-document data set show that, the search performance can be significantly improved, with +3.28% and +1.14% NDCG gains on dev and test sets respectively.</p><p>3 0.73203164 <a title="315-lda-3" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>4 0.71215713 <a title="315-lda-4" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>Author: Igor Labutov ; Hod Lipson</p><p>Abstract: We present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task. Recently, with an increase in computing resources, it became possible to learn rich word embeddings from massive amounts of unlabeled data. However, some methods take days or weeks to learn good embeddings, and some are notoriously difficult to train. We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with re- spect to several baselines, and observe that the approach is most useful when the training set is sufficiently small.</p><p>5 0.70082074 <a title="315-lda-5" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques.</p><p>6 0.64970541 <a title="315-lda-6" href="./acl-2013-Public_Dialogue%3A_Analysis_of_Tolerance_in_Online_Discussions.html">287 acl-2013-Public Dialogue: Analysis of Tolerance in Online Discussions</a></p>
<p>7 0.61948806 <a title="315-lda-7" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>8 0.61135191 <a title="315-lda-8" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>9 0.55395371 <a title="315-lda-9" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>10 0.53988826 <a title="315-lda-10" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>11 0.53973532 <a title="315-lda-11" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>12 0.53713977 <a title="315-lda-12" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>13 0.53390789 <a title="315-lda-13" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>14 0.53303492 <a title="315-lda-14" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>15 0.5326007 <a title="315-lda-15" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>16 0.53254056 <a title="315-lda-16" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>17 0.52998328 <a title="315-lda-17" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>18 0.5296824 <a title="315-lda-18" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>19 0.52967125 <a title="315-lda-19" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>20 0.52797252 <a title="315-lda-20" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
