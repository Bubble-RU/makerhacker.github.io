<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-16" href="#">acl2013-16</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</h1>
<br/><p>Source: <a title="acl-2013-16-pdf" href="http://aclweb.org/anthology//P/P13/P13-2066.pdf">pdf</a></p><p>Author: Mei Tu ; Yu Zhou ; Chengqing Zong</p><p>Abstract: Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively. 1</p><p>Reference: <a title="acl-2013-16-reference" href="../acl2013_reference/acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A Novel Translation Framework Based on Rhetorical Structure Theory Mei Tu Yu Zhou Chengqing Zong National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences { mtu yzhou cqz ong } @nlpr . [sent-1, score-0.058]
</p><p>2 cn  ,  ,  Abstract Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. [sent-4, score-0.331]
</p><p>3 In this paper, we propose a novel translation framework with the help of RST. [sent-5, score-0.207]
</p><p>4 1  Introduction  For statistical machine translation (SMT), a crucial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible. [sent-11, score-0.528]
</p><p>5 We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. [sent-14, score-0.263]
</p><p>6 , 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. [sent-17, score-0.176]
</p><p>7 Although some lexicons can be translated better by their models, the overall structure still remains unnatural. [sent-18, score-0.084]
</p><p>8 (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. [sent-20, score-0.161]
</p><p>9 Those reasons urge us to seek a new translation framework under the idea  of “translation with overall understanding”. [sent-21, score-0.207]
</p><p>10 Rhetorical structure theory (RST) (Mann and Thompson, 1988) provides us with a good perspective and inspiration to build such a framework. [sent-22, score-0.083]
</p><p>11 Generally, an RST tree can explicitly show the minimal spans with semantic functional integrity, which are called elementary discourse units (edus) (Marcu et al. [sent-23, score-0.308]
</p><p>12 , 2000), and it also depicts the hierarchical relations among edus. [sent-24, score-0.031]
</p><p>13 Furthermore, since different languages’ edus are usually equivalent on semantic level, it is intuitive to create a new framework based on RST by directly mapping the source edus to target ones. [sent-25, score-0.437]
</p><p>14 1 Annotation of Chinese RST Tree Similar to (Soricut and Marcu, 2003), a node of RST tree is represented as a tuple R-[s, m, e], which means the relation R controls two semantic spans U1 and U2 , U1 starts from word position s and stops at word position m. [sent-29, score-0.34]
</p><p>15 1 Although the rupe 's nominal rate against he dol ar was held down , India's real exchange rate rosebecause of high inflation . [sent-35, score-0.029]
</p><p>16 rhetorical relations for Chinese particularly, upon  which our Chinese RST parser is developed. [sent-37, score-0.311]
</p><p>17 Figure 1 illustrates an example of Chinese RST tree and its alignment to the English string. [sent-38, score-0.105]
</p><p>18 The Antithesis relation controls U1 from 0 to 9 and U2 from 10 to 21. [sent-40, score-0.1]
</p><p>19 Different shadow blocks denote the alignments of different edus. [sent-42, score-0.029]
</p><p>20 Links between source and target words are alignments of cue words. [sent-43, score-0.24]
</p><p>21 Cue words are viewed as the strongest clues for rhetorical relation recognition and always found at the beginning of text (Reitter, 2003), such as “即 使(although), 由 于(because of)”. [sent-44, score-0.293]
</p><p>22 With the cue words included, the relations are much easier to be analyzed. [sent-45, score-0.191]
</p><p>23 So we focus on the explicit relations with cue words in this paper as our first try. [sent-46, score-0.191]
</p><p>24 One is the segmentation of edu and the other is the relation tagging between two semantic spans. [sent-49, score-0.097]
</p><p>25 Inspired by the features used in English RST parser (Soricut and Marcu, 2003; Reitter, 2003; Duverle and Prendinger, 2009; Hernault et al. [sent-53, score-0.067]
</p><p>26 , 2010a), we design a Bayesian model to build a joint parser for segmentation and tagging simultaneously. [sent-54, score-0.114]
</p><p>27 In the table, punctuations include comma, semicolons, period and question mark. [sent-56, score-0.043]
</p><p>28 We view explicit connectives as cue words in this paper. [sent-57, score-0.16]
</p><p>29 Figure 2 illustrates the conditional independences of 9 features which are denoted with F1~F9. [sent-58, score-0.105]
</p><p>30 F1F2mF8F3RelF4F5Fe6F7F9 Figure 2: The graph for conditional independences of 9 features. [sent-59, score-0.105]
</p><p>31 The segmentation and parsing conditional probabilities are computed as follows: P(mjF19) = P(mjF13; F8)  (1)  P(ejF19) = P(ejF47;F9)  (2)  P(ReljF19) = P(ReljF34) (3) where Fn represents the nth feature , Fnl means features from n to l. [sent-60, score-0.131]
</p><p>32 (1) and (2) describe the conditional probabilities of m and e. [sent-62, score-0.084]
</p><p>33 Finally, the relation is figured out by Formula (3). [sent-69, score-0.05]
</p><p>34 A complete RST tree con-  structs until the end of the iterative process for this sentence. [sent-71, score-0.091]
</p><p>35 It is plausible in our cases, because we only have a small scale of manually-annotated Chinese RST corpus, which prefers simple rather than complicated models. [sent-73, score-0.056]
</p><p>36 1  Translation Model Rule Extraction  As shown in Figure 1, the RST tree-to-string alignment provides us with two types of translation rules. [sent-75, score-0.219]
</p><p>37 The other is RST tree-tostring rule, and it’s defined as, relation ::U1(®; X)=U2(°; Y ) ) U1(tr(®); tr(X)) » U2(tr(°); tr(Y )) where the terminal characters α and γ represent the cue words which are optimum match for maximizing Formula (3). [sent-78, score-0.21]
</p><p>38 The operator ~ is an operator to indicate that the order of tr(U1) and tr(U2) is monotone or  reverse. [sent-81, score-0.072]
</p><p>39 During rules’ extraction, if the mean position of all the words in tr(U1) precedes that in tr(U2), ~ is monotone. [sent-82, score-0.029]
</p><p>40 For example in Figure 1, the Reason relation controls U1: [10,13] and U2: [14,21]. [sent-84, score-0.1]
</p><p>41 Because the mean position of tr(U2) is before that of tr(U1), the reverse order is selected. [sent-85, score-0.029]
</p><p>42 We list the RSTbased rules for Example 1in Figure 1. [sent-86, score-0.054]
</p><p>43 2  Probabilities Estimation  For the phrase-based translation rules, we use four common probabilities and the probabilities’ estimation is the same with those in (Koehn et al. [sent-88, score-0.221]
</p><p>44 While the probabilities of RST-based translation rules are given as follows, (1) P(rejrf;Rel) CCouounnt(tr(er;frf;r;erlealtaiotino)n): where =  re  is the target side of the rule, ignorance of the order, i. [sent-90, score-0.304]
</p><p>45 U1(tr(®); tr(X)) » U2(tr(°); tr(Y )) with two directions, rf is the source side, i. [sent-92, score-0.165]
</p><p>46 U1(®; X)=U2(°; Y) , and Rel means the relation type. [sent-94, score-0.05]
</p><p>47 ¿ 2 fmonotone; It is the conditional probability of re-ordering. [sent-96, score-0.039]
</p><p>48 4 Decoding The decoding procedure of a discourse can be derived from the original decoding formula e1I = argmaxe1IP(e1I jfJ1) . [sent-97, score-0.329]
</p><p>49 es is the target string combined by series of en (translations of fn). [sent-99, score-0.029]
</p><p>50 eu1 and eu2 are translations of U1 and U2 respectively. [sent-101, score-0.055]
</p><p>51 fcp and ecp are cue-words pair of source and target sides. [sent-103, score-0.212]
</p><p>52 The first and second factors are just the probabilities introduced in Section 3. [sent-104, score-0.045]
</p><p>53 Suppose the best rules selected by (4) are just those written in the figure, Then span [11,13] and [14,21] are firstly translated by (5) and (6). [sent-107, score-0.147]
</p><p>54 Their translations are then re-packaged by the rule of Reason-  = =  ;  ;  ;  ;  [10,13,21]. [sent-108, score-0.106]
</p><p>55 Iteratively, the translations of span [1,9] and [10,21] are re-packaged by the rule of Antithesis-[0,9,21] to form the final translation. [sent-109, score-0.152]
</p><p>56 In Figure 1, U1 and U2 of Reason node are firstly translated. [sent-111, score-0.04]
</p><p>57 Then the translations of two spans of Antithesis node are re-ordered and constructed into the final translation. [sent-113, score-0.175]
</p><p>58 In our decoders, language model(LM) is used for translating edus in Formula(5),(6),(7),(8), but not for reordering the upper spans because with the bottom-to-up combination, the spans become longer and harder to be judged by a traditional language model. [sent-116, score-0.357]
</p><p>59 So we only use RST rules to guide the reordering. [sent-117, score-0.054]
</p><p>60 1 Setup In order to do Chinese RST parser, we annotated over 1,000 complicated sentences on CTB (Xue et al. [sent-120, score-0.056]
</p><p>61 We obtain  the word alignment with the grow-diag-final-and strategy by GIZA++4. [sent-127, score-0.043]
</p><p>62 For tuning and testing, we use NIST03 evaluation data as the development set, and extract the relatively long and complicated sentences from NIST04, NIST05 and CWMT085 evaluation data as the test set. [sent-133, score-0.056]
</p><p>63 To create the baseline system, we use the toolkit Moses6 to build a phrase-based translation system. [sent-136, score-0.176]
</p><p>64 (2009) have presented good results by dividing long and complicated sentences into subsentences only by punctuations during decoding,  we re-implement their method for comparison. [sent-138, score-0.099]
</p><p>65 The parsing errors mostly result from the segmentation errors, which are mainly caused by syntactic parsing errors. [sent-142, score-0.047]
</p><p>66 On the other hand, the polysemous cue words, such as “而(but, and, thus)” may lead ambiguity for relation recognition, because they can be clues for different relations. [sent-143, score-0.24]
</p><p>67 3  Results of Translation  Table 3 presents the translation comparison results. [sent-149, score-0.176]
</p><p>68 Observing and comparing the translation results, we find that our translation results are more readable by maintaining the semantic integrality of the edus and by giving more appreciate reorganization of the translated edus. [sent-161, score-0.562]
</p><p>69 HomePage 373  6  Conclusion and Future Work  In this paper, we present an RST-based translation framework for modeling semantic structures in translation model, so as to maintain the semantically functional integrity and hierarchical relations of edus during translating. [sent-171, score-0.67]
</p><p>70 With respect  to the existing models, we think our translation framework works more similarly to what human does, and we believe that this research is a crucial step towards discourse-oriented translation. [sent-172, score-0.207]
</p><p>71 In the next step, we will study on the implicit discourse relations for Chinese and further modify the RST-based framework. [sent-173, score-0.155]
</p><p>72 Besides, we will try to combine other current translation models such as syntactic model and hierarchical model into our framework. [sent-174, score-0.176]
</p><p>73 Furthermore, the more accurate evaluation metric for discourse-oriented translation will be further studied. [sent-175, score-0.176]
</p><p>74 A novel discourse parser based on support vector ma-  chine classification. [sent-181, score-0.191]
</p><p>75 Hilda: A discourse parser using support vector machine classification. [sent-195, score-0.191]
</p><p>76 Rhetorical structure theory: Description and construction of text structures. [sent-204, score-0.037]
</p><p>77 Rhetorical structure theory: A framework for the analysis of texts. [sent-208, score-0.068]
</p><p>78 Rhetorical structure theory: Toward a functional theory of text organization. [sent-212, score-0.125]
</p><p>79 Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models. [sent-221, score-0.213]
</p><p>80 Sentence level discourse parsing using syntactic and lexical in-  formation. [sent-225, score-0.124]
</p><p>81 Extending machine translation evaluation metrics with lexical cohesion to document level. [sent-230, score-0.176]
</p><p>82 The Penn Chinese treebank: Phrase structure annotation of a large corpus. [sent-244, score-0.037]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rst', 0.559), ('rel', 0.273), ('rhetorical', 0.213), ('tr', 0.194), ('translation', 0.176), ('antithesis', 0.166), ('jfn', 0.166), ('edus', 0.163), ('cue', 0.16), ('rejrf', 0.133), ('chinese', 0.131), ('jre', 0.127), ('discourse', 0.124), ('ey', 0.121), ('rf', 0.114), ('formula', 0.109), ('jfx', 0.1), ('jfy', 0.1), ('spans', 0.08), ('hernault', 0.076), ('mann', 0.074), ('ex', 0.071), ('parser', 0.067), ('ecp', 0.066), ('fcp', 0.066), ('fpr', 0.066), ('independences', 0.066), ('jrf', 0.066), ('reitter', 0.066), ('marcu', 0.065), ('tree', 0.062), ('xiong', 0.06), ('duverle', 0.059), ('decoder', 0.058), ('complicated', 0.056), ('fp', 0.056), ('translations', 0.055), ('rules', 0.054), ('formulae', 0.054), ('soricut', 0.052), ('fn', 0.052), ('sandra', 0.051), ('rule', 0.051), ('source', 0.051), ('dtic', 0.051), ('gong', 0.051), ('integrity', 0.051), ('prendinger', 0.051), ('controls', 0.05), ('relation', 0.05), ('decoding', 0.048), ('segmentation', 0.047), ('translated', 0.047), ('mitsuru', 0.046), ('span', 0.046), ('theory', 0.046), ('probabilities', 0.045), ('xd', 0.045), ('punctuations', 0.043), ('alignment', 0.043), ('functional', 0.042), ('hugo', 0.041), ('node', 0.04), ('conditional', 0.039), ('structure', 0.037), ('operator', 0.036), ('reason', 0.036), ('helmut', 0.036), ('simplified', 0.035), ('translating', 0.034), ('hao', 0.034), ('wong', 0.032), ('ldc', 0.031), ('framework', 0.031), ('relations', 0.031), ('optimization', 0.031), ('smt', 0.03), ('clues', 0.03), ('xiao', 0.03), ('william', 0.03), ('billy', 0.029), ('dol', 0.029), ('enablement', 0.029), ('fnl', 0.029), ('hilda', 0.029), ('ishizuka', 0.029), ('ju', 0.029), ('mtu', 0.029), ('rhetorics', 0.029), ('semicolons', 0.029), ('shadow', 0.029), ('structs', 0.029), ('wenwen', 0.029), ('yzhou', 0.029), ('position', 0.029), ('xue', 0.029), ('target', 0.029), ('lm', 0.029), ('koehn', 0.028), ('approximately', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="16-tfidf-1" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>Author: Mei Tu ; Yu Zhou ; Chengqing Zong</p><p>Abstract: Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively. 1</p><p>2 0.2400559 <a title="16-tfidf-2" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>3 0.15242675 <a title="16-tfidf-3" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>4 0.12062662 <a title="16-tfidf-4" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>Author: Graham Neubig</p><p>Abstract: In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http : / /phont ron . com/t ravat ar</p><p>5 0.11951277 <a title="16-tfidf-5" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>Author: Marzieh Bazrafshan ; Daniel Gildea</p><p>Abstract: We experiment with adding semantic role information to a string-to-tree machine translation system based on the rule extraction procedure of Galley et al. (2004). We compare methods based on augmenting the set of nonterminals by adding semantic role labels, and altering the rule extraction process to produce a separate set of rules for each predicate that encompass its entire predicate-argument structure. Our results demonstrate that the second approach is effective in increasing the quality of translations.</p><p>6 0.11853765 <a title="16-tfidf-6" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>7 0.11586376 <a title="16-tfidf-7" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>8 0.11368617 <a title="16-tfidf-8" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>9 0.11131318 <a title="16-tfidf-9" href="./acl-2013-Improving_Chinese_Word_Segmentation_on_Micro-blog_Using_Rich_Punctuations.html">193 acl-2013-Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations</a></p>
<p>10 0.11048167 <a title="16-tfidf-10" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>11 0.10862643 <a title="16-tfidf-11" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>12 0.10336972 <a title="16-tfidf-12" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>13 0.099280782 <a title="16-tfidf-13" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>14 0.099185556 <a title="16-tfidf-14" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>15 0.09534736 <a title="16-tfidf-15" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>16 0.093929455 <a title="16-tfidf-16" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>17 0.090991296 <a title="16-tfidf-17" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>18 0.090915084 <a title="16-tfidf-18" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>19 0.085525222 <a title="16-tfidf-19" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>20 0.085468367 <a title="16-tfidf-20" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.214), (1, -0.125), (2, 0.004), (3, 0.089), (4, 0.03), (5, 0.066), (6, -0.031), (7, 0.034), (8, 0.009), (9, 0.147), (10, 0.082), (11, 0.063), (12, -0.037), (13, 0.089), (14, 0.061), (15, -0.084), (16, 0.102), (17, -0.111), (18, -0.108), (19, -0.125), (20, -0.0), (21, 0.054), (22, -0.018), (23, -0.085), (24, -0.086), (25, -0.017), (26, 0.071), (27, 0.077), (28, 0.073), (29, 0.057), (30, 0.056), (31, -0.054), (32, 0.051), (33, -0.035), (34, -0.006), (35, 0.03), (36, -0.081), (37, 0.016), (38, 0.038), (39, 0.02), (40, -0.023), (41, -0.001), (42, -0.023), (43, -0.032), (44, 0.009), (45, -0.037), (46, -0.091), (47, -0.045), (48, 0.002), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91012818 <a title="16-lsi-1" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>Author: Mei Tu ; Yu Zhou ; Chengqing Zong</p><p>Abstract: Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively. 1</p><p>2 0.74506986 <a title="16-lsi-2" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>3 0.67252642 <a title="16-lsi-3" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>4 0.63657331 <a title="16-lsi-4" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>Author: Or Biran ; Kathleen McKeown</p><p>Abstract: We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation.</p><p>5 0.58966368 <a title="16-lsi-5" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>Author: Haibo Li ; Jing Zheng ; Heng Ji ; Qi Li ; Wen Wang</p><p>Abstract: We propose a Name-aware Machine Translation (MT) approach which can tightly integrate name processing into MT model, by jointly annotating parallel corpora, extracting name-aware translation grammar and rules, adding name phrase table and name translation driven decoding. Additionally, we also propose a new MT metric to appropriately evaluate the translation quality of informative words, by assigning different weights to different words according to their importance values in a document. Experiments on Chinese-English translation demonstrated the effectiveness of our approach on enhancing the quality of overall translation, name translation and word alignment over a high-quality MT baseline1 .</p><p>6 0.58283603 <a title="16-lsi-6" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>7 0.56441343 <a title="16-lsi-7" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>8 0.56239051 <a title="16-lsi-8" href="./acl-2013-Handling_Ambiguities_of_Bilingual_Predicate-Argument_Structures_for_Statistical_Machine_Translation.html">180 acl-2013-Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation</a></p>
<p>9 0.55230713 <a title="16-lsi-9" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>10 0.551337 <a title="16-lsi-10" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>11 0.54328668 <a title="16-lsi-11" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>12 0.53354234 <a title="16-lsi-12" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>13 0.5171563 <a title="16-lsi-13" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>14 0.51459312 <a title="16-lsi-14" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>15 0.51116616 <a title="16-lsi-15" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>16 0.50720453 <a title="16-lsi-16" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>17 0.49733207 <a title="16-lsi-17" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>18 0.4952572 <a title="16-lsi-18" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>19 0.48793021 <a title="16-lsi-19" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>20 0.48761567 <a title="16-lsi-20" href="./acl-2013-Enlisting_the_Ghost%3A_Modeling_Empty_Categories_for_Machine_Translation.html">137 acl-2013-Enlisting the Ghost: Modeling Empty Categories for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.047), (6, 0.025), (11, 0.068), (16, 0.012), (24, 0.055), (26, 0.049), (28, 0.021), (35, 0.06), (42, 0.061), (48, 0.052), (70, 0.039), (84, 0.276), (88, 0.033), (90, 0.055), (95, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73768383 <a title="16-lda-1" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>Author: Mei Tu ; Yu Zhou ; Chengqing Zong</p><p>Abstract: Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly includes three steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively. 1</p><p>2 0.6583091 <a title="16-lda-2" href="./acl-2013-Recognizing_Partial_Textual_Entailment.html">297 acl-2013-Recognizing Partial Textual Entailment</a></p>
<p>Author: Omer Levy ; Torsten Zesch ; Ido Dagan ; Iryna Gurevych</p><p>Abstract: Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. Indeed, our results show that these methods are useful for rec- ognizing partial entailment. We also provide a preliminary assessment of how partial entailment may be used for recognizing (complete) textual entailment.</p><p>3 0.64153934 <a title="16-lda-3" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>4 0.61791599 <a title="16-lda-4" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>Author: Marine Carpuat ; Hal Daume III ; Katharine Henry ; Ann Irvine ; Jagadeesh Jagarlamudi ; Rachel Rudinger</p><p>Abstract: Words often gain new senses in new domains. Being able to automatically identify, from a corpus of monolingual text, which word tokens are being used in a previously unseen sense has applications to machine translation and other tasks sensitive to lexical semantics. We define a task, SENSESPOTTING, in which we build systems to spot tokens that have new senses in new domain text. Instead of difficult and expensive annotation, we build a goldstandard by leveraging cheaply available parallel corpora, targeting our approach to the problem of domain adaptation for machine translation. Our system is able to achieve F-measures of as much as 80%, when applied to word types it has never seen before. Our approach is based on a large set of novel features that capture varied aspects of how words change when used in new domains.</p><p>5 0.52529061 <a title="16-lda-5" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>Author: Muhua Zhu ; Yue Zhang ; Wenliang Chen ; Min Zhang ; Jingbo Zhu</p><p>Abstract: Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser.</p><p>6 0.52439535 <a title="16-lda-6" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>7 0.52349216 <a title="16-lda-7" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>8 0.51977187 <a title="16-lda-8" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>9 0.51816112 <a title="16-lda-9" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>10 0.51785457 <a title="16-lda-10" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>11 0.517281 <a title="16-lda-11" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>12 0.51714724 <a title="16-lda-12" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>13 0.51678765 <a title="16-lda-13" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>14 0.51652062 <a title="16-lda-14" href="./acl-2013-Improving_Chinese_Word_Segmentation_on_Micro-blog_Using_Rich_Punctuations.html">193 acl-2013-Improving Chinese Word Segmentation on Micro-blog Using Rich Punctuations</a></p>
<p>15 0.51578915 <a title="16-lda-15" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>16 0.51503944 <a title="16-lda-16" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>17 0.51455033 <a title="16-lda-17" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>18 0.51441383 <a title="16-lda-18" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>19 0.51421958 <a title="16-lda-19" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>20 0.51387739 <a title="16-lda-20" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
