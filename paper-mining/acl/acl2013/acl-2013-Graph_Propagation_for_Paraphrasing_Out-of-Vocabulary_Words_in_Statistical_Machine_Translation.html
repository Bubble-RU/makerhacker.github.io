<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-174" href="#">acl2013-174</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2013-174-pdf" href="http://aclweb.org/anthology//P/P13/P13-1109.pdf">pdf</a></p><p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>Reference: <a title="acl-2013-174-reference" href="../acl2013_reference/acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose a novel approach to finding translations for oov words. [sent-4, score-0.767]
</p><p>2 We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. [sent-5, score-1.042]
</p><p>3 Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics. [sent-7, score-0.386]
</p><p>4 Although this is helpful in translating a small fraction of oovs such as named entities for languages with same writing systems, it harms the translation in other types of oovs and distant language pairs. [sent-10, score-0.708]
</p><p>5 In general, copied-over oovs are a hindrance to fluent, high quality translation, and we can see evidence of this in automatic measures such as BLEU (Papineni et al. [sent-11, score-0.361]
</p><p>6 Even noisy translation of oovs can aid the language model to better ∗This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. [sent-14, score-0.394]
</p><p>7 Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al. [sent-26, score-1.093]
</p><p>8 For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. [sent-28, score-0.313]
</p><p>9 Another line of work exploits spelling and morphological variants of oov words. [sent-30, score-0.56]
</p><p>10 Habash (2008) presents techniques for online handling of oov words for Arabic to English such as spelling expansion and morphological expansion. [sent-31, score-0.56]
</p><p>11 (201 1) proposes a method to combine sublexical/constituent translations of an oov word or phrase to generate its translations. [sent-33, score-0.81]
</p><p>12 (2009) use a monolingual text on the source side to find paraphrases to oov words for which the translations are available. [sent-36, score-1.11]
</p><p>13 The translations for these paraphrases are 1105  Proce dingsS o f ita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-37, score-0.313]
</p><p>14 Ac s2s0o1ci3a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 1 05–1 15, then used as the translations of the oov word. [sent-39, score-0.767]
</p><p>15 (2009) showed that this  method improves over the baseline system where oovs are untranslated. [sent-42, score-0.314]
</p><p>16 (2009) in which a graph is constructed from source language monolingual text1 and the source-side of the available parallel data. [sent-44, score-0.461]
</p><p>17 Nodes that have related meanings are connected together and nodes for which we have translations in the phrase-table are annotated with target-side translations and their feature values. [sent-45, score-0.659]
</p><p>18 A graph propagation algorithm is then used to propagate translations from labeled nodes to unlabeled nodes (phrases appearing only in the monolingual text and oovs). [sent-46, score-1.306]
</p><p>19 2  Collocational Lexicon Induction  Rapp (1995) introduced the notion of a distributional profile in bilingual lexicon induction from monolingual data. [sent-50, score-0.465]
</p><p>20 This approach has also been used in machine translation to find in-vocabulary paraphrases for oov words on the source side and find a way to translate them. [sent-56, score-0.829]
</p><p>21 (2009) was the first to successfully integrate a collocational approach to finding trans1Here on by monolingual data we always mean monolingual data on the source language 2Named entity oovs may be handled properly by copying  or transliteration. [sent-59, score-0.699]
</p><p>22 lations for oov words into an end-to-end SMT system. [sent-60, score-0.56]
</p><p>23 The method relies on monolingual distributional profiles (DPs) which are numerical vectors representing the context around each word. [sent-62, score-0.354]
</p><p>24 For each oov a distributional profile is created by collecting all words appearing in a fixed distance from all occurrences of the oov word in the monolingual text. [sent-64, score-1.442]
</p><p>25 Then, the most similar phrases to each oov are found by measuring the similarity of their DPs to that of the oov word. [sent-67, score-1.239]
</p><p>26 For each of these paraphrases, a DP is constructed and compared to that of the oov word using a similarity measure (Section 2. [sent-73, score-0.648]
</p><p>27 The top-k paraphrases that have translations in the phrase-table are used to assign translations and scores to each oov word by marginalizing translations over paraphrases: p(t|o) =  Xp(t|s)p(s|o) Xs  where t is a phrase on the target side, o is the oov word or phrase, and s is a paraphrase of o. [sent-75, score-1.962]
</p><p>28 dW pe( reimplemented rtohmis tchoell pohcraatisoen-taalb approach for finding translations for oovs and used it as a baseline system. [sent-77, score-0.521]
</p><p>29 2 Association Measures Given a word u, its distributional profile DP(u) is constructed by counting surrounding words (in a fixed window size) in a monolingual corpus. [sent-84, score-0.354]
</p><p>30 KL(DP(u),DP(v)) =wXi∈VA(u,wi)logAA((vu,,wwii)) 3  Graph-based Lexicon Induction  We propose a novel approach to alleviate the oov problem. [sent-112, score-0.56]
</p><p>31 To do so, the distributional profiles of all source phrase types are created. [sent-116, score-0.261]
</p><p>32 Each phrase type represents a vertex in the graph and is connected to other vertices with a weight defined by a similarity measure between the two profiles (Section 2. [sent-117, score-0.514]
</p><p>33 When a relatively small parallel data is used, unlabeled nodes outnumber labeled ones and many of them lie on the paths between an oov node to labeled ones. [sent-120, score-1.135]
</p><p>34 (2009)’s approach ignores these bridging nodes and connects each oov node to the k-nearest labeled nodes. [sent-122, score-0.89]
</p><p>35 One may argue that these unlabeled nodes do not play a major role in the graph and the labels will eventually get to the oov nodes from the labeled nodes by directly connecting them. [sent-123, score-1.516]
</p><p>36 However based on the definition of the similarity measures using context, it is quite possible that an oov node and a labeled node which are connected to the same unlabeled node do not share any context words and hence are not directly connected. [sent-124, score-1.119]
</p><p>37 Once a graph is constructed based on similarities of phrases, graph propagation is used to propagate the labels from labeled nodes to unlabeled and oov nodes. [sent-127, score-1.485]
</p><p>38 , 2009) can be formulated as a bipartite graph with two types of nodes: labeled nodes (L) and oov nodes (O). [sent-131, score-1.245]
</p><p>39 Each oov node is connected to a number of labeled nodes, and vice versa and there is no edge between nodes of the same type. [sent-132, score-0.967]
</p><p>40 In such a graph, the similarity of each pair of nodes is computed using one of the similarity measures discussed above. [sent-133, score-0.367]
</p><p>41 The labels are translations and their probabilities (more specifically p(e|f)) from the phrase-table (emxtorarcete sdp cfriofmica ltlhye parallel corpus. [sent-134, score-0.313]
</p><p>42 Tphraranssela-ttiaobnles get propagated to oov nodes using a label propagation technique. [sent-135, score-0.953]
</p><p>43 However beside the difference in the oov label assignment, there is a major difference between our bipartite graph and the baseline  (Marton et al. [sent-136, score-0.838]
</p><p>44 We also take advantage of unlabeled nodes to help connect oov nodes to labeled ones. [sent-143, score-1.117]
</p><p>45 The discussed bipartite graph can easily be expanded to a tripartite graph by adding unlabeled nodes. [sent-144, score-0.619]
</p><p>46 Figure 1 illustrate a tripartite graph in which unlabeled nodes are connected to both labeled and oov nodes. [sent-145, score-1.263]
</p><p>47 We also created the full graph where all nodes can be freely connected to nodes of any type including the same type. [sent-147, score-0.618]
</p><p>48 However, constructing such graph and doing graph propagation on it is computationally very expensive for large n-grams. [sent-148, score-0.5]
</p><p>49 The vertex set V consists of labeled VL and unlabeled VU nodes, and the goal of the labeling propagation algorithm is to compute soft labels for unlabeled vertices from the labeled vertices. [sent-151, score-0.652]
</p><p>50 Intuitively, the edge weight W(u, v) encodes the degree of our belief about the similarity of the soft labeling for nodes u and v. [sent-152, score-0.364]
</p><p>51 1108  Figure 1: A tripartite graph between oov, labeled and unlabeled nodes. [sent-157, score-0.458]
</p><p>52 Translations  propagate either directly from labeled  nodes to oov nodes or indirectly via unlabeled nodes. [sent-158, score-1.155]
</p><p>53 The second term (2)  enforces the smoothness of the labeling according to the graph structure and edge weights. [sent-160, score-0.266]
</p><p>54 for high-degree unlabeled nodes (hubs in the graph) we may believe that the neighbors are not going to produce reliable label and hence the probability of undefined label ⊥ should be higher. [sent-163, score-0.474]
</p><p>55 In our case, we use phrases in the monolingual text as graph vertices. [sent-170, score-0.388]
</p><p>56 A common practice to control the number of edges is to connect each node to at most k other nodes (k-nearest neighbor). [sent-174, score-0.264]
</p><p>57 However, finding the top-k nearest nodes to each node requires considering its similarity to all the other nodes which requires O(n2) computations and since n is usually very large, doing such is practically intractable. [sent-175, score-0.522]
</p><p>58 Fortunately, since we use context words as cues for relating their meaning and since the similarity measures are defined based on these cues, the number of neighbors we need to consider for each node is reduced by several orders of magnitude. [sent-179, score-0.259]
</p><p>59 We incorporate an inverted-index-style data structure which indicates what nodes are neighbors based on each context word. [sent-180, score-0.278]
</p><p>60 In order to deal with the computational challenges of such a large graph, we take advantage of the Hadoop’s MapReduce functionality to do both graph construction and label propagation steps. [sent-183, score-0.374]
</p><p>61 Table 2 shows the number of oov types and tokens for Europarl and EMEA systems in both dev and test sets. [sent-192, score-0.629]
</p><p>62 DatasettypesDevtokenstypesTesttokens EEuMroEpAarl21832935242321792128930424116930  Table 2: number of oovs in dev and test sets for Europarl and  EMEA systems. [sent-193, score-0.383]
</p><p>63 1 Phrase-table Integration Once the translations and their probabilities for each oov are extracted, they are added to the 8http://www. [sent-201, score-0.767]
</p><p>64 2 Evaluation If we have a list of possible translations for oovs with their probabilities, we become able to evaluate different methods we discussed. [sent-209, score-0.521]
</p><p>65 However, it gives a good estimate of how each oov should be translated without the need for human judgments. [sent-213, score-0.56]
</p><p>66 Two intrinsic evaluation metrics that we use to evaluate the possible translations for oovs are Mean Reciprocal Rank (MRR) (Voorhees, 1999) and Recall. [sent-215, score-0.584]
</p><p>67 Such score is averaged over a set, oov set 1110  in our case, to get the mean-reciprocal-rank score. [sent-225, score-0.56]
</p><p>68 MRR =|O1|Xi|=O|1ran1ki  O = {oov}  In a few cases, there are multiple translations for an oov word (i. [sent-226, score-0.767]
</p><p>69 2 Recall MRR takes the probabilities of oov translations into account in sorting the list of candidate translations. [sent-231, score-0.767]
</p><p>70 Since Moses uses a certain number of trans-  lations per source phrase (called the translation table limit or ttl which we set to 20 in our experiments) , we use the recall measure to evaluate the top ttl translations in the list. [sent-234, score-0.472]
</p><p>71 For example, it assigns score of 1if the correct translation of the oov word is in the top-k list and 0 otherwise. [sent-236, score-0.64]
</p><p>72 The scores are averaged over all oovs to compute recall. [sent-237, score-0.314]
</p><p>73 3, different types of association measures and similarity measures have been explained to build and compare distributional profiles. [sent-241, score-0.277]
</p><p>74 0 325021  Table 3: Results of intrinsic evaluations (MRR and Recall) on Europarl, window size 4 and paraphrase length 2 unigram 4. [sent-258, score-0.262]
</p><p>75 1 Graph-based Results Table 4 shows the intrinsic results on the Europarl corpus when using unigram nodes in each of the graphs. [sent-277, score-0.292]
</p><p>76 Each node is connected to at most 20 other nodes (same as the max-paraphrase-limit  in the baseline). [sent-279, score-0.313]
</p><p>77 For the tripartite graph, each node is connected to 15 labeled nodes and 5 unlabeled ones. [sent-280, score-0.594]
</p><p>78 The tripartite graph gets a slight improvement over the bipartite one, however, the full graph failed to have the same increase. [sent-281, score-0.52]
</p><p>79 One reason is that allowing paths longer than 2 between oov and labeled nodes causes more noise to propagate into the graph. [sent-282, score-0.86]
</p><p>80 In other words, a paraphrase of a paraphrase of a paraphrase is not necessarily a useful paraphrase for an oov as the translation may no longer be a valid one. [sent-283, score-0.928]
</p><p>81 4  Extrinsic Results  The generated candidate translations for the oovs can be added to the phrase-table created using the parallel corpus to increase the coverage of the phrase-table. [sent-288, score-0.601]
</p><p>82 Table 6 reports the Bleu scores for different domains when the oov translations from the graph propagation is added to the phrase-table and compares them with the baseline system (i. [sent-293, score-1.09]
</p><p>83 Table 7 shows some translations found by our system for oov words. [sent-297, score-0.767]
</p><p>84 Most have focused on extracting a translation lexicon by mining monolingual resources of data to find clues, using probabilistic methods to map words, or by exploiting the cross-language evidence of closely related languages. [sent-304, score-0.315]
</p><p>85 Table 6: Bleu scores for different domains with or without using oov translations. [sent-319, score-0.56]
</p><p>86 But all of these researches used an available seed lexicon as the basic source of simi-  larity between source and target languages unlike our method which just needs a monolingual corpus of source language which is freely available for many languages and a small bilingual corpora. [sent-329, score-0.483]
</p><p>87 Some methods tried to alleviate the lack of seed lexicon by using orthographic similarity to extract a seed lexicon (Koehn and Knight, 2002; Fiser and Ljubesic, 2011). [sent-330, score-0.322]
</p><p>88 (2012) extended Alexandrescu’s model to use translation consensus among similar sentences in bilingual training data by developing a new structured label propagation method. [sent-340, score-0.326]
</p><p>89 Our graph propagation method connects monolingual source phrases with oovs to obtain translation and so is a very different use of graph propagation from these previous works. [sent-342, score-1.301]
</p><p>90 Recently label propagation has been used for lexicon induction (Tamura et al. [sent-343, score-0.318]
</p><p>91 They used a graph based on context similarity as well as cooccurrence graph in propagation process. [sent-345, score-0.594]
</p><p>92 Similar to our approach they used unlabeled nodes in label propagation process. [sent-346, score-0.492]
</p><p>93 6  Conclusion  We presented a novel approach for inducing oov translations from a monolingual corpus on the source side and a parallel data using graph propagation. [sent-348, score-1.261]
</p><p>94 Future work includes studying the effect of size of parallel corpus on the induced oov translations. [sent-350, score-0.675]
</p><p>95 But, on the other hand, there will be more labeled paraphrases that increases the chance of finding the correct translation for oovs in the test set. [sent-352, score-0.566]
</p><p>96 However, oovs can be considered as n-grams (phrases) instead of unigrams. [sent-354, score-0.314]
</p><p>97 In this scenario, we also can look for paraphrases and translations for phrases containing oovs and add them to the phrase-table as new translations along with the translations for unigram oovs. [sent-355, score-1.131]
</p><p>98 Improving translation lexicon induction from monolingual corpora via dependency contexts and part-of-speech equivalences. [sent-408, score-0.355]
</p><p>99 Using sublexical translations to handle the oov problem in machine translation. [sent-434, score-0.767]
</p><p>100 A linguistically grounded graph model for bilingual lexicon extraction. [sent-453, score-0.307]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('oov', 0.56), ('oovs', 0.314), ('translations', 0.207), ('nodes', 0.196), ('dp', 0.178), ('graph', 0.177), ('monolingual', 0.154), ('propagation', 0.146), ('marton', 0.139), ('mrr', 0.138), ('garera', 0.116), ('tripartite', 0.116), ('europarl', 0.108), ('paraphrases', 0.106), ('unlabeled', 0.099), ('wi', 0.097), ('profiles', 0.085), ('distributional', 0.083), ('lexicon', 0.081), ('rapp', 0.08), ('parallel', 0.08), ('translation', 0.08), ('talukdar', 0.073), ('paraphrase', 0.072), ('dev', 0.069), ('node', 0.068), ('labeled', 0.066), ('laws', 0.065), ('pivot', 0.063), ('intrinsic', 0.063), ('similarity', 0.062), ('stroudsburg', 0.062), ('emea', 0.061), ('window', 0.059), ('profile', 0.058), ('phrases', 0.057), ('gdp', 0.056), ('koehn', 0.053), ('label', 0.051), ('source', 0.05), ('neighbors', 0.05), ('bipartite', 0.05), ('pa', 0.049), ('connected', 0.049), ('seed', 0.049), ('bilingual', 0.049), ('measures', 0.047), ('alexandrescu', 0.046), ('soft', 0.044), ('phrase', 0.043), ('tamura', 0.043), ('haghighi', 0.041), ('induction', 0.04), ('association', 0.038), ('propagate', 0.038), ('kl', 0.038), ('av', 0.038), ('fiser', 0.037), ('rcl', 0.037), ('terra', 0.037), ('yv', 0.037), ('vertices', 0.037), ('vertex', 0.035), ('size', 0.035), ('rv', 0.034), ('bleu', 0.034), ('moses', 0.034), ('labeling', 0.034), ('wxi', 0.033), ('junto', 0.033), ('schafer', 0.033), ('ttl', 0.033), ('side', 0.033), ('reciprocal', 0.033), ('unigram', 0.033), ('context', 0.032), ('knight', 0.032), ('dps', 0.031), ('monash', 0.031), ('va', 0.029), ('yarowsky', 0.029), ('fung', 0.029), ('goyal', 0.029), ('crammer', 0.028), ('edge', 0.028), ('undefined', 0.027), ('coin', 0.027), ('sliding', 0.027), ('collocational', 0.027), ('chapelle', 0.027), ('smoothness', 0.027), ('appearing', 0.027), ('sch', 0.027), ('measure', 0.026), ('labels', 0.026), ('dagan', 0.026), ('bridge', 0.026), ('binomial', 0.026), ('pratim', 0.026), ('volume', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999911 <a title="174-tfidf-1" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>2 0.24046005 <a title="174-tfidf-2" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>3 0.18845485 <a title="174-tfidf-3" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>4 0.18094189 <a title="174-tfidf-4" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>5 0.15994458 <a title="174-tfidf-5" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>Author: Hany Hassan ; Arul Menezes</p><p>Abstract: We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.</p><p>6 0.15886092 <a title="174-tfidf-6" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>7 0.14750211 <a title="174-tfidf-7" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>8 0.1430428 <a title="174-tfidf-8" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>9 0.13381119 <a title="174-tfidf-9" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>10 0.12191878 <a title="174-tfidf-10" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>11 0.11949573 <a title="174-tfidf-11" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>12 0.11902312 <a title="174-tfidf-12" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>13 0.11823579 <a title="174-tfidf-13" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>14 0.11479441 <a title="174-tfidf-14" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>15 0.11166204 <a title="174-tfidf-15" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>16 0.10482829 <a title="174-tfidf-16" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>17 0.10104777 <a title="174-tfidf-17" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>18 0.097723544 <a title="174-tfidf-18" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>19 0.096635319 <a title="174-tfidf-19" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>20 0.094416246 <a title="174-tfidf-20" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.274), (1, -0.077), (2, 0.159), (3, 0.025), (4, 0.07), (5, -0.027), (6, -0.074), (7, 0.032), (8, 0.012), (9, -0.032), (10, 0.038), (11, -0.007), (12, 0.045), (13, -0.021), (14, 0.048), (15, 0.025), (16, -0.04), (17, -0.049), (18, -0.116), (19, 0.125), (20, 0.033), (21, 0.004), (22, 0.109), (23, 0.1), (24, -0.053), (25, 0.001), (26, 0.009), (27, 0.181), (28, 0.028), (29, -0.005), (30, -0.118), (31, -0.062), (32, -0.079), (33, 0.146), (34, 0.068), (35, -0.018), (36, -0.08), (37, -0.092), (38, -0.021), (39, -0.009), (40, 0.124), (41, -0.104), (42, 0.086), (43, 0.015), (44, 0.011), (45, -0.0), (46, 0.006), (47, 0.012), (48, 0.121), (49, -0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95153332 <a title="174-lsi-1" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>2 0.7052846 <a title="174-lsi-2" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>Author: Lei Cui ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: The quality of bilingual data is a key factor in Statistical Machine Translation (SMT). Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance. Previous work often used supervised learning methods to filter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain. To reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data. The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks.</p><p>3 0.68782514 <a title="174-lsi-3" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>4 0.67678261 <a title="174-lsi-4" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>Author: Samira Tofighi Zahabi ; Somayeh Bakhshaei ; Shahram Khadivi</p><p>Abstract: Mapping phrases between languages as translation of each other by using an intermediate language (pivot language) may generate translation pairs that are wrong. Since a word or a phrase has different meanings in different contexts, we should map source and target phrases in an intelligent way. We propose a pruning method based on the context vectors to remove those phrase pairs that connect to each other by a polysemous pivot phrase or by weak translations. We use context vectors to implicitly disambiguate the phrase senses and to recognize irrelevant phrase translation pairs. Using the proposed method a relative improvement of 2.8 percent in terms of BLEU score is achieved. 1</p><p>5 0.65913171 <a title="174-lsi-5" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>Author: Dhouha Bouamor ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches.</p><p>6 0.65810013 <a title="174-lsi-6" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>7 0.60625839 <a title="174-lsi-7" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>8 0.59312856 <a title="174-lsi-8" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>9 0.58872521 <a title="174-lsi-9" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>10 0.58604574 <a title="174-lsi-10" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>11 0.56395358 <a title="174-lsi-11" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>12 0.55936354 <a title="174-lsi-12" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>13 0.54931945 <a title="174-lsi-13" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>14 0.54736352 <a title="174-lsi-14" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>15 0.54664218 <a title="174-lsi-15" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>16 0.53677619 <a title="174-lsi-16" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>17 0.52821088 <a title="174-lsi-17" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>18 0.52817315 <a title="174-lsi-18" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>19 0.52616966 <a title="174-lsi-19" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>20 0.52055943 <a title="174-lsi-20" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.063), (4, 0.03), (6, 0.024), (11, 0.09), (15, 0.014), (24, 0.052), (26, 0.068), (28, 0.01), (31, 0.011), (35, 0.088), (42, 0.066), (46, 0.105), (48, 0.041), (70, 0.053), (88, 0.035), (90, 0.077), (95, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93625051 <a title="174-lda-1" href="./acl-2013-Translating_Italian_connectives_into_Italian_Sign_Language.html">360 acl-2013-Translating Italian connectives into Italian Sign Language</a></p>
<p>Author: Camillo Lugaresi ; Barbara Di Eugenio</p><p>Abstract: We present a corpus analysis of how Italian connectives are translated into LIS, the Italian Sign Language. Since corpus resources are scarce, we propose an alignment method between the syntactic trees of the Italian sentence and of its LIS translation. This method, and clustering applied to its outputs, highlight the different ways a connective can be rendered in LIS: with a corresponding sign, by affecting the location or shape of other signs, or being omitted altogether. We translate these findings into a computational model that will be integrated into the pipeline of an existing Italian-LIS rendering system. Initial experiments to learn the four possible translations with Decision Trees give promising results.</p><p>2 0.91909623 <a title="174-lda-2" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>Author: Hoifung Poon</p><p>Abstract: We present the first unsupervised approach for semantic parsing that rivals the accuracy of supervised approaches in translating natural-language questions to database queries. Our GUSP system produces a semantic parse by annotating the dependency-tree nodes and edges with latent states, and learns a probabilistic grammar using EM. To compensate for the lack of example annotations or question-answer pairs, GUSP adopts a novel grounded-learning approach to leverage database for indirect supervision. On the challenging ATIS dataset, GUSP attained an accuracy of 84%, effectively tying with the best published results by supervised approaches.</p><p>same-paper 3 0.9041785 <a title="174-lda-3" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>4 0.86404848 <a title="174-lda-4" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>Author: Wenduan Xu ; Yue Zhang ; Philip Williams ; Philipp Koehn</p><p>Abstract: We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.</p><p>5 0.86110973 <a title="174-lda-5" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>Author: Xiaodong Zeng ; Derek F. Wong ; Lidia S. Chao ; Isabel Trancoso</p><p>Abstract: This paper introduces a graph-based semisupervised joint model of Chinese word segmentation and part-of-speech tagging. The proposed approach is based on a graph-based label propagation technique. One constructs a nearest-neighbor similarity graph over all trigrams of labeled and unlabeled data for propagating syntactic information, i.e., label distributions. The derived label distributions are regarded as virtual evidences to regularize the learning of linear conditional random fields (CRFs) on unlabeled data. An inductive character-based joint model is obtained eventually. Empirical results on Chinese tree bank (CTB-7) and Microsoft Research corpora (MSR) reveal that the proposed model can yield better results than the supervised baselines and other competitive semi-supervised CRFs in this task.</p><p>6 0.86032999 <a title="174-lda-6" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>7 0.85550189 <a title="174-lda-7" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>8 0.85495508 <a title="174-lda-8" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>9 0.8540765 <a title="174-lda-9" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>10 0.85397023 <a title="174-lda-10" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>11 0.85210568 <a title="174-lda-11" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>12 0.84965944 <a title="174-lda-12" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>13 0.84947628 <a title="174-lda-13" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>14 0.84931928 <a title="174-lda-14" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>15 0.84891105 <a title="174-lda-15" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>16 0.84872222 <a title="174-lda-16" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>17 0.84671128 <a title="174-lda-17" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>18 0.84519446 <a title="174-lda-18" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>19 0.84490114 <a title="174-lda-19" href="./acl-2013-Paraphrasing_Adaptation_for_Web_Search_Ranking.html">273 acl-2013-Paraphrasing Adaptation for Web Search Ranking</a></p>
<p>20 0.84463358 <a title="174-lda-20" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
