<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-84" href="#">acl2013-84</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</h1>
<br/><p>Source: <a title="acl-2013-84-pdf" href="http://aclweb.org/anthology//P/P13/P13-2037.pdf">pdf</a></p><p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>Reference: <a title="acl-2013-84-reference" href="../acl2013_reference/acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling Heike Adel  Ngoc Thang Vu  Tanja Schultz  Institute for Anthropomatics, Karlsruhe Institute of Technology  heike . [sent-1, score-0.049]
</p><p>2 edu @  (KIT)  thang  Abstract In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. [sent-4, score-0.945]
</p><p>3 We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. [sent-5, score-0.138]
</p><p>4 Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. [sent-6, score-0.32]
</p><p>5 Finally, we show that recurrent neural networks and factored language models can  . [sent-7, score-0.816]
</p><p>6 be combined using linear interpolation to achieve the best performance. [sent-8, score-0.083]
</p><p>7 8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32. [sent-10, score-0.149]
</p><p>8 7% on the evaluation set compared to the traditional n-gram language model. [sent-11, score-0.056]
</p><p>9 Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models 1 Introduction Code-Switching (CS) speech is defined as speech that contains more than one language (’code’). [sent-12, score-1.029]
</p><p>10 For the automated processing of spoken communication in these scenarios, a speech recognition system must be able to handle code switches. [sent-14, score-0.184]
</p><p>11 However, the components of speech recognition systems are usually trained on monolingual data. [sent-15, score-0.146]
</p><p>12 Recently, it has been shown that recurrent neural network language models (RNNLMs) can improve perplexity and error rates in speech recognition systems in comparison to traditional n-gram approaches (Mikolov et al. [sent-22, score-0.892]
</p><p>13 Furthermore, the integration of additional features as input is rather straightforward due to their structure. [sent-26, score-0.059]
</p><p>14 On the other hand, factored language models (FLMs) have been used successfully for languages with rich morphology due to their ability to process syntactical features, such as word stems or part-of-speech tags (Bilmes and Kirchhoff, 2003; El-Desoky et al. [sent-27, score-0.431]
</p><p>15 2 Related Work For this work, three different topics are investigated and combined: linguistic investigation of Code-Switching, recurrent neural network language modeling and factored language models. [sent-32, score-0.889]
</p><p>16 In (Muysken, 2000; Poplack, 1978; Bokamba, 1989), it is observed that code switches occur at positions in an utterance where they do not violate the syntactical rules of the involved languages. [sent-33, score-0.172]
</p><p>17 It can be observed that part-of-speech tags may predict Code-Switching points more reliable than words themselves. [sent-37, score-0.161]
</p><p>18 , 2008a) predict Code-  Switching points using several linguistic features, such as word form, language ID, part-of-speech tags or the position of the word relative to the phrase (BIO). [sent-41, score-0.161]
</p><p>19 It is discovered that clustering all foreign words into their part-of-speech classes leads to the best performance. [sent-46, score-0.104]
</p><p>20 In the last years, neural networks have been used for a variety of tasks, including language modeling (Mikolov et al. [sent-47, score-0.318]
</p><p>21 Recurrent neural networks are able to handle long-term contexts since the input vector does not only contain the current word but also the previous hidden layer. [sent-49, score-0.291]
</p><p>22 It is shown that these networks outperform traditional language models, such as n-grams which only contain very limited histories. [sent-50, score-0.154]
</p><p>23 , 2011), the network is extended by factorizing the output layer into classes to accelerate the training and testing processes. [sent-52, score-0.235]
</p><p>24 The input layer can be augmented to model features, such as part-  of-speech tags (Shi et al. [sent-53, score-0.189]
</p><p>25 , 2013), recurrent neural networks are applied to Code-Switching speech. [sent-57, score-0.579]
</p><p>26 It is shown that the integration of POS tags into the neural network, which predicts the next language as well as the next word, leads to significant perplexity reductions. [sent-58, score-0.459]
</p><p>27 A factored language model refers to a word as a vector of features, such as the word itself, morphological classes, POS tags or word stems. [sent-59, score-0.337]
</p><p>28 Hence, it provides another possibility to integrate syntactical features into the language modeling process. [sent-60, score-0.176]
</p><p>29 In (Bilmes and Kirchhoff, 2003), it is shown that factored language models are able to outperform standard n-gram techniques in terms of perplexity. [sent-61, score-0.237]
</p><p>30 In the same paper, generalized parallel backoff is introduced. [sent-62, score-0.248]
</p><p>31 This technique can be used to generalize traditional backoff methods and to improve the performance of factored language models. [sent-63, score-0.541]
</p><p>32 Due to the integration of various features, it is possible to handle rich morphology in languages like Arabic or Turkish (Duh and Kirchhoff, 2004; El-Desoky et al. [sent-64, score-0.06]
</p><p>33 1 Motivation Since there is a lack of Code-Switching data, language modeling is a challenging task. [sent-67, score-0.055]
</p><p>34 Hence, more general features than words should be integrated into the language models. [sent-69, score-0.052]
</p><p>35 Therefore, we apply recurrent neural networks and factored language models. [sent-70, score-0.816]
</p><p>36 As features, we use part-of-speech tags and language identifiers. [sent-71, score-0.1]
</p><p>37 2  Using Recurrent Neural Networks As Language Model This section describes the structure of the recurrent neural network (RNNLM) that we use as Code-Switching language model. [sent-73, score-0.597]
</p><p>38 , 2011)) Vector w(t), which represents the current word using 1-of-N coding, forms the input of the recurrent neural network. [sent-77, score-0.481]
</p><p>39 Vector s(t) contains the state of the network and is called ’hidden layer’ . [sent-79, score-0.116]
</p><p>40 The network is trained using backpropagation through time (BPTT), an extension of the back-propagation algorithm for recurrent neu-  ral networks. [sent-80, score-0.469]
</p><p>41 With BPTT, the error is propagated through recurrent connections back in time for a specific number of time steps t. [sent-81, score-0.373]
</p><p>42 Hence, the network is able to remember information for several time steps. [sent-82, score-0.116]
</p><p>43 Moreover, the output layer is factorized 207  into classes which provide language information. [sent-85, score-0.119]
</p><p>44 Hence, the probability P(wi |history) is computed as shown in equation 1. [sent-88, score-0.046]
</p><p>45 Hence according to equation 1, the probability of the next language is computed first and then the probability of each word given the language. [sent-90, score-0.046]
</p><p>46 Since the POS tags are integrated into the input layer, they are also propagated into the hidden layer and backpropagated into its history s(t). [sent-94, score-0.296]
</p><p>47 Hence, not only the previous feature is stored in the history but also features from several time steps in the past. [sent-95, score-0.079]
</p><p>48 3 Using Factored Language Models Factored language models (FLM) are another approach to integrate syntactical features, such as part-of-speech tags or language identifiers into the language modeling process. [sent-97, score-0.249]
</p><p>49 If a particular sequence of features has not been detected in the training data, backoff techniques will be applied. [sent-99, score-0.275]
</p><p>50 For our task of Code-Switching, we develop two different models: One model with only part-of-speech tags as features and one model including also language information tags. [sent-100, score-0.127]
</p><p>51 Un-  fortunately, the number of possible parameters is rather high: Different feature combinations from different time steps can be used to predict the next word (conditioning factors), different backoff paths and different smoothing methods may be applied. [sent-101, score-0.312]
</p><p>52 To detect useful parameters, the genetic algorithm described in (Duh and Kirchhoff, 2004) is used. [sent-102, score-0.051]
</p><p>53 Then, a loop follows that evaluates the fitness of the genes and mutates them until their average fitness is not improved any more. [sent-105, score-0.259]
</p><p>54 As fitness value, the inverse perplexity of the FLM corresponding to the gene on the development set is  Wt-1  Pt-1  Pt-2  PWt-21 Pt-2uniWgrta-1mWt-1 P t-1 Figure 2: Backoff graph of the FLM used. [sent-106, score-0.223]
</p><p>55 Hence, parameter solutions with lower perplexities are preferred in the selection of the genes for the following iteration. [sent-107, score-0.183]
</p><p>56 In (Duh and Kirchhoff, 2004), it is shown that this genetic method outperforms both knowledge-based and randomized choices. [sent-108, score-0.051]
</p><p>57 For the case of part-of-speech tags as features, the method results in three conditioning factors: the previous word Wt−1 and the two previous POS tags Pt−1 and Pt−2. [sent-109, score-0.232]
</p><p>58 The backoff graph obtained by the algorithm is illustrated in figure 2. [sent-110, score-0.273]
</p><p>59 According to the result of the genetic algorithm, different smoothing methods are used at  different backofflevels: For the backofffrom three factors to two factors, Kneser-Ney discounting is applied. [sent-111, score-0.167]
</p><p>60 If the probabilities for the factor combination Wt−1Pt−2 could not be estimated reliably, absolute discounting is used. [sent-112, score-0.058]
</p><p>61 1 Data Corpus SEAME (South East Asia Mandarin-English) is a conversational Mandarin-English Code-Switching speech corpus recorded from Singaporean and Malaysian speakers (D. [sent-116, score-0.156]
</p><p>62 6 per utterance and the duration of monolingual segments is quite short: The average duration of English and Mandarin segments is only 0. [sent-125, score-0.266]
</p><p>63 We divided the corpus into three disjoint sets (training, development and test set) and assigned the data based on several criteria (gender, speaking style, ratio of Singaporean and Malaysian speakers, ratio of the four categories, and the duration in each set). [sent-129, score-0.075]
</p><p>64 2  POS Tagger for Code-Switching Speech  To be able to assign part-of-speech tags to our bilingual text corpus, we apply the POS tagger described in (Schultz et al. [sent-135, score-0.157]
</p><p>65 It consists of two different monolingual (Stanford log-linear) taggers (Toutanova et al. [sent-138, score-0.057]
</p><p>66 , 2008b) passes the whole Code-Switching text to both monolingual taggers and combines their results using different heuristics, in this work, the text is splitted into different languages first. [sent-142, score-0.089]
</p><p>67 If three or more words of the embedded language are detected, they are passed to the English tagger. [sent-145, score-0.056]
</p><p>68 The rest of the text is passed to the Mandarin tagger, even if it contains foreign words. [sent-146, score-0.066]
</p><p>69 The idea is to provide the tagger as much context as possible. [sent-147, score-0.057]
</p><p>70 Since most English words in the Mandarin segments are falsely tagged as nouns by the Mandarin tagger, a postprocessing step is applied. [sent-148, score-0.049]
</p><p>71 It passes all foreign words of the Mandarin segments to the English tagger in order to replace the wrong tags with the correct ones. [sent-149, score-0.274]
</p><p>72 „Matrix language“  = Mandarin  „Embedded language“ = English  CS-text  L(a>n2guewaomgrbde sids)ldaned s  Remteaxinting  taEgPngOgelirSsfohr taMgaPgnOedrSaforin Outp Posr tpeE rmgontciagme nilxnsie tisn hitnsg :  Output  Figure 3: Tagging of Code-Switching speech 4. [sent-150, score-0.09]
</p><p>73 3  Evaluation  For evaluation, we compute the perplexity of each language model on the SEAME development and evaluation set und perform an analysis of the different back-off levels to understand in detail the behavior of each language model. [sent-151, score-0.149]
</p><p>74 A traditional 3gram LM trained with the SEAME transcriptions serves as baseline. [sent-152, score-0.056]
</p><p>75 s025e917t It can be noticed that both the RNNLM and the FLM model outperform the traditional 3-gram model. [sent-159, score-0.056]
</p><p>76 For the FLM, it leads to no improvement to add the language identifier as feature. [sent-161, score-0.038]
</p><p>77 In contrast, clustering the words into their languages on the output layer of the RNNLM leads to lower perplexities. [sent-162, score-0.127]
</p><p>78 Then, a level-dependent perplexity is computed for each model as shown in equation 2. [sent-168, score-0.17]
</p><p>79 PPLk =  10−N1kPwklog10P(wk|hk)  (2)  In the equation, k denotes the backoff-level, Nk the number of words on this level, wk the current word and hk its history. [sent-169, score-0.064]
</p><p>80 Table 3 shows how often each backoff-level is used and presents the leveldependent perplexities of each model on the development set. [sent-170, score-0.097]
</p><p>81 92 948  Table 3: Backoff-level-dependent PPLs In case of backoff to the 2-gram, the FLM provides the best perplexity, while for the 3-gram and backoff to the 1-gram, the RNNLM performs best. [sent-174, score-0.496]
</p><p>82 This may be correlated with the better over-all perplexity of the RNNLM in comparison to the FLM. [sent-175, score-0.124]
</p><p>83 Nevertheless, the backoff to the 2-gram is used about twice as often as the backoff to the 1-gram or the 3-gram. [sent-176, score-0.496]
</p><p>84 Thus, a combination of them may reduce the perplexities of table 2. [sent-179, score-0.072]
</p><p>85 Hence, we apply linear interpolation to the probabilities of each two models as shown in equation 3. [sent-180, score-0.101]
</p><p>86 P(w|h) = λ·PM1 (w|h)+(1−λ) ·PM2(w|h)  (3)  The equation shows the computation of the pobability for word w given its history h. [sent-181, score-0.098]
</p><p>87 The interpolation of RNNLM and FLM leads to the best results. [sent-185, score-0.093]
</p><p>88 While the RNNLM performs better for the 3-gram and for the backoff to the 1gram, the FLM performs the best in case of backoff to the 2-gram which is used more often than the other levels (table 3). [sent-191, score-0.496]
</p><p>89 5  Conclusions  In this paper, we presented two different methods  for language modeling of Code-Switching speech: Recurrent neural networks and factored language models. [sent-192, score-0.555]
</p><p>90 We integrated part-of-speech tags and language information to improve the performance of the language models. [sent-193, score-0.125]
</p><p>91 In addition, we analyzed their behavior on the different backoff levels. [sent-194, score-0.248]
</p><p>92 While the FLM performed better in case of backoff to the 2-gram, the RNNLM led to a better over-all performance. [sent-195, score-0.248]
</p><p>93 8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32. [sent-198, score-0.149]
</p><p>94 7% on the evaluation set compared to the traditional n-gram LM. [sent-199, score-0.056]
</p><p>95 Auer 1999 From codeswitching via language mixing to fused lects toward a dynamic typology of bilin-  gual speech In: International Journal of Bilingualism, vol. [sent-211, score-0.218]
</p><p>96 Cao 2006 Automatic speech recognition of Cantonese-English 210  code-mixing utterances speech 2006. [sent-232, score-0.205]
</p><p>97 Muysken 2000 Bilingual speech: A typology of code-mixing In: Cambridge University Press, vol. [sent-276, score-0.054]
</p><p>98 Poplack 1980 Sometimes ill start a sentence in spanish y termino en espanol: toward a typology of  code-switching In: Linguistics, vol. [sent-287, score-0.054]
</p><p>99 Liu 2008 Learning to predict codeswitching points In: Proceedings of the Conference on Empirical Methods in Natural Language Processing. [sent-309, score-0.135]
</p><p>100 Singer 2003 Feature-rich part-of-speech tagging with a cyclic dependency network In: Proceedings of NAACL 2003. [sent-321, score-0.116]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('flm', 0.361), ('recurrent', 0.316), ('rnnlm', 0.272), ('backoff', 0.248), ('factored', 0.237), ('mandarin', 0.234), ('adel', 0.222), ('seame', 0.167), ('vu', 0.166), ('neural', 0.165), ('mikolov', 0.142), ('perplexity', 0.124), ('network', 0.116), ('genes', 0.111), ('poplack', 0.111), ('schultz', 0.107), ('tags', 0.1), ('networks', 0.098), ('solorio', 0.098), ('syntactical', 0.094), ('kirchhoff', 0.09), ('speech', 0.09), ('layer', 0.089), ('flms', 0.083), ('lyu', 0.083), ('rnnlms', 0.083), ('auer', 0.075), ('codeswitching', 0.074), ('fitness', 0.074), ('malaysian', 0.074), ('singaporean', 0.074), ('duh', 0.072), ('perplexities', 0.072), ('interspeech', 0.07), ('kit', 0.062), ('discounting', 0.058), ('tagger', 0.057), ('traditional', 0.056), ('bokamba', 0.056), ('bptt', 0.056), ('jernocky', 0.056), ('muysken', 0.056), ('oparin', 0.056), ('ppls', 0.056), ('schlippe', 0.056), ('thang', 0.056), ('interpolation', 0.055), ('modeling', 0.055), ('typology', 0.054), ('hence', 0.053), ('history', 0.052), ('genetic', 0.051), ('icassp', 0.05), ('duration', 0.05), ('heike', 0.049), ('segments', 0.049), ('bilmes', 0.049), ('equation', 0.046), ('karlsruhe', 0.045), ('burget', 0.045), ('toutanova', 0.043), ('switching', 0.043), ('code', 0.041), ('rosenfeld', 0.039), ('conversational', 0.038), ('leads', 0.038), ('particles', 0.037), ('ral', 0.037), ('utterance', 0.037), ('pos', 0.037), ('predict', 0.036), ('lm', 0.036), ('foreign', 0.036), ('hk', 0.034), ('chan', 0.033), ('passes', 0.032), ('integration', 0.032), ('conditioning', 0.032), ('monolingual', 0.031), ('wk', 0.03), ('classes', 0.03), ('propagated', 0.03), ('passed', 0.03), ('factors', 0.03), ('seconds', 0.029), ('handle', 0.028), ('speakers', 0.028), ('smoothing', 0.028), ('combined', 0.028), ('connections', 0.027), ('features', 0.027), ('shi', 0.027), ('embedded', 0.026), ('furthermore', 0.026), ('taggers', 0.026), ('recognition', 0.025), ('illustrated', 0.025), ('development', 0.025), ('integrated', 0.025), ('points', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="84-tfidf-1" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>2 0.15740216 <a title="84-tfidf-2" href="./acl-2013-Adaptation_Data_Selection_using_Neural_Language_Models%3A_Experiments_in_Machine_Translation.html">35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</a></p>
<p>Author: Kevin Duh ; Graham Neubig ; Katsuhito Sudoh ; Hajime Tsukada</p><p>Abstract: Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling un- known word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams.</p><p>3 0.14665295 <a title="84-tfidf-3" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>Author: lemao liu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Most statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks.</p><p>4 0.1380423 <a title="84-tfidf-4" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>Author: Brian Roark ; Cyril Allauzen ; Michael Riley</p><p>Abstract: We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library.1</p><p>5 0.1315164 <a title="84-tfidf-5" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>6 0.11109685 <a title="84-tfidf-6" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>7 0.10183837 <a title="84-tfidf-7" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>8 0.10134749 <a title="84-tfidf-8" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>9 0.066895127 <a title="84-tfidf-9" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>10 0.062822066 <a title="84-tfidf-10" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>11 0.062076464 <a title="84-tfidf-11" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>12 0.061261076 <a title="84-tfidf-12" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>13 0.059107371 <a title="84-tfidf-13" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>14 0.056586813 <a title="84-tfidf-14" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>15 0.05587538 <a title="84-tfidf-15" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>16 0.055003483 <a title="84-tfidf-16" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>17 0.053117283 <a title="84-tfidf-17" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>18 0.048118681 <a title="84-tfidf-18" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>19 0.04744982 <a title="84-tfidf-19" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>20 0.047185075 <a title="84-tfidf-20" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.135), (1, -0.025), (2, 0.012), (3, 0.013), (4, 0.021), (5, -0.035), (6, 0.013), (7, -0.022), (8, -0.024), (9, 0.035), (10, -0.02), (11, -0.064), (12, 0.026), (13, -0.077), (14, -0.048), (15, 0.02), (16, -0.109), (17, -0.009), (18, 0.012), (19, -0.18), (20, 0.007), (21, -0.066), (22, 0.004), (23, -0.034), (24, 0.07), (25, -0.056), (26, 0.131), (27, -0.138), (28, 0.101), (29, -0.071), (30, -0.13), (31, -0.061), (32, -0.079), (33, -0.051), (34, 0.014), (35, -0.084), (36, 0.101), (37, -0.028), (38, -0.026), (39, -0.057), (40, -0.087), (41, 0.055), (42, -0.047), (43, -0.014), (44, -0.047), (45, -0.027), (46, -0.067), (47, 0.003), (48, 0.036), (49, -0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92453724 <a title="84-lsi-1" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>2 0.81896031 <a title="84-lsi-2" href="./acl-2013-Adaptation_Data_Selection_using_Neural_Language_Models%3A_Experiments_in_Machine_Translation.html">35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</a></p>
<p>Author: Kevin Duh ; Graham Neubig ; Katsuhito Sudoh ; Hajime Tsukada</p><p>Abstract: Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling un- known word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams.</p><p>3 0.78024316 <a title="84-lsi-3" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>Author: Tiberiu Boros ; Radu Ion ; Dan Tufis</p><p>Abstract: Radu Ion Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy radu@ racai . ro Dan Tufi? Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy tufi s @ racai . ro Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, ?????? ?????? ??????? ??????, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. 1</p><p>4 0.64470148 <a title="84-lsi-4" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>Author: Kenneth Heafield ; Ivan Pouzyrevsky ; Jonathan H. Clark ; Philipp Koehn</p><p>Abstract: We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM.</p><p>5 0.6332162 <a title="84-lsi-5" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>6 0.6008845 <a title="84-lsi-6" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>7 0.5929147 <a title="84-lsi-7" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>8 0.57580167 <a title="84-lsi-8" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>9 0.55391461 <a title="84-lsi-9" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>10 0.51622897 <a title="84-lsi-10" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>11 0.491945 <a title="84-lsi-11" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>12 0.45517918 <a title="84-lsi-12" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>13 0.45076579 <a title="84-lsi-13" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>14 0.44311261 <a title="84-lsi-14" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>15 0.4352046 <a title="84-lsi-15" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>16 0.39038125 <a title="84-lsi-16" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>17 0.38582832 <a title="84-lsi-17" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>18 0.38141 <a title="84-lsi-18" href="./acl-2013-Accurate_Word_Segmentation_using_Transliteration_and_Language_Model_Projection.html">34 acl-2013-Accurate Word Segmentation using Transliteration and Language Model Projection</a></p>
<p>19 0.37867492 <a title="84-lsi-19" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>20 0.37269169 <a title="84-lsi-20" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.048), (6, 0.021), (11, 0.04), (24, 0.051), (26, 0.058), (29, 0.427), (35, 0.051), (42, 0.044), (48, 0.042), (70, 0.035), (88, 0.015), (90, 0.033), (95, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81756043 <a title="84-lda-1" href="./acl-2013-TopicSpam%3A_a_Topic-Model_based_approach_for_spam_detection.html">350 acl-2013-TopicSpam: a Topic-Model based approach for spam detection</a></p>
<p>Author: Jiwei Li ; Claire Cardie ; Sujian Li</p><p>Abstract: Product reviews are now widely used by individuals and organizations for decision making (Litvin et al., 2008; Jansen, 2010). And because of the profits at stake, people have been known to try to game the system by writing fake reviews to promote target products. As a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle dif- ferences between deceptive reviews and truthful ones and achieves about 95% accuracy on review spam datasets, outperforming existing baselines by a large margin.</p><p>same-paper 2 0.71197897 <a title="84-lda-2" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>3 0.59040147 <a title="84-lda-3" href="./acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees.html">285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</a></p>
<p>Author: Alan Akbik ; Oresti Konomi ; Michail Melnikov</p><p>Abstract: The use ofdeep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction. Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. We introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the PROP- MINER tool.</p><p>4 0.56435513 <a title="84-lda-4" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>Author: Tze Yuang Chong ; Rafael E. Banchs ; Eng Siong Chng ; Haizhou Li</p><p>Abstract: In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1</p><p>5 0.48149022 <a title="84-lda-5" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>6 0.38875437 <a title="84-lda-6" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>7 0.31618968 <a title="84-lda-7" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>8 0.31553447 <a title="84-lda-8" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>9 0.3146351 <a title="84-lda-9" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>10 0.3142637 <a title="84-lda-10" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>11 0.31371081 <a title="84-lda-11" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>12 0.31317055 <a title="84-lda-12" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>13 0.31235373 <a title="84-lda-13" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>14 0.31234768 <a title="84-lda-14" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>15 0.31151128 <a title="84-lda-15" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>16 0.31130272 <a title="84-lda-16" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>17 0.31126875 <a title="84-lda-17" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>18 0.31070578 <a title="84-lda-18" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>19 0.31066355 <a title="84-lda-19" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>20 0.31001106 <a title="84-lda-20" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
