<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-247" href="#">acl2013-247</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</h1>
<br/><p>Source: <a title="acl-2013-247-pdf" href="http://aclweb.org/anthology//P/P13/P13-2042.pdf">pdf</a></p><p>Author: Tze Yuang Chong ; Rafael E. Banchs ; Eng Siong Chng ; Haizhou Li</p><p>Abstract: In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1</p><p>Reference: <a title="acl-2013-247-reference" href="../acl2013_reference/acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Modeling of term-distance and term-occurrence information for improving n-gram language model performance  2SchoT1zleofmYCauosemnkgpLuCat3behInro sEntaigou1grt,i2ney, fRoNear ifnIa geyf,oalNEncoga. [sent-1, score-0.051]
</p><p>2 s g ,  Abstract In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. [sent-9, score-0.15]
</p><p>3 We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. [sent-10, score-0.236]
</p><p>4 Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23. [sent-11, score-0.535]
</p><p>5 Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. [sent-14, score-0.441]
</p><p>6 1  Introduction  Language models have been extensively studied in natural language processing. [sent-15, score-0.033]
</p><p>7 The role of a lan-  guage model is to measure how probably a (target) word would occur based on some given evidence extracted from the history-context. [sent-16, score-0.051]
</p><p>8 1983) takes the immediately preceding history-word sequence, of length ? [sent-18, score-0.031]
</p><p>9 Although n-gram models are simple and effective, modeling long history-contexts lead to severe data scarcity problems. [sent-21, score-0.213]
</p><p>10 the trigram model, and any useful information beyond this window is neglected. [sent-24, score-0.185]
</p><p>11 In this work, we explore the possibility of modeling the presence of a history-word in terms of: (1) the distance and (2) the co-occurrence, with a target-word. [sent-25, score-0.203]
</p><p>12 These two attributes will be exploited and modeled independently from each other, i. [sent-26, score-0.192]
</p><p>13 the distance is described regardless the actual frequency of the history-word, while the co-occurrence is described regardless the actual position of the history-word. [sent-28, score-0.245]
</p><p>14 s g @ 2 two attributes as the term-distance (TD) and the term-occurrence (TO) components, respectively. [sent-32, score-0.069]
</p><p>15 The following section presents the most relevant related works. [sent-34, score-0.031]
</p><p>16 Section 4 presents in detail the derivation of both TD and TO model components. [sent-36, score-0.082]
</p><p>17 Finally, section 6 presents our conclusions and proposed future work. [sent-38, score-0.031]
</p><p>18 2  Related Work  The distant bigram model (Huang et. [sent-39, score-0.513]
</p><p>19 2007) disassembles the n-gram into (n−1) word-pairs, such that each pair is modeled by a distance-k bigram model, where 1? [sent-42, score-0.333]
</p><p>20 Each distance-k bigram model predicts the target-word based on the occurrence of a history-word located k positions behind. [sent-47, score-0.377]
</p><p>21 Zhou & Lua (1998) enhanced the effectiveness of the model by filtering out those wordpairs exhibiting low correlation, so that only the well associated distant bigrams are retained. [sent-48, score-0.336]
</p><p>22 This approach is referred to as the distance-dependent trigger model, and is similar to the earlier proposed trigger model (Lau et al. [sent-49, score-0.377]
</p><p>23 1996) that relies on the bigrams of arbitrary distance, i. [sent-51, score-0.036]
</p><p>24 Latent-semantic language model approaches (Bellegarda 1998, Coccaro 2005) weight word counts with TFIDF to highlight their semantic importance towards the prediction. [sent-54, score-0.13]
</p><p>25 In this type of approach, count statistics are accumulated from long contexts, typically beyond ten to twenty words. [sent-55, score-0.148]
</p><p>26 In order to confine the complexity introduced by such long contexts, word ordering is ignored (i. [sent-56, score-0.066]
</p><p>27 Other approaches such as the class-based language model (Brown 1992, Kneser & Ney 1993) 233  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-59, score-0.051]
</p><p>28 The structured language model (Chelba & Jelinek 2000) determines the “heads” in the history-context by using a parsing tree. [sent-62, score-0.051]
</p><p>29 Cache language models exploit temporal word frequencies in the history (Kuhn & Mori  1990, Clarkson & Robinson 1997). [sent-65, score-0.11]
</p><p>30 3  Motivation of the Proposed Approach  The attributes of distance and co-occurrence are exploited and modeled differently in each language modeling approach. [sent-66, score-0.427]
</p><p>31 In the n-gram model, for example, these two attributes are jointly taken into account in the ordered word-sequence. [sent-67, score-0.069]
</p><p>32 Consequently, the n-gram model can only be effectively implemented within a short history-context (e. [sent-68, score-0.093]
</p><p>33 Both, the conventional trigger model and the latent-semantic model capture the co-occurrence information while ignoring the distance information. [sent-71, score-0.399]
</p><p>34 It is reasonable to assume that distance information at far contexts is less likely to be informative and, hence, can be discarded. [sent-72, score-0.183]
</p><p>35 However, intermediate distances beyond the n-gram model limits can be very useful and should not be discarded. [sent-73, score-0.122]
</p><p>36 On the other hand, distant-bigram models and distance-dependent trigger models make use of both, distance and co-occurrence, information up to window sizes of ten to twenty. [sent-74, score-0.578]
</p><p>37 They achieve  this by compromising inter-dependencies among history-words (i. [sent-75, score-0.033]
</p><p>38 However, similarly to ngram models, distance and co-occurrence information are implicitly tied within the word-pairs. [sent-78, score-0.27]
</p><p>39 In our proposed approach, we attempt to exploit the TD and TO attributes, separately, to incorporate distant context information into the ngram, as a remedy to the data scarcity problem when learning the far context. [sent-79, score-0.336]
</p><p>40 4  Language Modeling with TD and TO  A language model estimates word probabilities given their history, i. [sent-80, score-0.051]
</p><p>41 Also, in order to alleviate the data scarcity problem, we assume the occurrences of the history-words to be independent from each other, conditioned to the occurrence of the target-word ? [sent-133, score-0.127]
</p><p>42 1  Derivation of the TD-TO Model  In order to define the TD and TO components for language modeling, we express the observation of an arbitrary history-word, ? [sent-230, score-0.149]
</p><p>43 at the kth position behind the target-word, as the joint of two events: i) the word ? [sent-234, score-0.033]
</p><p>44 This can be rewritten in terms of the likelihood function of the distance event (i. [sent-361, score-0.255]
</p><p>45 ), where both of them can be modeled and exploited separately, as follows: ? [sent-373, score-0.123]
</p><p>46 (3)  The formulation above yields three terms, referred to as the prior, the TD likelihood, and the TO likelihood, respectively. [sent-411, score-0.032]
</p><p>47 3, we have decoupled the observation of a word-pair into the events of distance and cooccurrence. [sent-413, score-0.233]
</p><p>48 4 is the log-linear interpolation (Klakow 1998) of these models. [sent-464, score-0.093]
</p><p>49 The prior, which is usually implemented as a unigram model, can be also replaced with a higher order n-gram model as, for instance, the bigram model: ? [sent-465, score-0.303]
</p><p>50 (5)  Replacing the unigram model with a higher order n-gram model is important to compensate the damage incurred by the conditional independence assumption made earlier. [sent-504, score-0.102]
</p><p>51 2 Term-Distance Model Component Basically, the TD likelihood measures how likely a given word-pair would be separated by a given distance. [sent-506, score-0.106]
</p><p>52 So, word-pairs possessing consistent separation distances will favor this likelihood. [sent-507, score-0.139]
</p><p>53 The above formulation of the TD likelihood requires smoothing for resolving two problems: i) a word-pair at a particular distance has a zero count, i. [sent-560, score-0.322]
</p><p>54 0 , which results in a zero probability, and ii) a word-pair is not seen at any distance within the observation window, i. [sent-577, score-0.281]
</p><p>55 For the first problem, we have attempted to redistribute the counts among the word-pairs at different distances (as observed within the win-  dow). [sent-593, score-0.225]
</p><p>56 We assumed that the counts of word-pairs are smooth in the distance domain and that the influence of a word decays as the distance increases. [sent-594, score-0.412]
</p><p>57 Notice, however, that this strategy is different from other conventional smoothing techniques (Chen & Goodman 1996), which rely mainly on the countof-count statistics for re-estimating and smoothing the original counts. [sent-597, score-0.126]
</p><p>58 For the second problem, when a word-pair was not seen at any distance (within the window), we arbitrarily assigned a small probability value, ? [sent-598, score-0.15]
</p><p>59 01 , to provide a slight chance for such a word-pair ? [sent-615, score-0.032]
</p><p>60 3 Term-Occurrence Model Component During the decoupling operation (from Eq. [sent-625, score-0.133]
</p><p>61 3), the TD model held only the distance information while the count information has been ignored. [sent-627, score-0.256]
</p><p>62 As a complement to the TD model, the TO model focuses on co-occurrence, and holds only count information. [sent-630, score-0.137]
</p><p>63 As the distance information is captured by the TD model, the co-occurrence count captured by the TO model is independent from the given word-pair distance. [sent-631, score-0.334]
</p><p>64 In fact, the TO model is closely related to the trigger language model (Rosenfeld 1996), as the prediction of the target-word (the triggered word) is based on the presence of a history-word (the trigger). [sent-632, score-0.249]
</p><p>65 However, differently from the trigger model, the TO model considers all the wordpairs without filtering out the weak associated ones. [sent-633, score-0.305]
</p><p>66 Additionally, the TO model takes into account multiple co-occurrences of the same history-word within the window, while the trigger model would count them only once (i. [sent-634, score-0.346]
</p><p>67 The word-pairs that frequently co-occur at arbitrary distances (within an observation window) would favor the TO likelihood. [sent-637, score-0.194]
</p><p>68 (7)  When a word-pair did not co-occur (within the observation window), we assigned a small probability value, ? [sent-670, score-0.052]
</p><p>69 01, to provide a slight chance for the history word to occur within the history-context of the target word. [sent-685, score-0.117]
</p><p>70 5  Perplexity Evaluation  A perplexity test was run on the BLLIP WSJ corpus (Charniak 2000) with the standard 5K vocabulary. [sent-686, score-0.131]
</p><p>71 We used them for parameter finetuning and performance evaluation. [sent-689, score-0.033]
</p><p>72 1 Capturing Distant Information In this experiment, we assessed the effectiveness of the TD and TO components in reducing the ngram’s perplexity. [sent-691, score-0.061]
</p><p>73 5, we interpolated n-gram models (of orders from two to six) with the TD, TO, and the both of them (referred  to as TD-TO model). [sent-693, score-0.084]
</p><p>74 The resulting interpolation weights were as follows: n-gram with TD = (0. [sent-703, score-0.093]
</p><p>75 Optimal sizes resulted to be 7, 5 and 8 for TD, TO, and TD-TO models, respectively. [sent-712, score-0.038]
</p><p>76 Deviating about two words from the optimum length only worsens the perplexity less than 1%. [sent-714, score-0.164]
</p><p>77 Baseline models, in each case, are standard ngram models with modified Kneser-Ney interpolation (Chen 1996). [sent-715, score-0.204]
</p><p>78 As seen from the table, for lower order n-gram models, the complementary information captured by the TD and TO components reduced the perplexity up to 23. [sent-725, score-0.266]
</p><p>79 hexagram, observe historycontexts of similar lengths as the ones observed by the TD, TO, and TD-TO models. [sent-730, score-0.075]
</p><p>80 Due to the incapability of n-grams to model long historycontexts, the TD and TO components are still effective in helping to enhance the prediction. [sent-731, score-0.178]
</p><p>81 Similar results were obtained by using the standard back-off model (Katz 1987) as baseline. [sent-732, score-0.051]
</p><p>82 2 Benefit of Decoupling Distant-Bigram In this second experiment, we examined whether the proposed decoupling procedure leads to better modeling of word-pairs compared to the distant bigram model. [sent-734, score-0.648]
</p><p>83 Here we compare the perplexity of both, the distance-k bigram model and distance-k TD model (for values of k ranging from two to ten), when combined with a standard bigram model. [sent-735, score-0.737]
</p><p>84 In order to make a fair comparison, without taking into account smoothing effects, we trained both models with raw counts and evaluated their perplexities over the train-set (so that no zeroprobability will be encountered). [sent-736, score-0.248]
</p><p>85 Perplexities of the distant bigram (DBG) and TD models when interpolated with a standard bigram model. [sent-744, score-0.798]
</p><p>86 The results from Table 2 show that the TD component complements the bigram model bet-  ter than the distant bigram itself. [sent-745, score-0.816]
</p><p>87 Firstly, these results suggest that the distance information (as modeled by the TD) offers better cue than the count information (as modeled by the distant bigram) to complement the n-gram model. [sent-746, score-0.608]
</p><p>88 The normalization of distant bigram counts, as indicated in Eq. [sent-747, score-0.462]
</p><p>89 This has been shown to be an effective manner to exploit the far context. [sent-749, score-0.034]
</p><p>90 Overall, decoupling the word historycontext into the TD and TO components offers a good approach to enhance language modeling. [sent-751, score-0.225]
</p><p>91 6  Conclusions  We have proposed a new approach to compute the n-gram probabilities, based on the TD and TO model components. [sent-752, score-0.051]
</p><p>92 Evaluated on the WSJ corpus, the proposed TD and TO models reduced the bigram’s and trigram’s perplexities up to 23. [sent-753, score-0.141]
</p><p>93 We have shown the advantages of modeling word-pairs with TD and TO, as compared to the distant bigram. [sent-756, score-0.263]
</p><p>94 As future work, we plan to explore the usefulness of the proposed model components in actual natural language processing applications such as machine translation and speech recognition. [sent-757, score-0.233]
</p><p>95 A multispan language modeling framework for larfge vocabulary speech recognition. [sent-769, score-0.143]
</p><p>96 An empirical study of smoothing techniques for language modeling. [sent-802, score-0.063]
</p><p>97 Language model adaptation using mixtures and an exponentially decaying cache. [sent-817, score-0.051]
</p><p>98 Latent semantic analysis as a tool to improve automatic speech recognition performance. [sent-823, score-0.09]
</p><p>99 Estimation of probabilities from sparse data for the language model component of a speech recognizer. [sent-843, score-0.141]
</p><p>100 Variable n-grams and extensions for conversational speech language modeling. [sent-896, score-0.09]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('td', 0.656), ('bigram', 0.252), ('distant', 0.21), ('distance', 0.15), ('trigger', 0.147), ('decoupling', 0.133), ('perplexity', 0.131), ('window', 0.119), ('wsj', 0.116), ('coccaro', 0.113), ('guthrie', 0.113), ('interpolation', 0.093), ('scarcity', 0.092), ('speech', 0.09), ('jelinek', 0.086), ('modeled', 0.081), ('counts', 0.079), ('rosenfeld', 0.079), ('ngram', 0.078), ('bahl', 0.075), ('clarkson', 0.075), ('historycontexts', 0.075), ('siu', 0.075), ('wordpairs', 0.075), ('perplexities', 0.073), ('likelihood', 0.072), ('distances', 0.071), ('attributes', 0.069), ('bellegarda', 0.066), ('brun', 0.066), ('klakow', 0.066), ('lua', 0.066), ('trigram', 0.066), ('smoothing', 0.063), ('chelba', 0.061), ('components', 0.061), ('ten', 0.058), ('robinson', 0.058), ('bllip', 0.058), ('mori', 0.055), ('count', 0.055), ('depicted', 0.054), ('ieee', 0.054), ('modeling', 0.053), ('observation', 0.052), ('model', 0.051), ('lv', 0.051), ('complements', 0.051), ('interpolated', 0.051), ('lau', 0.051), ('kneser', 0.047), ('ostendorf', 0.047), ('ntu', 0.047), ('star', 0.045), ('sg', 0.045), ('notice', 0.043), ('history', 0.043), ('exploited', 0.042), ('within', 0.042), ('goodman', 0.041), ('katz', 0.041), ('audio', 0.041), ('kuhn', 0.04), ('captured', 0.039), ('located', 0.039), ('sizes', 0.038), ('zhai', 0.037), ('zero', 0.037), ('ney', 0.036), ('arbitrary', 0.036), ('favor', 0.035), ('reduced', 0.035), ('occurrence', 0.035), ('long', 0.035), ('exploit', 0.034), ('separated', 0.034), ('models', 0.033), ('chng', 0.033), ('decays', 0.033), ('compromising', 0.033), ('possessing', 0.033), ('finetuning', 0.033), ('worsens', 0.033), ('redistribute', 0.033), ('hli', 0.033), ('charniak', 0.033), ('position', 0.033), ('contexts', 0.033), ('event', 0.033), ('referred', 0.032), ('slight', 0.032), ('differently', 0.032), ('enhance', 0.031), ('presents', 0.031), ('actual', 0.031), ('preceding', 0.031), ('complement', 0.031), ('dow', 0.031), ('confine', 0.031), ('decoupled', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="247-tfidf-1" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>Author: Tze Yuang Chong ; Rafael E. Banchs ; Eng Siong Chng ; Haizhou Li</p><p>Abstract: In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1</p><p>2 0.12238869 <a title="247-tfidf-2" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>3 0.11309238 <a title="247-tfidf-3" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>Author: Brian Roark ; Cyril Allauzen ; Michael Riley</p><p>Abstract: We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library.1</p><p>4 0.095192887 <a title="247-tfidf-4" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>5 0.091195688 <a title="247-tfidf-5" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>6 0.084714077 <a title="247-tfidf-6" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>7 0.074229166 <a title="247-tfidf-7" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>8 0.073655762 <a title="247-tfidf-8" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>9 0.070353493 <a title="247-tfidf-9" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>10 0.061261076 <a title="247-tfidf-10" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>11 0.058518339 <a title="247-tfidf-11" href="./acl-2013-A_Visual_Analytics_System_for_Cluster_Exploration.html">29 acl-2013-A Visual Analytics System for Cluster Exploration</a></p>
<p>12 0.057972006 <a title="247-tfidf-12" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>13 0.057337843 <a title="247-tfidf-13" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>14 0.056917466 <a title="247-tfidf-14" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>15 0.056440152 <a title="247-tfidf-15" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>16 0.052377012 <a title="247-tfidf-16" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>17 0.051336344 <a title="247-tfidf-17" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>18 0.049734585 <a title="247-tfidf-18" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>19 0.049703855 <a title="247-tfidf-19" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>20 0.049221672 <a title="247-tfidf-20" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.132), (1, 0.013), (2, 0.004), (3, -0.031), (4, 0.002), (5, 0.028), (6, 0.031), (7, 0.044), (8, -0.056), (9, 0.022), (10, -0.013), (11, -0.057), (12, -0.007), (13, -0.03), (14, 0.0), (15, -0.03), (16, -0.051), (17, -0.04), (18, 0.021), (19, -0.056), (20, 0.061), (21, -0.064), (22, 0.039), (23, -0.022), (24, 0.063), (25, -0.059), (26, 0.049), (27, -0.039), (28, 0.003), (29, -0.056), (30, -0.043), (31, -0.009), (32, -0.116), (33, 0.088), (34, -0.002), (35, 0.003), (36, 0.111), (37, 0.009), (38, -0.109), (39, 0.008), (40, -0.002), (41, 0.055), (42, -0.141), (43, -0.004), (44, -0.04), (45, 0.03), (46, 0.01), (47, -0.04), (48, 0.024), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93103892 <a title="247-lsi-1" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>Author: Tze Yuang Chong ; Rafael E. Banchs ; Eng Siong Chng ; Haizhou Li</p><p>Abstract: In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1</p><p>2 0.7513473 <a title="247-lsi-2" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>Author: Brian Roark ; Cyril Allauzen ; Michael Riley</p><p>Abstract: We present an algorithm for re-estimating parameters of backoff n-gram language models so as to preserve given marginal distributions, along the lines of wellknown Kneser-Ney (1995) smoothing. Unlike Kneser-Ney, our approach is designed to be applied to any given smoothed backoff model, including models that have already been heavily pruned. As a result, the algorithm avoids issues observed when pruning Kneser-Ney models (Siivola et al., 2007; Chelba et al., 2010), while retaining the benefits of such marginal distribution constraints. We present experimental results for heavily pruned backoff ngram models, and demonstrate perplexity and word error rate reductions when used with various baseline smoothing methods. An open-source version of the algorithm has been released as part of the OpenGrm ngram library.1</p><p>3 0.69858664 <a title="247-lsi-3" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>Author: Kenneth Heafield ; Ivan Pouzyrevsky ; Jonathan H. Clark ; Philipp Koehn</p><p>Abstract: We present an efficient algorithm to estimate large modified Kneser-Ney models including interpolation. Streaming and sorting enables the algorithm to scale to much larger models by using a fixed amount of RAM and variable amount of disk. Using one machine with 140 GB RAM for 2.8 days, we built an unpruned model on 126 billion tokens. Machine translation experiments with this model show improvement of 0.8 BLEU point over constrained systems for the 2013 Workshop on Machine Translation task in three language pairs. Our algorithm is also faster for small models: we estimated a model on 302 million tokens using 7.7% of the RAM and 14.0% of the wall time taken by SRILM. The code is open source as part of KenLM.</p><p>4 0.62762827 <a title="247-lsi-4" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>5 0.60850507 <a title="247-lsi-5" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>Author: David Kauchak</p><p>Abstract: In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams.</p><p>6 0.59490705 <a title="247-lsi-6" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>7 0.53037369 <a title="247-lsi-7" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>8 0.48373264 <a title="247-lsi-8" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>9 0.47454056 <a title="247-lsi-9" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>10 0.47305524 <a title="247-lsi-10" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>11 0.46939176 <a title="247-lsi-11" href="./acl-2013-A_Comparison_of_Techniques_to_Automatically_Identify_Complex_Words..html">3 acl-2013-A Comparison of Techniques to Automatically Identify Complex Words.</a></p>
<p>12 0.4638322 <a title="247-lsi-12" href="./acl-2013-Addressing_Ambiguity_in_Unsupervised_Part-of-Speech_Induction_with_Substitute_Vectors.html">39 acl-2013-Addressing Ambiguity in Unsupervised Part-of-Speech Induction with Substitute Vectors</a></p>
<p>13 0.46250805 <a title="247-lsi-13" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>14 0.45593557 <a title="247-lsi-14" href="./acl-2013-What_causes_a_causal_relation%3F_Detecting_Causal_Triggers_in_Biomedical_Scientific_Discourse.html">386 acl-2013-What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse</a></p>
<p>15 0.45471612 <a title="247-lsi-15" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>16 0.45134857 <a title="247-lsi-16" href="./acl-2013-Adaptation_Data_Selection_using_Neural_Language_Models%3A_Experiments_in_Machine_Translation.html">35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</a></p>
<p>17 0.44105124 <a title="247-lsi-17" href="./acl-2013-An_improved_MDL-based_compression_algorithm_for_unsupervised_word_segmentation.html">50 acl-2013-An improved MDL-based compression algorithm for unsupervised word segmentation</a></p>
<p>18 0.43608594 <a title="247-lsi-18" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>19 0.42240846 <a title="247-lsi-19" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>20 0.42159337 <a title="247-lsi-20" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.053), (6, 0.032), (11, 0.047), (15, 0.013), (24, 0.078), (26, 0.034), (29, 0.255), (35, 0.094), (42, 0.085), (48, 0.071), (70, 0.043), (88, 0.029), (90, 0.018), (95, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94370997 <a title="247-lda-1" href="./acl-2013-TopicSpam%3A_a_Topic-Model_based_approach_for_spam_detection.html">350 acl-2013-TopicSpam: a Topic-Model based approach for spam detection</a></p>
<p>Author: Jiwei Li ; Claire Cardie ; Sujian Li</p><p>Abstract: Product reviews are now widely used by individuals and organizations for decision making (Litvin et al., 2008; Jansen, 2010). And because of the profits at stake, people have been known to try to game the system by writing fake reviews to promote target products. As a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle dif- ferences between deceptive reviews and truthful ones and achieves about 95% accuracy on review spam datasets, outperforming existing baselines by a large margin.</p><p>2 0.8674528 <a title="247-lda-2" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>Author: Heike Adel ; Ngoc Thang Vu ; Tanja Schultz</p><p>Abstract: In this paper, we investigate the application of recurrent neural network language models (RNNLM) and factored language models (FLM) to the task of language modeling for Code-Switching speech. We present a way to integrate partof-speech tags (POS) and language information (LID) into these models which leads to significant improvements in terms of perplexity. Furthermore, a comparison between RNNLMs and FLMs and a detailed analysis of perplexities on the different backoff levels are performed. Finally, we show that recurrent neural networks and factored language models can . be combined using linear interpolation to achieve the best performance. The final combined language model provides 37.8% relative improvement in terms of perplexity on the SEAME development set and a relative improvement of 32.7% on the evaluation set compared to the traditional n-gram language model. Index Terms: multilingual speech processing, code switching, language modeling, recurrent neural networks, factored language models</p><p>same-paper 3 0.8096838 <a title="247-lda-3" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>Author: Tze Yuang Chong ; Rafael E. Banchs ; Eng Siong Chng ; Haizhou Li</p><p>Abstract: In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1</p><p>4 0.80656171 <a title="247-lda-4" href="./acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees.html">285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</a></p>
<p>Author: Alan Akbik ; Oresti Konomi ; Michail Melnikov</p><p>Abstract: The use ofdeep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction. Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. We introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the PROP- MINER tool.</p><p>5 0.71206164 <a title="247-lda-5" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>6 0.6669485 <a title="247-lda-6" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>7 0.60410339 <a title="247-lda-7" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>8 0.60385549 <a title="247-lda-8" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>9 0.59865922 <a title="247-lda-9" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>10 0.59681964 <a title="247-lda-10" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>11 0.59631431 <a title="247-lda-11" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>12 0.59143901 <a title="247-lda-12" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>13 0.5911479 <a title="247-lda-13" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>14 0.5909878 <a title="247-lda-14" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>15 0.58983052 <a title="247-lda-15" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>16 0.58833939 <a title="247-lda-16" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>17 0.58795589 <a title="247-lda-17" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>18 0.58652639 <a title="247-lda-18" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>19 0.58601183 <a title="247-lda-19" href="./acl-2013-ICARUS_-_An_Extensible_Graphical_Search_Tool_for_Dependency_Treebanks.html">183 acl-2013-ICARUS - An Extensible Graphical Search Tool for Dependency Treebanks</a></p>
<p>20 0.58593512 <a title="247-lda-20" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
