<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>49 acl-2013-An annotated corpus of quoted opinions in news articles</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-49" href="#">acl2013-49</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>49 acl-2013-An annotated corpus of quoted opinions in news articles</h1>
<br/><p>Source: <a title="acl-2013-49-pdf" href="http://aclweb.org/anthology//P/P13/P13-2092.pdf">pdf</a></p><p>Author: Tim O'Keefe ; James R. Curran ; Peter Ashwell ; Irena Koprinska</p><p>Abstract: Quotes are used in news articles as evidence of a person’s opinion, and thus are a useful target for opinion mining. However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. Using this we construct a corpus covering 7 topics with 2,228 quotes.</p><p>Reference: <a title="acl-2013-49-reference" href="../acl2013_reference/acl-2013-An_annotated_corpus_of_quoted_opinions_in_news_articles_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 au  Abstract Quotes are used in news articles as evidence of a person’s opinion, and thus are a useful target for opinion mining. [sent-5, score-0.361]
</p><p>2 However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. [sent-6, score-0.732]
</p><p>3 We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. [sent-7, score-0.961]
</p><p>4 Using this we construct a corpus covering 7 topics with 2,228 quotes. [sent-8, score-0.057]
</p><p>5 1 Introduction News articles are a useful target for opinion mining as they discuss salient opinions by newswor-  thy people. [sent-9, score-0.374]
</p><p>6 Rather than asserting what a person’s opinion is, journalists typically provide evidence by using reported speech, and in particular, direct quotes. [sent-10, score-0.311]
</p><p>7 We focus on direct quotes as expressions of opinion, as they can be accurately extracted and attributed to a speaker (O’Keefe et al. [sent-11, score-0.704]
</p><p>8 In sentiment analysis over product reviews, polarity labels are commonly used because the target, the product, is clearly identified. [sent-14, score-0.259]
</p><p>9 However, for quotes on topics of debate, the target and meaning of polarity labels is less clear. [sent-15, score-0.753]
</p><p>10 For example, labelling a quote about abortion as simply positive or negative is uninformative, as a speaker can use either positive or negative language to support or oppose either side of the debate. [sent-16, score-0.785]
</p><p>11 , 2010) has addressed this by giving each expression of opinion a textually-anchored target. [sent-19, score-0.171]
</p><p>12 Our solution is to instead define position statements, which are  Abortion: Women should have the right to choose an abortion. [sent-21, score-0.106]
</p><p>13 Carbon tax: Australia should introduce a tax on carbon or an emissions trading scheme to combat global warming. [sent-22, score-0.372]
</p><p>14 Republic: Australia should cease to be a monarchy with the Queen as head of state and become a republic with an Australian head of state. [sent-25, score-0.149]
</p><p>15 clear statements  of a viewpoint  or position on a  particular topic. [sent-29, score-0.269]
</p><p>16 Quotes related to this topic can  then be labelled as supporting, neutral, or opposing the position statement. [sent-30, score-0.351]
</p><p>17 This disambiguates the meaning of the polarity labels, and allows us to determine the side of the debate that the speaker  is on. [sent-31, score-0.379]
</p><p>18 Table 1 shows the topics and position statements used in this work, and some example quotes from the republic topic are given below. [sent-32, score-0.901]
</p><p>19 Note that the first example includes no explicit mention of the monarchy or the republic. [sent-33, score-0.095]
</p><p>20 ” Neutral: “The establishment of an Australian republic is essentially a symbolic change, with the main arguments, for and against, turning on national identity. [sent-38, score-0.054]
</p><p>21 ” Negative: “I personally think that the monarchy is a tradition which we want to keep. [sent-41, score-0.095]
</p><p>22 ” With this formulation we define an annotation scheme and build a corpus covering 7 topics, with 100 documents per topic. [sent-42, score-0.039]
</p><p>23 This corpus includes 3,428 quotes, of which 1,183 were marked invalid, leaving 2,228 that were marked as supporting, neutral, or opposing the relevant topic statement. [sent-43, score-0.271]
</p><p>24 All quotes in our corpus were annotated by three annotators, with Fleiss’ κ values of between 0. [sent-44, score-0.532]
</p><p>25 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioinngauli Lsitnicgsu,i psatgices 516–520,  2  Background  Early work in sentiment analysis (Turney, 2002; Pang et al. [sent-49, score-0.086]
</p><p>26 , 2007) focused on product and movie reviews, where the text under analysis discusses a single product or movie. [sent-52, score-0.076]
</p><p>27 In these cases, labels like positive and negative are appropriate as they align well with the overall communicative goal of the text. [sent-53, score-0.12]
</p><p>28 Later work established aspect-oriented opinion mining (Hu and Liu, 2004), where the aim is to find features or aspects of products that are discussed in a review. [sent-54, score-0.204]
</p><p>29 The reviewer’s position on each aspect can then be classified as positive or negative, which results in a more fine-grained classification that can be combined to form an opinion summary. [sent-55, score-0.277]
</p><p>30 These approaches assume that each document has a single source (the document’s author), whose communicative goal is to evaluate a well-defined target, such as a product or a movie. [sent-56, score-0.07]
</p><p>31 However this does not hold in news articles, where the goal of the journalist is to present the viewpoints of potentially many people. [sent-57, score-0.075]
</p><p>32 , 2007) have looked at sentiment in news text, with some (Balahur and Steinberger, 2009; Balahur et al. [sent-61, score-0.161]
</p><p>33 In all of these studies the authors have textually-anchored the target of the sentiment. [sent-63, score-0.029]
</p><p>34 While this makes sense for targets that can be resolved back to named entities, it does not apply as obviously when the quote is arguing for a particular viewpoint in a debate, as the topic may not be mentioned explicitly and polarity labels may not align to sides of the debate. [sent-64, score-0.718]
</p><p>35 Work on debate summarisation and subgroup detection (Somasundaran and Wiebe, 2010; AbuJbara et al. [sent-65, score-0.205]
</p><p>36 , 2012) has often used data from online debate forums, particularly those forums where users are asked to select whether they support or oppose a given proposition before they can participate. [sent-67, score-0.301]
</p><p>37 This is similar to our aim with news text, where instead of a textually-anchored target, we have a proposition, against which we can evaluate quotes. [sent-68, score-0.075]
</p><p>38 3  Position Statements  Our goal in this study is to determine which side of a debate a given quote supports. [sent-69, score-0.602]
</p><p>39 Assigning polarity labels to a textually-anchored target does not work here for several reasons. [sent-70, score-0.164]
</p><p>40 Quotes may not mention the debate topic, there may be many rel-  TopicQuotesANAo contκ. [sent-71, score-0.166]
</p><p>41 s4 ’45 κ over the valid quotes evant textually-anchored targets for a single topic, and polarity labels do not necessarily align with  sides of a debate. [sent-82, score-0.667]
</p><p>42 We instead define position statements, which clearly state the position that one side ofthe debate is arguing for. [sent-83, score-0.417]
</p><p>43 We can then characterise opinions as supporting, neutral towards, or opposing this particular position. [sent-84, score-0.405]
</p><p>44 Position statements should not argue for a particular position, rather they should simply state what the position is. [sent-85, score-0.194]
</p><p>45 Table 1 shows the position statements that we use in this work. [sent-86, score-0.194]
</p><p>46 4  Annotation  For our task we expect a set of news articles on a given topic as input, where the direct quotes in the articles have been extracted and attributed to speakers. [sent-87, score-0.79]
</p><p>47 A position statement will have been defined, that states a point of view on the topic, and a small subset of quotes will have been labelled as supporting, neutral, or opposing the given statement. [sent-88, score-0.856]
</p><p>48 A system performing this task would then label the remaining quotes as supporting, neutral, or opposing, and return them to the user. [sent-89, score-0.532]
</p><p>49 A major contribution of this work is that we construct a fully labelled corpus, which can be  used to evaluate systems that perform the task described above. [sent-90, score-0.036]
</p><p>50 Our data is drawn from the Sydney Morning Herald2 archive, which ranges from 1986 until 2009, and it covers seven topics that were subject to debate within Australian news media during that time. [sent-92, score-0.298]
</p><p>51 431 416940 TaTWboloteark l3 :ch Aovice rsage A2g,2 r64e95emen. [sent-105, score-0.054]
</p><p>52 s3 ’2 κ when the labels are neutral versus non-neutral Apache Solr3 to find the top 100 documents that matched a manually-constructed search query. [sent-109, score-0.217]
</p><p>53 Finally, the quotes were extracted and attributed to speakers using the system from O’Keefe et al. [sent-112, score-0.569]
</p><p>54 For the first part of the task, annotators were asked to label each quote without considering any context. [sent-114, score-0.588]
</p><p>55 In other words they were asked to only use the text of the quote itself as evidence for an opinion, not the speaker’s prior opinions or the  text of the document. [sent-115, score-0.646]
</p><p>56 They were then asked to label the quote a second time, while considering the text surrounding the quote, although they were still asked to ignore the prior opinions of the speaker. [sent-116, score-0.666]
</p><p>57 For each of these choices annotators were given a five-point scale ranging from strong or clear opposition to strong or clear support, where support or opposition is relative to the position statement. [sent-117, score-0.385]
</p><p>58 Annotators were also asked to mark instances where either the speaker or quote span was incorrectly identified, although they were asked to continue annotating the quote as though it were correct. [sent-118, score-1.166]
</p><p>59 They were also asked to mark quotes that were invalid due to either the quote being offtopic, or the item not being a quote (e. [sent-119, score-1.504]
</p><p>60 5  Corpus results  In order to achieve the least amount of noise in our corpus, we opted to discard quotes that any annotator had marked as invalid. [sent-123, score-0.595]
</p><p>61 From the original cor-  pus, 23% were marked off-topic, which shows that 3http://lucene. [sent-125, score-0.031]
</p><p>62 org/solr/ in order to label opinions in news, a system would first have to identify the topic-relevant parts of the text. [sent-127, score-0.1]
</p><p>63 The annotators further indicated that 16% were not quotes, and there were a small number of cases (< 1%) where the quote span was incorrect. [sent-128, score-0.523]
</p><p>64 Annotators were able to select multiple reasons for a quote being invalid. [sent-129, score-0.436]
</p><p>65 Table 2 shows both Fleiss’ κ and the raw agreement averaged between annotators for each topic. [sent-130, score-0.119]
</p><p>66 We collapsed the two supporting labels together, as well as the two opposing labels, such that we end up with a classification of opposes vs. [sent-131, score-0.295]
</p><p>67 Intuitively we expect that the confusion is largely between neutral and the two polar labels. [sent-139, score-0.198]
</p><p>68 To examine this we merged all the non-neutral labels into one group and calculated the agreement between the non-neutral group and the neutral la-  bel, as shown in Table 3. [sent-140, score-0.249]
</p><p>69 neutral agreement we find that despite stability in raw agreement, Fleiss’ κ drops substantially, to 0. [sent-142, score-0.192]
</p><p>70 For comparison we remove all neutral annotations and focus on disagreement between the polar labels. [sent-145, score-0.198]
</p><p>71 92) indicates that deciding when an opinion provides sufficient evidence of support or opposition is the main challenge facing annotators. [sent-154, score-0.281]
</p><p>72 To adjudicate the decisions annotators made, we opted to take a majority vote for cases of two or three-way agreement, while discarding cases where annotators did not agree (1% of quotes). [sent-155, score-0.206]
</p><p>73 The final distribution of labels in the corpus is shown in Table 4. [sent-156, score-0.057]
</p><p>74 For both the no context and context cases the largest class was neutral with  61% and 46% of the corpus respectively. [sent-157, score-0.224]
</p><p>75 The drop in neutrality between the no context and context cases shows that the interpretation of a quote can change based on the context it is placed in. [sent-158, score-0.532]
</p><p>76 6  Discussion  In refining our annotation scheme we noted several factors that make annotation difficult. [sent-159, score-0.039]
</p><p>77 Opinion relevance When discussing a topic, journalists will often delve into the related aspects and opinions that people hold. [sent-166, score-0.23]
</p><p>78 This introduces a challenge as annotators need to decide whether a particular quote is on-topic enough to be labelled. [sent-167, score-0.523]
</p><p>79 For instance, these quotes by the same speaker were in an article on the carbon tax:  1) “Whether it’s a stealth tax, the emissions trading scheme, whether it’s an upfront. [sent-168, score-0.877]
</p><p>80 tax like a carbon tax, there will not be any new taxes as part of the Coalition’s policy” 2) “I don’t think it’s something that we should rush into. [sent-171, score-0.255]
</p><p>81 But certainly I’m happy to see a debate about the nuclear option. [sent-172, score-0.204]
</p><p>82 ” In the first quote the speaker is voicing opposition to a tax on carbon, which is easy to annotate with our scheme. [sent-173, score-0.759]
</p><p>83 However in the second quote, the speaker is discussing nuclear power in relation to a carbon tax, which is much more difficult, as it is unclear whether is is off-topic or neutral. [sent-174, score-0.34]
</p><p>84 Obfuscation and self-contradiction While journalists usually quote someone to provide evidence of the person’s opinion, there are some cases where they include quotes to show that the person is inconsistent. [sent-175, score-1.14]
</p><p>85 The following quotes by the same speaker were included in an article to illustrate that the speaker’s position was inconsistent: 1) “My point is that. [sent-176, score-0.773]
</p><p>86 the most potent argument in favour of the republic, is that why should we have a Briton as the Queen – who, of course, in reality is also the Queen of Australia but a Briton as the head of State of Australia”  2) “The Coalition supports the Constitution not because we support the. [sent-179, score-0.039]
</p><p>87 notion ofthe monarchy, but because we support the way our present Constitution works” The above example also indicates a level of obfuscation that is reasonably common for politicians. [sent-182, score-0.054]
</p><p>88 Neither of the quotes actually expresses a clear statement of how the speaker feels about a potential republic. [sent-183, score-0.735]
</p><p>89 The first quote is an opinion about the strongest argument in favour of a republic, without necessarily making that argument, while the second quote states a party line, with a caveat that might indicate personal disagreement. [sent-184, score-1.082]
</p><p>90 Annotator bias This task is prone to be influenced by an annotator’s biases, including their political or cultural background, their opinion about the topic or speaker, or their level of knowledge about the topic. [sent-185, score-0.235]
</p><p>91 7  Conclusion  In this work we examined the problem of annotating opinions in news articles. [sent-186, score-0.204]
</p><p>92 We proposed to  exploit quotes, as they are used by journalists to provide evidence of an opinion, and are easy to extract and attribute to speakers. [sent-187, score-0.14]
</p><p>93 Our key contribution is that rather than requiring a textuallyanchored target for each quote, we instead label quotes as supporting, neutral, or opposing a position statement, which states a particular viewpoint on a topic. [sent-188, score-0.856]
</p><p>94 This allowed us to resolve ambiguities that arise when considering a polarity label towards a topic. [sent-189, score-0.078]
</p><p>95 We next defined an annotation scheme and built a corpus, which covers 7 topics, with 100 documents per topic, and a total of 2,228 annotated quotes. [sent-190, score-0.039]
</p><p>96 Rethinking sentiment analysis in the news: From theory to practice and back. [sent-200, score-0.086]
</p><p>97 Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. [sent-212, score-0.086]
</p><p>98 Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. [sent-216, score-0.038]
</p><p>99 Detecting subgroups in online discussions by modeling positive and negative relations among participants. [sent-229, score-0.031]
</p><p>100 Extracting opinions, opinion holders, and topics expressed in online news media text. [sent-237, score-0.303]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('quotes', 0.532), ('quote', 0.436), ('opinion', 0.171), ('debate', 0.166), ('neutral', 0.16), ('opposing', 0.145), ('speaker', 0.135), ('keefe', 0.135), ('carbon', 0.132), ('tax', 0.123), ('balahur', 0.118), ('position', 0.106), ('opinions', 0.1), ('journalists', 0.095), ('monarchy', 0.095), ('supporting', 0.093), ('australian', 0.091), ('statements', 0.088), ('annotators', 0.087), ('sentiment', 0.086), ('australia', 0.08), ('polarity', 0.078), ('news', 0.075), ('asked', 0.065), ('opposition', 0.065), ('fleiss', 0.065), ('topic', 0.064), ('wiebe', 0.062), ('abortion', 0.059), ('topics', 0.057), ('labels', 0.057), ('queen', 0.055), ('steinberger', 0.055), ('aovice', 0.054), ('briton', 0.054), ('cmaeb', 0.054), ('godbole', 0.054), ('isra', 0.054), ('koprinska', 0.054), ('mijail', 0.054), ('obfuscation', 0.054), ('rsage', 0.054), ('tatwboloteark', 0.054), ('topicquotesanao', 0.054), ('labelling', 0.054), ('republic', 0.054), ('somasundaran', 0.048), ('ralf', 0.048), ('hachey', 0.048), ('constitution', 0.048), ('immigration', 0.048), ('thumbs', 0.045), ('evidence', 0.045), ('janyce', 0.044), ('viewpoint', 0.044), ('irena', 0.044), ('amjad', 0.044), ('goot', 0.044), ('coalition', 0.041), ('articles', 0.041), ('swapna', 0.039), ('crc', 0.039), ('emissions', 0.039), ('arguing', 0.039), ('favour', 0.039), ('oppose', 0.039), ('subgroup', 0.039), ('trading', 0.039), ('wilson', 0.039), ('scheme', 0.039), ('product', 0.038), ('cont', 0.038), ('pouliquen', 0.038), ('nuclear', 0.038), ('ideological', 0.038), ('polar', 0.038), ('statement', 0.037), ('attributed', 0.037), ('labelled', 0.036), ('erik', 0.035), ('invalid', 0.035), ('dave', 0.035), ('discussing', 0.035), ('mining', 0.033), ('sydney', 0.033), ('agreement', 0.032), ('communicative', 0.032), ('opted', 0.032), ('context', 0.032), ('person', 0.032), ('capital', 0.031), ('alexandra', 0.031), ('marked', 0.031), ('negative', 0.031), ('forums', 0.031), ('clear', 0.031), ('bruno', 0.029), ('blitzer', 0.029), ('target', 0.029), ('annotating', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="49-tfidf-1" href="./acl-2013-An_annotated_corpus_of_quoted_opinions_in_news_articles.html">49 acl-2013-An annotated corpus of quoted opinions in news articles</a></p>
<p>Author: Tim O'Keefe ; James R. Curran ; Peter Ashwell ; Irena Koprinska</p><p>Abstract: Quotes are used in news articles as evidence of a person’s opinion, and thus are a useful target for opinion mining. However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. Using this we construct a corpus covering 7 topics with 2,228 quotes.</p><p>2 0.18985662 <a title="49-tfidf-2" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>Author: Amjad Abu-Jbara ; Ben King ; Mona Diab ; Dragomir Radev</p><p>Abstract: In this paper, we use Arabic natural language processing techniques to analyze Arabic debates. The goal is to identify how the participants in a discussion split into subgroups with contrasting opinions. The members of each subgroup share the same opinion with respect to the discussion topic and an opposing opinion to the members of other subgroups. We use opinion mining techniques to identify opinion expressions and determine their polarities and their targets. We opinion predictions to represent the discussion in one of two formal representations: signed attitude network or a space of attitude vectors. We identify opinion subgroups by partitioning the signed network representation or by clustering the vector space representation. We evaluate the system using a data set of labeled discussions and show that it achieves good results.</p><p>3 0.18774684 <a title="49-tfidf-3" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>Author: Bishan Yang ; Claire Cardie</p><p>Abstract: This paper addresses the task of finegrained opinion extraction the identification of opinion-related entities: the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the interdependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks – of opinion extraction, and seeks a globally optimal solution. Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction.</p><p>4 0.17478146 <a title="49-tfidf-4" href="./acl-2013-Mining_Opinion_Words_and_Opinion_Targets_in_a_Two-Stage_Framework.html">244 acl-2013-Mining Opinion Words and Opinion Targets in a Two-Stage Framework</a></p>
<p>Author: Liheng Xu ; Kang Liu ; Siwei Lai ; Yubo Chen ; Jun Zhao</p><p>Abstract: This paper proposes a novel two-stage method for mining opinion words and opinion targets. In the first stage, we propose a Sentiment Graph Walking algorithm, which naturally incorporates syntactic patterns in a Sentiment Graph to extract opinion word/target candidates. Then random walking is employed to estimate confidence of candidates, which improves extraction accuracy by considering confidence of patterns. In the second stage, we adopt a self-learning strategy to refine the results from the first stage, especially for filtering out high-frequency noise terms and capturing the long-tail terms, which are not investigated by previous methods. The experimental results on three real world datasets demonstrate the effectiveness of our approach compared with stateof-the-art unsupervised methods.</p><p>5 0.15829168 <a title="49-tfidf-5" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>6 0.14597644 <a title="49-tfidf-6" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>7 0.13620017 <a title="49-tfidf-7" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>8 0.11883128 <a title="49-tfidf-8" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>9 0.11232881 <a title="49-tfidf-9" href="./acl-2013-Extra-Linguistic_Constraints_on_Stance_Recognition_in_Ideological_Debates.html">151 acl-2013-Extra-Linguistic Constraints on Stance Recognition in Ideological Debates</a></p>
<p>10 0.10861623 <a title="49-tfidf-10" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>11 0.10373995 <a title="49-tfidf-11" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>12 0.10019447 <a title="49-tfidf-12" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>13 0.090027519 <a title="49-tfidf-13" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>14 0.089884639 <a title="49-tfidf-14" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>15 0.089581564 <a title="49-tfidf-15" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<p>16 0.087494791 <a title="49-tfidf-16" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>17 0.077385597 <a title="49-tfidf-17" href="./acl-2013-Public_Dialogue%3A_Analysis_of_Tolerance_in_Online_Discussions.html">287 acl-2013-Public Dialogue: Analysis of Tolerance in Online Discussions</a></p>
<p>18 0.069407493 <a title="49-tfidf-18" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>19 0.068171956 <a title="49-tfidf-19" href="./acl-2013-Bi-directional_Inter-dependencies_of_Subjective_Expressions_and_Targets_and_their_Value_for_a_Joint_Model.html">67 acl-2013-Bi-directional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model</a></p>
<p>20 0.067993395 <a title="49-tfidf-20" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, 0.21), (2, -0.027), (3, 0.166), (4, -0.074), (5, 0.042), (6, -0.085), (7, -0.047), (8, -0.062), (9, -0.01), (10, -0.032), (11, 0.058), (12, -0.008), (13, 0.034), (14, -0.047), (15, -0.027), (16, -0.02), (17, 0.061), (18, 0.009), (19, 0.016), (20, -0.053), (21, -0.04), (22, -0.023), (23, 0.001), (24, -0.012), (25, -0.02), (26, -0.034), (27, -0.037), (28, 0.013), (29, 0.026), (30, 0.003), (31, -0.036), (32, 0.034), (33, -0.046), (34, -0.022), (35, -0.004), (36, 0.122), (37, -0.033), (38, -0.036), (39, -0.04), (40, 0.035), (41, 0.031), (42, 0.05), (43, -0.054), (44, 0.026), (45, 0.038), (46, 0.022), (47, -0.073), (48, -0.02), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94103515 <a title="49-lsi-1" href="./acl-2013-An_annotated_corpus_of_quoted_opinions_in_news_articles.html">49 acl-2013-An annotated corpus of quoted opinions in news articles</a></p>
<p>Author: Tim O'Keefe ; James R. Curran ; Peter Ashwell ; Irena Koprinska</p><p>Abstract: Quotes are used in news articles as evidence of a person’s opinion, and thus are a useful target for opinion mining. However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. Using this we construct a corpus covering 7 topics with 2,228 quotes.</p><p>2 0.78554869 <a title="49-lsi-2" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>Author: Amjad Abu-Jbara ; Ben King ; Mona Diab ; Dragomir Radev</p><p>Abstract: In this paper, we use Arabic natural language processing techniques to analyze Arabic debates. The goal is to identify how the participants in a discussion split into subgroups with contrasting opinions. The members of each subgroup share the same opinion with respect to the discussion topic and an opposing opinion to the members of other subgroups. We use opinion mining techniques to identify opinion expressions and determine their polarities and their targets. We opinion predictions to represent the discussion in one of two formal representations: signed attitude network or a space of attitude vectors. We identify opinion subgroups by partitioning the signed network representation or by clustering the vector space representation. We evaluate the system using a data set of labeled discussions and show that it achieves good results.</p><p>3 0.68206906 <a title="49-lsi-3" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques.</p><p>4 0.67921066 <a title="49-lsi-4" href="./acl-2013-Extra-Linguistic_Constraints_on_Stance_Recognition_in_Ideological_Debates.html">151 acl-2013-Extra-Linguistic Constraints on Stance Recognition in Ideological Debates</a></p>
<p>Author: Kazi Saidul Hasan ; Vincent Ng</p><p>Abstract: Determining the stance expressed by an author from a post written for a twosided debate in an online debate forum is a relatively new problem. We seek to improve Anand et al.’s (201 1) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification.</p><p>5 0.67029995 <a title="49-lsi-5" href="./acl-2013-Bi-directional_Inter-dependencies_of_Subjective_Expressions_and_Targets_and_their_Value_for_a_Joint_Model.html">67 acl-2013-Bi-directional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model</a></p>
<p>Author: Roman Klinger ; Philipp Cimiano</p><p>Abstract: Opinion mining is often regarded as a classification or segmentation task, involving the prediction of i) subjective expressions, ii) their target and iii) their polarity. Intuitively, these three variables are bidirectionally interdependent, but most work has either attempted to predict them in isolation or proposing pipeline-based approaches that cannot model the bidirectional interaction between these variables. Towards better understanding the interaction between these variables, we propose a model that allows for analyzing the relation of target and subjective phrases in both directions, thus providing an upper bound for the impact of a joint model in comparison to a pipeline model. We report results on two public datasets (cameras and cars), showing that our model outperforms state-ofthe-art models, as well as on a new dataset consisting of Twitter posts.</p><p>6 0.66947043 <a title="49-lsi-6" href="./acl-2013-Public_Dialogue%3A_Analysis_of_Tolerance_in_Online_Discussions.html">287 acl-2013-Public Dialogue: Analysis of Tolerance in Online Discussions</a></p>
<p>7 0.62680191 <a title="49-lsi-7" href="./acl-2013-Mining_Opinion_Words_and_Opinion_Targets_in_a_Two-Stage_Framework.html">244 acl-2013-Mining Opinion Words and Opinion Targets in a Two-Stage Framework</a></p>
<p>8 0.62167948 <a title="49-lsi-8" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>9 0.61988097 <a title="49-lsi-9" href="./acl-2013-Linguistic_Models_for_Analyzing_and_Detecting_Biased_Language.html">232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</a></p>
<p>10 0.61040878 <a title="49-lsi-10" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>11 0.5879724 <a title="49-lsi-11" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>12 0.57383412 <a title="49-lsi-12" href="./acl-2013-Patient_Experience_in_Online_Support_Forums%3A_Modeling_Interpersonal_Interactions_and_Medication_Use.html">278 acl-2013-Patient Experience in Online Support Forums: Modeling Interpersonal Interactions and Medication Use</a></p>
<p>13 0.57342553 <a title="49-lsi-13" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>14 0.54785144 <a title="49-lsi-14" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>15 0.53624356 <a title="49-lsi-15" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<p>16 0.52250981 <a title="49-lsi-16" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>17 0.50207907 <a title="49-lsi-17" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>18 0.49052706 <a title="49-lsi-18" href="./acl-2013-A_computational_approach_to_politeness_with_application_to_social_factors.html">30 acl-2013-A computational approach to politeness with application to social factors</a></p>
<p>19 0.48682669 <a title="49-lsi-19" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>20 0.48332822 <a title="49-lsi-20" href="./acl-2013-Detecting_Chronic_Critics_Based_on_Sentiment_Polarity_and_User%C3%A2%E2%80%A2%C5%BDs_Behavior_in_Social_Media.html">114 acl-2013-Detecting Chronic Critics Based on Sentiment Polarity and Userâ•Žs Behavior in Social Media</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.04), (4, 0.012), (6, 0.034), (11, 0.051), (15, 0.022), (24, 0.058), (26, 0.063), (35, 0.052), (42, 0.05), (48, 0.031), (63, 0.034), (70, 0.056), (71, 0.015), (75, 0.263), (88, 0.05), (90, 0.028), (95, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78698719 <a title="49-lda-1" href="./acl-2013-An_annotated_corpus_of_quoted_opinions_in_news_articles.html">49 acl-2013-An annotated corpus of quoted opinions in news articles</a></p>
<p>Author: Tim O'Keefe ; James R. Curran ; Peter Ashwell ; Irena Koprinska</p><p>Abstract: Quotes are used in news articles as evidence of a person’s opinion, and thus are a useful target for opinion mining. However, labelling each quote with a polarity score directed at a textually-anchored target can ignore the broader issue that the speaker is commenting on. We address this by instead labelling quotes as supporting or opposing a clear expression of a point of view on a topic, called a position statement. Using this we construct a corpus covering 7 topics with 2,228 quotes.</p><p>2 0.61723495 <a title="49-lda-2" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>Author: Joanne Boisson ; Ting-Hui Kao ; Jian-Cheng Wu ; Tzu-Hsi Yen ; Jason S. Chang</p><p>Abstract: In this paper, we introduce a Web-scale linguistics search engine, Linggle, that retrieves lexical bundles in response to a given query. The query might contain keywords, wildcards, wild parts of speech (PoS), synonyms, and additional regular expression (RE) operators. In our approach, we incorporate inverted file indexing, PoS information from BNC, and semantic indexing based on Latent Dirichlet Allocation with Google Web 1T. The method involves parsing the query to transforming it into several keyword retrieval commands. Word chunks are retrieved with counts, further filtering the chunks with the query as a RE, and finally displaying the results according to the counts, similarities, and topics. Clusters of synonyms or conceptually related words are also provided. In addition, Linggle provides example sentences from The New York Times on demand. The current implementation of Linggle is the most functionally comprehensive, and is in principle language and dataset independent. We plan to extend Linggle to provide fast and convenient access to a wealth of linguistic information embodied in Web scale datasets including Google Web 1T and Google Books Ngram for many major languages in the world. 1</p><p>3 0.57099938 <a title="49-lda-3" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>Author: Christian Hardmeier ; Sara Stymne ; Jorg Tiedemann ; Joakim Nivre</p><p>Abstract: We describe Docent, an open-source decoder for statistical machine translation that breaks with the usual sentence-bysentence paradigm and translates complete documents as units. By taking translation to the document level, our decoder can handle feature models with arbitrary discourse-wide dependencies and constitutes an essential infrastructure component in the quest for discourse-aware SMT models. 1 Motivation Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of 193 document-level models with unrestricted dependencies, so that a model score can be conditioned on arbitrary elements occurring anywhere in the input document or in the translation that is being generated. In this paper, we present an open-source implementation of this search algorithm. The decoder is written in C++ and follows an objectoriented design that makes it easy to extend it with new feature models, new search operations or different types of local search algorithms. The code is released under the GNU General Public License and published on Github1 to make it easy for other researchers to use it in their own experiments. 2 Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is performed by splitting the input sentence into a number of contiguous word sequences, called phrases, which are translated into the target lan- guage through a phrase dictionary lookup and optionally reordered. The choice between different translations of an ambiguous source phrase and the ordering of the target phrases are guided by a scoring function that combines a set of scores taken from the phrase table with scores from other models such as an n-gram language model. The actual translation process is realised as a search for the highest-scoring translation in the space of all the possible translations that could be generated given the models. The decoding approach that is implemented in Docent was first proposed by Hardmeier et al. (2012) and is based on local search. This means that it has a state corresponding to a complete, if possibly bad, translation of a document at every 1https : //github .com/chardmeier/docent/wiki Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t.he ?c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 193–198, stage of the search progress. Search proceeds by making small changes to the current search state in order to transform it gradually into a better translation. This differs from the DP algorithm used in other decoders, which starts with an empty translation and expands it bit by bit. It is similar to previous work on phrase-based SMT decoding by Langlais et al. (2007), but enables the creation of document-level models, which was not addressed by earlier approaches. Docent currently implements two search algorithms that are different generalisations of the hill climbing local search algorithm by Hardmeier et al. (2012). The original hill climbing algorithm starts with an initial state and generates possible successor states by randomly applying simple elementary operations to the state. After each operation, the new state is scored and accepted if its score is better than that of the previous state, else rejected. Search terminates when the decoder cannot find an acceptable successor state after a certain number of attempts, or when a maximum number of steps is reached. Simulated annealing is a stochastic variant of hill climbing that always accepts moves towards better states, but can also accept moves towards lower-scoring states with a certain probability that depends on a temperature parameter in order to escape local maxima. Local beam search generalises hill climbing in a different way by keeping a beam of a fixed number of multiple states at any time and randomly picking a state from the beam to modify at each move. The original hill climbing procedure can be recovered as a special case of either one of these search algorithms, by calling simulated annealing with a fixed temperature of 0 or local beam search with a beam size of 1. Initial states for the search process can be generated either by selecting a random segmentation with random translations from the phrase table in monotonic order, or by running DP beam search with sentence-local models as a first pass. For the second option, which generally yields better search results, Docent is linked with the Moses decoder and makes direct calls to the DP beam search algorithm implemented by Moses. In addition to these state initialisation procedures, Docent can save a search state to a disk file which can be loaded again in a subsequent decoding pass. This saves time especially when running repeated experiments from the same starting point obtained 194 by DP search. In order to explore the complete search space of phrase-based SMT, the search operations in a local search decoder must be able to change the phrase translations, the order of the output phrases and the segmentation of the source sentence into phrases. The three operations used by Hardmeier et al. (2012), change-phrase-translation, resegment and swap-phrases, jointly meet this requirement and are all implemented in Docent. Additionally, Docent features three extra operations, all of which affect the target word order: The movephrases operation moves a phrase to another location in the sentence. Unlike swap-phrases, it does not require that another phrase be moved in the opposite direction at the same time. A pair of operations called permute-phrases and linearisephrasescanreorderasequenceofphrasesintorandom order and back into the order corresponding to the source language. Since the search algorithm in Docent is stochastic, repeated runs of the decoder will gen- erally produce different output. However, the variance of the output is usually small, especially when initialising with a DP search pass, and it tends to be lower than the variance introduced by feature weight tuning (Hardmeier et al., 2012; Stymne et al., 2013a). 3 Available Feature Models In its current version, Docent implements a selection of sentence-local feature models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level models provided by Docent include the phrase table, n-gram language models implemented with the KenLM toolkit (Heafield, 2011), an unlexicalised distortion cost model with geometric decay (Koehn et al., 2003) and a word penalty cost. All of these features are designed to be compatible with the corresponding features in Moses. From among the typical set of baseline features in Moses, we have not implemented the lexicalised distortion model, but this model could easily be added if required. Docent uses the same binary file format for phrase tables as Moses, so the same training apparatus can be used. DP-based SMT decoders have a parameter called distortion limit that limits the difference in word order between the input and the MT output. In DP search, this is formally considered to be a parameter of the search algorithm because it affects the algorithmic complexity of the search by controlling how many translation options must be considered at each hypothesis expansion. The stochastic search algorithm in Docent does not require this limitation, but it can still be useful because the standard models of SMT do not model long-distance reordering well. Docent therefore includes a separate indicator feature to indicate a violated distortion limit. In conjunction with a very large weight, this feature can effectively ensure that the distortion limit is enforced. In contrast with the distortion limit parameter of a DP decoder, the weight ofour distortion limit feature can potentially be tuned to permit occasional distortion limit violations when they contribute to better translations. The document-level models included in Docent include a length parity model, a semantic language model as well as a collection of documentlevel readability models. The length parity model is a proof-of-concept model that ensures that all sentences in a document have either consistently odd or consistently even length. It serves mostly as a template to demonstrate how a simple documentlevel model can be implemented in the decoder. The semantic language model was originally proposed by Hardmeier et al. (2012) to improve lexical cohesion in a document. It is a cross-sentence model over sequences of content words that are scored based on their similarity in a word vector space. The readability models serve to improve the readability of the translation by encouraging the selection of easier and more consistent target words. They are described and demonstrated in more detail in section 5. Docent can read input files both in the NISTXML format commonly used to encode documents in MT shared tasks such as NIST or WMT and in the more elaborate MMAX format (Müller and Strube, 2003). The MMAX format makes it possible to include a wide range of discourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using different optimisation algorithms such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), and selective tuning of a subset of features. Since document features only give meaningful scores on the document level and not on the sentence level, we naturally perform optimisation on document level, which typically means that we need more data than for the optimisation of sentence-based decoding. The results we obtain are relatively stable and competitive with sentence-level optimisation of the same models (Stymne et al., 2013a). 4 Implementing Feature Models Efficiently While translating a document, the local search decoder attempts to make a great number of moves. For each move, a score must be computed and tested against the acceptance criterion. An overwhelming majority of the proposed moves will be rejected. In order to achieve reasonably fast decoding times, efficient scoring is paramount. Recomputing the scores of the whole document at every step would be far too slow for the decoder to be useful. Fortunately, score computation can be sped up in two ways. Knowledge about how the state to be scored was generated from its predecessor helps to limit recomputations to a minimum, and by adopting a two-step scoring procedure that just computes the scores that can be calculated with little effort at first, we need to compute the complete score only if the new state has some chance of being accepted. The scores of SMT feature models can usually be decomposed in some way over parts of the document. The traditional models borrowed from sentence-based decoding are necessarily decomposable at the sentence level, and in practice, all common models are designed to meet the constraints of DP beam search, which ensures that they can in fact be decomposed over even smaller sequences of just a few words. For genuine document-level features, this is not the case, but even these models can often be decomposed in some way, for instance over paragraphs, anaphoric links or lexical chains. To take advantage of this fact, feature models in Docent always have access to the previous state and its score and to a list of the state modifications that transform the previous state into the next. The scores of the new state are calculated by identifying the parts of a document that are affected by the modifications, subtracting the old scores of this part from the previous score and adding the new scores. This approach to scoring makes feature model implementation a bit more complicated than in DP search, but it gives the feature models full control over how they decompose a document while still permitting efficient decoding. A feature model class in Docent implements three methods. The initDocument method is called once per document when decoding starts. It straightforwardly computes the model score for the entire document from scratch. When a state is modified, the decoder first invokes the estimateScoreUpdate method. Rather than calculating the new score exactly, this method is only required to return an upper bound that reflects the maximum score that could possibly be achieved by this state. The search algorithm then checks this upper bound against the acceptance criterion. Only if the upper bound meets the criterion does it call the updateScore method to calculate the exact score, which is then checked against the acceptance criterion again. The motivation for this two-step procedure is that some models can compute an upper bound approximation much more efficiently than an exact score. For any model whose score is a log probability, a value of 0 is a loose upper bound that can be returned instantly, but in many cases, we can do much better. In the case of the n-gram language model, for instance, a more accurate upper bound can be computed cheaply by subtracting from the old score all log-probabilities of n-grams that are affected by the state modifications without adding the scores of the n-grams replacing them in the new state. This approximation can be calculated without doing any language model lookups at all. On the other hand, some models like the distortion cost or the word penalty are very cheap to compute, so that the estimateScoreUpdate method 196 can simply return the precise score as a tight up- per bound. If a state gets rejected because of a low score on one of the cheap models, this means we will never have to compute the more expensive feature scores at all. 5 Readability: A Case Study As a case study we report initial results on how document-wide features can be used in Docent in order to improve the readability oftexts by encouraging simple and consistent terminology (Stymne et al., 2013b). This work is a first step towards achieving joint SMT and text simplification, with the final goal of adapting MT to user groups such as people with reading disabilities. Lexical consistency modelling for SMT has been attempted before. The suggested approaches have been limited by the use of sentence-level decoders, however, and had to resort to procedures like post processing (Carpuat, 2009), multiple decoding runs with frozen counts from previous runs (Ture et al., 2012), or cache-based models (Tiedemann, 2010). In Docent, however, we al- ways have access to a full document translation, which makes it straightforward to include features directly into the decoder. We implemented four features on the document level. The first two features are type token ratio (TTR) and a reformulation of it, OVIX, which is less sensitive to text length. These ratios have been related to the “idea density” of a text (Mühlenbock and Kokkinakis, 2009). We also wanted to encourage consistent translations of words, for which we used the Q-value (Deléger et al., 2006), which has been proposed to measure term quality. We applied it on word level (QW) and phrase level (QP). These features need access to the full target document, which we have in Docent. In addition, we included two sentence-level count features for long words that have been used to measure the readability of Swedish texts (Mühlenbock and Kokkinakis, 2009). We tested our features on English–Swedish translation using the Europarl corpus. For training we used 1,488,322 sentences. As test data, we extracted 20 documents with a total of 690 sen- tences. We used the standard set of baseline features: 5-gram language model, translation model with 5 weights, a word penalty and a distortion penalty. BaselineReadability featuresComment de ärade ledamöterna (the honourableledamöterna (the members) / ni+ Removal of non-essential words Members) (you) på ett sådant sätt att (in such a way så att (so that) + Simplified expression that) gemenskapslagstiftningen (the gemenskapens lagstiftning (the + Shorter community legislation) community’s compound to genitive construction Världshandelsorganisationen (World WTO (WTO) legislation) − Changing Trade Organisation) long compound to E−nCg hliasnhg-biansged lo handlingsplanen (the action plan) ägnat särskild uppmärksamhet particular attention to) words by changing long åt (paid planen (the plan) särskilt uppmärksam − Removal på (particular attentive on) anbgb creomvipatoiounn of important word −− RBaedm grammar bpeocratuasnet wofo rcdhanged p−ar Bt aodf gspraeemcmh aarn dbe mcaisussieng o fv cehrban Table 2: Example translation snippets with comments FeatureBLEUOVIXLIX Baseline0.24356.8851.17 TTR 0.243 55.25 51.04 OVIX 0.243 54.65 51.00 QW 0.242 57.16 51.16 QP 0.243 57.07 51.06 All 0.235 47.80 49.29 Table 1: Results for adding single lexical consistency features to Docent To evaluate our system we used the BLEU score (Papineni et al., 2002) together with a set of readability metrics, since readability is what we hoped to improve by adding consistency features. Here we used OVIX to confirm a direct impact on con- sistency, and LIX (Björnsson, 1968), which is a common readability measure for Swedish. Unfortunately we do not have access to simplified translated text, so we calculate the MT metrics against a standard reference, which means that simple texts will likely have worse scores than complicated texts closer to the reference translation. We tuned the standard features using Moses and MERT, and then added each lexical consistency feature with a small weight, using a grid search approach to find values with a small impact. The results are shown in Table 1. As can be seen, for individual features the translation quality was maintained, with small improvements in LIX, and in OVIX for the TTR and OVIX features. For the combination we lost a little bit on translation quality, but there was a larger effect on the readability metrics. When we used larger weights, there was a bigger impact on the readability metrics, with a further decrease on MT quality. We also investigated what types of changes the readability features could lead to. Table 2 shows a sample of translations where the baseline is compared to systems with readability features. There are both cases where the readability features help 197 and cases where they are problematic. Overall, these examples show that our simple features can help achieve some interesting simplifications. There is still much work to do on how to take best advantage of the possibilities in Docent in order to achieve readable texts. This attempt shows the feasibility of the approach. We plan to extend this work for instance by better feature optimisation, by integrating part-of-speech tags into our features in order to focus on terms rather than common words, and by using simplified texts for evaluation and tuning. 6 Conclusions In this paper, we have presented Docent, an opensource document-level decoder for phrase-based SMT released under the GNU General Public License. Docent is the first decoder that permits the inclusion of feature models with unrestricted dependencies between arbitrary parts of the output, even crossing sentence boundaries. A number of research groups have recently started to investigate the interplay between SMT and discourse-level phenomena such as pronominal anaphora, verb tense selection and the generation of discourse connectives. We expect that the availability of a document-level decoder will make it substantially easier to leverage discourse information in SMT and make SMT models explore new ground beyond the next sentence boundary. References Carl-Hugo Björnsson. 1968. Läsbarhet. Liber, Stockholm. Marine Carpuat. 2009. One translation per discourse. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 19–27, Boulder, Colorado. Louise Deléger, Magnus Merkel, and Pierre Zweigenbaum. 2006. Enriching medical terminologies: an approach based on aligned corpora. In International Congress of the European Federation for Medical Informatics, pages 747–752, Maastricht, The Netherlands. Zhengxian Gong, Min Zhang, Chew Lim Tan, and Guodong Zhou. 2012. N-gram-based tense models for statistical machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 276–285, Jeju Island, Korea. Liane Guillou. 2012. Improving pronoun translation for statistical machine translation. In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1–10, Avignon, France. Christian Hardmeier and Marcello Federico. 2010. Modelling pronominal anaphora in statistical machine translation. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 283–289, Paris, France. Christian Hardmeier, Joakim Nivre, and Jörg Tiedemann. 2012. Document-wide decoding for phrase-based statistical machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1179–1 190, Jeju Island, Korea. Christian Hardmeier. 2012. Discourse in statistical machine translation: A survey and a case study. Discours, 11. Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland. Mark Hopkins and Jonathan ranking. In Proceedings on Empirical Methods in cessing, pages 1352–1362, May. 2011. Tuning as of the 2011 Conference Natural Language ProEdinburgh, Scotland. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 conference of the North American chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Edmonton. Philipp Koehn, Hieu Hoang, Alexandra Birch, et al. 2007. Moses: open source toolkit for Statistical Machine Translation. In Annual meeting of the Associationfor Computational Linguistics: Demonstration session, pages 177–180, Prague, Czech Republic. Philippe Langlais, Alexandre Patry, and Fabrizio Gotti. 2007. A greedy decoder for phrase-based statistical machine translation. In TMI-2007: Proceedings 198 of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 104–1 13, Skövde, Sweden. Ronan Le Nagard and Philipp Koehn. 2010. Aiding pronoun translation with co-reference resolution. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 252–261, Uppsala, Sweden. Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui, and Andrea Gesmundo. 2012. Machine translation of labeled discourse connectives. In Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas (AMTA), San Diego, California, USA. Katarina Mühlenbock and Sofie Johansson Kokkinakis. 2009. LIX 68 revisited an extended readability. In Proceedings of the Corpus Linguistics Conference, Liverpool, UK. – Christoph Müller and Michael Strube. 2003. Multilevel annotation in MMAX. In Proceedings of the Fourth SIGdial Workshop on Discourse and Dialogue, pages 198–207, Sapporo, Japan. Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting ofthe Associationfor Computational Linguistics, pages 3 11–3 18, Philadelphia, Pennsylvania, USA. Sara Stymne, Christian Hardmeier, Jörg Tiedemann, and Joakim Nivre. 2013a. Feature weight optimization for discourse-level SMT. In Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), Sofia, Bulgaria. Sara Stymne, Jörg Tiedemann, Christian Hardmeier, and Joakim Nivre. 2013b. Statistical machine translation with readability constraints. In Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013), pages 375–386, Oslo, Norway. Jörg Tiedemann. 2010. Context adaptation in statistical machine translation using models with exponentially decaying cache. In Proceedings of the ACL 2010 Workshop on Domain Adaptation for Natural Language Processing (DANLP), pages 8–15, Uppsala, Sweden. Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012. Encouraging consistent translation choices. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 417–426, Montréal, Canada.</p><p>4 0.51871157 <a title="49-lda-4" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>5 0.5153228 <a title="49-lda-5" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>Author: Matt Post ; Shane Bergsma</p><p>Abstract: Syntactic features are useful for many text classification tasks. Among these, tree kernels (Collins and Duffy, 2001) have been perhaps the most robust and effective syntactic tool, appealing for their empirical success, but also because they do not require an answer to the difficult question of which tree features to use for a given task. We compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly avail- able tools) , we suggest they should always be included as baseline comparisons in tree kernel method evaluations.</p><p>6 0.51129127 <a title="49-lda-6" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>7 0.50913405 <a title="49-lda-7" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>8 0.50849992 <a title="49-lda-8" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>9 0.50782472 <a title="49-lda-9" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>10 0.50781095 <a title="49-lda-10" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>11 0.50756288 <a title="49-lda-11" href="./acl-2013-Evaluating_Text_Segmentation_using_Boundary_Edit_Distance.html">140 acl-2013-Evaluating Text Segmentation using Boundary Edit Distance</a></p>
<p>12 0.50732011 <a title="49-lda-12" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>13 0.50727361 <a title="49-lda-13" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>14 0.5072214 <a title="49-lda-14" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>15 0.50629169 <a title="49-lda-15" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>16 0.50626791 <a title="49-lda-16" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>17 0.50581962 <a title="49-lda-17" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>18 0.50547844 <a title="49-lda-18" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>19 0.50498819 <a title="49-lda-19" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>20 0.50449604 <a title="49-lda-20" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
