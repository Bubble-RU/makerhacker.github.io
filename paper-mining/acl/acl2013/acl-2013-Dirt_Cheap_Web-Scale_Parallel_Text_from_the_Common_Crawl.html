<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-120" href="#">acl2013-120</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</h1>
<br/><p>Source: <a title="acl-2013-120-pdf" href="http://aclweb.org/anthology//P/P13/P13-1135.pdf">pdf</a></p><p>Author: Jason R. Smith ; Herve Saint-Amand ; Magdalena Plamada ; Philipp Koehn ; Chris Callison-Burch ; Adam Lopez</p><p>Abstract: Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1</p><p>Reference: <a title="acl-2013-120-reference" href="../acl2013_reference/acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. [sent-15, score-0.426]
</p><p>2 We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. [sent-16, score-0.842]
</p><p>3 Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. [sent-17, score-0.568]
</p><p>4 Our large-scale experiment uncovers large amounts of parallel text in  dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. [sent-18, score-0.579]
</p><p>5 Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. [sent-19, score-0.29]
</p><p>6 1 1 Introduction A key bottleneck translation (SMT) and domains is the lel corpora beyond of language pairs,  in porting statistical machine technology to new languages lack of readily available paralcurated datasets. [sent-21, score-0.259]
</p><p>7 For a handful large amounts of parallel data  ∗ This research was conducted while Chris CallisonBurch was at Johns Hopkins University. [sent-22, score-0.328]
</p><p>8 However, for most language pairs and domains there is little to no curated parallel data available. [sent-26, score-0.544]
</p><p>9 Hence discovery of parallel data is an important first step for translation between most of the world’s languages. [sent-27, score-0.431]
</p><p>10 Many websites are available in multiple languages, and unlike other potential sources— such as multilingual news feeds (Munteanu and Marcu, 2005) or Wikipedia (Smith et al. [sent-29, score-0.186]
</p><p>11 , 2010)— it is common to find document pairs that are direct translations of one another. [sent-30, score-0.264]
</p><p>12 However, anything more sophisticated generally requires direct access to web-crawled documents themselves along with the computing power to process them. [sent-34, score-0.107]
</p><p>13 As a consequence, web-mined parallel text has become the exclusive purview of large companies with the computational resources to crawl, store, and process the entire Web. [sent-36, score-0.398]
</p><p>14 To put web-mined parallel text back in the hands of individual researchers, we mine parallel text from the Common Crawl, a regularly updated 81-terabyte snapshot of the public internet hosted 1374  Proce dingsS o f ita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-37, score-0.924]
</p><p>15 2 Using the Common Crawl completely removes the bottleneck of web crawling, and makes it possible to run algorithms on a substantial portion of the web at very low cost. [sent-40, score-0.16]
</p><p>16 Starting from nothing other than a set of language codes, our extension of the STRAND algorithm (Resnik and Smith,  2003) identifies potentially parallel documents using cues from URLs and document content (§2). [sent-41, score-0.489]
</p><p>17 f the web-mined data, demonstrating coverage in a wide variety of languages and domains (§3). [sent-43, score-0.156]
</p><p>18 improves translation performance on strong baseline news translation systems in five different language pairs (§4). [sent-45, score-0.343]
</p><p>19 In our pipeline, we perform the first step of identifying candidate document pairs using Amazon EMR, download the resulting document pairs, and perform the remaining steps on our local cluster. [sent-52, score-0.32]
</p><p>20 Candidate pair selection: Retrieve candidate document pairs from the CommonCrawl corpus. [sent-55, score-0.221]
</p><p>21 (b) Align the linearized HTML of candidate document pairs. [sent-61, score-0.149]
</p><p>22 Sentence Alignment: For each aligned pair of text chunks, perform the sentence alignment method of Gale and Church (1993). [sent-66, score-0.155]
</p><p>23 Candidate Pair Selection We adopt a strategy similar to that ofResnik and Smith (2003) for finding candidate parallel documents, adapted to the parallel architecture of Map-Reduce. [sent-69, score-0.706]
</p><p>24 The mapper operates on each website entry in the CommonCrawl data. [sent-70, score-0.104]
</p><p>25 entry)  The reducer then receives all websites mapped to the same “language independent” URL. [sent-85, score-0.193]
</p><p>26 If two or more websites are associated with the same key, the reducer will output all associated values, as long as they are not in the same language, as determined by the language identifier in the URL. [sent-86, score-0.193]
</p><p>27 This URL-based matching is a simple and inexpensive solution to the problem of finding candidate document pairs. [sent-87, score-0.194]
</p><p>28 The mapper will discard 1375  most, and neither the mapper nor the reducer do anything with the HTML of the documents aside from reading and writing them. [sent-88, score-0.342]
</p><p>29 This alignment is used to determine which document pairs are actually parallel, and if they are, to align pairs of text blocks within the documents. [sent-91, score-0.408]
</p><p>30 The objective of the alignment is to maximize the number of matching items. [sent-96, score-0.128]
</p><p>31 They annotated a set of document pairs as parallel or non-parallel, and trained a classifier on this data. [sent-98, score-0.499]
</p><p>32 We also annotated 101 Spanish-English document pairs in this way and trained a maximum entropy classifier. [sent-99, score-0.171]
</p><p>33 The strong performance of the naive baseline was likely due to the unbalanced nature of the annotated data— 80% of the document pairs that we annotated were parallel. [sent-102, score-0.216]
</p><p>34 Segmentation  The text chunks from the previ-  ous step may contain several sentences, so before the sentence alignment step we must perform sentence segmentation. [sent-103, score-0.238]
</p><p>35 We use the Punkt sentence splitter from NLTK (Loper and Bird, 2002) to perform both sentence and word segmentation on each text chunk. [sent-104, score-0.109]
</p><p>36 Sentence Alignment For each aligned text chunk pair, we perform sentence alignment using the algorithm of Gale and Church (1993). [sent-105, score-0.189]
</p><p>37 Sentence Filtering Since we do not perform any boilerplate removal in earlier steps, there are many sentence pairs produced by the pipeline which contain menu items or other bits of text which are not useful to an SMT system. [sent-106, score-0.31]
</p><p>38 We avoid performing any complex boilerplate removal and only remove segment pairs where either the source and target text are identical, or where the source or target segments appear more than once in the extracted corpus. [sent-107, score-0.235]
</p><p>39 This is different from running a single Map-Reduce job over the entire dataset, since websites in different subsets of the data cannot be matched. [sent-112, score-0.121]
</p><p>40 However, since the data is stored as it is crawled, it is likely that matching websites will be found in the same split of the data. [sent-113, score-0.211]
</p><p>41 Table 1 shows the amount of raw parallel data obtained for a large selection oflanguage pairs. [sent-114, score-0.328]
</p><p>42 As far as we know, ours is the first system built to mine parallel text from the Common Crawl. [sent-115, score-0.422]
</p><p>43 Since our mining heuristics are  very simple, these results can be construed as a lower bound on what is actually possible. [sent-118, score-0.109]
</p><p>44 1 Recall Estimates Our first question is about recall: of all the possible parallel text that is actually available on the Web, how much does our algorithm actually find in the Common Crawl? [sent-120, score-0.457]
</p><p>45 Although this question is difficult to answer precisely, we can estimate an answer by comparing our mined URLs against a large collection of previously mined URLs that were found using targeted techniques: those in the French-English Gigaword corpus (Callison-Burch et al. [sent-121, score-0.404]
</p><p>46 h0tKo  SToaurrgceet T Tookkeennss553773KK445797KK335386KK331285KK320975KK221088KK Table 1: The amount of parallel data mined from CommonCrawl for each language paired with English. [sent-147, score-0.53]
</p><p>47 4 If we had included “f” and “e” as identifiers for French and English respectively, coverage of the URL pairs would increase to 74%. [sent-151, score-0.149]
</p><p>48 2  Precision Estimates  Since our algorithms rely on cues that are mostly external to the contents of the extracted data and have no knowledge of actual languages, we wanted to evaluate the precision of our algorithm: how much of the mined data actually consists of parallel sentences? [sent-154, score-0.577]
</p><p>49 To measure this, we conducted a manual analysis of 200 randomly selected sentence pairs for each of three language pairs. [sent-155, score-0.109]
</p><p>50 Furthermore, 22% of the true positives are potentially machine translations (judging by the quality), whereas in 13% of the cases one of the sentences contains additional content not ex4The difference is likely due to the coverage of the CommonCrawl corpus. [sent-158, score-0.183]
</p><p>51 5% of them have either the source or target sentence in the wrong language, and the remaining ones representing failures in the alignment process. [sent-161, score-0.12]
</p><p>52 LGSaFpnerag nmuciashgnePr87c281i% sion  Table 2: Manual evaluation of precision (by sentence pair) on the extracted parallel data for Spanish, French, and German (paired with English). [sent-165, score-0.365]
</p><p>53 In addition to the manual evaluation of precision, we applied language identification to our extracted parallel data for several additional languages. [sent-166, score-0.37]
</p><p>54 py” tool (Lui and Baldwin, 2012) at the segment level, and report the percentage of sentence pairs where both sentences were recognized as the correct language. [sent-168, score-0.109]
</p><p>55 Comparing against our manual evaluation from Table 2, it appears that many sentence pairs are being incorrectly judged as nonparallel. [sent-170, score-0.109]
</p><p>56 A major difficulty in applying SMT even on languages for  which we have significant quantities of parallel text is that most of that parallel text is in the news and government domains. [sent-175, score-0.867]
</p><p>57 In LDA a topic is a unigram distribution over words, and each document is modeled as a distribution over topics. [sent-183, score-0.142]
</p><p>58 To create a set of documents from the extracted CommonCrawl data, we took the English side of the extracted parallel segments for each URL in the Spanish-English portion of the data. [sent-184, score-0.39]
</p><p>59 Some of the topics that LDA finds correspond closely with specific domains, such as topics 1 (bl ingee . [sent-187, score-0.138]
</p><p>60 Several of the topics correspond to the travel domain. [sent-190, score-0.104]
</p><p>61 We created a set of documents from both CommonCrawl and Europarl, and again used MALLET to generate 100 topics for this data. [sent-195, score-0.131]
</p><p>62 6 We then labeled each document by its most likely topic (as determined by that topic’s mixture weights), and counted the number of documents from Europarl and CommonCrawl for which each topic was most prominent. [sent-196, score-0.292]
</p><p>63 In addition to exploring topics in the datasets, we also performed additional intrinsic evaluation at the domain level, choosing top domains for three language pairs. [sent-199, score-0.198]
</p><p>64 We specifically classified sentence pairs as useful or boilerplate (Table 7). [sent-200, score-0.2]
</p><p>65 Among our observations, we find that commercial websites tend to contain less boilerplate material than encyclopedic websites, and that the ratios tend to be similar across languages in the same domain. [sent-201, score-0.253]
</p><p>66 In these experiments, a baseline system is trained on an existing parallel corpus, and the experimental system is trained on the baseline corpus plus the mined parallel data. [sent-209, score-0.858]
</p><p>67 In all experiments we include the target side of the mined parallel data in the language model, in order to distinguish whether results are due to influences from parallel or monolingual data. [sent-210, score-0.858]
</p><p>68 6Documents were created from Europarl by taking “SPEAKER” tags as document boundaries, giving us 208,431 documents total. [sent-212, score-0.199]
</p><p>69 are  ranked by  Table 5: A list of 20 topics generated using the MALLET toolkit (McCallum, 2002) and their most likely  tokens. [sent-225, score-0.114]
</p><p>70 , 2012) using all available parallel and monolingual data for that task, aside from the French-English Gigaword. [sent-228, score-0.328]
</p><p>71 These results show that even on top of a different, larger parallel corpus mined from the web, adding CommonCrawl data still yields an improvement. [sent-235, score-0.53]
</p><p>72 2  Open Domain Translation  A substantial appeal of web-mined parallel data is that it might be suitable to translation of domains other than news, and our topic modeling analysis (§3. [sent-237, score-0.594]
</p><p>73 1379  Table 6: A sample of topics along with the number of Europarl and CommonCrawl documents where they are the most likely topic in the mixture. [sent-241, score-0.219]
</p><p>74 1376E0412  Table 8: BLEU scores for several language pairs before and after adding the mined parallel data to  systems  trained  on  data from WMT data. [sent-248, score-0.602]
</p><p>75 756R 09  Table 9: BLEU scores for French-English and English-French before and after adding the mined parallel data to systems trained on data from WMT data including the French-English Gigaword (Callison-Burch et al. [sent-251, score-0.53]
</p><p>76 For these experiments, we also include training data mined from Wikipedia using a simplified version of the sentence aligner described by Smith et al. [sent-253, score-0.239]
</p><p>77 Wikipedia copied across the public internet, and we did not have a simple way to filter such data from our mined datasets. [sent-268, score-0.245]
</p><p>78 org was discovered by our URL matching heuristics, but we excluded any sentence pairs that were found in the CommonCrawl data from this test set. [sent-272, score-0.154]
</p><p>79 1380  The second dataset is a set of crowdsourced translation of Spanish speech transcriptions from  the Spanish Fisher corpus. [sent-273, score-0.103]
</p><p>80 The advantage of this data for our open domain translation test is twofold. [sent-276, score-0.153]
</p><p>81 Eu+r oBW poaitekrhbli98 629/ 78W7248/ M5 46T82/ 2 14709 9 46/ 87T5890a/ 546to850e/ 21b68a1798 671/ 867953F/ 345is9421h/ 12e350r9 Table 11: n-gram coverage percentages (up to 4-  grams) of the source side of our test sets given our different parallel training corpora computed at the type level. [sent-279, score-0.364]
</p><p>82 321e432r  Table 12: BLEU scores for Spanish-English before and after adding the mined parallel data to a baseline Europarl system. [sent-283, score-0.53]
</p><p>83 5  on Tatoeba and Fisher (almost 5  Discussion  Web-mined parallel texts have been an exclusive resource of large companies for several years. [sent-291, score-0.363]
</p><p>84 However, when web-mined parallel text is available to everyone at little or no cost, there will be much greater potential for groundbreaking research to come from all corners. [sent-292, score-0.363]
</p><p>85 As we have shown, it is possible to obtain parallel text for many language pairs in a variety of domains very cheaply and quickly, and in sufficient quantity and quality to improve statistical machine translation systems. [sent-294, score-0.617]
</p><p>86 There are many possible means through which the system could be improved, including more sophisticated techniques for identifying matching URLs, better alignment, better language identification, better filtering of data, and better exploitation of resulting cross-domain datasets. [sent-298, score-0.148]
</p><p>87 (201 1) gathered almost 1trillion tokens of French-English parallel data this way. [sent-302, score-0.328]
</p><p>88 Another strategy for mining parallel webpage pairs is to scan the HTML for links to the same page in another language (Nie et al. [sent-303, score-0.462]
</p><p>89 (2010), for example, translated all non-English webpages into English using an existing translation system and used near-duplicate detection methods to find candidate parallel document pairs. [sent-307, score-0.58]
</p><p>90 Ture and Lin (2012) had a similar approach for finding parallel Wikipedia documents by using near-duplicate detection, though they did not need to apply a full translation system to all non-English documents. [sent-308, score-0.493]
</p><p>91 1381  Instead, they represented documents in bag-ofwords vector space, and projected non-English document vectors into the English vector space using the translation probabilities of a word alignment model. [sent-309, score-0.347]
</p><p>92 However, with this system in place, we could obtain enough parallel data to bootstrap these more sophisticated approaches. [sent-311, score-0.408]
</p><p>93 (2010) mine parallel sentences from comparable documents in Wikipedia, demonstrating substantial gains on open domain translation. [sent-314, score-0.499]
</p><p>94 However, their approach required seed parallel data to learn models used in a classifier. [sent-315, score-0.328]
</p><p>95 We imagine a two-step process, first obtaining parallel data from the web, followed by comparable data from sources such as Wikipedia using models bootstrapped from the web-mined data. [sent-316, score-0.328]
</p><p>96 Such a process could be used to build translation systems for new language pairs in a very short period of time, hence fulfilling one of the original promises of SMT. [sent-317, score-0.175]
</p><p>97 Europarl: A parallel corpus for statis-  tical machine translation. [sent-377, score-0.328]
</p><p>98 Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the web. [sent-415, score-0.718]
</p><p>99 News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. [sent-440, score-0.328]
</p><p>100 mining large corpora for parallel sentences to improve translation modeling. [sent-450, score-0.493]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('commoncrawl', 0.384), ('parallel', 0.328), ('crawl', 0.278), ('mined', 0.202), ('wmt', 0.167), ('url', 0.167), ('europarl', 0.139), ('websites', 0.121), ('tatoeba', 0.118), ('mapper', 0.104), ('translation', 0.103), ('amazon', 0.1), ('document', 0.099), ('elastic', 0.096), ('hosted', 0.096), ('boilerplate', 0.091), ('smith', 0.086), ('strand', 0.086), ('fisher', 0.086), ('alignment', 0.083), ('johns', 0.082), ('domains', 0.079), ('urls', 0.079), ('emr', 0.078), ('resnik', 0.075), ('nie', 0.075), ('reducer', 0.072), ('pairs', 0.072), ('html', 0.07), ('topics', 0.069), ('chris', 0.069), ('munteanu', 0.068), ('hopkins', 0.066), ('curated', 0.065), ('news', 0.065), ('mallet', 0.064), ('smt', 0.063), ('bleu', 0.063), ('mining', 0.062), ('web', 0.062), ('documents', 0.062), ('codes', 0.061), ('wikipedia', 0.061), ('herve', 0.059), ('mine', 0.059), ('filtering', 0.058), ('koehn', 0.058), ('translations', 0.057), ('uszkoreit', 0.053), ('omar', 0.053), ('terabytes', 0.052), ('domain', 0.05), ('candidate', 0.05), ('monz', 0.049), ('gale', 0.048), ('germanenglish', 0.048), ('actually', 0.047), ('chunks', 0.046), ('christof', 0.046), ('matching', 0.045), ('sophisticated', 0.045), ('positives', 0.045), ('likely', 0.045), ('loper', 0.043), ('topic', 0.043), ('philipp', 0.043), ('public', 0.043), ('lda', 0.043), ('identification', 0.042), ('spanish', 0.042), ('appeal', 0.041), ('identifiers', 0.041), ('languages', 0.041), ('cloud', 0.04), ('mapreduce', 0.04), ('genres', 0.04), ('french', 0.039), ('venugopal', 0.038), ('ite', 0.038), ('lui', 0.038), ('dean', 0.038), ('zaidan', 0.038), ('pipeline', 0.038), ('tags', 0.038), ('sentence', 0.037), ('removal', 0.037), ('nltk', 0.036), ('excellence', 0.036), ('crawling', 0.036), ('bottleneck', 0.036), ('common', 0.036), ('coverage', 0.036), ('text', 0.035), ('companies', 0.035), ('travel', 0.035), ('jakob', 0.035), ('www', 0.035), ('bootstrap', 0.035), ('government', 0.035), ('chunk', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="120-tfidf-1" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>Author: Jason R. Smith ; Herve Saint-Amand ; Magdalena Plamada ; Philipp Koehn ; Chris Callison-Burch ; Adam Lopez</p><p>Abstract: Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1</p><p>2 0.26017588 <a title="120-tfidf-2" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>Author: Wang Ling ; Guang Xiang ; Chris Dyer ; Alan Black ; Isabel Trancoso</p><p>Abstract: In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring parallel text: some users create post multilingual messages targeting international audiences while others “retweet” translations. We present an efficient method for detecting these messages and extracting parallel segments from them. We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counterpart of Twitter) using only their public APIs. As a supplement to existing parallel training data, our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary. The resources in described in this paper are available at http://www.cs.cmu.edu/∼lingwang/utopia.</p><p>3 0.16684222 <a title="120-tfidf-3" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>Author: Felix Hieber ; Laura Jehl ; Stefan Riezler</p><p>Abstract: We present an approach to mine comparable data for parallel sentences using translation-based cross-lingual information retrieval (CLIR). By iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data. Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before. Our setup is time- and memory-efficient and of similar quality as CLIR-based adaptation on millions of parallel sentences.</p><p>4 0.14535636 <a title="120-tfidf-4" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>5 0.14265861 <a title="120-tfidf-5" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>6 0.13985448 <a title="120-tfidf-6" href="./acl-2013-Crawling_microblogging_services_to_gather_language-classified_URLs._Workflow_and_case_study.html">95 acl-2013-Crawling microblogging services to gather language-classified URLs. Workflow and case study</a></p>
<p>7 0.13721265 <a title="120-tfidf-7" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>8 0.13561811 <a title="120-tfidf-8" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>9 0.12993436 <a title="120-tfidf-9" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>10 0.12354423 <a title="120-tfidf-10" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>11 0.11949573 <a title="120-tfidf-11" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>12 0.11768735 <a title="120-tfidf-12" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>13 0.11425064 <a title="120-tfidf-13" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>14 0.11369652 <a title="120-tfidf-14" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>15 0.11031114 <a title="120-tfidf-15" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>16 0.11017385 <a title="120-tfidf-16" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>17 0.11000935 <a title="120-tfidf-17" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>18 0.10905857 <a title="120-tfidf-18" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>19 0.10584345 <a title="120-tfidf-19" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>20 0.10284972 <a title="120-tfidf-20" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.287), (1, -0.05), (2, 0.193), (3, 0.061), (4, 0.088), (5, -0.02), (6, 0.007), (7, -0.008), (8, 0.033), (9, -0.084), (10, 0.015), (11, 0.023), (12, 0.01), (13, 0.139), (14, -0.003), (15, -0.047), (16, -0.024), (17, -0.019), (18, -0.023), (19, -0.018), (20, -0.015), (21, -0.015), (22, -0.043), (23, -0.008), (24, -0.013), (25, 0.011), (26, -0.061), (27, 0.016), (28, 0.051), (29, 0.021), (30, -0.016), (31, -0.013), (32, -0.053), (33, 0.004), (34, 0.047), (35, -0.026), (36, 0.031), (37, 0.005), (38, 0.04), (39, 0.007), (40, -0.01), (41, -0.012), (42, -0.045), (43, 0.032), (44, 0.021), (45, 0.039), (46, 0.057), (47, -0.084), (48, 0.013), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96774715 <a title="120-lsi-1" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>Author: Jason R. Smith ; Herve Saint-Amand ; Magdalena Plamada ; Philipp Koehn ; Chris Callison-Burch ; Adam Lopez</p><p>Abstract: Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1</p><p>2 0.88494784 <a title="120-lsi-2" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>Author: Felix Hieber ; Laura Jehl ; Stefan Riezler</p><p>Abstract: We present an approach to mine comparable data for parallel sentences using translation-based cross-lingual information retrieval (CLIR). By iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data. Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before. Our setup is time- and memory-efficient and of similar quality as CLIR-based adaptation on millions of parallel sentences.</p><p>3 0.86675477 <a title="120-lsi-3" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>Author: Wang Ling ; Guang Xiang ; Chris Dyer ; Alan Black ; Isabel Trancoso</p><p>Abstract: In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring parallel text: some users create post multilingual messages targeting international audiences while others “retweet” translations. We present an efficient method for detecting these messages and extracting parallel segments from them. We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counterpart of Twitter) using only their public APIs. As a supplement to existing parallel training data, our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary. The resources in described in this paper are available at http://www.cs.cmu.edu/∼lingwang/utopia.</p><p>4 0.777592 <a title="120-lsi-4" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>Author: Lei Cui ; Dongdong Zhang ; Shujie Liu ; Mu Li ; Ming Zhou</p><p>Abstract: The quality of bilingual data is a key factor in Statistical Machine Translation (SMT). Low-quality bilingual data tends to produce incorrect translation knowledge and also degrades translation modeling performance. Previous work often used supervised learning methods to filter lowquality data, but a fair amount of human labeled examples are needed which are not easy to obtain. To reduce the reliance on labeled examples, we propose an unsupervised method to clean bilingual data. The method leverages the mutual reinforcement between the sentence pairs and the extracted phrase pairs, based on the observation that better sentence pairs often lead to better phrase extraction and vice versa. End-to-end experiments show that the proposed method substantially improves the performance in largescale Chinese-to-English translation tasks.</p><p>5 0.75337023 <a title="120-lsi-5" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>Author: Sanjika Hewavitharana ; Dennis Mehay ; Sankaranarayanan Ananthakrishnan ; Prem Natarajan</p><p>Abstract: We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations. Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. A significant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available. Thus, our approach is well-suited to the causal constraint of spoken conversations. On an English-to-Iraqi CSLT task, the proposed approach gives significant improvements over a baseline system as measured by BLEU, TER, and NIST. Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation.</p><p>6 0.73974848 <a title="120-lsi-6" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>7 0.73733729 <a title="120-lsi-7" href="./acl-2013-SORT%3A_An_Interactive_Source-Rewriting_Tool_for_Improved_Translation.html">305 acl-2013-SORT: An Interactive Source-Rewriting Tool for Improved Translation</a></p>
<p>8 0.73431337 <a title="120-lsi-8" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>9 0.72890395 <a title="120-lsi-9" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>10 0.72007346 <a title="120-lsi-10" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>11 0.71665937 <a title="120-lsi-11" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>12 0.69548595 <a title="120-lsi-12" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>13 0.69537234 <a title="120-lsi-13" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>14 0.68917626 <a title="120-lsi-14" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>15 0.68778437 <a title="120-lsi-15" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>16 0.68553364 <a title="120-lsi-16" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>17 0.68323857 <a title="120-lsi-17" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>18 0.68051386 <a title="120-lsi-18" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>19 0.67855155 <a title="120-lsi-19" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>20 0.67386997 <a title="120-lsi-20" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.06), (6, 0.044), (11, 0.059), (15, 0.012), (24, 0.073), (26, 0.09), (35, 0.081), (40, 0.014), (42, 0.052), (47, 0.131), (48, 0.024), (70, 0.038), (88, 0.033), (90, 0.063), (95, 0.139), (97, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90410447 <a title="120-lda-1" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>Author: Jason R. Smith ; Herve Saint-Amand ; Magdalena Plamada ; Philipp Koehn ; Chris Callison-Burch ; Adam Lopez</p><p>Abstract: Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1</p><p>2 0.89534295 <a title="120-lda-2" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>Author: Dongdong Zhang ; Shuangzhi Wu ; Nan Yang ; Mu Li</p><p>Abstract: Punctuations are not available in automatic speech recognition outputs, which could create barriers to many subsequent text processing tasks. This paper proposes a novel method to predict punctuation symbols for the stream of words in transcribed speech texts. Our method jointly performs parsing and punctuation prediction by integrating a rich set of syntactic features when processing words from left to right. It can exploit a global view to capture long-range dependencies for punctuation prediction with linear complexity. The experimental results on the test data sets of IWSLT and TDT4 show that our method can achieve high-level performance in punctuation prediction over the stream of words in transcribed speech text. 1</p><p>3 0.85141349 <a title="120-lda-3" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>Author: Wang Ling ; Guang Xiang ; Chris Dyer ; Alan Black ; Isabel Trancoso</p><p>Abstract: In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring parallel text: some users create post multilingual messages targeting international audiences while others “retweet” translations. We present an efficient method for detecting these messages and extracting parallel segments from them. We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counterpart of Twitter) using only their public APIs. As a supplement to existing parallel training data, our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary. The resources in described in this paper are available at http://www.cs.cmu.edu/∼lingwang/utopia.</p><p>4 0.8446272 <a title="120-lda-4" href="./acl-2013-VSEM%3A_An_open_library_for_visual_semantics_representation.html">380 acl-2013-VSEM: An open library for visual semantics representation</a></p>
<p>Author: Elia Bruni ; Ulisse Bordignon ; Adam Liska ; Jasper Uijlings ; Irina Sergienya</p><p>Abstract: VSEM is an open library for visual semantics. Starting from a collection of tagged images, it is possible to automatically construct an image-based representation of concepts by using off-theshelf VSEM functionalities. VSEM is entirely written in MATLAB and its objectoriented design allows a large flexibility and reusability. The software is accompanied by a website with supporting documentation and examples.</p><p>5 0.83795142 <a title="120-lda-5" href="./acl-2013-Mapping_Source_to_Target_Strings_without_Alignment_by_Analogical_Learning%3A_A_Case_Study_with_Transliteration.html">236 acl-2013-Mapping Source to Target Strings without Alignment by Analogical Learning: A Case Study with Transliteration</a></p>
<p>Author: Phillippe Langlais</p><p>Abstract: Analogical learning over strings is a holistic model that has been investigated by a few authors as a means to map forms of a source language to forms of a target language. In this study, we revisit this learning paradigm and apply it to the transliteration task. We show that alone, it performs worse than a statistical phrase-based machine translation engine, but the combination of both approaches outperforms each one taken separately, demonstrating the usefulness of the information captured by a so-called formal analogy.</p><p>6 0.83570117 <a title="120-lda-6" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>7 0.83336008 <a title="120-lda-7" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>8 0.83122981 <a title="120-lda-8" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>9 0.83104825 <a title="120-lda-9" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>10 0.83089471 <a title="120-lda-10" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>11 0.82958317 <a title="120-lda-11" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>12 0.82868266 <a title="120-lda-12" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>13 0.8283937 <a title="120-lda-13" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>14 0.82802701 <a title="120-lda-14" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>15 0.82718158 <a title="120-lda-15" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>16 0.82644504 <a title="120-lda-16" href="./acl-2013-Enlisting_the_Ghost%3A_Modeling_Empty_Categories_for_Machine_Translation.html">137 acl-2013-Enlisting the Ghost: Modeling Empty Categories for Machine Translation</a></p>
<p>17 0.82592356 <a title="120-lda-17" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>18 0.82339525 <a title="120-lda-18" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>19 0.82317388 <a title="120-lda-19" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>20 0.82306504 <a title="120-lda-20" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
