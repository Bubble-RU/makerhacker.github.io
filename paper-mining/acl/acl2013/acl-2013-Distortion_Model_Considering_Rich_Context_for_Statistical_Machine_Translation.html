<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-125" href="#">acl2013-125</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</h1>
<br/><p>Source: <a title="acl-2013-125-pdf" href="http://aclweb.org/anthology//P/P13/P13-1016.pdf">pdf</a></p><p>Author: Isao Goto ; Masao Utiyama ; Eiichiro Sumita ; Akihiro Tamura ; Sadao Kurohashi</p><p>Abstract: This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.</p><p>Reference: <a title="acl-2013-125-reference" href="../acl2013_reference/acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 jpt  Abstract This paper proposes new distortion models for phrase-based SMT. [sent-13, score-0.596]
</p><p>2 In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). [sent-14, score-1.236]
</p><p>3 We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. [sent-15, score-0.775]
</p><p>4 Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. [sent-16, score-0.174]
</p><p>5 It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances  from the training data. [sent-17, score-0.325]
</p><p>6 6 BLEU points for Chinese-English translation compared to the lexical reordering models. [sent-20, score-0.141]
</p><p>7 1 Introduction Estimating appropriate word order in a target language is one of the most difficult problems for statistical machine translation (SMT). [sent-21, score-0.128]
</p><p>8 This is particularly true when translating between languages with widely different word orders. [sent-22, score-0.072]
</p><p>9 To address this problem, there has been a lot of research done into word reordering: lexical reordering model (Tillman, 2004), which is one of the distortion models, reordering constraint (Zens et al. [sent-23, score-0.893]
</p><p>10 In general, source language syntax is useful for handling long distance word reordering. [sent-25, score-0.139]
</p><p>11 Phrase-based SMT mainly1 estimates word reordering using distortion models2. [sent-29, score-0.798]
</p><p>12 Therefore, distortion models are one of the most important components for phrase-based SMT. [sent-30, score-0.596]
</p><p>13 On the other hand, there are methods other than distortion models for improving word reordering for phrase-based SMT, such as pre-ordering or reordering constraints. [sent-31, score-0.875]
</p><p>14 However, these methods also use distortion models when translating by phrase-based SMT. [sent-32, score-0.615]
</p><p>15 Therefore, distortion models do not compete against these methods and are commonly used with them. [sent-33, score-0.615]
</p><p>16 If there is a good distortion model, it will improve the translation quality of phrase-based SMT and benefit to the methods using distortion models. [sent-34, score-1.184]
</p><p>17 In this paper, we propose two distortion models for phrase-based SMT. [sent-35, score-0.596]
</p><p>18 In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). [sent-36, score-1.236]
</p><p>19 The proposed models  are the pair model and the sequence model. [sent-37, score-0.076]
</p><p>20 The pair model utilizes the word at the CP, a word at an NP candidate site, and the words surrounding the CP and the NP candidates (context) simultaneously. [sent-38, score-0.249]
</p><p>21 In addition, the sequence model, which is the further improved model, considers richer context by identifying the label sequence that specify the span from the CP to the NP. [sent-39, score-0.182]
</p><p>22 It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. [sent-40, score-0.325]
</p><p>23 Our model learns the preference relations among NP 1A language model also supports the estimation. [sent-41, score-0.125]
</p><p>24 2In this paper, reordering models for phrase-based SMT, which are intended to estimate the source word position to be translated next in decoding, are called distortion models. [sent-42, score-1.047]
</p><p>25 This estimation is used to produce a hypothesis in the target language word order sequentially from left to right. [sent-43, score-0.131]
</p><p>26 Figure 1: An example of left-to-right translation for Japanese-English. [sent-71, score-0.028]
</p><p>27 Boxes represent phrases and arrows indicate the translation order of the phrases. [sent-72, score-0.046]
</p><p>28 Our model consists of one probabilistic model and does not require a parser. [sent-74, score-0.072]
</p><p>29 2  Distortion Model for Phrase-Based SMT  A Moses-style phrase-based SMT generates target hypotheses sequentially from left to right. [sent-77, score-0.06]
</p><p>30 Therefore, the role of the distortion model is to estimate the source phrase position to be translated  next whose target side phrase will be located immediately to the right of the already generated hypotheses. [sent-78, score-0.98]
</p><p>31 In Figure 1, we assume that only the kare wa (English side: “he”) has been translated. [sent-80, score-0.108]
</p><p>32 The target word to be generated next will be “bought” and the source word to be selected next will be its corresponding Japanese word katta. [sent-81, score-0.298]
</p><p>33 Thus, a distortion model should estimate phrases including katta as a source phrase position to be translated next. [sent-82, score-1.035]
</p><p>34 To explain the distortion model task in more detail, we need to redefine more precisely two terms, the current position (CP) and next position (NP) in the source sentence. [sent-83, score-0.9]
</p><p>35 CP is the source sentence position corresponding to the rightmost aligned target word in the generated target word sequence. [sent-84, score-0.365]
</p><p>36 NP is the source sentence position corresponding to the leftmost aligned target word in the target phrase to be generated next. [sent-85, score-0.338]
</p><p>37 The task of the distortion model is to estimate the NP3 from NP candidates (NPCs) for each CP in the source sentence. [sent-86, score-0.745]
</p><p>38 4 3NP is not always one position, because there tiple correct hypotheses. [sent-87, score-0.024]
</p><p>39 In existing methods, CP is the rightmost position of the last translated source phrase and NP is the leftmost position of the source phrase to be translated next. [sent-90, score-0.558]
</p><p>40 The upper sentence is the source sentence and the sentence underneath is a target hypothesis for each example. [sent-298, score-0.164]
</p><p>41 The NP is in bold, and the CP is in bold italics. [sent-299, score-0.021]
</p><p>42 The point of an arrow with a mark indicates a wrong NP candiadarrtoe. [sent-300, score-0.018]
</p><p>43 The superscript numbers indicate the word position in the source sentence. [sent-303, score-0.203]
</p><p>44 However, in Figure 2 (b), the word (kare) at the CP is the same as (a), but the NP is different (the NP is 10). [sent-305, score-0.053]
</p><p>45 From these examples, we see that distance is not the essential factor in deciding an NP. [sent-306, score-0.024]
</p><p>46 And it also turns out that the word at the CP alone is not enough to estimate the NP. [sent-307, score-0.095]
</p><p>47 Thus, not only the word at the CP but also the word at a NP candidate (NPC) should be considered simultaneously. [sent-308, score-0.126]
</p><p>48 In (c) and (d) in Figure 2, the word (kare) at the CP is the same and karita (borrowed) and katta (bought) are at the NPCs. [sent-309, score-0.295]
</p><p>49 Karita is the word at the NP and katta is not the word at the NP for (c), while katta is the word at the NP and karita is not the word at the NP for (d). [sent-310, score-0.588]
</p><p>50 From these examples, considering what the word is at the NP not consider word-level correspondences. [sent-311, score-0.053]
</p><p>51 One of the reasons for this difference is the relative word order between words. [sent-313, score-0.117]
</p><p>52 In (d) and (e) in Figure 2, the word (kare) at the CP and the word order between katta and karita are the same. [sent-315, score-0.366]
</p><p>53 However, the word at the NP for (d) and the word at the NP for (e) are different. [sent-316, score-0.106]
</p><p>54 From these examples, we can see that selecting a nearby word is not always correct. [sent-317, score-0.053]
</p><p>55 The difference is caused by the words surrounding the NPCs (context), the CP context, and the words between the CP and the NPC. [sent-318, score-0.037]
</p><p>56 Thus, these should be con-  sidered when estimating the NP. [sent-319, score-0.047]
</p><p>57 In summary, in order to estimate the NP, the following should be considered simultaneously: the word at the NP, the word at the CP, the relative word order among the NPCs, the words surrounding NP and CP (context), and the words between the CP and the NPC. [sent-320, score-0.344]
</p><p>58 There are distortion models that do not require a parser for phrase-based SMT. [sent-321, score-0.596]
</p><p>59 The linear distortion cost model used in Moses (Koehn et al. [sent-322, score-0.635]
</p><p>60 , 2007), whose costs are linearly proportional to the reordering distance, always gives a high cost to long distance reordering, even if the reordering is correct. [sent-323, score-0.271]
</p><p>61 The MSD lexical reordering model (Tillman, 2004; Koehn et al. [sent-324, score-0.149]
</p><p>62 , 2005; Galley and Manning, 2008) only calculates probabilities for the three kinds of phrase reorderings (monotone, swap, and discontinuous), and does not consider relative word order or words between the CP and the NPC. [sent-325, score-0.213]
</p><p>63 Thus, these models are not sufficient for long distance word reordering. [sent-326, score-0.095]
</p><p>64 Al-Onaizan and Papineni (2006) proposed a  distortion model that used the word at the CP and the word at an NPC. [sent-327, score-0.72]
</p><p>65 However, their model did not use context, relative word order, or words between the CP and the NPC. [sent-328, score-0.135]
</p><p>66 (2009) proposed a method that adjusts the linear distortion cost using the word at the CP and its context. [sent-330, score-0.672]
</p><p>67 Their model does not simultaneously consider both the word specified at the CP and the word specified at the NPCs. [sent-331, score-0.323]
</p><p>68 Their model (the outbound model) estimates how far the NP should be from the CP using the word at the CP and its context. [sent-334, score-0.214]
</p><p>69 5 Their model does not simultaneously con5They also proposed another model (the inbound model) sider both the word specified at the CP and the word specified at an NPC. [sent-335, score-0.425]
</p><p>70 For example, the outbound model considers the word specified at the CP, but does not consider the word specified at an NPC. [sent-336, score-0.389]
</p><p>71 Their models also do not consider relative word order. [sent-337, score-0.117]
</p><p>72 In contrast, our distortion model solves the aforementioned problems. [sent-338, score-0.614]
</p><p>73 Our distortion models  utilize the word specified at the CP, the word specified at an NPC, and also the context of the CP and the NPC simultaneously. [sent-339, score-0.879]
</p><p>74 Furthermore, our sequence model considers richer context including the relative word order among NPCs and also including all the words between the CP and the NPC. [sent-340, score-0.297]
</p><p>75 In addition, unlike previous methods, our models learn the preference relations among NPCs. [sent-341, score-0.091]
</p><p>76 3  Proposed Method  In this section, we first define our distortion model and explain our learning strategy. [sent-342, score-0.638]
</p><p>77 Then, we describe two proposed models: the pair model and the sequence model that is the further improved model. [sent-343, score-0.111]
</p><p>78 1 Distortion Model and Learning Strategy First, we define our distortion model. [sent-345, score-0.578]
</p><p>79 Let ibe a CP, j be an NPC, S be a source sentence, and X be the random variable of the NP. [sent-346, score-0.062]
</p><p>80 In this paper, distortion probability is defined as P(X = j |i, S), tworhitciohn nis p troheb probability eofifn an N asPC P j being tjh|ei, NS)P. [sent-347, score-0.597]
</p><p>81 ,  Our distortion model is defined as the model calculating the distortion probability. [sent-348, score-1.228]
</p><p>82 Next, we explain the learning strategy for our distortion model. [sent-349, score-0.634]
</p><p>83 We train this model as a discriminative model that discriminates the NP from NPCs. [sent-350, score-0.113]
</p><p>84 Let J be a set of word positions in S other than i. [sent-351, score-0.053]
</p><p>85 We train the distortion model subject to  ∑P(X  = j|i,S) = 1. [sent-352, score-0.614]
</p><p>86 ∑j∈J  The model parameters are learned to maximize the distortion probability of the NP among all of the NPCs J in each source sentence. [sent-353, score-0.7]
</p><p>87 This learning strategy is a kind of preference relation learning (Evgniou and Pontil, 2002). [sent-354, score-0.061]
</p><p>88 In this learning, the that estimates reverse direction distance. [sent-355, score-0.054]
</p><p>89 Each NPC is regarded as an NP, and the inbound model estimates how far the corresponding CP should be from the NP using the word at the NP and its context. [sent-356, score-0.187]
</p><p>90 157  distortion probability of the actual NP will be rel-  atively higher than those of all the other NPCs J. [sent-357, score-0.6]
</p><p>91 This learning strategy is different from that of (Al-Onaizan and Papineni, 2006; Green et al. [sent-358, score-0.032]
</p><p>92 ∑(2010) trained their outbound model subject to ∑c∈C P(Y = c|i, S) = 1, where C is the set of ∑thec ∈niCne distortci|oin,S Sc)la =sse 1s,6 wanhedr Ye C Cis tsh teh era snedto omf ∑vheari naibnlee doisf tohrecorrect distortion class that the correct distortion is classified into. [sent-361, score-1.308]
</p><p>93 o Ddeislt probabilities nthedat a they −le iar −ne 1d. [sent-364, score-0.021]
</p><p>94 were the probabilities of distortion classes in all of the training data, not the relative preferences among the NPCs in each source sentence. [sent-365, score-0.731]
</p><p>95 2 Pair Model The pair model utilizes the word at the CP, the word at an NPC, and the context of the CP and the NPC simultaneously to estimate the NP. [sent-367, score-0.281]
</p><p>96 This can be done by our distortion model definition and the learning strategy described in the previous section. [sent-368, score-0.646]
</p><p>97 The reason for this  is that a model based on the maximum entropy method can calculate probabilities. [sent-371, score-0.036]
</p><p>98 However, if we use scores as an approximation of the distortion probabilities, various discriminative machine learning methods can be applied to build the distortion model. [sent-372, score-1.173]
</p><p>99 We add a beginning of sentence (BOS) marker to the head of the source sentence and an end of sentence (EOS) marker to the end, so the source sentence S is expressed as (s0 = BOS, sn+1 = EOS). [sent-377, score-0.248]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('distortion', 0.578), ('cp', 0.558), ('np', 0.315), ('npc', 0.215), ('npcs', 0.188), ('katta', 0.134), ('reordering', 0.113), ('kare', 0.108), ('karita', 0.108), ('smt', 0.09), ('position', 0.088), ('outbound', 0.071), ('specified', 0.071), ('translated', 0.069), ('source', 0.062), ('estimates', 0.054), ('word', 0.053), ('bought', 0.048), ('relative', 0.046), ('green', 0.045), ('inbound', 0.044), ('estimate', 0.042), ('eos', 0.041), ('goto', 0.041), ('tillman', 0.039), ('simultaneously', 0.039), ('surrounding', 0.037), ('model', 0.036), ('context', 0.035), ('considers', 0.034), ('leftmost', 0.034), ('rightmost', 0.034), ('strategy', 0.032), ('bos', 0.031), ('sequentially', 0.031), ('calculates', 0.03), ('richer', 0.029), ('target', 0.029), ('preference', 0.029), ('marker', 0.028), ('translation', 0.028), ('estimating', 0.028), ('candidates', 0.027), ('phrase', 0.026), ('distances', 0.026), ('distance', 0.024), ('among', 0.024), ('ro', 0.024), ('explain', 0.024), ('tjh', 0.024), ('thec', 0.024), ('discriminates', 0.024), ('akihi', 0.024), ('isao', 0.024), ('tiple', 0.024), ('wanhedr', 0.024), ('next', 0.024), ('ni', 0.023), ('decoding', 0.023), ('utilizes', 0.023), ('specify', 0.023), ('koehn', 0.023), ('sequence', 0.022), ('underneath', 0.022), ('sider', 0.022), ('cis', 0.022), ('atively', 0.022), ('akihiro', 0.022), ('pontil', 0.022), ('cost', 0.021), ('probabilities', 0.021), ('omf', 0.021), ('mccord', 0.021), ('borrowed', 0.021), ('tamura', 0.021), ('bold', 0.021), ('candidate', 0.02), ('learn', 0.02), ('adjusts', 0.02), ('msd', 0.02), ('enables', 0.019), ('moses', 0.019), ('papineni', 0.019), ('compete', 0.019), ('sidered', 0.019), ('nis', 0.019), ('reorderings', 0.019), ('translating', 0.019), ('effect', 0.018), ('models', 0.018), ('order', 0.018), ('sse', 0.018), ('masao', 0.018), ('patent', 0.018), ('utiyama', 0.018), ('arrow', 0.018), ('discriminative', 0.017), ('sentence', 0.017), ('improved', 0.017), ('discontinuous', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="125-tfidf-1" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>Author: Isao Goto ; Masao Utiyama ; Eiichiro Sumita ; Akihiro Tamura ; Sadao Kurohashi</p><p>Abstract: This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.</p><p>2 0.20476797 <a title="125-tfidf-2" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>Author: Fei Huang ; Cezar Pendus</p><p>Abstract: We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation (SMT). Instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage. Furthermore, , we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules. Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach: we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric. 1</p><p>3 0.11446539 <a title="125-tfidf-3" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>4 0.10845855 <a title="125-tfidf-4" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>5 0.10735064 <a title="125-tfidf-5" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>6 0.1036802 <a title="125-tfidf-6" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.10276385 <a title="125-tfidf-7" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>8 0.075297043 <a title="125-tfidf-8" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>9 0.074652441 <a title="125-tfidf-9" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>10 0.07026495 <a title="125-tfidf-10" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>11 0.069409333 <a title="125-tfidf-11" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>12 0.062718704 <a title="125-tfidf-12" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>13 0.060713183 <a title="125-tfidf-13" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>14 0.057002559 <a title="125-tfidf-14" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>15 0.056921788 <a title="125-tfidf-15" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>16 0.054717951 <a title="125-tfidf-16" href="./acl-2013-Two-Neighbor_Orientation_Model_with_Cross-Boundary_Global_Contexts.html">363 acl-2013-Two-Neighbor Orientation Model with Cross-Boundary Global Contexts</a></p>
<p>17 0.054217216 <a title="125-tfidf-17" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>18 0.052560903 <a title="125-tfidf-18" href="./acl-2013-Using_subcategorization_knowledge_to_improve_case_prediction_for_translation_to_German.html">378 acl-2013-Using subcategorization knowledge to improve case prediction for translation to German</a></p>
<p>19 0.051898073 <a title="125-tfidf-19" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>20 0.05188597 <a title="125-tfidf-20" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, -0.094), (2, 0.081), (3, 0.059), (4, -0.035), (5, 0.025), (6, 0.018), (7, -0.011), (8, -0.011), (9, 0.043), (10, 0.017), (11, 0.012), (12, 0.028), (13, -0.007), (14, 0.014), (15, 0.031), (16, 0.095), (17, 0.028), (18, -0.048), (19, -0.037), (20, -0.066), (21, -0.017), (22, -0.01), (23, -0.11), (24, 0.075), (25, 0.021), (26, -0.038), (27, -0.069), (28, -0.12), (29, -0.05), (30, -0.08), (31, -0.04), (32, 0.015), (33, 0.051), (34, -0.046), (35, 0.032), (36, -0.028), (37, -0.022), (38, -0.01), (39, -0.039), (40, -0.049), (41, -0.107), (42, -0.003), (43, -0.009), (44, -0.15), (45, -0.039), (46, 0.031), (47, 0.032), (48, -0.015), (49, -0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92958647 <a title="125-lsi-1" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>Author: Isao Goto ; Masao Utiyama ; Eiichiro Sumita ; Akihiro Tamura ; Sadao Kurohashi</p><p>Abstract: This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.</p><p>2 0.83865142 <a title="125-lsi-2" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>Author: Fei Huang ; Cezar Pendus</p><p>Abstract: We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation (SMT). Instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage. Furthermore, , we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules. Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach: we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric. 1</p><p>3 0.74509448 <a title="125-lsi-3" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>4 0.70167708 <a title="125-lsi-4" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>Author: Minwei Feng ; Jan-Thorsten Peter ; Hermann Ney</p><p>Abstract: In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. Results on five Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average. Results of comparative study with other seven widely used reordering models will also be reported.</p><p>5 0.66909325 <a title="125-lsi-5" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>Author: Nadir Durrani ; Alexander Fraser ; Helmut Schmid ; Hieu Hoang ; Philipp Koehn</p><p>Abstract: The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve.</p><p>6 0.66560787 <a title="125-lsi-6" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>7 0.65618432 <a title="125-lsi-7" href="./acl-2013-Two-Neighbor_Orientation_Model_with_Cross-Boundary_Global_Contexts.html">363 acl-2013-Two-Neighbor Orientation Model with Cross-Boundary Global Contexts</a></p>
<p>8 0.44099879 <a title="125-lsi-8" href="./acl-2013-Using_subcategorization_knowledge_to_improve_case_prediction_for_translation_to_German.html">378 acl-2013-Using subcategorization knowledge to improve case prediction for translation to German</a></p>
<p>9 0.43290827 <a title="125-lsi-9" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>10 0.42767137 <a title="125-lsi-10" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>11 0.39896929 <a title="125-lsi-11" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>12 0.39699891 <a title="125-lsi-12" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>13 0.38597372 <a title="125-lsi-13" href="./acl-2013-Handling_Ambiguities_of_Bilingual_Predicate-Argument_Structures_for_Statistical_Machine_Translation.html">180 acl-2013-Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation</a></p>
<p>14 0.38559201 <a title="125-lsi-14" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>15 0.38053253 <a title="125-lsi-15" href="./acl-2013-SORT%3A_An_Interactive_Source-Rewriting_Tool_for_Improved_Translation.html">305 acl-2013-SORT: An Interactive Source-Rewriting Tool for Improved Translation</a></p>
<p>16 0.37837967 <a title="125-lsi-16" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>17 0.37255427 <a title="125-lsi-17" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>18 0.37128541 <a title="125-lsi-18" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>19 0.35994855 <a title="125-lsi-19" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>20 0.35139027 <a title="125-lsi-20" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.025), (6, 0.017), (11, 0.038), (15, 0.013), (24, 0.022), (26, 0.014), (35, 0.058), (42, 0.533), (48, 0.021), (70, 0.02), (88, 0.035), (90, 0.029), (95, 0.057)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98687083 <a title="125-lda-1" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>Author: Sina Zarriess ; Jonas Kuhn</p><p>Abstract: We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</p><p>same-paper 2 0.97926366 <a title="125-lda-2" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>Author: Isao Goto ; Masao Utiyama ; Eiichiro Sumita ; Akihiro Tamura ; Sadao Kurohashi</p><p>Abstract: This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.</p><p>3 0.97501397 <a title="125-lda-3" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>Author: Bharat Ram Ambati ; Tejaswini Deoskar ; Mark Steedman</p><p>Abstract: We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery.</p><p>4 0.94101197 <a title="125-lda-4" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>Author: Abhijit Mishra ; Pushpak Bhattacharyya ; Michael Carl</p><p>Abstract: In this paper we introduce Translation Difficulty Index (TDI), a measure of difficulty in text translation. We first define and quantify translation difficulty in terms of TDI. We realize that any measure of TDI based on direct input by translators is fraught with subjectivity and adhocism. We, rather, rely on cognitive evidences from eye tracking. TDI is measured as the sum of fixation (gaze) and saccade (rapid eye movement) times of the eye. We then establish that TDI is correlated with three properties of the input sentence, viz. length (L), degree of polysemy (DP) and structural complexity (SC). We train a Support Vector Regression (SVR) system to predict TDIs for new sentences using these features as input. The prediction done by our framework is well correlated with the empirical gold standard data, which is a repository of < L, DP, SC > and TDI pairs for a set of sentences. The primary use of our work is a way of “binning” sentences (to be translated) in “easy”, “medium” and “hard” categories as per their predicted TDI. This can decide pricing of any translation task, especially useful in a scenario where parallel corpora for Machine Translation are built through translation crowdsourcing/outsourcing. This can also provide a way of monitoring progress of second language learners.</p><p>5 0.93713409 <a title="125-lda-5" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>Author: Rico Sennrich ; Holger Schwenk ; Walid Aransa</p><p>Abstract: While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also de- scribe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1BLEU over unadapted systems and single-domain adaptation.</p><p>6 0.9142763 <a title="125-lda-6" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<p>7 0.91117465 <a title="125-lda-7" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>8 0.90762252 <a title="125-lda-8" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>9 0.73228312 <a title="125-lda-9" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>10 0.71363968 <a title="125-lda-10" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>11 0.70026034 <a title="125-lda-11" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>12 0.69199151 <a title="125-lda-12" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>13 0.67487174 <a title="125-lda-13" href="./acl-2013-Integrating_Multiple_Dependency_Corpora_for_Inducing_Wide-coverage_Japanese_CCG_Resources.html">199 acl-2013-Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</a></p>
<p>14 0.67405593 <a title="125-lda-14" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>15 0.65283829 <a title="125-lda-15" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>16 0.65183973 <a title="125-lda-16" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.63516843 <a title="125-lda-17" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>18 0.63242155 <a title="125-lda-18" href="./acl-2013-Two-Neighbor_Orientation_Model_with_Cross-Boundary_Global_Contexts.html">363 acl-2013-Two-Neighbor Orientation Model with Cross-Boundary Global Contexts</a></p>
<p>19 0.62722087 <a title="125-lda-19" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>20 0.62142181 <a title="125-lda-20" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
