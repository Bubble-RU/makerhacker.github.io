<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-259" href="#">acl2013-259</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</h1>
<br/><p>Source: <a title="acl-2013-259-pdf" href="http://aclweb.org/anthology//P/P13/P13-1061.pdf">pdf</a></p><p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>Reference: <a title="acl-2013-259-reference" href="../acl2013_reference/acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist-  ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data. [sent-6, score-0.931]
</p><p>2 1 Introduction Bilingual sentence alignment is a fundamental task to undertake for the purpose of facilitating many important natural language processing applications such as statistical machine translation (Brown et al. [sent-7, score-0.608]
</p><p>3 Its objective is to identify correspondences between bilingual sentences in given bitexts. [sent-11, score-0.277]
</p><p>4 As summarized by Wu (2010), existing sentence alignment techniques rely mainly on sentence length and bilingual lexical resource. [sent-12, score-0.873]
</p><p>5 Lexicon-based  approaches resort to word correspondences in a bilingual lexicon to match bilingual sentences. [sent-15, score-0.417]
</p><p>6 A few sentence alignment methods and tools have also been explored to combine the two. [sent-16, score-0.575]
</p><p>7 Moore (2002) proposes a multi-pass search procedure using both sentence length and an automaticallyderived bilingual lexicon. [sent-17, score-0.333]
</p><p>8 , 2005) is another sentence aligner that combines sentence length and a lexicon. [sent-19, score-0.424]
</p><p>9 Without a lexicon, it backs off to a length-based algorithm and then automatically derives a lexicon from the alignment result. [sent-20, score-0.519]
</p><p>10 Soon after, Ma (2006) develops the lexicon-based aligner Champollion, assuming that different words have different importance in aligning two sentences. [sent-21, score-0.26]
</p><p>11 Nevertheless, most existing approaches to sentence alignment follow the monotonicity assumption that coupled sentences in bitexts appear in a similar sequential order in two languages and crossings are not entertained in general (Langlais et al. [sent-22, score-0.99]
</p><p>12 Consequently the task of sentence alignment becomes handily solvable by means of such basic techniques as dynamic pro-  gramming. [sent-24, score-0.605]
</p><p>13 For example, bilingual clauses in legal bitexts are often coordinated in a way not to keep the same clause order, demanding fully or partially crossing pairings. [sent-26, score-0.494]
</p><p>14 Such monotonicity seriously impairs the existing alignment approaches founded on the monotonicity assumption. [sent-28, score-0.705]
</p><p>15 This paper is intended to explore the problem of non-monotonic alignment within the framework of semisupervised learning. [sent-29, score-0.596]
</p><p>16 First, monolingual sentences with high affinity are likely to have their translations with similar relatedness. [sent-31, score-0.461]
</p><p>17 Following this assumption, we propose the conception of monolingual consistency which, to the best of 622  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-32, score-0.253]
</p><p>18 Figure 1: A real example of non-monotonic sentence alignment from BLIS corpus. [sent-50, score-0.575]
</p><p>19 Second, initial alignment of certain quality can be obtained by means of existing alignment techniques. [sent-52, score-1.085]
</p><p>20 Our approach attempts to incorporate both monolingual consistency of sentences and bilingual consistency of initial alignment into a semisupervised learning framework to produce an optimal solution. [sent-53, score-1.344]
</p><p>21 Extensive evaluations are performed using real-world legislation bitexts from BLIS, a comprehensive legislation database maintained by the Department ofJustice, HKSAR. [sent-54, score-0.506]
</p><p>22 1 The Problem An alignment algorithm accepts as input a bitext consisting of a set of source-language sentences, S = {s1, s2, . [sent-57, score-0.563]
</p><p>23 rDgeift-fleanregnuta fgreom se previous Twor=ks relying on the} monotonicity assumption, our algorithm is generalized to allow the pairings of sentences in S and T ttoo cross arbitrarily. [sent-64, score-0.294]
</p><p>24 Figure 2(a) iclelsust irnat eSs monotonic alignment with no crossing correspondences in a bipartite graph and 2(b) non-monotonic alignment with scrambled pairings. [sent-65, score-1.331]
</p><p>25 Note that it is relatively straightforward to identify the type of manyto-many alignment in monotonic alignment using techniques such as dynamic programming if there is no scrambled pairing or the scrambled pairings are local, limited to a short distance. [sent-66, score-1.535]
</p><p>26 However, the situation of non-monotonic alignment is much more complicated. [sent-67, score-0.487]
</p><p>27 For the sake of simplicity, we will not consider non-monotonic alignment with many-to-  ×  many pairings but rather assume that each sentence may align to only one or zero sentence in the other language. [sent-69, score-0.803]
</p><p>28 n Lbeettmwaetrenix SF dnedn oTte , a specific alignment so ×lu Ttio . [sent-71, score-0.487]
</p><p>29 We then defiinne an alignment function A : iFn → Ae to produce athne a lfiignnalalignment, iwonhe Are A: Fis t →he alignment umceatr tihxe fo firn aSl aanlidg Tm ,e nwt,ith w Aij = s1 hfoer a a correspondence rb e Stawndeen T si wanitdh tj and Aij = 0 otherwise. [sent-73, score-1.076]
</p><p>30 Then, the optimal alignment solution is to be derived by minimizing the cost function Q(F), i. [sent-76, score-0.534]
</p><p>31 (2)  623  s s s23514t t21543 s6  (a)  t6  Figure 2: Illustration of monotonic (a) and non-monotonic alignment (b), with a line representing the  correspondence of two bilingual sentences. [sent-79, score-0.88]
</p><p>32 2, (3) where W and V are the symmetric matrices to rep-  resent the monolingual sentence affinity matrices in S and T , respectively, and D and E are the diagonal mda Tt ,ri rceessp ewcittihv eelynt,r aiensd Dii = Pj Wij ea dnidEii = Pj Vij. [sent-81, score-0.584]
</p><p>33 2, Aˆ Fˆ  (4)  where is the initial alignment matrix obtained by A : → Note that is the initial relation bmya Atrix : bFetw →een S and T . [sent-86, score-0.851]
</p><p>34 Fˆ  the final alignment to maintain the maximum consistency with the initial alignment. [sent-92, score-0.707]
</p><p>35 Non-positive entries in F∗ indicate unrealistic correspondences of sentences −  and are thus set to zero before applying the alignment function. [sent-98, score-0.596]
</p><p>36 3  Alignment Function  Once the optimal F∗ is acquired, the remaining task is to design an alignment function A to contvaesrkt i its in toto d an alignment smoleuntito fnu. [sent-100, score-1.021]
</p><p>37 , 2004), which produces an alignment with respect to the largest scores in each row and each column. [sent-102, score-0.487]
</p><p>38 Figure 3 illustrates a mapping relation matrix onto an alignment matrix, which also shows that the optimal alignment cannot be achieved by heuristic search. [sent-104, score-1.163]
</p><p>39 Banding is another approach frequently used to convert a relation matrix to alignment (Kay and R ¨oscheisen, 1993). [sent-105, score-0.629]
</p><p>40 It is founded on the observation that true monotonic alignment paths usually lie close to the diagonal of a relation matrix. [sent-106, score-0.838]
</p><p>41 We opt for converting a relation matrix into specific alignment by solving  1http://www. [sent-108, score-0.629]
</p><p>42 org/lapack/ 624  Figure 3: Illustration of sentence alignment from relation matrix to alignment matrix. [sent-110, score-1.204]
</p><p>43 The right matrix represents the corresponding alignment matrix by our algorithm. [sent-112, score-0.687]
</p><p>44 XXij ≤ 1,XXij ≤ 1,Xij ∈ {0,1} Xi=1  This  turns sentence  Xj=1  alignment into  a  problem  to  be resolved by binary linear programming (BIP), which has been successfully applied to word alignment (Taskar et al. [sent-115, score-1.093]
</p><p>45 4 Alignment Initialization Once the above alignment function is available, the initial alignment matrix can be derived from an initial relation matrix obtained by an available alignment method. [sent-119, score-1.925]
</p><p>46 These kinds of anchor strings provide quite reliable information to link bilingual sentences into pairs, and thus can serve as useful cues for sentence alignment. [sent-124, score-0.478]
</p><p>47 Fˆ  Aˆ  The anchor strings used in this work are derived by searching the bitexts using word-level inverted indexing, a basic technique widely used in information retrieval (Baeza-Yates and Ribeiro-Neto, 2011). [sent-126, score-0.382]
</p><p>48 The anchor strings, once found, are used to calculate the initial affinity of two sentences using Dice’s coefficient  Fˆij  Fˆij=|2C|1Ci1|i +∩ |C C22jj|  (8)  where C1i and C2j are the anchor sets in si and tj, respectively, adn Cd | · | is the cardinality of a set. [sent-130, score-0.644]
</p><p>49 , 2012), we have not yet been exposed to any attempt to leverage monolingual sentence affinity for sentence alignment. [sent-137, score-0.545]
</p><p>50 Let us take W as an example, where the entry Wij represents the affinity of sentence si and sentence sj, and it is set to 0 for i = j in order to avoid self-reinforcement during optimization (Zhou et al. [sent-139, score-0.437]
</p><p>51 Although semantic similarity estimation is a straightforward approach to deriving the two affinity matrices, other approaches are also feasible. [sent-147, score-0.265]
</p><p>52 An alternative approach can be based on sentence length under the assumption that two sentences with close lengths in one language tend to have their translations also with close lengths. [sent-148, score-0.263]
</p><p>53 6  Discussion  The proposed semisupervised framework for nonmonotonic alignment is in fact generalized beyond, and can also be applied to, monotonic alignment. [sent-150, score-0.885]
</p><p>54 One way to do it is to incorporate sentence positions into Equation (1) by introducing a position constraint Qp(F) to enfboyrc ien rthodatu bilingual sentences rina nclto Qser positions should have a higher chance to match one another. [sent-152, score-0.435]
</p><p>55 For example, the new constraint can be defined as  =  Xm  Xn  Qp(F) XX|pi − qj|Fi2j, Xi=1 Xj=1  where pi and qj are the absolute (or relative) positions of two bilingual sentences in their respective sequences. [sent-153, score-0.341]
</p><p>56 Another way follows the banding assumption that the actual couplings only appear in a narrow band along the main diagonal of relation matrix. [sent-154, score-0.285]
</p><p>57 Accordingly, all entries of F∗ outside this band are set to zero before the alignment function is applied. [sent-155, score-0.523]
</p><p>58 hk provides Chinese-English bilingual texts of ordinances and subsidiary legislation in effect on or after 30 June 1997. [sent-162, score-0.339]
</p><p>59 By web crawling, we have collected in total 3 1,516 English and 3 1,405 Chinese web pages, forming a bilingual corpus of 3 1,401 bitexts after filtering out null pages. [sent-164, score-0.388]
</p><p>60 In addition, to calculate the monolingual sentence affinity, stemming of En-  glish words is performed with the Porter Stemmer (Porter, 1980) after anchor string mining. [sent-171, score-0.338]
</p><p>61 The manual alignment of the evaluation data set is performed upon the initial alignment by Hunalign (Varga et al. [sent-172, score-1.085]
</p><p>62 , 2005), an effective sentence aligner that uses both sentence length and a bilexicon (if available). [sent-173, score-0.47]
</p><p>63 Its output is then double-checked and corrected by two experts in bilingual studies, resulting in a data set of 1747 1-1 and 70 1-0 or 0-1 sentence pairs. [sent-175, score-0.256]
</p><p>64 A three-fold cross-validation is thus performed on the initial 1-1 alignment and the parameters that give the best average performance are chosen. [sent-182, score-0.598]
</p><p>65 TypeTotalPrienditAliCgnorrNPorendmoACliogrnr 11--011774071646521136564177047155033 Table 1: Performance of the initial alignment and our aligner, where the Pred and Corr columns are the numbers of predicted and correct pairings. [sent-190, score-0.598]
</p><p>66 If the monolingual consistency assumption holds, the plotted points would appear nearby the diagonal. [sent-199, score-0.335]
</p><p>67 Figure 4 confirms this, indicating that sen-  tence pairs with high affinity in one language do have their counterparts with similarly high affinity in the other language. [sent-200, score-0.488]
</p><p>68 3 Impact of Initial Alignment The 1-1 initial alignment plays the role of labeled instances for the semisupervised learning. [sent-202, score-0.707]
</p><p>69 As shown in Table 1, our alignment function predicts 145 1 1-1 pairings by virtue of anchor strings, among which 1354 pairings are correct, yielding a relatively high precision in the non-monotonic circumstance. [sent-204, score-0.873]
</p><p>70 It also predicts null alignment for many sentences that contain no anchor. [sent-205, score-0.547]
</p><p>71 This explains why it outputs 662 1-0 pairings when there  Percentage of nitiail 1−1 ailgnment  Figure 5: Performance of non-monotonic alignment along the percentage of initial 1-1 alignment. [sent-206, score-0.738]
</p><p>72 Starting from this initial alignment, our aligner (let us call it NonmoAlign) discovers 179 more 1-1 pairings. [sent-208, score-0.348]
</p><p>73 A question here is concerned with how the scale of initial alignment affects the final alignment. [sent-209, score-0.598]
</p><p>74 The random selection for each proportion is performed ten times and their average alignment performance is taken as the final result and plotted in Figure 5. [sent-211, score-0.528]
</p><p>75 An observation from this figure is that the aligner consistently discovers significantly more 1-1 pairings on top of an initial 1-1 alignment, which has to be accounted for by the monolingual consistency. [sent-212, score-0.693]
</p><p>76 Another observation is that the alignment performance goes up along the increase of the percentage of initial alignment while performance gain slows down gradually. [sent-213, score-1.117]
</p><p>77 4 Non-Monotonic Alignment To test our aligner with non-monotonic sequences of sentences, we have them randomly scrambled in our experimental data. [sent-216, score-0.288]
</p><p>78 According to Varga et al (2005), this setting gives a higher alignment quality than otherwise. [sent-231, score-0.487]
</p><p>79 The performance of alignment is measured by precision (P), recall (R) and F-measure (F1). [sent-234, score-0.487]
</p><p>80 The particularly poor performance of Moore’s aligner has to be accounted for by its requirement of more than thousands of sentences in bitext input for reliable estimation of its parameters. [sent-237, score-0.371]
</p><p>81 Unlike traditional alignment approaches, ours does not found its performance on the degree ofmonotonicity. [sent-242, score-0.487]
</p><p>82 It shows  that both Moore’s aligner and Hunalign work relatively well on bitexts with a low degree of nonmonotonicity, but their performance drops dramatically when the non-monotonicity is increased. [sent-248, score-0.426]
</p><p>83 6 Monotonic Alignment The proposed alignment approach is also expected to work well on monotonic sentence alignment. [sent-252, score-0.771]
</p><p>84 Of the two strategies discussed above, banding is used to help our aligner incorporate the sequence information. [sent-254, score-0.275]
</p><p>85 The initial relation matrix is built with the aid of a dictionary automatically derived by Hunalign. [sent-255, score-0.253]
</p><p>86 The evaluation results are presented in Table 3, which shows that NonmoAlign still achieves very competitive performance on monotonic sentence alignment. [sent-258, score-0.284]
</p><p>87 4  Related Work  The research of sentence alignment originates in  the early 1990s. [sent-259, score-0.575]
</p><p>88 79 287976 Table 3: Performance of monotonic alignment in comparison with the baseline methods. [sent-271, score-0.683]
</p><p>89 The subsequent stage of sentence alignment research is accompanied by the advent of a handful ofwell-designed alignment tools. [sent-275, score-1.103]
</p><p>90 In the absence of a lexicon, it first performs an initial alignment wholly relying on sentence length and then automatically builds a lexicon based on this alignment. [sent-284, score-0.76]
</p><p>91 Then, the relation matrix of a bitext is built of similarity scores for the rough translation and the actual translation at sentence level. [sent-286, score-0.412]
</p><p>92 To deal with noisy input, Ma (2006) proposes  a lexicon-based sentence aligner - Champollion. [sent-288, score-0.329]
</p><p>93 For this purpose, the input bitexts are first divided into smaller aligned fragments before applying Champollion to derive finer-grained sentence pairs. [sent-293, score-0.308]
</p><p>94 (2007), a generative model is proposed, accompanied by two specific alignment strategies, i. [sent-295, score-0.528]
</p><p>95 5  Conclusion  In this paper we have proposed and tested a semisupervised learning approach to nonmonotonic sentence alignment by incorporating both monolingual and bilingual consistency. [sent-299, score-1.089]
</p><p>96 The utility of monolingual consistency in maintaining the consonance of high-affinity monolingual sentences with their translations has been demonstrated. [sent-300, score-0.489]
</p><p>97 This work also exhibits that bilingual consistency of initial alignment of certain quality is useful to boost alignment performance. [sent-301, score-1.362]
</p><p>98 Although initially proposed for nonmonotonic alignment, it works well on monotonic alignment by incorporating the constraint of sentence sequence. [sent-304, score-0.911]
</p><p>99 Segmentation and alignment of parallel text for statistical machine translation. [sent-339, score-0.52]
</p><p>100 Clause alignment for bilingual HK legal texts: A lexical-based approach. [sent-353, score-0.731]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('alignment', 0.487), ('hunalign', 0.301), ('affinity', 0.225), ('bitexts', 0.22), ('aligner', 0.206), ('monotonic', 0.196), ('bilingual', 0.168), ('monolingual', 0.144), ('legislation', 0.143), ('pairings', 0.14), ('varga', 0.132), ('initial', 0.111), ('semisupervised', 0.109), ('consistency', 0.109), ('anchor', 0.106), ('qb', 0.102), ('matrix', 0.1), ('monotonicity', 0.094), ('blis', 0.093), ('nonmoalign', 0.093), ('nonmonotonic', 0.093), ('sentence', 0.088), ('scrambled', 0.082), ('moore', 0.079), ('bitext', 0.076), ('legal', 0.076), ('champollion', 0.076), ('banding', 0.069), ('qm', 0.065), ('sentences', 0.06), ('wij', 0.058), ('strings', 0.056), ('aligning', 0.054), ('kay', 0.053), ('kit', 0.052), ('chunyu', 0.051), ('diagonal', 0.051), ('correspondences', 0.049), ('constraint', 0.047), ('optimal', 0.047), ('barlow', 0.046), ('bilexicon', 0.046), ('bundle', 0.046), ('couplings', 0.046), ('nonmonotonicity', 0.046), ('oscheisen', 0.046), ('sylvester', 0.046), ('ttio', 0.046), ('relation', 0.042), ('length', 0.042), ('wu', 0.041), ('viktor', 0.041), ('cityu', 0.041), ('kui', 0.041), ('assumption', 0.041), ('plotted', 0.041), ('accompanied', 0.041), ('kong', 0.04), ('similarity', 0.04), ('hong', 0.039), ('brown', 0.038), ('matrices', 0.038), ('counterparts', 0.038), ('ij', 0.038), ('fij', 0.038), ('tj', 0.037), ('xm', 0.037), ('positions', 0.036), ('si', 0.036), ('porter', 0.036), ('band', 0.036), ('proposes', 0.035), ('xn', 0.034), ('aij', 0.034), ('aligners', 0.034), ('klavans', 0.034), ('parallel', 0.033), ('translation', 0.033), ('simard', 0.032), ('langlais', 0.032), ('laws', 0.032), ('zhou', 0.032), ('observation', 0.032), ('translations', 0.032), ('gaussian', 0.032), ('lexicon', 0.032), ('discovers', 0.031), ('programming', 0.031), ('qj', 0.03), ('founded', 0.03), ('crossing', 0.03), ('dynamic', 0.03), ('correspondence', 0.029), ('deviation', 0.029), ('accounted', 0.029), ('haifeng', 0.029), ('severely', 0.029), ('nie', 0.029), ('hk', 0.028), ('qp', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="259-tfidf-1" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>2 0.28169578 <a title="259-tfidf-2" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>3 0.22134933 <a title="259-tfidf-3" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>4 0.1833403 <a title="259-tfidf-4" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>5 0.17158057 <a title="259-tfidf-5" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>6 0.16041882 <a title="259-tfidf-6" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>7 0.15776066 <a title="259-tfidf-7" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>8 0.14658448 <a title="259-tfidf-8" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>9 0.12669274 <a title="259-tfidf-9" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>10 0.12067223 <a title="259-tfidf-10" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>11 0.11671099 <a title="259-tfidf-11" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>12 0.10775699 <a title="259-tfidf-12" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>13 0.10641482 <a title="259-tfidf-13" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>14 0.10584345 <a title="259-tfidf-14" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>15 0.10515023 <a title="259-tfidf-15" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>16 0.099602304 <a title="259-tfidf-16" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>17 0.097600885 <a title="259-tfidf-17" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>18 0.096646033 <a title="259-tfidf-18" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>19 0.094845533 <a title="259-tfidf-19" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>20 0.090993553 <a title="259-tfidf-20" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.221), (1, -0.089), (2, 0.133), (3, 0.039), (4, 0.05), (5, -0.011), (6, -0.116), (7, -0.022), (8, -0.053), (9, -0.083), (10, 0.055), (11, -0.194), (12, 0.0), (13, -0.076), (14, 0.056), (15, 0.033), (16, 0.089), (17, 0.007), (18, 0.008), (19, -0.154), (20, -0.053), (21, -0.035), (22, -0.016), (23, 0.011), (24, -0.049), (25, 0.097), (26, -0.177), (27, 0.106), (28, 0.078), (29, 0.063), (30, -0.109), (31, 0.069), (32, -0.048), (33, -0.009), (34, -0.073), (35, -0.02), (36, -0.048), (37, 0.12), (38, 0.045), (39, 0.027), (40, 0.02), (41, 0.046), (42, 0.007), (43, 0.043), (44, 0.068), (45, -0.105), (46, 0.069), (47, -0.146), (48, 0.045), (49, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97296673 <a title="259-lsi-1" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>2 0.82953274 <a title="259-lsi-2" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Chris Callison-Burch ; Peter Clark</p><p>Abstract: Fast alignment is essential for many natural language tasks. But in the setting of monolingual alignment, previous work has not been able to align more than one sentence pair per second. We describe a discriminatively trained monolingual word aligner that uses a Conditional Random Field to globally decode the best alignment with features drawn from source and target sentences. Using just part-of-speech tags and WordNet as external resources, our aligner gives state-of-the-art result, while being an order-of-magnitude faster than the previous best performing system.</p><p>3 0.80272841 <a title="259-lsi-3" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>Author: Mengqiu Wang ; Wanxiang Che ; Christopher D. Manning</p><p>Abstract: Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We intro- duce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines.</p><p>4 0.75226086 <a title="259-lsi-4" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>Author: Thomas Schoenemann</p><p>Abstract: We derive variants of the fertility based models IBM-3 and IBM-4 that, while maintaining their zero and first order parameters, are nondeficient. Subsequently, we proceed to derive a method to compute a likely alignment and its neighbors as well as give a solution of EM training. The arising M-step energies are non-trivial and handled via projected gradient ascent. Our evaluation on gold alignments shows substantial improvements (in weighted Fmeasure) for the IBM-3. For the IBM4 there are no consistent improvements. Training the nondeficient IBM-5 in the regular way gives surprisingly good results. Using the resulting alignments for phrase- based translation systems offers no clear insights w.r.t. BLEU scores.</p><p>5 0.73999155 <a title="259-lsi-5" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>Author: Qun Liu ; Zhaopeng Tu ; Shouxun Lin</p><p>Abstract: In this paper, we propose a novel compact representation called weighted bipartite hypergraph to exploit the fertility model, which plays a critical role in word alignment. However, estimating the probabilities of rules extracted from hypergraphs is an NP-complete problem, which is computationally infeasible. Therefore, we propose a divide-and-conquer strategy by decomposing a hypergraph into a set of independent subhypergraphs. The experiments show that our approach outperforms both 1-best and n-best alignments.</p><p>6 0.72287261 <a title="259-lsi-6" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>7 0.69985813 <a title="259-lsi-7" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>8 0.64968681 <a title="259-lsi-8" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>9 0.64881945 <a title="259-lsi-9" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>10 0.62525225 <a title="259-lsi-10" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>11 0.52693135 <a title="259-lsi-11" href="./acl-2013-An_Open_Source_Toolkit_for_Quantitative_Historical_Linguistics.html">48 acl-2013-An Open Source Toolkit for Quantitative Historical Linguistics</a></p>
<p>12 0.50639379 <a title="259-lsi-12" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>13 0.4885698 <a title="259-lsi-13" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>14 0.48751962 <a title="259-lsi-14" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>15 0.47693107 <a title="259-lsi-15" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>16 0.47671843 <a title="259-lsi-16" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>17 0.47237098 <a title="259-lsi-17" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>18 0.46986338 <a title="259-lsi-18" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>19 0.45546141 <a title="259-lsi-19" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>20 0.44401705 <a title="259-lsi-20" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.053), (6, 0.096), (7, 0.24), (11, 0.063), (15, 0.014), (24, 0.054), (26, 0.041), (35, 0.056), (42, 0.039), (48, 0.052), (70, 0.036), (71, 0.01), (88, 0.034), (90, 0.052), (95, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78639472 <a title="259-lda-1" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>Author: Oliver Ferschke ; Iryna Gurevych ; Marc Rittberger</p><p>Abstract: With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. . We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reflects the situation a classifier would face in a real-life application.</p><p>same-paper 2 0.77950966 <a title="259-lda-2" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>Author: Xiaojun Quan ; Chunyu Kit ; Yan Song</p><p>Abstract: This paper studies the problem of nonmonotonic sentence alignment, motivated by the observation that coupled sentences in real bitexts do not necessarily occur monotonically, and proposes a semisupervised learning approach based on two assumptions: (1) sentences with high affinity in one language tend to have their counterparts with similar relatedness in the other; and (2) initial alignment is readily available with existing alignment techniques. They are incorporated as two constraints into a semisupervised learning framework for optimization to produce a globally optimal solution. The evaluation with realworld legal data from a comprehensive legislation corpus shows that while exist- ing alignment algorithms suffer severely from non-monotonicity, this approach can work effectively on both monotonic and non-monotonic data.</p><p>3 0.77441752 <a title="259-lda-3" href="./acl-2013-Multilingual_Affect_Polarity_and_Valence_Prediction_in_Metaphor-Rich_Texts.html">253 acl-2013-Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich Texts</a></p>
<p>Author: Zornitsa Kozareva</p><p>Abstract: Metaphor is an important way of conveying the affect of people, hence understanding how people use metaphors to convey affect is important for the communication between individuals and increases cohesion if the perceived affect of the concrete example is the same for the two individuals. Therefore, building computational models that can automatically identify the affect in metaphor-rich texts like “The team captain is a rock.”, “Time is money.”, “My lawyer is a shark.” is an important challenging problem, which has been of great interest to the research community. To solve this task, we have collected and manually annotated the affect of metaphor-rich texts for four languages. We present novel algorithms that integrate triggers for cognitive, affective, perceptual and social processes with stylistic and lexical information. By running evaluations on datasets in English, Spanish, Russian and Farsi, we show that the developed affect polarity and valence prediction technology of metaphor-rich texts is portable and works equally well for different languages.</p><p>4 0.73877603 <a title="259-lda-4" href="./acl-2013-Extracting_Events_with_Informal_Temporal_References_in_Personal_Histories_in_Online_Communities.html">153 acl-2013-Extracting Events with Informal Temporal References in Personal Histories in Online Communities</a></p>
<p>Author: Miaomiao Wen ; Zeyu Zheng ; Hyeju Jang ; Guang Xiang ; Carolyn Penstein Rose</p><p>Abstract: We present a system for extracting the dates of illness events (year and month of the event occurrence) from posting histories in the context of an online medical support community. A temporal tagger retrieves and normalizes dates mentioned informally in social media to actual month and year referents. Building on this, an event date extraction system learns to integrate the likelihood of candidate dates extracted from time-rich sentences with temporal constraints extracted from eventrelated sentences. Our integrated model achieves 89.7% of the maximum performance given the performance of the temporal expression retrieval step.</p><p>5 0.65196425 <a title="259-lda-5" href="./acl-2013-Online_Relative_Margin_Maximization_for_Statistical_Machine_Translation.html">264 acl-2013-Online Relative Margin Maximization for Statistical Machine Translation</a></p>
<p>Author: Vladimir Eidelman ; Yuval Marton ; Philip Resnik</p><p>Abstract: Recent advances in large-margin learning have shown that better generalization can be achieved by incorporating higher order information into the optimization, such as the spread of the data. However, these solutions are impractical in complex structured prediction problems such as statistical machine translation. We present an online gradient-based algorithm for relative margin maximization, which bounds the spread ofthe projected data while maximizing the margin. We evaluate our optimizer on Chinese-English and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant im- provements of 1.2-2 BLEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set.</p><p>6 0.61056638 <a title="259-lda-6" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>7 0.60928094 <a title="259-lda-7" href="./acl-2013-Modeling_Thesis_Clarity_in_Student_Essays.html">246 acl-2013-Modeling Thesis Clarity in Student Essays</a></p>
<p>8 0.60450649 <a title="259-lda-8" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>9 0.59593606 <a title="259-lda-9" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>10 0.5950579 <a title="259-lda-10" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>11 0.5918327 <a title="259-lda-11" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>12 0.58947021 <a title="259-lda-12" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>13 0.58285928 <a title="259-lda-13" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>14 0.58265024 <a title="259-lda-14" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>15 0.58253342 <a title="259-lda-15" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>16 0.58024061 <a title="259-lda-16" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>17 0.57902932 <a title="259-lda-17" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>18 0.57901198 <a title="259-lda-18" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>19 0.57858717 <a title="259-lda-19" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>20 0.57844234 <a title="259-lda-20" href="./acl-2013-Reducing_Annotation_Effort_for_Quality_Estimation_via_Active_Learning.html">300 acl-2013-Reducing Annotation Effort for Quality Estimation via Active Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
