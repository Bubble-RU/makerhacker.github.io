<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 acl-2013-A Context Free TAG Variant</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-4" href="#">acl2013-4</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 acl-2013-A Context Free TAG Variant</h1>
<br/><p>Source: <a title="acl-2013-4-pdf" href="http://aclweb.org/anthology//P/P13/P13-1030.pdf">pdf</a></p><p>Author: Ben Swanson ; Elif Yamangil ; Eugene Charniak ; Stuart Shieber</p><p>Abstract: We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient gram- mar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible.</p><p>Reference: <a title="acl-2013-4-reference" href="../acl2013_reference/acl-2013-A_Context_Free_TAG_Variant_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. [sent-5, score-0.729]
</p><p>2 In constituent-based parsing work, the prevailing technique to combat this divide between efficient models and real world data has been to selectively strengthen the dependencies in a CFG by increasing the grammar size through methods such as symbol refinement (Petrov et al. [sent-11, score-0.195]
</p><p>3 This grammar derives the sentences from a quote of Isaac Asimov’s - “I do not fear computers. [sent-31, score-0.138]
</p><p>4 We contrast these models and investigate the use of adjunction in the most probable derivations of the test corpus, demonstating the superior modeling performance of OSTAG over TSG. [sent-38, score-0.504]
</p><p>5 2  TAG and Variants  Here we provide a short history of the relevant work in related grammar formalisms, leading up to a definition of OSTAG. [sent-39, score-0.112]
</p><p>6 The rules R can be thought of as elementary trees of depth 1, which are combined by substituting a derived tree rooted at a nonterminal X at some leaf node in an elementary tree with a frontier node labeled with that same nonterminal. [sent-41, score-0.896]
</p><p>7 The derived trees rooted at the start symbol S are taken to be the trees generated by the grammar. [sent-42, score-0.252]
</p><p>8 1 Tree-Substitution Grammar By generalizing CFG to allow elementary trees in R to be of depth greater than or equal to 1, we get the Tree-Substitution Grammar. [sent-44, score-0.17]
</p><p>9 TSG remains in the family of context-free grammars, as can be easily seen by the removal of the internal nodes in all elementary trees; what is left is a CFG that generates the same language. [sent-45, score-0.143]
</p><p>10 As a reversible alternative that preserves the internal structure, annotation of each internal node with a unique index creates a large number of deterministic CFG rules that record the structure of the original elementary trees. [sent-46, score-0.437]
</p><p>11 A more compact CFG representation can be  obtained by marking each node in each elementary tree with a signature of its subtree. [sent-47, score-0.331]
</p><p>12 This transform, presented by Goodman (2003), can rein in the grammar constant G, as the crucial CFG algorithms for a sentence of length n have complexity O(Gn3). [sent-48, score-0.132]
</p><p>13 A simple probabilistic model for a TSG is a set of multinomials, one for each nonterminal in N corresponding to its possible substitutions in R. [sent-49, score-0.125]
</p><p>14 A more flexible model allows a potentially infinite number of substitution rules using a Dirichlet Process (Cohn et al. [sent-50, score-0.144]
</p><p>15 This model has proven effective for grammar induction via Markov Chain Monte Carlo (MCMC), in which TSG derivations of the training set are repeatedly sampled to find frequently occurring elementary trees. [sent-52, score-0.281]
</p><p>16 A straightforward technique for induction of a finite TSG is to perform this nonparametric induction and select the set of rules that appear in at least one sampled derivation at one or several of the final iterations. [sent-53, score-0.21]
</p><p>17 The adjunction site is circled, and the foot node of the auxiliary tree is denoted with an asterisk. [sent-56, score-0.96]
</p><p>18 The OSTAG constraint would disallow further adjunction at the bold VP node  only, as it is along the spine of the auxiliary tree. [sent-57, score-1.014]
</p><p>19 set of auxiliary trees A and the adjunction operation that governs their use. [sent-58, score-0.732]
</p><p>20 An auxiliary tree α is an elementary tree containing a single distinguished nonterminal leaf, the foot node, with the same symbol as the root of α. [sent-59, score-0.756]
</p><p>21 An auxiliary tree with root and foot node X can be adjoined into an internal node of an elementary tree labeled with X by splicing the auxiliary tree in at that internal node, as pictured in Figure 2. [sent-60, score-1.173]
</p><p>22 We refer to the path between the root and foot nodes in an auxiliary tree as the spine of the tree. [sent-61, score-0.678]
</p><p>23 As mentioned above, the added power afforded by adjunction comes at a serious price in time complexity. [sent-62, score-0.487]
</p><p>24 However, a large effort in non-probabilistic grammar induction has been performed through manual annotation with the XTAG project(Doran et al. [sent-64, score-0.152]
</p><p>25 Schabes and Waters (1995) showed that by restricting the form of the auxiliary trees in A and the set of auxiliary trees that may adjoin at particular nodes, a TAG generates only context-free languages. [sent-68, score-0.548]
</p><p>26 The TIG restriction on auxiliary trees states that the foot node must occur as either the leftmost or rightmost leaf node. [sent-69, score-0.517]
</p><p>27 This introduces an important distinction between left, right, and wrapping auxiliary trees, of which only the first two are allowed in TIG. [sent-70, score-0.332]
</p><p>28 Furthermore, TIG disallows adjunction ofleft auxiliary trees on the spines  of right auxiliary trees, and vice versa. [sent-71, score-0.919]
</p><p>29 This is to prevent the construction of wrapping auxiliary trees, whose removal is essential for the simplified complexity of TIG. [sent-72, score-0.31]
</p><p>30 , 2011; Yamangil and Shieber, 2012) were able to extend the non-parametric modeling of TSGs to TIG, providing methods for both modeling and grammar induction. [sent-76, score-0.112]
</p><p>31 We allow arbitrary initial and auxiliary trees, and place only one restriction on adjunction: we disallow adjunction at any node on the spine of an auxiliary tree below the root (though we discuss relaxing that constraint in Section 4. [sent-79, score-1.339]
</p><p>32 We refer to this variant as Off Spine TAG (OSTAG) and note that it allows the use of full wrapping rules, which are forbidden in TIG. [sent-81, score-0.184]
</p><p>33 We propose a simple but empirically effective heuristic for grammar induction for our experiments on Penn Treebank data. [sent-84, score-0.152]
</p><p>34 We take the nonterminals of the target CFG grammar to be nodes or pairs of nodes, elements of the set N +N N. [sent-90, score-0.177]
</p><p>35 Girsive ofn ntwodoe ns woditehs η and η0, we notate a target nonterminal as η(η0). [sent-92, score-0.139]
</p><p>36 Now for each tree τ and each interior node η in τ that is not on the spine of τ, with children η1 , . [sent-93, score-0.423]
</p><p>37 (2)  Rules of type (1) handle the expansion of a node not on the spine of an auxiliary tree and rules of type (2) a spinal node. [sent-97, score-0.738]
</p><p>38 In addition, to initiate adjunction at any node η0 where a tree τ with root η is adjoinable, we use a rule η0 → η(η0) (3) and for the foot node ηf of τ, we use a rule (η) → η (4) The OSTAG constraint follows immediately from the structure of the rules of type (2). [sent-98, score-1.179]
</p><p>39 Any child spine node ηs manifests as a CFG nonterminal ηs (η0). [sent-99, score-0.444]
</p><p>40 If child spine nodes themselves allowed adjunction, we would need a type (3) rule of the form ηs (η0) → ηs (η0) (η00). [sent-100, score-0.402]
</p><p>41 This arises from the spurious ambiguity between adjunction at a substitution site (before applying a type (5) rule) versus the same adjunction at the root of the substituted initial tree (after applying a type (5) rule). [sent-105, score-1.173]
</p><p>42 To avoid double-counting derivations, which can adversely effect probabilistic modeling, type (3) and type (4) rules in which the side with the unapplied symbol is a nonterminal leaf can be omitted. [sent-107, score-0.334]
</p><p>43 1 Example  The grammar of Figure 3(a) can be converted to a CFG by this method. [sent-109, score-0.112]
</p><p>44 For the initial tree α, we have the following generated rules (with nodes notated by the tree name and a Gorn number subscript): α? [sent-112, score-0.295]
</p><p>45 α1 α2  →1 →1 →1  α1 α2  α1  x  α1  y  α2  α2 For the auxiliary trees β and β? [sent-113, score-0.274]
</p><p>46 (α2) γ1(α1) γ1(α2)  →2 →4 →4  b γ1(α2) b α1  α2  The grammar of Figure 3(b) is simply a renaming of this grammar. [sent-121, score-0.112]
</p><p>47 The ability to use adjunction allows expression of the same language as an OSTAG with k + m elementary trees (Figure 4(b)). [sent-129, score-0.653]
</p><p>48 First, OSTAG allows zero adjunctions on each node on the spine below the root of an auxiliary tree, but any non-zero finite bound on the number of adjunctions allowed on-spine would similarly limit generative capacity. [sent-133, score-0.801]
</p><p>49 The tradeoff is in the grammar constant of the effective probabilistic CFG; an extension that allows k levels of on spine adjunction has a grammar constant that is  O(|N|(k+2)). [sent-134, score-0.997]
</p><p>50 Second, the OSTAG form of adjunction is consistent with the TIG form. [sent-135, score-0.458]
</p><p>51 That is, we can extend OSTAG by allowing on-spine adjunction ofleft- or right-auxiliary trees in keeping with the TIG constraints without increasing generative capacity. [sent-136, score-0.545]
</p><p>52 3  Probabilistic OSTAG  One major motivation for adherence to a contextfree grammar formalism is the ability to employ algorithms designed for probabilistic CFGs such as the CYK algorithm for parsing or the InsideOutside algorithm for grammar estimation. [sent-138, score-0.363]
</p><p>53 In this section we present a probabilistic model for an OSTAG grammar in PCFG form that can be used in such algorithms, and show that many parameters of this PCFG can be pooled or set equal to one and ignored. [sent-139, score-0.148]
</p><p>54 References to rules of types (1-5) below refer to the CFG transformation rules defined in Section 3. [sent-140, score-0.151]
</p><p>55 Furthermore, these rules employ a template in which the stored symbol appears in the left-hand side and in exactly one symbol on the right-hand side where the spine of the auxiliary tree proceeds. [sent-144, score-0.693]
</p><p>56 One deterministic rule exists for this template applied to each η, and so we may record only the template. [sent-145, score-0.14]
</p><p>57 In order to perform CYK or IO, it is not even necessary to record the index in the right-hand side where the spine continues; these algorithms fill a chart bottom up and we can  simply propagate the stored nonterminal up in the chart. [sent-146, score-0.366]
</p><p>58 CFG rules of type (4) are also deterministic and do not require parameters. [sent-147, score-0.123]
</p><p>59 All that is required is a check that a given symbol is adjoinable, which is true for all symbols except nonterminal leaves and applied symbols. [sent-149, score-0.143]
</p><p>60 To avoid this we propose the following factorization for the probabilistic expansion of an off spine node. [sent-152, score-0.25]
</p><p>61 First, a decision is made as to whether a type (1) or (3) rule will be used; this corresponds to deciding if adjunction will or will not take place at the node. [sent-153, score-0.56]
</p><p>62 If adjunction is rejected, then there is only one type (1) rule available, and so parameterization of type (1) rules is unnecessary. [sent-154, score-0.656]
</p><p>63 By conditioning the probability of adjunction on varying amounts of information about the node, alternative models can easily be defined. [sent-156, score-0.458]
</p><p>64 We also applied the annotation from Klein and Manning (2003) that appends “-U” to each nonterminal node with a single child, drastically reducing the presence of looping unary chains. [sent-160, score-0.199]
</p><p>65 Designed to facilitate sister adjunction, we define our binarization scheme by example in which the added nodes, indicated by @,  record both the parent and head child of the rule. [sent-163, score-0.147]
</p><p>66 NP @NN-NP @NN-NP DT  SBAR  @NN-NP JJ  NN  A compact TSG can be obtained automatically using the MCMC grammar induction technique of Cohn and Blunsom (2010), retaining all TSG rules that appear in at least one derivation in after 1000 iterations of sampling. [sent-164, score-0.297]
</p><p>67 For the OSTAG models, we list the number of adjunctions in the MPD of the full test set, as well as the number of wrapping adjunctions. [sent-177, score-0.213]
</p><p>68 ble auxiliary root and foot node pairs it contains. [sent-178, score-0.442]
</p><p>69 We also include the TSG rule left behind when the adjunction of this auxiliary tree is removed. [sent-180, score-0.814]
</p><p>70 With our full set of initial and auxiliary trees, we use EM and the PCFG reduction described above to estimate the parameters of an OSTAG. [sent-182, score-0.218]
</p><p>71 We investigate three models for the probability of adjunction at a node. [sent-183, score-0.458]
</p><p>72 The second employs more parameters, conditioning on both the node’s symbol and the symbol of its leftmost child (OSTAG2). [sent-185, score-0.139]
</p><p>73 Furthermore, the various parameterizations of adjunction with OSTAG indicate that, at least in the case of the Penn Treebank, the finer grained modeling of a full table of adjunction probabilities for each Goodman index OSTAG3  overcomes the danger of sparse data estimates. [sent-189, score-0.936]
</p><p>74 Not only does such a model lead to better parsing performance, but it uses adjunction more extensively than its more lightly parameterized alternatives. [sent-190, score-0.487]
</p><p>75 2 to allow any finite number of on-spine adjunctions without sacrificing contextfree form. [sent-196, score-0.161]
</p><p>76 However, the increase to the grammar constant quickly makes parsing with such models an arduous task. [sent-197, score-0.161]
</p><p>77 To determine the effect of such a relaxation, we allow a single level of on-spine adjunction using the adjunction model of OSTAG1, and estimate this model with EM on the training data. [sent-198, score-0.916]
</p><p>78 We parse sentences of length 40 or less in section 23 and observe that on-spine adjunction is never used in the MPD parses. [sent-199, score-0.458]
</p><p>79 As an artifact of the English language, the majority have their foot node on the left spine and would also be usable by TIG, and so we discuss the instances of wrapping auxiliary trees in these derivations that are uniquely available to OSTAG. [sent-202, score-0.873]
</p><p>80 We remove binarization for clarity and denote the foot node with an asterisk. [sent-203, score-0.236]
</p><p>81 A frequent use of wrapping adjunction is to coordinate symbols such as quotes, parentheses, and dashes on both sides of a noun phrase. [sent-204, score-0.581]
</p><p>82 One common wrapping auxiliary tree in our experiments is NP “  NP*  ”  PP  This is used frequently in the news text of the Wall Street Journal for reported speech when avoiding a full quotation. [sent-205, score-0.409]
</p><p>83 Another frequent wrapping rule, shown below, allows direct coordination between the contents of an appositive with the rest of the sentence. [sent-208, score-0.187]
</p><p>84 The wrapping rule allows us to coordinate the verb “fell” with the pattern “X %” instead of 156. [sent-213, score-0.218]
</p><p>85 These rules highlight the linguistic intuitions that back TAG; if their adjunction were undone, the remaining derivation would be a valid sentence that simply lacks the modifying structure of the auxiliary tree. [sent-215, score-0.751]
</p><p>86 However, the MPD parses reveal that not all useful adjunctions conform to this paradigm, and that left-auxiliary trees that are not used for sister adjunction are susceptible to this behavior. [sent-216, score-0.688]
</p><p>87 The most common such tree is used to create noun phrases such as P&G;’s share of [the Japanese market] the House’s repeal of [a law] Apple’s family of [Macintosh Computers] Canada’s output of [crude oil] by adjoining the shared unbracketed syntax onto the NP dominating the bracketed text. [sent-217, score-0.156]
</p><p>88 Furthermore, in some cases removing the adjunction can leave a grammatically incorrect sentence, as in the third example where the noun phrase changes plurality. [sent-219, score-0.458]
</p><p>89 While our grammar induction method is a crude (but effective) heuristic, we can still highlight the qualities of the more important auxiliary trees by examining aggregate statistics over the MPD parses, shown in Figure 6. [sent-220, score-0.426]
</p><p>90 The use of leftauxiliary trees for sister adjunction is a clear trend, as is the predominant use of right-auxiliary trees for the complementary set of “regular” adjunctions, which is to be expected in a right branching language such as English. [sent-221, score-0.685]
</p><p>91 The statistics also All Total Sister Lex FLex  Wrap  Right  Left  3585 (1374) 2851 (1180) 2244 (990) 1028 (558)  41 (26) 17 (11) 28 (19) 7 (2)  1698 (518) 1109 (400) 894 (299) 835 (472)  1846 (830) 1725 (769) 1322 (672) 186 (84)  Figure 6: Statistics for MPD auxiliary trees using OSTAG3. [sent-222, score-0.274]
</p><p>92 The columns indicate type of auxiliary tree and the rows correspond respectively to the full set found in the MPD, those that perform sister adjunction, those that are lexicalized, and those that are fully lexicalized. [sent-223, score-0.371]
</p><p>93 Each cell shows the number of tokens followed by the number of types of auxiliary tree that fit its conditions. [sent-224, score-0.286]
</p><p>94 6  Conclusion  The OSTAG variant of Tree-Adjoining Grammar is a simple weakly context-free formalism that still provides for all types of adjunction and is a bit more concise than TSG (quadratically so). [sent-226, score-0.521]
</p><p>95 OSTAG provides an alternative to TIG as a context-free TAG variant that offers wrapping adjunction in exchange for recursive left/right spine adjunction. [sent-228, score-0.831]
</p><p>96 The most important direction of future work for OSTAG is the development of a principled grammar induction model, perhaps using the same techniques that have been successfully applied to TSG and TIG. [sent-231, score-0.152]
</p><p>97 Our system performs the CFG transform described above and optionally employs coarse to fine pruning and relaxed (finite) limits on the number of spine adjunctions. [sent-233, score-0.214]
</p><p>98 As a TSG is simply a TAG without adjunction rules, our parser can easily be used as a TSG estimator and parser as well. [sent-234, score-0.458]
</p><p>99 An empirical evaluation of probabilistic lexicalized tree insertion grammars. [sent-280, score-0.175]
</p><p>100 Tree insertion grammar: a cubic-time, parsable formalism that lexicalizes context-free grammar without changing the trees produced. [sent-323, score-0.266]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ostag', 0.553), ('adjunction', 0.458), ('cfg', 0.255), ('tsg', 0.223), ('spine', 0.214), ('auxiliary', 0.187), ('mpd', 0.131), ('wrapping', 0.123), ('tig', 0.119), ('grammar', 0.112), ('node', 0.11), ('foot', 0.106), ('tree', 0.099), ('adjunctions', 0.09), ('nonterminal', 0.089), ('trees', 0.087), ('elementary', 0.083), ('tag', 0.082), ('rule', 0.07), ('rules', 0.064), ('np', 0.061), ('schabes', 0.061), ('gorn', 0.058), ('pretzels', 0.058), ('cfgs', 0.056), ('substitution', 0.055), ('symbol', 0.054), ('sister', 0.053), ('contextfree', 0.047), ('derivations', 0.046), ('cyk', 0.045), ('adjoinable', 0.044), ('joshi', 0.043), ('record', 0.043), ('pcfg', 0.042), ('derivation', 0.042), ('grammars', 0.041), ('vp', 0.041), ('induction', 0.04), ('insertion', 0.04), ('compact', 0.039), ('root', 0.039), ('appositive', 0.039), ('shindo', 0.039), ('adjoining', 0.037), ('probabilistic', 0.036), ('reversible', 0.036), ('yamangil', 0.036), ('variant', 0.036), ('shieber', 0.034), ('cohn', 0.033), ('nodes', 0.033), ('nonterminals', 0.032), ('type', 0.032), ('goodman', 0.032), ('reduction', 0.031), ('child', 0.031), ('afforded', 0.029), ('treesubstitution', 0.029), ('parsing', 0.029), ('deterministic', 0.027), ('formalism', 0.027), ('leaf', 0.027), ('internal', 0.027), ('fear', 0.026), ('hn', 0.026), ('elif', 0.026), ('lari', 0.026), ('notate', 0.026), ('providence', 0.026), ('wrap', 0.026), ('xtag', 0.026), ('charniak', 0.026), ('yves', 0.025), ('allows', 0.025), ('finite', 0.024), ('rooted', 0.024), ('disallow', 0.024), ('insideoutside', 0.024), ('isaac', 0.024), ('mohri', 0.024), ('ofn', 0.024), ('transformation', 0.023), ('prp', 0.022), ('treeadjoining', 0.022), ('allowed', 0.022), ('aravind', 0.022), ('penn', 0.021), ('em', 0.021), ('constraint', 0.021), ('doran', 0.021), ('frontier', 0.021), ('exactly', 0.021), ('dominating', 0.02), ('mcmc', 0.02), ('pal', 0.02), ('constant', 0.02), ('index', 0.02), ('bernoulli', 0.02), ('binarization', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="4-tfidf-1" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<p>Author: Ben Swanson ; Elif Yamangil ; Eugene Charniak ; Stuart Shieber</p><p>Abstract: We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient gram- mar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible.</p><p>2 0.53218663 <a title="4-tfidf-2" href="./acl-2013-Nonparametric_Bayesian_Inference_and_Efficient_Parsing_for_Tree-adjoining_Grammars.html">261 acl-2013-Nonparametric Bayesian Inference and Efficient Parsing for Tree-adjoining Grammars</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing. Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines.</p><p>3 0.3011601 <a title="4-tfidf-3" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>Author: Leon Bergen ; Edward Gibson ; Timothy J. O'Donnell</p><p>Abstract: We present a model for inducing sentential argument structure, which distinguishes arguments from optional modifiers. We use this model to study whether representing an argument/modifier distinction helps in learning argument structure, and whether a linguistically-natural argument/modifier distinction can be induced from distributional data alone. Our results provide evidence for both hypotheses.</p><p>4 0.20598443 <a title="4-tfidf-4" href="./acl-2013-Transfer_Learning_for_Constituency-Based_Grammars.html">357 acl-2013-Transfer Learning for Constituency-Based Grammars</a></p>
<p>Author: Yuan Zhang ; Regina Barzilay ; Amir Globerson</p><p>Abstract: In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing trans- fer between the formalisms, while preserving parsing efficiency. We evaluate our approach on three constituency-based grammars CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations.1 —</p><p>5 0.14210929 <a title="4-tfidf-5" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>Author: Matt Post ; Shane Bergsma</p><p>Abstract: Syntactic features are useful for many text classification tasks. Among these, tree kernels (Collins and Duffy, 2001) have been perhaps the most robust and effective syntactic tool, appealing for their empirical success, but also because they do not require an answer to the difficult question of which tree features to use for a given task. We compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly avail- able tools) , we suggest they should always be included as baseline comparisons in tree kernel method evaluations.</p><p>6 0.13441397 <a title="4-tfidf-6" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>7 0.10822773 <a title="4-tfidf-7" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>8 0.10249523 <a title="4-tfidf-8" href="./acl-2013-The_effect_of_non-tightness_on_Bayesian_estimation_of_PCFGs.html">348 acl-2013-The effect of non-tightness on Bayesian estimation of PCFGs</a></p>
<p>9 0.087153822 <a title="4-tfidf-9" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>10 0.081370711 <a title="4-tfidf-10" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>11 0.079074122 <a title="4-tfidf-11" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>12 0.075603947 <a title="4-tfidf-12" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>13 0.071168795 <a title="4-tfidf-13" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>14 0.07061106 <a title="4-tfidf-14" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>15 0.069736712 <a title="4-tfidf-15" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>16 0.064024515 <a title="4-tfidf-16" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>17 0.061117228 <a title="4-tfidf-17" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>18 0.059017751 <a title="4-tfidf-18" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>19 0.056587316 <a title="4-tfidf-19" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>20 0.055695906 <a title="4-tfidf-20" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.138), (1, -0.091), (2, -0.101), (3, -0.01), (4, -0.187), (5, 0.046), (6, 0.117), (7, -0.023), (8, 0.019), (9, -0.03), (10, 0.059), (11, 0.067), (12, 0.14), (13, -0.097), (14, -0.112), (15, -0.17), (16, 0.248), (17, 0.242), (18, -0.145), (19, 0.068), (20, 0.094), (21, 0.039), (22, 0.236), (23, 0.069), (24, -0.155), (25, -0.158), (26, 0.043), (27, -0.094), (28, 0.139), (29, 0.066), (30, -0.112), (31, -0.028), (32, -0.005), (33, -0.079), (34, -0.017), (35, -0.033), (36, 0.071), (37, 0.139), (38, 0.031), (39, -0.129), (40, 0.071), (41, -0.134), (42, -0.001), (43, 0.064), (44, -0.11), (45, 0.035), (46, 0.1), (47, -0.029), (48, 0.06), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9634465 <a title="4-lsi-1" href="./acl-2013-Nonparametric_Bayesian_Inference_and_Efficient_Parsing_for_Tree-adjoining_Grammars.html">261 acl-2013-Nonparametric Bayesian Inference and Efficient Parsing for Tree-adjoining Grammars</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing. Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines.</p><p>same-paper 2 0.95201886 <a title="4-lsi-2" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<p>Author: Ben Swanson ; Elif Yamangil ; Eugene Charniak ; Stuart Shieber</p><p>Abstract: We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient gram- mar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible.</p><p>3 0.82353532 <a title="4-lsi-3" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>Author: Leon Bergen ; Edward Gibson ; Timothy J. O'Donnell</p><p>Abstract: We present a model for inducing sentential argument structure, which distinguishes arguments from optional modifiers. We use this model to study whether representing an argument/modifier distinction helps in learning argument structure, and whether a linguistically-natural argument/modifier distinction can be induced from distributional data alone. Our results provide evidence for both hypotheses.</p><p>4 0.50308937 <a title="4-lsi-4" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>Author: Matthias Buchse ; Alexander Koller ; Heiko Vogler</p><p>Abstract: Binarization ofgrammars is crucial for improving the complexity and performance of parsing and translation. We present a versatile binarization algorithm that can be tailored to a number of grammar formalisms by simply varying a formal parameter. We apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation.</p><p>5 0.45442349 <a title="4-lsi-5" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>Author: David Chiang ; Jacob Andreas ; Daniel Bauer ; Karl Moritz Hermann ; Bevan Jones ; Kevin Knight</p><p>Abstract: Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing.</p><p>6 0.43971777 <a title="4-lsi-6" href="./acl-2013-The_effect_of_non-tightness_on_Bayesian_estimation_of_PCFGs.html">348 acl-2013-The effect of non-tightness on Bayesian estimation of PCFGs</a></p>
<p>7 0.42439693 <a title="4-lsi-7" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>8 0.3903352 <a title="4-lsi-8" href="./acl-2013-Transfer_Learning_for_Constituency-Based_Grammars.html">357 acl-2013-Transfer Learning for Constituency-Based Grammars</a></p>
<p>9 0.3482258 <a title="4-lsi-9" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>10 0.32298303 <a title="4-lsi-10" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>11 0.30091637 <a title="4-lsi-11" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>12 0.29177067 <a title="4-lsi-12" href="./acl-2013-Enlisting_the_Ghost%3A_Modeling_Empty_Categories_for_Machine_Translation.html">137 acl-2013-Enlisting the Ghost: Modeling Empty Categories for Machine Translation</a></p>
<p>13 0.2850922 <a title="4-lsi-13" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>14 0.27917963 <a title="4-lsi-14" href="./acl-2013-Fluid_Construction_Grammar_for_Historical_and_Evolutionary_Linguistics.html">161 acl-2013-Fluid Construction Grammar for Historical and Evolutionary Linguistics</a></p>
<p>15 0.27901655 <a title="4-lsi-15" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>16 0.26949838 <a title="4-lsi-16" href="./acl-2013-Reconstructing_an_Indo-European_Family_Tree_from_Non-native_English_Texts.html">299 acl-2013-Reconstructing an Indo-European Family Tree from Non-native English Texts</a></p>
<p>17 0.26634952 <a title="4-lsi-17" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>18 0.26082018 <a title="4-lsi-18" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>19 0.25568256 <a title="4-lsi-19" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>20 0.25236356 <a title="4-lsi-20" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.04), (2, 0.118), (6, 0.019), (11, 0.087), (14, 0.042), (19, 0.135), (24, 0.025), (26, 0.06), (28, 0.011), (35, 0.097), (42, 0.036), (48, 0.037), (70, 0.056), (76, 0.017), (88, 0.03), (90, 0.047), (95, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84755433 <a title="4-lda-1" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<p>Author: Ben Swanson ; Elif Yamangil ; Eugene Charniak ; Stuart Shieber</p><p>Abstract: We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient gram- mar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible.</p><p>2 0.82463425 <a title="4-lda-2" href="./acl-2013-Nonparametric_Bayesian_Inference_and_Efficient_Parsing_for_Tree-adjoining_Grammars.html">261 acl-2013-Nonparametric Bayesian Inference and Efficient Parsing for Tree-adjoining Grammars</a></p>
<p>Author: Elif Yamangil ; Stuart M. Shieber</p><p>Abstract: In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing. Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines.</p><p>3 0.77741265 <a title="4-lda-3" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>Author: Michael Speriosu ; Jason Baldridge</p><p>Abstract: Toponym resolvers identify the specific locations referred to by ambiguous placenames in text. Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. This paper shows that text-driven disambiguation for toponyms is far more effective. We exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles.</p><p>4 0.76050532 <a title="4-lda-4" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>Author: Maryam Habibi ; Andrei Popescu-Belis</p><p>Abstract: A new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. Inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. The method is evaluated on excerpts of the Fisher and AMI corpora, using a crowdsourcing platform to elicit comparative relevance judgments. The results demonstrate that the method outperforms two competitive baselines.</p><p>5 0.745206 <a title="4-lda-5" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>Author: Xiaoming Lu ; Lei Xie ; Cheung-Chi Leung ; Bin Ma ; Haizhou Li</p><p>Abstract: We present an efficient approach for broadcast news story segmentation using a manifold learning algorithm on latent topic distributions. The latent topic distribution estimated by Latent Dirichlet Allocation (LDA) is used to represent each text block. We employ Laplacian Eigenmaps (LE) to project the latent topic distributions into low-dimensional semantic representations while preserving the intrinsic local geometric structure. We evaluate two approaches employing LDA and probabilistic latent semantic analysis (PLSA) distributions respectively. The effects of different amounts of training data and different numbers of latent topics on the two approaches are studied. Experimental re- sults show that our proposed LDA-based approach can outperform the corresponding PLSA-based approach. The proposed approach provides the best performance with the highest F1-measure of 0.7860.</p><p>6 0.73428726 <a title="4-lda-6" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>7 0.73373717 <a title="4-lda-7" href="./acl-2013-Automatic_Coupling_of_Answer_Extraction_and_Information_Retrieval.html">60 acl-2013-Automatic Coupling of Answer Extraction and Information Retrieval</a></p>
<p>8 0.71619004 <a title="4-lda-8" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>9 0.70648807 <a title="4-lda-9" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>10 0.69295365 <a title="4-lda-10" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>11 0.66192347 <a title="4-lda-11" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>12 0.6588012 <a title="4-lda-12" href="./acl-2013-Modeling_Human_Inference_Process_for_Textual_Entailment_Recognition.html">245 acl-2013-Modeling Human Inference Process for Textual Entailment Recognition</a></p>
<p>13 0.65781224 <a title="4-lda-13" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>14 0.6537717 <a title="4-lda-14" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<p>15 0.65283722 <a title="4-lda-15" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>16 0.65000725 <a title="4-lda-16" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>17 0.64626122 <a title="4-lda-17" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>18 0.64574891 <a title="4-lda-18" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>19 0.64568484 <a title="4-lda-19" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>20 0.64546096 <a title="4-lda-20" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
