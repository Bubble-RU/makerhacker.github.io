<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-110" href="#">acl2013-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</h1>
<br/><p>Source: <a title="acl-2013-110-pdf" href="http://aclweb.org/anthology//P/P13/P13-3025.pdf">pdf</a></p><p>Author: Rudolf Rosa ; David Marecek ; Ales Tamchyna</p><p>Abstract: Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</p><p>Reference: <a title="acl-2013-110-reference" href="../acl2013_reference/acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 c z  Abstract Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. [sent-4, score-0.236]
</p><p>2 It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. [sent-5, score-0.594]
</p><p>3 On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge. [sent-6, score-0.506]
</p><p>4 1 Introduction Statistical machine translation (SMT) is the current state-of-the-art approach to machine translation see e. [sent-7, score-0.248]
</p><p>5 How–  ever, its outputs are still typically significantly worse than human translations, containing various types of errors (Bojar, 2011b), both in lexical choices and in grammar. [sent-11, score-0.123]
</p><p>6 Bojar (201 1a), incorporating deep linguistic knowledge directly into a translation system is often hard to do, and seldom leads to an improvement of translation output quality. [sent-14, score-0.27]
</p><p>7 It has been shown that it is often easier to correct the machine translation outputs in a second-stage post-processing, which is usually referred to as automatic post-editing. [sent-15, score-0.234]
</p><p>8 Several types of errors can be fixed by employing rule-based post-editing (Rosa et al. [sent-16, score-0.057]
</p><p>9 , 2012b), which can be seen as being orthogonal to the statistical methods employed in SMT and thus can capture different linguistic phenomena easily. [sent-17, score-0.056]
</p><p>10 But there are still other errors that cannot be corrected with hand-written rules, as there exist many linguistic phenomena that can never be fully described manually they need to be handled statistically by automatically analyzing large-scale text corpora. [sent-18, score-0.057]
</p><p>11 For Czech, the morphological cases of the nouns are also indicated. [sent-20, score-0.095]
</p><p>12 there is very little successful research in statistical post-editing (SPE) of SMT (see Section 2). [sent-24, score-0.056]
</p><p>13 In our paper, we describe a statistical approach  to correcting one particular type of English-toCzech SMT errors errors in the verb-noun valency. [sent-25, score-0.2]
</p><p>14 The term valency stands for the way in which verbs and their arguments are used together, usually together with prepositions and morphological cases, and is described in Section 4. [sent-26, score-0.551]
</p><p>15 Several examples of the valency of the English verb ‘to go’ and the corresponding Czech verb ‘j´ ıt’ are shown in Table 1. [sent-27, score-0.521]
</p><p>16 An example of Moses making a valency error is translating the sentence ‘The government spends on the middle schools. [sent-30, score-0.536]
</p><p>17 As shown in Table 2, Moses translates the sentence incorrectly, making an error in the valency of the ‘utr a´cet skola’ (‘spend school’) pair. [sent-32, score-0.448]
</p><p>18 The missing preposition changes the meaning dramatically, as the verb ‘utr a´cet’ is pol–  –  –  172  Sofia, BuPrlgoacreiead, iAngusgu osft 4h-e9 A 2C01L3 S. [sent-33, score-0.167]
</p><p>19 c d2en0t1 3Re Ases aorc hiat Wio nrk fsohro Cp,om papguesta 1ti7o2n–a1l7 L9in,guistics ysemous and can mean ‘to spend (esp. [sent-35, score-0.077]
</p><p>20 Our approach is to use deep linguistic analysis to automatically determine the structure of each sentence, and to detect and correct valency errors using a simple statistical valency model. [sent-38, score-1.067]
</p><p>21 2  Related Work  The first reported results of automatic post-editing of machine translation outputs are (Simard et al. [sent-42, score-0.19]
</p><p>22 , 2007) where the authors successfully performed statistical post-editing (SPE) of rule-based machine translation outputs. [sent-43, score-0.18]
</p><p>23 To perform the postediting, they used a phrase-based SMT system in a monolingual setting, trained on the outputs of the rule-based system as the source and the humanprovided reference translations as the target, to achieve massive translation quality improvements. [sent-44, score-0.163]
</p><p>24 The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. [sent-71, score-0.071]
</p><p>25 Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. [sent-72, score-0.044]
</p><p>26 We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than a given threshold. [sent-80, score-0.034]
</p><p>27 , editing on shallow-syntax (described in this paper) operating on deep-syntax  2012b) performs rule-based postdependency trees, while Deepfix is a statistical post-editing system dependency trees. [sent-90, score-0.056]
</p><p>28 We therefore proceed with a more complex approach which relies on deep linguistic knowledge. [sent-93, score-0.076]
</p><p>29 1 Tectogrammatical dependency trees  Tectogrammatical trees are deep syntactic dependency trees based on the Functional Generative Description (Sgall et al. [sent-95, score-0.202]
</p><p>30 Each node in a tectogrammatical tree corresponds to a content word, such as a noun, a full verb or an adjective; the node consists of the lemma of the content word and several other attributes. [sent-97, score-0.247]
</p><p>31 Functional words, such as prepositions or auxiliary verbs, are not directly present in the tectogrammatical tree, but are represented by attributes of the respective content nodes. [sent-98, score-0.2]
</p><p>32 See Figure 1for an example of two tectogrammatical trees (for simplicity, most of the attributes are not shown). [sent-99, score-0.204]
</p><p>33 In our work, we only use one of the many attributes of tectogrammatical nodes, called formeme (Du sˇek et al. [sent-100, score-0.537]
</p><p>34 A formeme is a string representation of selected morpho-syntactic features of the content word and selected auxiliary words that belong to the content word, devised to be used as a simple and efficient representation of the node. [sent-102, score-0.375]
</p><p>35 A noun formeme, which we are most interested in, consists of three parts (examples taken from  Figure 1): 1. [sent-103, score-0.087]
</p><p>36 The preposition if the noun has one (empty otherwise), as in n :on+X or n : za+4 . [sent-106, score-0.202]
</p><p>37 Isn t case ojfe a noun accompanied by a preposition, the third part is always X, as in n :on+X. [sent-110, score-0.087]
</p><p>38 • In Czech, it denotes the morphologiIcnal case ,of i tth dee noun, represented by its number (from 1 to 7 as there are seven cases in Czech), as in n :1and n : z a+4 . [sent-111, score-0.03]
</p><p>39 t-tree zone=en  t-tree zone=cs  spend  utrácet  gno:v sm:uefbirn dj mle ntsnc:ohno+ Xlvnl:áv1ds:fatiřnedšn k:ízoal +4 adj:attr  adj:attr  Figure 1: Tectogrammatical trees for the sentence ‘The government spends on the middle schools. [sent-112, score-0.238]
</p><p>40 ’ ; only lemmas and formemes of the nodes are shown. [sent-114, score-0.199]
</p><p>41 Adjectives and nouns can also have the adj : att r and n :att r formemes, respectively, meaning that the node is in morphological agreement with its parent. [sent-115, score-0.159]
</p><p>42 This is especially important in Czech, where this means that the word bears the same morphological case as its parent node. [sent-116, score-0.122]
</p><p>43 2 Valency The notion of valency (Tesni` ere and Fourquet, 1959) is semantic, but it is closely linked to syntax. [sent-118, score-0.443]
</p><p>44 In the theory of valency, each verb has one  or more valency frames. [sent-119, score-0.469]
</p><p>45 Each valency frame describes a meaning of the verb, together with arguments (usually nouns) that the verb must or can have, and each of the arguments has one or several fixed forms in which it must appear. [sent-120, score-0.58]
</p><p>46 These forms can typically be specified by prepositions and morphological cases to be used with the noun, and thus can be easily expressed by formemes. [sent-121, score-0.133]
</p><p>47 For example, the verb ‘to go’, shown in Table 1, has a valency frame that can be expressed as n : sub j go n :t o+X, meaning that the subject goes to some place. [sent-122, score-0.518]
</p><p>48 The valency frames of the verbs ‘spend’ and ‘utr a´cet’ in Figure 1 can be written as n : sub j spend n : on+X and n : 1 ut r ´acet n : z a+4 ; the subject (in Czech this is a noun in nominative case) spends on an object (in Czech, the preposition ‘za’ plus a noun in accusative case). [sent-123, score-0.871]
</p><p>49 the parent node can be either a verb or a noun, while the arguments are always nouns. [sent-126, score-0.114]
</p><p>50 Practice has proven this extension to  be useful, although the majority of the corrections 174  performed are still of the verb-noun valency type. [sent-127, score-0.417]
</p><p>51 Still, we keep the traditional notion of verb-noun valency throughout the text, especially to be able to always refer to the parent as “the verb” and to the child as “the noun”. [sent-128, score-0.448]
</p><p>52 1 Valency models To be able to detect and correct valency errors, we created statistical models of verb-noun valency. [sent-130, score-0.517]
</p><p>53 We model the conditional probability of the noun argument formeme based on several features ofthe verb-noun pair. [sent-131, score-0.493]
</p><p>54 2 Deepfix We introduce a new statistical post-editing system, Deepfix, whose input is a pair of an English sentence and its Czech machine translation, and the output is the Czech sentence with verb-noun valency errors corrected. [sent-138, score-0.619]
</p><p>55 the sentences are tokenized, tagged and lemmatized (a lemma and a morphological tag is assigned to each word) 2. [sent-140, score-0.098]
</p><p>56 deep-syntax dependency parse trees of the sentences are built, the nodes in the trees are labelled with formemes 4. [sent-142, score-0.283]
</p><p>57 improbable noun formemes are replaced with correct formemes according to the valency model 5. [sent-143, score-0.946]
</p><p>58 the words are regenerated according to the new formemes  6. [sent-144, score-0.238]
</p><p>59 the regenerating continues recursively to children of regenerated nodes if they are in morphological agreement with their parents (which is typical for adjectives) To decide whether the formeme of the noun is incorrect, we query the valency model for all possible formemes and their probabilities. [sent-145, score-1.209]
</p><p>60 If an alternative formeme probability exceeds a fixed threshold, we assume that the original formeme is incorrect, and we use the alternative formeme instead. [sent-146, score-1.125]
</p><p>61 For our example sentence, ‘The government spends on the middle schools. [sent-147, score-0.088]
</p><p>62 ’, we query the model (2) and get the following probabilities: –  •  •  P(n:4 | utr a´cet, skola, n:on+X) = 0. [sent-149, score-0.177]
</p><p>63 07 (the original formeme) P(n:za+4 | utr a´cet, skola, n:on+X) = 0. [sent-150, score-0.177]
</p><p>64 89 (the zma+os4t probable formeme)  The threshold for this change type is 0. [sent-151, score-0.036]
</p><p>65 86, is exceeded by the n : za+4 formeme and thus the  change is performed: skoly’ . [sent-152, score-0.411]
</p><p>66 We distinguish changes where only the morphological case of the noun is changed from changes to the preposition. [sent-156, score-0.152]
</p><p>67 There are three possible types of a change to a preposition: switching one preposition to another, adding a new preposition, and removing an existing preposition. [sent-157, score-0.151]
</p><p>68 –8 64 change to the preposition can also involve chang-  ing the morphological case of the noun, as each preposition typically requires a certain morphological case. [sent-162, score-0.396]
</p><p>69 For some combinations of a change type and a model, as in case of the preposition removing, we never perform a fix because we observed that it nearly never improves the translation. [sent-163, score-0.151]
</p><p>70 , if a verb-noun pair can be correct both with and without a preposition, the preposition-less variant is usually much more frequent than the prepositional variant (and thus is assigned a much higher probability by the model). [sent-166, score-0.112]
</p><p>71 The Czech sentence is analyzed by the Featurama tagger2 and the RUR parser (Rosa et al. [sent-173, score-0.031]
</p><p>72 For evaluation, we used outputs of a state-of-the-art SMT system, Moses (Koehn et al. [sent-179, score-0.066]
</p><p>73 net /  2007), tuned for English-to-Czech translation (Bojar et al. [sent-182, score-0.097]
</p><p>74 We used the WMT10 dataset  and its Moses translation as our development data to tune the thresholds. [sent-184, score-0.097]
</p><p>75 The improvements in automatic scores are low but consistently positive, which suggests that Deepfix does improve the translation quality. [sent-188, score-0.097]
</p><p>76 For 60 sentence pairs, both of the annotators were able to select which sentence is better, i. [sent-195, score-0.092]
</p><p>77 The inter-annotator agreement on these 60 sentence pairs was –  incorrect. [sent-198, score-0.058]
</p><p>78 the annotators did not know which sentence is before Deepfix and which is after Deepfix. [sent-202, score-0.061]
</p><p>79 4If all 100 sentence pairs are taken into account, requiring that the annotators also agree on the “indefinite” marker, the inter-annotator agreement is only 65%. [sent-204, score-0.088]
</p><p>80 This suggests that  deciding whether the translation quality differs significantly is much harder than deciding which translation is of a higher quality. [sent-205, score-0.194]
</p><p>81 *Please  note  on outputs  that WMT10  was  used  of the  as  Moses system on  WMT10, WMT1 1 and  the development dataset. [sent-216, score-0.066]
</p><p>82 3 Discussion When a formeme change was performed, it was usually either positive or at least not harmful (substituting one correct variant for another correct variant). [sent-219, score-0.533]
</p><p>83 However, we also observed a substantial amount of cases where the change of the formeme was incorrect. [sent-220, score-0.441]
</p><p>84 This is to be expected, as the Czech sentence is often erroneous, whereas the NLP tools that we used are trained on correct sentences; in many cases, it is not even clear what a correct analysis of an incorrect sentence should be. [sent-222, score-0.189]
</p><p>85 7  Conclusion and Future Work  On the English-Czech pair, we have shown that statistical post-editing of statistical machine translation outputs is possible, even when translating from a morphologically poor to a morphologically rich language, if it is grounded by deep linguistic knowledge. [sent-223, score-0.378]
</p><p>86 With our tool, Deepfix, we have achieved improvements on outputs of two state-of-the-art SMT systems by correcting verbnoun valency errors, using two simple probabilistic valency models computed on large-scale data. [sent-224, score-0.93]
</p><p>87 We encountered many cases where the performance of Deepfix was hindered by errors of the underlying tools, especially the taggers, the parsers and the aligner. [sent-226, score-0.087]
</p><p>88 , 2012a), which is partially adapted to SMT outputs parsing, lead to a reduction of the number of parser errors, we find the approach of adapting the tools for this specific kind of data to be promising. [sent-228, score-0.066]
</p><p>89 We believe that our method can be adapted to other language pairs, provided that there is a pipeline that can analyze at least the target language up to deep syntactic trees. [sent-229, score-0.076]
</p><p>90 Because we only use a small subset of information that a tectogrammatical tree provides, it is sufficient to use only simplified tectogrammatical trees. [sent-230, score-0.324]
</p><p>91 Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. [sent-262, score-0.207]
</p><p>92 Findings of the 2011 workshop on statistical machine translation. [sent-267, score-0.083]
</p><p>93 Findings of the 2012 workshop on statistical machine translation. [sent-272, score-0.083]
</p><p>94 Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. [sent-277, score-0.124]
</p><p>95 A sys-  tematic comparison of various statistical alignment models. [sent-302, score-0.056]
</p><p>96 Exploring different representational units in English-to-Turkish statistical machine translation. [sent-306, score-0.083]
</p><p>97 Using parallel features in parsing of machine-translated sentences for correction of grammatical errors. [sent-320, score-0.037]
</p><p>98 DEPFIX: A system for automatic correction of Czech MT outputs. [sent-325, score-0.037]
</p><p>99 The meaning of the sentence in its semantic and pragmatic aspects. [sent-330, score-0.031]
</p><p>100 The best of two worlds: Cooperation of statistical and rulebased taggers for Czech. [sent-339, score-0.056]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('valency', 0.417), ('deepfix', 0.375), ('formeme', 0.375), ('formemes', 0.199), ('utr', 0.177), ('tectogrammatical', 0.162), ('spe', 0.162), ('rosa', 0.144), ('czech', 0.136), ('smt', 0.128), ('fen', 0.117), ('rej', 0.116), ('preposition', 0.115), ('za', 0.113), ('ond', 0.102), ('cet', 0.102), ('translation', 0.097), ('lv', 0.089), ('echara', 0.088), ('spends', 0.088), ('noun', 0.087), ('mare', 0.085), ('spend', 0.077), ('deep', 0.076), ('bojar', 0.072), ('moses', 0.072), ('outputs', 0.066), ('depfix', 0.066), ('skola', 0.066), ('morphological', 0.065), ('popel', 0.06), ('oflazer', 0.06), ('abokrtsk', 0.06), ('czeng', 0.059), ('rudolf', 0.059), ('errors', 0.057), ('du', 0.057), ('statistical', 0.056), ('ek', 0.055), ('verb', 0.052), ('sek', 0.051), ('frame', 0.049), ('simard', 0.046), ('zden', 0.046), ('indefinite', 0.046), ('correct', 0.044), ('featurama', 0.044), ('redn', 0.044), ('rur', 0.044), ('skoly', 0.044), ('spoustov', 0.044), ('koehn', 0.043), ('trees', 0.042), ('bleu', 0.04), ('martin', 0.04), ('incorrect', 0.039), ('majli', 0.039), ('regenerated', 0.039), ('prepositions', 0.038), ('ln', 0.038), ('eal', 0.037), ('correction', 0.037), ('adj', 0.037), ('monz', 0.037), ('change', 0.036), ('montr', 0.036), ('tesni', 0.036), ('nov', 0.036), ('fn', 0.035), ('christof', 0.034), ('sgall', 0.034), ('degradation', 0.034), ('attr', 0.034), ('youth', 0.034), ('variant', 0.034), ('lemma', 0.033), ('fixes', 0.032), ('ale', 0.032), ('philipp', 0.032), ('parent', 0.031), ('tillmann', 0.031), ('michal', 0.031), ('sentence', 0.031), ('arguments', 0.031), ('argument', 0.031), ('thresholds', 0.03), ('annotators', 0.03), ('zone', 0.03), ('att', 0.03), ('vl', 0.03), ('cases', 0.03), ('correcting', 0.03), ('seventh', 0.029), ('concluded', 0.028), ('machine', 0.027), ('agreement', 0.027), ('decided', 0.026), ('bears', 0.026), ('ere', 0.026), ('omar', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="110-tfidf-1" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>Author: Rudolf Rosa ; David Marecek ; Ales Tamchyna</p><p>Abstract: Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</p><p>2 0.098076567 <a title="110-tfidf-2" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>3 0.096229911 <a title="110-tfidf-3" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>Author: Rico Sennrich ; Holger Schwenk ; Walid Aransa</p><p>Abstract: While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also de- scribe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1BLEU over unadapted systems and single-domain adaptation.</p><p>4 0.092093587 <a title="110-tfidf-4" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>Author: Ondrej Dusek ; Filip Jurcicek</p><p>Abstract: We present a novel method of statistical morphological generation, i.e. the prediction of inflected word forms given lemma, part-of-speech and morphological features, aimed at robustness to unseen inputs. Our system uses a trainable classifier to predict “edit scripts” that are then used to transform lemmas into inflected word forms. Suffixes of lemmas are included as features to achieve robustness. We evaluate our system on 6 languages with a varying degree of morphological richness. The results show that the system is able to learn most morphological phenomena and generalize to unseen inputs, producing significantly better results than a dictionarybased baseline.</p><p>5 0.085897319 <a title="110-tfidf-5" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>Author: Graham Neubig</p><p>Abstract: In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http : / /phont ron . com/t ravat ar</p><p>6 0.084629074 <a title="110-tfidf-6" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>7 0.082867496 <a title="110-tfidf-7" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>8 0.082204297 <a title="110-tfidf-8" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>9 0.081966355 <a title="110-tfidf-9" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>10 0.076500632 <a title="110-tfidf-10" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>11 0.07478366 <a title="110-tfidf-11" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>12 0.073124327 <a title="110-tfidf-12" href="./acl-2013-Using_subcategorization_knowledge_to_improve_case_prediction_for_translation_to_German.html">378 acl-2013-Using subcategorization knowledge to improve case prediction for translation to German</a></p>
<p>13 0.073048554 <a title="110-tfidf-13" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>14 0.071324646 <a title="110-tfidf-14" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>15 0.071076296 <a title="110-tfidf-15" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>16 0.068809144 <a title="110-tfidf-16" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>17 0.068166099 <a title="110-tfidf-17" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>18 0.067137629 <a title="110-tfidf-18" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>19 0.066586882 <a title="110-tfidf-19" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>20 0.066241398 <a title="110-tfidf-20" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.173), (1, -0.088), (2, 0.078), (3, 0.019), (4, -0.058), (5, 0.009), (6, -0.008), (7, 0.015), (8, 0.064), (9, 0.034), (10, -0.037), (11, 0.047), (12, -0.017), (13, 0.076), (14, -0.066), (15, -0.002), (16, -0.044), (17, -0.048), (18, 0.042), (19, 0.007), (20, 0.014), (21, 0.018), (22, -0.003), (23, -0.006), (24, 0.038), (25, -0.002), (26, -0.028), (27, -0.022), (28, 0.022), (29, 0.003), (30, -0.012), (31, -0.019), (32, -0.031), (33, 0.004), (34, 0.023), (35, -0.028), (36, -0.086), (37, -0.014), (38, 0.034), (39, -0.138), (40, -0.024), (41, -0.008), (42, 0.004), (43, -0.027), (44, -0.053), (45, -0.001), (46, 0.023), (47, 0.005), (48, -0.07), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91692823 <a title="110-lsi-1" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>Author: Rudolf Rosa ; David Marecek ; Ales Tamchyna</p><p>Abstract: Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</p><p>2 0.8234489 <a title="110-lsi-2" href="./acl-2013-Using_subcategorization_knowledge_to_improve_case_prediction_for_translation_to_German.html">378 acl-2013-Using subcategorization knowledge to improve case prediction for translation to German</a></p>
<p>Author: Marion Weller ; Alexander Fraser ; Sabine Schulte im Walde</p><p>Abstract: This paper demonstrates the need and impact of subcategorization information for SMT. We combine (i) features on sourceside syntactic subcategorization and (ii) an external knowledge base with quantitative, dependency-based information about target-side subcategorization frames. A manual evaluation of an English-toGerman translation task shows that the subcategorization information has a positive impact on translation quality through better prediction of case.</p><p>3 0.68623561 <a title="110-lsi-3" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>4 0.67444623 <a title="110-lsi-4" href="./acl-2013-SORT%3A_An_Interactive_Source-Rewriting_Tool_for_Improved_Translation.html">305 acl-2013-SORT: An Interactive Source-Rewriting Tool for Improved Translation</a></p>
<p>Author: Shachar Mirkin ; Sriram Venkatapathy ; Marc Dymetman ; Ioan Calapodescu</p><p>Abstract: The quality of automatic translation is affected by many factors. One is the divergence between the specific source and target languages. Another lies in the source text itself, as some texts are more complex than others. One way to handle such texts is to modify them prior to translation. Yet, an important factor that is often overlooked is the source translatability with respect to the specific translation system and the specific model that are being used. In this paper we present an interactive system where source modifications are induced by confidence estimates that are derived from the translation model in use. Modifications are automatically generated and proposed for the user’s ap- proval. Such a system can reduce postediting effort, replacing it by cost-effective pre-editing that can be done by monolinguals.</p><p>5 0.6708042 <a title="110-lsi-5" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>Author: Melania Duma ; Cristina Vertan ; Wolfgang Menzel</p><p>Abstract: Machine translation (MT) evaluation aims at measuring the quality of a candidate translation by comparing it with a reference translation. This comparison can be performed on multiple levels: lexical, syntactic or semantic. In this paper, we propose a new syntactic metric for MT evaluation based on the comparison of the dependency structures of the reference and the candidate translations. The dependency structures are obtained by means of a Weighted Constraints Dependency Grammar parser. Based on experiments performed on English to German translations, we show that the new metric correlates well with human judgments at the system level. 1</p><p>6 0.6621269 <a title="110-lsi-6" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>7 0.66091949 <a title="110-lsi-7" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>8 0.65735805 <a title="110-lsi-8" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<p>9 0.64740074 <a title="110-lsi-9" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>10 0.63160795 <a title="110-lsi-10" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>11 0.62675416 <a title="110-lsi-11" href="./acl-2013-Learning_to_lemmatise_Polish_noun_phrases.html">227 acl-2013-Learning to lemmatise Polish noun phrases</a></p>
<p>12 0.62267053 <a title="110-lsi-12" href="./acl-2013-Identifying_English_and_Hungarian_Light_Verb_Constructions%3A_A_Contrastive_Approach.html">186 acl-2013-Identifying English and Hungarian Light Verb Constructions: A Contrastive Approach</a></p>
<p>13 0.62071937 <a title="110-lsi-13" href="./acl-2013-Translating_Italian_connectives_into_Italian_Sign_Language.html">360 acl-2013-Translating Italian connectives into Italian Sign Language</a></p>
<p>14 0.6163736 <a title="110-lsi-14" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>15 0.61560804 <a title="110-lsi-15" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>16 0.6121828 <a title="110-lsi-16" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>17 0.61145085 <a title="110-lsi-17" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>18 0.59519708 <a title="110-lsi-18" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>19 0.59351915 <a title="110-lsi-19" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>20 0.59276265 <a title="110-lsi-20" href="./acl-2013-Stacking_for_Statistical_Machine_Translation.html">328 acl-2013-Stacking for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.032), (6, 0.038), (11, 0.047), (14, 0.016), (24, 0.025), (26, 0.057), (35, 0.069), (40, 0.012), (42, 0.081), (48, 0.037), (67, 0.292), (70, 0.032), (88, 0.02), (90, 0.061), (95, 0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8530817 <a title="110-lda-1" href="./acl-2013-Crowdsourcing_Interaction_Logs_to_Understand_Text_Reuse_from_the_Web.html">100 acl-2013-Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</a></p>
<p>Author: Martin Potthast ; Matthias Hagen ; Michael Volske ; Benno Stein</p><p>Abstract: unkown-abstract</p><p>same-paper 2 0.75342619 <a title="110-lda-2" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>Author: Rudolf Rosa ; David Marecek ; Ales Tamchyna</p><p>Abstract: Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</p><p>3 0.73505199 <a title="110-lda-3" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>4 0.73348492 <a title="110-lda-4" href="./acl-2013-Enriching_Entity_Translation_Discovery_using_Selective_Temporality.html">138 acl-2013-Enriching Entity Translation Discovery using Selective Temporality</a></p>
<p>Author: Gae-won You ; Young-rok Cha ; Jinhan Kim ; Seung-won Hwang</p><p>Abstract: This paper studies named entity translation and proposes “selective temporality” as a new feature, as using temporal features may be harmful for translating “atemporal” entities. Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6. 1%.</p><p>5 0.65124917 <a title="110-lda-5" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>Author: Sebastian Martschat</p><p>Abstract: We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task.</p><p>6 0.63380045 <a title="110-lda-6" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>7 0.60016233 <a title="110-lda-7" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>8 0.57039857 <a title="110-lda-8" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>9 0.53305316 <a title="110-lda-9" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>10 0.53279018 <a title="110-lda-10" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>11 0.53062308 <a title="110-lda-11" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>12 0.52360964 <a title="110-lda-12" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>13 0.52134949 <a title="110-lda-13" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>14 0.51916873 <a title="110-lda-14" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>15 0.51871443 <a title="110-lda-15" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>16 0.51786143 <a title="110-lda-16" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>17 0.5175038 <a title="110-lda-17" href="./acl-2013-Enlisting_the_Ghost%3A_Modeling_Empty_Categories_for_Machine_Translation.html">137 acl-2013-Enlisting the Ghost: Modeling Empty Categories for Machine Translation</a></p>
<p>18 0.51547849 <a title="110-lda-18" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>19 0.5153631 <a title="110-lda-19" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>20 0.51430857 <a title="110-lda-20" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
