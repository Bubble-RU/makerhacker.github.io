<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-252" href="#">acl2013-252</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</h1>
<br/><p>Source: <a title="acl-2013-252-pdf" href="http://aclweb.org/anthology//P/P13/P13-3012.pdf">pdf</a></p><p>Author: Sebastian Martschat</p><p>Abstract: We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task.</p><p>Reference: <a title="acl-2013-252-reference" href="../acl2013_reference/acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. [sent-3, score-0.893]
</p><p>2 The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task. [sent-4, score-0.161]
</p><p>3 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. [sent-5, score-0.489]
</p><p>4 Quite recently, however, rule-based ap-  proaches regained popularity due to Stanford’s multi-pass sieve approach which exhibits stateof-the-art performance on many standard coreference data sets (Raghunathan et al. [sent-7, score-0.564]
</p><p>5 , 2010) and also won the CoNLL-201 1 shared task on coreference resolution (Lee et al. [sent-8, score-0.828]
</p><p>6 In this paper we present a graph-based approach for coreference resolution that models a document to be processed as a graph. [sent-13, score-0.706]
</p><p>7 The nodes are mentions and the edges correspond to relations between mentions. [sent-14, score-0.452]
</p><p>8 Our approach belongs to a class of recently proposed graph models for coreference resolution (Cai and Strube, 2010; Sapena et al. [sent-16, score-0.811]
</p><p>9 In contrast to previous models be-  longing to this class we do not learn any edge weights but perform inference on the graph structure only which renders our model unsupervised. [sent-19, score-0.209]
</p><p>10 On the English data of the CoNLL’ 12 shared task the model outperforms most systems which participated in the shared task. [sent-20, score-0.271]
</p><p>11 While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight. [sent-22, score-1.134]
</p><p>12 This yields a model similar to the one presented in this paper though Mitkov’s work has only been applied to pronoun resolution. [sent-23, score-0.233]
</p><p>13 Nicolae and Nicolae (2006) phrase coreference resolution as a graph clustering problem: they first perform pairwise classification and then construct a graph using the derived confidence values as edge weights. [sent-24, score-1.078]
</p><p>14 (2010) and Cai and Strube (2010) perform coreference resolution in one step using graph partitioning approaches. [sent-29, score-0.811]
</p><p>15 These approaches participated in the recent CoNLL’ 11 shared task (Pradhan et al. [sent-30, score-0.149]
</p><p>16 (2012) and ranked second in the English track at the CoNLL’ 12 shared task (Pradhan et al. [sent-36, score-0.161]
</p><p>17 tc ud2e0n1t3 R Aessseoacricahti Wonor foksrh Coopm, ppaugteasti 8o1n–a8l8 L,inguistics  also represents the problem as a graph by performing inference on trees constructed using the multi-pass sieve approach by Raghunathan et al. [sent-41, score-0.182]
</p><p>18 Cardie and Wagstaff (1999) present an early approach to unsupervised coreference resolution based on a straightforward clustering approach. [sent-45, score-0.854]
</p><p>19 Poon and Domingos (2008) present a Markov Logic Network approach to unsupervised coreference resolution. [sent-49, score-0.545]
</p><p>20 These approaches reach competitive performance on gold mentions but not on system mentions (Ng, 2008). [sent-50, score-0.54]
</p><p>21 3  A Multigraph Model  We aim for a model which directly represents the relations between mentions in a graph structure. [sent-53, score-0.487]
</p><p>22 We want to model that Paris is not a likely candidate antecedent for They due to number disagreement, but that Leaders and recent developments are potential antecedents for They. [sent-59, score-0.243]
</p><p>23 The  graphical structure depicted in Figure 1 models these relations between the four mentions Leaders, Paris, recent developments and They. [sent-63, score-0.442]
</p><p>24 Figure 1: An example graph modeling relations between mentions. [sent-64, score-0.217]
</p><p>25 A directed edge from a mention m to n indicates that n precedes m and that there is some relation between m and n that indicates coreference or non-coreference. [sent-65, score-0.723]
</p><p>26 Labeled edges describe the relations between the mentions, multiple relations can hold between a pair. [sent-66, score-0.294]
</p><p>27 Many graph models for coreference resolution operate on A = V V . [sent-71, score-0.811]
</p><p>28 Our multigraph model allows us ttoe honavAe multiple edges mwuilthti gdriaffpehremnot dleableallslo o bwe-s tween mentions. [sent-72, score-0.187]
</p><p>29 To have a notion of order we employ a directed graph: We only allow an edge from m to n if m appears later in the text than n. [sent-73, score-0.153]
</p><p>30 To perform coreference resolution for a document d, we first construct a directed labeled multigraph (Section 3. [sent-74, score-0.862]
</p><p>31 The resulting graph is 82  clustered to obtain the mentions that refer to the same entity (Section 3. [sent-78, score-0.375]
</p><p>32 3 Graph Construction Given a set M of mentions extracted from a document d, we set V = M, i. [sent-81, score-0.27]
</p><p>33 To construct the edges A, we consider each pair (m, n) of mentions with n ≺ m. [sent-84, score-0.34]
</p><p>34 For simplicity, we restrict ourselves to binary relations that hold between pairs of mentions (see Section 4). [sent-88, score-0.382]
</p><p>35 The graph displayed in Figure 1 is the graph constructed for the mentions Leaders, Paris, recent developments and They from the example sentence at the beginning of this Section, where R = {P AnaPron, P Subject, N Number}. [sent-89, score-0.57]
</p><p>36 nnru m ∈b Rer disagreement) or for coreference (e. [sent-93, score-0.487]
</p><p>37 We therefore divide R into a set of negative relations R− and a set of positive relations R+. [sent-96, score-0.294]
</p><p>38 Previous work on multigraphs for coreference resolution disallows any edge between mentions for which a negative relations holds (Cai et al. [sent-97, score-1.242]
</p><p>39 tI nw contrast to previous ,wn,orrk) on Asi imfri la ∈r graph models we do not learn any edge weights from training data. [sent-106, score-0.209]
</p><p>40 1We experimented with different weighting schemes for negative relations on development data (e. [sent-111, score-0.15]
</p><p>41 The relations we employ are indicators for coreference (which get a positive weight) and indicators for non-coreference (which get a negative weight). [sent-116, score-0.785]
</p><p>42 We aim to employ a simple and efficient clustering scheme on this graph and therefore choose 1-nearest-neighbor clustering: for every m, we choose as antecedent m’s child n such that the sum ofedge weights is maximal and positive. [sent-117, score-0.418]
</p><p>43 4 this algorithm reduces to choosing the child that is connected via the highest number of positive relations and via no negative relation. [sent-120, score-0.182]
</p><p>44 4  Relations  The graph model described in Section 3 is based on expressing relations between pairs of mentions via edges built from such relations. [sent-122, score-0.557]
</p><p>45 They are well-known indicators and constraints for coreference and are taken from previous work (Cardie and Wagstaff, 1999; Soon et al. [sent-124, score-0.524]
</p><p>46 All relations operate on pairs of mentions (m, n), where m is the anaphor and n is a candidate antecedent. [sent-128, score-0.733]
</p><p>47 (1) N Gender, (2) N Number: Two mentions do not agree in gender or number. [sent-134, score-0.328]
</p><p>48 (3) N SemanticClass: Two mentions do not agree in semantic class (we only use the top categories Object, Date and Person from WordNet (Fellbaum, 1998)). [sent-136, score-0.27]
</p><p>49 (4) N ItDist: The anaphor is it or they and the sentence distance to the antecedent is larger 83  than one. [sent-137, score-0.5]
</p><p>50 (5) N Speaker12Pron: Two first person pronouns or two second person pronouns with different speakers, or one first person pronoun and one second person pronoun with the same speaker2. [sent-138, score-0.926]
</p><p>51 (6) N ContraSubObj: Two mentions are in the subject/object positions of the same verb, the anaphor is a non-possessive/reflexive pronoun. [sent-139, score-0.621]
</p><p>52 (7) N Mod: Two mentions have the same syntac-  tic heads, and the anaphor has a nominal modifier which does not occur in the antecedent. [sent-140, score-0.621]
</p><p>53 (8) N Embedding: Two mentions where one embeds the other, which is not a reflexive or possessive pronoun. [sent-141, score-0.329]
</p><p>54 (9) N 2PronNonSpeech: Two second person pronouns without speaker information and not in direct speech. [sent-142, score-0.209]
</p><p>55 2  Positive Relations  Positive relations are coreference indicators which are added as edges with positive weights. [sent-144, score-0.738]
</p><p>56 (10) P NonPron StrMatch: Applies only if the anaphor is definite or a proper name3. [sent-145, score-0.351]
</p><p>57 This relation holds if after discarding stop words the strings of mentions completely match. [sent-146, score-0.303]
</p><p>58 (11) P HeadMatch: If the syntactic heads of mentions match. [sent-147, score-0.302]
</p><p>59 (13) P Speaker12Pron: If the speaker of the second person pronoun is talking to the speaker  of the first person pronoun (applies only to first/second person pronouns). [sent-152, score-0.784]
</p><p>60 (14) P DSPron: One mention is a speak verb’s subject, the other mention is a first person pronoun within the corresponding direct speech. [sent-153, score-0.489]
</p><p>61 (15) P ReflPronSub: If the anaphor is a reflexive pronoun, and the antecedent is the subject of the sentence. [sent-154, score-0.575]
</p><p>62 (16) P PossPronSub: If the anaphor is a possessive pronoun, and the antecedent is the subject of the anaphor’s sentence or subclause. [sent-155, score-0.574]
</p><p>63 (17) P PossPronEmb: The anaphor is a posses2Like all relations using speaker information, this relation depends on the gold speaker annotation layer in the corpus. [sent-156, score-0.598]
</p><p>64 (18) P AnaPron: If the anaphor is a pronoun none of the mentions is a first or second son pronoun. [sent-159, score-0.881]
</p><p>65 (19) P VerbAgree: If the anaphor is a third  and perto a per-  son pronoun and has the same predicate as the antecedent. [sent-161, score-0.611]
</p><p>66 (20) P Subject, (21) P Object: The anaphor is a third person pronoun and both mentions are subjects/objects. [sent-163, score-0.926]
</p><p>67 (22) P Pron StrMatch: If both mentions are pronouns and their strings match. [sent-165, score-0.356]
</p><p>68 (23) P Pron Agreement: If both mentions are different pronoun tokens but agree in number, gender and person. [sent-166, score-0.561]
</p><p>69 1 Data and Evaluation Metrics We use the data provided for the English track of the CoNLL’ 12 shared task on multilingual coreference resolution (Pradhan et al. [sent-168, score-0.867]
</p><p>70 To extract system mentions we employ the mention extractor described in Martschat et al. [sent-175, score-0.404]
</p><p>71 We evaluate our system with the coreference resolution evaluation metrics that were used for the CoNLL shared tasks on coreference, which are MUC (Vilain et al. [sent-177, score-0.828]
</p><p>72 We also report the unweighted average of the three scores, which was the official evaluation metric in the shared tasks. [sent-179, score-0.158]
</p><p>73 To compute the scores we employed the official scorer supplied by the shared task organizers. [sent-180, score-0.158]
</p><p>74 CoNLL’ 12 shared task, which are denoted as best and median respectively. [sent-204, score-0.157]
</p><p>75 We also compare with two supervised variants of our model which use the same relations and the same clustering algorithm as the unsupervised model: weights fraction sets the weight of a relation to the fraction of positive instances in training data (as in Martschat et al. [sent-206, score-0.409]
</p><p>76 , 2008) and builds a graph where the weight of an edge connecting two mentions is the classifier’s prediction4. [sent-210, score-0.447]
</p><p>77 Our unsupervised model performs considerably better than the median system from the CoNLL’ 12  shared task on both data sets according to all metrics. [sent-212, score-0.215]
</p><p>78 While we observe a decrease of 1point average score when evaluating on test data the model still would have ranked fourth in the English track of the CoNLL’ 12 shared task with only 0. [sent-215, score-0.161]
</p><p>79 For an initial analysis we split the errors  according to the mention type of anaphor and antecedent (name, nominal and pronoun). [sent-224, score-0.653]
</p><p>80 We therefore count one precision error whenever the clustering algorithm assigns two non-coreferent mentions to the same cluster. [sent-227, score-0.36]
</p><p>81 Table 2 shows the  PN ROAOM 384N643A18(M63(37(2 %1%)%) 261N717O47(68M16( %4595)% ) 1P9531R(80O(8496%(2%)4 %) Table 2: Number of clustering decisions made according to mention type (rows anaphor, columns antecedent) and percentage of wrong decisions. [sent-228, score-0.216]
</p><p>82 number of clustering decisions made according to the mention type and in brackets the fraction of decisions that erroneously assign two non-coreferent mentions to the same cluster. [sent-229, score-0.546]
</p><p>83 We see that two main sources of error are nominal-nominal pairs and the resolution of pronouns. [sent-230, score-0.219]
</p><p>84 We now focus on gaining further insight into the system’s performance  for pronoun resolution by investigating the performance per pronoun type. [sent-231, score-0.685]
</p><p>85 We obtain good performance for I and my which in the majority of cases can be resolved unambiguously by the speaker relations employed by our system. [sent-233, score-0.163]
</p><p>86 Rows are pronoun surfaces, columns number of clustering decisions and percentage of wrong decisions for all and only anaphoric pronouns respectively. [sent-247, score-0.512]
</p><p>87 2  Recall Errors  Estimating recall errors by counting all missing pairwise links would consider each entity many times. [sent-252, score-0.136]
</p><p>88 We see that  PN RAO MM33 N0 2A16 M742N972O760M3P52 43R570O Table 4: Number of recall errors according to mention type (rows anaphor, columns antecedent). [sent-262, score-0.153]
</p><p>89 the main source of recall errors are missing links of nominal-nominal pairs. [sent-263, score-0.136]
</p><p>90 In these cases lexical or world knowledge is needed to build coreference links between mentions with different heads. [sent-266, score-0.783]
</p><p>91 In these cases the heads of the mentions matched but no link was built due to N Mod. [sent-269, score-0.302]
</p><p>92 7  Conclusions and Future Work  We presented an unsupervised graph-based model for coreference resolution. [sent-274, score-0.545]
</p><p>93 An error analysis revealed that two main sources of errors of our model are the inaccurate resolution of highly ambiguous pronouns such as it and missing links between nominals with different heads. [sent-276, score-0.441]
</p><p>94 Clustering al-  gorithms for noun phrase coreference resolution. [sent-287, score-0.487]
</p><p>95 Latent structure perceptron with feature induction for unrestricted coreference resolution. [sent-333, score-0.535]
</p><p>96 Stanford’s multi-pass sieve coreference resolution system at the CoNLL-201 1 shared task. [sent-341, score-0.905]
</p><p>97 CoNLL-201 1 Shared Task: Modeling unrestricted coreference in OntoNotes. [sent-372, score-0.535]
</p><p>98 CoNLL-2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes. [sent-377, score-0.535]
</p><p>99 RelaxCor participation in CoNLL shared task on coreference resolution. [sent-394, score-0.609]
</p><p>100 A machine learning approach to coreference resolution of noun phrases. [sent-399, score-0.706]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('coreference', 0.487), ('anaphor', 0.351), ('mentions', 0.27), ('pronoun', 0.233), ('resolution', 0.219), ('martschat', 0.156), ('antecedent', 0.149), ('cai', 0.145), ('leaders', 0.145), ('sapena', 0.132), ('conll', 0.129), ('shared', 0.122), ('multigraph', 0.117), ('relations', 0.112), ('graph', 0.105), ('mention', 0.092), ('clustering', 0.09), ('pronouns', 0.086), ('pradhan', 0.084), ('paris', 0.083), ('october', 0.08), ('sieve', 0.077), ('june', 0.077), ('edge', 0.072), ('person', 0.072), ('edges', 0.07), ('fernandes', 0.068), ('coreferent', 0.068), ('anapron', 0.066), ('raghunathan', 0.062), ('errors', 0.061), ('developments', 0.06), ('nicolae', 0.058), ('july', 0.058), ('gender', 0.058), ('unsupervised', 0.058), ('jie', 0.054), ('speaker', 0.051), ('missing', 0.049), ('august', 0.049), ('unrestricted', 0.048), ('subject', 0.045), ('angheluta', 0.044), ('ees', 0.044), ('emili', 0.044), ('fmirsa', 0.044), ('multigraphs', 0.044), ('strmatch', 0.044), ('wagstaff', 0.044), ('employ', 0.042), ('directed', 0.039), ('padr', 0.039), ('ceafe', 0.039), ('vilain', 0.039), ('bart', 0.039), ('track', 0.039), ('negative', 0.038), ('bergsma', 0.038), ('indicators', 0.037), ('versley', 0.036), ('pron', 0.036), ('jordi', 0.036), ('waikiki', 0.036), ('official', 0.036), ('anaphoric', 0.035), ('median', 0.035), ('va', 0.035), ('strube', 0.035), ('decisions', 0.034), ('antecedents', 0.034), ('bagga', 0.034), ('disagreement', 0.033), ('relation', 0.033), ('heads', 0.032), ('weights', 0.032), ('positive', 0.032), ('soon', 0.032), ('cardie', 0.031), ('muc', 0.031), ('lee', 0.031), ('displayed', 0.03), ('island', 0.03), ('heeyoung', 0.03), ('reflexive', 0.03), ('llu', 0.03), ('hypergraph', 0.029), ('culotta', 0.029), ('possessive', 0.029), ('poon', 0.028), ('participated', 0.027), ('son', 0.027), ('march', 0.027), ('uw', 0.027), ('portland', 0.026), ('mitkov', 0.026), ('korea', 0.026), ('links', 0.026), ('fraction', 0.026), ('heidelberg', 0.026), ('sameer', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="252-tfidf-1" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>Author: Sebastian Martschat</p><p>Abstract: We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task.</p><p>2 0.39172822 <a title="252-tfidf-2" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>Author: Nathan Gilbert ; Ellen Riloff</p><p>Abstract: Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.</p><p>3 0.32279846 <a title="252-tfidf-3" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>Author: Emmanuel Lassalle ; Pascal Denis</p><p>Abstract: This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of average F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. B3,</p><p>4 0.29845408 <a title="252-tfidf-4" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>Author: Greg Durrett ; David Hall ; Dan Klein</p><p>Abstract: Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.</p><p>5 0.14630191 <a title="252-tfidf-5" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>Author: Kartik Goyal ; Sujay Kumar Jauhar ; Huiying Li ; Mrinmaya Sachan ; Shashank Srivastava ; Eduard Hovy</p><p>Abstract: In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics pre- viously employed in the literature.</p><p>6 0.14457938 <a title="252-tfidf-6" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>7 0.14049685 <a title="252-tfidf-7" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>8 0.13399044 <a title="252-tfidf-8" href="./acl-2013-GuiTAR-based_Pronominal_Anaphora_Resolution_in_Bengali.html">177 acl-2013-GuiTAR-based Pronominal Anaphora Resolution in Bengali</a></p>
<p>9 0.13061987 <a title="252-tfidf-9" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>10 0.12812363 <a title="252-tfidf-10" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>11 0.1249682 <a title="252-tfidf-11" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>12 0.12449536 <a title="252-tfidf-12" href="./acl-2013-Joint_Apposition_Extraction_with_Syntactic_and_Semantic_Constraints.html">205 acl-2013-Joint Apposition Extraction with Syntactic and Semantic Constraints</a></p>
<p>13 0.09540575 <a title="252-tfidf-13" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>14 0.089601219 <a title="252-tfidf-14" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>15 0.079652332 <a title="252-tfidf-15" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>16 0.076317966 <a title="252-tfidf-16" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>17 0.07102605 <a title="252-tfidf-17" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>18 0.07041385 <a title="252-tfidf-18" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>19 0.066521548 <a title="252-tfidf-19" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>20 0.066152543 <a title="252-tfidf-20" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, 0.062), (2, -0.039), (3, -0.122), (4, 0.021), (5, 0.24), (6, -0.021), (7, 0.155), (8, -0.028), (9, 0.055), (10, 0.037), (11, -0.126), (12, -0.077), (13, -0.002), (14, -0.109), (15, 0.131), (16, -0.173), (17, 0.265), (18, -0.119), (19, 0.161), (20, -0.191), (21, 0.125), (22, -0.01), (23, -0.276), (24, -0.02), (25, 0.046), (26, -0.027), (27, 0.137), (28, 0.169), (29, -0.033), (30, 0.037), (31, 0.024), (32, 0.027), (33, -0.045), (34, 0.075), (35, 0.001), (36, 0.011), (37, -0.029), (38, -0.007), (39, -0.039), (40, -0.08), (41, 0.019), (42, 0.038), (43, -0.004), (44, 0.024), (45, 0.04), (46, -0.023), (47, 0.064), (48, 0.009), (49, -0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95914954 <a title="252-lsi-1" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>Author: Sebastian Martschat</p><p>Abstract: We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task.</p><p>2 0.91978109 <a title="252-lsi-2" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>Author: Greg Durrett ; David Hall ; Dan Klein</p><p>Abstract: Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.</p><p>3 0.90499777 <a title="252-lsi-3" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>Author: Nathan Gilbert ; Ellen Riloff</p><p>Abstract: Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.</p><p>4 0.79649246 <a title="252-lsi-4" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>Author: Emmanuel Lassalle ; Pascal Denis</p><p>Abstract: This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of average F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. B3,</p><p>5 0.75263667 <a title="252-lsi-5" href="./acl-2013-GuiTAR-based_Pronominal_Anaphora_Resolution_in_Bengali.html">177 acl-2013-GuiTAR-based Pronominal Anaphora Resolution in Bengali</a></p>
<p>Author: Apurbalal Senapati ; Utpal Garain</p><p>Abstract: This paper attempts to use an off-the-shelf anaphora resolution (AR) system for Bengali. The language specific preprocessing modules of GuiTAR (v3.0.3) are identified and suitably designed for Bengali. Anaphora resolution module is also modified or replaced in order to realize different configurations of GuiTAR. Performance of each configuration is evaluated and experiment shows that the off-the-shelf AR system can be effectively used for Indic languages. 1</p><p>6 0.62486243 <a title="252-lsi-6" href="./acl-2013-Joint_Apposition_Extraction_with_Syntactic_and_Semantic_Constraints.html">205 acl-2013-Joint Apposition Extraction with Syntactic and Semantic Constraints</a></p>
<p>7 0.51108283 <a title="252-lsi-7" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>8 0.46618098 <a title="252-lsi-8" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>9 0.42665321 <a title="252-lsi-9" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>10 0.42387387 <a title="252-lsi-10" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>11 0.41246173 <a title="252-lsi-11" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>12 0.38568491 <a title="252-lsi-12" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>13 0.36530167 <a title="252-lsi-13" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>14 0.35707206 <a title="252-lsi-14" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>15 0.32753283 <a title="252-lsi-15" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>16 0.32517675 <a title="252-lsi-16" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>17 0.2806437 <a title="252-lsi-17" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>18 0.2725772 <a title="252-lsi-18" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>19 0.27093691 <a title="252-lsi-19" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>20 0.26678374 <a title="252-lsi-20" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (6, 0.033), (11, 0.054), (14, 0.026), (15, 0.012), (24, 0.042), (26, 0.039), (35, 0.071), (42, 0.041), (48, 0.036), (64, 0.017), (67, 0.182), (70, 0.06), (80, 0.025), (85, 0.011), (88, 0.129), (90, 0.034), (95, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.82067662 <a title="252-lda-1" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>Author: Sebastian Martschat</p><p>Abstract: We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task.</p><p>2 0.77942091 <a title="252-lda-2" href="./acl-2013-Enriching_Entity_Translation_Discovery_using_Selective_Temporality.html">138 acl-2013-Enriching Entity Translation Discovery using Selective Temporality</a></p>
<p>Author: Gae-won You ; Young-rok Cha ; Jinhan Kim ; Seung-won Hwang</p><p>Abstract: This paper studies named entity translation and proposes “selective temporality” as a new feature, as using temporal features may be harmful for translating “atemporal” entities. Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6. 1%.</p><p>3 0.73499417 <a title="252-lda-3" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>4 0.72011453 <a title="252-lda-4" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>Author: Rudolf Rosa ; David Marecek ; Ales Tamchyna</p><p>Abstract: Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</p><p>5 0.7156952 <a title="252-lda-5" href="./acl-2013-Sorani_Kurdish_versus_Kurmanji_Kurdish%3A_An_Empirical_Comparison.html">327 acl-2013-Sorani Kurdish versus Kurmanji Kurdish: An Empirical Comparison</a></p>
<p>Author: Kyumars Sheykh Esmaili ; Shahin Salavati</p><p>Abstract: Resource scarcity along with diversity– both in dialect and script–are the two primary challenges in Kurdish language processing. In this paper we aim at addressing these two problems by (i) building a text corpus for Sorani and Kurmanji, the two main dialects of Kurdish, and (ii) highlighting some of the orthographic, phonological, and morphological differences between these two dialects from statistical and rule-based perspectives.</p><p>6 0.71299917 <a title="252-lda-6" href="./acl-2013-Crowdsourcing_Interaction_Logs_to_Understand_Text_Reuse_from_the_Web.html">100 acl-2013-Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</a></p>
<p>7 0.71282476 <a title="252-lda-7" href="./acl-2013-Reconstructing_an_Indo-European_Family_Tree_from_Non-native_English_Texts.html">299 acl-2013-Reconstructing an Indo-European Family Tree from Non-native English Texts</a></p>
<p>8 0.70603794 <a title="252-lda-8" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>9 0.70489192 <a title="252-lda-9" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>10 0.7031548 <a title="252-lda-10" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>11 0.69151419 <a title="252-lda-11" href="./acl-2013-Evaluating_a_City_Exploration_Dialogue_System_with_Integrated_Question-Answering_and_Pedestrian_Navigation.html">141 acl-2013-Evaluating a City Exploration Dialogue System with Integrated Question-Answering and Pedestrian Navigation</a></p>
<p>12 0.68508422 <a title="252-lda-12" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>13 0.67988753 <a title="252-lda-13" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>14 0.67165214 <a title="252-lda-14" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>15 0.64993763 <a title="252-lda-15" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>16 0.63066316 <a title="252-lda-16" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>17 0.63062125 <a title="252-lda-17" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>18 0.62164003 <a title="252-lda-18" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>19 0.62100774 <a title="252-lda-19" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>20 0.61916494 <a title="252-lda-20" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
