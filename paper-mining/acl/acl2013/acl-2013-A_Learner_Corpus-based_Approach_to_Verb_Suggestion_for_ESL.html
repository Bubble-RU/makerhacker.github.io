<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 acl-2013-A Learner Corpus-based Approach to Verb Suggestion for ESL</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-8" href="#">acl2013-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 acl-2013-A Learner Corpus-based Approach to Verb Suggestion for ESL</h1>
<br/><p>Source: <a title="acl-2013-8-pdf" href="http://aclweb.org/anthology//P/P13/P13-2124.pdf">pdf</a></p><p>Author: Yu Sawai ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion.</p><p>Reference: <a title="acl-2013-8-reference" href="../acl2013_reference/acl-2013-A_Learner_Corpus-based_Approach_to_Verb_Suggestion_for_ESL_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. [sent-2, score-0.856]
</p><p>2 Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. [sent-3, score-0.877]
</p><p>3 Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion. [sent-4, score-1.29]
</p><p>4 1 Introduction  ,  In this study, we address verb selection errors in the writing of English learners. [sent-5, score-0.351]
</p><p>5 Selecting the right verb based on the context of a sentence is difficult for the learners of English as a Second Language (ESL). [sent-6, score-0.253]
</p><p>6 This error type is one of the most common errors in various learner corpora ranging from elementary to proficient levels1 . [sent-7, score-0.596]
</p><p>7 connect/communicate with other businessmen and do their jobs with the help of computers. [sent-9, score-0.104]
</p><p>8 However, native speakers of English would less likely use “connect”, which means “forming a relationship (with other businessmen)”, whereas “communicate” means “exchanging information or ideas”, which is what the sentence is trying to convey. [sent-11, score-0.104]
</p><p>9 1For example, in the CLC-FCE dataset, the replacement error of verbs is the third most common out of 75 error types. [sent-13, score-0.309]
</p><p>10 In the KJ corpus, lexical choice of verb is the sixth most common out of 47 error types. [sent-14, score-0.242]
</p><p>11 jp s Previous work on verb selection usually treats the task as a multi-class classification problem (Wu et al. [sent-18, score-0.196]
</p><p>12 In this formalization, it is important to restrict verbs by a candidate set because verb vocabulary is more numerous than other classes, such as determiners. [sent-22, score-0.447]
</p><p>13 Candidate sets for verb selection are often extracted from thesauri and/or round-trip translations. [sent-23, score-0.255]
</p><p>14 However, these resources may not cover certain error patterns found in actual learner corpora, and suffer from low-coverage. [sent-24, score-0.56]
</p><p>15 Furthermore, all the existing classifier models are trained only using a native corpus, which may not be adequate for correcting learner errors. [sent-25, score-0.528]
</p><p>16 In this paper, we propose to use error patterns in ESL writing for verb suggestion task by using candidate sets and a domain adaptation technique. [sent-26, score-1.396]
</p><p>17 First, to increase the coverage, candidate sets are extracted from a large scale learner corpus derived from a language learning website. [sent-27, score-0.678]
</p><p>18 Second, a domain adaptation technique is applied to the model  to fill the gap between two domains: native corpus and ESL corpus. [sent-28, score-0.477]
</p><p>19 Experiments are carried out on publicly available learner corpora, the Cambridge Learner Corpus First Certificate of English dataset (CLC-FCE) and the Konan JIEM corpus (KJ). [sent-29, score-0.48]
</p><p>20 The results show that the proposed candidate sets improve the coverage, compared to the baseline candidate sets derived from the WordNet and a round-trip translation table. [sent-30, score-0.474]
</p><p>21 To our knowledge, this is the first work for verb suggestion that uses (1) a learner corpus as a source of candidate sets and (2) the domain adaptation technique to take learner errors into account. [sent-32, score-2.037]
</p><p>22 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 708–713, 2  Verb Suggestion Considering Error Patterns  The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya  and Roth, 2011; Wu et al. [sent-35, score-0.622]
</p><p>23 , 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. [sent-36, score-0.404]
</p><p>24 1  Candidate Sets  For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83. [sent-38, score-0.774]
</p><p>25 An advantage ofusing the learner corpus from such website is the size of annotated portion (Mizumoto et al. [sent-39, score-0.469]
</p><p>26 We have collected the learner writings on the site, and released the dataset for research purpose4. [sent-42, score-0.477]
</p><p>27 Second, we extracted the correction pairs which have “VB*” tag. [sent-45, score-0.121]
</p><p>28 The set of correction pairs given an incorrect verb is considered as a candidate set for the verb. [sent-46, score-0.447]
</p><p>29 We then performed the following preprocessing for the dataset because we focus on lexical selec-  tion of verbs: • Lemmatize verbs to reduce data sparseness. [sent-47, score-0.16]
</p><p>30 The target verbs are limited to the 500 most common verbs in the CLC-FCE Therefore, verbs that do not appear in the target list are not included in the candidate sets. [sent-51, score-0.623]
</p><p>31 The topmost 500 verbs cover almost 90 percent of the vocabulary of verbs in the CLC-FCE The average number of candidates in a set is Note that the number of candidates varies across each target  corpus5. [sent-52, score-0.391]
</p><p>32 Candidate sets will also be available at the same URL. [sent-61, score-0.059]
</p><p>33 5They are extracted from all “VB” tagged tokens, and they contain 1,292 unique verbs after removing non-English words. [sent-62, score-0.162]
</p><p>34 8For instance, the candidate set for “get” has 315 correction pairs, whereas “refund” has only 4. [sent-65, score-0.299]
</p><p>35 2 Suggestion Model The verb suggestion model consists of multi-class classifiers for each target verb; and based on the classifiers’ output, it suggests alternative verbs. [sent-67, score-0.663]
</p><p>36 When testing on the learner corpus, the model suggests a ranking of the possible verbs for the blank corresponding to a given context. [sent-70, score-0.503]
</p><p>37 Note that unlike the fill-in-the-blank task, the candidate sets and domain adaptation can be applied to this task to take the original word into account. [sent-71, score-0.51]
</p><p>38 The model is trained on a huge native corpus, namely the ukWaC corpus, because the data-size of learner corpora is limited compared to native corpora. [sent-72, score-0.636]
</p><p>39 In our experiment, the Lang8 corpus is used as the target domain corpus, since we assume that it shares the same characteristics with the CLC-FCE and the KJ corpora used for testing. [sent-76, score-0.27]
</p><p>40 3 Domain Adaptation To adapt the models to the learner corpus, we employ a domain adaptation technique to emphasize the importance of learner domain information. [sent-78, score-1.232]
</p><p>41 Although there are many studies on domain adaptation, we chose to use Feature Augmentation technique introduced by (Daum ´e III, 2007) for its simplicity. [sent-79, score-0.165]
</p><p>42 , 2012) proposed to apply this method to grammatical error correction for writings of Japanese learners and confirmed that this is more effective for correcting learner errors than simply adding the target domain instances. [sent-81, score-1.006]
</p><p>43 In this study, the source domain is the native writing, and the target domain is the ESL writing. [sent-82, score-0.393]
</p><p>44 Our motivation is to use the ESL corpus together with the huge native corpus to employ both an advantage of the size of training data and the ESL writing specific features. [sent-83, score-0.366]
</p><p>45 4  Features  In previous work, various features were used: lexical and POS n-grams, dependencies, and arguments in the verb context. [sent-89, score-0.175]
</p><p>46 The features include lexical and POS n-grams,  and lexical head words of the nearest NPs, and clustering features of these head words. [sent-93, score-0.184]
</p><p>47 Note that those features are also used when extracting examples from the target domain dataset (the learner domain corpus). [sent-96, score-0.737]
</p><p>48 The nearest NP’s head feature±s are divided into two (Left, Right). [sent-99, score-0.059]
</p><p>49 The additional clustering features are used for reducing sparseness, because the NP’s head words are usually proper nouns. [sent-100, score-0.098]
</p><p>50 To create the word clusters, we employ Brown clustering, a hierarchical clustering algorithm proposed by (Brown et al. [sent-101, score-0.068]
</p><p>51 4, we use the clustering features with three levels of granularity: 256, 128, and 64 dimensions. [sent-106, score-0.065]
</p><p>52 We used Percy Liang’s implementation9 to create 256 dimensional model from the ukWaC corpus,  which is used as the native corpus. [sent-107, score-0.104]
</p><p>53 3  Experiments  Performance of verb suggestion is evaluated on two error-tagged learner corpora: CLC-FCE and KJ. [sent-108, score-1.004]
</p><p>54 In the experiments, we assume that the target verb and its context for suggestion are already given. [sent-109, score-0.663]
</p><p>55 For the experiment on the CLC-FCE dataset, the targets are all words tagged with “RV” (re9https : / / github . [sent-110, score-0.082]
</p><p>56 We assume that all the verb selection errors are covered with this error tag. [sent-115, score-0.331]
</p><p>57 All error tagged parts with nested correction or multi-word expressions are excluded. [sent-116, score-0.256]
</p><p>58 Therefore the dataset is highly skewed to correct usages, though this setting expresses well the reality of ESL writing, as shown in (Chodorow et al. [sent-118, score-0.065]
</p><p>59 We carried out experiments with a variety of resources used for creating candidate sets. [sent-120, score-0.178]
</p><p>60 • WordNet CWaonrddidNaettes are retrieved from the synsets and verbs sharing the same hypernyms in the WordNet 3. [sent-121, score-0.147]
</p><p>61 • LearnerSmall LCeanadrnideartSesm are retrieved from following  •  •  •  •  learner corpora: NUS corpus of learner English (NUCLE), Konan-JIEM (KJ), and NICT Japanese learner English (JLE) corpus. [sent-123, score-1.231]
</p><p>62 WordNet+Roundtrip AW ocromdNbient+atRioonu onfdtthrei pthesaurus-based and the translation table-based candidate sets, similar to (Liu et al. [sent-125, score-0.178]
</p><p>63 Lang-8 TLahen proposed candidate sets obtained from a large scale learner corpus. [sent-128, score-0.619]
</p><p>64 Lang-8+DA Lang-8 +cDanAdidate sets with domain adapta-  of1 t0hOe Wur IoTu3ncdotr ip utsr a(Cnselat oiolon eletx ailc. [sent-129, score-0.211]
</p><p>65 )  Table 2: Comparison of candidate set size for each setting. [sent-135, score-0.178]
</p><p>66 Inter-corpus Evaluation We also evaluate the suggestion performance on the KJ corpus. [sent-142, score-0.474]
</p><p>67 The corpus contains diary-style writing by Japanese university students. [sent-143, score-0.173]
</p><p>68 The pro-  ficiency of the learners ranges from elementary to intermediate, so it is lower than that of the CLCFCE learners. [sent-144, score-0.138]
</p><p>69 The targets are all verbs tagged with “v lxc” (lexical selection error of verbs). [sent-145, score-0.345]
</p><p>70 To see the effect of L1 on the verb suggestion task, we added an alternative setting for the Roundtrip using only the English-Japanese and Japanese-English round-trip translation tables (En-Ja-En). [sent-146, score-0.663]
</p><p>71 , 2008) is used as a native corpus for training the suggestion model. [sent-149, score-0.637]
</p><p>72 Although this corpus consists of over 40 million sentences, 20,000 randomly selected sentences are used for each verb11 . [sent-150, score-0.059]
</p><p>73 The Lang-8 learner corpus is used for domain adaptation of the model in the Lang-8+DA configuration. [sent-151, score-0.714]
</p><p>74 The portion of data is the same as that used for constructing candidate sets. [sent-152, score-0.178]
</p><p>75 The mean reciprocal rank is calculated by taking 11e. [sent-154, score-0.074]
</p><p>76 , a classifier with a candidate set containing 50 verbs is trained with 1million sentences in total. [sent-156, score-0.299]
</p><p>77 the average of the reciprocal ranks for each instance. [sent-157, score-0.074]
</p><p>78 In both cases, the Lang-8 and its domain adaptation variant outperformed the others. [sent-159, score-0.273]
</p><p>79 The coverage of error patterns in the tables is the percentage of the cases where the suggestion list includes the gold correction. [sent-160, score-0.719]
</p><p>80 Generally, the suggestion performance and the coverage im-  prove as the size of the candidate sets increases. [sent-161, score-0.765]
</p><p>81 5  Discussions  Although the expert-annotated learner corpora contain candidates which are more reliable than a web-crawled Lang-8 corpus, the Lang-8 setting performed better as shown in Table 5. [sent-162, score-0.468]
</p><p>82 This can be explained by the broader coverage by the Lang-8 candidate sets than that of the LearnerSmall. [sent-163, score-0.291]
</p><p>83 We can conclude that, for the verb suggestion task, the coverage (recall) of candidate sets is more important than the quality (precision). [sent-165, score-0.913]
</p><p>84 As already mentioned, the number of error patterns contained in the candidate sets seems to have more importance than the quality. [sent-167, score-0.387]
</p><p>85 As shown in Tables 5 and 5, a positive effect of domain adaptation technique appeared in both test-corpora. [sent-168, score-0.342]
</p><p>86 In the case of the CLC-FCE,  280 out of 624 suggestions were improved compared to the setting without domain adaptation. [sent-169, score-0.124]
</p><p>87 solve/resolve” are improved, because sentences containing these confusions appear more frequently in the Lang8 corpus. [sent-173, score-0.073]
</p><p>88 Although the number of test-cases for the KJ corpus is smaller than the CLC-FCE, we can see the improvements for 33 out of 66 sug711  LRSWeao nturgidn-Nd8getsr S+ipmDRAaloundtrip0 M. [sent-174, score-0.059]
</p><p>89 The improvements appeared for frequent confusions of Japanese ESL learners such as “? [sent-185, score-0.206]
</p><p>90 Comparing the results of the Lang-8+DA on both test-corpora, the domain adaptation technique worked more effectively on the KJ corpus than on the CLC-FCE. [sent-188, score-0.373]
</p><p>91 This can be explained  by the fact that the style of writing of the additional data, i. [sent-189, score-0.148]
</p><p>92 More precisely, unlike the examination-type writing style of CLC-FCE, the KJ corpus consists of diary writing similar in style to the Lang-8 corpus, and it expresses more closely the proficiency of the learners. [sent-192, score-0.435]
</p><p>93 We think that the next step is to refine the suggestion models, since we currently take a simple fill-in-the-blank approach. [sent-193, score-0.474]
</p><p>94 As future work, we plan to extend the models as follows: (1) use both incorrect and correct sentences in learner corpora for training, and (2) employ ESL writing specific features such as learners’ L1 for domain adaptation. [sent-194, score-0.723]
</p><p>95 for kindly allowing us to use the Lang-8 learner corpus. [sent-196, score-0.408]
</p><p>96 Introducing and evaluating ukWaC, a very large web-derived corpus of English. [sent-221, score-0.059]
</p><p>97 Grammar error correction using pseudo-error sentences and domain adaptation. [sent-225, score-0.339]
</p><p>98 Correcting verb selection errors for ESL with the perceptron. [sent-233, score-0.237]
</p><p>99 Mining revi-  SNS for automated  Japanese error correction of second language learners. [sent-237, score-0.215]
</p><p>100 Algorithm selection and model adaptation for ESL correction tasks. [sent-241, score-0.318]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('suggestion', 0.474), ('learner', 0.382), ('esl', 0.346), ('kj', 0.307), ('candidate', 0.178), ('roundtrip', 0.155), ('adaptation', 0.149), ('verb', 0.148), ('domain', 0.124), ('correction', 0.121), ('verbs', 0.121), ('writing', 0.114), ('learners', 0.105), ('native', 0.104), ('error', 0.094), ('ukwac', 0.092), ('komachi', 0.077), ('reciprocal', 0.074), ('confusions', 0.073), ('sns', 0.073), ('japanese', 0.065), ('businessmen', 0.063), ('learnersmall', 0.063), ('rri', 0.063), ('sets', 0.059), ('corpus', 0.059), ('patterns', 0.056), ('imamura', 0.056), ('mizumoto', 0.056), ('rozovskaya', 0.056), ('writings', 0.056), ('coverage', 0.054), ('asterisk', 0.052), ('mamoru', 0.049), ('selection', 0.048), ('corpora', 0.046), ('ferraresi', 0.046), ('bannard', 0.046), ('liu', 0.044), ('nara', 0.044), ('xiaohua', 0.043), ('correcting', 0.042), ('target', 0.041), ('jobs', 0.041), ('tables', 0.041), ('technique', 0.041), ('targets', 0.041), ('tagged', 0.041), ('errors', 0.041), ('candidates', 0.04), ('da', 0.04), ('dataset', 0.039), ('clustering', 0.038), ('vb', 0.038), ('chodorow', 0.038), ('wordnet', 0.037), ('yuji', 0.036), ('xt', 0.035), ('style', 0.034), ('xs', 0.034), ('head', 0.033), ('elementary', 0.033), ('brown', 0.031), ('daum', 0.031), ('employ', 0.03), ('granularity', 0.03), ('della', 0.03), ('cover', 0.028), ('wu', 0.028), ('kugatsu', 0.028), ('sadamitsu', 0.028), ('alla', 0.028), ('zanchetta', 0.028), ('ating', 0.028), ('checkpoint', 0.028), ('diary', 0.028), ('ises', 0.028), ('kuniko', 0.028), ('metropolitan', 0.028), ('ofusing', 0.028), ('stiller', 0.028), ('utsr', 0.028), ('vhe', 0.028), ('appeared', 0.028), ('features', 0.027), ('ming', 0.027), ('nearest', 0.026), ('expresses', 0.026), ('nus', 0.026), ('certificate', 0.026), ('girardi', 0.026), ('ikoma', 0.026), ('kindly', 0.026), ('lemmatize', 0.026), ('proficiency', 0.026), ('saito', 0.026), ('takayama', 0.026), ('yangyang', 0.026), ('retrieved', 0.026), ('han', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="8-tfidf-1" href="./acl-2013-A_Learner_Corpus-based_Approach_to_Verb_Suggestion_for_ESL.html">8 acl-2013-A Learner Corpus-based Approach to Verb Suggestion for ESL</a></p>
<p>Author: Yu Sawai ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion.</p><p>2 0.38264742 <a title="8-tfidf-2" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>Author: Lis Pereira ; Erlyn Manguilimotan ; Yuji Matsumoto</p><p>Abstract: This study addresses issues of Japanese language learning concerning word combinations (collocations). Japanese learners may be able to construct grammatically correct sentences, however, these may sound “unnatural”. In this work, we analyze correct word combinations using different collocation measures and word similarity methods. While other methods use well-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1</p><p>3 0.19684094 <a title="8-tfidf-3" href="./acl-2013-Discriminative_Approach_to_Fill-in-the-Blank_Quiz_Generation_for_Language_Learners.html">122 acl-2013-Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners</a></p>
<p>Author: Keisuke Sakaguchi ; Yuki Arase ; Mamoru Komachi</p><p>Abstract: We propose discriminative methods to generate semantic distractors of fill-in-theblank quiz for language learners using a large-scale language learners’ corpus. Unlike previous studies, the proposed methods aim at satisfying both reliability and validity of generated distractors; distractors should be exclusive against answers to avoid multiple answers in one quiz, and distractors should discriminate learners’ proficiency. Detailed user evaluation with 3 native and 23 non-native speakers of English shows that our methods achieve better reliability and validity than previous methods.</p><p>4 0.11935728 <a title="8-tfidf-4" href="./acl-2013-Mr._MIRA%3A_Open-Source_Large-Margin_Structured_Learning_on_MapReduce.html">251 acl-2013-Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce</a></p>
<p>Author: Vladimir Eidelman ; Ke Wu ; Ferhan Ture ; Philip Resnik ; Jimmy Lin</p><p>Abstract: We present an open-source framework for large-scale online structured learning. Developed with the flexibility to handle cost-augmented inference problems such as statistical machine translation (SMT), our large-margin learner can be used with any decoder. Integration with MapReduce using Hadoop streaming allows efficient scaling with increasing size of training data. Although designed with a focus on SMT, the decoder-agnostic design of our learner allows easy future extension to other structured learning problems such as sequence labeling and parsing.</p><p>5 0.10658664 <a title="8-tfidf-5" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>Author: Rico Sennrich ; Holger Schwenk ; Walid Aransa</p><p>Abstract: While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also de- scribe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1BLEU over unadapted systems and single-domain adaptation.</p><p>6 0.089518107 <a title="8-tfidf-6" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>7 0.085681126 <a title="8-tfidf-7" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>8 0.085036226 <a title="8-tfidf-8" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>9 0.082068652 <a title="8-tfidf-9" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>10 0.079647653 <a title="8-tfidf-10" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>11 0.077352047 <a title="8-tfidf-11" href="./acl-2013-Language_Acquisition_and_Probabilistic_Models%3A_keeping_it_simple.html">213 acl-2013-Language Acquisition and Probabilistic Models: keeping it simple</a></p>
<p>12 0.075120777 <a title="8-tfidf-12" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<p>13 0.064740397 <a title="8-tfidf-13" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>14 0.063132264 <a title="8-tfidf-14" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>15 0.063029341 <a title="8-tfidf-15" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>16 0.062621213 <a title="8-tfidf-16" href="./acl-2013-Identifying_English_and_Hungarian_Light_Verb_Constructions%3A_A_Contrastive_Approach.html">186 acl-2013-Identifying English and Hungarian Light Verb Constructions: A Contrastive Approach</a></p>
<p>17 0.060256124 <a title="8-tfidf-17" href="./acl-2013-Reconstructing_an_Indo-European_Family_Tree_from_Non-native_English_Texts.html">299 acl-2013-Reconstructing an Indo-European Family Tree from Non-native English Texts</a></p>
<p>18 0.06006543 <a title="8-tfidf-18" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>19 0.060022827 <a title="8-tfidf-19" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>20 0.059836969 <a title="8-tfidf-20" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.171), (1, -0.011), (2, 0.023), (3, -0.034), (4, -0.025), (5, -0.047), (6, -0.067), (7, 0.017), (8, 0.027), (9, -0.017), (10, -0.046), (11, 0.05), (12, -0.029), (13, 0.021), (14, -0.09), (15, 0.017), (16, -0.053), (17, 0.042), (18, -0.015), (19, 0.005), (20, 0.197), (21, 0.011), (22, 0.157), (23, -0.039), (24, 0.223), (25, 0.177), (26, 0.008), (27, 0.024), (28, -0.019), (29, 0.028), (30, -0.031), (31, 0.034), (32, -0.172), (33, -0.11), (34, -0.088), (35, 0.168), (36, -0.099), (37, -0.006), (38, 0.059), (39, -0.113), (40, -0.075), (41, -0.155), (42, 0.077), (43, -0.076), (44, 0.137), (45, 0.103), (46, -0.078), (47, -0.053), (48, -0.181), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9437322 <a title="8-lsi-1" href="./acl-2013-A_Learner_Corpus-based_Approach_to_Verb_Suggestion_for_ESL.html">8 acl-2013-A Learner Corpus-based Approach to Verb Suggestion for ESL</a></p>
<p>Author: Yu Sawai ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion.</p><p>2 0.85336953 <a title="8-lsi-2" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>Author: Lis Pereira ; Erlyn Manguilimotan ; Yuji Matsumoto</p><p>Abstract: This study addresses issues of Japanese language learning concerning word combinations (collocations). Japanese learners may be able to construct grammatically correct sentences, however, these may sound “unnatural”. In this work, we analyze correct word combinations using different collocation measures and word similarity methods. While other methods use well-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1</p><p>3 0.8236165 <a title="8-lsi-3" href="./acl-2013-Discriminative_Approach_to_Fill-in-the-Blank_Quiz_Generation_for_Language_Learners.html">122 acl-2013-Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners</a></p>
<p>Author: Keisuke Sakaguchi ; Yuki Arase ; Mamoru Komachi</p><p>Abstract: We propose discriminative methods to generate semantic distractors of fill-in-theblank quiz for language learners using a large-scale language learners’ corpus. Unlike previous studies, the proposed methods aim at satisfying both reliability and validity of generated distractors; distractors should be exclusive against answers to avoid multiple answers in one quiz, and distractors should discriminate learners’ proficiency. Detailed user evaluation with 3 native and 23 non-native speakers of English shows that our methods achieve better reliability and validity than previous methods.</p><p>4 0.64626414 <a title="8-lsi-4" href="./acl-2013-Identifying_English_and_Hungarian_Light_Verb_Constructions%3A_A_Contrastive_Approach.html">186 acl-2013-Identifying English and Hungarian Light Verb Constructions: A Contrastive Approach</a></p>
<p>Author: Veronika Vincze ; Istvan Nagy T. ; Richard Farkas</p><p>Abstract: Here, we introduce a machine learningbased approach that allows us to identify light verb constructions (LVCs) in Hungarian and English free texts. We also present the results of our experiments on the SzegedParalellFX English–Hungarian parallel corpus where LVCs were manually annotated in both languages. With our approach, we were able to contrast the performance of our method and define language-specific features for these typologically different languages. Our presented method proved to be sufficiently robust as it achieved approximately the same scores on the two typologically different languages.</p><p>5 0.56747758 <a title="8-lsi-5" href="./acl-2013-Language_Acquisition_and_Probabilistic_Models%3A_keeping_it_simple.html">213 acl-2013-Language Acquisition and Probabilistic Models: keeping it simple</a></p>
<p>Author: Aline Villavicencio ; Marco Idiart ; Robert Berwick ; Igor Malioutov</p><p>Abstract: Hierarchical Bayesian Models (HBMs) have been used with some success to capture empirically observed patterns of under- and overgeneralization in child language acquisition. However, as is well known, HBMs are “ideal”learningsystems, assumingaccess to unlimited computational resources that may not be available to child language learners. Consequently, it remains crucial to carefully assess the use of HBMs along with alternative, possibly simpler, candidate models. This paper presents such an evaluation for a language acquisi- tion domain where explicit HBMs have been proposed: the acquisition of English dative constructions. In particular, we present a detailed, empiricallygrounded model-selection comparison of HBMs vs. a simpler alternative based on clustering along with maximum likelihood estimation that we call linear competition learning (LCL). Our results demonstrate that LCL can match HBM model performance without incurring on the high computational costs associated with HBMs.</p><p>6 0.49180284 <a title="8-lsi-6" href="./acl-2013-Reconstructing_an_Indo-European_Family_Tree_from_Non-native_English_Texts.html">299 acl-2013-Reconstructing an Indo-European Family Tree from Non-native English Texts</a></p>
<p>7 0.47608918 <a title="8-lsi-7" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>8 0.47006118 <a title="8-lsi-8" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>9 0.46860495 <a title="8-lsi-9" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<p>10 0.44163761 <a title="8-lsi-10" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>11 0.42693305 <a title="8-lsi-11" href="./acl-2013-Diathesis_alternation_approximation_for_verb_clustering.html">119 acl-2013-Diathesis alternation approximation for verb clustering</a></p>
<p>12 0.40775803 <a title="8-lsi-12" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>13 0.40398592 <a title="8-lsi-13" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>14 0.39289469 <a title="8-lsi-14" href="./acl-2013-Accurate_Word_Segmentation_using_Transliteration_and_Language_Model_Projection.html">34 acl-2013-Accurate Word Segmentation using Transliteration and Language Model Projection</a></p>
<p>15 0.38891286 <a title="8-lsi-15" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>16 0.38639295 <a title="8-lsi-16" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>17 0.37986976 <a title="8-lsi-17" href="./acl-2013-Stacking_for_Statistical_Machine_Translation.html">328 acl-2013-Stacking for Statistical Machine Translation</a></p>
<p>18 0.36761263 <a title="8-lsi-18" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>19 0.36209574 <a title="8-lsi-19" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>20 0.36187613 <a title="8-lsi-20" href="./acl-2013-Mr._MIRA%3A_Open-Source_Large-Margin_Structured_Learning_on_MapReduce.html">251 acl-2013-Mr. MIRA: Open-Source Large-Margin Structured Learning on MapReduce</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.044), (6, 0.057), (11, 0.078), (15, 0.013), (24, 0.041), (26, 0.036), (28, 0.012), (35, 0.127), (42, 0.058), (48, 0.045), (70, 0.032), (88, 0.034), (90, 0.033), (93, 0.212), (95, 0.106)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83601207 <a title="8-lda-1" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>Author: Xia Lu</p><p>Abstract: In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph (DAG). Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. Then probabilistic inference enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. Our model is not restricted to using only two values of a feature. Using imputation technique and EM algorithm it can handle missing val- ues well. Model averaging technique solves the problem of limited data. In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III and Campbell (2007). 1</p><p>same-paper 2 0.82719308 <a title="8-lda-2" href="./acl-2013-A_Learner_Corpus-based_Approach_to_Verb_Suggestion_for_ESL.html">8 acl-2013-A Learner Corpus-based Approach to Verb Suggestion for ESL</a></p>
<p>Author: Yu Sawai ; Mamoru Komachi ; Yuji Matsumoto</p><p>Abstract: We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion.</p><p>3 0.78580368 <a title="8-lda-3" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>Author: Ankit Ramteke ; Akshat Malu ; Pushpak Bhattacharyya ; J. Saketha Nath</p><p>Abstract: Thwarting and sarcasm are two uncharted territories in sentiment analysis, the former because of the lack of training corpora and the latter because of the enormous amount of world knowledge it demands. In this paper, we propose a working definition of thwarting amenable to machine learning and create a system that detects if the document is thwarted or not. We focus on identifying thwarting in product reviews, especially in the camera domain. An ontology of the camera domain is created. Thwarting is looked upon as the phenomenon of polarity reversal at a higher level of ontology compared to the polarity expressed at the lower level. This notion of thwarting defined with respect to an ontology is novel, to the best of our knowledge. A rule based implementation building upon this idea forms our baseline. We show that machine learning with annotated corpora (thwarted/nonthwarted) is more effective than the rule based system. Because of the skewed distribution of thwarting, we adopt the Areaunder-the-Curve measure of performance. To the best of our knowledge, this is the first attempt at the difficult problem of thwarting detection, which we hope will at Akshat Malu Dept. of Computer Science & Engg., Indian Institute of Technology Bombay, Mumbai, India. akshatmalu@ cse .i itb .ac .in J. Saketha Nath Dept. of Computer Science & Engg., Indian Institute of Technology Bombay, Mumbai, India. s aketh@ cse .i itb .ac .in least provide a baseline system to compare against. 1 Credits The authors thank the lexicographers at Center for Indian Language Technology (CFILT) at IIT Bombay for their support for this work. 2</p><p>4 0.72102404 <a title="8-lda-4" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>Author: Lis Pereira ; Erlyn Manguilimotan ; Yuji Matsumoto</p><p>Abstract: This study addresses issues of Japanese language learning concerning word combinations (collocations). Japanese learners may be able to construct grammatically correct sentences, however, these may sound “unnatural”. In this work, we analyze correct word combinations using different collocation measures and word similarity methods. While other methods use well-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1</p><p>5 0.69971365 <a title="8-lda-5" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>Author: Camille Guinaudeau ; Michael Strube</p><p>Abstract: We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems.</p><p>6 0.6986711 <a title="8-lda-6" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>7 0.69841087 <a title="8-lda-7" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>8 0.69641674 <a title="8-lda-8" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>9 0.69595796 <a title="8-lda-9" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>10 0.69549525 <a title="8-lda-10" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>11 0.69273096 <a title="8-lda-11" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>12 0.69267917 <a title="8-lda-12" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>13 0.69185406 <a title="8-lda-13" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>14 0.69041109 <a title="8-lda-14" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>15 0.69007647 <a title="8-lda-15" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>16 0.69004798 <a title="8-lda-16" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>17 0.6885705 <a title="8-lda-17" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>18 0.68816924 <a title="8-lda-18" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>19 0.6878742 <a title="8-lda-19" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>20 0.68633819 <a title="8-lda-20" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
