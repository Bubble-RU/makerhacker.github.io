<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-232" href="#">acl2013-232</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</h1>
<br/><p>Source: <a title="acl-2013-232-pdf" href="http://aclweb.org/anthology//P/P13/P13-1162.pdf">pdf</a></p><p>Author: Marta Recasens ; Cristian Danescu-Niculescu-Mizil ; Dan Jurafsky</p><p>Abstract: Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective inten- cs . sifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.</p><p>Reference: <a title="acl-2013-232-reference" href="../acl2013_reference/acl-2013-Linguistic_Models_for_Analyzing_and_Detecting_Biased_Language_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  cri st iand@ Abstract Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. [sent-3, score-0.083]
</p><p>2 Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. [sent-4, score-0.4]
</p><p>3 To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. [sent-5, score-0.786]
</p><p>4 The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. [sent-6, score-0.633]
</p><p>5 We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective inten-  cs  . [sent-7, score-0.276]
</p><p>6 These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. [sent-9, score-0.202]
</p><p>7 1 Introduction Writers and editors of reference works such as encyclopedias, textbooks, and scientific articles strive to keep their language unbiased. [sent-11, score-0.078]
</p><p>8 For example, Wikipedia advocates a policy called neutral point of view (NPOV), according to which articles should represent “fairly, proportionately, and as far as possible without bias, all significant views that have been published by reliable sources” (Wikipedia, 2013b). [sent-12, score-0.082]
</p><p>9 Wikipedia’s style guide asks editors to use nonjudgmental language, to indicate the relative prominence of opposing points of view, to avoid presenting uncontroversial . [sent-13, score-0.09]
</p><p>10 Understanding the linguistic realization of bias is important for linguistic theory; automatically detecting these biases is equally significant for computational linguistics. [sent-16, score-0.454]
</p><p>11 We propose to address both by using a powerful resource: edits in Wikipedia that are specifically designed to remove bias. [sent-17, score-0.422]
</p><p>12 Since Wikipedia maintains a complete revision history, the edits associated with NPOV tags allow us to compare the text in its biased (before) and unbiased (after) form, helping us better understand the linguistic realization of bias. [sent-18, score-0.707]
</p><p>13 Our work thus shares the intuition of prior NLP work applying Wikipedia’s revision history (Nelken and Yamangil, 2008; Yatskar et al. [sent-19, score-0.088]
</p><p>14 The analysis of Wikipedia’s edits provides valuable linguistic insights into the nature of biased language. [sent-21, score-0.583]
</p><p>15 The first, framing bias, is realized by subjective words or phrases linked with a particular point of view. [sent-23, score-0.302]
</p><p>16 The second class, epistemological bias, is related to linguistic features that subtly (often via presupposition) focus on the believability of a proposition. [sent-25, score-0.304]
</p><p>17 In (2), the assertive stated removes the bias introduced by claimed, which casts doubt on Kuypers’ statement. [sent-26, score-0.469]
</p><p>18 Usually, smaller cottage-style houses have been demolished to make way for these McMansions. [sent-28, score-0.089]
</p><p>19 Usually, smaller cottage-style houses have been demolished to make way for these homes. [sent-30, score-0.089]
</p><p>20 Bias is linked to the lexical and grammatical cues identified by the literature on subjectivity (Wiebe et al. [sent-35, score-0.154]
</p><p>21 , 2005; Turney, 2002), and especially stance 1650  ProceedingsS ooffita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-38, score-0.082]
</p><p>22 For exstance, framing bias is realized when  the writer of a text takes a particular position on a controversial topic and uses its metaphors and vocabulary. [sent-45, score-0.696]
</p><p>23 Our linguistic analysis identifies common classes of these subtle bias cues, including factive verbs, implicatives and other entailments, hedges, and subjective intensifiers. [sent-47, score-0.653]
</p><p>24 Using these cues could help automatically detect and correct instances of bias, by first finding biased phrases, then identifying the word that introduces the bias, and finally rewording to eliminate the bias. [sent-48, score-0.311]
</p><p>25 In this paper we propose a solution for the second of these tasks, identifying the bias-inducing word in a biased phrase. [sent-49, score-0.202]
</p><p>26 1 The NPOV Corpus from Wikipedia Given Wikipedia’s strict enforcement of an NPOV policy, we decided to build the NPOV corpus, containing Wikipedia edits that are specifically designed to remove bias. [sent-55, score-0.422]
</p><p>27 Editors are encouraged to identify and rewrite biased passages to achieve a more neutral tone, and they can use several NPOV 1The data and bias lexicon we developed are available at http : / /www . [sent-56, score-0.616]
</p><p>28 We constructed the NPOV corpus by retrieving all articles that were or had been in the NPOVdispute category3 together with their full revision history. [sent-67, score-0.125]
</p><p>29 Following Wikipedia’s terminology, we call each version of a Wikipedia article a revision, and so an article can be viewed as a set of (chronologically ordered) revisions. [sent-70, score-0.122]
</p><p>30 2 Extracting Edits Meant to Remove Bias Given all the revisions of a page, we extracted the changes between pairs of revisions with the wordmode diff function from the Diff Match and Patch library. [sent-72, score-0.243]
</p><p>31 5 We refer to these changes between revisions as edits, e. [sent-73, score-0.1]
</p><p>32 Our assumption was that among the edits happening in NPOV disputes, we would have a high density of edits intended to remove bias, which we call bias-driven edits, like (1) and (2) from Section 1. [sent-81, score-0.803]
</p><p>33 But many other edits occur even in NPOV disputes, including edits to fix spelling or grammatical errors, simplify the language, make the meaning more precise, or even vandalism (Max 2{{POV}}, {{POV-check}}, {{POV-section}}, etc. [sent-82, score-0.762]
</p><p>34 Add{in{gP tOheVs}e }t,ags{ d{iPspOlaVy-sc hae tcekm}p}l,ate{ s{uPcOhV Va-ss “eTcthieon nn}e}u,trality  of this article is disputed. [sent-83, score-0.061]
</p><p>35 We considered as bias-driven edits those that appeared in a revision whose comment mentioned (N)POV, e. [sent-95, score-0.469]
</p><p>36 , Attempts at presenting some claims in more NPOV way; or merging in a passage from the researchers article after basic NPOVing. [sent-97, score-0.061]
</p><p>37 We only kept edits whose before and after forms contained five or fewer words, and discarded those that only added a hyperlink or that involved a minimal change (character-based Levenshtein distance < 4). [sent-98, score-0.381]
</p><p>38 The final number of biasdriven edits for each of the data sets is shown in  the “Edits” column of Table 1. [sent-99, score-0.381]
</p><p>39 3 Linguistic Analysis Style guides talk about biased language in a prescriptive manner, listing a few words that should be avoided because they are flattering, vague, or endorse a particular point of view (Wikipedia, 2013a). [sent-101, score-0.202]
</p><p>40 Our focus is on analyzing actual biased text and bias-driven edits extracted from Wikipedia. [sent-102, score-0.583]
</p><p>41 As we suggested above, this analysis uncovered two major classes of bias: epistemological bias and framing bias. [sent-103, score-0.804]
</p><p>42 Table 2 shows the distribution (from a sample of 100 edits) of the different types and subtypes of bias presented in this section. [sent-104, score-0.364]
</p><p>43 (A) Epistemological bias involves propositions that are either commonly agreed to be true or commonly agreed to be false and that are subtly presupposed, entailed, asserted or hedged in the text. [sent-105, score-0.475]
</p><p>44 Factive verbs (Kiparsky and Kiparsky, 1970) presuppose the truth of their complement clause. [sent-107, score-0.199]
</p><p>45 In (3-a) and (4-a), realize and reveal presuppose the truth of “the oppression of black people. [sent-108, score-0.231]
</p><p>46 ”, whereas (3-b) and (4-b) present the two propositions as somebody’s  stand or an experimental result. [sent-114, score-0.051]
</p><p>47 He realized that the oppression of black people was more of a result of economic exploitation than anything innately racist. [sent-116, score-0.158]
</p><p>48 His stand was that the oppression of black people was more of a result of economic exploitation than anything innately racist. [sent-118, score-0.122]
</p><p>49 Epistemological bias  % 43  - Factive verbs  3  - Entailments  25  - Assertives  11  - Hedges B. [sent-124, score-0.405]
</p><p>50 Framing bias  4 57  - Intensifiers  19  - One-sided terms  38  Table 2: Proportion of the different bias  types. [sent-125, score-0.728]
</p><p>51 Entailments are directional relations that hold whenever the truth of one word or phrase follows from another, e. [sent-127, score-0.109]
</p><p>52 , murder entails kill because there cannot be murdering without killing (5). [sent-129, score-0.123]
</p><p>53 However, murder entails killing in an unlawful, premeditated way. [sent-130, score-0.123]
</p><p>54 This class includes implicative verbs (Karttunen, 1971), which imply the truth or untruth of their complement, depending on the polarity of the main predicate. [sent-131, score-0.15]
</p><p>55 In (6-a), coerced into accepting entails accepting in an unwilling way. [sent-132, score-0.215]
</p><p>56 After he murdered three policemen, the colony proclaimed Kelly a wanted outlaw. [sent-134, score-0.098]
</p><p>57 After he killed three policemen, the colony proclaimed Kelly a wanted outlaw. [sent-136, score-0.098]
</p><p>58 A computer engineer who was coerced into accepting a plea bargain. [sent-138, score-0.189]
</p><p>59 The truth of the proposition is not presupposed, but its level of certainty depends on the asserting verb. [sent-143, score-0.207]
</p><p>60 Whereas verbs of saying like say and state are usually  neutral,  point out and claim cast doubt on the certainty of the proposition. [sent-144, score-0.169]
</p><p>61 The “no Boeing” theory is a controversial issue, even among conspiracists, many of whom have pointed out that it is disproved by . [sent-146, score-0.149]
</p><p>62 The “no Boeing” theory is a controversial issue, even among conspiracists, many of whom have said that it is disproved by. [sent-150, score-0.149]
</p><p>63 Cooper says that slavery was worse in South America and the US than Canada, but clearly states that it was a horrible and cruel practice. [sent-154, score-0.138]
</p><p>64 Cooper says that slavery was worse in South America and the US than Canada, but points out that it was a horrible and cruel practice. [sent-156, score-0.138]
</p><p>65 Hedges are used to reduce one’s commitment to the truth of a proposition, thus avoiding any bold predictions (9-b) or statements (9) a. [sent-158, score-0.109]
</p><p>66 Eliminating the profit motive will decrease the rate of medical innovation. [sent-159, score-0.049]
</p><p>67 Eliminating the profit motive may have a lower rate of medical innovation. [sent-162, score-0.049]
</p><p>68 The lower cost of living in more rural areas means a possibly higher standard of living. [sent-164, score-0.082]
</p><p>69 The lower cost of living in more rural areas means a higher standard of living. [sent-166, score-0.082]
</p><p>70 Epistemological bias is bidirectional, that is, bias can occur because doubt is cast on a proposition commonly assumed to be true, or because a presupposition or implication is made about a proposition commonly assumed to be false. [sent-167, score-0.984]
</p><p>71 If the truth of the proposition is uncontroversially accepted by the community (i. [sent-169, score-0.257]
</p><p>72 In contrast, if only a specific viewpoint agrees with its truth, then using a factive is biased. [sent-173, score-0.146]
</p><p>73 (B) Framing bias is usually more explicit than epistemological bias because it occurs when subjective or one-sided words are used, revealing the author’s stance in a particular debate (Entman,  2007). [sent-174, score-1.169]
</p><p>74 Schnabel himself did the fantastic reproductions of Basquiat’s work. [sent-178, score-0.049]
</p><p>75 Schnabel himself did the accurate reproductions of Basquiat’s work. [sent-180, score-0.049]
</p><p>76 ) where the same event can be seen from two or more opposing perspectives, like the Israeli-Palestinian conflict (Lin et al. [sent-190, score-0.049]
</p><p>77 Concerned Women for America’s major areas of political activity have consisted of opposition to gay causes, pro-life law. [sent-199, score-0.082]
</p><p>78 Concerned Women for America’s major areas of political activity have consisted of opposition to gay causes, anti-abortion law. [sent-203, score-0.082]
</p><p>79 Framing bias has been studied within the literature on stance recognition and arguing subjectivity. [sent-210, score-0.446]
</p><p>80 Because this literature has focused on identifying which side an article takes on a two-sided debate such as the Israeli-Palestinian conflict (Lin et al. [sent-211, score-0.106]
</p><p>81 , 2012; Somasundaran and Wiebe, 2010), or into one of two opposing views (Yano et al. [sent-215, score-0.049]
</p><p>82 The features used by these models include subjectivity and sentiment lexicons, counts of unigrams and  bigrams, distributional similarity, discourse relationships, and so on. [sent-218, score-0.133]
</p><p>83 The datasets used by these studies come from genres that overtly take a specific stance (e. [sent-219, score-0.147]
</p><p>84 For this reason, overtly biased opinion statements such as “I believe that. [sent-223, score-0.267]
</p><p>85 The features used by the subjectivity literature help us detect framing bias, but we also need features that capture epistemological bias expressed through presuppositions and entailments. [sent-227, score-0.941]
</p><p>86 3  Automatically Identifying Biased Language  We now show how the bias cues identified in Section 2. [sent-228, score-0.424]
</p><p>87 This is part of a potential three-step  process for detecting and correcting biased language: (1) finding biased phrases, (2) identifying the word that introduces the bias, (3) rewording to eliminate the bias. [sent-233, score-0.453]
</p><p>88 As we will see below, it can be 1653  hard even for humans to track down the sources of bias, because biases in reference works are often subtle and implicit. [sent-234, score-0.054]
</p><p>89 An automatic bias detector that can highlight the bias-inducing word(s) and draw the editors’ attention to words that need to be modified could thus be important for improving reference works like Wikipedia or even in news reporting. [sent-235, score-0.364]
</p><p>90 The ones targeting framing bias draw on previous work on sentiment and subjectivity detection (Wiebe et al. [sent-247, score-0.693]
</p><p>91 Features to capture epistemological bias are based on the bias cues identified in Section 2. [sent-250, score-1.032]
</p><p>92 Taking context into account is important given that biases can be context-dependent, especially epistemological bias since it depends on the truth of a proposition. [sent-262, score-0.771]
</p><p>93 Features 9–10 use the list of hedges from Hyland (2005), features 11–14 use the factives and assertives from Hooper (1975), features 15–16 use the implicatives from Karttunen (1971), features 19–20 use the entailments from Berant et al. [sent-265, score-0.293]
</p><p>94 (2012), features 21–25 employ the subjectivity lexicon from Riloff and Wiebe (2003), and features 26–29 use the sentiment lexicon—positive and negative words—from Liu et al. [sent-266, score-0.183]
</p><p>95 Of the 654 words included in this lexicon, 433 were unique to this lexicon (i. [sent-271, score-0.05]
</p><p>96 , recorded in neither Riloff and Wiebe’s (2003) subjectivity lexicon nor Liu et al. [sent-273, score-0.144]
</p><p>97 ’s (2005) sentiment lexicon) and represented many one-sided or controversial terms, e. [sent-274, score-0.139]
</p><p>98 Finally, we also included a “collaborative feature” that, based on the previous revisions of the edit’s article, computes the ratio between the number of times that the word was NPOV-edited and its frequency of occurrence. [sent-277, score-0.1]
</p><p>99 This feature was designed to capture framing bias specific to an article or topic. [sent-278, score-0.621]
</p><p>100 2  Baselines  Previous work on subjectivity and stance recog-  nition has been evaluated on the task of classifying documents as opinionated vs. [sent-280, score-0.176]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('npov', 0.388), ('edits', 0.381), ('bias', 0.364), ('epistemological', 0.244), ('biased', 0.202), ('framing', 0.196), ('factive', 0.146), ('wikipedia', 0.132), ('truth', 0.109), ('controversial', 0.1), ('revisions', 0.1), ('subjectivity', 0.094), ('hedges', 0.089), ('revision', 0.088), ('entailments', 0.082), ('stance', 0.082), ('america', 0.078), ('implicatives', 0.073), ('kuypers', 0.073), ('meditation', 0.073), ('oppression', 0.073), ('subjective', 0.07), ('wiebe', 0.067), ('overtly', 0.065), ('proposition', 0.062), ('article', 0.061), ('cues', 0.06), ('subtly', 0.06), ('accepting', 0.06), ('presupposed', 0.056), ('doubt', 0.056), ('biases', 0.054), ('propositions', 0.051), ('lexicon', 0.05), ('opposing', 0.049), ('albums', 0.049), ('assertive', 0.049), ('assertives', 0.049), ('basquiat', 0.049), ('coerced', 0.049), ('colombian', 0.049), ('colony', 0.049), ('conrad', 0.049), ('conspiracists', 0.049), ('cruel', 0.049), ('demolished', 0.049), ('disproved', 0.049), ('hooper', 0.049), ('innately', 0.049), ('kiparsky', 0.049), ('mcmansion', 0.049), ('motive', 0.049), ('policemen', 0.049), ('presuppose', 0.049), ('proclaimed', 0.049), ('reproductions', 0.049), ('rewording', 0.049), ('schnabel', 0.049), ('shwekey', 0.049), ('slavery', 0.049), ('uncontroversially', 0.049), ('entails', 0.046), ('corenlp', 0.045), ('debate', 0.045), ('policy', 0.045), ('rural', 0.043), ('boeing', 0.043), ('presuppositions', 0.043), ('intensifiers', 0.043), ('yano', 0.043), ('gay', 0.043), ('encyclopedias', 0.043), ('diff', 0.043), ('pov', 0.043), ('remove', 0.041), ('editors', 0.041), ('verbs', 0.041), ('cri', 0.04), ('plea', 0.04), ('murder', 0.04), ('cooper', 0.04), ('presupposition', 0.04), ('houses', 0.04), ('horrible', 0.04), ('engineer', 0.04), ('disputes', 0.04), ('stanford', 0.04), ('sentiment', 0.039), ('areas', 0.039), ('killing', 0.037), ('sents', 0.037), ('articles', 0.037), ('accepted', 0.037), ('realized', 0.036), ('realization', 0.036), ('cast', 0.036), ('kelly', 0.036), ('recasens', 0.036), ('israeli', 0.036), ('certainty', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999964 <a title="232-tfidf-1" href="./acl-2013-Linguistic_Models_for_Analyzing_and_Detecting_Biased_Language.html">232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</a></p>
<p>Author: Marta Recasens ; Cristian Danescu-Niculescu-Mizil ; Dan Jurafsky</p><p>Abstract: Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective inten- cs . sifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.</p><p>2 0.10931554 <a title="232-tfidf-2" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>Author: Oliver Ferschke ; Iryna Gurevych ; Marc Rittberger</p><p>Abstract: With the increasing amount of user generated reference texts in the web, automatic quality assessment has become a key challenge. However, only a small amount of annotated data is available for training quality assessment systems. Wikipedia contains a large amount of texts annotated with cleanup templates which identify quality flaws. We show that the distribution of these labels is topically biased, since they cannot be applied freely to any arbitrary article. We argue that it is necessary to consider the topical restrictions of each label in order to avoid a sampling bias that results in a skewed classifier and overly optimistic evaluation results. . We factor out the topic bias by extracting reliable training instances from the revision history which have a topic distribution similar to the labeled articles. This approach better reflects the situation a classifier would face in a real-life application.</p><p>3 0.098270498 <a title="232-tfidf-3" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>4 0.096156046 <a title="232-tfidf-4" href="./acl-2013-Extra-Linguistic_Constraints_on_Stance_Recognition_in_Ideological_Debates.html">151 acl-2013-Extra-Linguistic Constraints on Stance Recognition in Ideological Debates</a></p>
<p>Author: Kazi Saidul Hasan ; Vincent Ng</p><p>Abstract: Determining the stance expressed by an author from a post written for a twosided debate in an online debate forum is a relatively new problem. We seek to improve Anand et al.’s (201 1) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification.</p><p>5 0.092811055 <a title="232-tfidf-5" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>Author: Svitlana Volkova ; Theresa Wilson ; David Yarowsky</p><p>Abstract: We study subjective language media and create Twitter-specific lexicons via bootstrapping sentiment-bearing terms from multilingual Twitter streams. Starting with a domain-independent, highprecision sentiment lexicon and a large pool of unlabeled data, we bootstrap Twitter-specific sentiment lexicons, using a small amount of labeled data to guide the process. Our experiments on English, Spanish and Russian show that the resulting lexicons are effective for sentiment classification for many underexplored languages in social media.</p><p>6 0.076580532 <a title="232-tfidf-6" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>7 0.065697178 <a title="232-tfidf-7" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>8 0.063577801 <a title="232-tfidf-8" href="./acl-2013-An_annotated_corpus_of_quoted_opinions_in_news_articles.html">49 acl-2013-An annotated corpus of quoted opinions in news articles</a></p>
<p>9 0.062013954 <a title="232-tfidf-9" href="./acl-2013-Bi-directional_Inter-dependencies_of_Subjective_Expressions_and_Targets_and_their_Value_for_a_Joint_Model.html">67 acl-2013-Bi-directional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model</a></p>
<p>10 0.056863867 <a title="232-tfidf-10" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>11 0.055504851 <a title="232-tfidf-11" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>12 0.053808965 <a title="232-tfidf-12" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>13 0.052537911 <a title="232-tfidf-13" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>14 0.052396532 <a title="232-tfidf-14" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>15 0.047279503 <a title="232-tfidf-15" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>16 0.045401104 <a title="232-tfidf-16" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>17 0.044631548 <a title="232-tfidf-17" href="./acl-2013-Automatic_detection_of_deception_in_child-produced_speech_using_syntactic_complexity_features.html">63 acl-2013-Automatic detection of deception in child-produced speech using syntactic complexity features</a></p>
<p>18 0.043268766 <a title="232-tfidf-18" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>19 0.042891115 <a title="232-tfidf-19" href="./acl-2013-Evaluating_Text_Segmentation_using_Boundary_Edit_Distance.html">140 acl-2013-Evaluating Text Segmentation using Boundary Edit Distance</a></p>
<p>20 0.042710118 <a title="232-tfidf-20" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.118), (1, 0.091), (2, -0.012), (3, 0.029), (4, -0.017), (5, -0.016), (6, -0.017), (7, 0.001), (8, 0.027), (9, 0.031), (10, -0.01), (11, 0.012), (12, -0.048), (13, -0.001), (14, -0.066), (15, -0.007), (16, -0.017), (17, 0.019), (18, 0.002), (19, -0.028), (20, 0.001), (21, 0.001), (22, -0.023), (23, 0.064), (24, 0.037), (25, 0.011), (26, -0.066), (27, -0.044), (28, -0.007), (29, 0.016), (30, -0.02), (31, -0.035), (32, 0.015), (33, -0.005), (34, -0.005), (35, 0.024), (36, 0.075), (37, 0.014), (38, -0.096), (39, -0.078), (40, 0.041), (41, 0.024), (42, 0.048), (43, -0.034), (44, 0.051), (45, -0.023), (46, -0.008), (47, -0.134), (48, 0.005), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89838701 <a title="232-lsi-1" href="./acl-2013-Linguistic_Models_for_Analyzing_and_Detecting_Biased_Language.html">232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</a></p>
<p>Author: Marta Recasens ; Cristian Danescu-Niculescu-Mizil ; Dan Jurafsky</p><p>Abstract: Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective inten- cs . sifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.</p><p>2 0.80734533 <a title="232-lsi-2" href="./acl-2013-Extra-Linguistic_Constraints_on_Stance_Recognition_in_Ideological_Debates.html">151 acl-2013-Extra-Linguistic Constraints on Stance Recognition in Ideological Debates</a></p>
<p>Author: Kazi Saidul Hasan ; Vincent Ng</p><p>Abstract: Determining the stance expressed by an author from a post written for a twosided debate in an online debate forum is a relatively new problem. We seek to improve Anand et al.’s (201 1) approach to debate stance classification by modeling two types of soft extra-linguistic constraints on the stance labels of debate posts, user-interaction constraints and ideology constraints. Experimental results on four datasets demonstrate the effectiveness of these inter-post constraints in improving debate stance classification.</p><p>3 0.75801033 <a title="232-lsi-3" href="./acl-2013-Public_Dialogue%3A_Analysis_of_Tolerance_in_Online_Discussions.html">287 acl-2013-Public Dialogue: Analysis of Tolerance in Online Discussions</a></p>
<p>Author: Arjun Mukherjee ; Vivek Venkataraman ; Bing Liu ; Sharon Meraz</p><p>Abstract: Social media platforms have enabled people to freely express their views and discuss issues of interest with others. While it is important to discover the topics in discussions, it is equally useful to mine the nature of such discussions or debates and the behavior of the participants. There are many questions that can be asked. One key question is whether the participants give reasoned arguments with justifiable claims via constructive debates or exhibit dogmatism and egotistic clashes of ideologies. The central idea of this question is tolerance, which is a key concept in the field of communications. In this work, we perform a computational study of tolerance in the context of online discussions. We aim to identify tolerant vs. intolerant participants and investigate how disagreement affects tolerance in discussions in a quantitative framework. To the best of our knowledge, this is the first such study. Our experiments using real-life discussions demonstrate the effective- ness of the proposed technique and also provide some key insights into the psycholinguistic phenomenon of tolerance in online discussions.</p><p>4 0.71133912 <a title="232-lsi-4" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>Author: Arjun Mukherjee ; Bing Liu</p><p>Abstract: Online discussion forums are a popular platform for people to voice their opinions on any subject matter and to discuss or debate any issue of interest. In forums where users discuss social, political, or religious issues, there are often heated debates among users or participants. Existing research has studied mining of user stances or camps on certain issues, opposing perspectives, and contention points. In this paper, we focus on identifying the nature of interactions among user pairs. The central questions are: How does each pair of users interact with each other? Does the pair of users mostly agree or disagree? What is the lexicon that people often use to express agreement and disagreement? We present a topic model based approach to answer these questions. Since agreement and disagreement expressions are usually multiword phrases, we propose to employ a ranking method to identify highly relevant phrases prior to topic modeling. After modeling, we use the modeling results to classify the nature of interaction of each user pair. Our evaluation results using real-life discussion/debate posts demonstrate the effectiveness of the proposed techniques.</p><p>5 0.68526983 <a title="232-lsi-5" href="./acl-2013-A_computational_approach_to_politeness_with_application_to_social_factors.html">30 acl-2013-A computational approach to politeness with application to social factors</a></p>
<p>Author: Cristian Danescu-Niculescu-Mizil ; Moritz Sudhof ; Dan Jurafsky ; Jure Leskovec ; Christopher Potts</p><p>Abstract: We propose a computational framework for identifying linguistic aspects of politeness. Our starting point is a new corpus of requests annotated for politeness, which we use to evaluate aspects of politeness theory and to uncover new interactions between politeness markers and context. These findings guide our construction of a classifier with domain-independent lexical and syntactic features operationalizing key components of politeness theory, such as indirection, deference, impersonalization and modality. Our classifier achieves close to human performance and is effective across domains. We use our framework to study the relationship between po- liteness and social power, showing that polite Wikipedia editors are more likely to achieve high status through elections, but, once elevated, they become less polite. We see a similar negative correlation between politeness and power on Stack Exchange, where users at the top of the reputation scale are less polite than those at the bottom. Finally, we apply our classifier to a preliminary analysis of politeness variation by gender and community.</p><p>6 0.62647539 <a title="232-lsi-6" href="./acl-2013-An_annotated_corpus_of_quoted_opinions_in_news_articles.html">49 acl-2013-An annotated corpus of quoted opinions in news articles</a></p>
<p>7 0.59618551 <a title="232-lsi-7" href="./acl-2013-Patient_Experience_in_Online_Support_Forums%3A_Modeling_Interpersonal_Interactions_and_Medication_Use.html">278 acl-2013-Patient Experience in Online Support Forums: Modeling Interpersonal Interactions and Medication Use</a></p>
<p>8 0.563263 <a title="232-lsi-8" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>9 0.56020027 <a title="232-lsi-9" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>10 0.53547049 <a title="232-lsi-10" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>11 0.53187811 <a title="232-lsi-11" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>12 0.53119534 <a title="232-lsi-12" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>13 0.51592529 <a title="232-lsi-13" href="./acl-2013-Bi-directional_Inter-dependencies_of_Subjective_Expressions_and_Targets_and_their_Value_for_a_Joint_Model.html">67 acl-2013-Bi-directional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model</a></p>
<p>14 0.49834985 <a title="232-lsi-14" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>15 0.47107556 <a title="232-lsi-15" href="./acl-2013-Crawling_microblogging_services_to_gather_language-classified_URLs._Workflow_and_case_study.html">95 acl-2013-Crawling microblogging services to gather language-classified URLs. Workflow and case study</a></p>
<p>16 0.46843088 <a title="232-lsi-16" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>17 0.44643858 <a title="232-lsi-17" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>18 0.43882707 <a title="232-lsi-18" href="./acl-2013-Automatic_detection_of_deception_in_child-produced_speech_using_syntactic_complexity_features.html">63 acl-2013-Automatic detection of deception in child-produced speech using syntactic complexity features</a></p>
<p>19 0.43529156 <a title="232-lsi-19" href="./acl-2013-A_user-centric_model_of_voting_intention_from_Social_Media.html">33 acl-2013-A user-centric model of voting intention from Social Media</a></p>
<p>20 0.4331871 <a title="232-lsi-20" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.028), (6, 0.024), (11, 0.042), (15, 0.492), (24, 0.044), (26, 0.05), (35, 0.063), (42, 0.03), (48, 0.026), (63, 0.021), (70, 0.025), (88, 0.02), (90, 0.013), (95, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.8944369 <a title="232-lda-1" href="./acl-2013-Linguistic_Models_for_Analyzing_and_Detecting_Biased_Language.html">232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</a></p>
<p>Author: Marta Recasens ; Cristian Danescu-Niculescu-Mizil ; Dan Jurafsky</p><p>Abstract: Unbiased language is a requirement for reference sources like encyclopedias and scientific texts. Bias is, nonetheless, ubiquitous, making it crucial to understand its nature and linguistic realization and hence detect bias automatically. To this end we analyze real instances of human edits designed to remove bias from Wikipedia articles. The analysis uncovers two classes of bias: framing bias, such as praising or perspective-specific words, which we link to the literature on subjectivity; and epistemological bias, related to whether propositions that are presupposed or entailed in the text are uncontroversially accepted as true. We identify common linguistic cues for these classes, including factive verbs, implicatives, hedges, and subjective inten- cs . sifiers. These insights help us develop features for a model to solve a new prediction task of practical importance: given a biased sentence, identify the bias-inducing word. Our linguistically-informed model performs almost as well as humans tested on the same task.</p><p>2 0.73157126 <a title="232-lda-2" href="./acl-2013-Integrating_Multiple_Dependency_Corpora_for_Inducing_Wide-coverage_Japanese_CCG_Resources.html">199 acl-2013-Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</a></p>
<p>Author: Sumire Uematsu ; Takuya Matsuzaki ; Hiroki Hanaoka ; Yusuke Miyao ; Hideki Mima</p><p>Abstract: This paper describes a method of inducing wide-coverage CCG resources for Japanese. While deep parsers with corpusinduced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexi- con and the accuracy of parsing.</p><p>3 0.64069033 <a title="232-lda-3" href="./acl-2013-Offspring_from_Reproduction_Problems%3A_What_Replication_Failure_Teaches_Us.html">262 acl-2013-Offspring from Reproduction Problems: What Replication Failure Teaches Us</a></p>
<p>Author: Antske Fokkens ; Marieke van Erp ; Marten Postma ; Ted Pedersen ; Piek Vossen ; Nuno Freire</p><p>Abstract: Repeating experiments is an important instrument in the scientific toolbox to validate previous work and build upon existing work. We present two concrete use cases involving key techniques in the NLP domain for which we show that reproducing results is still difficult. We show that the deviation that can be found in reproduction efforts leads to questions about how our results should be interpreted. Moreover, investigating these deviations provides new insights and a deeper understanding of the examined techniques. We identify five aspects that can influence the outcomes of experiments that are typically not addressed in research papers. Our use cases show that these aspects may change the answer to research questions leading us to conclude that more care should be taken in interpreting our results and more research involving systematic testing of methods is required in our field.</p><p>4 0.57574344 <a title="232-lda-4" href="./acl-2013-SEMILAR%3A_The_Semantic_Similarity_Toolkit.html">304 acl-2013-SEMILAR: The Semantic Similarity Toolkit</a></p>
<p>Author: Vasile Rus ; Mihai Lintean ; Rajendra Banjade ; Nobal Niraula ; Dan Stefanescu</p><p>Abstract: We present in this paper SEMILAR, the SEMantic simILARity toolkit. SEMILAR implements a number of algorithms for assessing the semantic similarity between two texts. It is available as a Java library and as a Java standalone ap-plication offering GUI-based access to the implemented semantic similarity methods. Furthermore, it offers facilities for manual se-mantic similarity annotation by experts through its component SEMILAT (a SEMantic simILarity Annotation Tool). 1</p><p>5 0.54983836 <a title="232-lda-5" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>Author: Weiwei Guo ; Hao Li ; Heng Ji ; Mona Diab</p><p>Abstract: Many current Natural Language Processing [NLP] techniques work well assuming a large context of text as input data. However they become ineffective when applied to short texts such as Twitter feeds. To overcome the issue, we want to find a related newswire document to a given tweet to provide contextual support for NLP tasks. This requires robust modeling and understanding of the semantics of short texts. The contribution of the paper is two-fold: 1. we introduce the Linking-Tweets-toNews task as well as a dataset of linked tweet-news pairs, which can benefit many NLP applications; 2. in contrast to previ- ous research which focuses on lexical features within the short texts (text-to-word information), we propose a graph based latent variable model that models the inter short text correlations (text-to-text information). This is motivated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task.</p><p>6 0.54109883 <a title="232-lda-6" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>7 0.35113505 <a title="232-lda-7" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>8 0.33994266 <a title="232-lda-8" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>9 0.33684918 <a title="232-lda-9" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>10 0.33024043 <a title="232-lda-10" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>11 0.32480749 <a title="232-lda-11" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>12 0.32284129 <a title="232-lda-12" href="./acl-2013-Automatic_detection_of_deception_in_child-produced_speech_using_syntactic_complexity_features.html">63 acl-2013-Automatic detection of deception in child-produced speech using syntactic complexity features</a></p>
<p>13 0.32153487 <a title="232-lda-13" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>14 0.32107934 <a title="232-lda-14" href="./acl-2013-From_Natural_Language_Specifications_to_Program_Input_Parsers.html">163 acl-2013-From Natural Language Specifications to Program Input Parsers</a></p>
<p>15 0.31794268 <a title="232-lda-15" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>16 0.31382075 <a title="232-lda-16" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>17 0.31266314 <a title="232-lda-17" href="./acl-2013-A_computational_approach_to_politeness_with_application_to_social_factors.html">30 acl-2013-A computational approach to politeness with application to social factors</a></p>
<p>18 0.30661938 <a title="232-lda-18" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>19 0.3065801 <a title="232-lda-19" href="./acl-2013-Evaluating_Text_Segmentation_using_Boundary_Edit_Distance.html">140 acl-2013-Evaluating Text Segmentation using Boundary Edit Distance</a></p>
<p>20 0.30576569 <a title="232-lda-20" href="./acl-2013-A_user-centric_model_of_voting_intention_from_Social_Media.html">33 acl-2013-A user-centric model of voting intention from Social Media</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
