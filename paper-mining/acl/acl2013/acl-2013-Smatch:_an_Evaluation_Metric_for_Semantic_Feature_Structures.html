<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-324" href="#">acl2013-324</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</h1>
<br/><p>Source: <a title="acl-2013-324-pdf" href="http://aclweb.org/anthology//P/P13/P13-2131.pdf">pdf</a></p><p>Author: Shu Cai ; Kevin Knight</p><p>Abstract: The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study.</p><p>Reference: <a title="acl-2013-324-reference" href="../acl2013_reference/acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Smatch: an Evaluation Metric for Semantic Feature Structures  Shu Cai USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 shucai @ i i s . [sent-1, score-0.043]
</p><p>2 edu Abstract The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. [sent-2, score-0.332]
</p><p>3 However, there is no widely-used metric to evaluate wholesentence semantic structures. [sent-3, score-0.25]
</p><p>4 In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. [sent-4, score-0.26]
</p><p>5 We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study. [sent-5, score-0.171]
</p><p>6 1 Introduction The goal of semantic parsing is to generate all semantic relationships in a text. [sent-6, score-0.208]
</p><p>7 Its output is often represented by whole-sentence semantic structures. [sent-7, score-0.084]
</p><p>8 Evaluating such structures is necessary for semantic parsing tasks, as well as semantic annotation tasks which create linguistic resources for semantic parsing. [sent-8, score-0.365]
</p><p>9 However, there is no widely-used evaluation method for whole-sentence semantic structures. [sent-9, score-0.084]
</p><p>10 Current whole-sentence semantic parsing is mainly evaluated in two ways: 1. [sent-10, score-0.124]
</p><p>11 task correctness (Tang and Mooney, 2001), which evaluates on an NLP task that uses the parsing results; 2. [sent-11, score-0.101]
</p><p>12 Nevertheless, it is worthwhile to explore evaluation methods that use scores which range from 0 to 1 (“partial credit”) to measure whole-sentence semantic structures. [sent-13, score-0.113]
</p><p>13 By using such methods, we are able to differentiate between two similar whole-  sentence semantic structures regardless of specific Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 knight @ i i s . [sent-14, score-0.2]
</p><p>14 In this work, we provide an evaluation metric that uses the degree of overlap between two whole-sentence semantic structures as the partial credit. [sent-16, score-0.3]
</p><p>15 In this paper, we observe that the difficulty of computing the degree of overlap between two whole-sentence semantic feature structures comes from determining an optimal variable alignment between them, and further prove that finding such alignment is NP-complete. [sent-17, score-0.391]
</p><p>16 We investigate how to compute this metric and provide several practical and replicable computing methods by using Integer Linear Programming (ILP) and hill-climbing method. [sent-18, score-0.164]
</p><p>17 We show that our metric can be used for measuring the annotator agreement in largescale linguistic annotation, and evaluating seman-  tic parsing. [sent-19, score-0.175]
</p><p>18 2 Semantic Overlap We work on a semantic feature structure representation in a standard neo-Davidsonian (Davidson, 1969; Parsons, 1990) framework. [sent-20, score-0.084]
</p><p>19 For example, semantics of the sentence “the boy wants to go” is represented by the following directed graph:  In this graph, there are three concepts: want01, boy, and go-01 . [sent-21, score-0.153]
</p><p>20 The frame want-01 has two arguments connected with ARG0 and ARG1, and go01 has an argument (which is also the same boy instance) connected with ARG0. [sent-23, score-0.118]
</p><p>21 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 748–752,  Following (Langkilde and Knight, 1998) and (Langkilde-Geary, 2002), we refer to this semantic representation as AMR (Abstract Meaning Representation). [sent-26, score-0.084]
</p><p>22 The difficulty  is that variable  names  are not  shared between the two AMRs, so there are multiple ways to compute  the propositional  based on different variable mappings. [sent-31, score-0.332]
</p><p>23 overlap  We there-  fore define the smatch score (for semantic match) as the maximum f-score obtainable  via a one-to-  one matching of variables between the two AMRs. [sent-32, score-0.995]
</p><p>24 In the example  above, there are six ways to  match up variables between the two AMRs: x=a x=a x=b x=b x=c x=c  , , , , , ,  y=b y=c y=a y=c y=a y=b  , , , , , ,  z =c z =b z =c z =a z =b z =a  : : : : : :  M 4 1 0 0 0 2  P 4/5 1/ 5 0/ 5 0/ 5 0/ 5 2/5  R 4/6 1/ 6 0/ 6 0/ 6 0/ 6 2/ 6  F 0 . [sent-33, score-0.153]
</p><p>25 7 3  Here, M is the number of propositional triples that agree given a variable mapping, P is the precision of the second AMR against the first, R is its recall, and F is its f-score. [sent-40, score-0.351]
</p><p>26 Exhaustively enumerating all variable mappings requires computing the f-score for n! [sent-43, score-0.198]
</p><p>27 −  3  Computing the Metric  This section describes how to compute the smatch score. [sent-51, score-0.69]
</p><p>28 For example, it would match a in the first AMR example with x in the second  AMR example of Section 2, because both are instances of want-01. [sent-57, score-0.033]
</p><p>29 If there are two or more variables to choose from, we pick the first available one. [sent-58, score-0.12]
</p><p>30 We can get an optimal solution using integer linear programming (ILP). [sent-61, score-0.072]
</p><p>31 We create two types of variables: •  •  (Variable mapping) vij = 1 iff the ith vari(aVblaer ainb lAeM mRap1 i sn mapped to the jth variable in AMR2 (otherwise vij = 0) (Triple match) tkl = 1 iff AMR1 triple (kT implatech mesa cAh)MR t2 triple l, otherwise tkl = 0. [sent-62, score-0.797]
</p><p>32 A triple relation1 (xy) matches relation2(wz) iff relation1 = relation2, vxw = 1, and vyz = 1or y and z are the same concept. [sent-63, score-0.337]
</p><p>33 Finally, we ask the ILP solver to maximize:  X tkl Xkl which denotes the maximum number of matching triples which lead to the smatch score. [sent-65, score-0.949]
</p><p>34 Finally, we develop a portable heuristic algorithm that does not require an ILP solver1 . [sent-67, score-0.029]
</p><p>35 We begin with m random one-to-one mappings between the m variables of AMR1 and the n variables of AMR2. [sent-69, score-0.331]
</p><p>36 Each variable mapping is a pair (i, map(i)) with 1 ≤ i ≤ m and 1 ≤ map(i) ≤ n. [sent-70, score-0.149]
</p><p>37 Wi))e wrefitehr t1o ≤the m mappings as a vmaaripab(il)e mapping esta rteef. [sent-71, score-0.114]
</p><p>38 e We first generate a random initial variable mapping state, compute its triple match number, then hill-climb via two types of small changes: 1. [sent-72, score-0.366]
</p><p>39 Move one of the m mappings to a currentlyunmapped variable from the n. [sent-73, score-0.165]
</p><p>40 +  Any variable mapping state has m(n m) m(m 1) = m(n 1) neighbors during )th +e hill-climbing =sea mrch(n. [sent-76, score-0.149]
</p><p>41 W −e greedily bcohrosos due rtihneg b tehset neighbor, repeating until no neighbor improves the number of triple matches. [sent-77, score-0.156]
</p><p>42 We experiment with two modifications to the greedy search: (1) executing multiple random restarts to avoid local optima, and (2) using our Baseline concept matching (“smart initialization”) −  −  −  instead of random initialization. [sent-78, score-0.169]
</p><p>43 There is unlikely to be an exact polynomial-time algorithm for computing smatch. [sent-80, score-0.033]
</p><p>44 We can reduce the 0-1 Maximum Quadratic Assignment Problem (0-1-Max-QAP) (Nagarajan and Sviridenko, 2009) and the subgraph isomorphism problem directly to the full smatch problem on graphs. [sent-81, score-0.752]
</p><p>45 Fortunately, the next section shows that the smatch methods above are efficient and effective. [sent-84, score-0.659]
</p><p>46 2Thanks to David Chiang for observing the subgraph isomorphism reduction. [sent-89, score-0.093]
</p><p>47 Our study has 4 annotators (A, B, C, D), who then converge on a consensus annotation E. [sent-92, score-0.132]
</p><p>48 Each time annotators build AMRs for 4 sentences from the Wall Street Journal corpus. [sent-99, score-0.036]
</p><p>49 Each individual smatch score is a document-level score of 4 AMR pairs. [sent-104, score-0.659]
</p><p>50 3 ILP scores are optimal, so lower scores (in bold) indicate search errors. [sent-105, score-0.058]
</p><p>51 Table 2 summarizes search accuracy as a percentage of smatch scores that equal that of ILP. [sent-106, score-0.688]
</p><p>52 Results show that the restarts are essential for hillclimbing, and that 9 restarts are sufficient to obtain good quality. [sent-107, score-0.184]
</p><p>53 The table also shows total runtimes over 200 AMR pairs (10 annotator pairs, 5 sentence groups, 4 AMR pairs per group). [sent-108, score-0.059]
</p><p>54 Heuristic search with smart initialization and 4 restarts (S+4R) gives the best trade-off between accuracy and speed, so this is the setting we use in practice. [sent-109, score-0.244]
</p><p>55 Figure 1 shows smatch scores of each annotator (A-D) against the consensus annotation (E). [sent-110, score-0.819]
</p><p>56 The 3For documents containing multiple AMRs, we use the sum of matched triples over all AMR pairs to compute precision, recall, and f-score, much like corpus-level Bleu (Papineni et al. [sent-111, score-0.181]
</p><p>57 750  Table 1: Inter-annotator smatch agreement for 5 groups of sentences, as computed with seven different methods (Base, ILP, R, 10R, S, S+4R, S+9R). [sent-113, score-0.699]
</p><p>58 96%R9 Table 2: Accuracy and running time (seconds) of various computing methods of smatch over 200 AMR pairs. [sent-123, score-0.692]
</p><p>59 plot demonstrates that, as time goes by, annotators reach better agreement with the consensus. [sent-124, score-0.076]
</p><p>60 We also note that smatch is used to measure the accuracy of machine-generated AMRs. [sent-125, score-0.659]
</p><p>61 , 2012) use it to evaluate automatic semantic parsing in a narrow domain, while Ulf Her-  mjakob4 has developed a heuristic algorithm that exploits and supplements Ontonotes annotations (Pradhan et al. [sent-127, score-0.179]
</p><p>62 , 2007) in order to automatically create AMRs for Ontonotes sentences, with a smatch score of 0. [sent-128, score-0.659]
</p><p>63 5  Related Work  Related work on directly measuring the semantic representation includes the method in (Dridan and Oepen, 2011), which evaluates semantic parser output directly by comparing semantic substructures, though they require an alignment between sentence spans and semantic sub-structures. [sent-130, score-0.396]
</p><p>64 In contrast, our metric does not require the align4personal communication  s  S  Time  Figure 1: Smatch scores of annotators (A-D) against the consensus annotation (E) over time. [sent-131, score-0.261]
</p><p>65 ment between an input sentence and its semantic analysis. [sent-132, score-0.084]
</p><p>66 , 2008) propose a metric which computes the maximum score by any alignment between LF graphs, but they do not address how to determine the alignments. [sent-134, score-0.157]
</p><p>67 6  Conclusion and Future Work  We present an evaluation metric for wholesentence semantic analysis, and show that it can be computed efficiently. [sent-135, score-0.25]
</p><p>68 We use the metric to measure semantic annotation agreement rates and parsing accuracy. [sent-136, score-0.297]
</p><p>69 In the future, we plan to investigate how to adapt smatch to other semantic representations. [sent-137, score-0.743]
</p><p>70 751 7  Acknowledgements  We would like to thank David Chiang, Hui Zhang, other ISI colleagues and our anonymous review-  ers for their thoughtful comments. [sent-138, score-0.027]
</p><p>71 Sentences  Classi-  nual Meeting on Association for Computational Linguistics. [sent-217, score-0.027]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('smatch', 0.659), ('amr', 0.333), ('amrs', 0.231), ('triples', 0.15), ('ance', 0.144), ('inst', 0.138), ('ilp', 0.129), ('triple', 0.127), ('variables', 0.12), ('smart', 0.12), ('boy', 0.118), ('wz', 0.116), ('propositional', 0.101), ('variable', 0.1), ('metric', 0.1), ('restarts', 0.092), ('tkl', 0.087), ('semantic', 0.084), ('xy', 0.083), ('initialisz', 0.066), ('irmanbdionmg', 0.066), ('vxw', 0.066), ('vyz', 0.066), ('wholesentence', 0.066), ('xvij', 0.066), ('mappings', 0.065), ('consensus', 0.063), ('ontonotes', 0.062), ('iff', 0.054), ('langkilde', 0.054), ('vij', 0.054), ('admiralty', 0.054), ('dridan', 0.054), ('nagarajan', 0.051), ('isomorphism', 0.051), ('rey', 0.051), ('overlap', 0.05), ('mapping', 0.049), ('kingsbury', 0.048), ('usc', 0.046), ('marina', 0.046), ('allen', 0.044), ('suite', 0.043), ('del', 0.043), ('subgraph', 0.042), ('pradhan', 0.042), ('structures', 0.04), ('parsing', 0.04), ('agreement', 0.04), ('tang', 0.038), ('snover', 0.038), ('integer', 0.037), ('annotators', 0.036), ('annotator', 0.035), ('wants', 0.035), ('programming', 0.035), ('zettlemoyer', 0.035), ('plus', 0.034), ('knight', 0.033), ('computing', 0.033), ('annotation', 0.033), ('match', 0.033), ('initialization', 0.032), ('evaluates', 0.031), ('compute', 0.031), ('jones', 0.03), ('correctness', 0.03), ('neighbor', 0.029), ('quadratic', 0.029), ('heuristic', 0.029), ('mesa', 0.029), ('dordrecht', 0.029), ('obtainable', 0.029), ('hillclimbing', 0.029), ('scores', 0.029), ('alignment', 0.029), ('maximum', 0.028), ('vari', 0.027), ('nual', 0.027), ('thoughtful', 0.027), ('parsons', 0.027), ('constructors', 0.027), ('exploits', 0.026), ('random', 0.026), ('degree', 0.026), ('logical', 0.026), ('palmer', 0.026), ('matching', 0.025), ('generalpurpose', 0.025), ('honor', 0.025), ('map', 0.025), ('bold', 0.025), ('assignment', 0.025), ('tr', 0.024), ('hyperedge', 0.024), ('runtimes', 0.024), ('exhaustively', 0.024), ('oepen', 0.024), ('matches', 0.024), ('mapped', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="324-tfidf-1" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>Author: Shu Cai ; Kevin Knight</p><p>Abstract: The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study.</p><p>2 0.078377672 <a title="324-tfidf-2" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>3 0.059825335 <a title="324-tfidf-3" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>Author: Dan Goldwasser ; Dan Roth</p><p>Abstract: Semantic parsing is a domain-dependent process by nature, as its output is defined over a set of domain symbols. Motivated by the observation that interpretation can be decomposed into domain-dependent and independent components, we suggest a novel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains.</p><p>4 0.058459882 <a title="324-tfidf-4" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>Author: Ndapandula Nakashole ; Tomasz Tylenda ; Gerhard Weikum</p><p>Abstract: Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and semantically typing newly emerging out-ofKB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE. Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowdsourced user studies, show our method performing significantly better than prior work.</p><p>5 0.053873774 <a title="324-tfidf-5" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>Author: David Chiang ; Jacob Andreas ; Daniel Bauer ; Karl Moritz Hermann ; Bevan Jones ; Kevin Knight</p><p>Abstract: Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing.</p><p>6 0.051967062 <a title="324-tfidf-6" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>7 0.049625073 <a title="324-tfidf-7" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>8 0.049140282 <a title="324-tfidf-8" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>9 0.048195086 <a title="324-tfidf-9" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>10 0.047911145 <a title="324-tfidf-10" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>11 0.045904309 <a title="324-tfidf-11" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>12 0.044757433 <a title="324-tfidf-12" href="./acl-2013-Margin-based_Decomposed_Amortized_Inference.html">237 acl-2013-Margin-based Decomposed Amortized Inference</a></p>
<p>13 0.043525305 <a title="324-tfidf-13" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>14 0.0432676 <a title="324-tfidf-14" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>15 0.04192353 <a title="324-tfidf-15" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>16 0.039746739 <a title="324-tfidf-16" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>17 0.039407223 <a title="324-tfidf-17" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>18 0.0392133 <a title="324-tfidf-18" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>19 0.03895786 <a title="324-tfidf-19" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>20 0.038844287 <a title="324-tfidf-20" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.114), (1, -0.007), (2, -0.011), (3, -0.037), (4, -0.037), (5, 0.016), (6, 0.003), (7, -0.021), (8, -0.017), (9, -0.012), (10, -0.03), (11, 0.02), (12, -0.05), (13, -0.061), (14, -0.006), (15, -0.004), (16, -0.003), (17, 0.009), (18, 0.033), (19, 0.019), (20, 0.001), (21, 0.005), (22, -0.026), (23, 0.041), (24, 0.0), (25, 0.025), (26, -0.004), (27, 0.031), (28, -0.026), (29, 0.031), (30, 0.027), (31, -0.039), (32, -0.02), (33, 0.024), (34, 0.041), (35, -0.017), (36, -0.03), (37, 0.002), (38, 0.018), (39, 0.0), (40, 0.064), (41, 0.076), (42, 0.004), (43, 0.033), (44, -0.008), (45, 0.056), (46, -0.03), (47, -0.047), (48, -0.018), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86679685 <a title="324-lsi-1" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>Author: Shu Cai ; Kevin Knight</p><p>Abstract: The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study.</p><p>2 0.60464269 <a title="324-lsi-2" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>Author: Gerasimos Lampouras ; Ion Androutsopoulos</p><p>Abstract: We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.</p><p>3 0.58564609 <a title="324-lsi-3" href="./acl-2013-Fluid_Construction_Grammar_for_Historical_and_Evolutionary_Linguistics.html">161 acl-2013-Fluid Construction Grammar for Historical and Evolutionary Linguistics</a></p>
<p>Author: Pieter Wellens ; Remi van Trijp ; Katrien Beuls ; Luc Steels</p><p>Abstract: Fluid Construction Grammar (FCG) is an open-source computational grammar formalism that is becoming increasingly popular for studying the history and evolution of language. This demonstration shows how FCG can be used to operationalise the cultural processes and cognitive mechanisms that underly language evolution and change.</p><p>4 0.58044446 <a title="324-lsi-4" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>Author: Joohyun Kim ; Raymond Mooney</p><p>Abstract: We adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees.</p><p>5 0.57473779 <a title="324-lsi-5" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>Author: Matthew R. Gormley ; Jason Eisner</p><p>Abstract: Many models in NLP involve latent variables, such as unknown parses, tags, or alignments. Finding the optimal model parameters is then usually a difficult nonconvex optimization problem. The usual practice is to settle for local optimization methods such as EM or gradient ascent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ?) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Linearization Technique to produce convex relaxations during branch-and-bound. Although these techniques do not yet provide a practical solution to our instance of this NP-hard problem, they sometimes find better solutions than Viterbi EM with random restarts, in the same time.</p><p>6 0.55699676 <a title="324-lsi-6" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>7 0.54590291 <a title="324-lsi-7" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>8 0.53104848 <a title="324-lsi-8" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>9 0.51835054 <a title="324-lsi-9" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>10 0.51286429 <a title="324-lsi-10" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>11 0.51249373 <a title="324-lsi-11" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>12 0.50687999 <a title="324-lsi-12" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>13 0.50179917 <a title="324-lsi-13" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>14 0.49088001 <a title="324-lsi-14" href="./acl-2013-Supervised_Model_Learning_with_Feature_Grouping_based_on_a_Discrete_Constraint.html">334 acl-2013-Supervised Model Learning with Feature Grouping based on a Discrete Constraint</a></p>
<p>15 0.48973471 <a title="324-lsi-15" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>16 0.48878965 <a title="324-lsi-16" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>17 0.48361856 <a title="324-lsi-17" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>18 0.47777212 <a title="324-lsi-18" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<p>19 0.47617781 <a title="324-lsi-19" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>20 0.47443956 <a title="324-lsi-20" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.047), (6, 0.044), (11, 0.052), (14, 0.015), (15, 0.02), (24, 0.024), (26, 0.029), (35, 0.072), (42, 0.042), (48, 0.055), (64, 0.015), (70, 0.069), (85, 0.314), (88, 0.014), (90, 0.013), (95, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75188315 <a title="324-lda-1" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>Author: Shu Cai ; Kevin Knight</p><p>Abstract: The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study.</p><p>2 0.68531245 <a title="324-lda-2" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>Author: Yoav Goldberg ; Kai Zhao ; Liang Huang</p><p>Abstract: Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empiri- cally. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treeb∼a2nxk sentences, a bnads are eosrd oenrs P eofn magnitude faster on much longer sentences.</p><p>3 0.64730161 <a title="324-lda-3" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>Author: Nathan Gilbert ; Ellen Riloff</p><p>Abstract: Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.</p><p>4 0.63892114 <a title="324-lda-4" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.</p><p>5 0.56252557 <a title="324-lda-5" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>Author: Graham Neubig</p><p>Abstract: In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http : / /phont ron . com/t ravat ar</p><p>6 0.47119635 <a title="324-lda-6" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>7 0.46668738 <a title="324-lda-7" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>8 0.4662075 <a title="324-lda-8" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>9 0.46389362 <a title="324-lda-9" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>10 0.45956933 <a title="324-lda-10" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>11 0.45935562 <a title="324-lda-11" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>12 0.45905939 <a title="324-lda-12" href="./acl-2013-Models_of_Semantic_Representation_with_Visual_Attributes.html">249 acl-2013-Models of Semantic Representation with Visual Attributes</a></p>
<p>13 0.45905578 <a title="324-lda-13" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>14 0.45899108 <a title="324-lda-14" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>15 0.45867109 <a title="324-lda-15" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>16 0.45849332 <a title="324-lda-16" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>17 0.45798746 <a title="324-lda-17" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>18 0.45741799 <a title="324-lda-18" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>19 0.45740426 <a title="324-lda-19" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>20 0.45700309 <a title="324-lda-20" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
