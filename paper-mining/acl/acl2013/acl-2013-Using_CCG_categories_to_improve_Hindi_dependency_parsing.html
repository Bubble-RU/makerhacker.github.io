<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>372 acl-2013-Using CCG categories to improve Hindi dependency parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-372" href="#">acl2013-372</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>372 acl-2013-Using CCG categories to improve Hindi dependency parsing</h1>
<br/><p>Source: <a title="acl-2013-372-pdf" href="http://aclweb.org/anthology//P/P13/P13-2107.pdf">pdf</a></p><p>Author: Bharat Ram Ambati ; Tejaswini Deoskar ; Mark Steedman</p><p>Abstract: We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery.</p><p>Reference: <a title="acl-2013-372-reference" href="../acl2013_reference/acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Using CCG categories to improve Hindi dependency parsing Bharat Ram Ambati Tejaswini Deoskar Mark Steedman Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh bharat . [sent-1, score-0.363]
</p><p>2 uk @  Abstract We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. [sent-7, score-0.364]
</p><p>3 We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. [sent-8, score-0.373]
</p><p>4 We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). [sent-9, score-0.325]
</p><p>5 Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery. [sent-10, score-0.198]
</p><p>6 1 Introduction  As compared to English, many Indian languages including Hindi have a freer word order and are also morphologically richer. [sent-11, score-0.045]
</p><p>7 Today, the best dependency parsing accuracies for Hindi are obtained by the shift-reduce parser of Nivre et al. [sent-13, score-0.224]
</p><p>8 It has been observed that Malt is relatively accurate at recovering short distance dependencies, like arguments of a verb, but is less accurate at recovering long distance dependencies like co-ordination, root of the sentence, etc (Mcdonald and Nivre, 2007; Ambati et al. [sent-15, score-0.538]
</p><p>9 In this work, we show that using CCG lexical categories (Steedman, 2000), which contain subcategorization information and capture long distance dependencies elegantly, can help Malt with those dependencies. [sent-17, score-0.321]
</p><p>10 Section 2 first shows how we extract a CCG lexicon from an existing Hindi dependency treebank (Bhatt et al. [sent-18, score-0.373]
</p><p>11 In section 3, we develop a supertagger using the CCGbank and explore different ways of providing CCG categories from the supertagger as features to Malt. [sent-20, score-0.356]
</p><p>12 Our re-  sults show that using CCG categories can help Malt by improving the recovery of long distance relations. [sent-21, score-0.267]
</p><p>13 (2009) created a CCGbank from an Italian dependency treebank by converting dependency trees into phrase structure trees and then applying an algorithm similar to Hockenmaier and Steedman (2007). [sent-24, score-0.507]
</p><p>14 In this work, following C ¸akıcı (2005), we first extract a Hindi CCG lexicon from a dependency treebank. [sent-25, score-0.208]
</p><p>15 We then use a CKY parser based on the CCG formalism to automatically obtain a treebank of CCG derivations from this lexicon, a novel methodology that may be applicable to obtaining CCG treebanks in other languages as well. [sent-26, score-0.401]
</p><p>16 5) released as part of Coling 2012 Shared Task on parsing (Bharati et al. [sent-29, score-0.058]
</p><p>17 HDT is a multi-layered dependency treebank (Bhatt et al. [sent-31, score-0.323]
</p><p>18 , 2009) annotated with morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) information (Bharati et al. [sent-32, score-0.223]
</p><p>19 Dependency labels are fine-grained, and mark dependencies that are syntactico-semantic in nature, such as agent (usually corresponding to subject), patient (object), and time and place expressions. [sent-35, score-0.224]
</p><p>20 There are special labels to mark long distance relations like relative clauses, co-ordination etc 604  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-36, score-0.249]
</p><p>21 The treebank contains 12,041 training, 1,233 development and 1,828 testing sentences with an average of 22 words per sentence. [sent-41, score-0.165]
</p><p>22 We used the  CoNLL format1 for our purposes, which contains word, lemma, pos-tag, and coarse pos-tag in the WORD, LEMMA, POS, and CPOS fields respectively and morphological features and chunk information in the FEATS column. [sent-42, score-0.328]
</p><p>23 2 Algorithm We first made a list of argument and adjunct dependency labels in the treebank. [sent-44, score-0.29]
</p><p>24 , dependencies with the label k1 and k2 (corresponding to subject and object respectively) are considered to be arguments, while labels like k7p and k7t (corresponding to place and time expressions) are considered to be adjuncts. [sent-47, score-0.198]
</p><p>25 For readability reasons, we will henceforth refer to dependency labels with their English equivalents (e. [sent-48, score-0.294]
</p><p>26 , SUBJ, OBJ, PURPOSE, CASE for k1, k2 , rt, lwg psp respectively). [sent-50, score-0.048]
</p><p>27 Starting from the root of the dependency tree, we traverse each node. [sent-51, score-0.271]
</p><p>28 The category of a node depends on both its parent and children. [sent-52, score-0.275]
</p><p>29 If the node is an argument of its parent, we assign the chunk tag of the node (e. [sent-53, score-0.455]
</p><p>30 Otherwise, we assign it a category of X | X, where X is the parent’s result category and | is directionality (\ or /), ws hreicsuhl depends on tdh |e position noftahliet yno (\de o rw. [sent-56, score-0.392]
</p><p>31 nTdhse ornes uthlte category ooff a node is the category obtained once its arguments are resolved. [sent-60, score-0.342]
</p><p>32 For example, S, is the result category for ( S\NP ) \NP. [sent-61, score-0.1]
</p><p>33 Once we get the partial category foofr a nSo\dNeP P b)a\seNdP on tnhcee wnoede g’est parent information, we traverse through the children of the node. [sent-62, score-0.336]
</p><p>34 If a child is an argument, we add that child’s chunk tag, with appropriate directionality, to the node’s category. [sent-63, score-0.261]
</p><p>35 The algorithm is sketched in Figure 1 and an example of a CCG derivation for a simple sentence (marked with chunk tags; NP and VGF are the chunk tags for noun and finite verb chunks respectively. [sent-64, score-0.549]
</p><p>36 In Type 1, we keep morphological information in noun categories and in Type 2, we don’t. [sent-68, score-0.168]
</p><p>37 For example, consider a noun chunk ‘raam ne (Ram ERG)’ . [sent-69, score-0.337]
</p><p>38 In Type 1, CCG categories for ‘raam’ and ‘ne’ are NP and  1http://nextens. [sent-70, score-0.088]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ccg', 0.545), ('hindi', 0.282), ('malt', 0.252), ('chunk', 0.223), ('bharati', 0.223), ('raam', 0.219), ('treebank', 0.165), ('dependency', 0.158), ('steedman', 0.138), ('supertagger', 0.134), ('mohan', 0.126), ('ccgbank', 0.126), ('np', 0.115), ('hdt', 0.109), ('khariidii', 0.109), ('kitaab', 0.109), ('hockenmaier', 0.107), ('treebanks', 0.102), ('ram', 0.101), ('category', 0.1), ('ambati', 0.089), ('bhatt', 0.089), ('dependencies', 0.088), ('categories', 0.088), ('bharat', 0.084), ('ne', 0.082), ('erg', 0.08), ('parent', 0.078), ('directionality', 0.076), ('node', 0.067), ('traverse', 0.065), ('etc', 0.064), ('recovering', 0.061), ('distance', 0.06), ('ak', 0.058), ('lie', 0.054), ('formalism', 0.051), ('long', 0.05), ('lexicon', 0.05), ('derivations', 0.05), ('ke', 0.05), ('morphological', 0.048), ('root', 0.048), ('psp', 0.048), ('foofr', 0.048), ('nso', 0.048), ('labels', 0.048), ('arguments', 0.046), ('argument', 0.046), ('tnhcee', 0.045), ('elegantly', 0.045), ('freer', 0.045), ('nivre', 0.044), ('tse', 0.042), ('sketched', 0.042), ('lemma', 0.04), ('gf', 0.04), ('child', 0.038), ('adjunct', 0.038), ('cky', 0.037), ('equivalents', 0.036), ('obj', 0.036), ('sults', 0.036), ('ed', 0.035), ('subj', 0.035), ('subcategorization', 0.035), ('est', 0.034), ('lexicalised', 0.034), ('parsing', 0.033), ('parser', 0.033), ('tdh', 0.033), ('combinatory', 0.033), ('recovery', 0.033), ('noun', 0.032), ('respectively', 0.032), ('bos', 0.031), ('place', 0.031), ('object', 0.031), ('patient', 0.03), ('pose', 0.03), ('inf', 0.03), ('depends', 0.03), ('indian', 0.03), ('ooff', 0.029), ('curran', 0.029), ('chunks', 0.029), ('today', 0.028), ('categorial', 0.028), ('book', 0.027), ('mark', 0.027), ('ws', 0.027), ('readability', 0.026), ('assign', 0.026), ('henceforth', 0.026), ('converting', 0.026), ('tag', 0.026), ('italian', 0.026), ('clauses', 0.025), ('coarse', 0.025), ('released', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="372-tfidf-1" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>Author: Bharat Ram Ambati ; Tejaswini Deoskar ; Mark Steedman</p><p>Abstract: We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery.</p><p>2 0.45695505 <a title="372-tfidf-2" href="./acl-2013-Integrating_Multiple_Dependency_Corpora_for_Inducing_Wide-coverage_Japanese_CCG_Resources.html">199 acl-2013-Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</a></p>
<p>Author: Sumire Uematsu ; Takuya Matsuzaki ; Hiroki Hanaoka ; Yusuke Miyao ; Hideki Mima</p><p>Abstract: This paper describes a method of inducing wide-coverage CCG resources for Japanese. While deep parsers with corpusinduced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexi- con and the accuracy of parsing.</p><p>3 0.31071728 <a title="372-tfidf-3" href="./acl-2013-Transfer_Learning_for_Constituency-Based_Grammars.html">357 acl-2013-Transfer Learning for Constituency-Based Grammars</a></p>
<p>Author: Yuan Zhang ; Regina Barzilay ; Amir Globerson</p><p>Abstract: In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing trans- fer between the formalisms, while preserving parsing efficiency. We evaluate our approach on three constituency-based grammars CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations.1 —</p><p>4 0.26923689 <a title="372-tfidf-4" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>Author: Karl Moritz Hermann ; Phil Blunsom</p><p>Abstract: Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.</p><p>5 0.12544009 <a title="372-tfidf-5" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>Author: Angelina Ivanova ; Stephan Oepen ; Lilja vrelid</p><p>Abstract: In this paper we focus on practical issues of data representation for dependency parsing. We carry out an experimental comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser.</p><p>6 0.11310245 <a title="372-tfidf-6" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>7 0.110602 <a title="372-tfidf-7" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>8 0.10833611 <a title="372-tfidf-8" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>9 0.095692508 <a title="372-tfidf-9" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>10 0.095197141 <a title="372-tfidf-10" href="./acl-2013-Coordination_Structures_in_Dependency_Treebanks.html">94 acl-2013-Coordination Structures in Dependency Treebanks</a></p>
<p>11 0.090373799 <a title="372-tfidf-11" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>12 0.089372925 <a title="372-tfidf-12" href="./acl-2013-Semantic_Parsing_with_Combinatory_Categorial_Grammars.html">313 acl-2013-Semantic Parsing with Combinatory Categorial Grammars</a></p>
<p>13 0.073954724 <a title="372-tfidf-13" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>14 0.068367876 <a title="372-tfidf-14" href="./acl-2013-ParGramBank%3A_The_ParGram_Parallel_Treebank.html">270 acl-2013-ParGramBank: The ParGram Parallel Treebank</a></p>
<p>15 0.067399591 <a title="372-tfidf-15" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>16 0.065080225 <a title="372-tfidf-16" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>17 0.063893445 <a title="372-tfidf-17" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>18 0.063666545 <a title="372-tfidf-18" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>19 0.063382246 <a title="372-tfidf-19" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<p>20 0.062659547 <a title="372-tfidf-20" href="./acl-2013-An_Empirical_Examination_of_Challenges_in_Chinese_Parsing.html">44 acl-2013-An Empirical Examination of Challenges in Chinese Parsing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, -0.078), (2, -0.172), (3, -0.011), (4, -0.234), (5, -0.017), (6, 0.049), (7, -0.003), (8, 0.145), (9, -0.096), (10, 0.041), (11, 0.02), (12, 0.207), (13, 0.035), (14, -0.108), (15, 0.045), (16, 0.066), (17, 0.008), (18, -0.254), (19, 0.096), (20, 0.039), (21, -0.238), (22, -0.246), (23, -0.037), (24, 0.183), (25, 0.058), (26, -0.058), (27, 0.068), (28, -0.006), (29, -0.086), (30, 0.147), (31, -0.073), (32, -0.021), (33, 0.153), (34, -0.025), (35, -0.024), (36, -0.052), (37, 0.057), (38, -0.068), (39, 0.043), (40, 0.019), (41, 0.037), (42, -0.089), (43, 0.014), (44, -0.026), (45, -0.078), (46, 0.023), (47, -0.012), (48, 0.035), (49, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95410544 <a title="372-lsi-1" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>Author: Bharat Ram Ambati ; Tejaswini Deoskar ; Mark Steedman</p><p>Abstract: We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery.</p><p>2 0.94717652 <a title="372-lsi-2" href="./acl-2013-Integrating_Multiple_Dependency_Corpora_for_Inducing_Wide-coverage_Japanese_CCG_Resources.html">199 acl-2013-Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</a></p>
<p>Author: Sumire Uematsu ; Takuya Matsuzaki ; Hiroki Hanaoka ; Yusuke Miyao ; Hideki Mima</p><p>Abstract: This paper describes a method of inducing wide-coverage CCG resources for Japanese. While deep parsers with corpusinduced grammars have been emerging for some languages, those for Japanese have not been widely studied, mainly because most Japanese syntactic resources are dependency-based. Our method first integrates multiple dependency-based corpora into phrase structure trees and then converts the trees into CCG derivations. The method is empirically evaluated in terms of the coverage of the obtained lexi- con and the accuracy of parsing.</p><p>3 0.77440596 <a title="372-lsi-3" href="./acl-2013-Transfer_Learning_for_Constituency-Based_Grammars.html">357 acl-2013-Transfer Learning for Constituency-Based Grammars</a></p>
<p>Author: Yuan Zhang ; Regina Barzilay ; Amir Globerson</p><p>Abstract: In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing trans- fer between the formalisms, while preserving parsing efficiency. We evaluate our approach on three constituency-based grammars CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations.1 —</p><p>4 0.59463 <a title="372-lsi-4" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>Author: Karl Moritz Hermann ; Phil Blunsom</p><p>Abstract: Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is a fundamental task of Natural Language Processing. In this paper we draw upon recent advances in the learning of vector space representations of sentential semantics and the transparent interface between syntax and semantics provided by Combinatory Categorial Grammar to introduce Combinatory Categorial Autoencoders. This model leverages the CCG combinatory operators to guide a non-linear transformation of meaning within a sentence. We use this model to learn high dimensional embeddings for sentences and evaluate them in a range of tasks, demonstrating that the incorporation of syntax allows a concise model to learn representations that are both effective and general.</p><p>5 0.51458079 <a title="372-lsi-5" href="./acl-2013-Semantic_Parsing_with_Combinatory_Categorial_Grammars.html">313 acl-2013-Semantic Parsing with Combinatory Categorial Grammars</a></p>
<p>Author: Yoav Artzi ; Nicholas FitzGerald ; Luke Zettlemoyer</p><p>Abstract: unkown-abstract</p><p>6 0.41830036 <a title="372-lsi-6" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>7 0.39570722 <a title="372-lsi-7" href="./acl-2013-ParGramBank%3A_The_ParGram_Parallel_Treebank.html">270 acl-2013-ParGramBank: The ParGram Parallel Treebank</a></p>
<p>8 0.38790822 <a title="372-lsi-8" href="./acl-2013-Coordination_Structures_in_Dependency_Treebanks.html">94 acl-2013-Coordination Structures in Dependency Treebanks</a></p>
<p>9 0.33850938 <a title="372-lsi-9" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>10 0.3339197 <a title="372-lsi-10" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>11 0.33051443 <a title="372-lsi-11" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>12 0.30276585 <a title="372-lsi-12" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<p>13 0.28172144 <a title="372-lsi-13" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>14 0.27569443 <a title="372-lsi-14" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<p>15 0.26326984 <a title="372-lsi-15" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>16 0.25967619 <a title="372-lsi-16" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>17 0.24701604 <a title="372-lsi-17" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>18 0.24537204 <a title="372-lsi-18" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>19 0.23913474 <a title="372-lsi-19" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>20 0.23749755 <a title="372-lsi-20" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.01), (11, 0.076), (14, 0.02), (24, 0.02), (26, 0.026), (35, 0.052), (42, 0.617), (48, 0.017), (70, 0.013), (95, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97612178 <a title="372-lda-1" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>Author: Sina Zarriess ; Jonas Kuhn</p><p>Abstract: We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</p><p>same-paper 2 0.97601533 <a title="372-lda-2" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>Author: Bharat Ram Ambati ; Tejaswini Deoskar ; Mark Steedman</p><p>Abstract: We show that informative lexical categories from a strongly lexicalised formalism such as Combinatory Categorial Grammar (CCG) can improve dependency parsing of Hindi, a free word order language. We first describe a novel way to obtain a CCG lexicon and treebank from an existing dependency treebank, using a CCG parser. We use the output of a supertagger trained on the CCGbank as a feature for a state-of-the-art Hindi dependency parser (Malt). Our results show that using CCG categories improves the accuracy of Malt on long distance dependencies, for which it is known to have weak rates of recovery.</p><p>3 0.96461612 <a title="372-lda-3" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>Author: Isao Goto ; Masao Utiyama ; Eiichiro Sumita ; Akihiro Tamura ; Sadao Kurohashi</p><p>Abstract: This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models.</p><p>4 0.91868615 <a title="372-lda-4" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>Author: Abhijit Mishra ; Pushpak Bhattacharyya ; Michael Carl</p><p>Abstract: In this paper we introduce Translation Difficulty Index (TDI), a measure of difficulty in text translation. We first define and quantify translation difficulty in terms of TDI. We realize that any measure of TDI based on direct input by translators is fraught with subjectivity and adhocism. We, rather, rely on cognitive evidences from eye tracking. TDI is measured as the sum of fixation (gaze) and saccade (rapid eye movement) times of the eye. We then establish that TDI is correlated with three properties of the input sentence, viz. length (L), degree of polysemy (DP) and structural complexity (SC). We train a Support Vector Regression (SVR) system to predict TDIs for new sentences using these features as input. The prediction done by our framework is well correlated with the empirical gold standard data, which is a repository of < L, DP, SC > and TDI pairs for a set of sentences. The primary use of our work is a way of “binning” sentences (to be translated) in “easy”, “medium” and “hard” categories as per their predicted TDI. This can decide pricing of any translation task, especially useful in a scenario where parallel corpora for Machine Translation are built through translation crowdsourcing/outsourcing. This can also provide a way of monitoring progress of second language learners.</p><p>5 0.90923113 <a title="372-lda-5" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>Author: Rico Sennrich ; Holger Schwenk ; Walid Aransa</p><p>Abstract: While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also de- scribe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1BLEU over unadapted systems and single-domain adaptation.</p><p>6 0.89293015 <a title="372-lda-6" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<p>7 0.88583517 <a title="372-lda-7" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>8 0.87497175 <a title="372-lda-8" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>9 0.67712295 <a title="372-lda-9" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>10 0.67076117 <a title="372-lda-10" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>11 0.66437203 <a title="372-lda-11" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>12 0.65578765 <a title="372-lda-12" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>13 0.64431995 <a title="372-lda-13" href="./acl-2013-Integrating_Multiple_Dependency_Corpora_for_Inducing_Wide-coverage_Japanese_CCG_Resources.html">199 acl-2013-Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</a></p>
<p>14 0.63117385 <a title="372-lda-14" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>15 0.61457515 <a title="372-lda-15" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>16 0.59891194 <a title="372-lda-16" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.59092295 <a title="372-lda-17" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>18 0.58638853 <a title="372-lda-18" href="./acl-2013-Two-Neighbor_Orientation_Model_with_Cross-Boundary_Global_Contexts.html">363 acl-2013-Two-Neighbor Orientation Model with Cross-Boundary Global Contexts</a></p>
<p>19 0.58221054 <a title="372-lda-19" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>20 0.57862282 <a title="372-lda-20" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
