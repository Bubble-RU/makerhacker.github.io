<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-93" href="#">acl2013-93</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</h1>
<br/><p>Source: <a title="acl-2013-93-pdf" href="http://aclweb.org/anthology//P/P13/P13-2133.pdf">pdf</a></p><p>Author: Dhouha Bouamor ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches.</p><p>Reference: <a title="acl-2013-93-reference" href="../acl2013_reference/acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. [sent-5, score-0.724]
</p><p>2 We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. [sent-6, score-0.7]
</p><p>3 On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches. [sent-7, score-0.298]
</p><p>4 1 Introduction Over the years, bilingual lexicon extraction from  comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). [sent-8, score-0.83]
</p><p>5 The so-called standardapproach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words. [sent-10, score-1.051]
</p><p>6 Each element in the context vector of a source or target word represents its association with a word which occurs within a window of N words. [sent-11, score-0.255]
</p><p>7 To enable the comparison of source and target vectors, words in the source vectors are translated into the target language using an existing bilingual dictionary. [sent-12, score-0.516]
</p><p>8 The core of the standard approach is the bilingual dictionary. [sent-13, score-0.305]
</p><p>9 fr  word action can be translated into English as share, stock, lawsuit or deed. [sent-17, score-0.118]
</p><p>10 In such cases, it is difficult to identify in flat resources like bilingual dictionaries which translations are most relevant. [sent-18, score-0.516]
</p><p>11 The standard approach considers all available translations and gives them the same importance in the resulting translated context vectors independently of the domain of interest and word ambiguity. [sent-19, score-0.443]
</p><p>12 Thus, in the financial domain, translating action into deed or lawsuit would introduce noise in context vectors. [sent-20, score-0.201]
</p><p>13 In this paper, we present a novel approach that addresses the word polysemy problem neglected in the standard approach. [sent-21, score-0.091]
</p><p>14 We introduce a Word Sense Disambiguation (WSD) process that identifies the translations of polysemous words that are more likely to give the best representation of context vectors in the target language. [sent-22, score-0.621]
</p><p>15 For this purpose, we employ five WordNet-based semantic similarity and relatedness measures and use a data fusion method that merges the results obtained by each measure. [sent-23, score-0.455]
</p><p>16 We test our approach on two specialized French-English comparable corpora (financial and medical) and report improved results  compared to two state-of-the-art approaches. [sent-24, score-0.404]
</p><p>17 2  Related Work  Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach. [sent-25, score-0.83]
</p><p>18 In order to improve the results of this approach, recent researches based on the assumption that more the context vectors are representative, better is the bilingual lexicon extraction were conducted. [sent-26, score-0.73]
</p><p>19 In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al. [sent-27, score-0.121]
</p><p>20 , 2009) were combined with the bilingual dic759  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-28, score-0.305]
</p><p>21 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 759–764, tionary to translate context vectors. [sent-30, score-0.097]
</p><p>22 Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. [sent-31, score-0.417]
</p><p>23 (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure  criteria but no improvements have been demonstrated. [sent-32, score-0.425]
</p><p>24 Recently, (Morin and Prochasson, 2011) proceed as the standard approach but weigh the different translations according to their frequency in the target corpus. [sent-38, score-0.249]
</p><p>25 (2004) in this way: If they focus on words ambiguities on source and target languages, we thought that it would be sufficient to disambiguate only translated source context vectors. [sent-40, score-0.216]
</p><p>26 1 Semantic similarity measures A large number of WSD techniques were proposed in the literature. [sent-42, score-0.236]
</p><p>27 In this work, we use it to derive a semantic similarity between lexical units within the same context vector. [sent-48, score-0.271]
</p><p>28 To the best of our knowledge, this is the first application of WordNet to bilingual lexicon extraction from comparable corpora. [sent-49, score-0.724]
</p><p>29 In this work, we use five similarity measures and compare their performances. [sent-51, score-0.272]
</p><p>30 These measures include three 1For consiseness, we often use “semantic similarity” to refer collectively to both similarity and relatedness. [sent-52, score-0.236]
</p><p>31 path-based semantic similarity measures denoted PATH,WUP (Wu and Palmer, 1994) and LEACOCK (Leacock and Chodorow, 1998). [sent-53, score-0.285]
</p><p>32 PATH is a baseline that is equal to the inverse of the shortest path between two words. [sent-54, score-0.096]
</p><p>33 WUP finds the depth of the least common subsumer of the words, and scales that by the sum of the depths of individual words. [sent-55, score-0.108]
</p><p>34 LEACOCK finds the shortest path between two words, and scales that by the maximum path length found in the is–a hierarchy in which they occur. [sent-57, score-0.237]
</p><p>35 Path length measures have the advantage of being independent of corpus statistics, and therefor uninfluenced by sparse data. [sent-58, score-0.111]
</p><p>36 Since semantic relatedness is considered to be more general than semantic similarity, we also use two relatedness measures: LESK (Banerjee and Pedersen, 2002) and VECTOR (Patwardhan, 2003). [sent-59, score-0.196]
</p><p>37 VECTOR creates a co-occurrence matrix for each gloss token. [sent-61, score-0.103]
</p><p>38 Each gloss is then represented as a vector that averages token co-occurrences. [sent-62, score-0.093]
</p><p>39 2  Disambiguation process  Once translated into the target language, the context vectors disambiguation process intervenes. [sent-64, score-0.355]
</p><p>40 This process operates locally on each context vector and aims at finding the most prominent translations of polysemous words. [sent-65, score-0.567]
</p><p>41 For this purpose, we use monosemic words as a seed set of disambiguated words to infer the polysemous word’s translations senses. [sent-66, score-0.687]
</p><p>42 We hypothesize that a word is monosemic if it is associated to only one entry in the bilingual dictionary. [sent-67, score-0.547]
</p><p>43 We checked this assumption by probing monosemic entries of the bilingual dictionary against WordNet and found that 95% of the entries are monosemic in both resources. [sent-68, score-0.952]
</p><p>44 According to the above-described semantic similarity measures, a similarity value SimV alue is derived between all the translations provided for each polysemous word by the bilingual dictionary and all monosemic words appearing within the same context vector. [sent-69, score-1.524]
</p><p>45 Hence, according to average similarity values  wpj  Ave Sim(wpj),  we obtain for each polysemous word wp an ordered list of translations wp1 . [sent-71, score-0.766]
</p><p>46 1 Resources and Experimental Setup We conducted our experiments on two FrenchEnglish comparable corpora specialized on the corporate finance and the breast cancer subdomains. [sent-76, score-0.864]
</p><p>47 We consider the domain topic in the source language (for instance cancer du sein [breast cancer]) as a query to Wikipedia and extract all its sub-topics (i. [sent-78, score-0.14]
</p><p>48 Then we collected all articles belonging to one of these categories and used interlanguage links to build the comparable corpus. [sent-81, score-0.208]
</p><p>49 Both corpora have been normalized through the following linguistic preprocessing steps: tokenisation, part-of-speech tagging, lemmatisation and function words removal. [sent-82, score-0.106]
</p><p>50 The polysemy rate indicates how much  words in the comparable corpora are associated to more than one translation in the seed bilingual dictionary. [sent-84, score-0.724]
</p><p>51 The dictionary consists of an in-house bilingual dictionary which contains about 120,000 entries belonging to the general language with an average of 7 translations per entry. [sent-85, score-0.744]
</p><p>52 In bilingual terminology extraction from comparable corpora, a reference list is required to evaluate the performance of the alignment. [sent-86, score-0.608]
</p><p>53 org/ 4Comparable corpora will be shared publicly  CoBr peoCarosat rpe cua fisn caenrce43F09r26en. [sent-89, score-0.106]
</p><p>54 8 li40sh054 P17%%R Table 1: Comparable corpora sizes in term of words and polysemy rates (PR) associated to each corpus terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). [sent-91, score-0.197]
</p><p>55 Here, we created two ref-  ×  erence lists5 for the corporate finance and the breast cancer sub-domains. [sent-92, score-0.46]
</p><p>56 Note that reference terms pairs appear more than five times in each part of both comparable corpora. [sent-95, score-0.213]
</p><p>57 Three other parameters need to be set up, namely the window size, the association measure and the similarity measure. [sent-96, score-0.174]
</p><p>58 They carried out a complete study of the influence of these parameters on the bilingual alignment. [sent-98, score-0.305]
</p><p>59 The context vectors were defined by computing the Discounted Log-Odds Ratio (equation 3) between words occurring in the same context window of size 7. [sent-99, score-0.329]
</p><p>60 Odds-Ratiodisc= log((OO1112++2121))((OO2221++2121))  (3)  where Oij are the cells of the 2 2 contingency matrix of a troek ethne s co-occurring ×w 2ith c tohnet tnegremn Sy  within a given window size. [sent-100, score-0.096]
</p><p>61 As similarity measure, we chose to use the cosine measure. [sent-101, score-0.125]
</p><p>62 2 Results of bilingual lexicon extraction To evaluate the performance of our approach, we used both the standard approach (SA) and the approach proposed by (Morin and Prochasson, 2011) (henceforth MP1 1) as baselines. [sent-103, score-0.547]
</p><p>63 The experiments were performed with respect to the five semantic similarity measures described in section 3. [sent-104, score-0.321]
</p><p>64 Each measure provides, for each polysemous word, a ranked list of translations. [sent-106, score-0.223]
</p><p>65 A question that arises here is whether we should introduce only the topranked translation into the context vector or consider a larger number of translations, mainly when a translation list contains synonyms. [sent-107, score-0.168]
</p><p>66 In each  column, italics shows best single similarity measure, bold shows best result. [sent-114, score-0.125]
</p><p>67 This choice is motivated by the fact that words in both corpora have on average 7 translations in the bilingual dictionary. [sent-117, score-0.622]
</p><p>68 Both baseline systems use all translations associated to each entry in the bilingual dictionary. [sent-118, score-0.516]
</p><p>69 The only difference is that in MP1 1 translations are weighted according to their frequency in the target corpus. [sent-119, score-0.249]
</p><p>70 The results of different works focusing on bilingual lexicon extraction from comparable corpora are evaluated on the number of correct candidates found in the first N first candidates output by the alignment process (the TopN). [sent-120, score-0.936]
</p><p>71 The results obtained for the corporate finance corpus are presented in Table 2a. [sent-122, score-0.187]
</p><p>72 The first notable observation is that disambiguating context vectors using semantic similarity measures outperforms the SA. [sent-123, score-0.468]
</p><p>73 Using the top two words (WN-T2) in context vectors increases the F-measure from 0. [sent-125, score-0.183]
</p><p>74 Concerning the breast cancer corpus, Table 2b shows improvements in most cases over both the SA and MP1 1. [sent-129, score-0.273]
</p><p>75 The maximum Fmeasure was obtained by LESK when for each polysemous word up to four translations (WN-T4) are considered in context vectors. [sent-130, score-0.497]
</p><p>76 Each of the tested 5 semantic similarity measures provides a different view of how to rank the translations of a given test word. [sent-134, score-0.496]
</p><p>77 For this, we used a voting method, and chose one in the Condorcet family the Condorcet data fusion method. [sent-137, score-0.134]
</p><p>78 If di is preferred to dj , then we add 1to the element at row iand column j (aij). [sent-147, score-0.141]
</p><p>79 For every element aij , if aij > m/2 , then di beats dj ; if aij < m/2, then dj beats di; otherwise (aij = m/2), there is a draw between di and  dj. [sent-149, score-0.712]
</p><p>80 Here, we view the ranking of the extraction results from different similarity measures as a special instance of the voting problem where the Top20 extraction results correspond to candidates and different semantic similarity measures are the voters. [sent-152, score-0.807]
</p><p>81 Even though the two corpora are fairly different (subject and polysemy rate), the optimal results are obtained when considering up to two most similar translations in context vectors. [sent-154, score-0.505]
</p><p>82 5  Conclusion  We presented in this paper a novel method that extends the standard approach used for bilingual lexicon extraction. [sent-161, score-0.455]
</p><p>83 This method disambiguates polysemous words in context vectors by selecting only the most relevant translations. [sent-162, score-0.372]
</p><p>84 Five semantic similarity and relatedness measures were used for this purpose. [sent-163, score-0.334]
</p><p>85 Experiments conducted on two specialized comparable corpora indicate that the combination of similarity metrics leads to a better performance than two state-of-the-art approaches. [sent-164, score-0.529]
</p><p>86 This shows that the ambiguity present in specialized comparable corpora hampers bilingual lexicon extraction, and that methods such as the one introduced here are needed. [sent-165, score-0.894]
</p><p>87 First, we plan to mine much larger specialized comparable corpora and focus  on their quality (Li and Gaussier, 2010). [sent-167, score-0.404]
</p><p>88 We also plan to test our method on bilingual lexicon extraction from general-domain corpora, where ambiguity is generally higher and disambiguation methods should be all the more needed. [sent-168, score-0.667]
</p><p>89 An adapted lesk algorithm for word sense disambiguation using wordnet. [sent-171, score-0.21]
</p><p>90 Looking for candidate translational equivalents in specialized, comparable corpora. [sent-176, score-0.177]
</p><p>91 The effect of a general lexicon in corpus-based identification of french-english medical word translations. [sent-181, score-0.197]
</p><p>92 A method for enhancing image retrieval based on annotation using modified wup similarity in wordnet. [sent-190, score-0.269]
</p><p>93 A statistical view on bilingual lexicon extraction: From parallel corpora to nonparallel corpora. [sent-195, score-0.561]
</p><p>94 A geometric view on bilingual lexicon extraction from comparable corpora. [sent-200, score-0.724]
</p><p>95 Adaptive dictionary for bilingual lexicon extraction from comparable corpora. [sent-210, score-0.801]
</p><p>96 Revisiting context-based projection methods for termtranslation spotting in comparable corpora. [sent-218, score-0.177]
</p><p>97 Combining local context and WordNet similarityfor word sense identification, pages 305–332. [sent-221, score-0.129]
</p><p>98 Improving corpus comparability for bilingual lexicon extraction from comparable corpora. [sent-227, score-0.724]
</p><p>99 Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora. [sent-237, score-0.525]
</p><p>100 Anchor points for bilingual lexicon extraction from small comparable corpora. [sent-252, score-0.724]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bilingual', 0.305), ('monosemic', 0.242), ('translations', 0.211), ('morin', 0.198), ('polysemous', 0.189), ('comparable', 0.177), ('chiao', 0.173), ('prochasson', 0.173), ('lexicon', 0.15), ('cancer', 0.14), ('wpj', 0.139), ('breast', 0.133), ('aij', 0.126), ('similarity', 0.125), ('zweigenbaum', 0.122), ('specialized', 0.121), ('emmanuel', 0.117), ('measures', 0.111), ('cea', 0.106), ('corpora', 0.106), ('alue', 0.104), ('condorcet', 0.104), ('hazem', 0.104), ('pankoo', 0.104), ('simv', 0.104), ('finance', 0.104), ('gaussier', 0.101), ('context', 0.097), ('lesk', 0.093), ('frenchenglish', 0.092), ('extraction', 0.092), ('polysemy', 0.091), ('leacock', 0.087), ('vectors', 0.086), ('fusion', 0.085), ('cedex', 0.085), ('disambiguation', 0.085), ('synsets', 0.083), ('corporate', 0.083), ('dictionary', 0.077), ('hwang', 0.073), ('dhouha', 0.069), ('lawsuit', 0.069), ('myunggwon', 0.069), ('nuray', 0.069), ('semsim', 0.069), ('wp', 0.068), ('sa', 0.068), ('wordnet', 0.066), ('choi', 0.065), ('path', 0.065), ('dj', 0.062), ('laroche', 0.061), ('semmar', 0.061), ('wseas', 0.057), ('montague', 0.057), ('gloss', 0.056), ('wup', 0.053), ('candidates', 0.053), ('image', 0.052), ('france', 0.051), ('voting', 0.049), ('pierre', 0.049), ('relatedness', 0.049), ('translated', 0.049), ('semantic', 0.049), ('window', 0.049), ('ave', 0.048), ('medical', 0.047), ('matrix', 0.047), ('ric', 0.047), ('di', 0.045), ('seed', 0.045), ('cho', 0.044), ('entries', 0.043), ('beats', 0.043), ('chodorow', 0.041), ('retrieval', 0.039), ('finds', 0.038), ('scales', 0.038), ('target', 0.038), ('vector', 0.037), ('banerjee', 0.036), ('five', 0.036), ('wsd', 0.036), ('vision', 0.036), ('synonymous', 0.035), ('financial', 0.035), ('ambiguity', 0.035), ('kim', 0.035), ('list', 0.034), ('element', 0.034), ('prominent', 0.033), ('depth', 0.032), ('revealed', 0.032), ('ambiguities', 0.032), ('sense', 0.032), ('engineering', 0.032), ('belonging', 0.031), ('shortest', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999875 <a title="93-tfidf-1" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>Author: Dhouha Bouamor ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches.</p><p>2 0.21984421 <a title="93-tfidf-2" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>3 0.17716312 <a title="93-tfidf-3" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>Author: Ahmet Aker ; Monica Paramita ; Rob Gaizauskas</p><p>Abstract: In this paper we present a method for extracting bilingual terminologies from comparable corpora. In our approach we treat bilingual term extraction as a classification problem. For classification we use an SVM binary classifier and training data taken from the EUROVOC thesaurus. We test our approach on a held-out test set from EUROVOC and perform precision, recall and f-measure evaluations for 20 European language pairs. The performance of our classifier reaches the 100% precision level for many language pairs. We also perform manual evaluation on bilingual terms extracted from English-German term-tagged comparable corpora. The results of this manual evaluation showed 60-83% of the term pairs generated are exact translations and over 90% exact or partial translations.</p><p>4 0.17411593 <a title="93-tfidf-4" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>Author: Jiajun Zhang ; Chengqing Zong</p><p>Abstract: Currently, almost all of the statistical machine translation (SMT) models are trained with the parallel corpora in some specific domains. However, when it comes to a language pair or a different domain without any bilingual resources, the traditional SMT loses its power. Recently, some research works study the unsupervised SMT for inducing a simple word-based translation model from the monolingual corpora. It successfully bypasses the constraint of bitext for SMT and obtains a relatively promising result. In this paper, we take a step forward and propose a simple but effective method to induce a phrase-based model from the monolingual corpora given an automatically-induced translation lexicon or a manually-edited translation dictionary. We apply our method for the domain adaptation task and the extensive experiments show that our proposed method can substantially improve the translation quality. 1</p><p>5 0.14411363 <a title="93-tfidf-5" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>6 0.14271282 <a title="93-tfidf-6" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>7 0.13381119 <a title="93-tfidf-7" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>8 0.13048297 <a title="93-tfidf-8" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>9 0.11999274 <a title="93-tfidf-9" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>10 0.1180684 <a title="93-tfidf-10" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>11 0.11091966 <a title="93-tfidf-11" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>12 0.10775699 <a title="93-tfidf-12" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>13 0.10097538 <a title="93-tfidf-13" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>14 0.098674968 <a title="93-tfidf-14" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>15 0.094636723 <a title="93-tfidf-15" href="./acl-2013-DKPro_Similarity%3A_An_Open_Source_Framework_for_Text_Similarity.html">104 acl-2013-DKPro Similarity: An Open Source Framework for Text Similarity</a></p>
<p>16 0.08849749 <a title="93-tfidf-16" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>17 0.086515799 <a title="93-tfidf-17" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>18 0.086449422 <a title="93-tfidf-18" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>19 0.085957155 <a title="93-tfidf-19" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>20 0.085256256 <a title="93-tfidf-20" href="./acl-2013-SEMILAR%3A_The_Semantic_Similarity_Toolkit.html">304 acl-2013-SEMILAR: The Semantic Similarity Toolkit</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.228), (1, 0.009), (2, 0.154), (3, -0.078), (4, 0.028), (5, -0.112), (6, -0.141), (7, 0.044), (8, 0.015), (9, -0.071), (10, 0.039), (11, -0.003), (12, 0.019), (13, 0.028), (14, 0.153), (15, -0.01), (16, -0.018), (17, -0.029), (18, -0.114), (19, 0.023), (20, 0.001), (21, -0.065), (22, -0.011), (23, 0.044), (24, -0.082), (25, 0.01), (26, -0.055), (27, 0.09), (28, 0.037), (29, 0.041), (30, -0.066), (31, -0.079), (32, 0.048), (33, 0.03), (34, 0.007), (35, 0.045), (36, -0.013), (37, 0.052), (38, 0.013), (39, -0.086), (40, 0.047), (41, -0.014), (42, 0.028), (43, -0.059), (44, -0.069), (45, -0.01), (46, 0.053), (47, 0.007), (48, 0.016), (49, -0.092)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95748013 <a title="93-lsi-1" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>Author: Dhouha Bouamor ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches.</p><p>2 0.76859969 <a title="93-lsi-2" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>Author: Lian Tze Lim ; Lay-Ki Soon ; Tek Yong Lim ; Enya Kong Tang ; Bali Ranaivo-Malancon</p><p>Abstract: Current approaches for word sense disambiguation and translation selection typically require lexical resources or large bilingual corpora with rich information fields and annotations, which are often infeasible for under-resourced languages. We extract translation context knowledge from a bilingual comparable corpora of a richer-resourced language pair, and inject it into a multilingual lexicon. The multilin- gual lexicon can then be used to perform context-dependent lexical lookup on texts of any language, including under-resourced ones. Evaluations on a prototype lookup tool, trained on a English–Malay bilingual Wikipedia corpus, show a precision score of 0.65 (baseline 0.55) and mean reciprocal rank score of 0.81 (baseline 0.771). Based on the early encouraging results, the context-dependent lexical lookup tool may be developed further into an intelligent reading aid, to help users grasp the gist of a second or foreign language text.</p><p>3 0.76632518 <a title="93-lsi-3" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>Author: Ahmet Aker ; Monica Paramita ; Rob Gaizauskas</p><p>Abstract: In this paper we present a method for extracting bilingual terminologies from comparable corpora. In our approach we treat bilingual term extraction as a classification problem. For classification we use an SVM binary classifier and training data taken from the EUROVOC thesaurus. We test our approach on a held-out test set from EUROVOC and perform precision, recall and f-measure evaluations for 20 European language pairs. The performance of our classifier reaches the 100% precision level for many language pairs. We also perform manual evaluation on bilingual terms extracted from English-German term-tagged comparable corpora. The results of this manual evaluation showed 60-83% of the term pairs generated are exact translations and over 90% exact or partial translations.</p><p>4 0.68756586 <a title="93-lsi-4" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>5 0.66789538 <a title="93-lsi-5" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>Author: Taesung Lee ; Seung-won Hwang</p><p>Abstract: This paper studies the problem of mining named entity translations from comparable corpora with some “asymmetry”. Unlike the previous approaches relying on the “symmetry” found in parallel corpora, the proposed method is tolerant to asymmetry often found in comparable corpora, by distinguishing different semantics of relations of entity pairs to selectively propagate seed entity translations on weakly comparable corpora. Our experimental results on English-Chinese corpora show that our selective propagation approach outperforms the previous approaches in named entity translation in terms of the mean reciprocal rank by up to 0.16 for organization names, and 0.14 in a low com- parability case.</p><p>6 0.63192552 <a title="93-lsi-6" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>7 0.61002755 <a title="93-lsi-7" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>8 0.60099298 <a title="93-lsi-8" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>9 0.60073012 <a title="93-lsi-9" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>10 0.58775109 <a title="93-lsi-10" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>11 0.58627462 <a title="93-lsi-11" href="./acl-2013-Creating_Similarity%3A_Lateral_Thinking_for_Vertical_Similarity_Judgments.html">96 acl-2013-Creating Similarity: Lateral Thinking for Vertical Similarity Judgments</a></p>
<p>12 0.58540952 <a title="93-lsi-12" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>13 0.58302146 <a title="93-lsi-13" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>14 0.57219297 <a title="93-lsi-14" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>15 0.56949884 <a title="93-lsi-15" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>16 0.56364924 <a title="93-lsi-16" href="./acl-2013-Translating_Italian_connectives_into_Italian_Sign_Language.html">360 acl-2013-Translating Italian connectives into Italian Sign Language</a></p>
<p>17 0.56270468 <a title="93-lsi-17" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<p>18 0.56218201 <a title="93-lsi-18" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>19 0.561077 <a title="93-lsi-19" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>20 0.54838449 <a title="93-lsi-20" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.062), (6, 0.035), (11, 0.077), (12, 0.236), (14, 0.01), (24, 0.081), (26, 0.035), (35, 0.097), (42, 0.034), (48, 0.057), (70, 0.038), (88, 0.052), (90, 0.038), (95, 0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84411657 <a title="93-lda-1" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>Author: Lian Tze Lim ; Lay-Ki Soon ; Tek Yong Lim ; Enya Kong Tang ; Bali Ranaivo-Malancon</p><p>Abstract: Current approaches for word sense disambiguation and translation selection typically require lexical resources or large bilingual corpora with rich information fields and annotations, which are often infeasible for under-resourced languages. We extract translation context knowledge from a bilingual comparable corpora of a richer-resourced language pair, and inject it into a multilingual lexicon. The multilin- gual lexicon can then be used to perform context-dependent lexical lookup on texts of any language, including under-resourced ones. Evaluations on a prototype lookup tool, trained on a English–Malay bilingual Wikipedia corpus, show a precision score of 0.65 (baseline 0.55) and mean reciprocal rank score of 0.81 (baseline 0.771). Based on the early encouraging results, the context-dependent lexical lookup tool may be developed further into an intelligent reading aid, to help users grasp the gist of a second or foreign language text.</p><p>same-paper 2 0.80646932 <a title="93-lda-2" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>Author: Dhouha Bouamor ; Nasredine Semmar ; Pierre Zweigenbaum</p><p>Abstract: This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches.</p><p>3 0.78102684 <a title="93-lda-3" href="./acl-2013-Accurate_Word_Segmentation_using_Transliteration_and_Language_Model_Projection.html">34 acl-2013-Accurate Word Segmentation using Transliteration and Language Model Projection</a></p>
<p>Author: Masato Hagiwara ; Satoshi Sekine</p><p>Abstract: Transliterated compound nouns not separated by whitespaces pose difficulty on word segmentation (WS) . Offline approaches have been proposed to split them using word statistics, but they rely on static lexicon, limiting their use. We propose an online approach, integrating source LM, and/or, back-transliteration and English LM. The experiments on Japanese and Chinese WS have shown that the proposed models achieve significant improvement over state-of-the-art, reducing 16% errors in Japanese.</p><p>4 0.6486516 <a title="93-lda-4" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>Author: Olivier Ferret</p><p>Abstract: Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.</p><p>5 0.63645661 <a title="93-lda-5" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>6 0.63640517 <a title="93-lda-6" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>7 0.63531005 <a title="93-lda-7" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>8 0.63430399 <a title="93-lda-8" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>9 0.6327154 <a title="93-lda-9" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>10 0.63101381 <a title="93-lda-10" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>11 0.62992287 <a title="93-lda-11" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>12 0.62934679 <a title="93-lda-12" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>13 0.62909615 <a title="93-lda-13" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>14 0.62884432 <a title="93-lda-14" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>15 0.62865388 <a title="93-lda-15" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>16 0.6261279 <a title="93-lda-16" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>17 0.62586993 <a title="93-lda-17" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>18 0.62568229 <a title="93-lda-18" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>19 0.62529492 <a title="93-lda-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.62393916 <a title="93-lda-20" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
