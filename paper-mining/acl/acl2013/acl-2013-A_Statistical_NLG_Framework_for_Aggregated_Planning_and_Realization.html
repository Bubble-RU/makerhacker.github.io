<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-21" href="#">acl2013-21</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</h1>
<br/><p>Source: <a title="acl-2013-21-pdf" href="http://aclweb.org/anthology//P/P13/P13-1138.pdf">pdf</a></p><p>Author: Ravi Kondadadi ; Blake Howald ; Frank Schilder</p><p>Abstract: We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated weather and biography texts fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. – – *∗Ravi Kondadadi is now affiliated with Nuance Communications, Inc.</p><p>Reference: <a title="acl-2013-21-reference" href="../acl2013_reference/acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. [sent-3, score-0.352]
</p><p>2 Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. [sent-4, score-0.247]
</p><p>3 We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. [sent-10, score-0.369]
</p><p>4 The metrics for generated weather and biography texts fall within acceptable ranges. [sent-11, score-0.63]
</p><p>5 However, document planning is arguably one of the most crucial components of an NLG system and is responsible for making the texts express the desired communicative goal in a coherent structure. [sent-18, score-0.286]
</p><p>6 If the document planning stage fails, the communicative goal of the generated text will not be met even if the other two stages are perfect. [sent-19, score-0.29]
</p><p>7 A template structure contains “gaps” that are filled to generate the output. [sent-34, score-0.162]
</p><p>8 The idea is to create a lot of templates from the historical data and select the right template based on some constraints. [sent-35, score-0.344]
</p><p>9 Experiments with different variants of our system (for biography and weather subject matter domains) demonstrate that our system generates reasonable texts. [sent-37, score-0.568]
</p><p>10 ), we present a unique text evaluation metric called syntactic variability to measure the linguistic vari-  ation of generated texts. [sent-41, score-0.179]
</p><p>11 This metric applies to the document collection level and is based on computing the number of unique template sequences among all the generated texts. [sent-42, score-0.333]
</p><p>12 We argue that this metric is useful for evaluating template-based systems and for any type of text generation for domains where linguistic variability is favored (e. [sent-44, score-0.261]
</p><p>13 The main contributions of this paper are (1) A statistical NLG system that combines document and sentence planning and surface realization into one single process; and (2) A new metric syntactic variability is proposed to measure the syntactic and morphological variability of the generated texts. [sent-47, score-0.602]
</p><p>14 We believe this is the first work to propose an automatic metric to measure linguistic variability of generated texts in NLG. [sent-48, score-0.234]
</p><p>15 2  Background  Typically, knowledge-based NLG systems are implemented by rules and, as mentioned above, have a pipelined architecture for the document and sentence planning stages and surface realization (Hovy, 1993; Moore and Paris, 1993). [sent-53, score-0.32]
</p><p>16 However, document planning is arguably the most important task (Sripada et al. [sent-54, score-0.201]
</p><p>17 It follows that approaches to document planning are rule-based as well and, concomitantly, are usually domain specific. [sent-56, score-0.245]
</p><p>18 (201 1) proposed document planning based on an ontology knowledge base to generate football summaries. [sent-58, score-0.229]
</p><p>19 For example, Duboue and McKeown (2003) proposed a statistical approach to extract content selection rules for biography descriptions. [sent-66, score-0.303]
</p><p>20 However, while the system consolidated both sentence planning and surface realization with this approach (described  in more detail in Section 3), the document plan was given via the input data and sequencing information was present in training documents. [sent-73, score-0.352]
</p><p>21 For the present research, we introduce a similar method that leverages the distributions of document–level features in the training corpus to incorporate a statistical document planning component. [sent-74, score-0.201]
</p><p>22 3  Methodology  In order to generate text for a given domain our system runs input data through a statistical ranking model to select a sequence of templates that best fit the input data (E). [sent-76, score-0.264]
</p><p>23 In order to build the ranking model, our system takes historical data (corpus) for the domain through four components: (A) preprocessing; (B) “conceptual unit” creation; (C) collecting statistics; and (D) ranking model build-  ing (summarized in Figure 1). [sent-77, score-0.212]
</p><p>24 Preprocessing involves uncovering the underlying semantic structure of the corpus and using this as a foundation for template creation (Lu et al. [sent-83, score-0.162]
</p><p>25 We developed the named-entity tagger for the weather domain ourselves. [sent-89, score-0.279]
</p><p>26 To tag entities in the biography domain, we used OpenCalais (www. [sent-90, score-0.309]
</p><p>27 For example, in the biography in (1), the conceptual meaning (semantic predicates and domain-specific entities) of sentences (a-b) are represented in (c-d). [sent-93, score-0.368]
</p><p>28 The outputs ofthe preprocessing stage are the template bank and predicate information for each template in the corpus. [sent-107, score-0.381]
</p><p>29 This is a semiautomatic process where we use the predicate information for each template to compute similarity between templates. [sent-110, score-0.162]
</p><p>30 We associate each template in the cor-  pus to a corresponding CuId. [sent-114, score-0.162]
</p><p>31 For example, in (2), using the templates in (1e-f), the identified named entities are assigned to a clustered CuId (2a-b). [sent-115, score-0.168]
</p><p>32 s –  –  At this stage, we will have a set of conceptual units with corresponding template collections (see Howald et al. [sent-122, score-0.23]
</p><p>33 However, to contrast our work from Duboue and McKeown, which focused on content selection, we are focused on learning templates from the semantic representations for the complete generation system (covering content selection, aggregation, sentence and document planning). [sent-128, score-0.374]
</p><p>34 4  Building a ranking model  The core component  of our system is a statistical  model that ranks a set of templates for a given position (sentence 1, sentence 2, . [sent-135, score-0.234]
</p><p>35 To generate the training data, we first filter the templates that have named entity tags not specified in the input data. [sent-141, score-0.2]
</p><p>36 We then rank templates according to the Levenshtein edit distance (Levenshtein, 1966) from the template corresponding to the current sentence in the training document (using the top 10 ranked templates in training for ease of processing effort). [sent-143, score-0.459]
</p><p>37 Similarity between the most likely template in CuId aSinmd cluarrirteyn bte ettewmepelna tthee: Emdoits tdi lsiktaelnyce t e bmetpwleaetne tnhe C cuuIrdrent template and the most likely template for the current CuId. [sent-156, score-0.486]
</p><p>38 Similarity between the most likely template in CuId gSiimvenila proitsyit bieontw aeendn tchuerr menostt t leikmeplyla tetem: pEladitet dinis CtauncIed between the current template and the most likely template for the current CuId at the current position. [sent-157, score-0.486]
</p><p>39 5  Generation  At generation time, our system has a set of input data, a semantically  organized template bank  (collection of templates organized by CuId) and a model from training on the documents for a given domain. [sent-160, score-0.445]
</p><p>40 We first filter out those templates  that  contain a named entity tag not present in the input data. [sent-161, score-0.2]
</p><p>41 The template with the highest overall score is selected and filled with matching entity tags from the input data and 1409  appended to the generated text. [sent-163, score-0.266]
</p><p>42 Before generating the next sentence, we track those entities used in the initial sentence generation and decide to either remove those entities from the input data or keep the entity for one or more additional sentence generations. [sent-164, score-0.283]
</p><p>43 For example, in the biography discourses, the name of the person may occur only once in the input data, but it may be useful for creating good texts to have that person’s name available for subsequent generations. [sent-165, score-0.388]
</p><p>44 To illustrate in (3), if we remove James Smithton from the input data after the initial generation, an irrelevant sentence (d) is generated as the input data will only have one company after the removal of James Smithton and the model will  only select a template with one company. [sent-166, score-0.306]
</p><p>45 If we keep James Smithton, then the generations in (a-b) are more cohesive. [sent-167, score-0.227]
</p><p>46 For example, some entities are very unique to a text and should not be made available for subsequent generations as doing so would lead to unwanted redundancies (e. [sent-180, score-0.263]
</p><p>47 , mentioning the name of current company in a biography discourse more than once as in (3)) and some entities are general and should be retained. [sent-182, score-0.368]
</p><p>48 Our system possesses the ability to monitor the data usage from historical data and we can set parameters (based on the distribution of en-  tities) on the usage to ensure coherent generations for a given domain. [sent-183, score-0.335]
</p><p>49 Then, the results of both automatic and human evaluations of our system’s generations against the original and baseline texts are considered as a means of determining performance. [sent-189, score-0.325]
</p><p>50 For all experiments reported in this section, the baseline system selects the most frequent conceptual unit at the given position, chooses the most likely template for the  conceptual unit, and fills the template with input data. [sent-190, score-0.518]
</p><p>51 1 Data We ran our system on two different domains: corporate officer and director biographies and offshore oil rig weather reports from the SUMTIMEMETEO corpus ((Reiter et al. [sent-193, score-0.353]
</p><p>52 The biography domain includes 1150 texts ranging from 3-17 sentences and the weather domain includes 1045 weather reports ranging from 1-6 sentences. [sent-195, score-0.886]
</p><p>53 (4) provides generation comparisons for the system ( DocSys), baseline ( DocBase) and original ( DocOrig) randomly selected text snippets from each domain. [sent-197, score-0.121]
</p><p>54 The variability of the generated texts ranges from a close similarity to slightly shorter - not an uncommon (Belz and Reiter, 2006), but not necessarily detrimental, observation for NLG systems (van Deemter et al. [sent-198, score-0.19]
</p><p>55 However, we provide no comparison between our system and SUMTIME-METEO as our system utilized the generated forecasts from SUMTIME-METEO’s system as the historical data. [sent-211, score-0.233]
</p><p>56 We cannot compare with other statistical generation systems like (Belz, 2007) as they only focussed on the part of the forecasts the predicts wind characteristics whereas our system generates the complete forecasts. [sent-212, score-0.15]
</p><p>57 The  DocSys  and  largely grammatical  DocBase  generations  are  and coherent on the surface  with some variance, but there are graded semantic variations (e. [sent-215, score-0.255]
</p><p>58 Both automatic and human evaluations are required in NLG to determine the impact ofthese variances on the understandability of the texts in general (non-experts) and as they are representative of particular subject matter domains (experts). [sent-220, score-0.18]
</p><p>59 These metrics only evaluate the text on a document level but fail to identify “syntactic repetitiveness” across documents in a document collection. [sent-230, score-0.126]
</p><p>60 In order to compute this metric, each document should be  represented as a sequence of templates by associating each sentence in the document with a template in the template bank. [sent-233, score-0.58]
</p><p>61 Syntactic variability is defined as the percentage of unique template sequences across all generated documents. [sent-234, score-0.325]
</p><p>62 3 As indicated in Figure 2, the BLEU-4 scores are low for all DocSys and DocBase generations (as compared to DocOrig) for each domain. [sent-237, score-0.227]
</p><p>63 Both BLEU–4 and METEOR measure the similarity of the generated text to the original text, but fail to penalize repetitiveness across texts, which is addressed by the syntactic variability metric. [sent-246, score-0.189]
</p><p>64 There is no statistically significant difference between DocSys and DocBase generations for METEOR and BLEU–4. [sent-247, score-0.252]
</p><p>65 4 However, there is a statistically  significant difference in the syntactic variability metric for both domains (weather - χ2=137. [sent-248, score-0.195]
</p><p>66 0001) - the variability of the DocSys generations is greater than the DocBase generations, which shows that texts generated by our system are more variable than the baseline texts. [sent-256, score-0.447]
</p><p>67 For the text–understandability task, 40 documents were chosen at random from the DocOrig test set along with the corresponding 40 DocSys and DocBase generations (240 documents total/120 for each domain). [sent-284, score-0.227]
</p><p>68 8 judgments per document were solicited from the crowd (1920 to-  tal judgments, 69. [sent-285, score-0.144]
</p><p>69 51 average agreement) and are summarized in Figures 3 and 4 (biography and weather respectively). [sent-286, score-0.235]
</p><p>70 If the system is performing well and the ranking model is actually contributing to increased performance, the accepted trend should be that the DocOrig texts are more fluent and preferred compared to both the DocSys and DocBase systems. [sent-287, score-0.216]
</p><p>71 Focusing on fluency ratings, it is expected that the DocOrig generations will have the highest fluency (as they are human generated). [sent-290, score-0.301]
</p><p>72 Figure 3, which shows the biography text evaluations, demonstrates this acceptable distribution of performances. [sent-292, score-0.304]
</p><p>73 For the weather discourses, as evident from Figure 4, the acceptable trend holds between the DocSys and DocBase generations, and the DocSys generation fluency is actually slightly higher than DocOrig. [sent-293, score-0.455]
</p><p>74 This is possibly because the DocOrig texts are from a particular subject matter weather forecasts for offshore oil rigs in the U. [sent-294, score-0.346]
</p><p>75 In terms of significance, there are no statistically significant differences between the systems for weather (DocOrig vs. [sent-298, score-0.26]
</p><p>76 For the sentence preference task, equivalent sentences across the 120 documents were chosen at random (80 sentences from biography and 74 sentences from weather). [sent-333, score-0.299]
</p><p>77 Similar to the text–understandability task, an acceptable performance pattern should include the DocOrig texts being preferred to both DocSys and DocBase generations and the DocSys generation preferred to the DocBase. [sent-336, score-0.454]
</p><p>78 The biography domain illus1412  Figure 5: Biography Sentence Evaluations. [sent-338, score-0.317]
</p><p>79 In contrast, for weather domain, sentences from DocBase system were preferred to our system’s (Figure 6). [sent-340, score-0.29]
</p><p>80 In terms of significance, there are no statistically significant differences between the systems for weather (DocOrig vs. [sent-343, score-0.26]
</p><p>81 The trend is different compared to the fluency metric above in that the DocBase system is outperforming the DocOrig generations to an almost statistically significant difference - the remaining comparisons follow the trend. [sent-359, score-0.4]
</p><p>82 More problematic is the results of the biography evaluations. [sent-363, score-0.273]
</p><p>83 Here there is a statistically significant difference between the DocSys and DocOrig and no statistically significant difference between the DocSys and DocBase generations (DocOrig vs. [sent-364, score-0.277]
</p><p>84 4  Expert Human Evaluations  We performed expert evaluations for the biography domain only as we do not have access to  weather experts. [sent-384, score-0.643]
</p><p>85 The four biography reviewers are journalists who write short biographies for news archives. [sent-385, score-0.273]
</p><p>86 For the biography domain, evaluations of the texts were largely similar to the evaluations of the non-expert crowd (76. [sent-386, score-0.445]
</p><p>87 For example, the disfluent ratings were highest for the DocBase generations and lowest for the DocOrig generations. [sent-389, score-0.254]
</p><p>88 Also, the fluent ratings were highest for the DocOrig generations, and while the combined fluent and understandable are higher for DocSys as compared to DocBase, the DocBase generations had a 10% higher fluent score (58. [sent-390, score-0.344]
</p><p>89 Based on notes from the reviewers, the succinctness of the the DocBase generations are preferred in some ways as they are in keeping with certain editorial standards. [sent-393, score-0.252]
</p><p>90 This is further reflected in the sentence preferences being 70% in favor of the DocBase generations as compared to the DocSys generations (all other sentence comparisons were consis-  tent with the non-expert crowd). [sent-394, score-0.506]
</p><p>91 The development time to adapt our system to new domains is small compared to other NLG systems; around a week to adapt the system to weather and biography domains. [sent-401, score-0.595]
</p><p>92 Most  of the development time was spent on creating the domain-specific entity taggers for the weather domain. [sent-402, score-0.275]
</p><p>93 The development time would be reduced to hours if the historical data for a domain is readily available with the corresponding input data. [sent-403, score-0.15]
</p><p>94 Our system does consolidate many traditional components (macroand micro-planning, lexical choice and aggregation),5 but the system cannot be applied to the domains with no historical data. [sent-405, score-0.165]
</p><p>95 The quality and the linguistic variability of the generated text is directly proportional to the amount of historical data available. [sent-406, score-0.213]
</p><p>96 We also presented a new automatic metric to evaluate generated texts at document collection level to identify boilerplate texts. [sent-407, score-0.198]
</p><p>97 This metric computes “syntactic repetitiveness” by counting the number of unique template sequences across the given document collection. [sent-408, score-0.297]
</p><p>98 For example, most NLG pipelines have a separate component responsible for referring expression generation (Krahmer and van Deemter, 2012). [sent-410, score-0.124]
</p><p>99 We believe that this is possible by identifying referring expressions in templates and adding features to the model to give higher scores to the templates having relevant referring expressions. [sent-413, score-0.274]
</p><p>100 Investigating content selection for language generation using machine learning. [sent-484, score-0.121]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('docsys', 0.43), ('docbase', 0.39), ('docorig', 0.349), ('biography', 0.273), ('weather', 0.235), ('cuid', 0.229), ('generations', 0.227), ('nlg', 0.19), ('template', 0.162), ('planning', 0.138), ('templates', 0.104), ('variability', 0.099), ('generation', 0.091), ('smithton', 0.081), ('historical', 0.078), ('meteor', 0.072), ('conceptual', 0.068), ('cornwall', 0.067), ('document', 0.063), ('director', 0.061), ('texts', 0.055), ('understandability', 0.055), ('duboue', 0.054), ('howald', 0.054), ('repetitiveness', 0.054), ('reiter', 0.051), ('sales', 0.049), ('expert', 0.048), ('position', 0.044), ('metric', 0.044), ('domain', 0.044), ('evaluations', 0.043), ('belz', 0.041), ('cuids', 0.04), ('entity', 0.04), ('realization', 0.039), ('ehud', 0.039), ('fluent', 0.039), ('serving', 0.038), ('trend', 0.037), ('fluency', 0.037), ('generated', 0.036), ('entities', 0.036), ('cfo', 0.036), ('kondadadi', 0.036), ('sripada', 0.036), ('discourse', 0.033), ('referring', 0.033), ('deemter', 0.033), ('numerically', 0.033), ('person', 0.032), ('crowd', 0.031), ('bio', 0.031), ('barriers', 0.031), ('cold', 0.031), ('enlg', 0.031), ('acceptable', 0.031), ('experts', 0.031), ('system', 0.03), ('content', 0.03), ('ranking', 0.03), ('bank', 0.03), ('forecasts', 0.029), ('mckeown', 0.029), ('bleu', 0.029), ('input', 0.028), ('football', 0.028), ('sequences', 0.028), ('surface', 0.028), ('named', 0.028), ('predicates', 0.027), ('stage', 0.027), ('adaptable', 0.027), ('domains', 0.027), ('afnredq', 0.027), ('appointed', 0.027), ('bateman', 0.027), ('boxer', 0.027), ('disfluent', 0.027), ('fordway', 0.027), ('internation', 0.027), ('keyes', 0.027), ('offshore', 0.027), ('solicited', 0.027), ('somayajulu', 0.027), ('company', 0.026), ('hybrid', 0.026), ('stages', 0.026), ('sentence', 0.026), ('ne', 0.025), ('statistically', 0.025), ('preferred', 0.025), ('holds', 0.024), ('anja', 0.024), ('denkowski', 0.024), ('kamp', 0.024), ('kees', 0.024), ('krahmer', 0.024), ('originals', 0.024), ('judgments', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="21-tfidf-1" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>Author: Ravi Kondadadi ; Blake Howald ; Frank Schilder</p><p>Abstract: We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated weather and biography texts fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. – – *∗Ravi Kondadadi is now affiliated with Nuance Communications, Inc.</p><p>2 0.12386379 <a title="21-tfidf-2" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>Author: Sina Zarriess ; Jonas Kuhn</p><p>Abstract: We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</p><p>3 0.11529034 <a title="21-tfidf-3" href="./acl-2013-Tag2Blog%3A_Narrative_Generation_from_Satellite_Tag_Data.html">337 acl-2013-Tag2Blog: Narrative Generation from Satellite Tag Data</a></p>
<p>Author: Kapila Ponnamperuma ; Advaith Siddharthan ; Cheng Zeng ; Chris Mellish ; Rene van der Wal</p><p>Abstract: The aim of the Tag2Blog system is to bring satellite tagged wild animals “to life” through narratives that place their movements in an ecological context. Our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. We are working with one of the largest nature conservation charities in Europe in this regard, focusing on a single species, the red kite. We describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual, and constructs a story around its use of the landscape.</p><p>4 0.092685014 <a title="21-tfidf-4" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>Author: Lu Wang ; Claire Cardie</p><p>Abstract: We address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion. We apply Multiple-Sequence Alignment to induce abstract generation templates that can be used for different domains. An Overgenerateand-Rank strategy is utilized to produce and rank candidate abstracts. Experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches. In addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality.</p><p>5 0.089492925 <a title="21-tfidf-5" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>Author: Gerasimos Lampouras ; Ion Androutsopoulos</p><p>Abstract: We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.</p><p>6 0.088341944 <a title="21-tfidf-6" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>7 0.075497061 <a title="21-tfidf-7" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>8 0.073595509 <a title="21-tfidf-8" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>9 0.066016339 <a title="21-tfidf-9" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>10 0.047548171 <a title="21-tfidf-10" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>11 0.046519876 <a title="21-tfidf-11" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>12 0.045269798 <a title="21-tfidf-12" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>13 0.045139637 <a title="21-tfidf-13" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>14 0.044609763 <a title="21-tfidf-14" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>15 0.0426322 <a title="21-tfidf-15" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>16 0.040107746 <a title="21-tfidf-16" href="./acl-2013-Translating_Italian_connectives_into_Italian_Sign_Language.html">360 acl-2013-Translating Italian connectives into Italian Sign Language</a></p>
<p>17 0.040106781 <a title="21-tfidf-17" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>18 0.039174087 <a title="21-tfidf-18" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>19 0.039172415 <a title="21-tfidf-19" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>20 0.038603719 <a title="21-tfidf-20" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.13), (1, 0.027), (2, 0.016), (3, -0.056), (4, -0.002), (5, 0.028), (6, 0.008), (7, -0.018), (8, -0.023), (9, 0.028), (10, -0.039), (11, 0.03), (12, -0.034), (13, 0.027), (14, -0.052), (15, -0.014), (16, 0.027), (17, -0.042), (18, -0.021), (19, -0.014), (20, -0.045), (21, -0.004), (22, 0.003), (23, 0.037), (24, -0.006), (25, 0.047), (26, 0.073), (27, -0.033), (28, -0.017), (29, 0.014), (30, -0.003), (31, -0.088), (32, 0.014), (33, 0.034), (34, -0.035), (35, 0.038), (36, -0.024), (37, 0.019), (38, -0.058), (39, -0.003), (40, 0.051), (41, 0.091), (42, 0.008), (43, -0.024), (44, -0.049), (45, 0.026), (46, 0.149), (47, -0.051), (48, -0.146), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87032974 <a title="21-lsi-1" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>Author: Ravi Kondadadi ; Blake Howald ; Frank Schilder</p><p>Abstract: We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated weather and biography texts fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. – – *∗Ravi Kondadadi is now affiliated with Nuance Communications, Inc.</p><p>2 0.8361423 <a title="21-lsi-2" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>Author: Sina Zarriess ; Jonas Kuhn</p><p>Abstract: We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</p><p>3 0.82969785 <a title="21-lsi-3" href="./acl-2013-Tag2Blog%3A_Narrative_Generation_from_Satellite_Tag_Data.html">337 acl-2013-Tag2Blog: Narrative Generation from Satellite Tag Data</a></p>
<p>Author: Kapila Ponnamperuma ; Advaith Siddharthan ; Cheng Zeng ; Chris Mellish ; Rene van der Wal</p><p>Abstract: The aim of the Tag2Blog system is to bring satellite tagged wild animals “to life” through narratives that place their movements in an ecological context. Our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. We are working with one of the largest nature conservation charities in Europe in this regard, focusing on a single species, the red kite. We describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual, and constructs a story around its use of the landscape.</p><p>4 0.75905544 <a title="21-lsi-4" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>Author: Gerasimos Lampouras ; Ion Androutsopoulos</p><p>Abstract: We present an ILP model of concept-totext generation. Unlike pipeline architectures, our model jointly considers the choices in content selection, lexicalization, and aggregation to avoid greedy decisions and produce more compact texts.</p><p>5 0.722224 <a title="21-lsi-5" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Heriberto Cuayahuitl ; Oliver Lemon</p><p>Abstract: Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher (up to 14%) than those for taking only local context into account.</p><p>6 0.67359072 <a title="21-lsi-6" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>7 0.59722543 <a title="21-lsi-7" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>8 0.59516543 <a title="21-lsi-8" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>9 0.5643239 <a title="21-lsi-9" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>10 0.56119168 <a title="21-lsi-10" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>11 0.52511668 <a title="21-lsi-11" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>12 0.48085588 <a title="21-lsi-12" href="./acl-2013-Computerized_Analysis_of_a_Verbal_Fluency_Test.html">89 acl-2013-Computerized Analysis of a Verbal Fluency Test</a></p>
<p>13 0.47648779 <a title="21-lsi-13" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>14 0.46416742 <a title="21-lsi-14" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>15 0.45691821 <a title="21-lsi-15" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>16 0.44976103 <a title="21-lsi-16" href="./acl-2013-Is_word-to-phone_mapping_better_than_phone-phone_mapping_for_handling_English_words%3F.html">203 acl-2013-Is word-to-phone mapping better than phone-phone mapping for handling English words?</a></p>
<p>17 0.43953079 <a title="21-lsi-17" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>18 0.42704192 <a title="21-lsi-18" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>19 0.42534614 <a title="21-lsi-19" href="./acl-2013-Simple%2C_readable_sub-sentences.html">322 acl-2013-Simple, readable sub-sentences</a></p>
<p>20 0.4246406 <a title="21-lsi-20" href="./acl-2013-A_Novel_Classifier_Based_on_Quantum_Computation.html">14 acl-2013-A Novel Classifier Based on Quantum Computation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.039), (6, 0.023), (11, 0.061), (15, 0.02), (24, 0.042), (26, 0.045), (34, 0.291), (35, 0.072), (42, 0.101), (48, 0.032), (70, 0.045), (88, 0.025), (90, 0.037), (95, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74706054 <a title="21-lda-1" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>Author: Ravi Kondadadi ; Blake Howald ; Frank Schilder</p><p>Abstract: We present a hybrid natural language generation (NLG) system that consolidates macro and micro planning and surface realization tasks into one statistical learning process. Our novel approach is based on deriving a template bank automatically from a corpus of texts from a target domain. First, we identify domain specific entity tags and Discourse Representation Structures on a per sentence basis. Each sentence is then organized into semantically similar groups (representing a domain specific concept) by k-means clustering. After this semi-automatic processing (human review of cluster assignments), a number of corpus–level statistics are compiled and used as features by a ranking SVM to develop model weights from a training corpus. At generation time, a set of input data, the collection of semantically organized templates, and the model weights are used to select optimal templates. Our system is evaluated with automatic, non–expert crowdsourced and expert evaluation metrics. We also introduce a novel automatic metric syntactic variability that represents linguistic variation as a measure of unique template sequences across a collection of automatically generated documents. The metrics for generated weather and biography texts fall within acceptable ranges. In sum, we argue that our statistical approach to NLG reduces the need for complicated knowledge-based architectures and readily adapts to different domains with reduced development time. – – *∗Ravi Kondadadi is now affiliated with Nuance Communications, Inc.</p><p>2 0.66636658 <a title="21-lda-2" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>3 0.56648678 <a title="21-lda-3" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>Author: Xipeng Qiu ; Qi Zhang ; Xuanjing Huang</p><p>Abstract: The growing need for Chinese natural language processing (NLP) is largely in a range of research and commercial applications. However, most of the currently Chinese NLP tools or components still have a wide range of issues need to be further improved and developed. FudanNLP is an open source toolkit for Chinese natural language processing (NLP) , which uses statistics-based and rule-based methods to deal with Chinese NLP tasks, such as word segmentation, part-ofspeech tagging, named entity recognition, dependency parsing, time phrase recognition, anaphora resolution and so on.</p><p>4 0.52047527 <a title="21-lda-4" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>5 0.51687199 <a title="21-lda-5" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>6 0.51568151 <a title="21-lda-6" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>7 0.51141846 <a title="21-lda-7" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>8 0.51120883 <a title="21-lda-8" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>9 0.50838226 <a title="21-lda-9" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>10 0.50805396 <a title="21-lda-10" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>11 0.50791979 <a title="21-lda-11" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>12 0.50735301 <a title="21-lda-12" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>13 0.50691015 <a title="21-lda-13" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>14 0.50659424 <a title="21-lda-14" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>15 0.5065015 <a title="21-lda-15" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>16 0.50640953 <a title="21-lda-16" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>17 0.50633591 <a title="21-lda-17" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>18 0.50406229 <a title="21-lda-18" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>19 0.50373048 <a title="21-lda-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.50337827 <a title="21-lda-20" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
