<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>311 acl-2013-Semantic Neighborhoods as Hypergraphs</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-311" href="#">acl2013-311</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>311 acl-2013-Semantic Neighborhoods as Hypergraphs</h1>
<br/><p>Source: <a title="acl-2013-311-pdf" href="http://aclweb.org/anthology//P/P13/P13-2040.pdf">pdf</a></p><p>Author: Chris Quirk ; Pallavi Choudhury</p><p>Abstract: Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. Lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. We advocate hypergraphs as compact representations for sets of utterances describing the same event or object. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy.</p><p>Reference: <a title="acl-2013-311-reference" href="../acl2013_reference/acl-2013-Semantic_Neighborhoods_as_Hypergraphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com avi  Abstract Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. [sent-2, score-0.366]
</p><p>2 Lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. [sent-3, score-0.088]
</p><p>3 We advocate hypergraphs as compact representations for sets of utterances describing the same event or object. [sent-4, score-0.405]
</p><p>4 We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. [sent-5, score-0.1]
</p><p>5 Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize  novel descriptions of the same event with high accuracy. [sent-6, score-0.516]
</p><p>6 1 Introduction Humans can construct a broad range of descriptions for almost any object or event. [sent-7, score-0.242]
</p><p>7 , 2010), videos (Chen and Dolan, 2011), translations of a sentence from another language (Dreyer and Marcu, 2012), or even paraphrases ofthe same sentence (Barzilay and Lee, 2003). [sent-10, score-0.151]
</p><p>8 One crucial problem is recognizing whether novel utterances are relevant descriptions of those groundings. [sent-11, score-0.372]
</p><p>9 Generating descriptions of events is also often an interesting task: we might like to find a novel paraphrase for a given sentence, or generate a description of a grounding that meets certain criteria (e. [sent-13, score-0.429]
</p><p>10 Much prior work has used lattices to compactly  represent a range of lexical choices (Pang et al. [sent-16, score-0.176]
</p><p>11 However, lattices cannot compactly represent alternate word orders, a common occurrence in linguistic descriptions. [sent-18, score-0.176]
</p><p>12 Consider the following excerpts from a video description corpus (Chen and Dolan, 2011): • A man is sliding a cat on the floor. [sent-19, score-0.465]
</p><p>13 • AA boy i iss cleaning th caet f loono trh we fitloh otrh. [sent-20, score-0.114]
</p><p>14 • AA cat i ss being pushed across hth teh efl coaot. [sent-22, score-0.276]
</p><p>15 Ideally we would like to recognize that the following utterance is also a valid description of that event: A cat is being pushed across the floor by a boy. [sent-24, score-0.371]
</p><p>16 Consider the following context free grammar: S → X0 X1 | X2 X3 X0 → a man | a boy X1 → is sliding X2 on X4 | is cleaning X4 with X2 X2 → a cat | the cat X3 → is being pushed across X4 by X0 X4 → the floor  This grammar compactly captures many lexical and syntactic variants of the input set. [sent-26, score-1.084]
</p><p>17 This hypergraph or grammar represents a semantic neighborhood: a set of utterances that describe the same entity in a semantic space. [sent-28, score-0.549]
</p><p>18 Semantic neighborhoods are defined in terms of a grounding. [sent-29, score-0.128]
</p><p>19 Two utterances are neighbors with respect to some grounding (semantic event) if they are both descriptions of that grounding. [sent-30, score-0.469]
</p><p>20 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 2 2–2 7, are considered paraphrases if there exists some grounding that they both describe. [sent-34, score-0.166]
</p><p>21 The paraphrase relation is more permissive than the semantic neighbor relation in that regard. [sent-35, score-0.122]
</p><p>22 Human annotators may have difficulty separating paraphrases from unrelated or merely  related utterances, and this line may not be consistent between judges. [sent-37, score-0.069]
</p><p>23 Annotating whether an utterance clearly describes a grounding is a much easier task. [sent-38, score-0.139]
</p><p>24 The method is evaluated in a paraphrase recognition task, inspired by a CAPTCHA task (Von Ahn et al. [sent-40, score-0.156]
</p><p>25 2  Inducing neighborhoods  Constructing a hypergraph to capture a set of utterances is a variant of grammar induction. [sent-42, score-0.677]
</p><p>26 Given a sample of positive examples, we infer a compact and accurate description of the underlying language. [sent-43, score-0.097]
</p><p>27 Conventional grammar induction attempts to define the set of grammatical sentences in the language. [sent-44, score-0.256]
</p><p>28 Here, we search for a grammar over the fluent and adequate descriptions of a particular input. [sent-45, score-0.463]
</p><p>29 This parsed set of utterances acts as a sort of treebank. [sent-49, score-0.243]
</p><p>30 Reading off a grammar from this treebank produces a grammar that can generate not only the seed sentences, but also a broad range of nearby sentences. [sent-50, score-0.662]
</p><p>31 In the case above with cat, man, and boy, we would be able to generate cases legitimate variants where man was replaced by boy as well as undesired variants where man is replaced by cat or floor. [sent-51, score-0.596]
</p><p>32 This initial grammar captures a large neighborhood of nearby utterances including many such undesirable ones. [sent-52, score-0.543]
</p><p>33 Inspired by the result that manual annotations of Treebank categories can substantially increase parser accuracy (Klein and Manning, 2003), several approaches have been introduced to automatically induce latent symbols on existing trees. [sent-55, score-0.233]
</p><p>34 In its original setting, the refinements captured details beyond that of the  original Penn Treebank symbols. [sent-58, score-0.269]
</p><p>35 Here, we capture both syntactic and semantic regularities in the descriptions of a given grounding. [sent-59, score-0.161]
</p><p>36 As we perform more rounds of refinement, the grammar becomes tightly constrained to the original sentences. [sent-60, score-0.39]
</p><p>37 Indeed, if we iterated to a fixed point, the resulting grammar would parse only the original sentences. [sent-61, score-0.293]
</p><p>38 This is a common dilemma in paraphrase learning: the safest meaning preserving rewrite is to change nothing. [sent-62, score-0.156]
</p><p>39 We optimize the number of split-merge rounds for task-accuracy; two or three rounds works well in practice. [sent-63, score-0.194]
</p><p>40 1 Split-merge induction We begin with a set of utterances that describe a specific grounding. [sent-66, score-0.211]
</p><p>41 They are parsed with a conventional Penn Treebank parser (Quirk et al. [sent-67, score-0.113]
</p><p>42 This treebank is the input to the split-merge process. [sent-70, score-0.11]
</p><p>43 Split: Given an input treebank, we propose refinements of the symbols in hopes of increasing the likelihood of the data. [sent-71, score-0.321]
</p><p>44 For each original symbol in the grammar such as NP, we consider two latent refinements: NP0 and NP1. [sent-72, score-0.402]
</p><p>45 The parameters of this grammar are then optimized using EM. [sent-74, score-0.256]
</p><p>46 Although we do not know the correct set of latent annotations, we can search for the parameters that optimize the likelihood of the given treebank. [sent-75, score-0.062]
</p><p>47 We initialize the parameters of this refined grammar with the counts from the original grammar along with a small random number. [sent-76, score-0.646]
</p><p>48 Merge: After EM has run to completion, we have a new grammar with twice as many symbols and eight times as many rules. [sent-79, score-0.35]
</p><p>49 Many of these symbols may not be necessary, however. [sent-80, score-0.094]
</p><p>50 For instance, nouns may require substantial refinement to distinguish a number of different actors and objects,  where determiners might not require much refinement at all. [sent-81, score-0.106]
</p><p>51 Therefore, we discard the splits that led to the least increase in likelihood, reestimate the grammar once again. [sent-82, score-0.314]
</p><p>52 First a conventional Treebank parser converts input utterances (a) into parse trees (b). [sent-84, score-0.324]
</p><p>53 A grammar could be directly read from this small treebank, but it would conflate all phrases of the same type. [sent-85, score-0.256]
</p><p>54 Instead we induce latent refinements of this small treebank (c). [sent-86, score-0.335]
</p><p>55 The resulting grammar (d) can match and generate novel variants of these inputs, such as the man plays the keyboard and the buy plays the piano. [sent-87, score-0.77]
</p><p>56 While this simplified example suggests a single hard assignment of latent annotations to symbols, in practice we maintain a distribution over these latent annotations and extract a weighted grammar. [sent-88, score-0.214]
</p><p>57 First the original grammar is split, then some of the least useful splits are discarded. [sent-90, score-0.351]
</p><p>58 This refined grammar is then split again, with the least useful splits discarded once again. [sent-91, score-0.447]
</p><p>59 Final grammar estimation: The EM procedure used during split and merge assigns fractional counts c(· · · ) to each refined symbol Xi and each production Xi → Yj Zk. [sent-93, score-0.532]
</p><p>60 dW sey mesbtoimlXa te the final grammar using these fractional counts. [sent-94, score-0.312]
</p><p>61 , these latent refinements are later discarded as the goal is to find the best parse with the original coarse symbols. [sent-96, score-0.294]
</p><p>62 Here, we retain the latent refinements during parsing, since they distinguish semantically related utterances from unrelated utterances. [sent-97, score-0.501]
</p><p>63 Note in Figure 1 how NN0 and NN1 refer to different objects; were we to ignore that distinction, the parser would recognize semantically different utterances such as the piano  plays the piano. [sent-98, score-0.556]
</p><p>64 Here we only use an absolute threshold; we vary this threshold and inspect the impact on task accuracy. [sent-102, score-0.053]
</p><p>65 Once the fully refined grammar has been trained, we only retain those rules with a probability above some threshold. [sent-103, score-0.386]
</p><p>66 By varying this threshold t we can adjust precision and recall: as the low probability rules are removed from the grammar, precision tends to increase and recall tends to decrease. [sent-104, score-0.135]
</p><p>67 When parsing with a grammar obtained from only 20 to 50 sentences, we are very likely to encounter words that have never been seen before. [sent-106, score-0.256]
</p><p>68 If the fractional count of a word given a pre-terminal symbol falls below a threshold k, then we consider that instance rare and reserve a fraction of its probability mass for unseen words. [sent-110, score-0.156]
</p><p>69 A broad range of approximations are available (Nederhof, 2000). [sent-113, score-0.09]
</p><p>70 Since the small grammars in our evaluation below seldom exhibit self-embedding (latent state identification 224  tends to remove recursion), these approximations would often be tight. [sent-114, score-0.094]
</p><p>71 Given a large set of videos and a number of de-  scriptions for each video (Chen and Dolan, 2011), we build a system that can recognize fluent and accurate descriptions of videos. [sent-116, score-0.451]
</p><p>72 One example currently in evaluation is a novel CAPTCHAs: to differentiate a human from a bot, a video is presented, and the response must be a reasonably accurate and fluent description of this video. [sent-118, score-0.203]
</p><p>73 Then we present these recognizers with a series of inputs, some of which are from the held out set of correct descriptions of this video, and some of which are from descriptions of other videos. [sent-121, score-0.322]
</p><p>74 This simulates the accuracy of the system when presented with a simple bot that supplies random, well-formed text as CAPTCHA answers. [sent-123, score-0.077]
</p><p>75 In this baseline we first pool all  the training descriptions of the video into a single virtual document. [sent-125, score-0.269]
</p><p>76 An incoming utterance to be classified is scored by computing the dot product of its counted terms with each document; it is assigned to the document with the highest dot product (cosine similarity). [sent-127, score-0.11]
</p><p>77 That said, grammar based approach shows improvements over the baseline tf-idf, especially in recall. [sent-130, score-0.256]
</p><p>78 Recall is crucial in a CAPTCHA style task: if we fail to recognize utterances provided by humans, we risk frustration or abandonment of the service protected by the CAPTCHA. [sent-131, score-0.265]
</p><p>79 The relative importance offalse positives versus false negatives –  1A bot might perform object recognition on the videos and supply a stream of object names. [sent-132, score-0.281]
</p><p>80 We might simulate this by classifying utterances consisting of appropriate object words but without appropriate syntax or function words. [sent-133, score-0.255]
</p><p>81 The descriptions from the video description corpus are randomly partitioned into training and test. [sent-135, score-0.318]
</p><p>82 (a) Comparison of  tf-idf baseline  against grammar  varying several free parameters. [sent-141, score-0.256]
</p><p>83 based approach, An oracle checks  if the correct video is in the top three. [sent-142, score-0.108]
</p><p>84 For the  variants, the number of splits S and the smoothing threshold k are varied. [sent-143, score-0.153]
</p><p>85 (b) Variations  grammar  on the rule pruning  threshold  split-merge rounds S. [sent-144, score-0.476]
</p><p>86 >  t and number  of  0 indicates that all rules  Here the smoothing  threshold  k is  fixed at 32. [sent-146, score-0.095]
</p><p>87 225  (a) Input descriptions: • •• •• •• •• •• •• •• •• •• •• •• ••  A cat pops a bunch of little balloons that are on the groung. [sent-147, score-0.368]
</p><p>88 n AA ddoogg iast tabcitiknsg a b baullnocohn so fa bndal l pooopnps. [sent-149, score-0.283]
</p><p>89 AA ddoogg i ss pblitaiynigng b a bllaolloonosns a. [sent-151, score-0.374]
</p><p>90 AA ddoogg ipsla pyosp pwiinthg ab a blluonochns o. [sent-160, score-0.283]
</p><p>91 The descriptions in (a) were parsed as-is (including the typographical error “groung”),  and a refined grammar was trained with 4 splits. [sent-193, score-0.546]
</p><p>92 The top k yields from  this grammar along with the probability of that derivation are listed in (b). [sent-194, score-0.256]
</p><p>93 No smoothing or pruning was performed on this grammar. [sent-196, score-0.112]
</p><p>94 We can see that rule pruning does not have a large impact on overall results, though it does allow yet another means of tradiing off precision vs. [sent-199, score-0.07]
</p><p>95 The refined symbols of the grammar act as a correspondence between related inputs. [sent-205, score-0.447]
</p><p>96 A straightforward extension would be to consider an n-best list or packed forest of input parses, which would allow the method to move past errors in the first input process. [sent-208, score-0.064]
</p><p>97 Perhaps also this reliance on symbols from the original Tree-  bank is not ideal. [sent-209, score-0.131]
</p><p>98 We could merge away some or all of the original distinctions, or explore different parameterizations of the grammar that allow more flexibility in parsing. [sent-210, score-0.333]
</p><p>99 We are investigating means of including additional paraphrase resources into the training to increase the effective lexical knowledge of the system. [sent-212, score-0.122]
</p><p>100 Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. [sent-242, score-0.069]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dog', 0.286), ('ddoogg', 0.283), ('grammar', 0.256), ('utterances', 0.211), ('refinements', 0.195), ('bunch', 0.189), ('aa', 0.188), ('descriptions', 0.161), ('captcha', 0.157), ('piano', 0.157), ('man', 0.139), ('dt', 0.138), ('neighborhoods', 0.128), ('nvpp', 0.126), ('cat', 0.123), ('paraphrase', 0.122), ('keyboard', 0.111), ('video', 0.108), ('plays', 0.102), ('refined', 0.097), ('rounds', 0.097), ('grounding', 0.097), ('symbols', 0.094), ('ss', 0.091), ('compactly', 0.088), ('lattices', 0.088), ('groundings', 0.083), ('playing', 0.082), ('hypergraph', 0.082), ('guy', 0.082), ('videos', 0.082), ('vbz', 0.08), ('treebank', 0.078), ('bot', 0.077), ('dolan', 0.075), ('boy', 0.075), ('pruning', 0.07), ('paraphrases', 0.069), ('hypergraphs', 0.066), ('bbaall', 0.063), ('epl', 0.063), ('ppllaayyiinngg', 0.063), ('ppooppppiinngg', 0.063), ('vdtbz', 0.063), ('latent', 0.062), ('pushed', 0.062), ('variants', 0.06), ('splits', 0.058), ('np', 0.057), ('fractional', 0.056), ('dreyer', 0.056), ('popping', 0.056), ('pops', 0.056), ('rashtchian', 0.056), ('recognize', 0.054), ('threshold', 0.053), ('refinement', 0.053), ('approximations', 0.053), ('pallavi', 0.051), ('description', 0.049), ('conventional', 0.049), ('compact', 0.048), ('petrov', 0.048), ('symbol', 0.047), ('sliding', 0.046), ('event', 0.046), ('fluent', 0.046), ('annotations', 0.045), ('object', 0.044), ('choudhury', 0.042), ('utterance', 0.042), ('smoothing', 0.042), ('neighborhood', 0.041), ('floor', 0.041), ('nn', 0.041), ('tends', 0.041), ('eal', 0.04), ('barzilay', 0.04), ('quirk', 0.04), ('merge', 0.04), ('cleaning', 0.039), ('broad', 0.037), ('original', 0.037), ('split', 0.036), ('von', 0.036), ('ahn', 0.036), ('nearby', 0.035), ('dot', 0.034), ('recognition', 0.034), ('preserving', 0.034), ('describing', 0.034), ('reordering', 0.033), ('retain', 0.033), ('parsed', 0.032), ('tthhee', 0.032), ('input', 0.032), ('parser', 0.032), ('parses', 0.031), ('objects', 0.03), ('chen', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="311-tfidf-1" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>Author: Chris Quirk ; Pallavi Choudhury</p><p>Abstract: Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. Lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. We advocate hypergraphs as compact representations for sets of utterances describing the same event or object. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy.</p><p>2 0.10703844 <a title="311-tfidf-2" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>Author: Haonan Yu ; Jeffrey Mark Siskind</p><p>Abstract: We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.</p><p>3 0.10396747 <a title="311-tfidf-3" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>Author: Hua He ; Denilson Barbosa ; Grzegorz Kondrak</p><p>Abstract: Speaker identification is the task of at- tributing utterances to characters in a literary narrative. It is challenging to auto- mate because the speakers of the majority ofutterances are not explicitly identified in novels. In this paper, we present a supervised machine learning approach for the task that incorporates several novel features. The experimental results show that our method is more accurate and general than previous approaches to the problem.</p><p>4 0.095986888 <a title="311-tfidf-4" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>Author: Gabor Angeli ; Jakob Uszkoreit</p><p>Abstract: Temporal resolution systems are traditionally tuned to a particular language, requiring significant human effort to translate them to new languages. We present a language independent semantic parser for learning the interpretation of temporal phrases given only a corpus of utterances and the times they reference. We make use of a latent parse that encodes a language-flexible representation of time, and extract rich features over both the parse and associated temporal semantics. The parameters of the model are learned using a weakly supervised bootstrapping approach, without the need for manually tuned parameters or any other language expertise. We achieve state-of-the-art accuracy on all languages in the TempEval2 temporal normalization task, reporting a 4% improvement in both English and Spanish accuracy, and to our knowledge the first results for four other languages.</p><p>5 0.092622474 <a title="311-tfidf-5" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>Author: Wenduan Xu ; Yue Zhang ; Philip Williams ; Philipp Koehn</p><p>Abstract: We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.</p><p>6 0.086108506 <a title="311-tfidf-6" href="./acl-2013-The_effect_of_non-tightness_on_Bayesian_estimation_of_PCFGs.html">348 acl-2013-The effect of non-tightness on Bayesian estimation of PCFGs</a></p>
<p>7 0.085914001 <a title="311-tfidf-7" href="./acl-2013-ParaQuery%3A_Making_Sense_of_Paraphrase_Collections.html">271 acl-2013-ParaQuery: Making Sense of Paraphrase Collections</a></p>
<p>8 0.082693666 <a title="311-tfidf-8" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>9 0.079567313 <a title="311-tfidf-9" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>10 0.077331953 <a title="311-tfidf-10" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>11 0.076850027 <a title="311-tfidf-11" href="./acl-2013-Transfer_Learning_for_Constituency-Based_Grammars.html">357 acl-2013-Transfer Learning for Constituency-Based Grammars</a></p>
<p>12 0.076314576 <a title="311-tfidf-12" href="./acl-2013-Predicting_and_Eliciting_Addressee%27s_Emotion_in_Online_Dialogue.html">282 acl-2013-Predicting and Eliciting Addressee's Emotion in Online Dialogue</a></p>
<p>13 0.07394594 <a title="311-tfidf-13" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>14 0.073885612 <a title="311-tfidf-14" href="./acl-2013-Paraphrasing_Adaptation_for_Web_Search_Ranking.html">273 acl-2013-Paraphrasing Adaptation for Web Search Ranking</a></p>
<p>15 0.073326811 <a title="311-tfidf-15" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>16 0.071880229 <a title="311-tfidf-16" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>17 0.07061106 <a title="311-tfidf-17" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<p>18 0.069381647 <a title="311-tfidf-18" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>19 0.063823961 <a title="311-tfidf-19" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>20 0.06234302 <a title="311-tfidf-20" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.183), (1, -0.03), (2, -0.036), (3, -0.025), (4, -0.092), (5, -0.007), (6, 0.069), (7, -0.032), (8, -0.001), (9, 0.019), (10, -0.033), (11, -0.024), (12, 0.028), (13, 0.026), (14, -0.006), (15, -0.092), (16, 0.065), (17, 0.064), (18, -0.023), (19, -0.013), (20, -0.046), (21, -0.081), (22, 0.061), (23, 0.02), (24, -0.038), (25, -0.012), (26, -0.007), (27, 0.053), (28, -0.054), (29, 0.083), (30, 0.021), (31, 0.012), (32, -0.046), (33, 0.029), (34, 0.071), (35, -0.082), (36, 0.024), (37, -0.027), (38, 0.045), (39, 0.036), (40, -0.023), (41, 0.001), (42, 0.024), (43, 0.009), (44, 0.062), (45, 0.049), (46, 0.004), (47, 0.044), (48, -0.077), (49, -0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93020755 <a title="311-lsi-1" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>Author: Chris Quirk ; Pallavi Choudhury</p><p>Abstract: Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. Lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. We advocate hypergraphs as compact representations for sets of utterances describing the same event or object. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy.</p><p>2 0.70933026 <a title="311-lsi-2" href="./acl-2013-Fluid_Construction_Grammar_for_Historical_and_Evolutionary_Linguistics.html">161 acl-2013-Fluid Construction Grammar for Historical and Evolutionary Linguistics</a></p>
<p>Author: Pieter Wellens ; Remi van Trijp ; Katrien Beuls ; Luc Steels</p><p>Abstract: Fluid Construction Grammar (FCG) is an open-source computational grammar formalism that is becoming increasingly popular for studying the history and evolution of language. This demonstration shows how FCG can be used to operationalise the cultural processes and cognitive mechanisms that underly language evolution and change.</p><p>3 0.66275865 <a title="311-lsi-3" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>Author: Matthias Buchse ; Alexander Koller ; Heiko Vogler</p><p>Abstract: Binarization ofgrammars is crucial for improving the complexity and performance of parsing and translation. We present a versatile binarization algorithm that can be tailored to a number of grammar formalisms by simply varying a formal parameter. We apply our algorithm to binarizing tree-to-string transducers used in syntax-based machine translation.</p><p>4 0.6571613 <a title="311-lsi-4" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>Author: Joohyun Kim ; Raymond Mooney</p><p>Abstract: We adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees.</p><p>5 0.65067834 <a title="311-lsi-5" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>Author: David Chiang ; Jacob Andreas ; Daniel Bauer ; Karl Moritz Hermann ; Bevan Jones ; Kevin Knight</p><p>Abstract: Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing.</p><p>6 0.64571255 <a title="311-lsi-6" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>7 0.63643432 <a title="311-lsi-7" href="./acl-2013-The_effect_of_non-tightness_on_Bayesian_estimation_of_PCFGs.html">348 acl-2013-The effect of non-tightness on Bayesian estimation of PCFGs</a></p>
<p>8 0.59553283 <a title="311-lsi-8" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>9 0.57686102 <a title="311-lsi-9" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>10 0.56906754 <a title="311-lsi-10" href="./acl-2013-Implicatures_and_Nested_Beliefs_in_Approximate_Decentralized-POMDPs.html">190 acl-2013-Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs</a></p>
<p>11 0.56322998 <a title="311-lsi-11" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>12 0.56290305 <a title="311-lsi-12" href="./acl-2013-Semantic_Parsing_with_Combinatory_Categorial_Grammars.html">313 acl-2013-Semantic Parsing with Combinatory Categorial Grammars</a></p>
<p>13 0.55916101 <a title="311-lsi-13" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>14 0.55854529 <a title="311-lsi-14" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>15 0.54749745 <a title="311-lsi-15" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>16 0.5456143 <a title="311-lsi-16" href="./acl-2013-ParaQuery%3A_Making_Sense_of_Paraphrase_Collections.html">271 acl-2013-ParaQuery: Making Sense of Paraphrase Collections</a></p>
<p>17 0.54063195 <a title="311-lsi-17" href="./acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees.html">285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</a></p>
<p>18 0.53133291 <a title="311-lsi-18" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>19 0.50148463 <a title="311-lsi-19" href="./acl-2013-Transfer_Learning_for_Constituency-Based_Grammars.html">357 acl-2013-Transfer Learning for Constituency-Based Grammars</a></p>
<p>20 0.50009215 <a title="311-lsi-20" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.038), (6, 0.037), (11, 0.04), (24, 0.053), (26, 0.041), (35, 0.514), (42, 0.041), (48, 0.025), (64, 0.016), (70, 0.025), (88, 0.021), (90, 0.019), (95, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98610353 <a title="311-lda-1" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>2 0.98508948 <a title="311-lda-2" href="./acl-2013-Patient_Experience_in_Online_Support_Forums%3A_Modeling_Interpersonal_Interactions_and_Medication_Use.html">278 acl-2013-Patient Experience in Online Support Forums: Modeling Interpersonal Interactions and Medication Use</a></p>
<p>Author: Annie Chen</p><p>Abstract: Though there has been substantial research concerning the extraction of information from clinical notes, to date there has been less work concerning the extraction of useful information from patient-generated content. Using a dataset comprised of online support group discussion content, this paper investigates two dimensions that may be important in the extraction of patient-generated experiences from text; significant individuals/groups and medication use. With regard to the former, the paper describes an approach involving the pairing of important figures (e.g. family, husbands, doctors, etc.) and affect, and suggests possible applications of such techniques to research concerning online social support, as well as integration into search interfaces for patients. Additionally, the paper demonstrates the extraction of side effects and sentiment at different phases in patient medication use, e.g. adoption, current use, discontinuation and switching, and demonstrates the utility of such an application for drug safety monitoring in online discussion forums. 1</p><p>3 0.97931761 <a title="311-lda-3" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>Author: Ndapandula Nakashole ; Tomasz Tylenda ; Gerhard Weikum</p><p>Abstract: Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and semantically typing newly emerging out-ofKB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE. Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowdsourced user studies, show our method performing significantly better than prior work.</p><p>4 0.97613239 <a title="311-lda-4" href="./acl-2013-Building_and_Evaluating_a_Distributional_Memory_for_Croatian.html">76 acl-2013-Building and Evaluating a Distributional Memory for Croatian</a></p>
<p>Author: Jan Snajder ; Sebastian Pado ; Zeljko Agic</p><p>Abstract: We report on the first structured distributional semantic model for Croatian, DM.HR. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available.</p><p>same-paper 5 0.97243553 <a title="311-lda-5" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>Author: Chris Quirk ; Pallavi Choudhury</p><p>Abstract: Ambiguity preserving representations such as lattices are very useful in a number of NLP tasks, including paraphrase generation, paraphrase recognition, and machine translation evaluation. Lattices compactly represent lexical variation, but word order variation leads to a combinatorial explosion of states. We advocate hypergraphs as compact representations for sets of utterances describing the same event or object. We present a method to construct hypergraphs from sets of utterances, and evaluate this method on a simple recognition task. Given a set of utterances that describe a single object or event, we construct such a hypergraph, and demonstrate that it can recognize novel descriptions of the same event with high accuracy.</p><p>6 0.96384686 <a title="311-lda-6" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>7 0.9612875 <a title="311-lda-7" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>8 0.96055925 <a title="311-lda-8" href="./acl-2013-Discriminative_Approach_to_Fill-in-the-Blank_Quiz_Generation_for_Language_Learners.html">122 acl-2013-Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners</a></p>
<p>9 0.82055187 <a title="311-lda-9" href="./acl-2013-Automatic_Coupling_of_Answer_Extraction_and_Information_Retrieval.html">60 acl-2013-Automatic Coupling of Answer Extraction and Information Retrieval</a></p>
<p>10 0.81151307 <a title="311-lda-10" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>11 0.79606456 <a title="311-lda-11" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>12 0.79379934 <a title="311-lda-12" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>13 0.77676088 <a title="311-lda-13" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>14 0.7713058 <a title="311-lda-14" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>15 0.7662605 <a title="311-lda-15" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>16 0.74996781 <a title="311-lda-16" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>17 0.74572641 <a title="311-lda-17" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>18 0.74555522 <a title="311-lda-18" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>19 0.74219066 <a title="311-lda-19" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>20 0.74015212 <a title="311-lda-20" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
