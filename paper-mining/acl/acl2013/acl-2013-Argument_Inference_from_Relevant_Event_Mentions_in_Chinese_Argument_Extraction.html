<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-56" href="#">acl2013-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</h1>
<br/><p>Source: <a title="acl-2013-56-pdf" href="http://aclweb.org/anthology//P/P13/P13-1145.pdf">pdf</a></p><p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>Reference: <a title="acl-2013-56-reference" href="../acl2013_reference/acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn ,  Abstract As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. [sent-3, score-0.521]
</p><p>2 Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. [sent-5, score-0.394]
</p><p>3 1  Introduction  The task of event extraction is to recognize event mentions of a predefined event type and their arguments (participants and attributes). [sent-6, score-2.285]
</p><p>4 Generally, it can be divided into two subtasks: trigger extraction, which aims to identify trigger/event mentions and determine their event type, and argument extraction, which aims to extract various arguments of a specific event and assign the roles to them. [sent-7, score-2.376]
</p><p>5 In this paper, we focus on argument extraction in Chinese event extraction. [sent-8, score-0.914]
</p><p>6 While most of previous studies in Chinese event extraction deal with Chinese trigger extraction (e. [sent-9, score-1.145]
</p><p>7 , 2012a, 2012b), there are only a few on Chinese argument extraction (e. [sent-13, score-0.346]
</p><p>8 Following previous studies, we divide argument extraction into two components, argument identification and role determination, where the former recognizes the arguments in a specific event mention and the latter classifies these arguments by roles. [sent-17, score-1.989]
</p><p>9 With regard to methodology, most of previous studies on argument extraction recast it as a Semantic Role Labeling (SRL) task and focus on intra-sentence information to identify the arguments and their roles. [sent-18, score-0.705]
</p><p>10 Therefore, some arguments of a specific event mention are far away from the trigger and how to recover those inter-sentence arguments becomes a challenging issue in Chinese argument extraction. [sent-22, score-1.966]
</p><p>11 0823 In above discourse, there are three event mentions, one kill (E1) and two Attack (E2, E3). [sent-27, score-0.568]
</p><p>12 While it is relatively easy to identify 20 号清晨 (morning of 20th), 加沙走廊 (Gaza Strip) and 炸 弹 (bomb) as the Time, Place and Instrument roles in E2 by a sentence-based argument 1477  Proce dingsS o f ita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-28, score-0.359]
</p><p>13 Similarly, it is hard to recognize 两名 以色列人 (two Israelites) as the Target role for event mention E2 and identify 炸 弹 (bomb) as the Instrument role for event  mention E1. [sent-31, score-1.711]
</p><p>14 An alternative way is to employ various relationships among relevant event mentions in a discourse to infer those intersentence arguments. [sent-32, score-1.08]
</p><p>15 The contributions of this paper are: 1) We propose a novel global argument inference model, in which various kinds of event relations are involved to infer more arguments on their semantic relations. [sent-33, score-1.318]
</p><p>16 (201 1), which only consider document-level consistency, we propose a more fine-gained consistency model to enforce the consistency in the sentence, discourse and document layers. [sent-35, score-0.32]
</p><p>17 3) We incorporate argument semantics into our global argument inference model to unify the semantics of the event and its arguments. [sent-36, score-1.301]
</p><p>18 Section 3 describes a state-of-the-art Chinese argument extraction system as the baseline. [sent-39, score-0.346]
</p><p>19 2  Related Work  Almost all the existing studies on argument extraction concern English. [sent-43, score-0.38]
</p><p>20 In comparison, there are only a few studies exploring inter-sentence information or argument semantics (e. [sent-50, score-0.344]
</p><p>21 Compared with the tremendous work on English event extraction, there are only a few studies (e. [sent-54, score-0.602]
</p><p>22 , 2012) on Chinese event extraction with focus on either feature engineering or trigger expansion, under the same framework as English trigger identification. [sent-60, score-1.459]
</p><p>23 In additional, there are only very  few of them focusing on Chinese argument extraction and almost all aim to feature engineering and are based on sentence-level information and recast this task as an SRL-style task. [sent-61, score-0.376]
</p><p>24 (2008) introduce multiple levels of patterns to improve the coverage in Chinese argument classification. [sent-63, score-0.281]
</p><p>25 Chen and Ji (2009b) apply various kinds of lexical, syntactic and semantic features to address the special issues in Chinese argument extraction. [sent-64, score-0.281]
</p><p>26 (2010) use a feature weighting scheme to re-weight various features for Chinese argument extraction. [sent-66, score-0.281]
</p><p>27 Specially, several studies have successfully incorporated cross-document or document-level information and argument semantics into event extraction, most of them focused on English. [sent-69, score-0.912]
</p><p>28 (2007) apply a crossdocument inference mechanism to refine local extraction results for the disease name, location and start/end time. [sent-71, score-0.124]
</p><p>29 Mann (2007) proposes some constraints on relationship rescoring to impose the discourse consistency on the CEO’s personal information. [sent-72, score-0.202]
</p><p>30 Chambers and Jurafsky (2008)  propose a narrative event chain which are partially ordered sets of event mentions centered around a common protagonist and this chain can represent the relationship among the relevant event mentions in a document. [sent-73, score-2.229]
</p><p>31 Ji and Grishman (2008) employ a rule-based approach to propagate consistent triggers and arguments across topic-related documents. [sent-74, score-0.314]
</p><p>32 Liao and Grishman (2010) mainly focus on employing the cross-event consistency information to improve sentence-level trigger extraction and they also propose an inference method to infer the arguments following role consistency in a document. [sent-75, score-1.062]
</p><p>33 (201 1) employ the background information to divide an entity type into more cohesive subtypes to create the bridge between two entities and then infer arguments and their roles using cross-entity inference on the subtypes of entities. [sent-77, score-0.625]
</p><p>34 Huang and Rillof (2012) propose a sequentially structured sentence classifier which uses lexical associations and discourse relations across sentences to identify event-related document contexts and then apply it to recognize arguments and their roles on the relation among triggers and arguments. [sent-78, score-0.742]
</p><p>35 1478  3  Baseline  In the task of event extraction as defined in ACE evaluations, an event is defined as a specific occurrence involving participants (e. [sent-79, score-1.249]
</p><p>36 Commonly, an event mention is triggered via a word (trigger) in a phrase or sentence which clearly expresses the occurrence of a specific event. [sent-84, score-0.815]
</p><p>37 The arguments are the entity mentions involved in an event mention with a specific role, the relation of an argument to an event where it participates. [sent-85, score-2.192]
</p><p>38 Hence, extracting an event consists of four basic steps, identifying an event trigger, determining its event type, identifying involved arguments (participants and attributes) and determining their roles. [sent-86, score-1.976]
</p><p>39 As the baseline, we choose a state-of-the-art Chinese event extraction system, as described in Li et al. [sent-87, score-0.633]
</p><p>40 (2012b), which consists of four typical components: trigger identification, event type determination, argument identification and role determination. [sent-88, score-1.37]
</p><p>41 In their system, the former two components, trigger identification and event type  determination, are processed in a joint model, where the latter two components are run in a pipeline way. [sent-89, score-1.06]
</p><p>42 This paper focuses on argument identification and role determination. [sent-91, score-0.358]
</p><p>43 , 2010), sememe of trigger in Hownet (Dong and Dong, 2006). [sent-98, score-0.413]
</p><p>44 4 Inferring Inter-Sentence Arguments on Relevant Event Mentions In this paper, a global argument inference model is proposed to infer those inter-sentence arguments and their roles, incorporating with semantic relations between relevant event mention pairs and argument semantics. [sent-99, score-1.844]
</p><p>45 1  Motivation  It’s well-known that Chinese is a paratactic language, with an open flexible sentence structure and often omits the subject or the object, while English is a hypotactic language with a strict sentence structure and emphasizes on cohesion between clauses. [sent-101, score-0.18]
</p><p>46 Hence, there are two issues in Chinese argument extraction, associated with its nature of the paratactic language. [sent-102, score-0.372]
</p><p>47 The first is that many arguments of an event  mention are out of the event mention scope since ellipsis is a common phenomenon in Chinese. [sent-103, score-1.79]
</p><p>48 Table 1 gives the statistics of intrasentence and inter-sentence arguments in the ACE 2005 Chinese corpus and it shows that 20. [sent-105, score-0.218]
</p><p>49 8% of the arguments are inter-sentence ones while this figure is less than 1% of the ACE 2005 English corpus. [sent-106, score-0.218]
</p><p>50 The main reason of that difference is that some Chinese arguments are omitted in the same sentence of the trigger since Chinese is a paratactic language with the wide spread of ellipsis. [sent-107, score-0.751]
</p><p>51 We detect sentence boundaries, relying on both full stop and comma signs, since in a Chinese document, comma can be also used to sign the end of a sentence. [sent-110, score-0.123]
</p><p>52 For example, a Die event mention “Three person died in this accident. [sent-118, score-0.761]
</p><p>53 In a word, above two issues indicate that syntactic feature-based approaches are limited in identifying Chinese arguments and it will lead to low recall in argument identification. [sent-120, score-0.526]
</p><p>54 Therefore, employing those high level information to  capture the semantic relation, not only the syntactic structure, between the trigger and its long distance arguments is the key to improve the performance of the Chinese argument identification. [sent-121, score-0.935]
</p><p>55 An alternative way is to link the different event mentions with their predicates (triggers) and use the trigger as a bridge to connect the arguments to the trigger in another event mention indirectly. [sent-123, score-2.627]
</p><p>56 Hence, the semantic relations among event mentions are helpful to be a bridge to identify those inter-sentence arguments. [sent-124, score-0.931]
</p><p>57 2  Relations of Event Mention Pairs  In a discourse, most event mentions are surrounding a specific topic. [sent-126, score-0.818]
</p><p>58 It’s obvious that those mentions have the intrinsic relationships to reveal the essential structure of a discourse. [sent-127, score-0.225]
</p><p>59 Those relevant semantics-based relations are helpful to infer the arguments for a specific trigger mention when the syntactic relations in Chinese argument extraction are not as effective as that in English. [sent-128, score-1.451]
</p><p>60 In this paper, we divide the  relations among relevant event mentions into three categories: Coreference, Sequence and Parallel. [sent-129, score-0.951]
</p><p>61 An event may have more than one mention in a document and coreference event mentions refer to the same event, as same as the definition in the ACE evaluations. [sent-130, score-1.668]
</p><p>62 Those coreference event mentions always have the same arguments and roles. [sent-131, score-1.087]
</p><p>63 Therefore, employing this relation can infer the arguments of an event mention from their Coreference ones. [sent-132, score-1.127]
</p><p>64 For example, we can recover the Time, Place and Instrument for E3 via its Coreference mention E2 in discourse D1, mentioned in Section 1. [sent-133, score-0.365]
</p><p>65 (2012a) find out that sometimes two trigger mentions are within a Chinese word whose morphological structure is Coordination. [sent-135, score-0.638]
</p><p>66 0005 In D2, 刺 死 (stab a person to death) is a trigger with the Coordination structure and can  死死  be divided into two single-morpheme words 刺 (stab) and 死 (die) while the former triggers an Attack event and the latter refers to a Die one. [sent-139, score-1.1]
</p><p>67 It’s interesting that they share all arguments in this sentence. [sent-140, score-0.218]
</p><p>68 The relation between those event mentions whose triggers merge a Chinese word or share the subject and the object are Parallel. [sent-141, score-0.942]
</p><p>69 For the errors in the syntactic parsing, the second single-morpheme trigger is often assigned a wrong tag (e. [sent-142, score-0.413]
</p><p>70 , NN, JJ) and this leads to the errors in the argument extraction. [sent-144, score-0.281]
</p><p>71 Therefore, inferring the arguments of the second singlemorpheme trigger from that of the first one based on Parallel relation is also an available way to recover arguments. [sent-145, score-0.764]
</p><p>72 Like that the topic is an axis in a discourse, the relations among those relevant event mentions with the different types is the bone to link them into a narration. [sent-146, score-0.911]
</p><p>73 There are a few studies on using the event relations in NLP (e. [sent-147, score-0.668]
</p><p>74 , 2006), learning narrative event chains (Chambers and Jurafsky, 2007)) to ensure its effectiveness. [sent-150, score-0.591]
</p><p>75 In this paper, we define two types of Sequence relations of relevant event mentions: Cause and Temporal for their high probabilities  of sharing arguments. [sent-151, score-0.709]
</p><p>76 The Cause relation between the event mentions are similar to that in the Penn Discourse TreeBank 2. [sent-152, score-0.846]
</p><p>77 For example, an Attack event often is the cause of an Die or Injure event. [sent-155, score-0.568]
</p><p>78 Our Temporal relation is limited to those mentions with the same or relevant event types (e. [sent-156, score-0.898]
</p><p>79 Take the following discourse as a sample:  这批战俘离离开开(E6)阿尔及利亚西部城市廷 杜夫前前往往(E7)摩洛哥西南部城市阿加迪尔。 D3:  (These prisoners left (E6) Tindouf, a western city of Algeria, and went (E7) to Agadir, a southwestern city of Morocco. [sent-159, score-0.122]
</p><p>80 0158 In D3, there are two Transport mentions and it is natural to infer 阿 加 迪 尔 (Agadir) as the Destination role of E6 and 廷杜夫 (Tindouf) as the Origin role of E7 via their Sequence relation. [sent-162, score-0.401]
</p><p>81 We try to achieve a higher accuracy in this stage so that our argument inference can recover more true arguments. [sent-168, score-0.39]
</p><p>82 These constraints are enlightened by the fact that only Chinese words with Coordination structure can be divided into two new words and each word can trigger an  event with the different event type 2 . [sent-171, score-1.58]
</p><p>83 The Coreference relation is divided into two types: Noun-based Coreference (NC) and Eventbased Coreference (EC) while the former always uses a verbal noun to refer to an event mentioned in current or previous sentence and the latter is that an event is mentioned twice or more actually. [sent-173, score-1.241]
</p><p>84 For example, the relation between E2 and E3 in D1 is NC while the trigger of E3 is only a verbal noun without any direct arguments and it refers to E2. [sent-174, score-0.684]
</p><p>85 We adopt a simple rule to recognize those NC relations: for each event mention whose trigger is a noun and doesn’t act as the subject/object, we regard their relation as NC if there is another event mention with the same trigger in current or previous sentence. [sent-175, score-2.477]
</p><p>86 Inspired by Ahn (2006), we use the following conditions to infer the EC relations between two event mentions with the same event type: 1) Their trigger mentions refer to the same trigger; 2) They have at least one same or similar  1 It acts as the governing semantic element in a Chinese word. [sent-176, score-2.137]
</p><p>87 2 If they have the same event type, they will be regarded a single event mention. [sent-177, score-1.136]
</p><p>88 as  subject/object; 3) The score of cosine similarity of two event mentions is more than a threshold3. [sent-178, score-0.793]
</p><p>89 Finally, for the Sequence relation, instead of identifying and classifying the relations clearly and correctly, our goal is to identify whether there are relevant event mentions in a long sentence or two adjacent short sentences who share arguments. [sent-179, score-1.033]
</p><p>90 Besides, ShareArg(mpi)is used to identify whether the event mention pair mpi sharing at least one argument. [sent-181, score-0.885]
</p><p>91 In this algorithm, since the relations on the event types are too coarse, we introduce a more fine-gained Sequence relation both on the event types and the head morphemes of the triggers which can divide an event type into many subtypes on the head morpheme. [sent-182, score-2.152]
</p><p>92 Li and Zhou (2012) have ensured the effectiveness of using head morpheme to infer the triggers and our experiment results also show it is helpful for  identifying relevant event mentions which aims to the higher accuracy. [sent-183, score-1.12]
</p><p>93 4  Global Argument Inference Model  Our global argument inference model is composed of two steps: 1) training two sentencebased classifiers: argument identifier (AI) and role determiner (RD) that estimate the score of a candidate acts as an argument and belongs to a 3 The threshold is tuned to 0. [sent-185, score-1.008]
</p><p>94 2) Using the scores of two classifiers and the event relations in a sentence, a discourse or a document, we perform global optimization to infer those missing or long distance arguments and their roles. [sent-188, score-1.1]
</p><p>95 To incorporate those event relations with our global argument inference model, we regard a document as a tree and divide it into three layers: document, discourse and sentence. [sent-189, score-1.262]
</p><p>96 A document is composed of a set of the discourses while a discourse contains three sentences. [sent-190, score-0.16]
</p><p>97 We incorporate different event relations into our model on the different layer and the goal of our global argument inference model is to achieve the maximized scores over a document on its three layers and two classifiers: AI and RD. [sent-192, score-1.096]
</p><p>98 5  Incorporating Argument Semantics into Global Argument Inference Model  We also introduce the argument semantics, which represent the semantic relations of argument-argument pair, argument-role pair and argument-trigger pair, to reflect the cohesion inside an event. [sent-197, score-0.378]
</p><p>99 (201 1) found out that there is a strong argument and role consistency in the ACE 2005 English corpus. [sent-199, score-0.413]
</p><p>100 Those consistencies also occur in Chinese and they reveal the relation between the trigger and its arguments, and also explore the relation between the argument and its role. [sent-200, score-0.8]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('event', 0.568), ('trigger', 0.413), ('argument', 0.281), ('mentions', 0.225), ('arguments', 0.218), ('mention', 0.193), ('chinese', 0.176), ('discourse', 0.122), ('tri', 0.109), ('ace', 0.101), ('triggers', 0.096), ('paratactic', 0.091), ('hm', 0.082), ('consistency', 0.08), ('coreference', 0.076), ('infer', 0.072), ('relations', 0.066), ('extraction', 0.065), ('entity', 0.061), ('inference', 0.059), ('mpi', 0.058), ('attack', 0.057), ('global', 0.054), ('grishman', 0.054), ('relation', 0.053), ('ji', 0.052), ('relevant', 0.052), ('role', 0.052), ('ellipsis', 0.05), ('liao', 0.05), ('recover', 0.05), ('die', 0.049), ('morph', 0.047), ('comma', 0.047), ('bomb', 0.045), ('identify', 0.043), ('neighbour', 0.043), ('head', 0.043), ('srl', 0.042), ('recognize', 0.042), ('riloff', 0.042), ('instrument', 0.042), ('agadir', 0.041), ('findallmp', 0.041), ('intersentence', 0.041), ('rillof', 0.041), ('sharearg', 0.041), ('stab', 0.041), ('tindouf', 0.041), ('nc', 0.041), ('subtypes', 0.04), ('li', 0.04), ('divide', 0.04), ('determination', 0.038), ('document', 0.038), ('morpheme', 0.037), ('neighbouring', 0.036), ('morphemes', 0.036), ('roles', 0.035), ('ahn', 0.035), ('hong', 0.035), ('regard', 0.034), ('studies', 0.034), ('transport', 0.034), ('gaza', 0.034), ('coordination', 0.033), ('type', 0.031), ('cohesion', 0.031), ('chambers', 0.031), ('besides', 0.031), ('sequence', 0.03), ('inferring', 0.03), ('layers', 0.03), ('tan', 0.03), ('recast', 0.03), ('morning', 0.03), ('az', 0.03), ('semantics', 0.029), ('bridge', 0.029), ('strip', 0.029), ('sentence', 0.029), ('zhou', 0.029), ('jurafsky', 0.028), ('identifying', 0.027), ('fd', 0.026), ('path', 0.026), ('ii', 0.025), ('specific', 0.025), ('mp', 0.025), ('chen', 0.025), ('identification', 0.025), ('death', 0.024), ('narrative', 0.023), ('former', 0.023), ('employing', 0.023), ('sharing', 0.023), ('place', 0.023), ('adjacent', 0.023), ('participants', 0.023), ('boundaries', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="56-tfidf-1" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>2 0.63796324 <a title="56-tfidf-2" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>Author: Qi Li ; Heng Ji ; Liang Huang</p><p>Abstract: Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers. By contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved. In addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents.</p><p>3 0.29758701 <a title="56-tfidf-3" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>Author: Goran Glavas ; Jan Snajder</p><p>Abstract: Identifying news stories that discuss the same real-world events is important for news tracking and retrieval. Most existing approaches rely on the traditional vector space model. We propose an approach for recognizing identical real-world events based on a structured, event-oriented document representation. We structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs. Our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events.</p><p>4 0.25489706 <a title="56-tfidf-4" href="./acl-2013-Extracting_Events_with_Informal_Temporal_References_in_Personal_Histories_in_Online_Communities.html">153 acl-2013-Extracting Events with Informal Temporal References in Personal Histories in Online Communities</a></p>
<p>Author: Miaomiao Wen ; Zeyu Zheng ; Hyeju Jang ; Guang Xiang ; Carolyn Penstein Rose</p><p>Abstract: We present a system for extracting the dates of illness events (year and month of the event occurrence) from posting histories in the context of an online medical support community. A temporal tagger retrieves and normalizes dates mentioned informally in social media to actual month and year referents. Building on this, an event date extraction system learns to integrate the likelihood of candidate dates extracted from time-rich sentences with temporal constraints extracted from eventrelated sentences. Our integrated model achieves 89.7% of the maximum performance given the performance of the temporal expression retrieval step.</p><p>5 0.20345193 <a title="56-tfidf-5" href="./acl-2013-What_causes_a_causal_relation%3F_Detecting_Causal_Triggers_in_Biomedical_Scientific_Discourse.html">386 acl-2013-What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse</a></p>
<p>Author: Claudiu Mihaila ; Sophia Ananiadou</p><p>Abstract: Current domain-specific information extraction systems represent an important resource for biomedical researchers, who need to process vaster amounts of knowledge in short times. Automatic discourse causality recognition can further improve their workload by suggesting possible causal connections and aiding in the curation of pathway models. We here describe an approach to the automatic identification of discourse causality triggers in the biomedical domain using machine learning. We create several baselines and experiment with various parameter settings for three algorithms, i.e., Conditional Random Fields (CRF), Support Vector Machines (SVM) and Random Forests (RF). Also, we evaluate the impact of lexical, syntactic and semantic features on each of the algorithms and look at er- rors. The best performance of 79.35% F-score is achieved by CRFs when using all three feature types.</p><p>6 0.18915121 <a title="56-tfidf-6" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>7 0.17512943 <a title="56-tfidf-7" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>8 0.17183056 <a title="56-tfidf-8" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>9 0.16978161 <a title="56-tfidf-9" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>10 0.16324717 <a title="56-tfidf-10" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>11 0.15660112 <a title="56-tfidf-11" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>12 0.14049685 <a title="56-tfidf-12" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>13 0.13891743 <a title="56-tfidf-13" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>14 0.13353555 <a title="56-tfidf-14" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>15 0.12827027 <a title="56-tfidf-15" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>16 0.12411423 <a title="56-tfidf-16" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>17 0.12017834 <a title="56-tfidf-17" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>18 0.11989705 <a title="56-tfidf-18" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>19 0.11983544 <a title="56-tfidf-19" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>20 0.10863641 <a title="56-tfidf-20" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.209), (1, 0.064), (2, -0.142), (3, -0.172), (4, 0.052), (5, 0.451), (6, -0.061), (7, 0.318), (8, -0.048), (9, 0.165), (10, 0.092), (11, -0.101), (12, 0.06), (13, 0.088), (14, 0.113), (15, -0.067), (16, -0.062), (17, -0.123), (18, 0.122), (19, 0.044), (20, 0.052), (21, -0.167), (22, -0.008), (23, -0.037), (24, -0.001), (25, -0.129), (26, -0.042), (27, -0.031), (28, 0.007), (29, -0.066), (30, -0.021), (31, -0.053), (32, -0.128), (33, 0.085), (34, -0.041), (35, 0.033), (36, 0.043), (37, 0.138), (38, 0.033), (39, -0.086), (40, 0.059), (41, 0.016), (42, -0.057), (43, -0.036), (44, -0.007), (45, -0.073), (46, 0.022), (47, 0.017), (48, -0.068), (49, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98565924 <a title="56-lsi-1" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>2 0.92612457 <a title="56-lsi-2" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>Author: Qi Li ; Heng Ji ; Liang Huang</p><p>Abstract: Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers. By contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved. In addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents.</p><p>3 0.68948758 <a title="56-lsi-3" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>Author: Goran Glavas ; Jan Snajder</p><p>Abstract: Identifying news stories that discuss the same real-world events is important for news tracking and retrieval. Most existing approaches rely on the traditional vector space model. We propose an approach for recognizing identical real-world events based on a structured, event-oriented document representation. We structure documents as graphs of event mentions and use graph kernels to measure the similarity between document pairs. Our experiments indicate that the proposed graph-based approach can outperform the traditional vector space model, and is especially suitable for distinguishing between topically similar, yet non-identical events.</p><p>4 0.63182676 <a title="56-lsi-4" href="./acl-2013-Extracting_Events_with_Informal_Temporal_References_in_Personal_Histories_in_Online_Communities.html">153 acl-2013-Extracting Events with Informal Temporal References in Personal Histories in Online Communities</a></p>
<p>Author: Miaomiao Wen ; Zeyu Zheng ; Hyeju Jang ; Guang Xiang ; Carolyn Penstein Rose</p><p>Abstract: We present a system for extracting the dates of illness events (year and month of the event occurrence) from posting histories in the context of an online medical support community. A temporal tagger retrieves and normalizes dates mentioned informally in social media to actual month and year referents. Building on this, an event date extraction system learns to integrate the likelihood of candidate dates extracted from time-rich sentences with temporal constraints extracted from eventrelated sentences. Our integrated model achieves 89.7% of the maximum performance given the performance of the temporal expression retrieval step.</p><p>5 0.58658224 <a title="56-lsi-5" href="./acl-2013-What_causes_a_causal_relation%3F_Detecting_Causal_Triggers_in_Biomedical_Scientific_Discourse.html">386 acl-2013-What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse</a></p>
<p>Author: Claudiu Mihaila ; Sophia Ananiadou</p><p>Abstract: Current domain-specific information extraction systems represent an important resource for biomedical researchers, who need to process vaster amounts of knowledge in short times. Automatic discourse causality recognition can further improve their workload by suggesting possible causal connections and aiding in the curation of pathway models. We here describe an approach to the automatic identification of discourse causality triggers in the biomedical domain using machine learning. We create several baselines and experiment with various parameter settings for three algorithms, i.e., Conditional Random Fields (CRF), Support Vector Machines (SVM) and Random Forests (RF). Also, we evaluate the impact of lexical, syntactic and semantic features on each of the algorithms and look at er- rors. The best performance of 79.35% F-score is achieved by CRFs when using all three feature types.</p><p>6 0.53583151 <a title="56-lsi-6" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>7 0.52836138 <a title="56-lsi-7" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>8 0.49374554 <a title="56-lsi-8" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>9 0.47253162 <a title="56-lsi-9" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<p>10 0.45688152 <a title="56-lsi-10" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>11 0.41659167 <a title="56-lsi-11" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>12 0.39899608 <a title="56-lsi-12" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>13 0.36381388 <a title="56-lsi-13" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>14 0.34083852 <a title="56-lsi-14" href="./acl-2013-Temporal_Signals_Help_Label_Temporal_Relations.html">339 acl-2013-Temporal Signals Help Label Temporal Relations</a></p>
<p>15 0.32322401 <a title="56-lsi-15" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>16 0.32164663 <a title="56-lsi-16" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>17 0.30784547 <a title="56-lsi-17" href="./acl-2013-Why-Question_Answering_using_Intra-_and_Inter-Sentential_Causal_Relations.html">387 acl-2013-Why-Question Answering using Intra- and Inter-Sentential Causal Relations</a></p>
<p>18 0.30675641 <a title="56-lsi-18" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>19 0.30114272 <a title="56-lsi-19" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>20 0.29831922 <a title="56-lsi-20" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.057), (6, 0.02), (11, 0.081), (15, 0.014), (24, 0.068), (26, 0.029), (29, 0.119), (35, 0.074), (42, 0.199), (48, 0.052), (70, 0.061), (88, 0.054), (90, 0.037), (95, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91438961 <a title="56-lda-1" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>2 0.89473599 <a title="56-lda-2" href="./acl-2013-Modeling_of_term-distance_and_term-occurrence_information_for_improving_n-gram_language_model_performance.html">247 acl-2013-Modeling of term-distance and term-occurrence information for improving n-gram language model performance</a></p>
<p>Author: Tze Yuang Chong ; Rafael E. Banchs ; Eng Siong Chng ; Haizhou Li</p><p>Abstract: In this paper, we explore the use of distance and co-occurrence information of word-pairs for language modeling. We attempt to extract this information from history-contexts of up to ten words in size, and found it complements well the n-gram model, which inherently suffers from data scarcity in learning long history-contexts. Evaluated on the WSJ corpus, bigram and trigram model perplexity were reduced up to 23.5% and 14.0%, respectively. Compared to the distant bigram, we show that word-pairs can be more effectively modeled in terms of both distance and occurrence. 1</p><p>3 0.86625105 <a title="56-lda-3" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<p>Author: Valia Kordoni ; Markus Egg</p><p>Abstract: unkown-abstract</p><p>4 0.8634755 <a title="56-lda-4" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>Author: Qi Li ; Heng Ji ; Liang Huang</p><p>Abstract: Traditional approaches to the task of ACE event extraction usually rely on sequential pipelines with multiple stages, which suffer from error propagation since event triggers and arguments are predicted in isolation by independent local classifiers. By contrast, we propose a joint framework based on structured prediction which extracts triggers and arguments together so that the local predictions can be mutually improved. In addition, we propose to incorporate global features which explicitly capture the dependencies of multiple triggers and arguments. Experimental results show that our joint approach with local features outperforms the pipelined baseline, and adding global features further improves the performance significantly. Our approach advances state-ofthe-art sentence-level event extraction, and even outperforms previous argument labeling methods which use external knowledge from other sentences and documents.</p><p>5 0.86278409 <a title="56-lda-5" href="./acl-2013-Propminer%3A_A_Workflow_for_Interactive_Information_Extraction_and_Exploration_using_Dependency_Trees.html">285 acl-2013-Propminer: A Workflow for Interactive Information Extraction and Exploration using Dependency Trees</a></p>
<p>Author: Alan Akbik ; Oresti Konomi ; Michail Melnikov</p><p>Abstract: The use ofdeep syntactic information such as typed dependencies has been shown to be very effective in Information Extraction. Despite this potential, the process of manually creating rule-based information extractors that operate on dependency trees is not intuitive for persons without an extensive NLP background. In this system demonstration, we present a tool and a workflow designed to enable initiate users to interactively explore the effect and expressivity of creating Information Extraction rules over dependency trees. We introduce the proposed five step workflow for creating information extractors, the graph query based rule language, as well as the core features of the PROP- MINER tool.</p><p>6 0.85833168 <a title="56-lda-6" href="./acl-2013-Automatically_Predicting_Sentence_Translation_Difficulty.html">64 acl-2013-Automatically Predicting Sentence Translation Difficulty</a></p>
<p>7 0.84389037 <a title="56-lda-7" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>8 0.84202248 <a title="56-lda-8" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>9 0.83495653 <a title="56-lda-9" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>10 0.83124554 <a title="56-lda-10" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>11 0.82423425 <a title="56-lda-11" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>12 0.82341808 <a title="56-lda-12" href="./acl-2013-Using_CCG_categories_to_improve_Hindi_dependency_parsing.html">372 acl-2013-Using CCG categories to improve Hindi dependency parsing</a></p>
<p>13 0.81498778 <a title="56-lda-13" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>14 0.81476897 <a title="56-lda-14" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>15 0.81065542 <a title="56-lda-15" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>16 0.80477428 <a title="56-lda-16" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>17 0.80351579 <a title="56-lda-17" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>18 0.80264807 <a title="56-lda-18" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>19 0.80032599 <a title="56-lda-19" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>20 0.79357785 <a title="56-lda-20" href="./acl-2013-Bilingual_Lexical_Cohesion_Trigger_Model_for_Document-Level_Machine_Translation.html">69 acl-2013-Bilingual Lexical Cohesion Trigger Model for Document-Level Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
