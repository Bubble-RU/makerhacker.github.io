<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-326" href="#">acl2013-326</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</h1>
<br/><p>Source: <a title="acl-2013-326-pdf" href="http://aclweb.org/anthology//P/P13/P13-1155.pdf">pdf</a></p><p>Author: Hany Hassan ; Arul Menezes</p><p>Abstract: We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.</p><p>Reference: <a title="acl-2013-326-reference" href="../acl2013_reference/acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com Abstract We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. [sent-2, score-1.439]
</p><p>2 The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. [sent-3, score-0.97]
</p><p>3 The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. [sent-4, score-0.551]
</p><p>4 When used as  a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. [sent-8, score-0.492]
</p><p>5 The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text. [sent-9, score-0.48]
</p><p>6 1 Introduction Social Media text is usually very noisy and contains a lot of typos, ad-hoc abbreviations, phonetic substitutions, customized abbreviations and slang language. [sent-10, score-0.449]
</p><p>7 Natural language processing and understanding systems such as Machine Translation, Information Extraction and Text-to-Speech are usually trained and optimized for clean data; therefore such systems would face a challenging problem with social media text. [sent-12, score-0.393]
</p><p>8 It is crucial to have a solution for text normalization that can adapt to such variations automatically. [sent-26, score-0.663]
</p><p>9 We propose a text normalization approach using an unsupervised method to induce normalization equivalences from noisy data which can adapt to any genre of social media. [sent-27, score-1.961]
</p><p>10 In this paper, we focus on providing a solution for social media text normalization as a preprocessing step for NLP applications. [sent-28, score-1.031]
</p><p>11 Ac s2s0o1ci3a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 157 –1586, appropriate normalization depending on the context and on the domain. [sent-36, score-0.656]
</p><p>12 Third, text normalization as a preprocessing step should have very high precision; in other words, it should provide conservative and confident normalization and not overcorrect. [sent-37, score-1.414]
</p><p>13 Moreover, the text normalization should have high recall, as well, to have a good impact on the NLP applications. [sent-38, score-0.663]
</p><p>14 In this paper, we introduce a social media text normalization system which addresses the challenges mentioned above. [sent-39, score-0.995]
</p><p>15 The proposed system is based on constructing a lattice from possible normalization candidates and finding the best normalization sequence according to an n-gram language model using a Viterbi decoder. [sent-40, score-1.447]
</p><p>16 We propose an unsupervised approach to learn the normalization candidates from unlabeled text data. [sent-41, score-0.807]
</p><p>17 We evaluate the approach on the normalization task as well as machine translation task. [sent-44, score-0.671]
</p><p>18 2  Related Work  Early work handled the text normalization problem as a noisy channel model where the normalized words go through a noisy channel to produce the noisy text. [sent-46, score-1.778]
</p><p>19 (Brill and Moore, 2000) introduced an approach for modeling the spelling errors as a noisy channel model based on string to string  edits. [sent-47, score-0.56]
</p><p>20 , 2007) introduced a supervised HMM channel model for text normalization which has been expanded by (Cook and Stevenson, 2009) to introduce unsupervised noisy channel model using probabilistic models for common abbreviation and various spelling errors types. [sent-51, score-1.228]
</p><p>21 Some researchers used Statistical Machine Translation approach for text normalization; formalizing the problem as a translation from the noisy forms to the normalized forms. [sent-52, score-0.495]
</p><p>22 The main drawback of these approaches is that the noisy channel model cannot accurately represent the errors types without contextual information. [sent-55, score-0.44]
</p><p>23 More recent approaches tried to handle the text  normalization problem using normalization lexicons which map the noisy form of the word to a normalized form. [sent-56, score-1.773]
</p><p>24 , 2011) proposed an approach using a classifier to identify the noisy words candidate for normalization; then using some rules to generate lexical variants and a small normalization lexicon. [sent-58, score-0.981]
</p><p>25 , 2011) proposed an approach using an impoverished normalization lexicon based on string and distributional similarity along with a dictionary lookup approach to detect noisy words. [sent-60, score-1.227]
</p><p>26 , 2012) introduced a similar approach by generating a normalization lexicon based on distributional similarity and string similarity. [sent-62, score-0.891]
</p><p>27 This approach uses pairwise similarity where any two words that share the same context are considered as normalization equivalences. [sent-63, score-0.815]
</p><p>28 First, it does not take into account the relative frequencies of the normalization equivalences that might share different contexts. [sent-65, score-0.872]
</p><p>29 Therefore, the selection of the normalization equivalences is performed on pairwise basis only and is not optimized over the  whole data. [sent-66, score-0.889]
</p><p>30 Secondly, the normalization equivalences must appear in the exact same context to be considered as a normalization candidate. [sent-67, score-1.485]
</p><p>31 Our approach also adopts a lexicon based approach for text normalization, we construct a lattice from possible normalization candidates and find the best normalization sequence according to an n-gram language model using a Viterbi decoder. [sent-69, score-1.57]
</p><p>32 The normalization lexicon is acquired from unlabeled data using random walks on a contextual similarity graph constructed form n-gram sequences on large unlabeled text corpus. [sent-70, score-1.518]
</p><p>33 However, our approach is significantly different since we acquire the lexicon using random walks on a contextual similarity graph which has a number of advantages  over the pairwise similarity approach used in (Han et al. [sent-73, score-0.796]
</p><p>34 Namely, the acquired normalization equivalence are optimized globally over the whole data, the rare equivalences are not considered as good candidates unless there is a strong statistical evidence across the data, and finally the normalization equivalences may not share the same context. [sent-75, score-1.769]
</p><p>35 3  Text Normalization System  In this paper, we handle text normalization as a lattice scoring approach, where the translation is performed from noisy text as the source side to the normalized text as the target side. [sent-78, score-1.292]
</p><p>36 We construct  a lattice from possible normalization candidates and find the best normalization sequence according to an n-gram language model using a Viterbi decoder. [sent-80, score-1.382]
</p><p>37 In this paper, we restrict the normalization lexicon to one-to-one word mappings, we do not consider multi words mapping for the lexicon induction. [sent-81, score-0.931]
</p><p>38 To identify OOV candidates for normalization; we restrict proposing normalization candidates to the words that we have in our induced normalization lexicon only. [sent-82, score-1.578]
</p><p>39 1 Baseline Normalization Candidates Generation We experimented with two normalization candidate generators as baseline systems. [sent-86, score-0.779]
</p><p>40 This approach overcomes the main problem of the dictionary-based approach which is providing inappropriate normalization candidates to the errors styles in the social media text. [sent-94, score-1.026]
</p><p>41 As we will show in the experiments in Section(5), dictionary-based normalization methods proved to be inadequate for social media domain normalization for many reasons. [sent-95, score-1.584]
</p><p>42 1 Bipartite Graph Representation The main motivation of this approach is that normalization equivalences share similar context; which we call contextual similarity. [sent-100, score-0.934]
</p><p>43 For instance, assume 5-gram sequences of words, two words may be normalization equivalences if their n-gram context shares the same two words on the left and the same two words on the right. [sent-101, score-0.926]
</p><p>44 This contextual similarity can be represented as a bipartite graph with the first partite representing the words and the second partite representing the n-gram contexts that may be shared by words. [sent-103, score-0.413]
</p><p>45 A  word node can be either normalized word or noisy word. [sent-104, score-0.48]
</p><p>46 Identifying if a word is normalized or noisy (candidate for normalization) is crucial since this decision limits the candidate noisy words to be normalized. [sent-105, score-0.733]
</p><p>47 We adopted a soft criteria for iden1579  Figure 1: Bipartite Graph Representation, left nodes represent contexts, gray right nodes represent the noisy words and white right nodes rep-  resent the normalized words. [sent-106, score-0.715]
</p><p>48 10 times) is considered as a candidate for normalization (noisy word). [sent-112, score-0.677]
</p><p>49 Figure(1) shows a sample of the bipartite graph G(W, C, E), where noisy words are shown as gray nodes. [sent-113, score-0.495]
</p><p>50 While constructing the graph, we identify if a node represents a noisy word (N) (called source node) or a normalized word (M) (called absorbing node). [sent-117, score-0.581]
</p><p>51 The main objective is to identify pairs of noisy and normalized words that can be considered as normalization equivalences. [sent-122, score-1.042]
</p><p>52 For example, (Hughes and Ramage, 2007) used random walks on Wordnet graph to measure lexical semantic relatedness between words. [sent-124, score-0.41]
</p><p>53 In this paper, we apply the label propagation approach to the text normalization problem. [sent-127, score-0.663]
</p><p>54 Consider a random walk on the bipartite graph G(W, C, E) starting at a noisy word (source  node) and ending at a normalized word (absorbing node). [sent-128, score-0.837]
</p><p>55 The walker starts from any source node Ni belonging to the noisy words then move to any other connected node Mj with probability Pij. [sent-129, score-0.426]
</p><p>56 This is due to the probability normalization which is done according to the nodes connectivity. [sent-132, score-0.727]
</p><p>57 It is worth noting that due to the bipartite graph representation; any word node, either noisy (source) or normalized (absorbing), is only connected to context nodes and not directly connected to any other word node. [sent-134, score-0.858]
</p><p>58 For any random walk the number of steps taken to traverse between any two nodes is called the hitting time (Norris, 1997). [sent-136, score-0.467]
</p><p>59 Therefore, the hitting time between a noisy and a normalized pair of nodes (n, m) with a walk r is hr (n, m). [sent-137, score-0.718]
</p><p>60 It is worth noting that the random walks are selected according to the transition probability in Eqn(1); therefore, the more probable paths will be picked more frequently. [sent-142, score-0.422]
</p><p>61 The same pair of nodes can be connected with many walks ofvarious steps (hits), and the same noisy word can be connected to many other normalized words. [sent-143, score-0.865]
</p><p>62 We define the contextual similarity probability of a normalization equivalence pair n, m as L(n, m). [sent-144, score-0.745]
</p><p>63 Which is the relative frequency of the average hitting of those two nodes, H(n, m), and all other normalized nodes linked to that noisy word. [sent-145, score-0.593]
</p><p>64 Thus L(n, m), is calculated as:  L(n,m)  = H(n,m)/∑H(n,mi)  (3)  ∑i  Furthermore, we add another similarity cost between a noisy word and a normalized word based on the lexical similarity cost, SimCost(n, m), which we will describe in the next section. [sent-146, score-0.603]
</p><p>65 This cost function is defined as the ratio  of LCSR and Edit distance between two strings as follows: SimCost(n, m) = LCSR(n, m)/ED(n, m) LCSR(n, m) = LCS(n, m)/MaxLenght(n, m)  (5) (6)  We have modified the Edit Distance calculation ED(n,m) to be more adequate for social media text. [sent-155, score-0.379]
</p><p>66 1 Training and Evaluation Data We collected large amount of social media data to generate the normalization lexicon using the ran1581  dom walk approach. [sent-158, score-1.207]
</p><p>67 We combined both data, noisy and clean, together to induce the normalization dictionary from them. [sent-162, score-0.954]
</p><p>68 We constructed a test set of 1000 sentences of social media which had been corrected by a native human annotator, the main guidelines were to normalize noisy words to its corresponding clean words in a consistent way according to the evidences in the context. [sent-164, score-0.743]
</p><p>69 Furthermore, we developed a test set for evaluating the effect of the normalization system when used as a preprocessing step for Machine translation. [sent-166, score-0.721]
</p><p>70 The machine translation test set is composed of 500 sentences of social media English text translated to normalized Spanish text by a bi-lingual translator. [sent-167, score-0.566]
</p><p>71 2  Evaluating Normalization Lexicon Generation  We extracted 5-gram sequences from the combined noisy and clean data; then we limited the space of noisy 5-gram sequences to those which contain only one noisy word as the center word and all other words, representing the context, are not noisy. [sent-169, score-0.974]
</p><p>72 As we mentioned before, we identify whether the word is noisy or not by looking up a vocabulary list constructed from clean data. [sent-170, score-0.44]
</p><p>73 Any word that appears less than 10 times in this vocabulary is considered noisy and candidate for normalization during the lexicon induction process. [sent-172, score-1.126]
</p><p>74 It is worth noting that our notion of noisy word does not mean it is an OOV that has to be corrected; instead it indicates that it is candidate for correction but may be opted not to be normalized if there is no confident normalization for it. [sent-173, score-1.219]
</p><p>75 We experimented with two candidate generators as baseline systems, namely the dictionary-based spelling correction and the trie approximate match with K errors; where K=3. [sent-181, score-0.396]
</p><p>76 We compared those approaches with our newly proposed unsupervised normalization lexicon induction; for this case the cost for a candidate is the combined cost of the contextual similarity probability and the lexical similarity cost as defined in Eqn(4). [sent-183, score-1.302]
</p><p>77 We examine the effect of data size and the steps of the random walks on the accuracy and the coverage of the induced dictionary. [sent-184, score-0.4]
</p><p>78 Finally, we pruned the lexicon to keep the top 5 candidates per noisy word. [sent-192, score-0.488]
</p><p>79 Next, we will examine the effect of lexicon size on the normalization task. [sent-199, score-0.779]
</p><p>80 On the other hand, the induced normalization lexicon approach is doing much better even with a small amount of data as we can see with system RW1 which uses Lex1 generated from 20M sentences and has 123K lexicon entry. [sent-210, score-0.996]
</p><p>81 , 2012) which used pairwise contextual similarity to induce a normalization lexicon of 40K entries, we will refer to this lexicon as HB-Dict. [sent-217, score-1.136]
</p><p>82 The contextual graph random walks approach helps in providing high precision lexicon since the sampling nature of the approach helps in filtering out unreliable normalization equivalences. [sent-226, score-1.284]
</p><p>83 The random walks will traverse more frequent paths; which would lead to more probable normalization equivalence. [sent-227, score-0.989]
</p><p>84 Since the proposed approach deploys random walks to sample paths that can traverse many steps, this relaxes the constraints that the  normalization equivalences have to share the same context. [sent-229, score-1.305]
</p><p>85 Instead a noisy word may share a context with another noisy word which in turn shares a context with a clean equivalent normalization word. [sent-230, score-1.382]
</p><p>86 Therefore, we end up with a lexicon that have much higher recall than the pairwise similarity approach since it explores equivalences beyond the pairwise relation. [sent-231, score-0.53]
</p><p>87 5 Output Analysis Table(4) shows some examples of the induced normalization equivalences, the first part shows good examples where vowels are restored and phonetic similar words are matched. [sent-234, score-0.759]
</p><p>88 On the other hand, the lexicon has some bad normalization such as ”‘unrecycled ”’ which should be normalized to ”‘non recycled”’  but since the system is limited to one word correction it did not get it. [sent-237, score-1.02]
</p><p>89 Another interesting bad normalization is ”‘tutting”’ which is new type of 1583  dancing and should not be corrected to ”‘tweeting”’ . [sent-238, score-0.656]
</p><p>90 vunietdNrwaotgucrilyhnsbgtiTledyabtvrwoiC4csranel:ybicgLtalnhiegtydxcVopnRuhetrogmwisndaStegrciloskmcnersdpiglamtcnoeirsladengityp Table 5 lists a number of examples and their normalization using both Baseline1 and RW3. [sent-239, score-0.627]
</p><p>91 At the first example, RW3 got the correct normalization as ”interesting ” which apparently is not the one with the shortest edit distance, though it is the most frequent candidate at the generated lexicon. [sent-240, score-0.737]
</p><p>92 The baseline system did not get it right; it  got a wrong normalization with shorter edit distance. [sent-241, score-0.716]
</p><p>93 At Example(3), both the baseline and RW3 did not get the correct normalization of ”yur” to ”you are ” which is currently a limitation in our system since we only allow one-to-one word mapping in the generated lexicons not one-to-many or many-tomany. [sent-243, score-0.686]
</p><p>94 This shows a characteristic of the proposed approach; it is very conservative in proposing normalization which is desirable as a preprocessing step for NLP applications. [sent-245, score-0.756]
</p><p>95 Finally, Example 4 shows also that the system normalize ”gr8” which is mainly due to having a flexible similarity cost during the normalization lexicon construction. [sent-247, score-0.94]
</p><p>96 The translation with normalization was improved by about 6% from 29. [sent-257, score-0.671]
</p><p>97 036ol%7t9vs% e%mnet 6  Conclusion and Future Work  We introduced a social media text normalization system that can be deployed as a preprocessor for MT and various NLP applications to handle social media text. [sent-262, score-1.374]
</p><p>98 We show that the proposed unsupervised  approach provides a normalization system with very high precision and a reasonable recall. [sent-264, score-0.76]
</p><p>99 As an extension to this work, we will extend the approach to handle many-to-many normalization pairs; also we plan to apply the approach to more languages. [sent-267, score-0.665]
</p><p>100 An improved error model for noisy channel spelling correction, In ACL 2000: Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, Englewood Cliffs, NJ, USA. [sent-280, score-0.42]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('normalization', 0.627), ('noisy', 0.268), ('walks', 0.246), ('equivalences', 0.202), ('media', 0.164), ('lexicon', 0.152), ('normalized', 0.147), ('social', 0.139), ('bipartite', 0.133), ('walk', 0.125), ('absorbing', 0.101), ('nodes', 0.1), ('graph', 0.094), ('clean', 0.09), ('channel', 0.082), ('oov', 0.079), ('trie', 0.078), ('hitting', 0.078), ('eqn', 0.076), ('cost', 0.076), ('spelling', 0.07), ('random', 0.07), ('generators', 0.07), ('han', 0.069), ('candidates', 0.068), ('lcsr', 0.067), ('preprocessing', 0.065), ('node', 0.065), ('correction', 0.065), ('contextual', 0.062), ('pairwise', 0.06), ('edit', 0.06), ('lattice', 0.06), ('norris', 0.057), ('tkin', 0.057), ('similarity', 0.056), ('string', 0.056), ('phonetic', 0.054), ('constructed', 0.053), ('candidate', 0.05), ('steps', 0.048), ('sms', 0.048), ('hits', 0.047), ('traverse', 0.046), ('translation', 0.044), ('viterbi', 0.044), ('share', 0.043), ('pij', 0.042), ('vowels', 0.042), ('unlabeled', 0.041), ('transition', 0.04), ('sequences', 0.04), ('deployed', 0.038), ('handle', 0.038), ('contractor', 0.038), ('simcost', 0.038), ('zobel', 0.038), ('text', 0.036), ('induced', 0.036), ('proposed', 0.036), ('unsupervised', 0.035), ('paths', 0.035), ('partite', 0.034), ('checker', 0.034), ('customized', 0.034), ('szummer', 0.034), ('minkov', 0.034), ('precision', 0.033), ('substitution', 0.032), ('dictionary', 0.032), ('experimented', 0.032), ('accent', 0.031), ('dro', 0.031), ('gouws', 0.031), ('approximate', 0.031), ('abbreviations', 0.031), ('confident', 0.031), ('noting', 0.031), ('lexicons', 0.03), ('cook', 0.03), ('corrected', 0.029), ('system', 0.029), ('hughes', 0.029), ('messaging', 0.029), ('vocabulary', 0.029), ('context', 0.029), ('conservative', 0.028), ('errors', 0.028), ('connected', 0.028), ('spell', 0.028), ('normalisation', 0.028), ('melamed', 0.028), ('shares', 0.028), ('induce', 0.027), ('discusses', 0.027), ('messages', 0.027), ('inadequate', 0.027), ('brill', 0.027), ('slang', 0.026), ('ramage', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="326-tfidf-1" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>Author: Hany Hassan ; Arul Menezes</p><p>Abstract: We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.</p><p>2 0.44981983 <a title="326-tfidf-2" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>Author: Congle Zhang ; Tyler Baldwin ; Howard Ho ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normal- ization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</p><p>3 0.15994458 <a title="326-tfidf-3" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>4 0.1109902 <a title="326-tfidf-4" href="./acl-2013-Exploiting_Social_Media_for_Natural_Language_Processing%3A_Bridging_the_Gap_between_Language-centric_and_Real-world_Applications.html">146 acl-2013-Exploiting Social Media for Natural Language Processing: Bridging the Gap between Language-centric and Real-world Applications</a></p>
<p>Author: Simone Paolo Ponzetto ; Andrea Zielinski</p><p>Abstract: unkown-abstract</p><p>5 0.1045595 <a title="326-tfidf-5" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<p>Author: Alexandra Balahur ; Hristo Tanev</p><p>Abstract: Nowadays, the importance of Social Media is constantly growing, as people often use such platforms to share mainstream media news and comment on the events that they relate to. As such, people no loger remain mere spectators to the events that happen in the world, but become part of them, commenting on their developments and the entities involved, sharing their opinions and distributing related content. This paper describes a system that links the main events detected from clusters of newspaper articles to tweets related to them, detects complementary information sources from the links they contain and subsequently applies sentiment analysis to classify them into positive, negative and neutral. In this manner, readers can follow the main events happening in the world, both from the perspective of mainstream as well as social media and the public’s perception on them. This system will be part of the EMM media monitoring framework working live and it will be demonstrated using Google Earth.</p><p>6 0.10044308 <a title="326-tfidf-6" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>7 0.096646287 <a title="326-tfidf-7" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>8 0.089110874 <a title="326-tfidf-8" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>9 0.088827476 <a title="326-tfidf-9" href="./acl-2013-Resolving_Entity_Morphs_in_Censored_Data.html">301 acl-2013-Resolving Entity Morphs in Censored Data</a></p>
<p>10 0.083756536 <a title="326-tfidf-10" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>11 0.083440304 <a title="326-tfidf-11" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>12 0.082608983 <a title="326-tfidf-12" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>13 0.077755168 <a title="326-tfidf-13" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>14 0.068232998 <a title="326-tfidf-14" href="./acl-2013-An_Empirical_Study_on_Uncertainty_Identification_in_Social_Media_Context.html">45 acl-2013-An Empirical Study on Uncertainty Identification in Social Media Context</a></p>
<p>15 0.067297712 <a title="326-tfidf-15" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>16 0.066812068 <a title="326-tfidf-16" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>17 0.064774074 <a title="326-tfidf-17" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>18 0.063611686 <a title="326-tfidf-18" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>19 0.062255733 <a title="326-tfidf-19" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>20 0.058413308 <a title="326-tfidf-20" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, 0.024), (2, 0.039), (3, 0.017), (4, 0.072), (5, 0.002), (6, 0.02), (7, 0.061), (8, 0.062), (9, -0.064), (10, -0.08), (11, 0.03), (12, 0.026), (13, -0.118), (14, 0.017), (15, -0.038), (16, 0.034), (17, 0.012), (18, -0.048), (19, 0.027), (20, 0.042), (21, 0.047), (22, 0.116), (23, -0.016), (24, -0.026), (25, 0.073), (26, 0.016), (27, 0.123), (28, -0.031), (29, -0.039), (30, -0.124), (31, -0.113), (32, -0.185), (33, 0.183), (34, 0.165), (35, -0.149), (36, -0.142), (37, -0.061), (38, -0.121), (39, 0.037), (40, 0.109), (41, 0.085), (42, 0.167), (43, -0.194), (44, -0.103), (45, 0.053), (46, -0.123), (47, -0.042), (48, 0.03), (49, 0.163)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95892739 <a title="326-lsi-1" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>Author: Hany Hassan ; Arul Menezes</p><p>Abstract: We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.</p><p>2 0.89022785 <a title="326-lsi-2" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>Author: Congle Zhang ; Tyler Baldwin ; Howard Ho ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normal- ization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</p><p>3 0.52107537 <a title="326-lsi-3" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>Author: Majid Razmara ; Maryam Siahbani ; Reza Haffari ; Anoop Sarkar</p><p>Abstract: Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation especially when a limited amount of parallel text is available for training or when there is a domain shift from training data to test data. In this paper, we propose a novel approach to finding translations for oov words. We induce a lexicon by constructing a graph on source language monolingual text and employ a graph propagation technique in order to find translations for all the source language phrases. Our method differs from previous approaches by adopting a graph propagation approach that takes into account not only one-step (from oov directly to a source language phrase that has a translation) but multi-step paraphrases from oov source language words to other source language phrases and eventually to target language translations. Experimental results show that our graph propagation method significantly improves performance over two strong baselines under intrinsic and extrinsic evaluation metrics.</p><p>4 0.48321399 <a title="326-lsi-4" href="./acl-2013-Parsing_Graphs_with_Hyperedge_Replacement_Grammars.html">274 acl-2013-Parsing Graphs with Hyperedge Replacement Grammars</a></p>
<p>Author: David Chiang ; Jacob Andreas ; Daniel Bauer ; Karl Moritz Hermann ; Bevan Jones ; Kevin Knight</p><p>Abstract: Hyperedge replacement grammar (HRG) is a formalism for generating and transforming graphs that has potential applications in natural language understanding and generation. A recognition algorithm due to Lautemann is known to be polynomial-time for graphs that are connected and of bounded degree. We present a more precise characterization of the algorithm’s complexity, an optimization analogous to binarization of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing.</p><p>5 0.47008687 <a title="326-lsi-5" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>6 0.4447785 <a title="326-lsi-6" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>7 0.42303354 <a title="326-lsi-7" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>8 0.4181135 <a title="326-lsi-8" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>9 0.41408008 <a title="326-lsi-9" href="./acl-2013-Exploiting_Social_Media_for_Natural_Language_Processing%3A_Bridging_the_Gap_between_Language-centric_and_Real-world_Applications.html">146 acl-2013-Exploiting Social Media for Natural Language Processing: Bridging the Gap between Language-centric and Real-world Applications</a></p>
<p>10 0.39165887 <a title="326-lsi-10" href="./acl-2013-Computerized_Analysis_of_a_Verbal_Fluency_Test.html">89 acl-2013-Computerized Analysis of a Verbal Fluency Test</a></p>
<p>11 0.3898586 <a title="326-lsi-11" href="./acl-2013-Detecting_Chronic_Critics_Based_on_Sentiment_Polarity_and_User%C3%A2%E2%80%A2%C5%BDs_Behavior_in_Social_Media.html">114 acl-2013-Detecting Chronic Critics Based on Sentiment Polarity and Userâ•Žs Behavior in Social Media</a></p>
<p>12 0.38629153 <a title="326-lsi-12" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>13 0.38192123 <a title="326-lsi-13" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>14 0.37179852 <a title="326-lsi-14" href="./acl-2013-Resolving_Entity_Morphs_in_Censored_Data.html">301 acl-2013-Resolving Entity Morphs in Censored Data</a></p>
<p>15 0.35912099 <a title="326-lsi-15" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>16 0.35749969 <a title="326-lsi-16" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>17 0.34574464 <a title="326-lsi-17" href="./acl-2013-An_Open_Source_Toolkit_for_Quantitative_Historical_Linguistics.html">48 acl-2013-An Open Source Toolkit for Quantitative Historical Linguistics</a></p>
<p>18 0.33899236 <a title="326-lsi-18" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>19 0.33258155 <a title="326-lsi-19" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>20 0.32787055 <a title="326-lsi-20" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.064), (6, 0.029), (11, 0.064), (24, 0.048), (26, 0.084), (28, 0.013), (35, 0.076), (42, 0.038), (48, 0.033), (64, 0.016), (68, 0.111), (70, 0.072), (88, 0.022), (90, 0.042), (95, 0.205)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93490601 <a title="326-lda-1" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>Author: Hany Hassan ; Arul Menezes</p><p>Abstract: We introduce a social media text normalization system that can be deployed as a preprocessing step for Machine Translation and various NLP applications to handle social media text. The proposed system is based on unsupervised learning of the normalization equivalences from unlabeled text. The proposed approach uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus. We show that the proposed approach has a very high precision of (92.43) and a reasonable recall of (56.4). When used as a preprocessing step for a state-of-the-art machine translation system, the translation quality on social media text improved by 6%. The proposed approach is domain and language independent and can be deployed as a preprocessing step for any NLP application to handle social media text.</p><p>2 0.89431334 <a title="326-lda-2" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>Author: Jason R. Smith ; Herve Saint-Amand ; Magdalena Plamada ; Philipp Koehn ; Chris Callison-Burch ; Adam Lopez</p><p>Abstract: Parallel text is the fuel that drives modern machine translation systems. The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies. We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon’s Elastic Cloud. Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500. Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets. Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU. We make our code and data available for other researchers seeking to mine this rich new data resource.1</p><p>3 0.89330208 <a title="326-lda-3" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>Author: Wang Ling ; Guang Xiang ; Chris Dyer ; Alan Black ; Isabel Trancoso</p><p>Abstract: In the ever-expanding sea of microblog data, there is a surprising amount of naturally occurring parallel text: some users create post multilingual messages targeting international audiences while others “retweet” translations. We present an efficient method for detecting these messages and extracting parallel segments from them. We have been able to extract over 1M Chinese-English parallel segments from Sina Weibo (the Chinese counterpart of Twitter) using only their public APIs. As a supplement to existing parallel training data, our automatically extracted parallel data yields substantial translation quality improvements in translating microblog text and modest improvements in translating edited news commentary. The resources in described in this paper are available at http://www.cs.cmu.edu/∼lingwang/utopia.</p><p>4 0.88769329 <a title="326-lda-4" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>Author: Lucia Specia ; ; ; Kashif Shah ; Jose G.C. de Souza ; Trevor Cohn</p><p>Abstract: We describe QUEST, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of features and algorithms.</p><p>5 0.88527602 <a title="326-lda-5" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>6 0.88350725 <a title="326-lda-6" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>7 0.88264 <a title="326-lda-7" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>8 0.88222218 <a title="326-lda-8" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>9 0.88170183 <a title="326-lda-9" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>10 0.87866902 <a title="326-lda-10" href="./acl-2013-Question_Classification_Transfer.html">292 acl-2013-Question Classification Transfer</a></p>
<p>11 0.87512553 <a title="326-lda-11" href="./acl-2013-Handling_Ambiguities_of_Bilingual_Predicate-Argument_Structures_for_Statistical_Machine_Translation.html">180 acl-2013-Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation</a></p>
<p>12 0.87405801 <a title="326-lda-12" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>13 0.87186497 <a title="326-lda-13" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>14 0.86977971 <a title="326-lda-14" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>15 0.86933166 <a title="326-lda-15" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>16 0.86849332 <a title="326-lda-16" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>17 0.86732793 <a title="326-lda-17" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>18 0.86499524 <a title="326-lda-18" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>19 0.863612 <a title="326-lda-19" href="./acl-2013-Co-Regression_for_Cross-Language_Review_Rating_Prediction.html">81 acl-2013-Co-Regression for Cross-Language Review Rating Prediction</a></p>
<p>20 0.86334014 <a title="326-lda-20" href="./acl-2013-Enlisting_the_Ghost%3A_Modeling_Empty_Categories_for_Machine_Translation.html">137 acl-2013-Enlisting the Ghost: Modeling Empty Categories for Machine Translation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
