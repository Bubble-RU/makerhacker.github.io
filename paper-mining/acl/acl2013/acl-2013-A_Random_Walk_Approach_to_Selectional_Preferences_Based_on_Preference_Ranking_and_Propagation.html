<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-17" href="#">acl2013-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</h1>
<br/><p>Source: <a title="acl-2013-17-pdf" href="http://aclweb.org/anthology//P/P13/P13-1115.pdf">pdf</a></p><p>Author: Zhenhua Tian ; Hengheng Xiang ; Ziqi Liu ; Qinghua Zheng</p><p>Abstract: This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate’s smoothed preferences. Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements.</p><p>Reference: <a title="acl-2013-17-reference" href="../acl2013_reference/acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. [sent-4, score-0.523]
</p><p>2 Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate’s smoothed preferences. [sent-5, score-1.06]
</p><p>3 Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements. [sent-6, score-0.374]
</p><p>4 1 Introduction  Selectional preferences (SP) or selectional restrictions capture the plausibility of predicates and their arguments for a given relation. [sent-7, score-1.096]
</p><p>5 Kaze and Fodor (1963) describe that predicates and their arguments have strict boolean restrictions, either satisfied or violated. [sent-8, score-0.456]
</p><p>6 ” He further states selectional restrictions as preferences between the predicates and arguments, where the violation can be less preferred, but not fatal. [sent-12, score-0.665]
</p><p>7 For instance, given the predicate word eat, word food is likely to be its object, iPhone is likely to be implausible for it, and tiger is less preferred but not curious. [sent-13, score-0.214]
</p><p>8 cisions, such as semantic role labeling (Resnik, 1993; Gildea and Jurafsky, 2002), word sense dis-  ambiguation (Resnik, 1997), human plausibility judgements (Spasi´ c and Ananiadou, 2004), syntactic disambiguation (Toutanova et al. [sent-15, score-0.421]
</p><p>9 A direct approach to acquire SP is to extract triples (q, r, a) of predicates, relations, and arguments from a syntactically analyzed corpus, and then conduct maximum likelihood estimation (MLE) on the data. [sent-20, score-0.241]
</p><p>10 } But we may not see plausible and implausible triples such as: eat - {watermelon, ziti, escarole, iPhone. [sent-25, score-0.204]
</p><p>11 } Then how to use a smooth model to alleviate data sparsity for SP? [sent-28, score-0.173]
</p><p>12 Random walk models have been successfully applied to alleviate the data sparsity issue on collaborative filtering in recommender systems. [sent-29, score-0.323]
</p><p>13 In this paper, we present an extension of using the random walk model to alleviate data sparsity for SP. [sent-32, score-0.333]
</p><p>14 The main intuition is to aggregate all the transitions from a given predicate to its nearby predicates, and propagate their preferences on arguments as the given predicate’s smoothed argu1169  ProceedingsS ooffita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-33, score-0.679]
</p><p>15 Our work and contributions are summarized as follows: •  We present a framework of random walk approach to SP. [sent-36, score-0.241]
</p><p>16 Each component is corresponding to a specific functional operation on the bipartite and monopartite graphs which representing the SP data;  •  •  •  We propose an adjusted preference ranking method to measure SP based on the popularity and association of predicate-argument pairs. [sent-38, score-0.513]
</p><p>17 It also helps to discover similar predicates more precisely; We introduce a probability function for random walk based on the predicate distances. [sent-40, score-0.784]
</p><p>18 It controls the influence of nearby and distant predicates to achieve more accurate results; We find out that propagate the measured preferences of predicate-argument pairs is more proper and natural for SP smooth. [sent-41, score-0.705]
</p><p>19 We conduct experiments using two sections of the LDC English gigaword corpora as the generalization data. [sent-43, score-0.162]
</p><p>20 We further investigate the correla-  tions of smoothed scores with human plausibility judgements. [sent-46, score-0.41]
</p><p>21 Section 5 provides experiments on both the pseudo-disambiguation task and human plausibility judgements. [sent-51, score-0.335]
</p><p>22 For a given predicate q, the system firstly computes its distribution of argument semantic classes based on WordNet. [sent-55, score-0.206]
</p><p>23 Then for a given argument a, the system collects the set of candidate semantic classes which contain the argument a, and ensures they are seen in q. [sent-56, score-0.162]
</p><p>24 Finally the system picks a semantic class from  the candidates with the maximal selectional association score, and defines the score as smoothed score of (q, a). [sent-57, score-0.265]
</p><p>25 Beyond induction on argument classes only, Agirre and Martinez (2001) propose a class-toclass model that simultaneously learns SP on both the predicate and argument classes. [sent-63, score-0.287]
</p><p>26 Random walk model falls into the non-class based distributional approach. [sent-85, score-0.229]
</p><p>27 Previous literatures have fully studied the selection of distance or similarity functions to find out similar predicates and arguments (Dagan et al. [sent-86, score-0.609]
</p><p>28 , 2010), or learn the weights between the predicates (Bergsma et al. [sent-88, score-0.323]
</p><p>29 Instead, we put effort in following issues: 1) how to measure SP; 2) how to transfer between predicates using random walk; 3) how to propagate the preferences for smooth. [sent-90, score-0.569]
</p><p>30 eWs,e a nindit Yiat =e{ athe links E }w airteh thhee n raw co-occurrence counts of seen predicate-argument pairs in a given generalization data. [sent-105, score-0.162]
</p><p>31 We represent the graph by an adjacency matrix with rows representing predicates and columns as arguments. [sent-106, score-0.478]
</p><p>32 For convenience, we use indices i,j to represent predicates qi , qj , and k, lfor arguments ak , al . [sent-107, score-0.669]
</p><p>33 We employ a preference ranking function Ψ to measure the SP between the predicates and arguments. [sent-108, score-0.583]
</p><p>34 It transforms G to a corresponding bipartite graph R, with links representing the strength of SP. [sent-109, score-0.166]
</p><p>35 Each row of the adjacency matrix R denotes the predicate vector q⃗i or q⃗ j. [sent-110, score-0.159]
</p><p>36 Ψ := G → R  (1)  Figure 1: Illustration of (R) the bipartite graph of the verb-dobj-noun relation, (Q) the predicate-projection monopartite graph, and (A) the argument-projection monopartite graph. [sent-113, score-0.498]
</p><p>37 Monopartite Graph Projection: In order to conduct random walk on the graph, we project the bipartite graph R onto a monopartite graph Q=(X, E) between the predicates, or A=(Y, E) between the arguments (Zhou et al. [sent-114, score-0.806]
</p><p>38 The  links in Q represent the indirect connects between the predicates in R. [sent-117, score-0.323]
</p><p>39 Two predicates are connected in Q if they share at least one common neighbor argument in R. [sent-118, score-0.404]
</p><p>40 Φ := R → D (2) Stochastic Walking Strategy: We introduce a probability function ∆ to transform the predicate distances D into transition probabilities P. [sent-121, score-0.308]
</p><p>41 Where P is a stochastic matrix, with each element pij represents the transition probability from predicate qi to qj. [sent-122, score-0.331]
</p><p>42 Generally speaking, nearby predicates gain higher probabilities to be visited, while distant predicates will be penalized. [sent-123, score-0.834]
</p><p>43 But in SP, the preferences b[e0t,w10ee] no nth IeM predicates Bauntd i arguments are implicit: their co-occurrence counts follow the power law distribution and vary greatly. [sent-138, score-0.642]
</p><p>44 Therefore, we employ a ranking function Ψ to measure the SP of the seen predicate-argument pairs. [sent-139, score-0.142]
</p><p>45 investigate the correlations between the co-occurrence counts (CT) c(q, a), or smoothed counts with the human plausibility judgements (Lapata et al. [sent-146, score-0.621]
</p><p>46 Some introduce conditional probability (CP) p(a|q) for the decision of preference judgements (Chambers athned Jurafsky, 2010; eErerkn ceet al. [sent-149, score-0.24]
</p><p>47 Intuitively, it mea-  sures the preferences by combining both the popularity and association, with parameters control the uncertainty of the trade-off between the two. [sent-155, score-0.198]
</p><p>48 This is potentially similar to the process of human plausibility judgements. [sent-157, score-0.335]
</p><p>49 One may judge the plausibility of a predicate-argument collocation from two sides: 1) if it has enough evidences and commonly to be seen; 2) if it has strong association according to the cognition based on kinds of background knowledge. [sent-158, score-0.298]
</p><p>50 α1  (8)  , α2 ∈ [0, 1]  We verify if a metric is better by two tasks: 1) how well it correlates with human plausibility judgements; 2) how well it helps with the smooth inference to disambiguate plausible and implausible instances. [sent-162, score-0.637]
</p><p>51 2  Distance Function: Projection of the Monopartite Graph In Equation 9, the distance function Φ is used to discover nearby predicates with distance dij. [sent-167, score-0.604]
</p><p>52 We calculate Φ based on the vectors ⃗q i, q⃗ j represented by the measured preferences in R. [sent-170, score-0.152]
</p><p>53 dij = Φ(⃗ qi, q⃗ j)  (9)  Where Φ can be distance functions such as Euclidean (norm) distance or Kullback-Leibler divergence (KL) etc. [sent-171, score-0.247]
</p><p>54 Where the transition probability p(qj |qi) in P is defined as a function of the distance dij with a parameter δ. [sent-178, score-0.283]
</p><p>55 Intuitively, it means in a given walk step, a predicate qj which is far away from qi will get much less probability to be visited, and qi has high probabilities to start walk from itself and its nearby predicates to pursue good precision. [sent-179, score-1.345]
</p><p>56 δ ≥ 0,  (11)  dij ∈ [0, 1]  Where the parameter δ is used to control the balance of nearby and distant predicates. [sent-183, score-0.235]
</p><p>57 4 Propagation Function The propagation function in Equation 5 is represented by the matrix form. [sent-190, score-0.198]
</p><p>58 e Washuerreed p p(qreference of predicate qj with argument ak. [sent-195, score-0.301]
</p><p>59 Pfr(ak,qi) =∑me p(qj|qi) · Pr(ak,qj)  (12)  ∑j=1  We employ two prope pag(qation modes (PropMode) for the preference propagation function. [sent-196, score-0.223]
</p><p>60 We set ranking function Ψ=Pr(q, a) always to be the same in both the distance function and the  propagation function. [sent-203, score-0.359]
</p><p>61 The resulting data consist of: •  AFP: 26, 118, 892 verb-dobj-noun observations with 1, 918, 275 distinct triples, totally  4, 771 predicates and 44, 777 arguments. [sent-216, score-0.323]
</p><p>62 •  NYT: 29, 149, 574 verb-dobj-noun observations with 3, 281, 391 distinct triples, totally 5, 782 predicates and 57, 480 arguments. [sent-217, score-0.323]
</p><p>63 We keep dependencies whose predicates and arguments are seen in the generalization data. [sent-221, score-0.584]
</p><p>64 Human Plausibility Judgements Data: We employ two human plausibility judgements data 1http://nlp. [sent-224, score-0.421]
</p><p>65 In each they col-  lect a set of predicate-argument pairs, and annotate with two kinds of human ratings: one for an argument takes the role as the patient of a predicate, and the other for the argument as the agent. [sent-230, score-0.199]
</p><p>66 (2007) develop a set of human plausibility ratings on the basis of the Penn TreeBank and FrameNet respectively. [sent-237, score-0.414]
</p><p>67 Without explicit explanation, we remove all the selected PTB tests and human plausibility pairs from AFP and NYT to treat them unseen. [sent-242, score-0.335]
</p><p>68 (2010) focus on inferring latent topics and their distributions over multiple arguments and relations (e. [sent-261, score-0.174]
</p><p>69 First the system removes a portion of seen predicate-argument pairs from the generalization data to treat them as unseen positive tests (q, a+). [sent-273, score-0.161]
</p><p>70 Confounder Selection: for a given (q, a+), the system selects an argument a′ from the argument vocabulary. [sent-276, score-0.162]
</p><p>71 Then by ensure (q, a′) is unseen in the generalization data, it treats a′ as pseudo a− . [sent-277, score-0.161]
</p><p>72 Random confounder (RND) most closes to the realistic case; While nearest confounder (NER) is reproducible and it avoids frequency bias (Chambers and Jurafsky, 2010). [sent-280, score-0.25]
</p><p>73 While using NYT as  a+  a+,  the generalization data, we hold the same parameter settings as AFP to ensure the results are robust. [sent-301, score-0.165]
</p><p>74 Note that indeed the parameter settings would vary among different generalization and test data. [sent-302, score-0.165]
</p><p>75 With respect to the ranking function Ψ, CP performs the worst as it considers only the popularity rather than association. [sent-311, score-0.188]
</p><p>76 The heavy bias on frequent predicates and arguments has two major drawbacks: a) The computation of predicate distances would rely much more on frequent arguments, rather than those arguments they preferred; b) While propagation, it may bias more on frequent arguments, too. [sent-312, score-0.714]
</p><p>77 Even these frequent arguments are less preferred and not proper to be propagated. [sent-313, score-0.207]
</p><p>78 For MI, it biases infrequent arguments with strong association, without regarding to the popular arguments with more evidences. [sent-315, score-0.266]
</p><p>79 Furthermore, the generalization data is automatically parsed and kind of noisy, especially on infrequent predicates and arguments. [sent-316, score-0.451]
</p><p>80 We set the ranking function Ψ as AR (with tuned α1=0. [sent-332, score-0.142]
</p><p>81 This means for a given predicate, the penalty on its distant predicates helps to get more accurate smooth. [sent-341, score-0.359]
</p><p>82 Iteratively, by employing the adjusted ranking function, smoothing with preference propagation method, and revising the probability function with the parameter δ, RSP outperforms all previous methods. [sent-353, score-0.401]
</p><p>83 Figure 3 show the macro (left) and micro (right) receiver-operating-characteristic (ROC) curves of different models, using AFP as the generalization  data and RND confounders. [sent-359, score-0.172]
</p><p>84 We also verified the AUC metric using NYT as the generalization data. [sent-362, score-0.172]
</p><p>85 4 Human Plausibility Judgements We conduct empirical studies on the correlations between different preference ranking func1176  RLDESrDakAPgea-nAtSaePl. [sent-367, score-0.292]
</p><p>86 21 3681  Table 3: Correlation results on the human plausibility judgements data. [sent-377, score-0.421]
</p><p>87 (2001), we first collect the co-occurrence counts of predicate-argument pairs in the human plausibility data from AFP and NYT (before removing them as unseen pairs). [sent-380, score-0.402]
</p><p>88 We also compare the correlations between the smoothed scores of different models with human ratings. [sent-386, score-0.169]
</p><p>89 We hold that automatic models of plausibility can not be expected to surpass this upper bound. [sent-390, score-0.298]
</p><p>90 The first 5 rows are the correlations between the preference ranking functions and human ratings based on MLE. [sent-393, score-0.487]
</p><p>91 On both the PBP and MRP data, the proposed AR metric better correlates with human ratings than others, with α2 >0. [sent-394, score-0.199]
</p><p>92 RSP model gains the best correlation on the two plausibility data in most cases, where the parameter settings are the same as pseudo-disambiguation. [sent-401, score-0.335]
</p><p>93 6  Conclusions and Future Work  In this work we present an random walk approach to SP. [sent-402, score-0.241]
</p><p>94 We find out that a proper measure on SP between the predicates and arguments is important for SP. [sent-405, score-0.492]
</p><p>95 It helps with the discovering of nearby predicates and it makes the preference propagation to be more accurate. [sent-406, score-0.662]
</p><p>96 Explaining away ambiguity: Learning verb selectional preference with bayesian networks. [sent-431, score-0.308]
</p><p>97 A flexible, corpus-driven model of regular and inverse selectional preferences. [sent-446, score-0.19]
</p><p>98 Detecting compositionality of verb-  object combinations using selectional preferences. [sent-505, score-0.19]
</p><p>99 Using automatically learnt verb selectional preferences for classification of biomedical terms. [sent-551, score-0.342]
</p><p>100 A random walk method for alleviating the sparsity problem in collaborative filtering. [sent-565, score-0.293]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('predicates', 0.323), ('plausibility', 0.298), ('sp', 0.275), ('selectional', 0.19), ('walk', 0.189), ('afp', 0.169), ('monopartite', 0.166), ('preferences', 0.152), ('nyt', 0.147), ('rsp', 0.145), ('arguments', 0.133), ('generalization', 0.128), ('predicate', 0.125), ('confounder', 0.125), ('rnd', 0.119), ('qi', 0.118), ('preference', 0.118), ('nearby', 0.116), ('erk', 0.11), ('propagation', 0.105), ('bipartite', 0.1), ('bergsma', 0.095), ('qj', 0.095), ('judgements', 0.086), ('confounders', 0.083), ('dij', 0.083), ('propmode', 0.083), ('ranking', 0.083), ('smooth', 0.081), ('argument', 0.081), ('auc', 0.08), ('ratings', 0.079), ('pe', 0.077), ('smoothed', 0.075), ('triples', 0.074), ('keller', 0.073), ('ritter', 0.073), ('ptb', 0.07), ('graph', 0.066), ('lapata', 0.063), ('cp', 0.063), ('pbp', 0.062), ('pad', 0.061), ('eaghdha', 0.061), ('function', 0.059), ('functions', 0.058), ('correlations', 0.057), ('rows', 0.055), ('distance', 0.053), ('transition', 0.052), ('random', 0.052), ('equation', 0.052), ('sparsity', 0.052), ('implausible', 0.051), ('dp', 0.049), ('dagan', 0.049), ('rooth', 0.048), ('verify', 0.046), ('popularity', 0.046), ('clusterings', 0.046), ('ner', 0.045), ('micro', 0.044), ('metric', 0.044), ('propagate', 0.042), ('chambers', 0.042), ('recommender', 0.042), ('pr', 0.042), ('ddpp', 0.042), ('literatures', 0.042), ('macroauc', 0.042), ('microauc', 0.042), ('ngcm', 0.042), ('selpref', 0.042), ('spasi', 0.042), ('yildirim', 0.042), ('latent', 0.041), ('plausible', 0.041), ('distributional', 0.04), ('alleviate', 0.04), ('correlates', 0.039), ('ar', 0.039), ('eat', 0.038), ('preferred', 0.038), ('human', 0.037), ('settings', 0.037), ('probabilities', 0.036), ('cosine', 0.036), ('probability', 0.036), ('proper', 0.036), ('distant', 0.036), ('pointwise', 0.036), ('transitions', 0.036), ('jurafsky', 0.035), ('pantel', 0.035), ('counts', 0.034), ('flexible', 0.034), ('conduct', 0.034), ('mcrae', 0.034), ('matrix', 0.034), ('unseen', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="17-tfidf-1" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>Author: Zhenhua Tian ; Hengheng Xiang ; Ziqi Liu ; Qinghua Zheng</p><p>Abstract: This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate’s smoothed preferences. Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements.</p><p>2 0.24602647 <a title="17-tfidf-2" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>Author: Tiziano Flati ; Roberto Navigli</p><p>Abstract: We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best f∗it) tahned ∗ argument. Taon idco this, we extract failtl thhee ∗ occurrences ion Wikipedia ewxthraiccht match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach.</p><p>3 0.17585529 <a title="17-tfidf-3" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>Author: Oren Melamud ; Jonathan Berant ; Ido Dagan ; Jacob Goldberger ; Idan Szpektor</p><p>Abstract: Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level. A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors. We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations. Evaluations on a naturallydistributed dataset show that our model significantly outperforms prior word-level and topic-level models. We also release a first context-sensitive inference rule set.</p><p>4 0.13490492 <a title="17-tfidf-4" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>Author: Roi Reichart ; Anna Korhonen</p><p>Abstract: Subcategorization frames (SCFs), selectional preferences (SPs) and verb classes capture related aspects of the predicateargument structure. We present the first unified framework for unsupervised learning of these three types of information. We show how to utilize Determinantal Point Processes (DPPs), elegant probabilistic models that are defined over the possible subsets of a given dataset and give higher probability mass to high quality and diverse subsets, for clustering. Our novel clustering algorithm constructs a joint SCF-DPP DPP kernel matrix and utilizes the efficient sampling algorithms of DPPs to cluster together verbs with similar SCFs and SPs. We evaluate the induced clusters in the context of the three tasks and show results that are superior to strong baselines for each 1.</p><p>5 0.13053526 <a title="17-tfidf-5" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>Author: Travis Wolfe ; Benjamin Van Durme ; Mark Dredze ; Nicholas Andrews ; Charley Beller ; Chris Callison-Burch ; Jay DeYoung ; Justin Snyder ; Jonathan Weese ; Tan Xu ; Xuchen Yao</p><p>Abstract: We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17% F1.</p><p>6 0.13003753 <a title="17-tfidf-6" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>7 0.1256935 <a title="17-tfidf-7" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>8 0.10952908 <a title="17-tfidf-8" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>9 0.10592392 <a title="17-tfidf-9" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>10 0.10495981 <a title="17-tfidf-10" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>11 0.09030401 <a title="17-tfidf-11" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>12 0.088471219 <a title="17-tfidf-12" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>13 0.0861395 <a title="17-tfidf-13" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>14 0.083998412 <a title="17-tfidf-14" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>15 0.08348731 <a title="17-tfidf-15" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>16 0.083465345 <a title="17-tfidf-16" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>17 0.079908527 <a title="17-tfidf-17" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>18 0.078375168 <a title="17-tfidf-18" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>19 0.078360111 <a title="17-tfidf-19" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>20 0.078308538 <a title="17-tfidf-20" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.212), (1, 0.047), (2, -0.01), (3, -0.141), (4, -0.065), (5, 0.01), (6, -0.036), (7, 0.073), (8, -0.061), (9, 0.02), (10, 0.053), (11, 0.037), (12, 0.092), (13, -0.009), (14, 0.047), (15, -0.014), (16, 0.02), (17, 0.01), (18, 0.199), (19, 0.052), (20, 0.05), (21, 0.053), (22, -0.073), (23, 0.014), (24, 0.112), (25, 0.015), (26, 0.086), (27, 0.049), (28, 0.068), (29, 0.013), (30, -0.075), (31, -0.142), (32, -0.009), (33, 0.083), (34, 0.015), (35, -0.061), (36, -0.083), (37, -0.044), (38, -0.107), (39, 0.083), (40, 0.009), (41, -0.089), (42, -0.009), (43, -0.001), (44, -0.002), (45, -0.044), (46, 0.018), (47, -0.044), (48, 0.11), (49, -0.121)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95058846 <a title="17-lsi-1" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>Author: Zhenhua Tian ; Hengheng Xiang ; Ziqi Liu ; Qinghua Zheng</p><p>Abstract: This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate’s smoothed preferences. Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements.</p><p>2 0.74416053 <a title="17-lsi-2" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>Author: Egoitz Laparra ; German Rigau</p><p>Abstract: This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling. The system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data.</p><p>3 0.71371317 <a title="17-lsi-3" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>Author: Tiziano Flati ; Roberto Navigli</p><p>Abstract: We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best f∗it) tahned ∗ argument. Taon idco this, we extract failtl thhee ∗ occurrences ion Wikipedia ewxthraiccht match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach.</p><p>4 0.64873618 <a title="17-lsi-4" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>Author: Oren Melamud ; Ido Dagan ; Jacob Goldberger ; Idan Szpektor</p><p>Abstract: Automatic acquisition of inference rules for predicates is widely addressed by computing distributional similarity scores between vectors of argument words. In this scheme, prior work typically refrained from learning rules for low frequency predicates associated with very sparse argument vectors due to expected low reliability. To improve the learning of such rules in an unsupervised way, we propose to lexically expand sparse argument word vectors with semantically similar words. Our evaluation shows that lexical expansion significantly improves performance in comparison to state-of-the-art baselines.</p><p>5 0.6040284 <a title="17-lsi-5" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>Author: Travis Wolfe ; Benjamin Van Durme ; Mark Dredze ; Nicholas Andrews ; Charley Beller ; Chris Callison-Burch ; Jay DeYoung ; Justin Snyder ; Jonathan Weese ; Tan Xu ; Xuchen Yao</p><p>Abstract: We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17% F1.</p><p>6 0.59772396 <a title="17-lsi-6" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>7 0.585379 <a title="17-lsi-7" href="./acl-2013-Improved_Lexical_Acquisition_through_DPP-based_Verb_Clustering.html">192 acl-2013-Improved Lexical Acquisition through DPP-based Verb Clustering</a></p>
<p>8 0.57978851 <a title="17-lsi-8" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>9 0.5090785 <a title="17-lsi-9" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>10 0.49939993 <a title="17-lsi-10" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>11 0.46791527 <a title="17-lsi-11" href="./acl-2013-Diathesis_alternation_approximation_for_verb_clustering.html">119 acl-2013-Diathesis alternation approximation for verb clustering</a></p>
<p>12 0.46093181 <a title="17-lsi-12" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>13 0.4529641 <a title="17-lsi-13" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>14 0.45199311 <a title="17-lsi-14" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>15 0.45100811 <a title="17-lsi-15" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>16 0.43433034 <a title="17-lsi-16" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>17 0.42776352 <a title="17-lsi-17" href="./acl-2013-Language_Acquisition_and_Probabilistic_Models%3A_keeping_it_simple.html">213 acl-2013-Language Acquisition and Probabilistic Models: keeping it simple</a></p>
<p>18 0.42547056 <a title="17-lsi-18" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>19 0.41292316 <a title="17-lsi-19" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>20 0.40582684 <a title="17-lsi-20" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.061), (6, 0.03), (11, 0.106), (15, 0.022), (24, 0.035), (26, 0.046), (28, 0.017), (35, 0.086), (42, 0.045), (48, 0.085), (64, 0.032), (70, 0.044), (82, 0.172), (88, 0.044), (90, 0.033), (95, 0.074)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86820459 <a title="17-lda-1" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>Author: Zhenhua Tian ; Hengheng Xiang ; Ziqi Liu ; Qinghua Zheng</p><p>Abstract: This paper presents an unsupervised random walk approach to alleviate data sparsity for selectional preferences. Based on the measure of preferences between predicates and arguments, the model aggregates all the transitions from a given predicate to its nearby predicates, and propagates their argument preferences as the given predicate’s smoothed preferences. Experimental results show that this approach outperforms several state-of-the-art methods on the pseudo-disambiguation task, and it better correlates with human plausibility judgements.</p><p>2 0.8598786 <a title="17-lda-2" href="./acl-2013-A_Stacking-based_Approach_to_Twitter_User_Geolocation_Prediction.html">20 acl-2013-A Stacking-based Approach to Twitter User Geolocation Prediction</a></p>
<p>Author: Bo Han ; Paul Cook ; Timothy Baldwin</p><p>Abstract: We implement a city-level geolocation prediction system for Twitter users. The system infers a user’s location based on both tweet text and user-declared metadata using a stacking approach. We demonstrate that the stacking method substantially outperforms benchmark methods, achieving 49% accuracy on a benchmark dataset. We further evaluate our method on a recent crawl of Twitter data to investigate the impact of temporal factors on model generalisation. Our results suggest that user-declared location metadata is more sensitive to temporal change than the text of Twitter messages. We also describe two ways of accessing/demoing our system.</p><p>3 0.82822108 <a title="17-lda-3" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>Author: Veronica Perez-Rosas ; Rada Mihalcea ; Louis-Philippe Morency</p><p>Abstract: During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. This paper presents a method for multimodal sentiment classification, which can identify the sentiment expressed in utterance-level visual datastreams. Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality.</p><p>4 0.74606258 <a title="17-lda-4" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>5 0.73736691 <a title="17-lda-5" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>Author: Tyler Baldwin ; Yunyao Li ; Bogdan Alexe ; Ioana R. Stanoi</p><p>Abstract: While the resolution of term ambiguity is important for information extraction (IE) systems, the cost of resolving each instance of an entity can be prohibitively expensive on large datasets. To combat this, this work looks at ambiguity detection at the term, rather than the instance, level. By making a judgment about the general ambiguity of a term, a system is able to handle ambiguous and unambiguous cases differently, improving throughput and quality. To address the term ambiguity detection problem, we employ a model that combines data from language models, ontologies, and topic modeling. Results over a dataset of entities from four product domains show that the proposed approach achieves significantly above baseline F-measure of 0.96.</p><p>6 0.73632056 <a title="17-lda-6" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>7 0.73308158 <a title="17-lda-7" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>8 0.73300773 <a title="17-lda-8" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>9 0.73206633 <a title="17-lda-9" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>10 0.7304275 <a title="17-lda-10" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>11 0.72953218 <a title="17-lda-11" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>12 0.72893107 <a title="17-lda-12" href="./acl-2013-DErivBase%3A_Inducing_and_Evaluating_a_Derivational_Morphology_Resource_for_German.html">102 acl-2013-DErivBase: Inducing and Evaluating a Derivational Morphology Resource for German</a></p>
<p>13 0.72803921 <a title="17-lda-13" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>14 0.72718471 <a title="17-lda-14" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>15 0.72513109 <a title="17-lda-15" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>16 0.72474074 <a title="17-lda-16" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>17 0.72471303 <a title="17-lda-17" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>18 0.72458547 <a title="17-lda-18" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>19 0.72228765 <a title="17-lda-19" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>20 0.72181445 <a title="17-lda-20" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
