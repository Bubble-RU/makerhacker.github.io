<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-196" href="#">acl2013-196</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</h1>
<br/><p>Source: <a title="acl-2013-196-pdf" href="http://aclweb.org/anthology//P/P13/P13-1049.pdf">pdf</a></p><p>Author: Emmanuel Lassalle ; Pascal Denis</p><p>Abstract: This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of average F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. B3,</p><p>Reference: <a title="acl-2013-196-reference" href="../acl2013_reference/acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Improving pairwise coreference models through feature space hierarchy learning Emmanuel Lassalle Alpage Project-team INRIA & Univ. [sent-1, score-0.829]
</p><p>2 a Abstract This paper proposes a new method for significantly improving the performance of pairwise coreference models. [sent-4, score-0.452]
</p><p>3 Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. [sent-5, score-0.405]
</p><p>4 In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. [sent-6, score-0.664]
</p><p>5 Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. [sent-7, score-0.545]
</p><p>6 In this kind of architecture, the performance of the entire coreference system strongly depends on the quality of the local pairwise classifier. [sent-19, score-0.452]
</p><p>7 1 Consequently, a lot of research effort on coreference resolution has focused on trying to boost the performance of the pairwise classifier. [sent-20, score-0.547]
</p><p>8 It is worth noting that, from a machine learning point of view, this is related to feature extraction in that both approaches in effect recast the pairwise classification problem in higher dimensional feature spaces. [sent-32, score-0.341]
</p><p>9 In this paper, we claim that mention pairs should not be processed by a single classifier, and instead should be handled through specific models. [sent-33, score-0.302]
</p><p>10 The main question we raise is, given a set of indicators (such as grammatical types, distance between two mentions, or named entity types), how to best partition the pool of mention pair examples in order to best discriminate coref-  erential pairs from non coreferential ones. [sent-36, score-0.83]
</p><p>11 We will see that this is also equivalent to selecting a single large adequate feature space by using the data. [sent-42, score-0.297]
</p><p>12 More importantly, we propose an original method for learning the best set of models that can be built from a given set of indicators and a training set. [sent-45, score-0.298]
</p><p>13 These models are organized in a hierarchy, wherein each leafcorresponds to a mutually disjoint subset of mention pair examples and the classifier that can be trained from it. [sent-46, score-0.3]
</p><p>14 Our method is exact in that it explores the full space of hierarchies (of size at least 22n) definable on an indicator sequence, while remaining scalable by exploiting the particular structure of these during the training of the distinct local models (Ng and Cardie, 2002; Uryupina, 2004). [sent-49, score-0.593]
</p><p>15 Section 2 discusses the underlying statistical hypotheses of the standard pairwise model and defines a simple alternative framework that uses a  simple separation of mention pairs based on grammatical types. [sent-55, score-0.617]
</p><p>16 Next, in section 3, we generalize the method by introducing indicator hierarchies and explain how to learn the best models associated with them. [sent-56, score-0.396]
</p><p>17 2  Modeling pairs  Pairwise models basically employ one local classifier to decide whether two mentions are coreferential or not. [sent-58, score-0.346]
</p><p>18 When using machine learning techniques, this involves certain assumptions about the statistical behavior of mention pairs. [sent-59, score-0.2]
</p><p>19 Given a document, the number of mentions is fixed and each pair ofmentions follows a certain distribution (that we partly observe in a feature space). [sent-62, score-0.251]
</p><p>20 The basic idea of pairwise models is to consider mention pairs independently from each other (that is why a  decoder is necessary to enforce transitivity). [sent-63, score-0.48]
</p><p>21 We claim that pairs should not be processed by a single classifier because they are not identically distributed (or a least the distribution is too complex for the classifier); rather, we should separate different “types” on pairs and create a specific model for each of them. [sent-65, score-0.325]
</p><p>22 For instance, some coreference resolution systems process different kinds of anaphors separately, which suggests for example that pairs containing an anaphoric pronoun behave differently from pairs with non498  pronominal anaphors. [sent-67, score-0.639]
</p><p>23 Instead of imposing heuristic product of features, we will show that a clever separation of instances leads to significant improvements of the pairwise model. [sent-69, score-0.302]
</p><p>24 Now we define a mapping: φF : X  →  F  x  →  x  that casts pairs into a feature space F through twhahitch ca we opbaisresrv ien othe am f. [sent-76, score-0.271]
</p><p>25 For technical coherence, we assume that φF1 (x(ω)) and φF2 (x(ω)) have the same values when projected on the feature space F1 ∩ F2: iwt means tjhecatt common efe faetautrueres f srpomac etw Fo f∩eat Fure spaces have the same values. [sent-78, score-0.383]
</p><p>26 From this formal point of view, the task of coreference resolution consists in fixing φF, observing labeled samples {(φF(x) , y)t}t∈TrainSet and, given partially so {bs(φerved new variables {(φF(x))t}t∈TestSet, recovering the corresponding values o}f y. [sent-79, score-0.388]
</p><p>27 2 Formalizing the statistical assumptions We claimed before that all mention pairs seemed not to be identically distributed since, for example, pronouns do not behave like nominals. [sent-82, score-0.367]
</p><p>28 In this way, we may separate positive and negative pairs more easily if we cast each kind of pair into a specific feature space. [sent-88, score-0.236]
</p><p>29 We can eituhse rc cllre tahtees etw feoa independent claasnsdifi Fers on F1 and F2 rto c process oe ianchde kpienndd eofn pair or dieerfsin oen a single mFodel on a larger feature space F = F1 ⊕ F2. [sent-90, score-0.332]
</p><p>30 From a theoretical viewpoint, the higher the dimension of the feature space (imagine taking the direct sum of all feature spaces), the more we get details on the distribution of mention pairs and the more we can expect to separate positives and negatives accurately. [sent-94, score-0.64]
</p><p>31 Finally, we seek a feature space situated between the two extremes of a space that is too big (sparseness) or too small (noisy data). [sent-96, score-0.397]
</p><p>32 The core of this work is to define a general method for choosing the most adequate space F among a huge numtbheer mofo possibilities w sphaecne we admo nonotg k ano hwug a priori which is the best. [sent-97, score-0.204]
</p><p>33 , 2006): gthrees smivoede (Pl Ale)a ranlgso a parameter vector w that defines a hyperplane that cuts the space into two parts. [sent-101, score-0.204]
</p><p>34 But we would like to define a method that adapts a feature space to the data by choosing the most adequate separation of pairs. [sent-110, score-0.4]
</p><p>35 3  Hierarchizing feature spaces  In this section, we have to keep in mind that separating the pairs in different models is the same as building a large feature space in which the parameter w can be learned by parts in independent subspaces. [sent-111, score-0.61]
</p><p>36 1 Indicators on pairs For establishing a structure on feature spaces, we use indicators which are deterministic functions on mention pairs with a small number of outputs. [sent-113, score-0.676]
</p><p>37 This is equivalent to using 9 different feature spaces F1, . [sent-135, score-0.223]
</p><p>38 , tFhe PA, this is also a single model with feature space F = F1 ⊕ · · · ⊕ F9. [sent-142, score-0.249]
</p><p>39 The small number of outputs of an indicator is required for practical reasons: if a category of pairs is too refined, the associated feature space will suffer from data sparsity. [sent-147, score-0.329]
</p><p>40 Accordingly, distance-based indicators must be approximated by coarse histograms. [sent-148, score-0.294]
</p><p>41 This operation produces a hierarchy of indicators which is exactly the structure we exploit in what follows. [sent-151, score-0.429]
</p><p>42 So a hierarchy is basically a sub-tree of the complete decision tree that contains copies of the same indicator at each level. [sent-155, score-0.301]
</p><p>43 If all the leaves of the decision tree have the 500  same depth, this corresponds to taking the Cartesian product of outputs of all indicators for indexing the categories. [sent-156, score-0.379]
</p><p>44 Figure 1: GRAMTYPE seen as a product-hierarchy Product-hierarchies will be the starting point of our method to find a feature space that fits the data. [sent-159, score-0.209]
</p><p>45 Now choosing a relevant sequence of indicators should be achieved through linguistic intuitions and theoretical work (gramtype separation is one of them). [sent-160, score-0.447]
</p><p>46 The system will find by itself the best usage of the indicators when optimizing the hierarchy. [sent-161, score-0.298]
</p><p>47 3 Relation with feature spaces Like we did for the GRAMTYPE model, we asso-  ciate a feature space Fi to each leaf of a hierarchy. [sent-164, score-0.466]
</p><p>48 Given a sequence of indicators, the numinb Fer of different hierarchies we can define is equal to the number of sub-trees of the complete decision tree (each non-leaf node having all its children). [sent-167, score-0.483]
</p><p>49 The minimal case is when all indicators are Boolean. [sent-168, score-0.261]
</p><p>50 So T(n) ≥ : even with small values of n, the nTu(mnb)er ≥ ≥o f2 different hierarchies (or large feature spaces) definable with a sequence of indicators is gigantic (e. [sent-170, score-0.744]
</p><p>51 4 Optimizing hierarchies Let us assume now that the sequence of indicators  22n  is fixed, and let n be its length. [sent-180, score-0.605]
</p><p>52 To find the best feature space among a very high number of possibilities, we need a criterion we can apply without too much additional computation. [sent-181, score-0.246]
</p><p>53 For that we only evaluate the feature space locally on pairs, i. [sent-182, score-0.209]
</p><p>54 Training the hierarchy Starting from the product-hierarchy, we associate a classifier and its proper feature space to each node of the tree5. [sent-188, score-0.5]
</p><p>55 Cutting down the hierarchy For the moment we have a complete tree with a classifier at each node. [sent-197, score-0.259]
</p><p>56 We use a dynamic programming technique to compute the best hierarchy by cutting this tree and only keeping classifiers situated at the leaf. [sent-198, score-0.556]
</p><p>57 It goes from the leaves to the root and cuts the subtree starting at a node whenever it does not pro5In the experiments, the classifiers use a copy of a same feature space, but not the same data, which corresponds to crossing the features with the categories of the decision tree. [sent-200, score-0.293]
</p><p>58 children ← ∅ endn end end Algorithm 1: Cutting down a hierarchy  =  × ×  Let us briefly discuss the correctness and complexity of the algorithm. [sent-212, score-0.218]
</p><p>59 (line 6) cuts the children of a node when they are not used in the best score. [sent-221, score-0.203]
</p><p>60 The algorithm thus propagates the best scores from the leaves to the root which finally gives a single score corresponding to the best hierarchy. [sent-222, score-0.223]
</p><p>61 Relation between cutting and the global feature space We can see the operation of cutting as replacing a group of subspaces by a single subspace in the sum (see figure 2). [sent-224, score-0.859]
</p><p>62 So cutting down the product-hierarchy amounts to reducing the global initial feature space in an optimal way. [sent-225, score-0.511]
</p><p>63 8In our experiments, cutting down the hierarchy was achieved very quickly, and the total training time was about five times longer than with a single model. [sent-226, score-0.457]
</p><p>64 Figure 2: Cutting down the hierarchy reduces the feature space  To sum up, the whole procedure is equivalent to training more than O(2n) perceptrons simultaneously agnd m selecting tOhe(2 2best performing. [sent-227, score-0.377]
</p><p>65 4  System description  Our system consists in the pairwise model obtained by cutting a hierarchy (the PA with selected feature space) and using a greedy decoder to create clusters from the output. [sent-228, score-0.808]
</p><p>66 2 Indicators As indicators we used: left and right grammatical types and subtypes, entity types, a boolean indicating if the mentions are in the same sentence, and a very coarse histogram of distance in terms of sentences. [sent-233, score-0.568]
</p><p>67 We systematically included right gramtype and left gramtype in the sequences and added other indicators, producing sequences of different lengths. [sent-234, score-0.718]
</p><p>68 The parameter was optimized by document categories using a development set after decoding the output of the pairwise model. [sent-235, score-0.234]
</p><p>69 3 Decoders We tested 3 classical greedy link selection strategies that form clusters from the classifier decision: Closest-First (merge mentions with their closest coreferent mention on the left) (Soon et al. [sent-237, score-0.428]
</p><p>70 , 2001), 502  Best-first (merge mentions with the mention on the left having the highest positive score) (Ng and Cardie, 2002; Bengston and Roth, 2008), and Aggressive-Merge (transitive closure on positive  pairs) (McCarthy and Lehnert, 1995). [sent-238, score-0.319]
</p><p>71 2 Settings Our baselines are a SINGLE MODEL, the GRAMTYPE model (section 2) and a RIGHT-TYPE model, defined as the first level of the gramtype product hierarchy (i. [sent-254, score-0.527]
</p><p>72 grammatical type of the anaphora (Morton, 2000)), with each greedy decoder and also the original sampling with a single model associated with those decoders. [sent-256, score-0.246]
</p><p>73 The obtained hierarchy is referred to as the BEST HIERARCHY in the results. [sent-258, score-0.2]
</p><p>74 This is a rather idealized setting but our focus is on comparing various pairwise local models rather than on building a full coreference resolution system. [sent-261, score-0.547]
</p><p>75 Global recall, precision, and F1 scores are obtained by averaging over the mention scores. [sent-271, score-0.263]
</p><p>76 The P/R/F1 pairwise scores before decoding are given in table 1. [sent-280, score-0.231]
</p><p>77 However we can mention that in our ex-  pwehraimt wenasts, o ubssienrvged hie onra rBch3iaensd h CaEdA aF p. [sent-283, score-0.2]
</p><p>78 Despite the use of greedy decoders, we observe a large positive effect of pair separation in the pairwise models on the outputs. [sent-315, score-0.393]
</p><p>79 Interestingly, we see that the increment in pairwise and global score are not proportional: for instance, the strong improvement of F1 between RIGHT-TYPE and GRAMTYPE results in a small amelioration of the global score. [sent-320, score-0.265]
</p><p>80 We observed that product-hierarchies did not performed well without cutting (especially when using longer sequences of indicators, because of data sparsity) and could obtain scores lower than the single model. [sent-322, score-0.32]
</p><p>81 Hopefully, after cutting them the results always became better as the resulting hierarchy was more balanced. [sent-323, score-0.417]
</p><p>82 Looking at the different metrics, we notice that overall, pair separation improves B3 and CEAF (but not always MUC) after decoding the output: GRAMTYPE provides a better mean score than the single model, and BEST HIERARCHY gives the highest B3, CEAF and mean score. [sent-324, score-0.265]
</p><p>83 6  Conclusion and perspectives  In this paper, we described a method for selecting a feature space among a very large number of choices by using linearity and by combining indicators to separate the instances. [sent-331, score-0.547]
</p><p>84 We employed dynamic programming on hierarchies of indicators to compute the feature space providing the best  pairwise classifications efficiently. [sent-332, score-0.967]
</p><p>85 We applied this 504  method to optimize the pairwise model of a coreference resolution system. [sent-333, score-0.547]
</p><p>86 In the future we will apply the hierarchies on finer feature spaces to make more accurate optimizations. [sent-336, score-0.524]
</p><p>87 Observing that the general method of cutting down hierarchies is not restricted to modeling mention pairs, but can be applied to problems having Boolean aspects, we aim at employing hierarchies to address other tasks in computational linguistics (e. [sent-337, score-1.051]
</p><p>88 So, a natural extension of this work is to combine our method for learning pairwise models with more  sophisticated decoding strategies (like Bestcut or using ILP). [sent-341, score-0.2]
</p><p>89 Then we can test the impact of hierarchies with more realistic settings. [sent-342, score-0.301]
</p><p>90 Finally, the method for cutting hierarchies should be compared to more general but similar methods, for instance polynomial kernels for SVM and tree-based methods (Hastie et al. [sent-343, score-0.585]
</p><p>91 Instead of cutting product-hierarchies, we will employ usual techniques to build decision trees10 and apply our cutting method on their structure. [sent-346, score-0.541]
</p><p>92 The objective is twofold: first, we will get rid of the sequence of indicators as parameter. [sent-347, score-0.304]
</p><p>93 10(Bansal and Klein, 2012) show good performances ofdecision trees on coreference resolution. [sent-361, score-0.293]
</p><p>94 A unified event coreference resolution by integrating multiple resolvers. [sent-377, score-0.388]
</p><p>95 Global joint models for coreference resolution and named entity classification. [sent-390, score-0.421]
</p><p>96 Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. [sent-446, score-0.293]
</p><p>97 Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes. [sent-451, score-0.325]
</p><p>98 Narrowing the modeling gap: a cluster-ranking approach to coreference resolution. [sent-456, score-0.293]
</p><p>99 A  machine learning approach to coreference resolution of noun phrases. [sent-473, score-0.388]
</p><p>100 Disambiguation and filtering methods in using web knowledge for coreference resolution. [sent-477, score-0.293]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gramtype', 0.359), ('hierarchies', 0.301), ('coreference', 0.293), ('indicators', 0.261), ('cutting', 0.249), ('mention', 0.2), ('hierarchy', 0.168), ('pairwise', 0.159), ('separation', 0.143), ('spaces', 0.132), ('bengston', 0.127), ('mentions', 0.119), ('space', 0.118), ('coreferential', 0.106), ('ceaf', 0.096), ('uryupina', 0.096), ('resolution', 0.095), ('denis', 0.092), ('feature', 0.091), ('separating', 0.082), ('situated', 0.07), ('node', 0.064), ('nicolae', 0.063), ('pairs', 0.062), ('decoders', 0.062), ('identically', 0.06), ('classifier', 0.059), ('decoder', 0.059), ('subspaces', 0.059), ('indicator', 0.058), ('soon', 0.058), ('global', 0.053), ('grammatical', 0.053), ('cuts', 0.052), ('children', 0.05), ('greedy', 0.05), ('muc', 0.05), ('adequate', 0.048), ('definable', 0.048), ('inria', 0.048), ('roth', 0.048), ('cardie', 0.045), ('behave', 0.045), ('anaphora', 0.044), ('pronoun', 0.044), ('decision', 0.043), ('sequence', 0.043), ('leaves', 0.043), ('kehler', 0.042), ('etw', 0.042), ('anaphoricity', 0.042), ('bestcut', 0.042), ('lehnert', 0.042), ('morton', 0.042), ('vilain', 0.042), ('wt', 0.042), ('ng', 0.042), ('separate', 0.042), ('decoding', 0.041), ('pair', 0.041), ('single', 0.04), ('versley', 0.039), ('anaphoric', 0.038), ('possibilities', 0.038), ('boolean', 0.038), ('paris', 0.038), ('mccarthy', 0.038), ('best', 0.037), ('ais', 0.037), ('olga', 0.037), ('bagga', 0.037), ('yij', 0.037), ('positives', 0.036), ('crammer', 0.036), ('explores', 0.035), ('instance', 0.035), ('propagates', 0.035), ('pij', 0.035), ('recasens', 0.035), ('linearity', 0.035), ('metrics', 0.035), ('leaf', 0.034), ('parameter', 0.034), ('hastie', 0.033), ('rahman', 0.033), ('distinct', 0.033), ('entity', 0.033), ('coarse', 0.033), ('tree', 0.032), ('obtained', 0.032), ('eins', 0.032), ('baldridge', 0.032), ('typing', 0.032), ('antecedent', 0.032), ('intersection', 0.032), ('pascal', 0.032), ('shared', 0.032), ('pa', 0.032), ('scores', 0.031), ('types', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="196-tfidf-1" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>Author: Emmanuel Lassalle ; Pascal Denis</p><p>Abstract: This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of average F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. B3,</p><p>2 0.32279846 <a title="196-tfidf-2" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>Author: Sebastian Martschat</p><p>Abstract: We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task.</p><p>3 0.25801012 <a title="196-tfidf-3" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>Author: Nathan Gilbert ; Ellen Riloff</p><p>Abstract: Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.</p><p>4 0.18297504 <a title="196-tfidf-4" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>Author: Greg Durrett ; David Hall ; Dan Klein</p><p>Abstract: Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.</p><p>5 0.16752736 <a title="196-tfidf-5" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>Author: Xiaohua Liu ; Yitong Li ; Haocheng Wu ; Ming Zhou ; Furu Wei ; Yi Lu</p><p>Abstract: We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.</p><p>6 0.1590136 <a title="196-tfidf-6" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>7 0.12411423 <a title="196-tfidf-7" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>8 0.11803173 <a title="196-tfidf-8" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>9 0.099507466 <a title="196-tfidf-9" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>10 0.093135461 <a title="196-tfidf-10" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>11 0.09140256 <a title="196-tfidf-11" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>12 0.082410552 <a title="196-tfidf-12" href="./acl-2013-Joint_Apposition_Extraction_with_Syntactic_and_Semantic_Constraints.html">205 acl-2013-Joint Apposition Extraction with Syntactic and Semantic Constraints</a></p>
<p>13 0.081740856 <a title="196-tfidf-13" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>14 0.080771163 <a title="196-tfidf-14" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>15 0.080321416 <a title="196-tfidf-15" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>16 0.075459108 <a title="196-tfidf-16" href="./acl-2013-Online_Relative_Margin_Maximization_for_Statistical_Machine_Translation.html">264 acl-2013-Online Relative Margin Maximization for Statistical Machine Translation</a></p>
<p>17 0.072802283 <a title="196-tfidf-17" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>18 0.069009416 <a title="196-tfidf-18" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>19 0.06880296 <a title="196-tfidf-19" href="./acl-2013-GuiTAR-based_Pronominal_Anaphora_Resolution_in_Bengali.html">177 acl-2013-GuiTAR-based Pronominal Anaphora Resolution in Bengali</a></p>
<p>20 0.068330854 <a title="196-tfidf-20" href="./acl-2013-Fast_and_Adaptive_Online_Training_of_Feature-Rich_Translation_Models.html">156 acl-2013-Fast and Adaptive Online Training of Feature-Rich Translation Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.22), (1, 0.025), (2, -0.023), (3, -0.11), (4, 0.014), (5, 0.192), (6, 0.021), (7, 0.108), (8, -0.027), (9, 0.051), (10, 0.017), (11, -0.113), (12, -0.089), (13, -0.048), (14, -0.073), (15, 0.127), (16, -0.149), (17, 0.24), (18, -0.102), (19, 0.119), (20, -0.1), (21, 0.144), (22, 0.0), (23, -0.152), (24, -0.013), (25, 0.029), (26, 0.011), (27, 0.084), (28, 0.117), (29, -0.012), (30, 0.074), (31, 0.021), (32, 0.049), (33, -0.033), (34, 0.029), (35, -0.014), (36, -0.021), (37, 0.045), (38, -0.041), (39, -0.029), (40, -0.016), (41, 0.019), (42, 0.01), (43, 0.053), (44, -0.018), (45, 0.009), (46, 0.026), (47, 0.01), (48, 0.005), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93210989 <a title="196-lsi-1" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>Author: Sebastian Martschat</p><p>Abstract: We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task.</p><p>same-paper 2 0.92883891 <a title="196-lsi-2" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>Author: Emmanuel Lassalle ; Pascal Denis</p><p>Abstract: This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of average F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. B3,</p><p>3 0.92315501 <a title="196-lsi-3" href="./acl-2013-Decentralized_Entity-Level_Modeling_for_Coreference_Resolution.html">106 acl-2013-Decentralized Entity-Level Modeling for Coreference Resolution</a></p>
<p>Author: Greg Durrett ; David Hall ; Dan Klein</p><p>Abstract: Efficiently incorporating entity-level information is a challenge for coreference resolution systems due to the difficulty of exact inference over partitions. We describe an end-to-end discriminative probabilistic model for coreference that, along with standard pairwise features, enforces structural agreement constraints between specified properties of coreferent mentions. This model can be represented as a factor graph for each document that admits efficient inference via belief propagation. We show that our method can use entity-level information to outperform a basic pairwise system.</p><p>4 0.90916675 <a title="196-lsi-4" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>Author: Nathan Gilbert ; Ellen Riloff</p><p>Abstract: Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.</p><p>5 0.70921296 <a title="196-lsi-5" href="./acl-2013-GuiTAR-based_Pronominal_Anaphora_Resolution_in_Bengali.html">177 acl-2013-GuiTAR-based Pronominal Anaphora Resolution in Bengali</a></p>
<p>Author: Apurbalal Senapati ; Utpal Garain</p><p>Abstract: This paper attempts to use an off-the-shelf anaphora resolution (AR) system for Bengali. The language specific preprocessing modules of GuiTAR (v3.0.3) are identified and suitably designed for Bengali. Anaphora resolution module is also modified or replaced in order to realize different configurations of GuiTAR. Performance of each configuration is evaluated and experiment shows that the off-the-shelf AR system can be effectively used for Indic languages. 1</p><p>6 0.63978112 <a title="196-lsi-6" href="./acl-2013-Joint_Apposition_Extraction_with_Syntactic_and_Semantic_Constraints.html">205 acl-2013-Joint Apposition Extraction with Syntactic and Semantic Constraints</a></p>
<p>7 0.62174058 <a title="196-lsi-7" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>8 0.55942488 <a title="196-lsi-8" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>9 0.53764403 <a title="196-lsi-9" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>10 0.52905953 <a title="196-lsi-10" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>11 0.50886023 <a title="196-lsi-11" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>12 0.49747708 <a title="196-lsi-12" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>13 0.45949098 <a title="196-lsi-13" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>14 0.43849382 <a title="196-lsi-14" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>15 0.42338568 <a title="196-lsi-15" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>16 0.40667757 <a title="196-lsi-16" href="./acl-2013-Learning_Non-linear_Features_for_Machine_Translation_Using_Gradient_Boosting_Machines.html">221 acl-2013-Learning Non-linear Features for Machine Translation Using Gradient Boosting Machines</a></p>
<p>17 0.39055553 <a title="196-lsi-17" href="./acl-2013-A_Novel_Classifier_Based_on_Quantum_Computation.html">14 acl-2013-A Novel Classifier Based on Quantum Computation</a></p>
<p>18 0.38588265 <a title="196-lsi-18" href="./acl-2013-Recognizing_Identical_Events_with_Graph_Kernels.html">296 acl-2013-Recognizing Identical Events with Graph Kernels</a></p>
<p>19 0.38072169 <a title="196-lsi-19" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>20 0.37639034 <a title="196-lsi-20" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.051), (6, 0.039), (11, 0.072), (14, 0.011), (15, 0.016), (24, 0.037), (26, 0.06), (35, 0.076), (42, 0.046), (48, 0.057), (64, 0.015), (70, 0.049), (71, 0.012), (80, 0.199), (88, 0.076), (90, 0.04), (95, 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93436146 <a title="196-lda-1" href="./acl-2013-A_Novel_Classifier_Based_on_Quantum_Computation.html">14 acl-2013-A Novel Classifier Based on Quantum Computation</a></p>
<p>Author: Ding Liu ; Xiaofang Yang ; Minghu Jiang</p><p>Abstract: In this article, we propose a novel classifier based on quantum computation theory. Different from existing methods, we consider the classification as an evolutionary process of a physical system and build the classifier by using the basic quantum mechanics equation. The performance of the experiments on two datasets indicates feasibility and potentiality of the quantum classifier.</p><p>2 0.8883574 <a title="196-lda-2" href="./acl-2013-Learning_to_lemmatise_Polish_noun_phrases.html">227 acl-2013-Learning to lemmatise Polish noun phrases</a></p>
<p>Author: Adam Radziszewski</p><p>Abstract: We present a novel approach to noun phrase lemmatisation where the main phase is cast as a tagging problem. The idea draws on the observation that the lemmatisation of almost all Polish noun phrases may be decomposed into transformation of singular words (tokens) that make up each phrase. We perform evaluation, which shows results similar to those obtained earlier by a rule-based system, while our approach allows to separate chunking from lemmatisation.</p><p>3 0.87970001 <a title="196-lda-3" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>Author: Song Feng ; Jun Seok Kang ; Polina Kuznetsova ; Yejin Choi</p><p>Abstract: Understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text, as seemingly objective statements often allude nuanced sentiment of the writer, and even purposefully conjure emotion from the readers’ minds. The focus of this paper is drawing nuanced, connotative sentiments from even those words that are objective on the surface, such as “intelligence ”, “human ”, and “cheesecake ”. We propose induction algorithms encoding a diverse set of linguistic insights (semantic prosody, distributional similarity, semantic parallelism of coordination) and prior knowledge drawn from lexical resources, resulting in the first broad-coverage connotation lexicon.</p><p>same-paper 4 0.8261084 <a title="196-lda-4" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>Author: Emmanuel Lassalle ; Pascal Denis</p><p>Abstract: This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of average F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. B3,</p><p>5 0.77093601 <a title="196-lda-5" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>Author: Pavel Braslavski ; Alexander Beloborodov ; Maxim Khalilov ; Serge Sharoff</p><p>Abstract: This paper presents the settings and the results of the ROMIP 2013 MT shared task for the English→Russian language directfioorn. t Teh Een quality Rofu generated utraagnsel datiiroencswas assessed using automatic metrics and human evaluation. We also discuss ways to reduce human evaluation efforts using pairwise sentence comparisons by human judges to simulate sort operations.</p><p>6 0.75197136 <a title="196-lda-6" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>7 0.67893845 <a title="196-lda-7" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>8 0.67859244 <a title="196-lda-8" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>9 0.67270291 <a title="196-lda-9" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>10 0.67034811 <a title="196-lda-10" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>11 0.66851199 <a title="196-lda-11" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>12 0.66795152 <a title="196-lda-12" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>13 0.66484338 <a title="196-lda-13" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>14 0.66397738 <a title="196-lda-14" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>15 0.66200495 <a title="196-lda-15" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>16 0.66108304 <a title="196-lda-16" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>17 0.65995598 <a title="196-lda-17" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<p>18 0.65914661 <a title="196-lda-18" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>19 0.65849274 <a title="196-lda-19" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>20 0.65817451 <a title="196-lda-20" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
