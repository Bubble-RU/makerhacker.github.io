<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 acl-2013-Beam Search for Solving Substitution Ciphers</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-66" href="#">acl2013-66</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>66 acl-2013-Beam Search for Solving Substitution Ciphers</h1>
<br/><p>Source: <a title="acl-2013-66-pdf" href="http://aclweb.org/anthology//P/P13/P13-1154.pdf">pdf</a></p><p>Author: Malte Nuhn ; Julian Schamper ; Hermann Ney</p><p>Abstract: In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet- ter (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.</p><p>Reference: <a title="acl-2013-66-reference" href="../acl2013_reference/acl-2013-Beam_Search_for_Solving_Substitution_Ciphers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. [sent-2, score-0.955]
</p><p>2 We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38. [sent-3, score-0.555]
</p><p>3 13% decipherment error in a fraction of time by using a 6-gram language model. [sent-5, score-0.48]
</p><p>4 We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet-  ter (and near to optimal) results than previously published. [sent-6, score-0.512]
</p><p>5 Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7. [sent-8, score-1.137]
</p><p>6 de  decipherment accuracy of the proposed algorithms is still low. [sent-16, score-0.388]
</p><p>7 Improving the core decipherment algorithms is an important step for making decipherment techniques useful for practical applications. [sent-17, score-0.776]
</p><p>8 In this paper we present an effective beam search algorithm which provides high decipherment accuracies while having low computational requirements. [sent-18, score-0.674]
</p><p>9 We show significant improvements in decipherment accuracy in a variety of experiments while being computationally more effective than previous published works. [sent-20, score-0.426]
</p><p>10 2  Related Work  The experiments proposed in this paper touch many of previously published works in the decipherment field. [sent-21, score-0.426]
</p><p>11 Regarding the decipherment of 1:1 substitution ciphers various works have been published: Most older papers do not use a statistical approach and instead define some heuristic measures for scoring  candidate decipherments. [sent-22, score-0.942]
</p><p>12 Approaches like (Hart, 1994) and (Olson, 2007) use a dictionary to check if a decipherment is useful. [sent-23, score-0.406]
</p><p>13 We use our own implementation of these methods to report optimal solutions to 1:1 substitution ci1568  ProceedingsS ooffita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-26, score-0.308]
</p><p>14 (Ravi and Knight, 2011a) report the first automatic decipherment of the Zodiac-408 cipher. [sent-29, score-0.388]
</p><p>15 We run our beam  search approach on the same cipher and report better results without using an additional word dictionary—just by using a high order n-gram language model. [sent-31, score-0.823]
</p><p>16 (Ravi and Knight, 2011b) report experiments on large vocabulary substitution ciphers based on the Transtac corpus. [sent-32, score-0.583]
</p><p>17 (Dou and Knight, 2012) improve upon these results and provide state-of-the-art results on a large vocabulary word substitution cipher based on the Gigaword corpus. [sent-33, score-0.848]
</p><p>18 Even though this work is currently only able to deal with substitution ciphers, phenomena like reordering, insertions and deletions can in principle be included in our approach. [sent-37, score-0.283]
</p><p>19 3  Definitions  In the following we will use the machine translation notation and denote the ciphertext with f1N = f1. [sent-38, score-0.187]
</p><p>20 A general substitution cipher uses a table s(e|f) which contains for each cipher token f a probability cthha ct othntea tionske fno f eisa shub csitpihtuetred to wkeitnh fthe a plaintext token e. [sent-54, score-1.514]
</p><p>21 Such a table for substituting cipher tokens {A, B, C, D} with plaintext tokens {a, b, c, d} cnosu {ldA ,foBr example loithok p lliakien a b c d A0. [sent-55, score-0.683]
</p><p>22 1  The 1:1 substitution cipher encrypts a given plaintext into a ciphertext by replacing each plaintext token with a unique substitute: This means that the table s(e|f) contains all zeroes, except for one t“h1e. [sent-71, score-1.326]
</p><p>23 We formalize the 1:1 substitutions with a bijective function φ : Vf → Ve and homophonic substitutions with a general Vfunction φ : Vf → Ve. [sent-78, score-0.205]
</p><p>24 Following (Corlett and Penn, 2010), we call cipher functions φ, for which not all φ(f) ’s are fixed, partial cipher functions . [sent-79, score-1.111]
</p><p>25 4  Beam Search  In this Section we present our beam search approach to solving Equation 3. [sent-91, score-0.281]
</p><p>26 1 General Algorithm Figure 1 shows the general structure of the beam search algorithm for the decipherment of substitution ciphers. [sent-95, score-0.957]
</p><p>27 The general idea is to keep track of all partial hypotheses in two arrays Hs and Ht. [sent-96, score-0.138]
</p><p>28 During search all possible extensions of the partial hypotheses in Hs are generated and scored. [sent-97, score-0.216]
</p><p>29 Here, the function EXT ORDER chooses which cipher symbol is used next for extension, EXT LIMITS decides which extensions are allowed, and SCORE scores the new partial hypotheses. [sent-98, score-0.7]
</p><p>30 Due to the structure of the algorithm the cardinality of all hypotheses in Hs increases in each step. [sent-101, score-0.197]
</p><p>31 Thus only hypotheses of the same cardinality 1shorthand notation for φ0 extends φ  1: function  BEAM SEARCH(EXT ORDER,  2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21:  EXT LIMITS, PRUNE) init sets Hs, Ht CARDINALITY = 0 Hs. [sent-102, score-0.19]
</p><p>32 CLEAR() end while return best scoring cipher function in Hs end function  Figure 1: The general structure of the beam  search algorithm for decipherment of substitution ciphers. [sent-105, score-1.469]
</p><p>33 When Hs contains full cipher relations, the cipher relation with the maximal score is returned. [sent-108, score-1.052]
</p><p>34 2 Figure 2 illustrates how the algorithm explores the search space for a homophonic substitution cipher. [sent-109, score-0.53]
</p><p>35 fSin ∈ce V partial hypotheses violating this condition can never “recover” when being extended, it becomes clear that these partial  hypotheses can be left out from search. [sent-113, score-0.293]
</p><p>36 2n-best output can be implemented by returning  the n best  scoring hypotheses in the final array Hs. [sent-114, score-0.125]
</p><p>37 At each level only those 4 hypotheses that survived the histogram pruning process are extended. [sent-118, score-0.291]
</p><p>38 Homophonic  substitution  ciphers  dled by the beam search algorithm, the condition that  φ  fulfill  must  can be hantoo. [sent-119, score-0.833]
</p><p>39 Here  is that the num-  ber of cipher letters  f  ∈  Vf that map to any  e  nmax  (which we will call  ∈  Ve is at most  this condition is violated, all further extensions will also EXT LIMITS HOMOPHONIC). [sent-120, score-0.679]
</p><p>40 Given a partial hypothesis φ with given SCORE(φ) the score of an extension φ0 can be calculated as  ≤  SCORE(φ0) = SCORE(φ) + NEWLY FIXED(φ, φ0) (8) where NEWLY FIXED only includes scores for n-grams that have been newly fixed in φ0 during the extension step from φ to φ0. [sent-136, score-0.261]
</p><p>41 4 Extension Order (EXT ORDER) For the choice which ciphertext symbol should be fixed next during search, several possibilities exist: The overall goal is to choose an extension order that leads to an overall low error rate. [sent-138, score-0.468]
</p><p>42 It is also clear that the choice of a good extension order is dependent on the score estimation function SCORE: The extension order should lead to informative scores early on so that misleading hypotheses can be pruned out early. [sent-140, score-0.305]
</p><p>43 In most of our experiments we will make use of a very simple extension order: HIGHEST UNIGRAM FREQUENCY simply fixes  the most frequent symbols first. [sent-141, score-0.143]
</p><p>44 In each step it greedily chooses the symbol that will maximize the number of fixed ciphertext n-grams. [sent-143, score-0.334]
</p><p>45 5 Pruning (PRUNE) We propose two pruning methods: HISTOGRAM PRUNING sorts all hypotheses according to their score and then keeps only the best nkeep hypotheses. [sent-146, score-0.279]
</p><p>46 THRESHOLD PRUNING keeps only those hypotheses φkeep for which SCORE(φkeep)  ≥ SCORE(φbest)  −β  (9)  holds for a given parameter β ≥ 0. [sent-147, score-0.112]
</p><p>47 The basic idea is to run a decipherment algorithm—in their case an EM algorithm based approach—on a subset of the vocabulary. [sent-151, score-0.439]
</p><p>48 After having obtained the results from the restricted vocabulary run, these results are used to initialize a decipherment run with a larger vocabulary. [sent-152, score-0.469]
</p><p>49 The results from this run will then be used for a further decipherment run with an even larger vocabulary and so on. [sent-153, score-0.497]
</p><p>50 In our large vocabulary word substitution cipher experiments we iteratively increase the vocabulary from the 1000 most frequent words, until we finally reach the 50000 most frequent words. [sent-154, score-0.951]
</p><p>51 6  Experimental Evaluation  We conduct experiments on letter based 1:1 substitution ciphers, the homophonic substitution ci-  pher Zodiac-408, and word based 1:1 substitution ciphers. [sent-155, score-1.085]
</p><p>52 Roughly speaking, SER reports the fraction of symbols in the deciphered text that are not correct, while MER reports the fraction of incorrect mappings in φ. [sent-157, score-0.144]
</p><p>53 In decipherment experiments, SER will often be lower than MER, since it is often easier to decipher frequent words. [sent-159, score-0.436]
</p><p>54 1 Letter Substitution Ciphers As ciphertext we use the text of the English Wikipedia article about History4, remove all pictures, tables, and captions, convert all letters to lowercase, and then remove all non-letter and nonspace symbols. [sent-161, score-0.209]
</p><p>55 We create the ciphertext using a 1:1 substitution cipher in which we fix the mapping of the space symbol ’ ’ . [sent-164, score-1.102]
</p><p>56 85  Table 1: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for decipherment  of letter  substitution ciphers of length 128. [sent-230, score-1.576]
</p><p>57 Results for  beam size “∞” were obtained using A∗ search. [sent-232, score-0.229]
</p><p>58 Note that fixing the symbol makes the problem much easier: The exact methods show much higher computational demands for lengths beyond 256 letters when not fixing the space symbol. [sent-234, score-0.195]
</p><p>59 The plaintext language model we use a letter based (Ve = {a, . [sent-235, score-0.234]
</p><p>60 We use extension limits fitting the 1: 1 substitution cipher nmax = 1 and histogram pruning with different beam sizes. [sent-242, score-1.443]
</p><p>61 Figure 3 shows the results of our algorithm for different cipher length. [sent-244, score-0.535]
</p><p>62 We use a beam size of 100k for the 4, 5 and 6-gram case. [sent-245, score-0.229]
</p><p>63 Most remarkably our 6-gram beam search results are significantly better than all methods presented in the lit-  ’’  erature. [sent-246, score-0.263]
</p><p>64 For the cipher length of 32 we obtain a symbol error rate of just 4. [sent-247, score-0.719]
</p><p>65 without search errors) for a 3-gram  Cipher Length  Figure 3: Symbol error rates for decipherment of letter substitution ciphers of different lengths. [sent-250, score-1.163]
</p><p>66 Error bars show the 95% confidence interval based on decipherment on 50 different ciphers. [sent-251, score-0.388]
</p><p>67 Beam search was performed with a beam size of “100k”. [sent-252, score-0.28]
</p><p>68 language model has a symbol error rate as high as 38. [sent-253, score-0.207]
</p><p>69 Table 1 shows error rates and runtimes of our algorithm for different beam sizes and language model orders given a fixed ciphertext length of 128 letters. [sent-255, score-0.722]
</p><p>70 scores  if only  To summarize: The beam search method is significantly faster and obtains significantly better results than previously published methods. [sent-258, score-0.301]
</p><p>71 Furthermore it offers a good trade-off between CPU time and decipherment accuracy. [sent-259, score-0.388]
</p><p>72 2 Zodiac-408 Cipher As ciphertext we use a transcription of the Zodiac-408 cipher. [sent-262, score-0.171]
</p><p>73 Furthermore, the last 17 letters of the cipher do not form understandable English when applying the same homophonic substitution that deciphers the rest of the cipher. [sent-269, score-1.006]
</p><p>74 This makes the Zodiac-408 a good candidate for testing the robustness of a decipherment algorithm. [sent-270, score-0.388]
</p><p>75 We assume a homophonic substitution cipher, even though the cipher is not strictly homophonic: It contains three cipher symbols that correspond to two or more plaintext symbols. [sent-271, score-1.706]
</p><p>76 We ignore this fact for our experiments, and count—in case of the MER only—the decipherment for these symbols  as correct when the obtained mapping is contained in the set of reference symbols. [sent-272, score-0.477]
</p><p>77 We use extension limits with nmax  = 8 and histogram pruning  with beam sizes of 10k up to 10M. [sent-273, score-0.67]
</p><p>78 The plaintext language model is based on the same subset of Gigaword (Graff et al. [sent-274, score-0.171]
</p><p>79 , 2007) data as the experiments  for the letter substitution ci-  phers. [sent-275, score-0.346]
</p><p>80 96  262 1992 17 701 167 181  Table 2: Symbol error rates (SER), Mapping error rates (MER) and runtimes (RT) in dependence of language model order (ORDER) and histogram pruning size (BEAM) for the decipherment of the Zodiac-408 cipher. [sent-300, score-0.983]
</p><p>81 Figure 4 shows the first parts of the cipher and our best decipherment. [sent-304, score-0.512]
</p><p>82 Table 2 shows the results of our algorithm on the Zodiac-408 cipher for different language model orders and pruning settings. [sent-305, score-0.678]
</p><p>83 To summarize: Our final decipherment—for which we only use a 6-gram language model—has a symbol error rate of only 2. [sent-306, score-0.207]
</p><p>84 0%, which is slightly better than the best decipherment reported in (Ravi and Knight, 2011a). [sent-307, score-0.388]
</p><p>85 They used an n-gram lan-  guage model together with a word dictionary and obtained a symbol error rate of 2. [sent-308, score-0.225]
</p><p>86 We run experiments for three different setups: The “JRC” and “Gigaword” setups use the first half of the respective corpus as ciphertext, while the plaintext language model of order n = 3 was 1574  Setup  Top  Gigaword  1k 10k 20k 50k  MER  [%]  81. [sent-317, score-0.241]
</p><p>87 58  00h 31m 13h 03m  Table 3: Word error rates (WER), Mapping error rates (MER) and runtimes (RT) for iterative decipherment run on the (TOP) most frequent words. [sent-333, score-0.818]
</p><p>88 We encrypt the ciphertext using a 1:1 substitution cipher on word level, imposing a much larger vocabulary size. [sent-338, score-1.019]
</p><p>89 We use histogram pruning with a beam size of 128 and use extension limits of nmax = 1. [sent-339, score-0.665]
</p><p>90 Different to the previous experiments, we use iterative beam search with iterations as shown in Table 3. [sent-340, score-0.296]
</p><p>91 The results for the Gigaword task are directly comparable to the word substitution experiments presented in (Dou and Knight, 2012). [sent-341, score-0.283]
</p><p>92 Their final decipherment has a symbol error rate of 7. [sent-342, score-0.595]
</p><p>93 8% symbol error rate correspond to a larger improvement in terms of mapping error rate. [sent-347, score-0.308]
</p><p>94 This can also be seen when looking at Table 3: An improvement of the symbol error rate from 6. [sent-348, score-0.207]
</p><p>95 96% corresponds to an improvement of mapping error rate from 21. [sent-350, score-0.139]
</p><p>96 To summarize: Using our beam search algorithm in an iterative fashion, we are able to improve the state-of-the-art decipherment accuracy for word substitution ciphers. [sent-353, score-0.99]
</p><p>97 7  Conclusion  We have presented a simple and effective beam search approach to the decipherment problem. [sent-354, score-0.651]
</p><p>98 We have shown in a variety of experiments—letter substitution ciphers, the Zodiac-408, and word  substitution ciphers—that our approach outperforms the current state of the art while being conceptually simpler and keeping computational demands low. [sent-355, score-0.602]
</p><p>99 We want to note that the presented algorithm is not restricted to 1:1 and homophonic substitution ciphers: It is possible to extend the algorithm to solve n:m mappings. [sent-356, score-0.502]
</p><p>100 Along with more sophisticated pruning strategies, score estimation functions, and extension orders, this will be left for future research. [sent-357, score-0.214]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cipher', 0.512), ('decipherment', 0.388), ('substitution', 0.283), ('ciphers', 0.247), ('beam', 0.212), ('mer', 0.193), ('ext', 0.173), ('homophonic', 0.173), ('plaintext', 0.171), ('ciphertext', 0.171), ('vf', 0.119), ('ser', 0.113), ('ravi', 0.108), ('pruning', 0.107), ('symbol', 0.102), ('hypotheses', 0.095), ('limits', 0.092), ('histogram', 0.089), ('nmax', 0.085), ('runtimes', 0.082), ('jrc', 0.08), ('cardinality', 0.079), ('hs', 0.073), ('error', 0.067), ('rates', 0.064), ('letter', 0.063), ('extension', 0.063), ('ve', 0.061), ('gigaword', 0.061), ('knight', 0.06), ('corlett', 0.057), ('symbols', 0.055), ('vocabulary', 0.053), ('ref', 0.051), ('search', 0.051), ('fixed', 0.045), ('partial', 0.043), ('prune', 0.04), ('deciphered', 0.039), ('fj', 0.038), ('rt', 0.038), ('letters', 0.038), ('published', 0.038), ('rate', 0.038), ('aachen', 0.037), ('nuhn', 0.037), ('graff', 0.037), ('ht', 0.037), ('orders', 0.036), ('deciphering', 0.035), ('mapping', 0.034), ('dou', 0.034), ('iterative', 0.033), ('nkeep', 0.032), ('veval', 0.032), ('sujith', 0.031), ('array', 0.03), ('cryptograms', 0.028), ('enciphered', 0.028), ('score', 0.028), ('run', 0.028), ('extensions', 0.027), ('optimal', 0.025), ('fn', 0.025), ('frequent', 0.025), ('fraction', 0.025), ('malte', 0.025), ('heuristics', 0.024), ('heuristic', 0.024), ('optimally', 0.023), ('rwth', 0.023), ('decipher', 0.023), ('algorithm', 0.023), ('summarize', 0.023), ('fulfill', 0.023), ('functions', 0.022), ('sizes', 0.022), ('steinberger', 0.022), ('setups', 0.022), ('order', 0.02), ('demands', 0.019), ('newly', 0.019), ('cpu', 0.018), ('kevin', 0.018), ('solving', 0.018), ('dictionary', 0.018), ('token', 0.018), ('fixing', 0.018), ('dependence', 0.018), ('conceptually', 0.017), ('condition', 0.017), ('keeps', 0.017), ('size', 0.017), ('notation', 0.016), ('trivial', 0.016), ('estimation', 0.016), ('roughly', 0.016), ('substitutions', 0.016), ('ilp', 0.016), ('chooses', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="66-tfidf-1" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Julian Schamper ; Hermann Ney</p><p>Abstract: In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet- ter (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.</p><p>2 0.4769572 <a title="66-tfidf-2" href="./acl-2013-Decipherment.html">108 acl-2013-Decipherment</a></p>
<p>Author: Kevin Knight</p><p>Abstract: The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. This tutorial explores connections between early decipherment research and today’s NLP work. We cover classic military and diplomatic ciphers, automatic decipherment algorithms, unsolved ciphers, language translation as decipherment, and analyzing ancient writing as decipherment. 1 Tutorial Overview The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. Sixty years later, we have many more applications, including web search, question answering, summarization, speech recognition, and language translation. This tutorial explores connections between early decipherment research and today’s NLP work. We find that many ideas from the earlier era have become core to the field, while others still remain to be picked up and developed. We first cover classic military and diplomatic cipher types, including complex substitution ciphers implemented in the first electro-mechanical encryption machines. We look at mathematical tools (language recognition, frequency counting, smoothing) developed to decrypt such ciphers on proto-computers. We show algorithms and extensive empirical results for solving different types of ciphers, and we show the role of algorithms in recent decipherments of historical documents. We then look at how foreign language can be viewed as a code for English, a concept developed by Alan Turing and Warren Weaver. We describe recently published work on building automatic translation systems from non-parallel data. We also demonstrate how some of the same algorithmic tools can be applied to natural language tasks like part-of-speech tagging and word alignment. Turning back to historical ciphers, we explore a number of unsolved ciphers, giving results of initial computer experiments on several of them. Finally, we look briefly at writing as a way to encipher phoneme sequences, covering ancient scripts and modern applications. 2 Outline 1. Classical military/diplomatic ciphers (15 minutes) • 60 cipher types (ACA) • Ciphers vs. codes • Enigma cipher: the mother of natural language processing computer analysis of text language recognition Good-Turing smoothing – – – 2. Foreign language as a code (10 minutes) • • Alan Turing’s ”Thinking Machines” Warren Weaver’s Memorandum 3. Automatic decipherment (55 minutes) • Cipher type detection • Substitution ciphers (simple, homophonic, polyalphabetic, etc) plaintext language recognition ∗ how much plaintext knowledge is – nheowede mdu 3 Proce diSnogfsia, of B thuleg5a r1iast, A Anungu aslt M4-9e t2in01g3 o.f ? tc he20 A1s3so Acsiasoticoinat fio rn C fo rm Cpoumtaptuiotantaioln Lainlg Luinisgtuicis ,tpi casges 3–4, – ∗ index of coincidence, unicity distance, oanf dc oointhceidr measures navigating a difficult search space ∗ frequencies of letters and words ∗ pattern words and cribs ∗ pElMin,g ILP, Bayesian models, sam– recent decipherments ∗ Jefferson cipher, Copiale cipher, cJievfifle war ciphers, n Caovaplia Enigma • • • • Application to part-of-speech tagging, Awopprdli alignment Application to machine translation withoAuptp parallel t teoxtm Parallel development of cryptography aPnarda ltrleanls dlaetvioenlo Recently released NSA internal nReewcselnettlyter (1974-1997) 4. *** Break *** (30 minutes) 5. Unsolved ciphers (40 minutes) • Zodiac 340 (1969), including computatZioodnaial cw 3o4r0k • Voynich Manuscript (early 1400s), including computational ewarolyrk • Beale (1885) • Dorabella (1897) • Taman Shud (1948) • Kryptos (1990), including computatKiorynaplt owsor (k1 • McCormick (1999) • Shoeboxes in attics: DuPonceau jour- nal, Finnerana, SYP, Mopse, diptych 6. Writing as a code (20 minutes) • Does writing encode ideas, or does it encDoodees phonemes? • Ancient script decipherment Egyptian hieroglyphs Linear B Mayan glyphs – – – – wUgoarkritic, including computational Chinese N ¨ushu, including computational work • Automatic phonetic decipherment • Application to transliteration 7. Undeciphered writing systems (15 minutes) • Indus Valley Script (3300BC) • Linear A (1900BC) • Phaistos disc (1700BC?) • Rongorongo (1800s?) – 8. Conclusion and further questions (15 minutes) 3 About the Presenter Kevin Knight is a Senior Research Scientist and Fellow at the Information Sciences Institute of the University of Southern California (USC), and a Research Professor in USC’s Computer Science Department. He received a PhD in computer science from Carnegie Mellon University and a bachelor’s degree from Harvard University. Professor Knight’s research interests include natural language processing, machine translation, automata theory, and decipherment. In 2001, he co-founded Language Weaver, Inc., and in 2011, he served as President of the Association for Computational Linguistics. Dr. Knight has taught computer science courses at USC for more than fifteen years and co-authored the widely adopted textbook Artificial Intelligence. 4</p><p>3 0.45453888 <a title="66-tfidf-3" href="./acl-2013-Decipherment_Complexity_in_1%3A1_Substitution_Ciphers.html">109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Hermann Ney</p><p>Abstract: In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchanging them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before.</p><p>4 0.22821806 <a title="66-tfidf-4" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>Author: Sujith Ravi</p><p>Abstract: In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</p><p>5 0.13823324 <a title="66-tfidf-5" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>Author: Ji Ma ; Jingbo Zhu ; Tong Xiao ; Nan Yang</p><p>Abstract: In this paper, we combine easy-first dependency parsing and POS tagging algorithms with beam search and structured perceptron. We propose a simple variant of “early-update” to ensure valid update in the training process. The proposed solution can also be applied to combine beam search and structured perceptron with other systems that exhibit spurious ambiguity. On CTB, we achieve 94.01% tagging accuracy and 86.33% unlabeled attachment score with a relatively small beam width. On PTB, we also achieve state-of-the-art performance. 1</p><p>6 0.10116324 <a title="66-tfidf-6" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>7 0.093444638 <a title="66-tfidf-7" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>8 0.079735167 <a title="66-tfidf-8" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>9 0.068399638 <a title="66-tfidf-9" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>10 0.058168367 <a title="66-tfidf-10" href="./acl-2013-Joint_Event_Extraction_via_Structured_Prediction_with_Global_Features.html">206 acl-2013-Joint Event Extraction via Structured Prediction with Global Features</a></p>
<p>11 0.054615788 <a title="66-tfidf-11" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>12 0.046606284 <a title="66-tfidf-12" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>13 0.046041623 <a title="66-tfidf-13" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>14 0.042624768 <a title="66-tfidf-14" href="./acl-2013-Unsupervised_Transcription_of_Historical_Documents.html">370 acl-2013-Unsupervised Transcription of Historical Documents</a></p>
<p>15 0.041970771 <a title="66-tfidf-15" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>16 0.040223982 <a title="66-tfidf-16" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>17 0.038645729 <a title="66-tfidf-17" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>18 0.037849456 <a title="66-tfidf-18" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>19 0.037466638 <a title="66-tfidf-19" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>20 0.036689125 <a title="66-tfidf-20" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, -0.064), (2, 0.007), (3, 0.015), (4, -0.029), (5, -0.015), (6, 0.076), (7, -0.005), (8, -0.116), (9, -0.054), (10, -0.041), (11, -0.1), (12, -0.124), (13, -0.302), (14, 0.075), (15, -0.431), (16, -0.094), (17, -0.168), (18, -0.103), (19, 0.304), (20, -0.006), (21, 0.053), (22, -0.149), (23, -0.155), (24, 0.096), (25, 0.046), (26, 0.13), (27, -0.06), (28, -0.118), (29, -0.03), (30, -0.057), (31, 0.048), (32, 0.03), (33, -0.058), (34, -0.062), (35, -0.047), (36, 0.044), (37, -0.008), (38, 0.062), (39, -0.057), (40, 0.008), (41, 0.003), (42, 0.046), (43, 0.099), (44, 0.046), (45, 0.035), (46, 0.014), (47, -0.071), (48, -0.017), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95426565 <a title="66-lsi-1" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Julian Schamper ; Hermann Ney</p><p>Abstract: In this paper we address the problem of solving substitution ciphers using a beam search approach. We present a conceptually consistent and easy to implement method that improves the current state of the art for decipherment of substitution ciphers and is able to use high order n-gram language models. We show experiments with 1:1 substitution ciphers in which the guaranteed optimal solution for 3-gram language models has 38.6% decipherment error, while our approach achieves 4.13% decipherment error in a fraction of time by using a 6-gram language model. We also apply our approach to the famous Zodiac-408 cipher and obtain slightly bet- ter (and near to optimal) results than previously published. Unlike the previous state-of-the-art approach that uses additional word lists to evaluate possible decipherments, our approach only uses a letterbased 6-gram language model. Furthermore we use our algorithm to solve large vocabulary substitution ciphers and improve the best published decipherment error rate based on the Gigaword corpus of 7.8% to 6.0% error rate.</p><p>2 0.94775707 <a title="66-lsi-2" href="./acl-2013-Decipherment.html">108 acl-2013-Decipherment</a></p>
<p>Author: Kevin Knight</p><p>Abstract: The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. This tutorial explores connections between early decipherment research and today’s NLP work. We cover classic military and diplomatic ciphers, automatic decipherment algorithms, unsolved ciphers, language translation as decipherment, and analyzing ancient writing as decipherment. 1 Tutorial Overview The first natural language processing systems had a straightforward goal: decipher coded messages sent by the enemy. Sixty years later, we have many more applications, including web search, question answering, summarization, speech recognition, and language translation. This tutorial explores connections between early decipherment research and today’s NLP work. We find that many ideas from the earlier era have become core to the field, while others still remain to be picked up and developed. We first cover classic military and diplomatic cipher types, including complex substitution ciphers implemented in the first electro-mechanical encryption machines. We look at mathematical tools (language recognition, frequency counting, smoothing) developed to decrypt such ciphers on proto-computers. We show algorithms and extensive empirical results for solving different types of ciphers, and we show the role of algorithms in recent decipherments of historical documents. We then look at how foreign language can be viewed as a code for English, a concept developed by Alan Turing and Warren Weaver. We describe recently published work on building automatic translation systems from non-parallel data. We also demonstrate how some of the same algorithmic tools can be applied to natural language tasks like part-of-speech tagging and word alignment. Turning back to historical ciphers, we explore a number of unsolved ciphers, giving results of initial computer experiments on several of them. Finally, we look briefly at writing as a way to encipher phoneme sequences, covering ancient scripts and modern applications. 2 Outline 1. Classical military/diplomatic ciphers (15 minutes) • 60 cipher types (ACA) • Ciphers vs. codes • Enigma cipher: the mother of natural language processing computer analysis of text language recognition Good-Turing smoothing – – – 2. Foreign language as a code (10 minutes) • • Alan Turing’s ”Thinking Machines” Warren Weaver’s Memorandum 3. Automatic decipherment (55 minutes) • Cipher type detection • Substitution ciphers (simple, homophonic, polyalphabetic, etc) plaintext language recognition ∗ how much plaintext knowledge is – nheowede mdu 3 Proce diSnogfsia, of B thuleg5a r1iast, A Anungu aslt M4-9e t2in01g3 o.f ? tc he20 A1s3so Acsiasoticoinat fio rn C fo rm Cpoumtaptuiotantaioln Lainlg Luinisgtuicis ,tpi casges 3–4, – ∗ index of coincidence, unicity distance, oanf dc oointhceidr measures navigating a difficult search space ∗ frequencies of letters and words ∗ pattern words and cribs ∗ pElMin,g ILP, Bayesian models, sam– recent decipherments ∗ Jefferson cipher, Copiale cipher, cJievfifle war ciphers, n Caovaplia Enigma • • • • Application to part-of-speech tagging, Awopprdli alignment Application to machine translation withoAuptp parallel t teoxtm Parallel development of cryptography aPnarda ltrleanls dlaetvioenlo Recently released NSA internal nReewcselnettlyter (1974-1997) 4. *** Break *** (30 minutes) 5. Unsolved ciphers (40 minutes) • Zodiac 340 (1969), including computatZioodnaial cw 3o4r0k • Voynich Manuscript (early 1400s), including computational ewarolyrk • Beale (1885) • Dorabella (1897) • Taman Shud (1948) • Kryptos (1990), including computatKiorynaplt owsor (k1 • McCormick (1999) • Shoeboxes in attics: DuPonceau jour- nal, Finnerana, SYP, Mopse, diptych 6. Writing as a code (20 minutes) • Does writing encode ideas, or does it encDoodees phonemes? • Ancient script decipherment Egyptian hieroglyphs Linear B Mayan glyphs – – – – wUgoarkritic, including computational Chinese N ¨ushu, including computational work • Automatic phonetic decipherment • Application to transliteration 7. Undeciphered writing systems (15 minutes) • Indus Valley Script (3300BC) • Linear A (1900BC) • Phaistos disc (1700BC?) • Rongorongo (1800s?) – 8. Conclusion and further questions (15 minutes) 3 About the Presenter Kevin Knight is a Senior Research Scientist and Fellow at the Information Sciences Institute of the University of Southern California (USC), and a Research Professor in USC’s Computer Science Department. He received a PhD in computer science from Carnegie Mellon University and a bachelor’s degree from Harvard University. Professor Knight’s research interests include natural language processing, machine translation, automata theory, and decipherment. In 2001, he co-founded Language Weaver, Inc., and in 2011, he served as President of the Association for Computational Linguistics. Dr. Knight has taught computer science courses at USC for more than fifteen years and co-authored the widely adopted textbook Artificial Intelligence. 4</p><p>3 0.92487979 <a title="66-lsi-3" href="./acl-2013-Decipherment_Complexity_in_1%3A1_Substitution_Ciphers.html">109 acl-2013-Decipherment Complexity in 1:1 Substitution Ciphers</a></p>
<p>Author: Malte Nuhn ; Hermann Ney</p><p>Abstract: In this paper we show that even for the case of 1:1 substitution ciphers—which encipher plaintext symbols by exchanging them with a unique substitute—finding the optimal decipherment with respect to a bigram language model is NP-hard. We show that in this case the decipherment problem is equivalent to the quadratic assignment problem (QAP). To the best of our knowledge, this connection between the QAP and the decipherment problem has not been known in the literature before.</p><p>4 0.38291961 <a title="66-lsi-4" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>Author: Sujith Ravi</p><p>Abstract: In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</p><p>5 0.27097976 <a title="66-lsi-5" href="./acl-2013-Unsupervised_Transcription_of_Historical_Documents.html">370 acl-2013-Unsupervised Transcription of Historical Documents</a></p>
<p>Author: Taylor Berg-Kirkpatrick ; Greg Durrett ; Dan Klein</p><p>Abstract: We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 3 1% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google’s open source OCR system.</p><p>6 0.26764017 <a title="66-lsi-6" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>7 0.22506955 <a title="66-lsi-7" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>8 0.19402516 <a title="66-lsi-8" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>9 0.18296865 <a title="66-lsi-9" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>10 0.18235759 <a title="66-lsi-10" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>11 0.1807857 <a title="66-lsi-11" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>12 0.17845201 <a title="66-lsi-12" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>13 0.17388247 <a title="66-lsi-13" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<p>14 0.16912186 <a title="66-lsi-14" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>15 0.16487786 <a title="66-lsi-15" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>16 0.16128479 <a title="66-lsi-16" href="./acl-2013-Variational_Inference_for_Structured_NLP_Models.html">382 acl-2013-Variational Inference for Structured NLP Models</a></p>
<p>17 0.15504937 <a title="66-lsi-17" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>18 0.15133837 <a title="66-lsi-18" href="./acl-2013-Variable_Bit_Quantisation_for_LSH.html">381 acl-2013-Variable Bit Quantisation for LSH</a></p>
<p>19 0.15023977 <a title="66-lsi-19" href="./acl-2013-Smoothed_marginal_distribution_constraints_for_language_modeling.html">325 acl-2013-Smoothed marginal distribution constraints for language modeling</a></p>
<p>20 0.14871068 <a title="66-lsi-20" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.084), (6, 0.029), (8, 0.026), (11, 0.025), (24, 0.02), (26, 0.025), (35, 0.049), (42, 0.061), (48, 0.042), (70, 0.031), (88, 0.015), (90, 0.014), (95, 0.479)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99027485 <a title="66-lda-1" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>Author: Chi-kiu Lo ; Karteek Addanki ; Markus Saers ; Dekai Wu</p><p>Abstract: We present the first ever results showing that tuning a machine translation system against a semantic frame based objective function, MEANT, produces more robustly adequate translations than tuning against BLEU or TER as measured across commonly used metrics and human subjective evaluation. Moreover, for informal web forum data, human evaluators preferred MEANT-tuned systems over BLEU- or TER-tuned systems by a significantly wider margin than that for formal newswire—even though automatic semantic parsing might be expected to fare worse on informal language. We argue thatbypreserving the meaning ofthe trans- lations as captured by semantic frames right in the training process, an MT system is constrained to make more accurate choices of both lexical and reordering rules. As a result, MT systems tuned against semantic frame based MT evaluation metrics produce output that is more adequate. Tuning a machine translation system against a semantic frame based objective function is independent ofthe translation model paradigm, so, any translation model can benefit from the semantic knowledge incorporated to improve translation adequacy through our approach.</p><p>2 0.98194575 <a title="66-lda-2" href="./acl-2013-Syntactic_Patterns_versus_Word_Alignment%3A_Extracting_Opinion_Targets_from_Online_Reviews.html">336 acl-2013-Syntactic Patterns versus Word Alignment: Extracting Opinion Targets from Online Reviews</a></p>
<p>Author: Kang Liu ; Liheng Xu ; Jun Zhao</p><p>Abstract: Mining opinion targets is a fundamental and important task for opinion mining from online reviews. To this end, there are usually two kinds of methods: syntax based and alignment based methods. Syntax based methods usually exploited syntactic patterns to extract opinion targets, which were however prone to suffer from parsing errors when dealing with online informal texts. In contrast, alignment based methods used word alignment model to fulfill this task, which could avoid parsing errors without using parsing. However, there is no research focusing on which kind of method is more better when given a certain amount of reviews. To fill this gap, this paper empiri- cally studies how the performance of these two kinds of methods vary when changing the size, domain and language of the corpus. We further combine syntactic patterns with alignment model by using a partially supervised framework and investigate whether this combination is useful or not. In our experiments, we verify that our combination is effective on the corpus with small and medium size.</p><p>3 0.98119062 <a title="66-lda-3" href="./acl-2013-Translating_Dialectal_Arabic_to_English.html">359 acl-2013-Translating Dialectal Arabic to English</a></p>
<p>Author: Hassan Sajjad ; Kareem Darwish ; Yonatan Belinkov</p><p>Abstract: We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation. In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic characterlevel transformational model that changes Egyptian to EG0, which looks similar to MSA. The transformations include morphological, phonological and spelling changes. The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points. Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points.</p><p>4 0.98097801 <a title="66-lda-4" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>Author: Kareem Darwish</p><p>Abstract: Some languages lack large knowledge bases and good discriminative features for Name Entity Recognition (NER) that can generalize to previously unseen named entities. One such language is Arabic, which: a) lacks a capitalization feature; and b) has relatively small knowledge bases, such as Wikipedia. In this work we address both problems by incorporating cross-lingual features and knowledge bases from English using cross-lingual links. We show that such features have a dramatic positive effect on recall. We show the effectiveness of cross-lingual features and resources on a standard dataset as well as on two new test sets that cover both news and microblogs. On the standard dataset, we achieved a 4.1% relative improvement in Fmeasure over the best reported result in the literature. The features led to improvements of 17.1% and 20.5% on the new news and mi- croblogs test sets respectively.</p><p>5 0.96916342 <a title="66-lda-5" href="./acl-2013-Adaptive_Parser-Centric_Text_Normalization.html">37 acl-2013-Adaptive Parser-Centric Text Normalization</a></p>
<p>Author: Congle Zhang ; Tyler Baldwin ; Howard Ho ; Benny Kimelfeld ; Yunyao Li</p><p>Abstract: Text normalization is an important first step towards enabling many Natural Language Processing (NLP) tasks over informal text. While many of these tasks, such as parsing, perform the best over fully grammatically correct text, most existing text normalization approaches narrowly define the task in the word-to-word sense; that is, the task is seen as that of mapping all out-of-vocabulary non-standard words to their in-vocabulary standard forms. In this paper, we take a parser-centric view of normalization that aims to convert raw informal text into grammatically correct text. To understand the real effect of normalization on the parser, we tie normal- ization performance directly to parser performance. Additionally, we design a customizable framework to address the often overlooked concept of domain adaptability, and illustrate that the system allows for transfer to new domains with a minimal amount of data and effort. Our experimental study over datasets from three domains demonstrates that our approach outperforms not only the state-of-the-art wordto-word normalization techniques, but also manual word-to-word annotations.</p><p>6 0.96905059 <a title="66-lda-6" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>same-paper 7 0.96453077 <a title="66-lda-7" href="./acl-2013-Beam_Search_for_Solving_Substitution_Ciphers.html">66 acl-2013-Beam Search for Solving Substitution Ciphers</a></p>
<p>8 0.95337176 <a title="66-lda-8" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>9 0.92591166 <a title="66-lda-9" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>10 0.87168932 <a title="66-lda-10" href="./acl-2013-Using_Context_Vectors_in_Improving_a_Machine_Translation_System_with_Bridge_Language.html">374 acl-2013-Using Context Vectors in Improving a Machine Translation System with Bridge Language</a></p>
<p>11 0.86190522 <a title="66-lda-11" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>12 0.85745811 <a title="66-lda-12" href="./acl-2013-Name-aware_Machine_Translation.html">255 acl-2013-Name-aware Machine Translation</a></p>
<p>13 0.84546041 <a title="66-lda-13" href="./acl-2013-Sentence_Level_Dialect_Identification_in_Arabic.html">317 acl-2013-Sentence Level Dialect Identification in Arabic</a></p>
<p>14 0.84144568 <a title="66-lda-14" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>15 0.83862096 <a title="66-lda-15" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>16 0.83717233 <a title="66-lda-16" href="./acl-2013-Social_Text_Normalization_using_Contextual_Graph_Random_Walks.html">326 acl-2013-Social Text Normalization using Contextual Graph Random Walks</a></p>
<p>17 0.82905728 <a title="66-lda-17" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>18 0.81458747 <a title="66-lda-18" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>19 0.81323469 <a title="66-lda-19" href="./acl-2013-Microblogs_as_Parallel_Corpora.html">240 acl-2013-Microblogs as Parallel Corpora</a></p>
<p>20 0.80277658 <a title="66-lda-20" href="./acl-2013-Cross-lingual_Projections_between_Languages_from_Different_Families.html">97 acl-2013-Cross-lingual Projections between Languages from Different Families</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
