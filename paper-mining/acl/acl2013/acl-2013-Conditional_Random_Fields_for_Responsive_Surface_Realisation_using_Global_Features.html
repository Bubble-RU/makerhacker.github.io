<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-90" href="#">acl2013-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</h1>
<br/><p>Source: <a title="acl-2013-90-pdf" href="http://aclweb.org/anthology//P/P13/P13-1123.pdf">pdf</a></p><p>Author: Nina Dethlefs ; Helen Hastie ; Heriberto Cuayahuitl ; Oliver Lemon</p><p>Abstract: Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher (up to 14%) than those for taking only local context into account.</p><p>Reference: <a title="acl-2013-90-reference" href="../acl2013_reference/acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. [sent-5, score-0.703]
</p><p>2 They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. [sent-6, score-0.303]
</p><p>3 We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. [sent-7, score-0.929]
</p><p>4 Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. [sent-8, score-0.317]
</p><p>5 This leads to more natural and less repetitive surface realisation. [sent-9, score-0.35]
</p><p>6 It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. [sent-10, score-0.726]
</p><p>7 1 Introduction Surface realisation typically aims to produce output that is grammatically well-formed, natural and cohesive. [sent-12, score-0.461]
</p><p>8 In interactive settings such as generation within a spoken dialogue system (SDS), a cuayahuit l | o . [sent-18, score-0.512]
</p><p>9 In addition, since interactions  are dynamic, generator inputs from the dialogue manager can sometimes be partial or subject to subsequent modification. [sent-22, score-0.608]
</p><p>10 Since dialogue acts are passed on to the generation module as soon as possible, this can sometimes lead to incomplete generator inputs (because the user is still speaking), or inputs that are subject to later modification (because of an initial ASR mis-recognition). [sent-24, score-0.769]
</p><p>11 In this paper, we propose to formulate surface realisation as a sequence labelling task. [sent-25, score-0.816]
</p><p>12 Our main hypothesis is that the use of global context in a CRF with semantic trees can lead to surface realisations that are better phrased, more natural and less repetitive than taking only local features into account. [sent-32, score-0.618]
</p><p>13 In addition, we compare our system with alternative surface realisation methods from the literature, namely, a rank and boost approach and n-grams. [sent-34, score-0.74]
</p><p>14 Ac s2s0o1ci3a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 1254–1263, to surface realisation within incremental systems, because CRFs are able to model context across full as well as partial generator inputs which may undergo modifications during generation. [sent-37, score-1.174]
</p><p>15 As a demonstration, we apply our model to incremental surface realisation in a proof-of-concept study. [sent-38, score-0.944]
</p><p>16 (2009) who also use CRFs to find the best surface realisation from a semantic tree. [sent-40, score-0.781]
</p><p>17 ’s generator does not take context beyond the current utterance into account and is thus restricted to local features. [sent-43, score-0.346]
</p><p>18 In terms of surface realisation from graphical models (and within the context of SDSs), our approach is also related to work by Georgila et al. [sent-45, score-0.785]
</p><p>19 The last approach is also concerned with generating restaurant recommendations within an SDS. [sent-48, score-0.34]
</p><p>20 In terms of surface realisation for SDSs, Oh and Rudnicky (2000) present foundational work in using an n-gram-based system. [sent-51, score-0.74]
</p><p>21 They train a surface realiser based on a domain-dependent language model and use an overgeneration and ranking approach. [sent-52, score-0.374]
</p><p>22 Candidate utterances are ranked according to a penalty function which penalises too long or short utterances, repetitious utterances and utterances which either contain more or less information than required by the dialogue act. [sent-53, score-0.522]
</p><p>23 SPaRKy was also developed for the domain of restaurant recommendations and was shown to be equivalent  to or better than a carefully designed templatebased generator which had received high human ratings in the past (Stent et al. [sent-58, score-0.472]
</p><p>24 This could present a problem in incremental settings, where generation speed is of particular importance. [sent-65, score-0.35]
</p><p>25 More work on trainable realisation for SDSs generally includes Bulyko and Ostendorf (2002) who use finite state transducers, Nakatsu and White (2006) who use supervised learning, Varges (2006) who uses chart generation, and Konstas and Lapata (2012) who use weighted hypergraphs, among others. [sent-69, score-0.461]
</p><p>26 1 Tree-based Semantic Representations The restaurant recommendations we generate can include any of the attributes shown in Table 1. [sent-71, score-0.435]
</p><p>27 It is then the task of the surface realiser to find the best realisation, including whether to present them in one or several sentences. [sent-72, score-0.347]
</p><p>28 This often is a sentence planning decision, but in our approach it is handled using CRF-based surface realisation. [sent-73, score-0.279]
</p><p>29 The semantic forms underlying surface realisation can be produced in many ways. [sent-74, score-0.781]
</p><p>30 While the user is speaking, the dialogue manager sends dialogue acts to the NLG module, which uses reinforcement learning to order semantic attributes and produce a semantic tree (see Dethlefs et al. [sent-76, score-1.148]
</p><p>31 This paper focuses on surface realisation from these trees using  a CRF as shown in the surface realisation module. [sent-78, score-1.48]
</p><p>32 As shown in the architecture diagram in Figure 1, a CRF surface realiser takes a semantic tree as input. [sent-122, score-0.445]
</p><p>33 2  Conditional Random Fields for Phrase-Based Surface Realisation The main idea of our approach is to treat surface realisation as a sequence labelling task in which a sequence of semantic inputs needs to be labelled with appropriate surface realisations. [sent-138, score-1.225]
</p><p>34 The task is therefore to find a mapping between (observed) 1256  lexical, syntactic and semantic features and a (hidden) best surface realisation. [sent-139, score-0.32]
</p><p>35 We use the linear-chain Conditional Random Field (CRF) model for statistical phrase-based surface realisation, see Figure 2 (a). [sent-140, score-0.279]
</p><p>36 This probabilistic model defines the posterior probability of la-  bels (surface realisation phrases) y={y1 , . [sent-141, score-0.461]
</p><p>37 The generation context includes everything that has been generated for the current utterance so far. [sent-162, score-0.315]
</p><p>38 shtml The semantics for each node are derived from the  input dialogue acts (these are listed in Table 1) and are associated with nodes. [sent-173, score-0.38]
</p><p>39 The lexical items are present in the generation context and are mapped to semantic tree nodes. [sent-174, score-0.289]
</p><p>40 , each generation step needs to take the features of the entire generation history into account. [sent-177, score-0.292]
</p><p>41 For the first constituent, The Beluga, this corresponds to the features { ˆ BEGIN NAME} indicating dthse beginning ourfe a sentence (where empty fdeicatautrinesg are omitted), gt ohef beginning of a new generation context and the next semantic slot required. [sent-179, score-0.279]
</p><p>42 In this way, a sequence of surface form constituents is generated correspond-  ing to latent states in the CRF. [sent-185, score-0.317]
</p><p>43 Since global utterance features capture the full generation context (i. [sent-186, score-0.374]
</p><p>44 This is useful for longer restaurant recommendations which may span over more than one utterance. [sent-189, score-0.34]
</p><p>45 In this way, our approach implicitly treats sentence planning decisions such as the distribution of content over a set of messages in the same way as (or as part of) surface realisation. [sent-196, score-0.279]
</p><p>46 A further capability of our surface realiser is that it can generate complete phrases from full as well as partial dialogue acts. [sent-197, score-0.723]
</p><p>47 A  demonstration of this is given in Section 5 in an application to incremental surface realisation. [sent-199, score-0.483]
</p><p>48 To train the CRF, we used a data set of 552 restaurant recommendations from the website The 1257  List. [sent-200, score-0.34]
</p><p>49 3 The data contains recommendations such as Located in the city centre, Beluga is a stylish yet laid-back restaurant with a smart menu of modern European cuisine. [sent-201, score-0.377]
</p><p>50 4  Grammar Induction  The grammar g of surface realisation candidates is obtained through an automatic grammar induction algorithm which can be run on unlabelled data and requires only minimal human intervention. [sent-203, score-0.8]
</p><p>51 This grammar defines the surface realisation space for the CRFs. [sent-204, score-0.77]
</p><p>52 We provide the human corpus of restaurant recommendations from Section 3. [sent-205, score-0.34]
</p><p>53 The remainder needs to be hand-annotated at the moment, which includes mainly attributes like restaurant names or quality attributes, such as delicate, exquisite, etc. [sent-211, score-0.346]
</p><p>54 We assume that cohesion can be identified by untrained judges as natural, well-phrased and non-repetitive surface forms. [sent-220, score-0.333]
</p><p>55 1: functionFINDGRAMMAR(utterances u, semantic attributes a) return grammar 2: for each utterance u do 3: if u contains a semantic attribute from a, such as venue, cuisine, etc. [sent-225, score-0.366]
</p><p>56 (201 w syhsitcehm generates restaurant recommendations based on the SPaRKy system (Walker et al. [sent-234, score-0.34]
</p><p>57 1 Human Rating Study We carried out a user rating study on the CrowdFlower crowd sourcing Each participant was shown part of a real human-system dialogue that emerged as part of the CLASSiC project evaluation (Rieser et al. [sent-241, score-0.431]
</p><p>58 Each dialogue contained two variations for one of the utterances. [sent-246, score-0.33]
</p><p>59 Table 2 gives an example of a dialogue segment presented to the participants. [sent-249, score-0.33]
</p><p>60 The restaurant Gourmet Burger is an outstanding, expensive restaurant located in the central area. [sent-258, score-0.557]
</p><p>61 Table 2:  Example  dialogue  for participants  to  compare alternative outputs in italics, USR=user, SYS A=CRF (global), SYS B=CRF(local). [sent-266, score-0.33]
</p><p>62 com  pare  Possibly this is because the local context taken into account by both systems was not enough to ensure cohesion across surface phrases. [sent-298, score-0.467]
</p><p>63 While CRF (global) often decides to aggregate attributes into one sentence,  such as the Beluga is an outstanding restaurant in the city centre, CLASSiC tends to rely more on individual messages, such as The Beluga is an outstanding restaurant. [sent-315, score-0.443]
</p><p>64 (2010) who also generate restaurant recommendations and asked similar questions to participants as we did. [sent-320, score-0.34]
</p><p>65 8])  Table 4: Example dialogue  where the dialogue  manager needs to send incremental updates to the NLG. [sent-355, score-1.027]
</p><p>66 Incremental surface realisation from semantic trees for this dialogue is shown in Figure 3. [sent-356, score-1.111]
</p><p>67 5  Incremental Surface Realisation  Recent years have seen increased interest in incremental dialogue processing (Skantze and Schlangen, 2009; Schlangen and Skantze, 2009). [sent-358, score-0.534]
</p><p>68 From a dialogue perspective, they can be said to work on partial rather than full dialogue acts. [sent-360, score-0.706]
</p><p>69 With respect to surface realisation, incremental NLG systems have predominantly relied on pre-defined templates (Purver and Otsuka, 2003; Skantze and Hjalmarsson, 2010; Dethlefs et al. [sent-361, score-0.483]
</p><p>70 , 2003), a constraint satisfaction-based NLG architecture and marks important progress towards more flexible incremental surface realisation. [sent-366, score-0.483]
</p><p>71 Especially for long utterances or such that are separated by user turns, this may lead to surface form increments that are not well connected and lack cohesion. [sent-368, score-0.396]
</p><p>72 1 Application to Incremental SR This section will discuss a proof-of-concept application of our approach to incremental surface realisation. [sent-370, score-0.483]
</p><p>73 Table 4 shows an example dialogue between a user and system that contains a number of incremental phenomena that require hypothesis updates, system corrections and user bargeins. [sent-371, score-0.64]
</p><p>74 Incremental surface realisation for this dialogue is shown in Figure 3, where processing steps are indicated as bold-face numbers and are triggered by partial dialogue acts that are sent from the dialogue manager, such as inform(area=centre [0. [sent-372, score-1.826]
</p><p>75 Once a dialogue act is observed by the NLG system, a reinforcement learning agent determines the order of attributes and produces a semantic tree, as described in Section 3. [sent-375, score-0.524]
</p><p>76 In the dialogue in Table 4, the user first asks for a nice restaurant in the centre. [sent-378, score-0.634]
</p><p>77 The dialogue manager constructs a first attribute-value slot, inform(area=centre [0. [sent-379, score-0.423]
</p><p>78 In a second step, the semantically annotated node gets expanded into a surface form that is chosen from a set of candidates (shown in curly brackets). [sent-385, score-0.279]
</p><p>79 Step 3 then further expands the current tree adding a node for the food type and the name of a restaurant that the dialogue manager had passed. [sent-392, score-0.876]
</p><p>80 Primitive attributes contain a single semantic type, such as area, whereas complex attributes contain multiple types, such as food, name and need to be decomposed in a later processing step (see steps 4 and 6). [sent-394, score-0.284]
</p><p>81 Step 5 again uses the CRF 7Note here that the information passed on to the NLG is distinct from the dialogue manager’s own actions. [sent-395, score-0.33]
</p><p>82 In the example, the NLG is asked to generate a recommendation, but the dialogue manager actually decides to clarify the user’s preferences due to low confidence. [sent-396, score-0.423]
</p><p>83 }  Figure 3: Example of incremental surface realisation, where each generation step is indicated by a number. [sent-407, score-0.629]
</p><p>84 Syntactic information in the form of parse categories are also taken into account for surface realisation, but have been omitted in this figure. [sent-410, score-0.376]
</p><p>85 obtain the next surface realisation that connects with the previous one (so that a sequence of realisation “labels” appears: Right in the city centre and Bangkok). [sent-411, score-1.33]
</p><p>86 This is important, because the local context would otherwise be restricted to a partial dialogue act, which can be much smaller than a full dialogue act and thus lead to short, repetitive sentences. [sent-413, score-0.877]
</p><p>87 The dialogue continues as the system implicitly confirms the user’s preferred restaurant (SYS 1). [sent-414, score-0.581]
</p><p>88 As a consequence, the dialogue manager needs to update its initial hypotheses and communicate this to NLG. [sent-416, score-0.423]
</p><p>89 Afterwards, the dialogue continues and NLG involves mainly expanding the current tree into a full sequence of surface realisations for partial dialogue acts which come together into a full utterance. [sent-419, score-1.198]
</p><p>90 They add new partial dialogue acts to the semantic tree. [sent-422, score-0.467]
</p><p>91 For our application, the maximal context is 9 semantic attributes (for a surface form that uses all possible 10 attributes). [sent-425, score-0.46]
</p><p>92 Updates are triggered by the hypothesis updates of the dialogue manager. [sent-428, score-0.4]
</p><p>93 Whenever generated output needs to be modified, old expansions and surface forms are deleted first, before new ones can be expanded in their place. [sent-431, score-0.341]
</p><p>94 2  Updates and Processing Speed Results  Since fast responses are crucial in incremental systems, we measured the average time our system took for a surface realisation. [sent-433, score-0.483]
</p><p>95 Since updates take effect directly on partial dialogue acts, rather than the full generated utterance, we require around 50% less updates as if generating from scratch for every changed input hypothesis. [sent-439, score-0.516]
</p><p>96 6  Conclusion and Future Directions  We have presented a novel technique for surface realisation that treats generation as a sequence la-  belling task by combining a CRF with tree-based semantic representations. [sent-441, score-0.965]
</p><p>97 An essential property of interactive surface realisers is to keep track of the utterance context including dependencies between linguistic features to generate cohesive utterances. [sent-442, score-0.506]
</p><p>98 Keeping track of the global context is also important for incremental systems since generator inputs can be incomplete or subject to modification. [sent-446, score-0.474]
</p><p>99 In a proof-of-concept study, we have argued that our approach is applicable to incremental surface realisation. [sent-447, score-0.483]
</p><p>100 In addition, we may compare different sequence labelling algorithms for surface realisation (Nguyen and Guo, 2007) or segmented CRFs (Sarawagi and Cohen, 2005) and apply our method to more complex surface realisation domains such as text generation or summarisation. [sent-451, score-1.702]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('realisation', 0.461), ('dialogue', 0.33), ('surface', 0.279), ('restaurant', 0.251), ('incremental', 0.204), ('crf', 0.188), ('sparky', 0.169), ('generation', 0.146), ('beluga', 0.138), ('dethlefs', 0.136), ('utterance', 0.124), ('venue', 0.123), ('rieser', 0.109), ('skantze', 0.108), ('attributes', 0.095), ('sys', 0.094), ('manager', 0.093), ('nlg', 0.093), ('food', 0.092), ('recommendations', 0.089), ('generator', 0.088), ('gourmet', 0.077), ('schlangen', 0.077), ('repetitive', 0.071), ('updates', 0.07), ('inform', 0.069), ('cuay', 0.068), ('realisations', 0.068), ('realiser', 0.068), ('classic', 0.067), ('crfs', 0.065), ('utterances', 0.064), ('sdss', 0.061), ('global', 0.059), ('reinforcement', 0.058), ('tree', 0.057), ('located', 0.055), ('local', 0.055), ('cohesion', 0.054), ('centre', 0.054), ('usr', 0.054), ('mairesse', 0.054), ('nina', 0.054), ('name', 0.053), ('user', 0.053), ('inputs', 0.051), ('acts', 0.05), ('rating', 0.048), ('phrasing', 0.047), ('slot', 0.047), ('bangkok', 0.046), ('partial', 0.046), ('context', 0.045), ('ratings', 0.044), ('phone', 0.043), ('burger', 0.043), ('walker', 0.043), ('semantic', 0.041), ('heriberto', 0.041), ('huitl', 0.041), ('sds', 0.041), ('stent', 0.041), ('conditional', 0.04), ('area', 0.039), ('labelling', 0.038), ('sequence', 0.038), ('phrased', 0.038), ('city', 0.037), ('spoken', 0.036), ('oliver', 0.036), ('expansions', 0.035), ('helen', 0.035), ('verena', 0.035), ('attribute', 0.035), ('account', 0.034), ('gabriel', 0.034), ('parse', 0.032), ('fields', 0.032), ('hastie', 0.032), ('categories', 0.031), ('barges', 0.031), ('bulyko', 0.031), ('buschmeier', 0.031), ('postcode', 0.031), ('realisers', 0.031), ('spud', 0.031), ('grammar', 0.03), ('outstanding', 0.03), ('nodes', 0.028), ('sigdial', 0.028), ('sarawagi', 0.027), ('georgila', 0.027), ('nakatsu', 0.027), ('optimising', 0.027), ('overgeneration', 0.027), ('purver', 0.027), ('responsive', 0.027), ('rudnicky', 0.027), ('track', 0.027), ('deleted', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="90-tfidf-1" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Heriberto Cuayahuitl ; Oliver Lemon</p><p>Abstract: Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher (up to 14%) than those for taking only local context into account.</p><p>2 0.13175569 <a title="90-tfidf-2" href="./acl-2013-Evaluating_a_City_Exploration_Dialogue_System_with_Integrated_Question-Answering_and_Pedestrian_Navigation.html">141 acl-2013-Evaluating a City Exploration Dialogue System with Integrated Question-Answering and Pedestrian Navigation</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon ; Phil Bartie ; Tiphaine Dalmas ; Anna Dickinson ; Xingkun Liu ; William Mackaness ; Bonnie Webber</p><p>Abstract: We present a city navigation and tourist information mobile dialogue app with integrated question-answering (QA) and geographic information system (GIS) modules that helps pedestrian users to navigate in and learn about urban environments. In contrast to existing mobile apps which treat these problems independently, our Android app addresses the problem of navigation and touristic questionanswering in an integrated fashion using a shared dialogue context. We evaluated our system in comparison with Samsung S-Voice (which interfaces to Google navigation and Google search) with 17 users and found that users judged our system to be significantly more interesting to interact with and learn from. They also rated our system above Google search (with the Samsung S-Voice interface) for tourist information tasks.</p><p>3 0.11848418 <a title="90-tfidf-3" href="./acl-2013-Predicting_and_Eliciting_Addressee%27s_Emotion_in_Online_Dialogue.html">282 acl-2013-Predicting and Eliciting Addressee's Emotion in Online Dialogue</a></p>
<p>Author: Takayuki Hasegawa ; Nobuhiro Kaji ; Naoki Yoshinaga ; Masashi Toyoda</p><p>Abstract: While there have been many attempts to estimate the emotion of an addresser from her/his utterance, few studies have explored how her/his utterance affects the emotion of the addressee. This has motivated us to investigate two novel tasks: predicting the emotion of the addressee and generating a response that elicits a specific emotion in the addressee’s mind. We target Japanese Twitter posts as a source of dialogue data and automatically build training data for learning the predictors and generators. The feasibility of our approaches is assessed by using 1099 utterance-response pairs that are built by . five human workers.</p><p>4 0.11171058 <a title="90-tfidf-4" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>Author: Sina Zarriess ; Jonas Kuhn</p><p>Abstract: We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</p><p>5 0.10388429 <a title="90-tfidf-5" href="./acl-2013-Tag2Blog%3A_Narrative_Generation_from_Satellite_Tag_Data.html">337 acl-2013-Tag2Blog: Narrative Generation from Satellite Tag Data</a></p>
<p>Author: Kapila Ponnamperuma ; Advaith Siddharthan ; Cheng Zeng ; Chris Mellish ; Rene van der Wal</p><p>Abstract: The aim of the Tag2Blog system is to bring satellite tagged wild animals “to life” through narratives that place their movements in an ecological context. Our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. We are working with one of the largest nature conservation charities in Europe in this regard, focusing on a single species, the red kite. We describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual, and constructs a story around its use of the landscape.</p><p>6 0.10229472 <a title="90-tfidf-6" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>7 0.090632737 <a title="90-tfidf-7" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>8 0.089914888 <a title="90-tfidf-8" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>9 0.088392958 <a title="90-tfidf-9" href="./acl-2013-Generating_Recommendation_Dialogs_by_Extracting_Information_from_User_Reviews.html">168 acl-2013-Generating Recommendation Dialogs by Extracting Information from User Reviews</a></p>
<p>10 0.086309902 <a title="90-tfidf-10" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>11 0.068079256 <a title="90-tfidf-11" href="./acl-2013-Semi-Supervised_Semantic_Tagging_of_Conversational_Understanding_using_Markov_Topic_Regression.html">315 acl-2013-Semi-Supervised Semantic Tagging of Conversational Understanding using Markov Topic Regression</a></p>
<p>12 0.067616172 <a title="90-tfidf-12" href="./acl-2013-Models_of_Semantic_Representation_with_Visual_Attributes.html">249 acl-2013-Models of Semantic Representation with Visual Attributes</a></p>
<p>13 0.067450047 <a title="90-tfidf-13" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>14 0.066016339 <a title="90-tfidf-14" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>15 0.063893206 <a title="90-tfidf-15" href="./acl-2013-Lightly_Supervised_Learning_of_Procedural_Dialog_Systems.html">230 acl-2013-Lightly Supervised Learning of Procedural Dialog Systems</a></p>
<p>16 0.061626304 <a title="90-tfidf-16" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>17 0.061221179 <a title="90-tfidf-17" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>18 0.059515398 <a title="90-tfidf-18" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>19 0.059299231 <a title="90-tfidf-19" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>20 0.053151917 <a title="90-tfidf-20" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, 0.02), (2, -0.027), (3, -0.021), (4, -0.022), (5, 0.016), (6, 0.076), (7, -0.068), (8, -0.012), (9, 0.047), (10, -0.084), (11, 0.026), (12, -0.035), (13, 0.009), (14, -0.024), (15, -0.058), (16, 0.02), (17, 0.032), (18, 0.044), (19, -0.072), (20, -0.115), (21, -0.101), (22, 0.11), (23, 0.033), (24, 0.052), (25, 0.057), (26, 0.114), (27, 0.036), (28, 0.002), (29, 0.007), (30, -0.003), (31, -0.01), (32, 0.068), (33, 0.113), (34, -0.037), (35, 0.071), (36, -0.035), (37, 0.027), (38, 0.036), (39, 0.031), (40, 0.075), (41, 0.099), (42, 0.073), (43, 0.006), (44, -0.048), (45, 0.056), (46, 0.121), (47, 0.008), (48, -0.178), (49, -0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93565798 <a title="90-lsi-1" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Heriberto Cuayahuitl ; Oliver Lemon</p><p>Abstract: Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher (up to 14%) than those for taking only local context into account.</p><p>2 0.77796263 <a title="90-lsi-2" href="./acl-2013-Combining_Referring_Expression_Generation_and_Surface_Realization%3A_A_Corpus-Based_Investigation_of_Architectures.html">86 acl-2013-Combining Referring Expression Generation and Surface Realization: A Corpus-Based Investigation of Architectures</a></p>
<p>Author: Sina Zarriess ; Jonas Kuhn</p><p>Abstract: We suggest a generation task that integrates discourse-level referring expression generation and sentence-level surface realization. We present a data set of German articles annotated with deep syntax and referents, including some types of implicit referents. Our experiments compare several architectures varying the order of a set of trainable modules. The results suggest that a revision-based pipeline, with intermediate linearization, significantly outperforms standard pipelines or a parallel architecture.</p><p>3 0.73710674 <a title="90-lsi-3" href="./acl-2013-Tag2Blog%3A_Narrative_Generation_from_Satellite_Tag_Data.html">337 acl-2013-Tag2Blog: Narrative Generation from Satellite Tag Data</a></p>
<p>Author: Kapila Ponnamperuma ; Advaith Siddharthan ; Cheng Zeng ; Chris Mellish ; Rene van der Wal</p><p>Abstract: The aim of the Tag2Blog system is to bring satellite tagged wild animals “to life” through narratives that place their movements in an ecological context. Our motivation is to use such automatically generated texts to enhance public engagement with a specific species reintroduction programme, although the protocols developed here can be applied to any animal or other movement study that involves signal data from tags. We are working with one of the largest nature conservation charities in Europe in this regard, focusing on a single species, the red kite. We describe a system that interprets a sequence of locational fixes obtained from a satellite tagged individual, and constructs a story around its use of the landscape.</p><p>4 0.66896671 <a title="90-lsi-4" href="./acl-2013-Evaluating_a_City_Exploration_Dialogue_System_with_Integrated_Question-Answering_and_Pedestrian_Navigation.html">141 acl-2013-Evaluating a City Exploration Dialogue System with Integrated Question-Answering and Pedestrian Navigation</a></p>
<p>Author: Srinivasan Janarthanam ; Oliver Lemon ; Phil Bartie ; Tiphaine Dalmas ; Anna Dickinson ; Xingkun Liu ; William Mackaness ; Bonnie Webber</p><p>Abstract: We present a city navigation and tourist information mobile dialogue app with integrated question-answering (QA) and geographic information system (GIS) modules that helps pedestrian users to navigate in and learn about urban environments. In contrast to existing mobile apps which treat these problems independently, our Android app addresses the problem of navigation and touristic questionanswering in an integrated fashion using a shared dialogue context. We evaluated our system in comparison with Samsung S-Voice (which interfaces to Google navigation and Google search) with 17 users and found that users judged our system to be significantly more interesting to interact with and learn from. They also rated our system above Google search (with the Samsung S-Voice interface) for tourist information tasks.</p><p>5 0.63942772 <a title="90-lsi-5" href="./acl-2013-Implicatures_and_Nested_Beliefs_in_Approximate_Decentralized-POMDPs.html">190 acl-2013-Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs</a></p>
<p>Author: Adam Vogel ; Christopher Potts ; Dan Jurafsky</p><p>Abstract: Conversational implicatures involve reasoning about multiply nested belief structures. This complexity poses significant challenges for computational models of conversation and cognition. We show that agents in the multi-agent DecentralizedPOMDP reach implicature-rich interpretations simply as a by-product of the way they reason about each other to maximize joint utility. Our simulations involve a reference game of the sort studied in psychology and linguistics as well as a dynamic, interactional scenario involving implemented artificial agents.</p><p>6 0.61143458 <a title="90-lsi-6" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>7 0.60907155 <a title="90-lsi-7" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>8 0.58564067 <a title="90-lsi-8" href="./acl-2013-%22Let_Everything_Turn_Well_in_Your_Wife%22%3A_Generation_of_Adult_Humor_Using_Lexical_Constraints.html">1 acl-2013-"Let Everything Turn Well in Your Wife": Generation of Adult Humor Using Lexical Constraints</a></p>
<p>9 0.580697 <a title="90-lsi-9" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>10 0.53890419 <a title="90-lsi-10" href="./acl-2013-Meet_EDGAR%2C_a_tutoring_agent_at_MONSERRATE.html">239 acl-2013-Meet EDGAR, a tutoring agent at MONSERRATE</a></p>
<p>11 0.53513855 <a title="90-lsi-11" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>12 0.52035171 <a title="90-lsi-12" href="./acl-2013-Sign_Language_Lexical_Recognition_With_Propositional_Dynamic_Logic.html">321 acl-2013-Sign Language Lexical Recognition With Propositional Dynamic Logic</a></p>
<p>13 0.51054972 <a title="90-lsi-13" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>14 0.4927496 <a title="90-lsi-14" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>15 0.49175045 <a title="90-lsi-15" href="./acl-2013-Is_word-to-phone_mapping_better_than_phone-phone_mapping_for_handling_English_words%3F.html">203 acl-2013-Is word-to-phone mapping better than phone-phone mapping for handling English words?</a></p>
<p>16 0.47624046 <a title="90-lsi-16" href="./acl-2013-Predicting_and_Eliciting_Addressee%27s_Emotion_in_Online_Dialogue.html">282 acl-2013-Predicting and Eliciting Addressee's Emotion in Online Dialogue</a></p>
<p>17 0.46154648 <a title="90-lsi-17" href="./acl-2013-Robust_multilingual_statistical_morphological_generation_models.html">303 acl-2013-Robust multilingual statistical morphological generation models</a></p>
<p>18 0.45879835 <a title="90-lsi-18" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>19 0.44734606 <a title="90-lsi-19" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>20 0.44196057 <a title="90-lsi-20" href="./acl-2013-Translating_Italian_connectives_into_Italian_Sign_Language.html">360 acl-2013-Translating Italian connectives into Italian Sign Language</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.046), (6, 0.054), (11, 0.048), (15, 0.013), (24, 0.051), (26, 0.041), (28, 0.016), (35, 0.063), (42, 0.099), (48, 0.035), (70, 0.04), (83, 0.235), (88, 0.054), (90, 0.046), (95, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79313529 <a title="90-lda-1" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Heriberto Cuayahuitl ; Oliver Lemon</p><p>Abstract: Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher (up to 14%) than those for taking only local context into account.</p><p>2 0.73329484 <a title="90-lda-2" href="./acl-2013-Is_a_204_cm_Man_Tall_or_Small_%3F_Acquisition_of_Numerical_Common_Sense_from_the_Web.html">202 acl-2013-Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web</a></p>
<p>Author: Katsuma Narisawa ; Yotaro Watanabe ; Junta Mizuno ; Naoaki Okazaki ; Kentaro Inui</p><p>Abstract: This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical common sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numerical expressions and their context from the Web. One approach estimates the distribution ofnumbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distri- bution. Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. Experimental results demonstrate the effectiveness of both approaches.</p><p>3 0.64257568 <a title="90-lda-3" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>Author: Olivier Ferret</p><p>Abstract: Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.</p><p>4 0.62619454 <a title="90-lda-4" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>Author: Bishan Yang ; Claire Cardie</p><p>Abstract: This paper addresses the task of finegrained opinion extraction the identification of opinion-related entities: the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the interdependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks – of opinion extraction, and seeks a globally optimal solution. Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction.</p><p>5 0.59175432 <a title="90-lda-5" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>Author: Peifeng Li ; Qiaoming Zhu ; Guodong Zhou</p><p>Abstract: As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline. 1</p><p>6 0.58927643 <a title="90-lda-6" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>7 0.58779907 <a title="90-lda-7" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>8 0.58572388 <a title="90-lda-8" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>9 0.58546007 <a title="90-lda-9" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>10 0.58527571 <a title="90-lda-10" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>11 0.58427405 <a title="90-lda-11" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>12 0.58409309 <a title="90-lda-12" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>13 0.58237123 <a title="90-lda-13" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>14 0.58204496 <a title="90-lda-14" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>15 0.58168423 <a title="90-lda-15" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>16 0.58167762 <a title="90-lda-16" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>17 0.58150035 <a title="90-lda-17" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>18 0.58148861 <a title="90-lda-18" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>19 0.58040702 <a title="90-lda-19" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>20 0.57769179 <a title="90-lda-20" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
