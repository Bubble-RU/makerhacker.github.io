<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-200" href="#">acl2013-200</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</h1>
<br/><p>Source: <a title="acl-2013-200-pdf" href="http://aclweb.org/anthology//P/P13/P13-1156.pdf">pdf</a></p><p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>Reference: <a title="acl-2013-200-reference" href="../acl2013_reference/acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu Abstract Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. [sent-3, score-0.632]
</p><p>2 Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. [sent-5, score-0.566]
</p><p>3 The tree-based translation model, by using a synchronous context-free grammar formalism, can capture longer reordering between source and target language. [sent-10, score-0.476]
</p><p>4 Yet, tree-based translation often underperforms phrase-based translation in language pairs with short range reordering such as Arabic-English translation (Zollmann et al. [sent-11, score-0.675]
</p><p>5 Phrase-based systems often use a lexical reordering model in addition to the distance cost feature. [sent-27, score-0.474]
</p><p>6 The biggest difference in a Hiero system and a phrase-based system is in how the reordering is modeled. [sent-28, score-0.372]
</p><p>7 In the Hiero system, the reordering decision is encoded in weighted translation rules, determined by nonterminal mappings. [sent-29, score-0.475]
</p><p>8 For example, the rule X → ne X1 pas ; not X1 : w indicates tthhee trrualnesl Xati →on nofe t hXe phrase between ne and pas to be after the English word not with score w. [sent-30, score-0.571]
</p><p>9 Most phrase-based systems are equipped with a distance reordering cost feature to tune the system towards the right amount of reordering, but then also a lexicalized reordering 1587  Proce dingsS o f ita h,e B 5u1lgsta Arinan,u Aaulg Musete 4ti-n9g 2 o0f1 t3h. [sent-34, score-0.99]
</p><p>10 Ac s2s0o1ci3a Atiosnso fcoirat Cio nm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 1587–1596, model to model the direction of adjacent source phrases reordering as either monotone, swap or discontinuous. [sent-36, score-0.394]
</p><p>11 It does not have the expressive lexicalized reordering model and distance cost features of the phrase-based system. [sent-40, score-0.678]
</p><p>12 aptiaves, reordering efe daetcuoredse ro sft phrase-based, but also uses on average longer phrases. [sent-51, score-0.337]
</p><p>13 However, these phrase pairs with gaps do not capture structure reordering as do Hiero rules with nonterminal mappings. [sent-52, score-0.743]
</p><p>14 For example, the rule X → ne X1 pas ; npointg X1 explicitly places et rhuel etra Xnsl →ation of the phrase between ne and pas behind the English word not through nonterminal X1. [sent-53, score-0.642]
</p><p>15 If we look at the leaves of a Hiero derivation tree, the lexicals also form a segmentation of the source and target sentence, thus also form a discontinuous phrasebased translation path. [sent-59, score-0.406]
</p><p>16 As an example, let us look at the translation ofthe French sentenceje neparle pas le fran c¸aise into English idon ’t speak french in Figure 1. [sent-60, score-0.389]
</p><p>17 Ordering these phrase pairs according the word sequence on the target side, shown on the right side of Figure 1, we have a phrasebased translation path consisting of four phrase pairs: (je, i) , (ne . [sent-65, score-0.654]
</p><p>18 m0∈XMPh\H  We focus on improving the modelling of reordering within Hiero and include discriminative reordering features (Tillmann, 2004) and a distance cost feature, both of which are not modeled in the original Hiero system. [sent-77, score-0.809]
</p><p>19 Our approach can be viewed as applying soft context constraint to make the probability of substituting a nonterminal by a subtree depending on the corresponding phrase-based reordering features. [sent-87, score-0.381]
</p><p>20 Training: Represent each rule as a sequence of phrase pairs and nonterminals. [sent-90, score-0.331]
</p><p>21 Decoding: Use the rules’ sequences of phrase pairs and nonterminals to find the corresponding phrase-based path of a Hiero derivation and calculate its feature scores. [sent-92, score-0.454]
</p><p>22 These phrase pairs will be part ofthe phrasebased translation path in the decoding step. [sent-95, score-0.448]
</p><p>23 We now explain how to map a rule to a sequence of phrase pairs and nonterminals. [sent-97, score-0.331]
</p><p>24 k  are phrases between nonterminals, they can be empty because nonterminals can be at the border of the rule or two nonterminals are adjacent. [sent-109, score-0.469]
</p><p>25 For example the rule X → ne X1 pas ; not X1 hexasa mkp = 1, s0 = ne, s1 = pas, t0 = not, t1 is an empty phrase because the target X1 is at the rightmost position. [sent-110, score-0.438]
</p><p>26 Phrasal-Hiero retains both nonterminals and lexical alignments of Hiero rules instead of only nonterminal mappings as in (Chiang, 2005). [sent-111, score-0.422]
</p><p>27 We use the lexical alignments of a rule to decide how source phrases and target phrases are connected. [sent-113, score-0.34]
</p><p>28 In the rule r, a source phrase si is connected to a target phrase ti0 if at least one word in si aligns to a target word in ti0 . [sent-114, score-0.517]
</p><p>29 In the rule X → Je X1 le Fran ¸cais ; I french X1 ethxetr rauclte f Xro m→ s Jeente Xnce pair in Figure 1, the phrase le Fran ¸cais connects to the phrase french because the French word Fran ¸cais aligns with the English word french even though le is unaligned. [sent-115, score-0.775]
</p><p>30 We then group the source phrases and target phrases into phrase pairs such that only phrases that are connected to each other are in the same phrase pair. [sent-116, score-0.562]
</p><p>31 Table 1shows an example ofrules, alignments and their sequences of phrase pairs and nonterminals on the last column. [sent-119, score-0.396]
</p><p>32 For example in the rule r4 = X → je X1 le X2 ; iX1 X2 extracted from the= s eXnt →enc jee pair in Figure 2, the phrase le is not aligned. [sent-122, score-0.526]
</p><p>33 In our Arabic-English experiment, rules without nonaligned phrases account for only 48. [sent-123, score-0.382]
</p><p>34 We compared the baseline Hiero translation from the full set of rules and the translation from only rules without nonaligned phrases. [sent-125, score-0.653]
</p><p>35 We therefore decided to not use rules with nonaligned phrases in Phrasal-Hiero. [sent-127, score-0.382]
</p><p>36 It is important to note that there are different ways to use all the rules and map rules with unaligned phrases into a sequence of phrase pairs. [sent-128, score-0.565]
</p><p>37 Compare BLEU scores of translation using all extracted rules (the first row) and translation using only rules without nonaligned subphrases (the second row). [sent-134, score-0.675]
</p><p>38 For example, adding these unaligned phrases to the previous phrase pair i. [sent-135, score-0.35]
</p><p>39 the rule r4 has one discontinuous phrase pair (je . [sent-137, score-0.41]
</p><p>40 We started the work with Arabic-English translation and decided not to use rules with nonaligned phrases in Phrasal-Hiero. [sent-141, score-0.476]
</p><p>41 In the experiment section, we will discuss the impact of removing rules with nonaligned sub-phrases in our GermanEnglish and Chinese-English experiments. [sent-142, score-0.375]
</p><p>42 2 Training: Lexicalized Reordering Table Phrasal-Hiero needs a phrase-based lexicalized reordering table to calculate the features. [sent-144, score-0.526]
</p><p>43 The lexicalized reordering table could be from a discontinuous phrase-based system. [sent-145, score-0.607]
</p><p>44 To guarantee the lexicalized reordering table to cover all phrase pairs  of the rule table, we extract phrase-pairs and their reordering directions during rule extraction. [sent-146, score-1.22]
</p><p>45 The lexical phrase pair corresponding to the rule r is ph = (s0 . [sent-154, score-0.388]
</p><p>46 Because the nonterminal could be at the border of the rule, the lexical phrase pair might have smaller coverage than the rule. [sent-167, score-0.319]
</p><p>47 For example, the training sentence pair in Figure 2 generates the rule r2 = X → ne X1 pas ; ed 2on g0et X1 spanning (1. [sent-168, score-0.349]
</p><p>48 Phrase pairs are generated together with phrase-based reordering orientations to build lexicalized reordering table. [sent-206, score-0.878]
</p><p>49 Each chart cell [X, i,j,r] indicates a subtree with rule r at the root covers the translation of the i-th word upto the j-th word of the source sentence. [sent-208, score-0.437]
</p><p>50 We extend the chart parsing, mapping the subtree to the equivalent discontinuous phrase-based path and includes phrasebased features to the log-linear model. [sent-209, score-0.392]
</p><p>51 In Phrasal-Hiero, each chart cell [X, i,j,r] also stores the first phrase pair and the last phrase pair of the phrase-based translation path covered the ith to the j-th word of the source sentence. [sent-210, score-0.821]
</p><p>52 These two phrase pairs are the back pointers to calcu-  late reordering features of later larger spans. [sent-211, score-0.529]
</p><p>53 Because the distance cost feature and phrase-based discriminative reordering feature calculation are both only required the source coverage of two adjacent phrase pairs, we explain here the distance cost calculation. [sent-212, score-0.787]
</p><p>54 We will again use three rules r1, r2, r3 in Table 1and the translation je ne parle pas le fran ¸cais into I ’t speak French to present the technique. [sent-213, score-0.76]
</p><p>55 First, when the rule r has only terminals, the rule’s sequence of phrase pairs and nonterminals consists of only a phrase pair. [sent-215, score-0.623]
</p><p>56 No calculation is needed, the first phrase pair and the last phrase pair are the same. [sent-216, score-0.428]
</p><p>57 The first phrase pair and the lXast → phrase pair point t Toh thee fi phrase (parle, speak) spanning 2 . [sent-221, score-0.588]
</p><p>58 When the translation rule’s right hand side has nonterminals, the nonterminals in the sequence belong to smaller chart cells that we already found phrase-based paths and calculated their features  before. [sent-225, score-0.467]
</p><p>59 The decoder then substitute these paths into the rule’s sequence of phrase pairs and nonterminals to form the complete path for the current span. [sent-226, score-0.514]
</p><p>60 We now demonstrate finding the phrase based path and calculate distance cost of the chart cell X2 spanning 1. [sent-227, score-0.609]
</p><p>61 pas, don0t) is the first phrase pair of the chart cell X1 which is (parle, speak). [sent-234, score-0.415]
</p><p>62 The distance cost of these two phrase pairs according to discontinuous phrase-based model is |2 3 1| = 2. [sent-235, score-0.43]
</p><p>63 The distance cost of the |w2h −ole 3 c−h 1a|rt cell X2 also includes the cost of the translation path covered by chart cell X1 which is 0, therefore the distance cost for X2 is 2 + dist(X1) = 2. [sent-236, score-0.812]
</p><p>64 We then update the first phrase pair and the last phrase pair of cell X2. [sent-237, score-0.513]
</p><p>65 pas, don0t), the last phrase pair is also the last phrase pair of cell X1 which is (parle, speak). [sent-241, score-0.531]
</p><p>66 Experiment Results  In all experiments we use phrase-orientation lexicalized reordering (Galley and Manning, 2008)2 which models monotone, swap, discontinuous orientations from both reordering with previous phrase pair and with the next phrase pair. [sent-243, score-1.29]
</p><p>67 There are total six features in lexicalized reordering model. [sent-244, score-0.537]
</p><p>68 (PB+nolex)  •  •  Phrase-based with lexicalized reordering features. [sent-248, score-0.508]
</p><p>69 The score difference between PB+nolex and PB+lex results indicates the impact of lexicalized reordering features on phrase-based system. [sent-252, score-0.558]
</p><p>70 , 2007) to include distance-based reordering as well as the lexicalized phrase orientation reordering model. [sent-279, score-1.0]
</p><p>71 distance  Phrasal-Hiero with phrase-based lexicalized reordering features(P. [sent-286, score-0.57]
</p><p>72 Phrase-based with lexicalized reordering features(PB+lex) shows significant improvement on all test sets over the simple phrase-based system without lexicalized reordering (PB+nolex). [sent-300, score-1.047]
</p><p>73 e, that the Hiero baseline system underperforms compared to the phrasebased system with lexicalized phrase-based reordering for Arabic-English in all test sets, on av-  erage by about 0. [sent-329, score-0.689]
</p><p>74 This is because Arabic language has relative free reordering, but mostly short distance, which is better captured by discriminative reordering features. [sent-333, score-0.329]
</p><p>75 line is the result of Hiero experiment on only a subset of rules without nonaligned phrases. [sent-337, score-0.354]
</p><p>76 We do not benefit from adding only the 1592  distance-based reordering feature (P. [sent-342, score-0.354]
</p><p>77 H+dist) to the Arabic-English experiment but get significant improvements when adding the six features of the lexicalized reordering (P. [sent-343, score-0.61]
</p><p>78 Adding both distance cost and lexicalized reordering features (P. [sent-358, score-0.678]
</p><p>79 Note that Hiero rules already have lexical context in the reordering, but adding phrase-based lexicalized reordering features to the system still gives us about as much improvement as the phrase-based system gets from lexicalized reordering features, here 1. [sent-368, score-1.314]
</p><p>80 This shows that the underperformance of the Hiero system is due to its lack of lexicalized reordering features rather than a limited hypothesis space. [sent-372, score-0.598]
</p><p>81 Even though the phrasebased system benefits from lexicalized reordering, PB+lex on average outperforms PB+nolex by 1. [sent-380, score-0.352]
</p><p>82 We have better improvements when adding the six fea-  tures of the lexicalized reordering model: P. [sent-417, score-0.552]
</p><p>83 We again get the best translation when adding both the distance cost feature and the lexicalized reordering features. [sent-429, score-0.787]
</p><p>84 So adding phrasebased features to the Hiero system yields nearly the same improvement as adding lexicalized reordering features to the phrase-based system. [sent-434, score-0.762]
</p><p>85 26 BLEU points better than the phrase-based system with lexicalized reordering features (PB+lex). [sent-459, score-0.6]
</p><p>86 We do, however, see improvements on all test sets when adding lexicalized reordering features. [sent-465, score-0.552]
</p><p>87 38 BLEU score by adding both distance cost and discriminative reordering features. [sent-475, score-0.514]
</p><p>88 5  Impact of segment rules into phrase pairs  Phrasal Hiero is the first system using rules’ lexical alignments. [sent-477, score-0.412]
</p><p>89 Table 7 shows the examples rules and its new sequence of nonterminals and phrase pairs. [sent-481, score-0.467]
</p><p>90 Without segment rules into phrase pairs, the rule r3 has only one phrase pair: ph = (Je . [sent-483, score-0.626]
</p><p>91 According to the new sequence of phrase pairs and nonterminals, during decoding the rule r3 has discontinous translation directions on both from phrase pair ph to the nonterminal X1 and from X1 to ph. [sent-491, score-0.78]
</p><p>92 But using lexical alignment and divide the rule into phrase pairs as in section 2. [sent-492, score-0.319]
</p><p>93 32 BLEU score when adding discriminated reordering features on Hiero (using the whole set of rules and no rule segmentation). [sent-507, score-0.654]
</p><p>94 The second block is the impact of adding discriminated re-  ordering features on Phrasal Hiero (using a subset of rules and segment rules into phrase pairs). [sent-508, score-0.641]
</p><p>95 5-934E62%  Table 9: The impact of using only rules without nonaligned phrases on Phrasal-Hiero results. [sent-521, score-0.403]
</p><p>96 Table 9 summarizes the impact of using only rules without nonaligned phrases on Phrasal1594  r R321u= =lesX X → → pJaenrXel 1X;le1spFpea rska. [sent-522, score-0.403]
</p><p>97 Using only rules without nonaligned phrases can get the same performance with translation with full set of rules for Arabic-English and German-English experiments but underperforms for the Chinese-English system. [sent-536, score-0.658]
</p><p>98 So leaving out the rules with nonaligned phrases could degrade  the system. [sent-541, score-0.382]
</p><p>99 We achieved the best result when adding both distance cost and lexicalized reordering features. [sent-558, score-0.693]
</p><p>100 A future work on the model can include complete rule sets together with word insertion/deletion features for nonaligned phrases. [sent-560, score-0.32]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hiero', 0.677), ('reordering', 0.31), ('lex', 0.221), ('lexicalized', 0.198), ('nonaligned', 0.185), ('phrase', 0.149), ('nonterminals', 0.143), ('rules', 0.14), ('chart', 0.125), ('je', 0.119), ('bleu', 0.108), ('rule', 0.106), ('parle', 0.1), ('discontinuous', 0.099), ('translation', 0.094), ('pas', 0.089), ('pb', 0.085), ('cell', 0.085), ('cost', 0.079), ('dist', 0.079), ('phrasebased', 0.077), ('nonterminal', 0.071), ('cais', 0.071), ('ne', 0.069), ('decoder', 0.065), ('block', 0.065), ('distance', 0.062), ('path', 0.062), ('zollmann', 0.058), ('phrases', 0.057), ('french', 0.057), ('nolex', 0.057), ('pair', 0.056), ('ph', 0.054), ('fran', 0.051), ('speak', 0.05), ('le', 0.048), ('alignments', 0.045), ('adding', 0.044), ('unaligned', 0.044), ('lexicals', 0.043), ('phrasalhiero', 0.043), ('reodering', 0.043), ('underperforms', 0.042), ('derivation', 0.041), ('pairs', 0.041), ('galley', 0.041), ('sequence', 0.035), ('germanenglish', 0.035), ('phrasal', 0.034), ('atb', 0.033), ('orientation', 0.033), ('points', 0.032), ('gaps', 0.032), ('system', 0.031), ('hypothesis', 0.03), ('versus', 0.03), ('features', 0.029), ('wmt', 0.029), ('spanning', 0.029), ('experiment', 0.029), ('francaise', 0.028), ('huck', 0.028), ('lesx', 0.028), ('xksk', 0.028), ('segment', 0.028), ('chiang', 0.028), ('source', 0.027), ('average', 0.027), ('rows', 0.027), ('monotone', 0.026), ('ixs', 0.025), ('discriminated', 0.025), ('target', 0.025), ('decoding', 0.025), ('lexical', 0.023), ('mhm', 0.023), ('side', 0.022), ('treebased', 0.022), ('scores', 0.022), ('impact', 0.021), ('koehn', 0.02), ('formalism', 0.02), ('distortion', 0.02), ('border', 0.02), ('indexes', 0.02), ('hierarchical', 0.02), ('synchronous', 0.02), ('paths', 0.019), ('hb', 0.019), ('orientations', 0.019), ('outperforms', 0.019), ('venugopal', 0.019), ('discriminative', 0.019), ('calculate', 0.018), ('last', 0.018), ('chineseenglish', 0.018), ('qatar', 0.018), ('arabicenglish', 0.018), ('si', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="200-tfidf-1" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>2 0.33471695 <a title="200-tfidf-2" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>Author: Fei Huang ; Cezar Pendus</p><p>Abstract: We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation (SMT). Instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage. Furthermore, , we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules. Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach: we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric. 1</p><p>3 0.24609074 <a title="200-tfidf-3" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>4 0.20971103 <a title="200-tfidf-4" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>Author: Minwei Feng ; Jan-Thorsten Peter ; Hermann Ney</p><p>Abstract: In this paper, we propose a novel reordering model based on sequence labeling techniques. Our model converts the reordering problem into a sequence labeling problem, i.e. a tagging task. Results on five Chinese-English NIST tasks show that our model improves the baseline system by 1.32 BLEU and 1.53 TER on average. Results of comparative study with other seven widely used reordering models will also be reported.</p><p>5 0.16383378 <a title="200-tfidf-5" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>Author: Graham Neubig</p><p>Abstract: In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http : / /phont ron . com/t ravat ar</p><p>6 0.16165045 <a title="200-tfidf-6" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>7 0.14713244 <a title="200-tfidf-7" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>8 0.13827664 <a title="200-tfidf-8" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>9 0.12934498 <a title="200-tfidf-9" href="./acl-2013-Two-Neighbor_Orientation_Model_with_Cross-Boundary_Global_Contexts.html">363 acl-2013-Two-Neighbor Orientation Model with Cross-Boundary Global Contexts</a></p>
<p>10 0.12435944 <a title="200-tfidf-10" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>11 0.12260062 <a title="200-tfidf-11" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>12 0.12243254 <a title="200-tfidf-12" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>13 0.11646527 <a title="200-tfidf-13" href="./acl-2013-Enlisting_the_Ghost%3A_Modeling_Empty_Categories_for_Machine_Translation.html">137 acl-2013-Enlisting the Ghost: Modeling Empty Categories for Machine Translation</a></p>
<p>14 0.11542309 <a title="200-tfidf-14" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>15 0.11427609 <a title="200-tfidf-15" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>16 0.11156893 <a title="200-tfidf-16" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>17 0.10266726 <a title="200-tfidf-17" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>18 0.097347751 <a title="200-tfidf-18" href="./acl-2013-Handling_Ambiguities_of_Bilingual_Predicate-Argument_Structures_for_Statistical_Machine_Translation.html">180 acl-2013-Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation</a></p>
<p>19 0.095685892 <a title="200-tfidf-19" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>20 0.095111549 <a title="200-tfidf-20" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.175), (1, -0.19), (2, 0.176), (3, 0.118), (4, -0.078), (5, 0.1), (6, 0.03), (7, 0.018), (8, 0.018), (9, 0.098), (10, -0.028), (11, 0.076), (12, 0.046), (13, 0.026), (14, 0.065), (15, 0.078), (16, 0.177), (17, 0.078), (18, 0.025), (19, 0.053), (20, -0.148), (21, 0.026), (22, 0.023), (23, -0.155), (24, 0.035), (25, -0.012), (26, -0.003), (27, -0.124), (28, -0.217), (29, -0.064), (30, -0.026), (31, 0.001), (32, 0.038), (33, -0.006), (34, -0.003), (35, 0.024), (36, 0.046), (37, -0.038), (38, -0.075), (39, 0.034), (40, 0.041), (41, -0.07), (42, -0.025), (43, -0.015), (44, -0.025), (45, -0.016), (46, 0.015), (47, 0.031), (48, 0.02), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94418019 <a title="200-lsi-1" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>Author: Fei Huang ; Cezar Pendus</p><p>Abstract: We present a simple yet effective approach to syntactic reordering for Statistical Machine Translation (SMT). Instead of solely relying on the top-1 best-matching rule for source sentence preordering, we generalize fully lexicalized rules into partially lexicalized and unlexicalized rules to broaden the rule coverage. Furthermore, , we consider multiple permutations of all the matching rules, and select the final reordering path based on the weighed sum of reordering probabilities of these rules. Our experiments in English-Chinese and English-Japanese translations demonstrate the effectiveness of the proposed approach: we observe consistent and significant improvement in translation quality across multiple test sets in both language pairs judged by both humans and automatic metric. 1</p><p>same-paper 2 0.93160462 <a title="200-lsi-2" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>3 0.83051986 <a title="200-lsi-3" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>Author: Karthik Visweswariah ; Mitesh M. Khapra ; Ananthakrishnan Ramanathan</p><p>Abstract: Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine align- ments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</p><p>4 0.80351532 <a title="200-lsi-4" href="./acl-2013-Two-Neighbor_Orientation_Model_with_Cross-Boundary_Global_Contexts.html">363 acl-2013-Two-Neighbor Orientation Model with Cross-Boundary Global Contexts</a></p>
<p>Author: Hendra Setiawan ; Bowen Zhou ; Bing Xiang ; Libin Shen</p><p>Abstract: Long distance reordering remains one of the greatest challenges in statistical machine translation research as the key contextual information may well be beyond the confine of translation units. In this paper, we propose Two-Neighbor Orientation (TNO) model that jointly models the orientation decisions between anchors and two neighboring multi-unit chunks which may cross phrase or rule boundaries. We explicitly model the longest span of such chunks, referred to as Maximal Orientation Span, to serve as a global parameter that constrains underlying local decisions. We integrate our proposed model into a state-of-the-art string-to-dependency translation system and demonstrate the efficacy of our proposal in a large-scale Chinese-to-English translation task. On NIST MT08 set, our most advanced model brings around +2.0 BLEU and -1.0 TER improvement.</p><p>5 0.80170864 <a title="200-lsi-5" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>Author: Nadir Durrani ; Alexander Fraser ; Helmut Schmid ; Hieu Hoang ; Philipp Koehn</p><p>Abstract: The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve.</p><p>6 0.77775604 <a title="200-lsi-6" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>7 0.74506867 <a title="200-lsi-7" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<p>8 0.61848652 <a title="200-lsi-8" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>9 0.56510371 <a title="200-lsi-9" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>10 0.53957409 <a title="200-lsi-10" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>11 0.53017116 <a title="200-lsi-11" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>12 0.51104867 <a title="200-lsi-12" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>13 0.50425011 <a title="200-lsi-13" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>14 0.4790085 <a title="200-lsi-14" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>15 0.45877808 <a title="200-lsi-15" href="./acl-2013-Handling_Ambiguities_of_Bilingual_Predicate-Argument_Structures_for_Statistical_Machine_Translation.html">180 acl-2013-Handling Ambiguities of Bilingual Predicate-Argument Structures for Statistical Machine Translation</a></p>
<p>16 0.44183463 <a title="200-lsi-16" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.44052467 <a title="200-lsi-17" href="./acl-2013-Language_Independent_Connectivity_Strength_Features_for_Phrase_Pivot_Statistical_Machine_Translation.html">214 acl-2013-Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation</a></p>
<p>18 0.43589726 <a title="200-lsi-18" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>19 0.4305425 <a title="200-lsi-19" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>20 0.43043521 <a title="200-lsi-20" href="./acl-2013-Enlisting_the_Ghost%3A_Modeling_Empty_Categories_for_Machine_Translation.html">137 acl-2013-Enlisting the Ghost: Modeling Empty Categories for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.019), (6, 0.032), (11, 0.042), (24, 0.021), (26, 0.037), (28, 0.011), (35, 0.052), (42, 0.084), (48, 0.026), (70, 0.036), (88, 0.016), (90, 0.445), (95, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92311531 <a title="200-lda-1" href="./acl-2013-On_the_Predictability_of_Human_Assessment%3A_when_Matrix_Completion_Meets_NLP_Evaluation.html">263 acl-2013-On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation</a></p>
<p>Author: Guillaume Wisniewski</p><p>Abstract: This paper tackles the problem of collecting reliable human assessments. We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality. To reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example.</p><p>2 0.88619769 <a title="200-lda-2" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>Author: Fabienne Braune ; Nina Seemann ; Daniel Quernheim ; Andreas Maletti</p><p>Abstract: We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German tlriannes olnati tohne tWasMk.T TA 2s0 an a Edndgitliisonha →l c Gonetrrmibauntion we make the developed software and complete tool-chain publicly available for further experimentation.</p><p>3 0.88583833 <a title="200-lda-3" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>same-paper 4 0.88355756 <a title="200-lda-4" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>5 0.87347364 <a title="200-lda-5" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>Author: Stefan L. Frank ; Leun J. Otten ; Giulia Galli ; Gabriella Vigliocco</p><p>Abstract: We investigated the effect of word surprisal on the EEG signal during sentence reading. On each word of 205 experimental sentences, surprisal was estimated by three types of language model: Markov models, probabilistic phrasestructure grammars, and recurrent neural networks. Four event-related potential components were extracted from the EEG of 24 readers of the same sentences. Surprisal estimates under each model type formed a significant predictor of the amplitude of the N400 component only, with more surprising words resulting in more negative N400s. This effect was mostly due to content words. These findings provide support for surprisal as a gener- ally applicable measure of processing difficulty during language comprehension.</p><p>6 0.81449568 <a title="200-lda-6" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>7 0.80906445 <a title="200-lda-7" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>8 0.58009458 <a title="200-lda-8" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>9 0.56123304 <a title="200-lda-9" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>10 0.55524743 <a title="200-lda-10" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>11 0.53414339 <a title="200-lda-11" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>12 0.5339945 <a title="200-lda-12" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>13 0.51362193 <a title="200-lda-13" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>14 0.49636734 <a title="200-lda-14" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>15 0.49243879 <a title="200-lda-15" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>16 0.47833934 <a title="200-lda-16" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>17 0.47445637 <a title="200-lda-17" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>18 0.47390217 <a title="200-lda-18" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>19 0.47366354 <a title="200-lda-19" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>20 0.47346717 <a title="200-lda-20" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
