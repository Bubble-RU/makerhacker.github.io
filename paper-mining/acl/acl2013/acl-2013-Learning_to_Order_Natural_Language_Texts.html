<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>225 acl-2013-Learning to Order Natural Language Texts</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-225" href="#">acl2013-225</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>225 acl-2013-Learning to Order Natural Language Texts</h1>
<br/><p>Source: <a title="acl-2013-225-pdf" href="http://aclweb.org/anthology//P/P13/P13-2016.pdf">pdf</a></p><p>Author: Jiwei Tan ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 1</p><p>Reference: <a title="acl-2013-225-reference" href="../acl2013_reference/acl-2013-Learning_to_Order_Natural_Language_Texts_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn b  ,  Abstract Ordering texts is an important task for many NLP applications. [sent-4, score-0.04]
</p><p>2 Most previous works on summary sentence ordering rely on the contextual information (e. [sent-5, score-0.599]
</p><p>3 adjacent sentences) of each sentence in the source document. [sent-7, score-0.09]
</p><p>4 In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. [sent-8, score-0.609]
</p><p>5 We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. [sent-9, score-0.246]
</p><p>6 We also propose to use the genetic algorithm to determine the total order of all sentences. [sent-10, score-0.346]
</p><p>7 Evaluation results on a news corpus show the effectiveness of our proposed method. [sent-11, score-0.051]
</p><p>8 1  Introduction  Ordering texts is an important task in many natu-  ral language processing (NLP) applications. [sent-12, score-0.04]
</p><p>9 It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. [sent-13, score-0.201]
</p><p>10 However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers. [sent-14, score-0.53]
</p><p>11 Previous works on sentence ordering mainly focus on the MDS task (Barzilay et al. [sent-15, score-0.547]
</p><p>12 In this task, each summary sentence is extracted from a source document. [sent-24, score-0.142]
</p><p>13 The timestamp of the source documents and the adjacent sentences in the source documents can be used as important clues for ordering summary sentences. [sent-25, score-0.53]
</p><p>14 In this study, we investigate a more challenging and more general task of ordering a set of unordered sentences (e. [sent-26, score-0.609]
</p><p>15 sentences in a text paragraph) without any con-  textual information. [sent-29, score-0.057]
</p><p>16 This task can be applied to almost all text generation applications without restriction. [sent-30, score-0.054]
</p><p>17 In order to address this challenging task, we first introduce a few useful features to characterize the order and coherence of natural language texts, and then propose to use the learning to rank algorithm to determine the order of two sentences. [sent-31, score-0.341]
</p><p>18 Moreover, we propose to use the genetic algorithm to decide the overall text order. [sent-32, score-0.383]
</p><p>19 Evaluations are conducted on a news corpus, and the results show the prominence of our method. [sent-33, score-0.051]
</p><p>20 2  Related Work  For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. [sent-35, score-0.547]
</p><p>21 However, the model only works well when using single feature, but unfortunately, it becomes worse when multiple  features are combined. [sent-37, score-0.08]
</p><p>22 Nahnsen (2009) employed features which were based on discourse entities, shallow syntactic analysis, and temporal precedence relations retrieved from VerbOcean. [sent-39, score-0.114]
</p><p>23 1  Our Proposed Method Overview  The task of text ordering can be modeled like (Cohen et al. [sent-42, score-0.421]
</p><p>24 , 1998), as measuring the coherence of a text by summing the association strength of any sentence pairs. [sent-43, score-0.206]
</p><p>25 Then the objective of a text ordering model is to find a permutation which can maximize the summation. [sent-44, score-0.622]
</p><p>26 c e2 A0s1s3oc Aiastsio cnia fotiron C fo mrp Cuotmatpiounta tlio Lninaglu Li sntgicusi,s ptaicgses 87–91, Formally, we define an association strength function PREF(u , v ) ∈ R to measure how strong  it is that sentence u should be arranged before sentence v (denoted as u ? [sent-47, score-0.384]
</p><p>27 We then define function AGREE(ρ ,PREF) as:  AGREE(ρ ,PREF) = u, v :ρ∑ (u )>ρ(v )PREF(u , v )  (1)  where ρ denotes a sentence permutation and ρ(u ) > ρ(v ) means u ? [sent-49, score-0.339]
</p><p>28 Then the objective of finding an overall order of the sentences becomes finding a permutation ρ to maximize AGREE(ρ ,PREF) . [sent-51, score-0.298]
</p><p>29 The main framework is made up of two parts: defining a pairwise order relation and determining an overall order. [sent-52, score-0.138]
</p><p>30 Our study focuses on both the two parts by learning a better pairwise relation and proposing a better search strategy, as described respectively in next sections. [sent-53, score-0.103]
</p><p>31 2  Pairwise Relation Learning  The goal for pairwise relation learning is defining the strength function PREF for any sentence pair. [sent-55, score-0.287]
</p><p>32 Method: Traditionally, there are two main methods for defining a strength function: inte-  grating features by a linear combination (He et al. [sent-57, score-0.148]
</p><p>33 However, the binary classification method is very coarsegrained since it considers any pair of sentences either “positive” or “negative”. [sent-61, score-0.106]
</p><p>34 Instead we propose to use a better model of learning to rank to integrate multiple features. [sent-62, score-0.043]
</p><p>35 In this study, we use Ranking SVM implemented in the svmrank toolkit (Joachims, 2002; Joachims, 2006) as the ranking model. [sent-63, score-0.171]
</p><p>36 The examples to be ranked in our ranking model are sequential sentence pairs like u ? [sent-64, score-0.235]
</p><p>37 The feature values for a training example are generated by a few feature functions fi( u , v ) , and we will introduce the features later. [sent-66, score-0.044]
</p><p>38 We build the training examples for svmrank as follows: For a training query, which is a paragraph with n sequential sentences as s1 ? [sent-67, score-0.286]
</p><p>39 sa+ k (k > 0) the target rank values are set to n − k , which means that the longer the distance between the two sentences is, the smaller the target value is. [sent-75, score-0.1]
</p><p>40 In order to better capture the order information of each feature, for every sentence pair u ? [sent-78, score-0.09]
</p><p>41 A paragraph of unordered sentences is viewed as a test query, and the predicted target value for u ? [sent-88, score-0.285]
</p><p>42 Features: We select four types of features to characterize text coherence. [sent-90, score-0.107]
</p><p>43 Every type of features is quantified with several functions distinguished by i in the formulation of fi( u , v ) and normalized to [0,1] . [sent-91, score-0.095]
</p><p>44 The features and definitions of fi( u , v ) are introduced in Table 1. [sent-92, score-0.044]
</p><p>45 For the coreference features we use the ARKref1 tool. [sent-95, score-0.132]
</p><p>46 It can output the coreference chains containing words which represent the same entity for two sequential sentences u ? [sent-96, score-0.195]
</p><p>47 The probability model originates from (Lapata, 2003), and we implement the model with four features of lemmatized noun, verb, adjective or adverb, and verb and noun related dependency. [sent-98, score-0.131]
</p><p>48 (1998) proved finding a permutation ρ to maximize AGREE(ρ ,PREF) is NPcomplete. [sent-101, score-0.201]
</p><p>49 To solve this, they proposed a greedy algorithm for finding an approximately optimal order. [sent-102, score-0.118]
</p><p>50 Most later works adopted the greedy search strategy to determine the overall order. [sent-103, score-0.292]
</p><p>51 However, a greedy algorithm does not always lead to satisfactory results, as our experiment  shows in Section 4. [sent-104, score-0.118]
</p><p>52 Therefore, we propose to use the genetic algorithm (Holland, 1992) as the search strategy, which can lead to better results. [sent-106, score-0.355]
</p><p>53 Genetic Algorithm: The genetic algorithm (GA) is an artificial intelligence algorithm for optimization and search problems. [sent-107, score-0.398]
</p><p>54 The key point of using GA is modeling the individual, fitness function and three operators of crossover, mutation and selection. [sent-108, score-0.305]
</p><p>55 Once a problem is modeled, the algorithm can be constructed conventionally. [sent-109, score-0.043]
</p><p>56 In our method we set a permutation ρ as an individual encoded by a numerical path, for example a permutation s2 ? [sent-110, score-0.322]
</p><p>57 Then the function AGREE(ρ ,PREF) is just the fitness function. [sent-113, score-0.191]
</p><p>58 We adopt the order-based crossover operator which is described in (Davis, 1985). [sent-114, score-0.199]
</p><p>59 The mutation operator is a random inversion of two sentences. [sent-115, score-0.208]
</p><p>60 For selection operator we take a tournament selection operator which randomly selects two individuals to choose the one with the greater fitness value AGREE(ρ ,PREF) . [sent-116, score-0.34]
</p><p>61 edu/ARKref/  After several generations of evolution, the individual with the greatest fitness value will be a close solution to the optimal result. [sent-121, score-0.152]
</p><p>62 Comparisons: It is incomparable with other methods for summary sentence ordering based on special summarization corpus, so we implemented Lapata’s probability model for comparison, which is considered the state of the art for this task. [sent-126, score-0.602]
</p><p>63 In addition, we implemented a random ordering as a baseline. [sent-127, score-0.421]
</p><p>64 We also tried to use a classification model in place of the ranking model. [sent-128, score-0.144]
</p><p>65 In the classification model, sentence pairs like sa ? [sent-129, score-0.306]
</p><p>66 sa+1 were viewed as positive examples and all other pairs were viewed as negative examples. [sent-130, score-0.092]
</p><p>67 When deciding the overall order for either ranking or classification model we used three search strategies: greedy, genetic and exhaustive (or brutal) algorithms. [sent-131, score-0.559]
</p><p>68 For comparative analysis of features, we tested with an exhaustive search algorithm to determine the overall order. [sent-134, score-0.235]
</p><p>69 2 Experiment Results The comparison results in Table 2 show that our Ranking SVM based method improves the performance over the baselines and the classification based method with any of the search algorithms. [sent-136, score-0.098]
</p><p>70 We can also see the greedy search strategy does not perform well and the genetic algorithm can provide a good approximate solution to  obCtPalTBriRMnosa bensoiftalkpehbciton2admlegit:oayAlnvresG0aug. [sent-137, score-0.482]
</p><p>71 Classification: It is not surprising that the ranking model is better, because when using a classification model, an example should be labeled either positive or negative. [sent-148, score-0.144]
</p><p>72 It is not very reasonable to label a sentence pair like sa ? [sent-149, score-0.257]
</p><p>73 sa+ k (k > 1) as a negative example, nor a positive one, because in some cases, it is easy to conclude one sentence should be arranged after another but hard to decide whether they should be adjacent. [sent-150, score-0.232]
</p><p>74 In a ranking model, this informa-  tion can be quantified by the different priorities of sentence pairs with different distances. [sent-152, score-0.274]
</p><p>75 Single Feature Effect: The effects of different types of features are shown in Table 3. [sent-153, score-0.044]
</p><p>76 It can be seen in Table 3 that all these features contribute to the final result. [sent-155, score-0.044]
</p><p>77 The two features of noun probability and dependency probability play an important role as demonstrated in (Lapata, 2003). [sent-156, score-0.044]
</p><p>78 A paragraph which is ordered entirely right by our method is shown in Figure 1. [sent-158, score-0.103]
</p><p>79 (1) Vanunu, 43, is serving an 18-year sentence for treason. [sent-159, score-0.09]
</p><p>80 (2) He was kidnapped by Israel's Mossad spy agency in Rome in 1986 after giving The Sunday Times of London photographs of the inside of the Dimona reactor. [sent-160, score-0.086]
</p><p>81 (3) From the photographs, experts determined that Israel had the world's sixth largest stockpile of nuclear weapons. [sent-161, score-0.09]
</p><p>82 (4) Israel has never confirmed or denied that it has a nuclear capability. [sent-162, score-0.128]
</p><p>83 Sentences which should be arranged together tend to have a higher similarity and overlap. [sent-164, score-0.105]
</p><p>84 Like sentence (3) and (4) in Figure 1, they have a highest cosine similarity of 0. [sent-165, score-0.09]
</p><p>85 However, the similarity or overlap of the two sentences does not help to decide which sentence should be arranged before another. [sent-167, score-0.355]
</p><p>86 In this case the overlap and similarity of half part of the sentences may help. [sent-168, score-0.123]
</p><p>87 For example latter((3)) and former((4)) share an overlap of “Israel” while there is no overlap for latter((4)) and former((3)). [sent-169, score-0.132]
</p><p>88 Coreference is also an important clue for ordering natural language texts. [sent-170, score-0.421]
</p><p>89 For example when conducting coreference resolution for (1) ? [sent-172, score-0.088]
</p><p>90 3  Genetic Algorithm  There are three main parameters for GA including the crossover probability (PC), the mutation probability (PM) and the population size (PS). [sent-177, score-0.219]
</p><p>91 As we can see in Table 4, when adjusting the three parameters the average τ values are all  close to the exhaustive result of 0. [sent-187, score-0.063]
</p><p>92 Table 4 shows that in our case the genetic algorithm is not very sensible to the parameters. [sent-189, score-0.306]
</p><p>93 5  Conclusion and Discussion  In this paper we propose a method for ordering sentences which have no contextual information by making use of Ranking SVM and the genetic algorithm. [sent-195, score-0.741]
</p><p>94 In future work, we will explore more features such as semantic features to further improve the performance. [sent-197, score-0.088]
</p><p>95 A machine learning approach to sentence ordering for multi-document summarization and its evaluation. [sent-201, score-0.55]
</p><p>96 A bottom-up approach to sentence ordering for multi-document summarization. [sent-205, score-0.511]
</p><p>97 Inferring strategies for sentence ordering in multidocument news summarization. [sent-253, score-0.637]
</p><p>98 Catching the drift: Probabilistic content models, with applications to generation and summarization. [sent-257, score-0.054]
</p><p>99 Sentence ordering with event-enriched semantics and two-layered clustering for multi-document news summarization. [sent-261, score-0.472]
</p><p>100 An  adjacency model for sentence ordering in multidocument summarization. [sent-299, score-0.586]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ordering', 0.421), ('genetic', 0.263), ('pref', 0.257), ('bollegala', 0.188), ('sa', 0.167), ('permutation', 0.161), ('fi', 0.158), ('fitness', 0.152), ('israel', 0.128), ('okazaki', 0.116), ('mutation', 0.114), ('arranged', 0.105), ('naoaki', 0.105), ('crossover', 0.105), ('paragraph', 0.103), ('agree', 0.1), ('usa', 0.096), ('ranking', 0.095), ('operator', 0.094), ('mitsuru', 0.09), ('nuclear', 0.09), ('sentence', 0.09), ('coreference', 0.088), ('lapata', 0.087), ('photographs', 0.086), ('vanunu', 0.086), ('unordered', 0.079), ('svmrank', 0.076), ('donghong', 0.076), ('greedy', 0.075), ('multidocument', 0.075), ('barzilay', 0.072), ('ji', 0.072), ('ga', 0.072), ('stroudsburg', 0.071), ('danushka', 0.07), ('precedence', 0.07), ('mds', 0.066), ('overlap', 0.066), ('exhaustive', 0.063), ('characterize', 0.063), ('inproceedings', 0.06), ('madnani', 0.06), ('strength', 0.06), ('sentences', 0.057), ('pa', 0.057), ('coherence', 0.056), ('pairwise', 0.054), ('generation', 0.054), ('nie', 0.054), ('xiaojun', 0.053), ('adverb', 0.053), ('summary', 0.052), ('challenging', 0.052), ('strategy', 0.052), ('quantified', 0.051), ('pc', 0.051), ('lemmatized', 0.051), ('news', 0.051), ('sequential', 0.05), ('denotes', 0.049), ('classification', 0.049), ('judith', 0.049), ('comma', 0.049), ('search', 0.049), ('former', 0.049), ('pm', 0.048), ('cohen', 0.048), ('viewed', 0.046), ('features', 0.044), ('kdd', 0.044), ('defining', 0.044), ('algorithm', 0.043), ('rank', 0.043), ('texts', 0.04), ('determine', 0.04), ('overall', 0.04), ('maximize', 0.04), ('function', 0.039), ('summarization', 0.039), ('latter', 0.039), ('wana', 0.038), ('inversions', 0.038), ('denied', 0.038), ('nova', 0.038), ('wanxiaoj', 0.038), ('busemann', 0.038), ('exits', 0.038), ('kearns', 0.038), ('ayan', 0.038), ('lingpeng', 0.038), ('priorities', 0.038), ('renxian', 0.038), ('ps', 0.037), ('decide', 0.037), ('regina', 0.037), ('joachims', 0.037), ('works', 0.036), ('adjective', 0.036), ('thorsten', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="225-tfidf-1" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>Author: Jiwei Tan ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 1</p><p>2 0.14432311 <a title="225-tfidf-2" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>Author: Camille Guinaudeau ; Michael Strube</p><p>Abstract: We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems.</p><p>3 0.11074369 <a title="225-tfidf-3" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>4 0.095082544 <a title="225-tfidf-4" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>Author: Nathan Gilbert ; Ellen Riloff</p><p>Abstract: Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.</p><p>5 0.080321416 <a title="225-tfidf-5" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>Author: Emmanuel Lassalle ; Pascal Denis</p><p>Abstract: This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of average F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. B3,</p><p>6 0.076317966 <a title="225-tfidf-6" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>7 0.068530954 <a title="225-tfidf-7" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>8 0.06837476 <a title="225-tfidf-8" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>9 0.067240424 <a title="225-tfidf-9" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>10 0.067187235 <a title="225-tfidf-10" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>11 0.067160703 <a title="225-tfidf-11" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>12 0.065603182 <a title="225-tfidf-12" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>13 0.064145088 <a title="225-tfidf-13" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>14 0.063831948 <a title="225-tfidf-14" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>15 0.063674726 <a title="225-tfidf-15" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>16 0.063496657 <a title="225-tfidf-16" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>17 0.06253738 <a title="225-tfidf-17" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>18 0.06172407 <a title="225-tfidf-18" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>19 0.061703201 <a title="225-tfidf-19" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>20 0.061396528 <a title="225-tfidf-20" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, 0.044), (2, 0.014), (3, -0.063), (4, 0.031), (5, 0.043), (6, 0.057), (7, 0.01), (8, -0.079), (9, 0.001), (10, 0.006), (11, 0.001), (12, -0.092), (13, -0.017), (14, -0.068), (15, 0.078), (16, -0.027), (17, 0.008), (18, -0.069), (19, 0.02), (20, -0.021), (21, 0.016), (22, -0.0), (23, -0.06), (24, 0.005), (25, 0.039), (26, 0.003), (27, 0.034), (28, 0.004), (29, 0.047), (30, -0.057), (31, -0.02), (32, 0.029), (33, -0.052), (34, 0.047), (35, -0.084), (36, 0.048), (37, 0.003), (38, -0.023), (39, -0.03), (40, -0.023), (41, 0.008), (42, -0.035), (43, 0.012), (44, -0.029), (45, -0.005), (46, 0.011), (47, -0.013), (48, -0.012), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91252011 <a title="225-lsi-1" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>Author: Jiwei Tan ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 1</p><p>2 0.80628252 <a title="225-lsi-2" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>Author: Camille Guinaudeau ; Michael Strube</p><p>Abstract: We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems.</p><p>3 0.74237525 <a title="225-lsi-3" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>4 0.70948344 <a title="225-lsi-4" href="./acl-2013-Improving_pairwise_coreference_models_through_feature_space_hierarchy_learning.html">196 acl-2013-Improving pairwise coreference models through feature space hierarchy learning</a></p>
<p>Author: Emmanuel Lassalle ; Pascal Denis</p><p>Abstract: This paper proposes a new method for significantly improving the performance of pairwise coreference models. Given a set of indicators, our method learns how to best separate types of mention pairs into equivalence classes for which we construct distinct classification models. In effect, our approach finds an optimal feature space (derived from a base feature set and indicator set) for discriminating coreferential mention pairs. Although our approach explores a very large space of possible feature spaces, it remains tractable by exploiting the structure of the hierarchies built from the indicators. Our exper- iments on the CoNLL-2012 Shared Task English datasets (gold mentions) indicate that our method is robust relative to different clustering strategies and evaluation metrics, showing large and consistent improvements over a single pairwise model using the same base features. Our best system obtains a competitive 67.2 of average F1 over MUC, and CEAF which, despite its simplicity, places it above the mean score of other systems on these datasets. B3,</p><p>5 0.66953331 <a title="225-lsi-5" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>6 0.6625908 <a title="225-lsi-6" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>7 0.66050309 <a title="225-lsi-7" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>8 0.65178132 <a title="225-lsi-8" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>9 0.63928908 <a title="225-lsi-9" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>10 0.63670981 <a title="225-lsi-10" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>11 0.63082212 <a title="225-lsi-11" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>12 0.629996 <a title="225-lsi-12" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>13 0.62495565 <a title="225-lsi-13" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>14 0.62349916 <a title="225-lsi-14" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>15 0.61850893 <a title="225-lsi-15" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>16 0.61781174 <a title="225-lsi-16" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>17 0.6123538 <a title="225-lsi-17" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>18 0.6106351 <a title="225-lsi-18" href="./acl-2013-Simple%2C_readable_sub-sentences.html">322 acl-2013-Simple, readable sub-sentences</a></p>
<p>19 0.61010194 <a title="225-lsi-19" href="./acl-2013-Joint_Apposition_Extraction_with_Syntactic_and_Semantic_Constraints.html">205 acl-2013-Joint Apposition Extraction with Syntactic and Semantic Constraints</a></p>
<p>20 0.60872036 <a title="225-lsi-20" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.062), (6, 0.044), (11, 0.072), (15, 0.013), (24, 0.056), (26, 0.063), (34, 0.01), (35, 0.081), (42, 0.062), (48, 0.058), (61, 0.213), (64, 0.011), (70, 0.052), (88, 0.062), (90, 0.029), (95, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81924355 <a title="225-lda-1" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>Author: Ryan McDonald ; Joakim Nivre ; Yvonne Quirmbach-Brundage ; Yoav Goldberg ; Dipanjan Das ; Kuzman Ganchev ; Keith Hall ; Slav Petrov ; Hao Zhang ; Oscar Tackstrom ; Claudia Bedini ; Nuria Bertomeu Castello ; Jungmee Lee</p><p>Abstract: We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English, Swedish, Spanish, French and Korean. To show the usefulness of such a resource, we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before. This ‘universal’ treebank is made freely available in order to facilitate research on multilingual dependency parsing.1</p><p>2 0.81396794 <a title="225-lda-2" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>Author: Rebecca J. Passonneau ; Emily Chen ; Weiwei Guo ; Dolores Perin</p><p>Abstract: The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students’ summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics.</p><p>same-paper 3 0.80670267 <a title="225-lda-3" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>Author: Jiwei Tan ; Xiaojun Wan ; Jianguo Xiao</p><p>Abstract: Ordering texts is an important task for many NLP applications. Most previous works on summary sentence ordering rely on the contextual information (e.g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 1</p><p>4 0.6534729 <a title="225-lda-4" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>5 0.65163213 <a title="225-lda-5" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>6 0.64213878 <a title="225-lda-6" href="./acl-2013-Coordination_Structures_in_Dependency_Treebanks.html">94 acl-2013-Coordination Structures in Dependency Treebanks</a></p>
<p>7 0.64140141 <a title="225-lda-7" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>8 0.64090288 <a title="225-lda-8" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>9 0.63875234 <a title="225-lda-9" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>10 0.63778269 <a title="225-lda-10" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>11 0.6365965 <a title="225-lda-11" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>12 0.63503784 <a title="225-lda-12" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>13 0.63494396 <a title="225-lda-13" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>14 0.63409686 <a title="225-lda-14" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>15 0.63330042 <a title="225-lda-15" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>16 0.63279641 <a title="225-lda-16" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>17 0.63192099 <a title="225-lda-17" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>18 0.63121974 <a title="225-lda-18" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>19 0.63107342 <a title="225-lda-19" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>20 0.63022679 <a title="225-lda-20" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
