<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-197" href="#">acl2013-197</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</h1>
<br/><p>Source: <a title="acl-2013-197-pdf" href="http://aclweb.org/anthology//P/P13/P13-2122.pdf">pdf</a></p><p>Author: Sanjika Hewavitharana ; Dennis Mehay ; Sankaranarayanan Ananthakrishnan ; Prem Natarajan</p><p>Abstract: We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations. Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. A significant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available. Thus, our approach is well-suited to the causal constraint of spoken conversations. On an English-to-Iraqi CSLT task, the proposed approach gives significant improvements over a baseline system as measured by BLEU, TER, and NIST. Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation.</p><p>Reference: <a title="acl-2013-197-reference" href="../acl2013_reference/acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations. [sent-3, score-1.017]
</p><p>2 Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. [sent-4, score-1.188]
</p><p>3 A significant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available. [sent-5, score-1.354]
</p><p>4 Thus,  our approach is well-suited to the causal constraint of spoken conversations. [sent-6, score-0.145]
</p><p>5 Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation. [sent-8, score-0.25]
</p><p>6 1 Introduction Conversational spoken language translation (CSLT) systems facilitate communication between subjects who do not speak the same language. [sent-9, score-0.256]
</p><p>7 Distribution Statement A (Approved for Public Release, Distribution Unlimited)  conversations typically revolve around a set of central topics, which may not be evident at the beginning of the interaction. [sent-18, score-0.313]
</p><p>8 As the conversation progresses, however, the gradual accumulation of contextual information can be used to infer the topic(s) of discussion, and to deploy contextually appropriate translation phrase pairs. [sent-19, score-0.879]
</p><p>9 For example, the word ‘drugs’ will predominantly translate into Spanish as ‘medicamentos’ (medicines) in a medical scenario, whereas the translation ‘drogas’ (illegal drugs) will predominate in a law enforcement scenario. [sent-20, score-0.2]
</p><p>10 Most CSLT systems do not take high-level global context into account, and instead translate each utterance in isolation. [sent-21, score-0.156]
</p><p>11 This often results in contextually inappropriate translations, and is particularly problematic in conversational speech, which usually exhibits short, spontaneous, and often ambiguous utterances. [sent-22, score-0.198]
</p><p>12 In this paper, we describe a novel topic-based adaptation technique for phrase-based statistical machine translation (SMT) of spoken conversations. [sent-23, score-0.4]
</p><p>13 We begin by building a monolingual la-  tent Dirichlet allocation (LDA) topic model on the training conversations (each conversation corresponds to a “document” in the LDA paradigm). [sent-24, score-1.169]
</p><p>14 At run-time, this model is used to infer a topic distribution over the evolving test conversation up to and including the current utterance. [sent-25, score-1.079]
</p><p>15 Translation phrase pairs that originate in training conversations whose topic distribution is similar to that of the current conversation are given preference through a single similarity feature, which augments the standard phrase-based SMT log-linear model. [sent-26, score-1.433]
</p><p>16 The topic distribution for the test conversation is updated incrementally for each new utterance as the available history grows. [sent-27, score-1.21]
</p><p>17 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 697–701, 2  Relation to Prior Work  Domain adaptation to improve SMT performance  has attracted considerable attention in recent years (Foster and Kuhn, 2007; Finch and Sumita, 2008; Matsoukas et al. [sent-31, score-0.144]
</p><p>18 The general theme is to divide the training data into partitions representing different domains, and to prefer translation options for a test sentence from training domains that most resemble the current document context. [sent-33, score-0.36]
</p><p>19 Weaknesses of this approach include (a) assuming the existence of discrete, non-overlapping domains; and (b) the unreliability of models generated by segments with little training data. [sent-34, score-0.05]
</p><p>20 To avoid the need for hard decisions about domain membership, some have used topic modeling to improve SMT performance, e. [sent-35, score-0.303]
</p><p>21 Perhaps most relevant are the approaches of Gong et al. [sent-40, score-0.034]
</p><p>22 (2012), who both describe adaptation techniques where monolingual LDA topic models are used to ob-  tain a topic distribution over the training data, followed by dynamic adaptation of the phrase table based on the inferred topic of the test document. [sent-42, score-1.516]
</p><p>23 While our proposed approach also employs monolingual LDA topic models, it deviates from the above methods in the following important ways. [sent-43, score-0.373]
</p><p>24 First, the existing approaches are geared towards batch-mode text translation, and assume that the full document context of a test sentence is always available. [sent-44, score-0.033]
</p><p>25 This assumption is incompatible with translation of spoken conversations, which are inherently causal. [sent-45, score-0.256]
</p><p>26 Our proposed approach infers topic distributions incrementally as the conversation progresses. [sent-46, score-0.946]
</p><p>27 Thus, it is not only consistent with the causal requirement, but is also capable of tracking topical changes during the course of a conversation. [sent-47, score-0.077]
</p><p>28 Second, we do not directly augment the translation table with the inferred topic distribution. [sent-48, score-0.506]
</p><p>29 Rather, we compute a similarity between the current conversation history and each of the training conversations, and use this measure to dynamically score the relevance of candidate translation  phrase pairs during decoding. [sent-49, score-1.031]
</p><p>30 3  Corpus Data and Baseline SMT  We use the DARPA TransTac English-Iraqi parallel two-way spoken dialogue collection to train both translation and LDA topic models. [sent-50, score-0.559]
</p><p>31 This data set contains a variety of scenarios, including medical diagnosis; force protection (e. [sent-51, score-0.075]
</p><p>32 ; each transcribed from spoken bilingual conversations and manually translated. [sent-54, score-0.385]
</p><p>33 The SMT parallel training corpus contains approximately 773K sentence pairs (7. [sent-55, score-0.091]
</p><p>34 We used this corpus to extract translation phrase pairs from bidirectional IBM Model 4 word alignment (Och and Ney, 2003) based on the heuristic approach of (Koehn et al. [sent-57, score-0.279]
</p><p>35 Our phrase-based decoder is similar to Moses (Koehn et al. [sent-60, score-0.049]
</p><p>36 , 2007) and uses the phrase pairs and target LM to perform beam search stack decoding based on a standard loglinear model, the parameters of which were tuned  with MERT (Och, 2003) on a held-out development set (3,534 sentence pairs, 45K words) using BLEU as the tuning metric. [sent-61, score-0.158]
</p><p>37 Finally, we evaluated translation performance on a separate, unseen test set (3,138 sentence pairs, 38K words). [sent-62, score-0.186]
</p><p>38 Of the 773K training sentence pairs, about 100K (corresponding to 1,600 conversations) are marked with conversation boundaries. [sent-63, score-0.545]
</p><p>39 We use the English side of these conversations for training LDA topic models. [sent-64, score-0.635]
</p><p>40 All other sentence pairs are assigned to a “background conversation”, which signals the absence of the topic similarity feature for phrase pairs derived from these instances. [sent-65, score-0.54]
</p><p>41 All of the development and test set data were marked with conversation boundaries. [sent-66, score-0.528]
</p><p>42 The training, development and test sets were partitioned at the conversation level, so that we could model a topic distribution for entire conversations, both during training and during tuning and testing. [sent-67, score-0.975]
</p><p>43 4  Incremental Topic-Based Adaptation  Our approach is based on the premise that biasing the translation model to favor phrase pairs origi-  nating in training conversations that are contextually similar to the current conversation will lead to better translation quality. [sent-68, score-1.434]
</p><p>44 The topic distribution is incrementally updated as the conversation history grows, and we recompute the topic similarity between the current conversation and the training conversations for each new source utterance. [sent-69, score-2.266]
</p><p>45 For each conversation di in the training collection (1,600 conversations), LDA infers a topic distribution θdi = p(zk |di) for all latent topics zk = {1, . [sent-73, score-1.189]
</p><p>46 The full conversation history Kis ∈ava {2ila0b,3le0 f,4or0 training ftuhlel topic rmsaodtioenls ahnisdestimating topic distributions in the training set. [sent-79, score-1.333]
</p><p>47 At run-time, however, we construct the conversation history for the tuning and test sets incrementally, one utterance at a time, mirroring a real-world scenario where our knowledge is lim-  ited to the utterances that have been spoken up to that point in time. [sent-80, score-1.067]
</p><p>48 Thus, each development/test utterance is associated with a different conversation history d∗, for which we infer a topic distribution θd∗ = p(zk |d∗) using the trained LDA model. [sent-81, score-1.152]
</p><p>49 We use Mallet (McCallum, 2002) for training topic models and inferring topic distributions. [sent-82, score-0.69]
</p><p>50 2 Topic Similarity Computation For each test utterance, we are able to infer the topic distribution θd∗ based on the accumulated history of the current conversation. [sent-84, score-0.579]
</p><p>51 We use this to compute a measure of similarity between the evolving test conversation and each of the training conversations, for which we already have topic distributions θdi . [sent-85, score-1.088]
</p><p>52 Because θdi and θd∗ are probability distributions, we use the Jensen-Shannon divergence (JSD) to evaluate their similarity (Manning and Sch u¨tze, 1999). [sent-86, score-0.1]
</p><p>53 We define the similarity score as sim(θdi , θd∗ ) = 1− JSD(θdi | |θd∗ ). [sent-88, score-0.07]
</p><p>54 1  Thus, we obtain a vecto)r =of similarity scores indexed by the training conversations. [sent-89, score-0.12]
</p><p>55 3 Integration with the Decoder We provide the SMT decoder with the similarity vector for each test utterance. [sent-91, score-0.152]
</p><p>56 Additionally, the SMT phrase table tracks, for each phrase pair, the set of parent training conversations (including the “background conversation”) from which that phrase pair originated. [sent-92, score-0.587]
</p><p>57 Using this information, the decoder evaluates, for each candidate phrase pair 1JSD(θdi | |θd∗ ) ∈ [0, 1] when defined using log2. [sent-93, score-0.134]
</p><p>58 (Key: incrN = incremental LDA with N topics; convN = non-incremental, whole-conversation LDA with N topics. [sent-123, score-0.176]
</p><p>59 ) X → Y added to the search graph, its topic similarity score as dfo tlolo thwes: s  FX→Y =i∈Pamr(aXx→Y )sim(θdi,θd∗)  (1)  where Par(X → Y ) is the set of training convwehrsearetio Pnsa f(rXom →wh Yich ) itshe th cean sedtid oafte tr phrase pair originated. [sent-124, score-0.601]
</p><p>60 Phrase pairs from the “background conversation” only are assigned a similarity score FX→Y = 0. [sent-125, score-0.111]
</p><p>61 In this way we distill the inferred topic distributions down to a single feature for each candidate phrase pair. [sent-127, score-0.478]
</p><p>62 We add this feature to the log-linear translation model with its own weight, which is tuned with MERT. [sent-128, score-0.153]
</p><p>63 The intuition behind this feature is that the lower bound of suitability of a candidate phrase pair should be directly proportional to the similarity between its most relevant conversational provenance and the current context. [sent-129, score-0.33]
</p><p>64 Phrase pairs which only occur in the background conversation are not directly penalized, but contribute nothing to the topic similarity score. [sent-130, score-0.949]
</p><p>65 699  Figure 1: Rank trajectories of 4 LDA inferred topics, with incremental topic inference. [sent-131, score-0.582]
</p><p>66 This system translated each utterance independently, ignoring higher-level conversational context. [sent-135, score-0.252]
</p><p>67 For the topic-adapted system, we compared translation performance with a varying number of LDA topics. [sent-136, score-0.153]
</p><p>68 In intuitive agreement with the approximate number of scenario types known to be covered by our data set, a range of 20-40 topics yielded the best results. [sent-137, score-0.107]
</p><p>69 We compared the proposed incremental topic tracking approach to a non-causal oracle approach that had up-front access to the entire source conversations at run-time. [sent-138, score-0.87]
</p><p>70 In all cases, we compared translation performance on both clean-text and automatic speech recognition (ASR) transcriptions of the source utterances. [sent-139, score-0.196]
</p><p>71 ASR transcriptions were generated using a high-performance two-pass HMM-based system, which delivered a word error rate (WER) of 10. [sent-140, score-0.043]
</p><p>72 Table 1 summarizes test set performance in  BLEU (Papineni et al. [sent-142, score-0.033]
</p><p>73 Given the morphological complexity of Iraqi Arabic, computing string-based metrics on raw output can be misleadingly low and does not always reflect whether the core message was conveyed. [sent-145, score-0.031]
</p><p>74 We note that in all settings (incremental and non-causal oracle) our adaptation approach matches or significantly outperforms the baseline across multiple evaluation metrics. [sent-147, score-0.144]
</p><p>75 In particular, the incremental LDA system with 40 topics is the top-scoring system in both clean-text and ASR settings. [sent-148, score-0.233]
</p><p>76 6  Discussion and Future Directions  We have presented a novel, incremental topic-  based translation model adaptation approach that obeys the causality constraint imposed by spoken conversations. [sent-153, score-0.607]
</p><p>77 We have also demonstrated that incremental adaptation on an evolving conversation performs better than oracle adaptation based on the complete conversation history. [sent-155, score-1.625]
</p><p>78 This figure illustrates the rank trajectory of four LDA topics as the incremental conversation grows. [sent-157, score-0.728]
</p><p>79 We indicate (in superscript) the topic identity of most relevant words in an utterance that are associated with that topic. [sent-159, score-0.493]
</p><p>80 At the first utterance, the top-ranked topic is “5”, due to the occurrence of “captain” in the greeting. [sent-160, score-0.303]
</p><p>81 As the conversation evolves, we note that this topic become less prominent. [sent-161, score-0.798]
</p><p>82 The conversation shifts to a discussion on “windows”, raising the prominence of topic “4”. [sent-162, score-0.798]
</p><p>83 Finally, topic “3” becomes prominent due to the presence of the 700  words “project” and “contract”. [sent-163, score-0.303]
</p><p>84 Thus, the incremental approach is able to track the topic trajectories in the conversation, and is able to select more relevant phrase pairs than oracle LDA, which estimates one topic distribution for the entire conversation. [sent-164, score-1.131]
</p><p>85 In this work we have used only the source language utterance in inferring the topic distribution. [sent-165, score-0.493]
</p><p>86 As a next step, we plan to use SMT-generated English translation of Iraqi utterances to improve topic estimation. [sent-167, score-0.5]
</p><p>87 Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. [sent-177, score-0.153]
</p><p>88 A study of translation edit rate with targeted human annotation. [sent-247, score-0.153]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('conversation', 0.495), ('topic', 0.303), ('conversations', 0.282), ('lda', 0.226), ('cslt', 0.209), ('incremental', 0.176), ('utterance', 0.156), ('translation', 0.153), ('adaptation', 0.144), ('smt', 0.106), ('di', 0.103), ('spoken', 0.103), ('contextually', 0.102), ('iraqi', 0.102), ('evolving', 0.097), ('conversational', 0.096), ('history', 0.092), ('ter', 0.091), ('asr', 0.088), ('phrase', 0.085), ('jsd', 0.085), ('zk', 0.08), ('oracle', 0.074), ('nist', 0.071), ('similarity', 0.07), ('biiannsc', 0.07), ('bitam', 0.07), ('incrementally', 0.069), ('bleu', 0.063), ('distribution', 0.062), ('checkpoint', 0.061), ('systembleu', 0.061), ('topics', 0.057), ('morristown', 0.057), ('gong', 0.053), ('tam', 0.053), ('drugs', 0.053), ('trajectories', 0.053), ('bbn', 0.053), ('training', 0.05), ('inferred', 0.05), ('scenario', 0.05), ('decoder', 0.049), ('koehn', 0.049), ('finch', 0.049), ('diagnosis', 0.049), ('stroudsburg', 0.048), ('medical', 0.047), ('matsoukas', 0.047), ('eidelman', 0.047), ('current', 0.045), ('infer', 0.044), ('statmt', 0.044), ('nj', 0.044), ('utterances', 0.044), ('fx', 0.043), ('transcriptions', 0.043), ('och', 0.042), ('causal', 0.042), ('pairs', 0.041), ('background', 0.04), ('arabic', 0.04), ('distributions', 0.04), ('snover', 0.04), ('monolingual', 0.039), ('infers', 0.039), ('pa', 0.038), ('tracking', 0.035), ('inferring', 0.034), ('relevant', 0.034), ('test', 0.033), ('franz', 0.033), ('josef', 0.032), ('tuning', 0.032), ('foster', 0.032), ('dirichlet', 0.032), ('revolve', 0.031), ('transtac', 0.031), ('misleadingly', 0.031), ('ited', 0.031), ('itshe', 0.031), ('cean', 0.031), ('clst', 0.031), ('deviates', 0.031), ('dfo', 0.031), ('mirroring', 0.031), ('natarajan', 0.031), ('obeys', 0.031), ('vecto', 0.031), ('bing', 0.03), ('divergence', 0.03), ('sim', 0.03), ('philipp', 0.03), ('lm', 0.03), ('options', 0.029), ('illegal', 0.028), ('protection', 0.028), ('biasing', 0.028), ('prem', 0.028), ('lane', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="197-tfidf-1" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>Author: Sanjika Hewavitharana ; Dennis Mehay ; Sankaranarayanan Ananthakrishnan ; Prem Natarajan</p><p>Abstract: We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations. Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. A significant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available. Thus, our approach is well-suited to the causal constraint of spoken conversations. On an English-to-Iraqi CSLT task, the proposed approach gives significant improvements over a baseline system as measured by BLEU, TER, and NIST. Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation.</p><p>2 0.21440108 <a title="197-tfidf-2" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>3 0.19852756 <a title="197-tfidf-3" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>4 0.19076806 <a title="197-tfidf-4" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>Author: Maryam Habibi ; Andrei Popescu-Belis</p><p>Abstract: A new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. Inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. The method is evaluated on excerpts of the Fisher and AMI corpora, using a crowdsourcing platform to elicit comparative relevance judgments. The results demonstrate that the method outperforms two competitive baselines.</p><p>5 0.17145211 <a title="197-tfidf-5" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>Author: Rico Sennrich ; Holger Schwenk ; Walid Aransa</p><p>Abstract: While domain adaptation techniques for SMT have proven to be effective at improving translation quality, their practicality for a multi-domain environment is often limited because of the computational and human costs of developing and maintaining multiple systems adapted to different domains. We present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also de- scribe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1BLEU over unadapted systems and single-domain adaptation.</p><p>6 0.16329937 <a title="197-tfidf-6" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>7 0.15862973 <a title="197-tfidf-7" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>8 0.15147743 <a title="197-tfidf-8" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>9 0.15046999 <a title="197-tfidf-9" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>10 0.13649377 <a title="197-tfidf-10" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>11 0.13170509 <a title="197-tfidf-11" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>12 0.11797266 <a title="197-tfidf-12" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>13 0.11425064 <a title="197-tfidf-13" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>14 0.11362423 <a title="197-tfidf-14" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>15 0.10888942 <a title="197-tfidf-15" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>16 0.10702363 <a title="197-tfidf-16" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>17 0.1064868 <a title="197-tfidf-17" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>18 0.10579217 <a title="197-tfidf-18" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>19 0.1045297 <a title="197-tfidf-19" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>20 0.10399131 <a title="197-tfidf-20" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.243), (1, -0.038), (2, 0.2), (3, 0.056), (4, 0.081), (5, -0.028), (6, 0.109), (7, -0.017), (8, -0.155), (9, -0.021), (10, 0.074), (11, 0.148), (12, 0.086), (13, 0.203), (14, 0.067), (15, -0.056), (16, -0.138), (17, 0.038), (18, -0.01), (19, -0.009), (20, -0.036), (21, -0.046), (22, 0.029), (23, 0.015), (24, 0.034), (25, 0.076), (26, 0.03), (27, -0.033), (28, 0.053), (29, -0.024), (30, 0.119), (31, 0.049), (32, -0.004), (33, -0.0), (34, 0.011), (35, 0.045), (36, 0.051), (37, 0.027), (38, 0.068), (39, -0.0), (40, 0.014), (41, 0.072), (42, -0.004), (43, 0.042), (44, -0.056), (45, 0.019), (46, -0.035), (47, -0.059), (48, -0.036), (49, -0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95958221 <a title="197-lsi-1" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>Author: Sanjika Hewavitharana ; Dennis Mehay ; Sankaranarayanan Ananthakrishnan ; Prem Natarajan</p><p>Abstract: We describe a translation model adaptation approach for conversational spoken language translation (CSLT), which encourages the use of contextually appropriate translation options from relevant training conversations. Our approach employs a monolingual LDA topic model to derive a similarity measure between the test conversation and the set of training conversations, which is used to bias translation choices towards the current context. A significant novelty of our adaptation technique is its incremental nature; we continuously update the topic distribution on the evolving test conversation as new utterances become available. Thus, our approach is well-suited to the causal constraint of spoken conversations. On an English-to-Iraqi CSLT task, the proposed approach gives significant improvements over a baseline system as measured by BLEU, TER, and NIST. Interestingly, the incremental approach outperforms a non-incremental oracle that has up-front knowledge of the whole conversation.</p><p>2 0.74739397 <a title="197-lsi-2" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>3 0.72574323 <a title="197-lsi-3" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>4 0.70881486 <a title="197-lsi-4" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>Author: Maryam Habibi ; Andrei Popescu-Belis</p><p>Abstract: A new method for keyword extraction from conversations is introduced, which preserves the diversity of topics that are mentioned. Inspired from summarization, the method maximizes the coverage of topics that are recognized automatically in transcripts of conversation fragments. The method is evaluated on excerpts of the Fisher and AMI corpora, using a crowdsourcing platform to elicit comparative relevance judgments. The results demonstrate that the method outperforms two competitive baselines.</p><p>5 0.67795146 <a title="197-lsi-5" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>6 0.65503734 <a title="197-lsi-6" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>7 0.64332002 <a title="197-lsi-7" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>8 0.63959247 <a title="197-lsi-8" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>9 0.61906582 <a title="197-lsi-9" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>10 0.61338627 <a title="197-lsi-10" href="./acl-2013-Are_School-of-thought_Words_Characterizable%3F.html">54 acl-2013-Are School-of-thought Words Characterizable?</a></p>
<p>11 0.60019094 <a title="197-lsi-11" href="./acl-2013-Natural_Language_Models_for_Predicting_Programming_Comments.html">257 acl-2013-Natural Language Models for Predicting Programming Comments</a></p>
<p>12 0.59635848 <a title="197-lsi-12" href="./acl-2013-Improved_Bayesian_Logistic_Supervised_Topic_Models_with_Data_Augmentation.html">191 acl-2013-Improved Bayesian Logistic Supervised Topic Models with Data Augmentation</a></p>
<p>13 0.58604741 <a title="197-lsi-13" href="./acl-2013-Vector_Space_Model_for_Adaptation_in_Statistical_Machine_Translation.html">383 acl-2013-Vector Space Model for Adaptation in Statistical Machine Translation</a></p>
<p>14 0.57243562 <a title="197-lsi-14" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>15 0.56532127 <a title="197-lsi-15" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>16 0.5615136 <a title="197-lsi-16" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>17 0.55970317 <a title="197-lsi-17" href="./acl-2013-Stacking_for_Statistical_Machine_Translation.html">328 acl-2013-Stacking for Statistical Machine Translation</a></p>
<p>18 0.55453277 <a title="197-lsi-18" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>19 0.55382514 <a title="197-lsi-19" href="./acl-2013-Task_Alternation_in_Parallel_Sentence_Retrieval_for_Twitter_Translation.html">338 acl-2013-Task Alternation in Parallel Sentence Retrieval for Twitter Translation</a></p>
<p>20 0.54501742 <a title="197-lsi-20" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.045), (6, 0.029), (11, 0.039), (24, 0.056), (26, 0.03), (35, 0.083), (42, 0.073), (48, 0.031), (70, 0.028), (88, 0.023), (90, 0.4), (95, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95008069 <a title="197-lda-1" href="./acl-2013-On_the_Predictability_of_Human_Assessment%3A_when_Matrix_Completion_Meets_NLP_Evaluation.html">263 acl-2013-On the Predictability of Human Assessment: when Matrix Completion Meets NLP Evaluation</a></p>
<p>Author: Guillaume Wisniewski</p><p>Abstract: This paper tackles the problem of collecting reliable human assessments. We show that knowing multiple scores for each example instead of a single score results in a more reliable estimation of a system quality. To reduce the cost of collecting these multiple ratings, we propose to use matrix completion techniques to predict some scores knowing only scores of other judges and some common ratings. Even if prediction performance is pretty low, decisions made using the predicted score proved to be more reliable than decision based on a single rating of each example.</p><p>2 0.92918456 <a title="197-lda-2" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>Author: Akiko Eriguchi ; Ichiro Kobayashi</p><p>Abstract: In a multi-class document categorization using graph-based semi-supervised learning (GBSSL), it is essential to construct a proper graph expressing the relation among nodes and to use a reasonable categorization algorithm. Furthermore, it is also important to provide high-quality correct data as training data. In this context, we propose a method to construct a similarity graph by employing both surface information and latent information to express similarity between nodes and a method to select high-quality training data for GBSSL by means of the PageR- ank algorithm. Experimenting on Reuters21578 corpus, we have confirmed that our proposed methods work well for raising the accuracy of a multi-class document categorization.</p><p>3 0.91127932 <a title="197-lda-3" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>Author: Fabienne Braune ; Nina Seemann ; Daniel Quernheim ; Andreas Maletti</p><p>Abstract: We present a new translation model integrating the shallow local multi bottomup tree transducer. We perform a largescale empirical evaluation of our obtained system, which demonstrates that we significantly beat a realistic tree-to-tree baseline on the WMT 2009 English → German tlriannes olnati tohne tWasMk.T TA 2s0 an a Edndgitliisonha →l c Gonetrrmibauntion we make the developed software and complete tool-chain publicly available for further experimentation.</p><p>4 0.90740633 <a title="197-lda-4" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>Author: Stefan L. Frank ; Leun J. Otten ; Giulia Galli ; Gabriella Vigliocco</p><p>Abstract: We investigated the effect of word surprisal on the EEG signal during sentence reading. On each word of 205 experimental sentences, surprisal was estimated by three types of language model: Markov models, probabilistic phrasestructure grammars, and recurrent neural networks. Four event-related potential components were extracted from the EEG of 24 readers of the same sentences. Surprisal estimates under each model type formed a significant predictor of the amplitude of the N400 component only, with more surprising words resulting in more negative N400s. This effect was mostly due to content words. These findings provide support for surprisal as a gener- ally applicable measure of processing difficulty during language comprehension.</p><p>5 0.89980352 <a title="197-lda-5" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<p>Author: ThuyLinh Nguyen ; Stephan Vogel</p><p>Abstract: Hiero translation models have two limitations compared to phrase-based models: 1) Limited hypothesis space; 2) No lexicalized reordering model. We propose an extension of Hiero called PhrasalHiero to address Hiero’s second problem. Phrasal-Hiero still has the same hypothesis space as the original Hiero but incorporates a phrase-based distance cost feature and lexicalized reodering features into the chart decoder. The work consists of two parts: 1) for each Hiero translation derivation, find its corresponding dis- continuous phrase-based path. 2) Extend the chart decoder to incorporate features from the phrase-based path. We achieve significant improvement over both Hiero and phrase-based baselines for ArabicEnglish, Chinese-English and GermanEnglish translation.</p><p>6 0.86670446 <a title="197-lda-6" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>same-paper 7 0.8557924 <a title="197-lda-7" href="./acl-2013-Incremental_Topic-Based_Translation_Model_Adaptation_for_Conversational_Spoken_Language_Translation.html">197 acl-2013-Incremental Topic-Based Translation Model Adaptation for Conversational Spoken Language Translation</a></p>
<p>8 0.62617409 <a title="197-lda-8" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>9 0.60931653 <a title="197-lda-9" href="./acl-2013-Generalized_Reordering_Rules_for_Improved_SMT.html">166 acl-2013-Generalized Reordering Rules for Improved SMT</a></p>
<p>10 0.60051197 <a title="197-lda-10" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>11 0.60032594 <a title="197-lda-11" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>12 0.59827143 <a title="197-lda-12" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>13 0.58821321 <a title="197-lda-13" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>14 0.56620187 <a title="197-lda-14" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>15 0.55884612 <a title="197-lda-15" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>16 0.54427004 <a title="197-lda-16" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>17 0.53880435 <a title="197-lda-17" href="./acl-2013-Docent%3A_A_Document-Level_Decoder_for_Phrase-Based_Statistical_Machine_Translation.html">127 acl-2013-Docent: A Document-Level Decoder for Phrase-Based Statistical Machine Translation</a></p>
<p>18 0.53876734 <a title="197-lda-18" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>19 0.53836274 <a title="197-lda-19" href="./acl-2013-Diverse_Keyword_Extraction_from_Conversations.html">126 acl-2013-Diverse Keyword Extraction from Conversations</a></p>
<p>20 0.53546011 <a title="197-lda-20" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
