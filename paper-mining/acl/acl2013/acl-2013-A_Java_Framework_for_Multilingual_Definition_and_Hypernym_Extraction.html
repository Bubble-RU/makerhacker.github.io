<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-6" href="#">acl2013-6</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</h1>
<br/><p>Source: <a title="acl-2013-6-pdf" href="http://aclweb.org/anthology//P/P13/P13-4018.pdf">pdf</a></p><p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.</p><p>Reference: <a title="acl-2013-6-reference" href="../acl2013_reference/acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 it i  Abstract In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. [sent-3, score-0.255]
</p><p>2 Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. [sent-4, score-0.113]
</p><p>3 We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. [sent-5, score-0.688]
</p><p>4 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al. [sent-6, score-0.177]
</p><p>5 , 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al. [sent-9, score-0.15]
</p><p>6 Nonetheless, raw text often contains several definitional sentences, that is, it provides within itselfformal explanations for terms ofinterest. [sent-13, score-0.282]
</p><p>7 Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. [sent-14, score-0.369]
</p><p>8 However, these methods suffer from two signifiicant drawbacks: on the one hand, low recall (as definitional sentences occur in highly variable syntactic structures), and, on the other hand, noise (because the most frequent definitional pattern X is a Y is inherently very –  –  noisy). [sent-17, score-0.603]
</p><p>9 A recent approach to definition and hypernym extraction, called Word-Class Lattices (Navigli and Velardi, 2010, WCLs), overcomes these issues by addressing the variability of definitional sentences and providing a flexible way of automatically extracting hypernyms from them. [sent-18, score-0.936]
</p><p>10 To do so, lattice-based classifiers are learned from a training set of textual definitions. [sent-19, score-0.059]
</p><p>11 Training sentences are automatically clustered by similarity and, for each such cluster, a lattice classifier is learned which models the variants of the definition template detected. [sent-20, score-0.326]
</p><p>12 A lattice is a directed acyclic graph, a subclass ofnon-deterministic finite state automata. [sent-21, score-0.06]
</p><p>13 The purpose of the lattice structure is to preserve (in a compact form) the salient differences among distinct sequences. [sent-22, score-0.06]
</p><p>14 Since multilinguality is a key need in today’s information society, and because WCLs have been tested overwhelmingly only with the English language, we provide experiments for three different languages, namely English, French and Italian. [sent-24, score-0.041]
</p><p>15 To do so, in contrast to Navigli and Velardi (2010), who created a manually annotated training set of definitions, we provide a heuristic method for the au-  tomatic acquisition of reliable training sets from Wikipedia, and use them to determine the robustness and generalization power of WCLs. [sent-25, score-0.136]
</p><p>16 We show high performance in definition and hypernym extraction for our three languages. [sent-26, score-0.581]
</p><p>17 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioinngauli Lsitnicgsu,i psatgices 103–108, [In geography, a country]DF [is]V F [a political division]GF . [sent-34, score-0.032]
</p><p>18 [In finance, a bond]DF [is]V F [a negotiable certificate]GF [that that acknowledges. [sent-35, score-0.046]
</p><p>19 Table 1: Example definitions (defined  terms are  marked in bold face, their hypernyms in italics). [sent-43, score-0.19]
</p><p>20 geography finance  division certificate  InpoNetNr1y,ahTARGETi  aJJNNm2easure  country bond foot  Figure 1: The DF and GF Word-Class Lattices for the  political negotiable  sentences  in Table 1. [sent-44, score-0.399]
</p><p>21 following fields (Storrer and Wellinghoff, 2006): definiendum (DF), definitor (VF), definiens (GF) and rest (REST), where DF is the part of the definition including the word being defined (e. [sent-45, score-0.378]
</p><p>22 , “In computer science, a closure”), VF is the verb phrase used to introduce the definition (e. [sent-47, score-0.167]
</p><p>23 , “a firstclass function”, hypernym marked in italics) and RF includes additional clauses (e. [sent-51, score-0.408]
</p><p>24 Consider a set of training sentences T, each of which is automatically part-of-speech tagged and manually bracketed with the DF, VF, GF and REST fields (examples are shown in Table 1). [sent-54, score-0.158]
</p><p>25 , w|s| , where wi is the i-th word of s, we generalize |s it|s words wi to word classes ωi as follows:  ωi=? [sent-66, score-0.08]
</p><p>26 wPiOS(wi) oift hweirw∈i Fse that is, a word wi is left unchanged if it occurs frequently in the training corpus (i. [sent-67, score-0.073]
</p><p>27 , wi ∈ F) or is transformed to its part of speech tag (P∈O FS)( owri) is) otherwise. [sent-69, score-0.04]
</p><p>28 As a result, we obtain a generalized sentence s0 = ω1 , ω2, . [sent-70, score-0.037]
</p><p>29 For instance, given the first sentence in Tabl|es| 1, we obtain the corresponding generalized sentence: “In NN, a hTARGETi is a JJ NN”, where NN and JJ indicate thhTeA nRoGunE Tanid is sa adj JeJc tNivNe” c,l washseerse, NreNsp aencdtiv JeJly in. [sent-74, score-0.037]
</p><p>30 2 Learning The WCL learning algorithm consists of 3 steps: • Star patterns: each sentence in the training sSetta irs ppartetperroncse:ss eeadc han sden gteenncerea ilnize thde t tor ain sintagr pattern by replacing with * all the words wi ∈ F, i. [sent-76, score-0.11]
</p><p>31 In Figure 1 we show the resulting lattices for the DF and GF fields built for the cluster of sentences of Table 1. [sent-80, score-0.325]
</p><p>32 Note that during the construction of the lattice the nodes associated with the hypernym words in the learning sentences (i. [sent-81, score-0.474]
</p><p>33 , division, certificate and measure) are marked as hypernyms in order to determine the hypernym of a test sentence at classification time (see (Navigli and Velardi, 2010) for details). [sent-83, score-0.571]
</p><p>34 Given a test sentence s, we consider all possible combinations of definiendum, definitor and definiens lattices and select the combination ofthe three WCLs that best fits the sentence, if such a combination exists. [sent-86, score-0.41]
</p><p>35 In fact, choosing the most appropriate combination of lattices impacts the performance of hypernym extraction. [sent-87, score-0.633]
</p><p>36 selecting the words in the input sentence that are marked as hypernyms in the WCL selected for GF. [sent-90, score-0.14]
</p><p>37 As a result, we are able to fully automatize the definition and hypernym extraction by utilizing collaboratively-curated encyclopedia content. [sent-92, score-0.581]
</p><p>38 candidate definition extraction: we iterate through the collection of Wikipedia pages for the language of interest. [sent-95, score-0.198]
</p><p>39 For each article we extract the first paragraph, which usually, but not always, contains a definitional sentence for the concept expressed by the page title. [sent-96, score-0.356]
</p><p>40 We discard all those pages for which the title corresponds to a special page (i. [sent-97, score-0.095]
</p><p>41 part-of-speech tagging and phrase chunking: for each candidate definition we perform part-of-speech tagging and chunking, thus automatically identifying noun, verb, and prepositional phrases (we use TreeTagger (Schmid, 1997)). [sent-111, score-0.225]
</p><p>42 automatic annotation: we replace all the occurrences in the candidate definition of the target term (i. [sent-113, score-0.205]
</p><p>43 h, definiens field) the phrase which contains the hypernym and as DF (i. [sent-123, score-0.469]
</p><p>44 , definiendum field) the phrase which starts at the beginning of the sentence and ends right before the start of the VP tag. [sent-125, score-0.131]
</p><p>45 Finally we mark as REST the remaining phrases after the phrase already tagged as GF. [sent-126, score-0.055]
</p><p>46 For example, given the sentence “Albert Einstein was a German-born theoretical physicist. [sent-127, score-0.037]
</p><p>47 ”, we produce the following sentence annotation: “[Albert Einstein]DF [was]V F [a German-born theoretical physicist]GF . [sent-128, score-0.037]
</p><p>48 ” (target term marked in bold and hypernym in italics). [sent-129, score-0.441]
</p><p>49 filtering: we finally discard all the candidate definitions for which not all fields could be found during the previous step (i. [sent-131, score-0.145]
</p><p>50 , either the  hTARGETi, hypernym or any DF, VF, GF, hRTEASRTG GtEagT iis, m hiyspseinrngy). [sent-133, score-0.375]
</p><p>51 The numbers are shown in Table 2: starting with 3,904,360 Wikipedia pages for English, 1,617,359 for French and 1,008,044 for Italian (first column), we obtained 1,552,493, 447,772, and 291,259 automatically tagged sentences, respectively, for the three languages (second column in the Table). [sent-135, score-0.137]
</p><p>52 Since we next had to use these sentences for training our WCLs, we took out a random sample of 1000 sentences for each language which we used for testing purposes. [sent-136, score-0.111]
</p><p>53 We manually annotated each of these sentences as definitional or nondefinitional2 and, in the case of the former, also with the correct hypernym. [sent-137, score-0.321]
</p><p>54 2  Evaluation  We tested the newly acquired training dataset against two test datasets. [sent-139, score-0.08]
</p><p>55 The first dataset was our random sampling of 1000 Wikipedia test sentences which we had set aside for each language (no intersection with the training set, see previous section). [sent-140, score-0.072]
</p><p>56 The second dataset was the same one used in Navigli and Velardi (2010), made up of  sentences from the ukWaC Web corpus (Ferraresi et al. [sent-141, score-0.039]
</p><p>57 , 2008) and used to estimate the definition and hypernym extraction performance on an open text corpus. [sent-142, score-0.581]
</p><p>58 3  Results  Table 3 shows the results obtained on definition (column 2-4) and hypernym extraction (column 57) in terms of precision (P), recall (R) and accuracy (A) on our first dataset. [sent-144, score-0.581]
</p><p>59 Note that accuracy also takes into account candidate definitions in the test set which were tagged as non-definitional (see Section 3. [sent-145, score-0.147]
</p><p>60 In the Table we compare the performance of our English WCL trained from Wikipedia sentences using our automatic procedure against the original performance of WCL 1We used the 21-09-2012 (EN), 17-09-2012 (FR), 21-092012 (IT) dumps. [sent-147, score-0.039]
</p><p>61 2Note that the first sentence of a Wikipedia page might seldom be non-definitional, such as “Basmo fortress is located in the north-western part . [sent-148, score-0.074]
</p><p>62 a ncy (A) of definition and hypernym extraction when testing on our dataset of 1000 randomly sampled Wikipedia first-paragraph sentences. [sent-158, score-0.581]
</p><p>63 EN (2010) refers to the WCL learned from the original manually-curated training set from Navigli and Velardi (2010), while EN, FR and IT refer to WCL trained, respectively, with one of the three training sets automatically acquired from Wikipedia. [sent-159, score-0.167]
</p><p>64 56  Table 4: Estimated WCL definition extraction precision (P) and recall (R), testing a sample of ukWaC sentences. [sent-162, score-0.206]
</p><p>65 on It c 1a,9n0 b8e m saeneuna tlhlya-ts tehle c atuetdom tra ti ncianlgly s aecn-  quired training set considerably improves the performance, as it covers higher variability. [sent-164, score-0.033]
</p><p>66 We note that the recall in both definition and hypernym extraction is higher for French and Italian. [sent-165, score-0.581]
</p><p>67 We remark that the presented results were obtained without any human effort, except for the independent collaborative editing and hyperlinking of Wikipedia pages, and that the overall performances can be improved by manually checking the automatically annotated training datasets. [sent-167, score-0.061]
</p><p>68 As can be seen in Table 4, the estimated precision and recall for WCL definition extraction with the 2010 training set were 94. [sent-169, score-0.239]
</p><p>69 5%, respectively, while with our automatically acquired English training set we obtained a higher precision of 98. [sent-171, score-0.108]
</p><p>70 This second experiment shows that learning WCLs from hundreds of thousands of defini-  tion candidates does not overfit to Wikipedia-style definitional sentences. [sent-174, score-0.282]
</p><p>71 After looking at the automatically acquired training datasets, we noted some erroneous annotations mainly due to the following factors: i) some Wikipedia pages do not start with defini3Available from  http :  / / l . [sent-175, score-0.134]
</p><p>72 tional sentences; ii) they may contain more than one verbal phrase between the defined term and the hypernym; iii) the first link after the verbal phrase does not cover, or partially covers, the correct hypernym. [sent-178, score-0.083]
</p><p>73 The elimination of the above wrongly acquired definitional patterns can be implemented with some language-dependent heuristics or can be done by human annotators. [sent-179, score-0.39]
</p><p>74 In any case, given the presence of a high number of correct annotated sentences, these wrong definitional patterns have a very low impact on the definition and hypernym extraction precision as shown in the above experiments (see Table 3 and Table 4). [sent-180, score-0.924]
</p><p>75 4  Multilingual WCL API  Together with the training and test sets of the above experiments, we also release here our implementation of Word-Class Lattices, available as a Java API. [sent-181, score-0.068]
</p><p>76 As a result the WCL classifier can easily be used programmatically in any Java project. [sent-182, score-0.069]
</p><p>77 After the selection of the target language (line 2), we load the training dataset for the target language (line 4). [sent-184, score-0.033]
</p><p>78 Then an instance of WCLClas s i fier is created (line 8) and the training phase is launched  on the input training corpora (line 9). [sent-185, score-0.066]
</p><p>79 Now the classifier is ready to be tested on any given sentence in the target language (lines 11-16). [sent-186, score-0.068]
</p><p>80 If the classifier output is positive (line 18) we can print the extracted hypernym (line 19). [sent-187, score-0.406]
</p><p>81 The output of the presented code is the string “classifier” which corresponds to the hypernym extracted by WCL for the input sentence “WCL is a kind of classifier”. [sent-188, score-0.412]
</p><p>82 1 Web user interface We also release a Web interface to enable online usage of our WCLs for the English, French and Italian languages. [sent-190, score-0.035]
</p><p>83 term of interest, the candidate definition, select the language of interest and, after submission, in the case of positive response from WCL, obtain  the corresponding hypernym and a graphical representation of the lattices matching the given sentence, as shown in the bottom part of the Figure. [sent-193, score-0.696]
</p><p>84 The graphical representation shows the concatenation of the learned lattices which match the DF, VF, GF parts of the given sentence (see Section 2). [sent-194, score-0.321]
</p><p>85 We also allow the user not to provide the term of interest: in this case all the nouns in the sentence are considered as candidate defined terms. [sent-195, score-0.1]
</p><p>86 The server side produces an HTML page (like the one shown in Figure 3), using the WCL API (see Section 4) to process and test the submitted definition candidate. [sent-197, score-0.179]
</p><p>87 5  Related Work  A great deal of work is concerned with the language independent extraction of definitions. [sent-198, score-0.064]
</p><p>88 Much recent work uses symbolic methods that depend on lexico-syntactic patterns or features, which are manually created or semi-automatically learned as recently done in (Zhang and Jiang, 2009; Westerhout, 2009). [sent-199, score-0.087]
</p><p>89 (2008), in order to improve precision while keeping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. [sent-203, score-0.061]
</p><p>90 (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. [sent-205, score-0.282]
</p><p>91 gov  Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al. [sent-209, score-0.125]
</p><p>92 , 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. [sent-214, score-0.694]
</p><p>93 They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are  taken from WordNet (Miller et al. [sent-217, score-0.039]
</p><p>94 , 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. [sent-218, score-0.089]
</p><p>95 Finally, they train a hypernym classifier based on these features. [sent-219, score-0.406]
</p><p>96 Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. [sent-220, score-0.131]
</p><p>97 Using Hearst’s rules for the automatic acquisition of hyponyms for mining a pharmaceutical corpus. [sent-303, score-0.073]
</p><p>98 The acquisition of lexical knowledge from combined machinereadable dictionary sources. [sent-315, score-0.043]
</p><p>99 Automated detection and annotation of term definitions in German text corpora. [sent-332, score-0.12]
</p><p>100 In Proceedings of the RANLP  2009 Workshop on Definition Extraction, page 61–67, Borovets, Bulgaria. [sent-345, score-0.037]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hypernym', 0.375), ('wcl', 0.346), ('wcls', 0.322), ('definitional', 0.282), ('lattices', 0.258), ('gf', 0.235), ('velardi', 0.202), ('df', 0.193), ('navigli', 0.185), ('htargeti', 0.161), ('definition', 0.142), ('vf', 0.109), ('faralli', 0.088), ('definitions', 0.087), ('roberto', 0.087), ('stefano', 0.084), ('api', 0.078), ('wikipedia', 0.077), ('hypernyms', 0.07), ('definiendum', 0.069), ('definiens', 0.069), ('ukwac', 0.067), ('java', 0.066), ('roma', 0.064), ('extraction', 0.064), ('french', 0.062), ('patterns', 0.061), ('lattice', 0.06), ('paola', 0.058), ('certificate', 0.056), ('italian', 0.054), ('division', 0.049), ('acquired', 0.047), ('geography', 0.047), ('italics', 0.047), ('multilingual', 0.046), ('definitor', 0.046), ('negotiable', 0.046), ('storrer', 0.046), ('yap', 0.046), ('acquisition', 0.043), ('borovets', 0.041), ('hearst', 0.041), ('flati', 0.041), ('multilinguality', 0.041), ('sanfilippo', 0.041), ('wi', 0.04), ('sentences', 0.039), ('web', 0.039), ('cui', 0.039), ('benedictis', 0.038), ('borg', 0.038), ('einstein', 0.038), ('programmatically', 0.038), ('page', 0.037), ('sentence', 0.037), ('hovy', 0.036), ('glossaries', 0.035), ('release', 0.035), ('country', 0.034), ('foot', 0.034), ('bond', 0.034), ('ferraresi', 0.034), ('term', 0.033), ('training', 0.033), ('snow', 0.033), ('marked', 0.033), ('political', 0.032), ('title', 0.032), ('kozareva', 0.031), ('classifier', 0.031), ('candidate', 0.03), ('albert', 0.03), ('hyponyms', 0.03), ('screenshot', 0.03), ('tagged', 0.03), ('automatically', 0.028), ('closure', 0.028), ('fields', 0.028), ('eduard', 0.028), ('dolan', 0.028), ('finance', 0.028), ('star', 0.028), ('ritter', 0.027), ('generalization', 0.027), ('column', 0.027), ('pages', 0.026), ('line', 0.026), ('languages', 0.026), ('learned', 0.026), ('demonstration', 0.025), ('duan', 0.025), ('trec', 0.025), ('phrase', 0.025), ('uppsala', 0.024), ('chunking', 0.024), ('ranlp', 0.024), ('agirre', 0.024), ('rest', 0.024), ('field', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="6-tfidf-1" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.</p><p>2 0.52061415 <a title="6-tfidf-2" href="./acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines.html">152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</a></p>
<p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>3 0.2084866 <a title="6-tfidf-3" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>Author: Tiziano Flati ; Roberto Navigli</p><p>Abstract: We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best f∗it) tahned ∗ argument. Taon idco this, we extract failtl thhee ∗ occurrences ion Wikipedia ewxthraiccht match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach.</p><p>4 0.14786683 <a title="6-tfidf-4" href="./acl-2013-GlossBoot%3A_Bootstrapping_Multilingual_Domain_Glossaries_from_the_Web.html">170 acl-2013-GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web</a></p>
<p>Author: Flavio De Benedictis ; Stefano Faralli ; Roberto Navigli</p><p>Abstract: We present GlossBoot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages. For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns. Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages.</p><p>5 0.083744586 <a title="6-tfidf-5" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>6 0.05546264 <a title="6-tfidf-6" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>7 0.055405781 <a title="6-tfidf-7" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>8 0.054491244 <a title="6-tfidf-8" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>9 0.052095495 <a title="6-tfidf-9" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>10 0.048600797 <a title="6-tfidf-10" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>11 0.048256356 <a title="6-tfidf-11" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>12 0.045629926 <a title="6-tfidf-12" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>13 0.044330966 <a title="6-tfidf-13" href="./acl-2013-Creating_Similarity%3A_Lateral_Thinking_for_Vertical_Similarity_Judgments.html">96 acl-2013-Creating Similarity: Lateral Thinking for Vertical Similarity Judgments</a></p>
<p>14 0.044006828 <a title="6-tfidf-14" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>15 0.043144047 <a title="6-tfidf-15" href="./acl-2013-Question_Classification_Transfer.html">292 acl-2013-Question Classification Transfer</a></p>
<p>16 0.042229321 <a title="6-tfidf-16" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>17 0.042223681 <a title="6-tfidf-17" href="./acl-2013-A_Learner_Corpus-based_Approach_to_Verb_Suggestion_for_ESL.html">8 acl-2013-A Learner Corpus-based Approach to Verb Suggestion for ESL</a></p>
<p>18 0.041441407 <a title="6-tfidf-18" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>19 0.040538222 <a title="6-tfidf-19" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>20 0.040186454 <a title="6-tfidf-20" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.145), (1, 0.039), (2, 0.001), (3, -0.091), (4, -0.018), (5, -0.018), (6, -0.083), (7, 0.002), (8, 0.054), (9, -0.028), (10, -0.027), (11, 0.018), (12, -0.056), (13, 0.004), (14, 0.032), (15, -0.051), (16, 0.054), (17, 0.01), (18, -0.016), (19, 0.014), (20, -0.036), (21, 0.047), (22, -0.069), (23, 0.09), (24, 0.236), (25, 0.072), (26, 0.004), (27, -0.153), (28, 0.054), (29, 0.241), (30, -0.127), (31, 0.325), (32, 0.087), (33, 0.083), (34, 0.321), (35, 0.017), (36, 0.009), (37, 0.241), (38, 0.006), (39, -0.021), (40, 0.027), (41, 0.011), (42, -0.012), (43, -0.049), (44, -0.018), (45, -0.049), (46, -0.092), (47, 0.136), (48, 0.023), (49, 0.001)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92897916 <a title="6-lsi-1" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.</p><p>2 0.88613737 <a title="6-lsi-2" href="./acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines.html">152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</a></p>
<p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>3 0.80402666 <a title="6-lsi-3" href="./acl-2013-GlossBoot%3A_Bootstrapping_Multilingual_Domain_Glossaries_from_the_Web.html">170 acl-2013-GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web</a></p>
<p>Author: Flavio De Benedictis ; Stefano Faralli ; Roberto Navigli</p><p>Abstract: We present GlossBoot, an effective minimally-supervised approach to acquiring wide-coverage domain glossaries for many languages. For each language of interest, given a small number of hypernymy relation seeds concerning a target domain, we bootstrap a glossary from the Web for that domain by means of iteratively acquired term/gloss extraction patterns. Our experiments show high performance in the acquisition of domain terminologies and glossaries for three different languages.</p><p>4 0.51415259 <a title="6-lsi-4" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>Author: Tiziano Flati ; Roberto Navigli</p><p>Abstract: We present SPred, a novel method for the creation of large repositories of semantic predicates. We start from existing collocations to form lexical predicates (e.g., break ∗) and learn the semantic classes that best f∗it) tahned ∗ argument. Taon idco this, we extract failtl thhee ∗ occurrences ion Wikipedia ewxthraiccht match the predicate and abstract its arguments to general semantic classes (e.g., break BODY PART, break AGREEMENT, etc.). Our experiments show that we are able to create a large collection of semantic predicates from the Oxford Advanced Learner’s Dictionary with high precision and recall, and perform well against the most similar approach.</p><p>5 0.4114413 <a title="6-lsi-5" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>Author: Stephen Tratz ; Eduard Hovy</p><p>Abstract: The English ’s possessive construction occurs frequently in text and can encode several different semantic relations; however, it has received limited attention from the computational linguistics community. This paper describes the creation of a semantic relation inventory covering the use of ’s, an inter-annotator agreement study to calculate how well humans can agree on the relations, a large collection of possessives annotated according to the relations, and an accurate automatic annotation system for labeling new examples. Our 21,938 example dataset is by far the largest annotated possessives dataset we are aware of, and both our automatic classification system, which achieves 87.4% accuracy in our classification experiment, and our annotation data are publicly available.</p><p>6 0.37769002 <a title="6-lsi-6" href="./acl-2013-IndoNet%3A_A_Multilingual_Lexical_Knowledge_Network_for_Indian_Languages.html">198 acl-2013-IndoNet: A Multilingual Lexical Knowledge Network for Indian Languages</a></p>
<p>7 0.36041835 <a title="6-lsi-7" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>8 0.35719335 <a title="6-lsi-8" href="./acl-2013-Transfer_Learning_Based_Cross-lingual_Knowledge_Extraction_for_Wikipedia.html">356 acl-2013-Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia</a></p>
<p>9 0.34658411 <a title="6-lsi-9" href="./acl-2013-BRAINSUP%3A_Brainstorming_Support_for_Creative_Sentence_Generation.html">65 acl-2013-BRAINSUP: Brainstorming Support for Creative Sentence Generation</a></p>
<p>10 0.34062105 <a title="6-lsi-10" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>11 0.33166137 <a title="6-lsi-11" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>12 0.32822102 <a title="6-lsi-12" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>13 0.31631923 <a title="6-lsi-13" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>14 0.31388751 <a title="6-lsi-14" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>15 0.2999022 <a title="6-lsi-15" href="./acl-2013-Understanding_Tables_in_Context_Using_Standard_NLP_Toolkits.html">365 acl-2013-Understanding Tables in Context Using Standard NLP Toolkits</a></p>
<p>16 0.29903787 <a title="6-lsi-16" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>17 0.29566535 <a title="6-lsi-17" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>18 0.2917428 <a title="6-lsi-18" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>19 0.28033641 <a title="6-lsi-19" href="./acl-2013-Translating_Italian_connectives_into_Italian_Sign_Language.html">360 acl-2013-Translating Italian connectives into Italian Sign Language</a></p>
<p>20 0.27972135 <a title="6-lsi-20" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.043), (6, 0.024), (11, 0.101), (15, 0.013), (24, 0.041), (26, 0.057), (35, 0.073), (42, 0.036), (48, 0.071), (64, 0.101), (70, 0.044), (85, 0.218), (88, 0.026), (90, 0.01), (95, 0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78940749 <a title="6-lda-1" href="./acl-2013-A_Java_Framework_for_Multilingual_Definition_and_Hypernym_Extraction.html">6 acl-2013-A Java Framework for Multilingual Definition and Hypernym Extraction</a></p>
<p>Author: Stefano Faralli ; Roberto Navigli</p><p>Abstract: In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.</p><p>2 0.78672051 <a title="6-lda-2" href="./acl-2013-Smatch%3A_an_Evaluation_Metric_for_Semantic_Feature_Structures.html">324 acl-2013-Smatch: an Evaluation Metric for Semantic Feature Structures</a></p>
<p>Author: Shu Cai ; Kevin Knight</p><p>Abstract: The evaluation of whole-sentence semantic structures plays an important role in semantic parsing and large-scale semantic structure annotation. However, there is no widely-used metric to evaluate wholesentence semantic structures. In this paper, we present smatch, a metric that calculates the degree of overlap between two semantic feature structures. We give an efficient algorithm to compute the metric and show the results of an inter-annotator agreement study.</p><p>3 0.7620703 <a title="6-lda-3" href="./acl-2013-Efficient_Implementation_of_Beam-Search_Incremental_Parsers.html">133 acl-2013-Efficient Implementation of Beam-Search Incremental Parsers</a></p>
<p>Author: Yoav Goldberg ; Kai Zhao ; Liang Huang</p><p>Abstract: Beam search incremental parsers are accurate, but not as fast as they could be. We demonstrate that, contrary to popular belief, most current implementations of beam parsers in fact run in O(n2), rather than linear time, because each statetransition is actually implemented as an O(n) operation. We present an improved implementation, based on Tree Structured Stack (TSS), in which a transition is performed in O(1), resulting in a real lineartime algorithm, which is verified empiri- cally. We further improve parsing speed by sharing feature-extraction and dotproduct across beam items. Practically, our methods combined offer a speedup of ∼2x over strong baselines on Penn Treeb∼a2nxk sentences, a bnads are eosrd oenrs P eofn magnitude faster on much longer sentences.</p><p>4 0.70305932 <a title="6-lda-4" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>Author: Nathan Gilbert ; Ellen Riloff</p><p>Abstract: Most coreference resolvers rely heavily on string matching, syntactic properties, and semantic attributes of words, but they lack the ability to make decisions based on individual words. In this paper, we explore the benefits of lexicalized features in the setting of domain-specific coreference resolution. We show that adding lexicalized features to off-the-shelf coreference resolvers yields significant performance gains on four domain-specific data sets and with two types of coreference resolution architectures.</p><p>5 0.66122663 <a title="6-lda-5" href="./acl-2013-Extracting_Definitions_and_Hypernym_Relations_relying_on_Syntactic_Dependencies_and_Support_Vector_Machines.html">152 acl-2013-Extracting Definitions and Hypernym Relations relying on Syntactic Dependencies and Support Vector Machines</a></p>
<p>Author: Guido Boella ; Luigi Di Caro</p><p>Abstract: In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</p><p>6 0.657929 <a title="6-lda-6" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>7 0.64125031 <a title="6-lda-7" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>8 0.63764781 <a title="6-lda-8" href="./acl-2013-PATHS%3A_A_System_for_Accessing_Cultural_Heritage_Collections.html">268 acl-2013-PATHS: A System for Accessing Cultural Heritage Collections</a></p>
<p>9 0.63709778 <a title="6-lda-9" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>10 0.61780971 <a title="6-lda-10" href="./acl-2013-TransDoop%3A_A_Map-Reduce_based_Crowdsourced_Translation_for_Complex_Domain.html">355 acl-2013-TransDoop: A Map-Reduce based Crowdsourced Translation for Complex Domain</a></p>
<p>11 0.61357778 <a title="6-lda-11" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>12 0.61181492 <a title="6-lda-12" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>13 0.60250056 <a title="6-lda-13" href="./acl-2013-Outsourcing_FrameNet_to_the_Crowd.html">265 acl-2013-Outsourcing FrameNet to the Crowd</a></p>
<p>14 0.59345061 <a title="6-lda-14" href="./acl-2013-A_Lattice-based_Framework_for_Joint_Chinese_Word_Segmentation%2C_POS_Tagging_and_Parsing.html">7 acl-2013-A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing</a></p>
<p>15 0.59155995 <a title="6-lda-15" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>16 0.59102577 <a title="6-lda-16" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>17 0.59022373 <a title="6-lda-17" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>18 0.59019542 <a title="6-lda-18" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>19 0.58784914 <a title="6-lda-19" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>20 0.58679992 <a title="6-lda-20" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
