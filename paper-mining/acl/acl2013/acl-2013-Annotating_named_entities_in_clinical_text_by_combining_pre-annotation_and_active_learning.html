<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-52" href="#">acl2013-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</h1>
<br/><p>Source: <a title="acl-2013-52-pdf" href="http://aclweb.org/anthology//P/P13/P13-3011.pdf">pdf</a></p><p>Author: Maria Skeppstedt</p><p>Abstract: For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. The task of the annotator is to select the correct version among these two alternatives. To minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classifier.</p><p>Reference: <a title="acl-2013-52-reference" href="../acl2013_reference/acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Annotating named entities in clinical text by combining pre-annotation and active learning Maria Skeppstedt Dept. [sent-1, score-0.874]
</p><p>2 se  Abstract For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. [sent-4, score-0.861]
</p><p>3 In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. [sent-5, score-0.26]
</p><p>4 The task of the annotator is to select the correct version among these two alternatives. [sent-6, score-0.365]
</p><p>5 Therefore, there is a need to explore methods for simplifying the annotation task and for reducing the amount of data that must be annotated. [sent-10, score-0.171]
</p><p>6 Annotation can be simplified by automatic preannotation, in which the task of the annotator is to improve or correct annotations provided by an existing system. [sent-11, score-0.485]
</p><p>7 The amount of data needed to be annotated can be reduced by active learning, i. [sent-12, score-0.406]
</p><p>8 by actively selecting data to annotate that is useful to a machine learning system. [sent-14, score-0.252]
</p><p>9 When using pretagged data, the annotator might, however, be biased to choose the annotation provided by the pretagger. [sent-15, score-0.539]
</p><p>10 2  Background  The background discusses basic ideas of preannotation and active learning, as well as the particular challenges associated with annotating clinical text. [sent-18, score-0.872]
</p><p>11 1 Annotating clinical text A number of text annotation projects have been carried out in the clinical domain, some of them including annotations of clinical named entities, such as mentions of symptoms, diseases and medication. [sent-20, score-1.567]
</p><p>12 As in many specialised domains, expert annotators are typically required to create a reliable annotated clinical corpus. [sent-28, score-0.551]
</p><p>13 A further challenge is posed by the content of the clinical data, which is often sensitive and should therefore only be accessed by a limited number of people. [sent-33, score-0.377]
</p><p>14 Research community annotation is consequently another option that is not always open to annotation projects in the clinical domain, even if there are examples of such community annotations also for clinical text, e. [sent-34, score-1.216]
</p><p>15 tc ud2e0n1t3 R Aessseoacricahti Wonor foksrh Coopm, ppaugteasti 7o4n–a8l0 L,inguistics  To simplify the annotation process, and to minimise the amount of annotated data is therefore  even more important for annotations in the clinical domain than for annotation in general. [sent-40, score-0.963]
</p><p>16 2 Pre-annotation A way to simplify annotation is automatic preannotation (or pre-tagging), in which a text is automatically annotated by an existing system, before it is given to the annotator. [sent-42, score-0.396]
</p><p>17 Instead of annotating unlabelled data, the annotator either corrects mistakes made by this existing system (Chou et al. [sent-43, score-0.536]
</p><p>18 The system providing the pre-annotations could be rule- or terminology based, not requiring annotated data (Mykowiecka and Marciniak, 2011), as well as a machine learning/hybrid system that uses the annotations provided by the annotator to constantly improve the pre-annotation (Tomanek et al. [sent-45, score-0.544]
</p><p>19 There exist several annotation tools that facilitate the use of pre-annotation by allowing the user to import pre-annotations or by providing pre-annotation included in the tools (Neves and Leser, 2012). [sent-47, score-0.171]
</p><p>20 A condition for pre-annotation to be useful is that the produced annotations are good enough, or the effect can be the opposite, slowing the annotators down (Ogren et al. [sent-48, score-0.173]
</p><p>21 Another potential problem with pre-annotation is that it might bias towards the annotations given by the pre-tagging, for instance if a good pre-tagger reduces the attention of the annotators (Fort and Sagot, 2010). [sent-50, score-0.289]
</p><p>22 3 Active learning Active learning can be used to reduce the amount of annotated data needed to successfully train a machine learning model. [sent-52, score-0.231]
</p><p>23 Instead of randomly selecting annotation data, instances in the data that are highly informative, and thereby also highly useful for the machine learning system, are then actively selected. [sent-53, score-0.466]
</p><p>24 There are several methods for selecting the most informative instances among the unlabelled ones in the available pool of data. [sent-56, score-0.39]
</p><p>25 A frequently used method is uncertainty sampling, in which instances that the machine learner is least certain how to classify are selected for annotation. [sent-57, score-0.185]
</p><p>26 If there are more than two classes, the confidence for the most probable class can be used as the measure of uncertainty. [sent-59, score-0.27]
</p><p>27 Only using the certainty level for the most probable classification means that not all available information is used, i. [sent-60, score-0.462]
</p><p>28 the information of the certainty levels for the less probable classes. [sent-62, score-0.414]
</p><p>29 (Settles, 2009) An alternative for a multi-class classifier is therefore to instead use the difference of the certainty levels for the two most probable classes. [sent-63, score-0.452]
</p><p>30 Instances with a small margin, on  the other hand, are difficult to classify, and therefore instances with a small margin are selected for annotation (Schein and Ungar, 2007). [sent-65, score-0.409]
</p><p>31 A common alternative is to use entropy as an uncertainty measure, which takes the certainty levels of all possible classes into account (Settles, 2009). [sent-66, score-0.182]
</p><p>32 There are also methods to ensure that the selected data correctly reflects the distribution in the pool of unlabelled data, avoiding a selection of outliers that would not lead to a correct model of the available data. [sent-68, score-0.324]
</p><p>33 Many different machine learning methods have been used together with active learning for solving various NLP tasks. [sent-71, score-0.37]
</p><p>34 Support vector machines have  been used for text classification (Tong and Koller, 2002), using properties of the support vector machine algorithm for determining what unlabelled data to select for classification. [sent-72, score-0.216]
</p><p>35 75  Olsson (2008) suggests combining active learning and pre-annotation for a named entity recognition task, that is providing the annotator with pretagged data from an actively learned named entity recogniser. [sent-76, score-1.062]
</p><p>36 A pre-annotation that is not good enough might slow the human annotator down, whereas a good pre-annotation might make the annotator lose concentration, trusting the pre-annotation too much, resulting in a biased annotation. [sent-79, score-0.752]
</p><p>37 For annotations of named entities in text, this would mean to only provide pre-tagged entities for which the pre-annotations system is certain. [sent-81, score-0.307]
</p><p>38 Such a high precision pre-tagger might, however, also bias the human annotator towards not correcting the pre-annotation. [sent-82, score-0.387]
</p><p>39 Even more incompatible seems a combination between pre-annotation and active learning, that is to provide the human annotator with pre-tagged data that has been selected for active learning. [sent-83, score-0.985]
</p><p>40 The data selected for annotation when using active learning, is the data for which the pre-annotator is most uncertain and therefore the data which would be least suitable for pre-annotation. [sent-84, score-0.568]
</p><p>41 The method proposed here aims at finding a  way of combining pre-annotation and active learning while reducing the risk of annotation bias. [sent-85, score-0.502]
</p><p>42 The method is focused on the annotation of named entities in clinical text, that is marking of spans of text as well as classification of the spans into an entity class. [sent-88, score-0.806]
</p><p>43 1 Pre-annotation As in standard pre-annotation, the annotator will be presented with pre-tagged data, and does not have to annotate the data from scratch. [sent-90, score-0.416]
</p><p>44 Instead of presenting the best tagging for the human annotator to correct, or to present the n best taggings, the two best taggings produced by a pre-tagger will be presented, with-  out informing the annotator which of them that the pre-tagger considers most likely. [sent-92, score-0.777]
</p><p>45 When being presented with two possible annotations of the same text without knowing which of them that the pre-annotation system considers as most likely, the annotator always has to make an active choice of which annotation to choose. [sent-93, score-0.981]
</p><p>46 Having to consider two alternatives might add cognitive load to the annotator compared to correcting one alternative, but ought to be easier than annotating a text that is not pre-tagged. [sent-95, score-0.544]
</p><p>47 Therefore, having two optional annotations to choose from, reduces the bias problem while at  the same time still offering a method for speeding up the annotation. [sent-98, score-0.195]
</p><p>48 The data will be split into sentences, and one sentence at time will be presented to the annotator for annotation. [sent-102, score-0.363]
</p><p>49 2 Active learning To choose from two presented annotations might also potentially be faster than making corrections to one presented annotation. [sent-104, score-0.302]
</p><p>50 For this to be the case, however, one of the presented annotations has to be a correct annotation. [sent-105, score-0.224]
</p><p>51 In order to achieve that, the proposed method is to use a version of active learning. [sent-106, score-0.292]
</p><p>52 The standard use of active learning is to actively select instances to annotate that are useful to a machine learner. [sent-108, score-0.605]
</p><p>53 Instances for which the machine learning model can make a confident classification are not presented to the annotator, as these instances will be of little benefit for improving the machine learning system. [sent-109, score-0.273]
</p><p>54 The version of active learning proposed here is retaining this general idea of active learning, but is also adding an additional constraint to what instances that are actively selected for annotation. [sent-110, score-0.899]
</p><p>55 This constraint is to only select text passages for which it is probable that one of the two best pre-taggings is correct, i. [sent-111, score-0.371]
</p><p>56 For ensuring that the sentences selected for annotation are informative enough, the previously  described difference of the certainty level of the two most probable classes will be used. [sent-114, score-0.683]
</p><p>57 The same standard for expressing margin as used in (1), can be used here, except that in (1), cp1 and cp2 stand for classification of one instance, whereas in this case the output is a sequence of labels, labelling each token in a sentence. [sent-115, score-0.228]
</p><p>58 Let cp1 be the most probable labelling sequence, cp2 the second most probable labelling sequence and cp3 the third most probable labelling sequence. [sent-117, score-1.015]
</p><p>59 However, instead of minimising the margin  between two classification instances, it is ensured that the margin in high enough. [sent-119, score-0.222]
</p><p>60 That is, the difference in certainty level between the two most probable annotations and the third most probable must be high enough to make it probable that one of the two best classification candidates is correct. [sent-120, score-1.168]
</p><p>61 If the annotator often rejects the two presented pretaggings, text passages for which the pre-tagger is more certain ought to be selected, that is the value of t ought to be increased. [sent-125, score-0.594]
</p><p>62 On the other hand, if one of the presented pre-taggings often is selected by the annotator as the correct annotation, the value of t can be decreased, possibly allowing for annotation instances with a smaller MtoSecond. [sent-126, score-0.738]
</p><p>63 CRF++ can also give the conditional probably  for the output, that is for the entire classification sequence ofa sentence, which is needed in the proposed active learning algorithm. [sent-131, score-0.452]
</p><p>64 4 Materials There is a corpus of Swedish clinical text, i. [sent-133, score-0.377]
</p><p>65 the text in the narrative part of the health record, that contains clinical text from the Stockholm area, from the years 2006-2008 (Dalianis et al. [sent-135, score-0.524]
</p><p>66 A subset of this corpus, containing texts from an emergency unit of internal medicine, has been annotated for four types of named entities: disorder, finding, pharmaceutical drug and body structure (Skeppstedt et al. [sent-137, score-0.356]
</p><p>67 For approximately one third of this annotated corpus, double annotation has been performed, and the instances, for which there were a disagreement, have been resolved by one of the annotators. [sent-139, score-0.305]
</p><p>68 The annotated corpus will form the main source of materials for the study proposed here, and additional data to annotate will be selected from a pool of unlabelled data from internal medicine emergency notes. [sent-140, score-0.569]
</p><p>69 The larger subset of the annotated data, only annotated by one annotator, will be referred to  as Single (containing 45 482 tokens), and the smaller subset, annotated by two annotators, will be referred to as Double (containing 25 370 tokens). [sent-141, score-0.274]
</p><p>70 Train a CRF model with a randomly selected subset of the Single part of the annotated corpus, the seed set. [sent-145, score-0.234]
</p><p>71 Apply the constructed CRF model on unlabelled data from the pool of data from internal medicine emergency notes. [sent-150, score-0.342]
</p><p>72 Let the model, which operates on a sentence level, provide the three most probable label sequences for each sentence, together with its level of certainty. [sent-151, score-0.27]
</p><p>73 Calculate the difference in certainty between the most probable and the third most probable suggestion sequence for each sentence, that is MtoThird. [sent-153, score-0.721]
</p><p>74 Start with a low threshold t and place all sentences with MtoThird above the threshold t in a list of candidates for presenting to the annotator (that is the sentences fulfilling the criterion P(cp1 |x) − P(cp3 |x) > t). [sent-154, score-0.489]
</p><p>75 This is  seof the the  sentence, for which the pre-tagger is most uncertain ofwhich one ofthe two most probable pre-taggings is correct. [sent-158, score-0.32]
</p><p>76 Present the most probable pre-annotation as well as the second most probable preannotation, as shown in Figure 1. [sent-159, score-0.54]
</p><p>77 If the annotator chooses that none of the presented pre-annotations is correct, discard the previous candidate selection and make a new one from the pool with a higher threshold value t. [sent-161, score-0.574]
</p><p>78 , gradually increasing the threshold until the annotator accepts one of the presented pre-annotations. [sent-166, score-0.417]
</p><p>79 Continue presenting the annotator with the two most probable pre-annotations for the sentences in the list of selected candidate  sentences, and allow the human annotator to choose one of the pre-annotations. [sent-168, score-1.018]
</p><p>80 Each selected annotation is added to a set of annotated data. [sent-171, score-0.304]
</p><p>81 The retraining of the model can be carried out as a background process while the human annotator is annotating. [sent-173, score-0.312]
</p><p>82 In order to use the annotator time efficiently, there should not be any waiting time while retraining. [sent-174, score-0.312]
</p><p>83 6 Evaluation The text passages chosen in the selection process will, as explained above, be used to re-train the machine learning model, and used when select-  ing new text passages for annotation. [sent-178, score-0.241]
</p><p>84 The effect of adding additional annotations will also be constantly measured, using cross validation on the seed set. [sent-179, score-0.25]
</p><p>85 The additional data added by the active learning experiments will, however, not be used in the validation part of the cross validation, but only be used as additional training data, in order to make sure that the results are not improved due to easily classified examples being added to the corpus. [sent-180, score-0.366]
</p><p>86 When an actively selected corpus of the same size as the entire Single subset of the corpus has been created, this actively selected corpus will be used for training a machine learning model. [sent-181, score-0.439]
</p><p>87 The hypothesis is that the machine learning model trained on the corpus partly created by pre-tagging and active learning will perform better than the model created on the original Single subset. [sent-184, score-0.37]
</p><p>88 4  Conclusion  A method that combines pre-annotation and active learning, while reducing annotation bias, is proposed. [sent-185, score-0.463]
</p><p>89 A program for presenting pre-annotated data to the human annotator for selection has been constructed, and a corpus of annotated data suitable as a seed set and as evaluation data has been constructed. [sent-186, score-0.52]
</p><p>90 The active learning part of the proposed method remains, however, to be implemented. [sent-187, score-0.331]
</p><p>91 Towards comprehensive syntactic and semantic annotations of the clinical narrative. [sent-194, score-0.497]
</p><p>92 Evaluation of training with an annotation schema for manual annotation of clinical conditions from emergency department reports. [sent-203, score-0.795]
</p><p>93 Annotating a corpus of clinical text records for learning to recognize symptoms automatically. [sent-222, score-0.497]
</p><p>94 Some remarks on automatic semantic annotation of a medical corpus. [sent-233, score-0.171]
</p><p>95 A survey on annotation tools for the biomedical literature. [sent-237, score-0.23]
</p><p>96 Constructing evaluation corpora for automated clinical named entity recognition. [sent-241, score-0.496]
</p><p>97 An analysis of active learning strategies for sequence labeling tasks. [sent-271, score-0.368]
</p><p>98 Using active learning and pre-tagging for annotating clinical findings in health record text. [sent-280, score-0.876]
</p><p>99 Rule-based entity recognition and coverage of SNOMED CT in Swedish clinical text. [sent-284, score-0.421]
</p><p>100 Support vector machine active learning with applications to text classification. [sent-301, score-0.366]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('clinical', 0.377), ('annotator', 0.312), ('active', 0.292), ('probable', 0.27), ('annotation', 0.171), ('skeppstedt', 0.169), ('certainty', 0.144), ('unlabelled', 0.133), ('actively', 0.125), ('annotations', 0.12), ('hercules', 0.112), ('mtosecond', 0.112), ('preannotation', 0.112), ('xn', 0.11), ('settles', 0.107), ('instances', 0.096), ('annotating', 0.091), ('margin', 0.087), ('crf', 0.086), ('mtothird', 0.084), ('symons', 0.084), ('taggings', 0.084), ('uzuner', 0.084), ('pool', 0.083), ('annotated', 0.078), ('health', 0.077), ('emergency', 0.076), ('named', 0.075), ('dalianis', 0.075), ('bias', 0.075), ('presenting', 0.069), ('ogren', 0.069), ('passages', 0.066), ('ought', 0.065), ('med', 0.065), ('stockholm', 0.062), ('seed', 0.061), ('biomedical', 0.059), ('roberts', 0.059), ('biomedicine', 0.056), ('chou', 0.056), ('disorder', 0.056), ('dsv', 0.056), ('louhi', 0.056), ('neves', 0.056), ('pretagged', 0.056), ('schein', 0.056), ('smbm', 0.056), ('entities', 0.056), ('labelling', 0.056), ('maria', 0.056), ('double', 0.056), ('selected', 0.055), ('threshold', 0.054), ('annotators', 0.053), ('annotate', 0.053), ('correct', 0.053), ('presented', 0.051), ('uncertain', 0.05), ('medicine', 0.05), ('olsson', 0.05), ('classification', 0.048), ('inform', 0.047), ('enough', 0.046), ('pharmaceutical', 0.046), ('fort', 0.046), ('craven', 0.046), ('guergana', 0.046), ('minimise', 0.046), ('mykowiecka', 0.046), ('symptoms', 0.046), ('entity', 0.044), ('informative', 0.043), ('koeling', 0.043), ('specialised', 0.043), ('assoc', 0.043), ('retrained', 0.043), ('scheffer', 0.043), ('tomanek', 0.043), ('chooses', 0.041), ('might', 0.041), ('materials', 0.041), ('drug', 0.041), ('subset', 0.04), ('learning', 0.039), ('xia', 0.039), ('zurich', 0.039), ('switzerland', 0.039), ('burr', 0.038), ('september', 0.038), ('alternative', 0.038), ('sequence', 0.037), ('needed', 0.036), ('text', 0.035), ('validation', 0.035), ('selecting', 0.035), ('classify', 0.034), ('constantly', 0.034), ('incompatible', 0.034), ('none', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999923 <a title="52-tfidf-1" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>Author: Maria Skeppstedt</p><p>Abstract: For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. The task of the annotator is to select the correct version among these two alternatives. To minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classifier.</p><p>2 0.15091746 <a title="52-tfidf-2" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>3 0.1372693 <a title="52-tfidf-3" href="./acl-2013-Reducing_Annotation_Effort_for_Quality_Estimation_via_Active_Learning.html">300 acl-2013-Reducing Annotation Effort for Quality Estimation via Active Learning</a></p>
<p>Author: Daniel Beck ; Lucia Specia ; Trevor Cohn</p><p>Abstract: Quality estimation models provide feedback on the quality of machine translated texts. They are usually trained on humanannotated datasets, which are very costly due to its task-specific nature. We investigate active learning techniques to reduce the size of these datasets and thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors. ,t .</p><p>4 0.13146706 <a title="52-tfidf-4" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>5 0.12150823 <a title="52-tfidf-5" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>Author: Dan Garrette ; Jason Mielens ; Jason Baldridge</p><p>Abstract: Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and eval- uate how the amounts of various kinds of data affect performance of a trained POS-tagger. Our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available.</p><p>6 0.10505758 <a title="52-tfidf-6" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>7 0.099658296 <a title="52-tfidf-7" href="./acl-2013-WebAnno%3A_A_Flexible%2C_Web-based_and_Visually_Supported_System_for_Distributed_Annotations.html">385 acl-2013-WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations</a></p>
<p>8 0.074015997 <a title="52-tfidf-8" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>9 0.07262487 <a title="52-tfidf-9" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>10 0.072286911 <a title="52-tfidf-10" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>11 0.068613857 <a title="52-tfidf-11" href="./acl-2013-What_causes_a_causal_relation%3F_Detecting_Causal_Triggers_in_Biomedical_Scientific_Discourse.html">386 acl-2013-What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse</a></p>
<p>12 0.066771053 <a title="52-tfidf-12" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>13 0.064691059 <a title="52-tfidf-13" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>14 0.063121855 <a title="52-tfidf-14" href="./acl-2013-Domain-Specific_Coreference_Resolution_with_Lexicalized_Features.html">130 acl-2013-Domain-Specific Coreference Resolution with Lexicalized Features</a></p>
<p>15 0.06293191 <a title="52-tfidf-15" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>16 0.062706769 <a title="52-tfidf-16" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>17 0.062127575 <a title="52-tfidf-17" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>18 0.06087739 <a title="52-tfidf-18" href="./acl-2013-An_Empirical_Study_on_Uncertainty_Identification_in_Social_Media_Context.html">45 acl-2013-An Empirical Study on Uncertainty Identification in Social Media Context</a></p>
<p>19 0.06076666 <a title="52-tfidf-19" href="./acl-2013-Online_Relative_Margin_Maximization_for_Statistical_Machine_Translation.html">264 acl-2013-Online Relative Margin Maximization for Statistical Machine Translation</a></p>
<p>20 0.056863867 <a title="52-tfidf-20" href="./acl-2013-Linguistic_Models_for_Analyzing_and_Detecting_Biased_Language.html">232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.164), (1, 0.045), (2, -0.031), (3, -0.042), (4, 0.028), (5, 0.006), (6, 0.013), (7, -0.009), (8, 0.059), (9, 0.018), (10, -0.038), (11, 0.025), (12, -0.109), (13, 0.04), (14, -0.118), (15, -0.058), (16, -0.046), (17, 0.091), (18, 0.018), (19, -0.045), (20, -0.021), (21, 0.027), (22, -0.13), (23, 0.052), (24, -0.023), (25, -0.008), (26, -0.015), (27, -0.037), (28, -0.094), (29, -0.006), (30, -0.013), (31, 0.006), (32, -0.02), (33, 0.041), (34, -0.1), (35, 0.065), (36, 0.067), (37, 0.045), (38, 0.038), (39, -0.072), (40, 0.046), (41, -0.042), (42, 0.138), (43, -0.008), (44, 0.081), (45, -0.012), (46, 0.058), (47, 0.039), (48, 0.048), (49, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94157058 <a title="52-lsi-1" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>Author: Maria Skeppstedt</p><p>Abstract: For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. The task of the annotator is to select the correct version among these two alternatives. To minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classifier.</p><p>2 0.75104374 <a title="52-lsi-2" href="./acl-2013-WebAnno%3A_A_Flexible%2C_Web-based_and_Visually_Supported_System_for_Distributed_Annotations.html">385 acl-2013-WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations</a></p>
<p>Author: Seid Muhie Yimam ; Iryna Gurevych ; Richard Eckart de Castilho ; Chris Biemann</p><p>Abstract: We present WebAnno, a general purpose web-based annotation tool for a wide range of linguistic annotations. WebAnno offers annotation project management, freely configurable tagsets and the management of users in different roles. WebAnno uses modern web technology for visualizing and editing annotations in a web browser. It supports arbitrarily large documents, pluggable import/export filters, the curation of annotations across various users, and an interface to farming out annotations to a crowdsourcing platform. Currently WebAnno allows part-ofspeech, named entity, dependency parsing and co-reference chain annotations. The architecture design allows adding additional modes of visualization and editing, when new kinds of annotations are to be supported.</p><p>3 0.7334463 <a title="52-lsi-3" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>Author: Ulle Endriss ; Raquel Fernandez</p><p>Abstract: Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</p><p>4 0.73002028 <a title="52-lsi-4" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>Author: Dan Garrette ; Jason Mielens ; Jason Baldridge</p><p>Abstract: Developing natural language processing tools for low-resource languages often requires creating resources from scratch. While a variety of semi-supervised methods exist for training from incomplete data, there are open questions regarding what types of training data should be used and how much is necessary. We discuss a series of experiments designed to shed light on such questions in the context of part-of-speech tagging. We obtain timed annotations from linguists for the low-resource languages Kinyarwanda and Malagasy (as well as English) and eval- uate how the amounts of various kinds of data affect performance of a trained POS-tagger. Our results show that annotation of word types is the most important, provided a sufficiently capable semi-supervised learning infrastructure is in place to project type information onto a raw corpus. We also show that finitestate morphological analyzers are effective sources of type information when few labeled examples are available.</p><p>5 0.66872019 <a title="52-lsi-5" href="./acl-2013-Reducing_Annotation_Effort_for_Quality_Estimation_via_Active_Learning.html">300 acl-2013-Reducing Annotation Effort for Quality Estimation via Active Learning</a></p>
<p>Author: Daniel Beck ; Lucia Specia ; Trevor Cohn</p><p>Abstract: Quality estimation models provide feedback on the quality of machine translated texts. They are usually trained on humanannotated datasets, which are very costly due to its task-specific nature. We investigate active learning techniques to reduce the size of these datasets and thus annotation effort. Experiments on a number of datasets show that with as little as 25% of the training instances it is possible to obtain similar or superior performance compared to that of the complete datasets. In other words, our active learning query strategies can not only reduce annotation effort but can also result in better quality predictors. ,t .</p><p>6 0.63314354 <a title="52-lsi-6" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>7 0.6254859 <a title="52-lsi-7" href="./acl-2013-AnnoMarket%3A_An_Open_Cloud_Platform_for_NLP.html">51 acl-2013-AnnoMarket: An Open Cloud Platform for NLP</a></p>
<p>8 0.59729344 <a title="52-lsi-8" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>9 0.59165972 <a title="52-lsi-9" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>10 0.58485389 <a title="52-lsi-10" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<p>11 0.53784531 <a title="52-lsi-11" href="./acl-2013-Linguistic_Models_for_Analyzing_and_Detecting_Biased_Language.html">232 acl-2013-Linguistic Models for Analyzing and Detecting Biased Language</a></p>
<p>12 0.52557117 <a title="52-lsi-12" href="./acl-2013-Development_and_Analysis_of_NLP_Pipelines_in_Argo.html">118 acl-2013-Development and Analysis of NLP Pipelines in Argo</a></p>
<p>13 0.52381796 <a title="52-lsi-13" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>14 0.52269608 <a title="52-lsi-14" href="./acl-2013-Part-of-speech_tagging_with_antagonistic_adversaries.html">277 acl-2013-Part-of-speech tagging with antagonistic adversaries</a></p>
<p>15 0.51768893 <a title="52-lsi-15" href="./acl-2013-A_computational_approach_to_politeness_with_application_to_social_factors.html">30 acl-2013-A computational approach to politeness with application to social factors</a></p>
<p>16 0.50637484 <a title="52-lsi-16" href="./acl-2013-Patient_Experience_in_Online_Support_Forums%3A_Modeling_Interpersonal_Interactions_and_Medication_Use.html">278 acl-2013-Patient Experience in Online Support Forums: Modeling Interpersonal Interactions and Medication Use</a></p>
<p>17 0.47268721 <a title="52-lsi-17" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>18 0.46905929 <a title="52-lsi-18" href="./acl-2013-QuEst_-_A_translation_quality_estimation_framework.html">289 acl-2013-QuEst - A translation quality estimation framework</a></p>
<p>19 0.46517742 <a title="52-lsi-19" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>20 0.45651531 <a title="52-lsi-20" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.058), (6, 0.096), (11, 0.066), (15, 0.034), (24, 0.046), (26, 0.056), (35, 0.083), (42, 0.047), (48, 0.035), (64, 0.011), (70, 0.029), (88, 0.039), (90, 0.015), (91, 0.256), (95, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77568656 <a title="52-lda-1" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>Author: Maria Skeppstedt</p><p>Abstract: For expanding a corpus of clinical text, annotated for named entities, a method that combines pre-tagging with a version of active learning is proposed. In order to facilitate annotation and to avoid bias, two alternative automatic pre-taggings are presented to the annotator, without revealing which of them is given a higher confidence by the pre-tagging system. The task of the annotator is to select the correct version among these two alternatives. To minimise the instances in which none of the presented pre-taggings is correct, the texts presented to the annotator are actively selected from a pool of unlabelled text, with the selection criterion that one of the presented pre-taggings should have a high probability of being correct, while still being useful for improving the result of an automatic classifier.</p><p>2 0.67869741 <a title="52-lda-2" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>Author: Rui Xia ; Tao Wang ; Xuelei Hu ; Shoushan Li ; Chengqing Zong</p><p>Abstract: Bag-of-words (BOW) is now the most popular way to model text in machine learning based sentiment classification. However, the performance of such approach sometimes remains rather limited due to some fundamental deficiencies of the BOW model. In this paper, we focus on the polarity shift problem, and propose a novel approach, called dual training and dual prediction (DTDP), to address it. The basic idea of DTDP is to first generate artificial samples that are polarity-opposite to the original samples by polarity reversion, and then leverage both the original and opposite samples for (dual) training and (dual) prediction. Experimental results on four datasets demonstrate the effectiveness of the proposed approach for polarity classification. 1</p><p>3 0.6353423 <a title="52-lda-3" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>Author: Oleg Rokhlenko ; Idan Szpektor</p><p>Abstract: We introduce the novel task of automatically generating questions that are relevant to a text but do not appear in it. One motivating example of its application is for increasing user engagement around news articles by suggesting relevant comparable questions, such as “is Beyonce a better singer than Madonna?”, for the user to answer. We present the first algorithm for the task, which consists of: (a) offline construction of a comparable question template database; (b) ranking of relevant templates to a given article; and (c) instantiation of templates only with entities in the article whose comparison under the template’s relation makes sense. We tested the suggestions generated by our algorithm via a Mechanical Turk experiment, which showed a significant improvement over the strongest baseline of more than 45% in all metrics.</p><p>4 0.60061824 <a title="52-lda-4" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>Author: Lis Pereira ; Erlyn Manguilimotan ; Yuji Matsumoto</p><p>Abstract: This study addresses issues of Japanese language learning concerning word combinations (collocations). Japanese learners may be able to construct grammatically correct sentences, however, these may sound “unnatural”. In this work, we analyze correct word combinations using different collocation measures and word similarity methods. While other methods use well-formed text, our approach makes use of a large Japanese language learner corpus for generating collocation candidates, in order to build a system that is more sensitive to constructions that are difficult for learners. Our results show that we get better results compared to other methods that use only wellformed text. 1</p><p>5 0.58913326 <a title="52-lda-5" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>Author: Joohyun Kim ; Raymond Mooney</p><p>Abstract: We adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees.</p><p>6 0.58231801 <a title="52-lda-6" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>7 0.57847273 <a title="52-lda-7" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>8 0.56896478 <a title="52-lda-8" href="./acl-2013-Modeling_Thesis_Clarity_in_Student_Essays.html">246 acl-2013-Modeling Thesis Clarity in Student Essays</a></p>
<p>9 0.56764632 <a title="52-lda-9" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>10 0.56498456 <a title="52-lda-10" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>11 0.56460464 <a title="52-lda-11" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>12 0.56185901 <a title="52-lda-12" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>13 0.56030214 <a title="52-lda-13" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>14 0.55947161 <a title="52-lda-14" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>15 0.55788243 <a title="52-lda-15" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>16 0.55757684 <a title="52-lda-16" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>17 0.55502951 <a title="52-lda-17" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>18 0.5549047 <a title="52-lda-18" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>19 0.55410087 <a title="52-lda-19" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>20 0.55370319 <a title="52-lda-20" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
