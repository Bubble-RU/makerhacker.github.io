<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 acl-2013-Annotation of regular polysemy and underspecification</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-53" href="#">acl2013-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 acl-2013-Annotation of regular polysemy and underspecification</h1>
<br/><p>Source: <a title="acl-2013-53-pdf" href="http://aclweb.org/anthology//P/P13/P13-2127.pdf">pdf</a></p><p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>Reference: <a title="acl-2013-53-reference" href="../acl2013_reference/acl-2013-Annotation_of_regular_polysemy_and_underspecification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Annotation of regular polysemy and underspecification  H ´ector Mart ı´nez Alonso, Bolette Sandford Pedersen University of Copenhagen Copenhagen (Denmark) alons o @ hum . [sent-1, score-0.297]
</p><p>2 ,  Abstract We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. [sent-5, score-0.475]
</p><p>3 1 Introduction  This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. [sent-7, score-0.156]
</p><p>4 Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al. [sent-8, score-0.118]
</p><p>5 The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al. [sent-12, score-0.255]
</p><p>6 Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. [sent-15, score-0.21]
</p><p>7 We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. [sent-16, score-0.27]
</p><p>8 Here we describe the annotation process, its results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy and, MACE an unsupervised system to choose the most likely sense from all the annotations. [sent-17, score-0.819]
</p><p>9 b) England was being kept busy with other concerns c) England was, after all, an important wine market In case a), England refers to the English territory (Location), whereas in b) it refers arguably to England as a political entity (Organization). [sent-24, score-0.09]
</p><p>10 The ability of certain  words to switch between semantic types in a predictable manner is referred to as regular polysemy. [sent-26, score-0.092]
</p><p>11 Unlike other forms of meaning variation caused by metaphor or homonymy, regular polysemy is considered to be caused by metonymy (Apresjan, 1974; Lapata and Lascarides, 2003). [sent-27, score-0.352]
</p><p>12 Regular polysemy is different from other forms of polysemy in that both senses can be active at the same in a predicate, which we refer to as underspecification. [sent-28, score-0.418]
</p><p>13 Contextually complex: England was, after all, an important wine market 2. [sent-30, score-0.05]
</p><p>14 Vague, in which no contextual element enforces a reading: The case of England is similar 3  Choice of semantic classes  The Generative Lexicon (GL) (Pustejovsky, 1995) groups nouns with their most frequent metonymic sense in a semantic class called a dot type. [sent-32, score-0.718]
</p><p>15 For  English, we annotate 5 dot types from the GL: 1. [sent-33, score-0.224]
</p><p>16 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioinngauli Lsitnicgsu,i psatgicess 725–730,  ”the chicken was delicious ”. [sent-37, score-0.054]
</p><p>17 We chose the first one because we consider it the most prototypical case of metonymy from the ones listed in the GL. [sent-51, score-0.117]
</p><p>18 , 2005) and a previous area of research in metonymy resolution (Markert and Nissim, 2009). [sent-53, score-0.117]
</p><p>19 4  Annotation Scheme  For each of the nine (five for English, two for Danish, two for Spanish) dot types, we have randomly selected 500 corpus examples. [sent-54, score-0.201]
</p><p>20 Each example consists of a sentence with a selected headword belonging to the corresponding dot type. [sent-55, score-0.231]
</p><p>21 In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. [sent-56, score-0.106]
</p><p>22 For most of the English examples we used the words in Rumshisky (2007), except for Location/Organization. [sent-59, score-0.04]
</p><p>23 , 2012) as thesaurus to make the total of occurrences reach 500 after we had removed homonyms and other forms of semantic variation outside of the purview of  regular polysemy. [sent-63, score-0.117]
</p><p>24 Every dot type has its particularities that we had to deal with. [sent-68, score-0.223]
</p><p>25 This lexical phenomenon does not impede metonymy for the animal names, it just makes it less likely. [sent-70, score-0.148]
</p><p>26 In order to assess this, we have included 20 examples of cow. [sent-71, score-0.04]
</p><p>27 The rest of the dataset consists of animal names that do not participate in this lexical alternation, like eel, duck, chicken, or sardine. [sent-72, score-0.061]
</p><p>28 We call the first sense in the pair of metonyms that make up the dot type the literal sense, and the second sense the metonymic sense, e. [sent-73, score-1.135]
</p><p>29 Each block of 500 sentences belonging to a  dot type was an independent annotation subtask with an isolated description. [sent-76, score-0.287]
</p><p>30 The annotator was shown an example and had to determine whether the headword in the example had the literal, metonymic or the underspecified sense. [sent-77, score-0.801]
</p><p>31 Figure 1 shows an instance of the annotation process. [sent-78, score-0.064]
</p><p>32 Markert and Nissim acknowledge a mixed sense they define as being literal and metonymic at the same time. [sent-80, score-0.739]
</p><p>33 For English we used Amazon Mechanical Turk (AMT) with five annotations per example by turkers certified as Classification Masters. [sent-81, score-0.141]
</p><p>34 Using AMT provides annotations very quickly, possibly at the expense of reliability, but it has been proven suitable for sense-disambiguation task (Snow et al. [sent-82, score-0.048]
</p><p>35 Moreover, it is not possible to obtain annotations for every language using AMT. [sent-84, score-0.048]
</p><p>36 Thus, for Danish and Spanish, we obtained annotations from volunteers, most of them native or very proficient non-natives. [sent-85, score-0.048]
</p><p>37 See Table 1 for a summary of the annotation setup for each language. [sent-86, score-0.064]
</p><p>38 After the annotation task we obtained the agree726  SEDLnap gan l igsui hs ahgean 3o6t5-7a4torsv o Atlyu Mpn etTe er Table 1: Amount and type of annotators per in-  stance for each language. [sent-87, score-0.108]
</p><p>39 4523 Table 2: Averaged observed agreement and its standard deviation and alpha Average observed agreement (Ao) is the mean across examples for the proportion of matching senses assigned by the annotators. [sent-97, score-0.384]
</p><p>40 Krippendorff’s alpha is an aggregate measure that takes chance disagreement in consideration and accounts for the replicability of an annotation scheme. [sent-98, score-0.149]
</p><p>41 The scheme can only provide reliable (Artstein and Poesio, 2008) annotations (α > 0. [sent-100, score-0.089]
</p><p>42 This indicates that not all dot types are equally easy to annotate, regardless of the kind of annotator. [sent-102, score-0.201]
</p><p>43 In spite of the number and type of annotators, the Location/Organization dot type gives fairly high agreement values for a semantic task, and this behavior is consistent across languages. [sent-103, score-0.304]
</p><p>44 5  Assigning sense by majority voting  Each example has more than one annotation and we need to determine a single sense tag for each example. [sent-104, score-0.708]
</p><p>45 However, if we assign senses by majority voting, we need a backoff strategy in case of ties. [sent-105, score-0.364]
</p><p>46 The common practice of backing off to the most frequent sense is not valid in this scenario, where there can be a tie between the metonymic and the underspecified sense. [sent-106, score-0.989]
</p><p>47 We use a backoff that incorporates our assumption about the relations 2We have made the data freely available at http://metashare. [sent-107, score-0.119]
</p><p>48 q=regular+polysemy  between senses, namely that the underspecified sense sits between the literal and the metonymic senses: 1. [sent-110, score-1.166]
</p><p>49 If there is a tie between the underspecified and literal senses, the sense is literal. [sent-111, score-0.867]
</p><p>50 If there is a tie between the underspecified and metonymic sense, the sense is metonymic. [sent-113, score-0.989]
</p><p>51 If there is a tie between the literal and metonymic sense or between all three senses, the sense is underspecified. [sent-115, score-0.957]
</p><p>52 The preference for the underspecified sense varies greatly, from the very infrequent for English in Animal/Meat to the two Danish datasets where the underspecified sense evens with the metonymic one. [sent-117, score-1.565]
</p><p>53 However, the Danish examples have mostly three annotators, and chance disagreement is the highest for this language in this setup, i. [sent-118, score-0.094]
</p><p>54 , the chance for an underspecified sense in Danish to be assigned by our backoff strategy is the highest. [sent-120, score-0.76]
</p><p>55 Columns V and B show respectively whether the underspecified senses are a result of majority voting or backoff. [sent-121, score-0.878]
</p><p>56 In contrast to volunteers, turkers disprefer the underspecified option and most of the English underspecified senses are assigned by backoff. [sent-122, score-1.128]
</p><p>57 However, it cannot be argued that turkers have overused clicking on the first option (a common spamming behavior) because we can see that two of the English dot types (eng:artinfo, eng:procres) have majority of metonymic senses,  which are always second in the scheme (cf. [sent-123, score-0.723]
</p><p>58 Looking at the amount of underspecified senses that have been obtained by majority voting for Danish and Spanish, we suggest that the level of abstraction required by this annotation is too high for turkers to perform at a level compara727  ble to that of our volunteer annotators. [sent-126, score-1.014]
</p><p>59 Figure 2: Proportion of non-literality in location names across languages Figure 2 shows the proportion of non-literal  (metonymic+underspecified) examples for the Location/Organization words that are common across languages. [sent-127, score-0.147]
</p><p>60 We can see that individual words show sense skewdness. [sent-128, score-0.173]
</p><p>61 America has a high proportion of non-literal senses in the ANC, where it usually means ”the population or government of the US”. [sent-131, score-0.219]
</p><p>62 Similarly, it is literal less than 50% of the times for the other two languages. [sent-132, score-0.222]
</p><p>63 In contrast, Afghanistan is most often used in its literal location sense. [sent-133, score-0.262]
</p><p>64 6  Assigning senses with MACE  Besides using majority voting with backoff , we use MACE (Hovy et al. [sent-134, score-0.548]
</p><p>65 0037485460 Table 4: Sense distributions calculated with MACE, plus Difference and Intersection of underspecified senses between methods MACE is an unsupervised system that uses Expectation-Maximization (EM) to estimate the competence of annotators and recover the most likely answer. [sent-139, score-0.654]
</p><p>66 This EM method can also be understood as a clustering that assigns the value of the closest calculated latent variable (the sense tag) to each data point (the distribution of annotations). [sent-141, score-0.173]
</p><p>67 Datasets that show less variation between senses calculated using majority voting and using MACE will be more reliable. [sent-142, score-0.476]
</p><p>68 Table 4 shows a smoother distribution of senses than Table 3, as majority classes are downweighted by MACE. [sent-144, score-0.247]
</p><p>69 It takes very different decisions than majority voting for the two English datasets with lowest agreement (eng:artinfo, eng:procres) and for the Danish datasets, which have the fewest annotators. [sent-145, score-0.327]
</p><p>70 Although MACE increases the frequency of the underspecified senses for all datasets but one (eng:locorg), the underspecified examples in Table 3 are not subsumed by the MACE results. [sent-149, score-1.097]
</p><p>71 The values in the Icolumn show that none of the underspecified senses of eng:contcont receive the underspecified sense by MACE. [sent-150, score-1.238]
</p><p>72 All of these examples, however, were resolved by backoff, as well as most of the other underspecified cases in the other English datasets. [sent-151, score-0.427]
</p><p>73 In contrast to the voting method, MACE does not operate with any theoretical assumption about the relation between the three senses and treats them independently when assigning the most likely sense tag to each distri-  bution of annotations. [sent-152, score-0.628]
</p><p>74 7  Comparison between methods  The voting system and MACE provide different sense tags. [sent-153, score-0.377]
</p><p>75 The following examples (three from eng:contcont and four from eng:locorg) show disagreement between the sense tag assigned by voting and by MACE: d) To ship a crate of lettuce across the country, a trucker needed permission from a federal regulatory agency. [sent-154, score-0.497]
</p><p>76 e) Controls were sent a package containing stool collection vials and instructions for collection and mailing of samples. [sent-155, score-0.041]
</p><p>77 h) This is Boston ’s commercial and financial  heart , but it sfarfrom being an homogeneous district [. [sent-158, score-0.072]
</p><p>78 All of the previous examples were tagged as underspecified by either the voting system or MACE, but not by both. [sent-164, score-0.671]
</p><p>79 Table 5 breaks down the five annotations that each example received by turkers in literal, metonymic and underspecified. [sent-165, score-0.507]
</p><p>80 The last two columns show the sense tag provided by voting or MACE. [sent-166, score-0.445]
</p><p>81 Exampdefl) e)321LM12 1U21VOTUMLINGMALU CE ghij) ) 212320222112MUULUMMU  Table 5: Annotation summary and sense tags for the examples in this section Just by looking at the table it is not immediate which method is preferable to assign sense tags in cases that are not clear-cut. [sent-167, score-0.386]
</p><p>82 In the case of i), we consider the underspecified sense more adequate than the literal one obtained by voting, just like we are also more prone to prefer the underspecified meaning in f), which has been assigned by MACE. [sent-168, score-1.269]
</p><p>83 In the case of h), we consider that the strictly metonymic sense assigned by MACE does not capture both the organization- (”commercial and financial”) and location-related (”district”) aspects of the meaning, and we would prefer the underspecifed reading. [sent-169, score-0.537]
</p><p>84 However, MACE can also overgenerate the underspecified sense, as the vials mentioned in example e) are empty and have no content yet, thereby being literal containers and not their content. [sent-170, score-0.69]
</p><p>85 Examples d), g) and h) have the same distribution of annotations—namely 2 literal, 2  metonymic and 1 underspecified—but d) has received the literal sense from MACE, whereas the other two are metonymic. [sent-171, score-0.761]
</p><p>86 The three examples receive the underspecified sense from the voting scheme, since neither the literal or metonymic sense is more present in the annotations. [sent-173, score-1.612]
</p><p>87 On the other hand, e) and i) are skewed towards literality and receive the literal sense by plurality without having to resort to any backoff, but they are marked as underspecified by MACE. [sent-174, score-0.851]
</p><p>88 8  Conclusions  We have described the annotation process of a regular-polysemy corpus in English, Danish and Spanish which deals with five different dot types. [sent-175, score-0.286]
</p><p>89 After annotating the examples for their literal, metonymic or underspecified reading, we have determined that this scheme can provide reliable (α over 0. [sent-176, score-0.852]
</p><p>90 Not all the dot types are equally easy to annotate. [sent-178, score-0.201]
</p><p>91 The main source of variation in agreement, and  thus annotation reliability, is the dot type itself. [sent-179, score-0.312]
</p><p>92 9  Further work  After collecting annotated data, the natural next step is to attempt class-based word-sense disambiguation (WSD) to predict the senses in Tables 3 and 4 using a state-of-the-art system like Nastase et al. [sent-181, score-0.182]
</p><p>93 We will consider a sense-assignment method (voting or MACE) as more appropriate if it provides the sense tags that are easiest to learn by our WSD system. [sent-183, score-0.209]
</p><p>94 We also consider the possibility of developing a sense-assignment method that relies both on the theoretical assumption behind the voting scheme and the latent-variable approach used by MACE. [sent-185, score-0.245]
</p><p>95 Acknowledgments The research leading to these results has been funded by the European Commission’s 7th Framework Program under grant agreement 238405 (CLARA). [sent-186, score-0.037]
</p><p>96 Modeling regular polysemy: A study on the semantic classification of catalan adjectives. [sent-205, score-0.092]
</p><p>97 Towards a corpus annotated for metonymies: the case of location names. [sent-253, score-0.04]
</p><p>98 Local and global context for supervised and unsupervised metonymy resolution. [sent-265, score-0.117]
</p><p>99 Dannet: the challenge of compiling a wordnet for danish  by reusing a monolingual dictionary. [sent-286, score-0.179]
</p><p>100 Sense-based interpretation of logical metonymy using a statistical method. [sent-313, score-0.117]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mace', 0.43), ('underspecified', 0.427), ('metonymic', 0.344), ('literal', 0.222), ('voting', 0.204), ('dot', 0.201), ('senses', 0.182), ('danish', 0.179), ('sense', 0.173), ('eng', 0.151), ('markert', 0.145), ('england', 0.138), ('polysemy', 0.118), ('metonymy', 0.117), ('nissim', 0.109), ('backoff', 0.097), ('regular', 0.092), ('rumshisky', 0.084), ('spanish', 0.073), ('turkers', 0.072), ('majority', 0.065), ('annotation', 0.064), ('artinfo', 0.061), ('locorg', 0.061), ('procres', 0.061), ('chicken', 0.054), ('underspecification', 0.054), ('pustejovsky', 0.053), ('annotations', 0.048), ('tie', 0.045), ('scheme', 0.041), ('contcont', 0.041), ('johannessen', 0.041), ('vials', 0.041), ('location', 0.04), ('examples', 0.04), ('columns', 0.039), ('proportion', 0.037), ('agreement', 0.037), ('afghanistan', 0.036), ('apresjan', 0.036), ('asmussen', 0.036), ('easiest', 0.036), ('euralex', 0.036), ('metonymies', 0.036), ('ide', 0.035), ('hum', 0.033), ('readings', 0.033), ('pedersen', 0.032), ('district', 0.031), ('alpha', 0.031), ('anc', 0.031), ('animal', 0.031), ('disagreement', 0.031), ('names', 0.03), ('headword', 0.03), ('andersen', 0.03), ('receive', 0.029), ('tag', 0.029), ('boleda', 0.029), ('katja', 0.029), ('nastase', 0.028), ('alternation', 0.028), ('macleod', 0.028), ('volunteers', 0.027), ('tax', 0.027), ('gl', 0.026), ('copenhagen', 0.026), ('wine', 0.026), ('briscoe', 0.025), ('amt', 0.025), ('variation', 0.025), ('market', 0.024), ('artstein', 0.024), ('distributions', 0.023), ('annotate', 0.023), ('chance', 0.023), ('spite', 0.022), ('english', 0.022), ('annotators', 0.022), ('received', 0.022), ('america', 0.022), ('freely', 0.022), ('type', 0.022), ('five', 0.021), ('wsd', 0.021), ('datasets', 0.021), ('assigning', 0.021), ('ao', 0.021), ('financial', 0.021), ('contrastive', 0.02), ('turk', 0.02), ('book', 0.02), ('commercial', 0.02), ('refers', 0.02), ('assigned', 0.02), ('snow', 0.02), ('strategy', 0.02), ('treats', 0.019), ('box', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="53-tfidf-1" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>2 0.15623511 <a title="53-tfidf-2" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>Author: Eirini Florou</p><p>Abstract: As one of the most challenging issues in NLP, metaphor identification and its interpretation have seen many models and methods proposed. This paper presents a study on metaphor identification based on the semantic similarity between literal and non literal meanings of words that can appear at the same context.</p><p>3 0.15005606 <a title="53-tfidf-3" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>Author: Mohammad Taher Pilehvar ; David Jurgens ; Roberto Navigli</p><p>Abstract: Semantic similarity is an essential component of many Natural Language Processing applications. However, prior methods for computing semantic similarity often operate at different levels, e.g., single words or entire documents, which requires adapting the method for each data type. We present a unified approach to semantic similarity that operates at multiple levels, all the way from comparing word senses to comparing text documents. Our method leverages a common probabilistic representation over word senses in order to compare different types of linguistic data. This unified representation shows state-ofthe-art performance on three tasks: seman- tic textual similarity, word similarity, and word sense coarsening.</p><p>4 0.10332312 <a title="53-tfidf-4" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>5 0.099779323 <a title="53-tfidf-5" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>6 0.09382125 <a title="53-tfidf-6" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>7 0.093634062 <a title="53-tfidf-7" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>8 0.091858849 <a title="53-tfidf-8" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>9 0.080715455 <a title="53-tfidf-9" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>10 0.073923945 <a title="53-tfidf-10" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>11 0.06895873 <a title="53-tfidf-11" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>12 0.067186996 <a title="53-tfidf-12" href="./acl-2013-Implicatures_and_Nested_Beliefs_in_Approximate_Decentralized-POMDPs.html">190 acl-2013-Implicatures and Nested Beliefs in Approximate Decentralized-POMDPs</a></p>
<p>13 0.063910685 <a title="53-tfidf-13" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>14 0.056062762 <a title="53-tfidf-14" href="./acl-2013-A_user-centric_model_of_voting_intention_from_Social_Media.html">33 acl-2013-A user-centric model of voting intention from Social Media</a></p>
<p>15 0.055549301 <a title="53-tfidf-15" href="./acl-2013-Simple%2C_readable_sub-sentences.html">322 acl-2013-Simple, readable sub-sentences</a></p>
<p>16 0.054156099 <a title="53-tfidf-16" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>17 0.047575817 <a title="53-tfidf-17" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>18 0.046694718 <a title="53-tfidf-18" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>19 0.043609913 <a title="53-tfidf-19" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>20 0.043122783 <a title="53-tfidf-20" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, 0.043), (2, 0.008), (3, -0.094), (4, -0.02), (5, -0.109), (6, -0.083), (7, 0.051), (8, 0.066), (9, -0.016), (10, -0.021), (11, 0.061), (12, -0.091), (13, -0.039), (14, 0.023), (15, -0.043), (16, 0.001), (17, 0.067), (18, 0.008), (19, -0.073), (20, 0.004), (21, -0.077), (22, -0.009), (23, -0.029), (24, -0.019), (25, -0.137), (26, -0.023), (27, -0.034), (28, 0.021), (29, -0.001), (30, 0.072), (31, -0.064), (32, -0.008), (33, 0.081), (34, 0.02), (35, -0.038), (36, 0.101), (37, -0.022), (38, -0.004), (39, -0.004), (40, 0.014), (41, 0.004), (42, 0.033), (43, 0.037), (44, 0.125), (45, 0.027), (46, 0.051), (47, 0.105), (48, -0.021), (49, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95430005 <a title="53-lsi-1" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>2 0.74589276 <a title="53-lsi-2" href="./acl-2013-Neighbors_Help%3A_Bilingual_Unsupervised_WSD_Using_Context.html">258 acl-2013-Neighbors Help: Bilingual Unsupervised WSD Using Context</a></p>
<p>Author: Sudha Bhingardive ; Samiulla Shaikh ; Pushpak Bhattacharyya</p><p>Abstract: Word Sense Disambiguation (WSD) is one of the toughest problems in NLP, and in WSD, verb disambiguation has proved to be extremely difficult, because of high degree of polysemy, too fine grained senses, absence of deep verb hierarchy and low inter annotator agreement in verb sense annotation. Unsupervised WSD has received widespread attention, but has performed poorly, specially on verbs. Recently an unsupervised bilingual EM based algorithm has been proposed, which makes use only of the raw counts of the translations in comparable corpora (Marathi and Hindi). But the performance of this approach is poor on verbs with accuracy level at 25-38%. We suggest a modifica- tion to this mentioned formulation, using context and semantic relatedness of neighboring words. An improvement of 17% 35% in the accuracy of verb WSD is obtained compared to the existing EM based approach. On a general note, the work can be looked upon as contributing to the framework of unsupervised WSD through context aware expectation maximization.</p><p>3 0.68800712 <a title="53-lsi-3" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>Author: Tristan Miller ; Nicolai Erbs ; Hans-Peter Zorn ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</p><p>4 0.66588777 <a title="53-lsi-4" href="./acl-2013-Density_Maximization_in_Context-Sense_Metric_Space_for_All-words_WSD.html">111 acl-2013-Density Maximization in Context-Sense Metric Space for All-words WSD</a></p>
<p>Author: Koichi Tanigaki ; Mitsuteru Shiba ; Tatsuji Munaka ; Yoshinori Sagisaka</p><p>Abstract: This paper proposes a novel smoothing model with a combinatorial optimization scheme for all-words word sense disambiguation from untagged corpora. By generalizing discrete senses to a continuum, we introduce a smoothing in context-sense space to cope with data-sparsity resulting from a large variety of linguistic context and sense, as well as to exploit senseinterdependency among the words in the same text string. Through the smoothing, all the optimal senses are obtained at one time under maximum marginal likelihood criterion, by competitive probabilistic kernels made to reinforce one another among nearby words, and to suppress conflicting sense hypotheses within the same word. Experimental results confirmed the superiority of the proposed method over conventional ones by showing the better performances beyond most-frequent-sense baseline performance where none of SemEval2 unsupervised systems reached.</p><p>5 0.62752897 <a title="53-lsi-5" href="./acl-2013-Detecting_Metaphor_by_Contextual_Analogy.html">116 acl-2013-Detecting Metaphor by Contextual Analogy</a></p>
<p>Author: Eirini Florou</p><p>Abstract: As one of the most challenging issues in NLP, metaphor identification and its interpretation have seen many models and methods proposed. This paper presents a study on metaphor identification based on the semantic similarity between literal and non literal meanings of words that can appear at the same context.</p><p>6 0.61998999 <a title="53-lsi-6" href="./acl-2013-SenseSpotting%3A_Never_let_your_parallel_data_tie_you_to_an_old_domain.html">316 acl-2013-SenseSpotting: Never let your parallel data tie you to an old domain</a></p>
<p>7 0.58762169 <a title="53-lsi-7" href="./acl-2013-Understanding_Verbs_based_on_Overlapping_Verbs_Senses.html">366 acl-2013-Understanding Verbs based on Overlapping Verbs Senses</a></p>
<p>8 0.56008887 <a title="53-lsi-8" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<p>9 0.54682893 <a title="53-lsi-9" href="./acl-2013-FrameNet_on_the_Way_to_Babel%3A_Creating_a_Bilingual_FrameNet_Using_Wiktionary_as_Interlingual_Connection.html">162 acl-2013-FrameNet on the Way to Babel: Creating a Bilingual FrameNet Using Wiktionary as Interlingual Connection</a></p>
<p>10 0.53744453 <a title="53-lsi-10" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>11 0.53309274 <a title="53-lsi-11" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>12 0.53155267 <a title="53-lsi-12" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>13 0.50367028 <a title="53-lsi-13" href="./acl-2013-Real-World_Semi-Supervised_Learning_of_POS-Taggers_for_Low-Resource_Languages.html">295 acl-2013-Real-World Semi-Supervised Learning of POS-Taggers for Low-Resource Languages</a></p>
<p>14 0.47145021 <a title="53-lsi-14" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<p>15 0.43860978 <a title="53-lsi-15" href="./acl-2013-Computational_considerations_of_comparisons_and_similes.html">88 acl-2013-Computational considerations of comparisons and similes</a></p>
<p>16 0.42339915 <a title="53-lsi-16" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<p>17 0.41174695 <a title="53-lsi-17" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>18 0.39625546 <a title="53-lsi-18" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>19 0.3781352 <a title="53-lsi-19" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>20 0.37702501 <a title="53-lsi-20" href="./acl-2013-Multilingual_Affect_Polarity_and_Valence_Prediction_in_Metaphor-Rich_Texts.html">253 acl-2013-Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.05), (6, 0.026), (11, 0.066), (15, 0.013), (24, 0.039), (26, 0.027), (28, 0.017), (30, 0.332), (35, 0.091), (42, 0.051), (48, 0.032), (70, 0.025), (88, 0.037), (90, 0.024), (95, 0.059)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80019432 <a title="53-lda-1" href="./acl-2013-Annotation_of_regular_polysemy_and_underspecification.html">53 acl-2013-Annotation of regular polysemy and underspecification</a></p>
<p>Author: Hector Martinez Alonso ; Bolette Sandford Pedersen ; Nuria Bel</p><p>Abstract: We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in English, Danish and Spanish. This article describes the annotation process, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations.</p><p>2 0.68282211 <a title="53-lda-2" href="./acl-2013-The_Effects_of_Lexical_Resource_Quality_on_Preference_Violation_Detection.html">344 acl-2013-The Effects of Lexical Resource Quality on Preference Violation Detection</a></p>
<p>Author: Jesse Dunietz ; Lori Levin ; Jaime Carbonell</p><p>Abstract: Lexical resources such as WordNet and VerbNet are widely used in a multitude of NLP tasks, as are annotated corpora such as treebanks. Often, the resources are used as-is, without question or examination. This practice risks missing significant performance gains and even entire techniques. This paper addresses the importance of resource quality through the lens of a challenging NLP task: detecting selectional preference violations. We present DAVID, a simple, lexical resource-based preference violation detector. With asis lexical resources, DAVID achieves an F1-measure of just 28.27%. When the resource entries and parser outputs for a small sample are corrected, however, the F1-measure on that sample jumps from 40% to 61.54%, and performance on other examples rises, suggesting that the algorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements.</p><p>3 0.58987933 <a title="53-lda-3" href="./acl-2013-DKPro_WSD%3A_A_Generalized_UIMA-based_Framework_for_Word_Sense_Disambiguation.html">105 acl-2013-DKPro WSD: A Generalized UIMA-based Framework for Word Sense Disambiguation</a></p>
<p>Author: Tristan Miller ; Nicolai Erbs ; Hans-Peter Zorn ; Torsten Zesch ; Iryna Gurevych</p><p>Abstract: Implementations of word sense disambiguation (WSD) algorithms tend to be tied to a particular test corpus format and sense inventory. This makes it difficult to test their performance on new data sets, or to compare them against past algorithms implemented for different data sets. In this paper we present DKPro WSD, a freely licensed, general-purpose framework for WSD which is both modular and extensible. DKPro WSD abstracts the WSD process in such a way that test corpora, sense inventories, and algorithms can be freely swapped. Its UIMA-based architecture makes it easy to add support for new resources and algorithms. Related tasks such as word sense induction and entity linking are also supported.</p><p>4 0.55097061 <a title="53-lda-4" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>Author: Guangyou Zhou ; Fang Liu ; Yang Liu ; Shizhu He ; Jun Zhao</p><p>Abstract: Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issue. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization. Experiments conducted on a real CQA data show that our proposed approach is promising.</p><p>5 0.43421239 <a title="53-lda-5" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>Author: Olivier Ferret</p><p>Abstract: Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.</p><p>6 0.43326056 <a title="53-lda-6" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>7 0.43179959 <a title="53-lda-7" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>8 0.43156409 <a title="53-lda-8" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>9 0.43130961 <a title="53-lda-9" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>10 0.43116248 <a title="53-lda-10" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>11 0.43103623 <a title="53-lda-11" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>12 0.43017644 <a title="53-lda-12" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>13 0.4291243 <a title="53-lda-13" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>14 0.42811334 <a title="53-lda-14" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>15 0.42769301 <a title="53-lda-15" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>16 0.42692882 <a title="53-lda-16" href="./acl-2013-Is_a_204_cm_Man_Tall_or_Small_%3F_Acquisition_of_Numerical_Common_Sense_from_the_Web.html">202 acl-2013-Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web</a></p>
<p>17 0.42675257 <a title="53-lda-17" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>18 0.42664549 <a title="53-lda-18" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>19 0.42618722 <a title="53-lda-19" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>20 0.4261376 <a title="53-lda-20" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
