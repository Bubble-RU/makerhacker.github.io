<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-72" href="#">acl2013-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</h1>
<br/><p>Source: <a title="acl-2013-72-pdf" href="http://aclweb.org/anthology//P/P13/P13-1064.pdf">pdf</a></p><p>Author: Vivi Nastase ; Carlo Strapparava</p><p>Abstract: We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in crosslanguage (English-Italian) document categorization. In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically significant, but a large improvement a jump of almost 40 points in F1-score over the raw (vanilla bag-ofwords) representation.</p><p>Reference: <a title="acl-2013-72-reference" href="../acl2013_reference/acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Bridging Languages through Etymology: The case of cross language text categorization Vivi Nastase and Carlo Strapparava Human Language Technologies, Fondazione Bruno Kessler Trento, Italy {nastase, strappa} @fbk. [sent-1, score-0.272]
</p><p>2 eu  Abstract We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. [sent-2, score-0.238]
</p><p>3 We support this hypothesis with experiments in crosslanguage (English-Italian) document categorization. [sent-3, score-0.091]
</p><p>4 In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on English data, on Italian test data (and viceversa). [sent-4, score-1.108]
</p><p>5 The results show not only statistically significant, but a large improvement a jump of almost 40 points in F1-score over the raw (vanilla bag-ofwords) representation. [sent-5, score-0.071]
</p><p>6 He may be reminded however of the Latin word expensa which is also the etymological root of the Italian word spesa, which usually means “cost”/”shopping”, and may thus infer that the English word refers to the cost of things. [sent-10, score-0.843]
</p><p>7 In the experiments presented here we investigate whether an automatic text categorization system could benefit from knowledge about the etymological roots of words. [sent-11, score-1.017]
</p><p>8 The cross language text categorization (CLTC) task consists of categorizing documents in a target language Lt using a model built from labeled examples in a source language Ls. [sent-12, score-0.338]
</p><p>9 The task becomes more difficult when the data consists of comparable corpora in the two languages documents on the same topics (e. [sent-13, score-0.124]
</p><p>10 sports, economy) instead of parallel corpora there exists a one-to-one correspondence between documents in the corpora for the two lan–  –  –  guages, one document being the translation of the other. [sent-15, score-0.175]
</p><p>11 To test the usefulness of etymological information we work with comparable collections of news articles in English and Italian, whose articles are assigned one of four categories: culture and school, tourism, quality of life, made in Italy. [sent-16, score-0.849]
</p><p>12 We perform a progression of experiments, which embed etymological information deeper and deeper into the model. [sent-17, score-0.843]
</p><p>13 We start with the basic set-up, representing the documents as bag-of-words, where we train a model on the English training data, and use this model to categorize documents from the Italian test data (and viceversa). [sent-18, score-0.144]
</p><p>14 We then add the etymological roots of the words in the data to the bag-of-words, and notice a large 21 points increase in performance in terms of F1-score. [sent-20, score-0.892]
</p><p>15 We then use the bag-of-words representation of the training data to build a semantic space using LSA, and use the generated word vectors to represent the training and test data. [sent-21, score-0.15]
</p><p>16 Compared to related work, presented in Section 3, where cross language text categorization is approached through translation or mapping of features (i. [sent-23, score-0.294]
</p><p>17 words) from the source to the target language, word etymologies are a novel source of cross-lingual knowledge. [sent-25, score-0.155]
</p><p>18 The experiments presented show unequivocally that word etymology is a useful addition to computational models, just as they are to readers who have such knowledge. [sent-27, score-0.209]
</p><p>19 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 651–659,  2  Word Etymology  Word etymology gives us a glimpse into the evolution of words in a language. [sent-31, score-0.191]
</p><p>20 In time these words “adjust” to the language that adopted them their sense may change to various degrees but they are still semantically related to their etymological roots. [sent-33, score-0.82]
</p><p>21 The Italian obliterare has the “milder” sense of cancellare cancel (which is also shared by the English obliterate, but is less frequent according to Merriam-Webster), and both come from the Latin obliterare erase, efface, cause to disappear. [sent-39, score-0.218]
</p><p>22 While there has been some sense migration in English the more (physically) destructive sense of the word has higher prominence, while in Italian the word is closer in meaning to its etymological root the Italian and the English words are still semantically related. [sent-40, score-0.87]
</p><p>23 Dictionaries customarily include etymological information for their entries, and recently, Wikipedia’s Wiktionary has joined this trend. [sent-41, score-0.789]
</p><p>24 The etymological information can, and indeed has been extracted and prepared for machine consumption (de Melo and Weikum, 2010): Etymological WordNet1 contains 6,03 1,43 1 entries for 2,877,036 words (actually, morphemes) in 397 –  –  –  –  languages. [sent-42, score-0.803]
</p><p>25 The information in Etymological WordNet is organized around 5 relations: etymology with its inverse etymological origin of; is derived from with its inverse has derived form; and the symmetrical etymologically related. [sent-44, score-1.012]
</p><p>26 The etymology relation links a word with its etymological ancestors, and it is the relation used in the experiments presented here. [sent-45, score-0.998]
</p><p>27 The depth of the etymological hierarchy (considering the etymology relations) is 10. [sent-50, score-1.117]
</p><p>28 Figure 1 shows an example of a word with several levels of etymological ancestry. [sent-51, score-0.807]
</p><p>29 The most frequently, and successfully, used document representation is the bag-of-words (BoWs). [sent-57, score-0.1]
</p><p>30 Within the area of cross-language text categorization (CLTC) several methods have been explored for producing the model for a target language Lt using information and data from the source language Ls. [sent-60, score-0.208]
</p><p>31 (1997) find semantic correspondences in parallel (different language) corpora through latent semantic analysis (LSA). [sent-62, score-0.087]
</p><p>32 MT has been used: to cast the cross-language text categorization problem to the monolingual setting (Fortuna and Shawe-Taylor, 2005); to cast the cross-language text categorization problem into two monolingual settings for active learning (Liu et al. [sent-64, score-0.514]
</p><p>33 Prettenhofer and Stein (2010) use a word translation oracle to produce pivots pairs of semantically similar words and use the data partitions induced by these words to find cross language structural correspondences. [sent-70, score-0.121]
</p><p>34 (2008) use bilingual lexicons and aligned WordNet synsets to obtain shared features between the training data in language Ls and the testing data in language Lt. [sent-72, score-0.172]
</p><p>35 The bag-of-word document representation maps a document di from a corpus D into a k-  dimensional space Rk, where k is the dimension of the (possibly filtered) vocabulary of the corpus: W = {w1, . [sent-74, score-0.26]
</p><p>36 For the task of cross language text categorization, the problem of sharing a model across languages is that the dimensions, a. [sent-79, score-0.116]
</p><p>37 Limited overlap can be achieved through shared names and words. [sent-82, score-0.145]
</p><p>38 As we have seen in the literature review, machine translation and bilingual dictionaries can be used to cast these dimensions from the source language Ls to the target language Lt. [sent-83, score-0.131]
</p><p>39 In this work we explore expanding the shared dimensions through word etymologies. [sent-84, score-0.145]
</p><p>40 Figure 2 shows schematically the binary k dimensional representation for English and Italian data, and shared –  dimensions. [sent-85, score-0.166]
</p><p>41 Cross language text categorization could be used to obtain comparable corpora for building translation models. [sent-86, score-0.279]
</p><p>42 This information impacts the data representation, by introducing new shared features between the different language corpora without the need for translation or other forms of mapping. [sent-90, score-0.127]
</p><p>43 Word etymologies are a novel source of linguistic information in NLP, possibly because resources that capture this information in a machine readable format are also novel. [sent-92, score-0.137]
</p><p>44 The experiments presented in this paper use the bag-of-word document representation with absolute frequency values. [sent-95, score-0.136]
</p><p>45 To this basic representation we add word etymological ancestors and run classification experiments. [sent-96, score-1.162]
</p><p>46 , 1997) and (Gliozzo and Strapparava, 2005) to be useful for this task to induce the latent semantic dimensions of documents and words respectively, hypothesizing that word etymological ancestors will lead to semantic dimensions that transcend language boundaries. [sent-98, score-1.301]
</p><p>47 The vectors obtained through LSA (on the training data only) for words that are shared by the English training data and the Italian test data (names, and most importantly, etymological ancestors ofwords in the original documents) are then used for rerepresenting the training and test data. [sent-99, score-1.265]
</p><p>48 2 Raw cross-lingual text categorization As is commonly done in text categorization (Sebastiani, 2005), the documents in our data are represented as bag-of-words, and classification is done using support vector machines (SVMs). [sent-108, score-0.487]
</p><p>49 The high results, on a par with text categorization ex–  periments in the field, validates our experimental set-up. [sent-111, score-0.208]
</p><p>50 97  Table 2: Performance for monolingual raw text categorization experimental set-up as for the monolingual scenario (4 binary problems). [sent-120, score-0.283]
</p><p>51 The categorization baseline (BoW baseline in Figure 4) was obtained in this set-up. [sent-121, score-0.18]
</p><p>52 This baseline is higher than the ran-  dom baseline or the positive class baseline2 (all instances are assigned the target class in each of the 4 binary classification experiments) due to shared words and names between the two languages. [sent-122, score-0.127]
</p><p>53 3  Enriching the bag-of-word representation with word etymology As personal experience has shown us that etymological information is useful for comprehending a text in a different language, we set out to test whether this information can be useful in an automatic processing setting. [sent-124, score-1.091]
</p><p>54 We first verified whether the vocabularies of our two corpora, English and Italian, have shared word etymologies. [sent-125, score-0.098]
</p><p>55 Relying on word etymologies from the Etymological dictionary, we found that from our data’s vocabulary, 518 English terms and 543 Italian terms shared 490 direct etymological ancestors. [sent-126, score-1.024]
</p><p>56 Etymological ancestors also help cluster related terms within one language 887 etymological ancestors for 4727 English and 864 ancestors for 5167 Italian terms. [sent-127, score-1.65]
</p><p>57 This overlap further increases when adding derived forms (through the has derived form rela–  tion). [sent-128, score-0.07]
</p><p>58 The fact that this overlap exists strengthens the motivation to try using etymological ancestors for the task of text categorization. [sent-129, score-1.142]
</p><p>59 In this first step of integrating word etymology 2In this situation the random and positive class baseline are the same: 25% F1 score. [sent-130, score-0.209]
</p><p>60 into the experiment, we extract for each word in each document in the dataset its ancestors from the Etymological dictionary. [sent-131, score-0.357]
</p><p>61 For our dataset we noticed that the representation does not change af-  ter N=4, so this is the maximum depth we consider. [sent-133, score-0.185]
</p><p>62 The bag-of-words representation for each document is expanded with the corresponding etymological features. [sent-134, score-0.889]
</p><p>63 expansion  training data vocabulary size  vocabulary overlap with testing  Train EN /Test IT raw7112214207 depth 1 78936 depth 2 79068 depth 3 79100 depth 4 79103  (19. [sent-135, score-0.712]
</p><p>64 9%) depth depth depth depth  1 2 3 4  83656 83746 83769 83771  18682 18785 18812 18814  (22. [sent-141, score-0.548]
</p><p>65 5%)  Table 3: Feature expansion with word etymologies  Table 3 shows the training data vocabulary size and increase in the overlap between the training and test data with the addition of etymological fea655  tures. [sent-145, score-1.111]
</p><p>66 The increase is largest when introducing the immediate etymological ancestors, of approximately 4000 new (overlapping) features for both combinations of training and testing. [sent-146, score-0.881]
</p><p>67 Without etymological features the overlap was approximately 14000 for both configurations. [sent-147, score-0.844]
</p><p>68 The results obtained with this enriched BoW representation for etymological ancestor depth 1, 2 and 3 are presented in Figure 4. [sent-148, score-0.974]
</p><p>69 4  Cross-lingual text categorization in a latent semantic space adding etymology Shared word etymologies can serve as a bridge between two languages as we have seen in the previous configuration. [sent-150, score-0.652]
</p><p>70 When using shared word etymologies in the bag-of-words representation, we only take advantage of the shallow association between these new features and the classes within  × ×  which they appear. [sent-151, score-0.235]
</p><p>71 The process relies on the assumption that word co-occurrences across different documents are the surface manifestation of shared semantic dimensions. [sent-155, score-0.184]
</p><p>72 t V Vse wmoaunltdic dimensioni matrix, Uo Ta hisw othred transposed mofa a hdocument alatrtiexn,t semantic dimensioni matrix, oacnudm Σen its a diagonal mmaantrtiixc dwimhoesnes vioanluies m are iixnd,ic aandtive Σ of isth ae “strength” moafttrhixe  ×  semantic dimensions. [sent-159, score-0.096]
</p><p>73 By reducing the size of Σ, for example by selecting the dimensions with the top K values, we can obtain an approximation of the original matrix D ≈ DK = VKΣKUKT, where we orerisgtriincatl t mhea rlaixten Dt ≈sem Dantic dimensions taken into account to the K chosen ones. [sent-160, score-0.115]
</p><p>74 We perform this decomposition and dimension reduction step on the hword documenti matrreidxu bctuiioltn f srtoemp othne training ddat ×a only, manendt using K=400. [sent-162, score-0.123]
</p><p>75 Both the training and test data are then  dsrwoVxFDigureSdaVinmdDunce3tiso:n rdswSchVxemKatxicnmestnal emidnovewKx ofLtalnmestn oisemdSAKxD documents  latent semantic dimension  ci  latent semantic dimension  ci  documents  re-represented through the new word vectors from matrix VK. [sent-163, score-0.315]
</p><p>76 Because the LSA space was built only from the training data, only the shared words and  shared etymological ancestors are used to produce representations of the test data. [sent-164, score-1.293]
</p><p>77 The results of this experiment are shown in Figure 4, together with an LSA baseline using the raw data and relying on shared words and names as overlap. [sent-166, score-0.148]
</p><p>78 –  4  Discussion  The experiments whose results we present here were produced using unfiltered data all words in the datasets, all etymological ancestors up to the desired depth, no filtering based on frequency of occurrence. [sent-167, score-1.119]
</p><p>79 Feature filtering is commonly done in machine learning when the data has many features, and in text categorization when using the bag-ofwords representation in particular. [sent-168, score-0.278]
</p><p>80 The point that etymology information is a useful addition to the task of cross-language text categorization can be made without finding the optimal filtering set–  up. [sent-170, score-0.421]
</p><p>81 Adding a first batch of etymological information approximately 4000 shared immediate ancestors leads to an increase of 18 points in terms of F1-score on the BoW experimental set-up for English training/Italian testing, and 21 points for Italian training/English testing. [sent-173, score-1.319]
</p><p>82 Further additions of etymological ancestors at depths 2 and 3 results in an increase of 21 points in terms of F1-score for English training/Italian testing, and 27 points for Italian training/English testing. [sent-174, score-1.232]
</p><p>83 The next processing step induced a representation of the shared words that encodes deeper level dependencies between words and documents based on word co-occurrences in documents. [sent-176, score-0.224]
</p><p>84 The LSA space built on the training data leads to a vector representation of the shared words, including the shared etymological ancestors, that captures more than the obvious word-document cooccurrences. [sent-177, score-1.037]
</p><p>85 Using this representation leads to a further increase of 15 points in F1-score for English training/Italian testing set-up over the BoW representation, and 14 points over the baseline LSA-based categorization. [sent-178, score-0.226]
</p><p>86 The increase for the Italian training/English testing is 5 points over the BoW representation, but 20 points over the baseline LSA. [sent-179, score-0.178]
</p><p>87 The clue to why the increase when using LSA is lower than for English training/Italian testing is in the way LSA operates it relies heavily on word co-occurrences in finding the latent semantic dimensions of documents and –  words. [sent-181, score-0.261]
</p><p>88 We expect then that in the Italian training collection, words are “less shared” among documents, which means a lower average document frequency. [sent-182, score-0.077]
</p><p>89 Figure 5 shows the changes in average document frequency for the two training collections, starting with the raw data (depth 0), and with additional etymological features. [sent-183, score-0.91]
</p><p>90 The difference in results on the two dictionary versions was significant: a 4 and 5 points increase respectively in micro-averaged F1-score in the bag-ofwords setting for English training/Italian testing and Italian training/English testing, and a 2 and 6 points increase in the LSA setting. [sent-187, score-0.213]
</p><p>91 This indicates that more etymological information is better, and the dynamic nature of Wikipedia and the Wiktionary could lead to an ever increasing and better etymological resource for NLP applications. [sent-188, score-1.578]
</p><p>92 5  Conclusion  The motivation for this work was to test the hy-  pothesis that information about word etymology is useful for computational approaches to language, in particular for text classification. [sent-189, score-0.254]
</p><p>93 Cross-language text classification can be used to build comparable corpora in different languages, using a single language starting point, preferably one with more resources, that can thus spill over to other languages. [sent-190, score-0.097]
</p><p>94 The experiments presented have shown clearly that etymological ancestors can be used to provide the necessary bridge between the languages we considered English and Italian. [sent-191, score-1.129]
</p><p>95 We plan to expand our experiments to more languages with shared etymologies, and investigate what characteristics of languages and data indicate that etymological information is –  beneficial for the task at hand. [sent-194, score-0.917]
</p><p>96 Monolingual and cross-lingual textual entailment in particular would be interesting applications, because they require finding shared meaning on two text fragments. [sent-196, score-0.108]
</p><p>97 Word etymologies would allow recognizing words with shared ancestors, and thus with shared meaning, both within and across languages. [sent-197, score-0.297]
</p><p>98 Cross language text categorization by acquiring multilingual domain models from comparable corpora. [sent-235, score-0.249]
</p><p>99 Exploiting comparable corpora and bilingual dictionaries for cross-language text categorization. [sent-239, score-0.116]
</p><p>100 An EM based training algorithm for crosslanguage text categorization. [sent-259, score-0.092]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('etymological', 0.789), ('ancestors', 0.287), ('italian', 0.203), ('etymology', 0.191), ('categorization', 0.18), ('depth', 0.137), ('etymologies', 0.137), ('lsa', 0.103), ('cltc', 0.093), ('shared', 0.08), ('bow', 0.068), ('cross', 0.064), ('gliozzo', 0.063), ('obliterare', 0.062), ('latin', 0.062), ('document', 0.052), ('documents', 0.051), ('points', 0.048), ('wj', 0.048), ('representation', 0.048), ('dimensions', 0.047), ('testing', 0.047), ('hword', 0.046), ('di', 0.046), ('strapparava', 0.044), ('crosslanguage', 0.039), ('overlap', 0.038), ('increase', 0.035), ('english', 0.033), ('borko', 0.031), ('dimensioni', 0.031), ('documenti', 0.031), ('rigutini', 0.031), ('viceversa', 0.031), ('dumais', 0.029), ('bridge', 0.029), ('text', 0.028), ('carlo', 0.028), ('latent', 0.028), ('vocabulary', 0.027), ('fortuna', 0.027), ('obliterate', 0.027), ('names', 0.027), ('life', 0.027), ('sebastiani', 0.027), ('deeper', 0.027), ('monolingual', 0.026), ('additions', 0.025), ('fij', 0.025), ('physically', 0.025), ('training', 0.025), ('corpora', 0.025), ('lt', 0.024), ('comparable', 0.024), ('schematically', 0.024), ('languages', 0.024), ('raw', 0.023), ('cast', 0.023), ('weighing', 0.023), ('alfio', 0.023), ('ls', 0.022), ('translation', 0.022), ('filtering', 0.022), ('melo', 0.022), ('dimension', 0.021), ('frequency', 0.021), ('matrix', 0.021), ('nastase', 0.021), ('prettenhofer', 0.021), ('bilingual', 0.02), ('classification', 0.02), ('fang', 0.02), ('tourism', 0.02), ('roots', 0.02), ('bnc', 0.02), ('culture', 0.019), ('wiktionary', 0.019), ('dictionaries', 0.019), ('word', 0.018), ('deerwester', 0.018), ('relies', 0.018), ('relying', 0.018), ('semantic', 0.017), ('multilingual', 0.017), ('approximately', 0.017), ('cultural', 0.017), ('guo', 0.017), ('semantically', 0.017), ('test', 0.017), ('wan', 0.016), ('derived', 0.016), ('built', 0.015), ('immediate', 0.015), ('shi', 0.015), ('absolute', 0.015), ('svms', 0.015), ('entries', 0.014), ('trento', 0.014), ('dimensional', 0.014), ('sense', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="72-tfidf-1" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>Author: Vivi Nastase ; Carlo Strapparava</p><p>Abstract: We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in crosslanguage (English-Italian) document categorization. In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically significant, but a large improvement a jump of almost 40 points in F1-score over the raw (vanilla bag-ofwords) representation.</p><p>2 0.10739921 <a title="72-tfidf-2" href="./acl-2013-Translating_Italian_connectives_into_Italian_Sign_Language.html">360 acl-2013-Translating Italian connectives into Italian Sign Language</a></p>
<p>Author: Camillo Lugaresi ; Barbara Di Eugenio</p><p>Abstract: We present a corpus analysis of how Italian connectives are translated into LIS, the Italian Sign Language. Since corpus resources are scarce, we propose an alignment method between the syntactic trees of the Italian sentence and of its LIS translation. This method, and clustering applied to its outputs, highlight the different ways a connective can be rendered in LIS: with a corresponding sign, by affecting the location or shape of other signs, or being omitted altogether. We translate these findings into a computational model that will be integrated into the pipeline of an existing Italian-LIS rendering system. Initial experiments to learn the four possible translations with Decision Trees give promising results.</p><p>3 0.074545629 <a title="72-tfidf-3" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>4 0.07330434 <a title="72-tfidf-4" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>Author: Efsun Sarioglu ; Kabir Yadav ; Hyeong-Ah Choi</p><p>Abstract: Kabir Yadav Emergency Medicine Department The George Washington University Washington, DC, USA kyadav@ gwu . edu Hyeong-Ah Choi Computer Science Department The George Washington University Washington, DC, USA hcho i gwu . edu @ such as recommending the need for a certain medical test while avoiding intrusive tests or medical Electronic health records (EHRs) contain important clinical information about pa- tients. Some of these data are in the form of free text and require preprocessing to be able to used in automated systems. Efficient and effective use of this data could be vital to the speed and quality of health care. As a case study, we analyzed classification of CT imaging reports into binary categories. In addition to regular text classification, we utilized topic modeling of the entire dataset in various ways. Topic modeling of the corpora provides interpretable themes that exist in these reports. Representing reports according to their topic distributions is more compact than bag-of-words representation and can be processed faster than raw text in subsequent automated processes. A binary topic model was also built as an unsupervised classification approach with the assumption that each topic corresponds to a class. And, finally an aggregate topic classifier was built where reports are classified based on a single discriminative topic that is determined from the training dataset. Our proposed topic based classifier system is shown to be competitive with existing text classification techniques and provides a more efficient and interpretable representation.</p><p>5 0.06172622 <a title="72-tfidf-5" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>Author: Abdellah Fourtassi ; Emmanuel Dupoux</p><p>Abstract: Evaluation methods for Distributional Semantic Models typically rely on behaviorally derived gold standards. These methods are difficult to deploy in languages with scarce linguistic/behavioral resources. We introduce a corpus-based measure that evaluates the stability of the lexical semantic similarity space using a pseudo-synonym same-different detection task and no external resources. We show that it enables to predict two behaviorbased measures across a range of parameters in a Latent Semantic Analysis model.</p><p>6 0.060155328 <a title="72-tfidf-6" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>7 0.055924911 <a title="72-tfidf-7" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>8 0.04878132 <a title="72-tfidf-8" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>9 0.044210453 <a title="72-tfidf-9" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>10 0.043288663 <a title="72-tfidf-10" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>11 0.041609097 <a title="72-tfidf-11" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>12 0.040902819 <a title="72-tfidf-12" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>13 0.040624291 <a title="72-tfidf-13" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>14 0.040040817 <a title="72-tfidf-14" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>15 0.038939238 <a title="72-tfidf-15" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>16 0.038573686 <a title="72-tfidf-16" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>17 0.036022048 <a title="72-tfidf-17" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>18 0.03541971 <a title="72-tfidf-18" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>19 0.034739628 <a title="72-tfidf-19" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>20 0.034292541 <a title="72-tfidf-20" href="./acl-2013-Linking_and_Extending_an_Open_Multilingual_Wordnet.html">234 acl-2013-Linking and Extending an Open Multilingual Wordnet</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.107), (1, 0.022), (2, 0.033), (3, -0.028), (4, 0.005), (5, -0.039), (6, -0.007), (7, 0.006), (8, -0.009), (9, -0.021), (10, 0.041), (11, -0.028), (12, 0.006), (13, 0.045), (14, -0.005), (15, 0.003), (16, -0.042), (17, 0.009), (18, -0.026), (19, 0.001), (20, -0.008), (21, -0.008), (22, -0.016), (23, -0.011), (24, -0.013), (25, 0.025), (26, -0.018), (27, 0.001), (28, -0.025), (29, -0.017), (30, -0.051), (31, 0.047), (32, 0.021), (33, 0.05), (34, 0.017), (35, 0.021), (36, -0.026), (37, -0.007), (38, 0.045), (39, -0.021), (40, -0.028), (41, 0.051), (42, -0.001), (43, 0.002), (44, -0.058), (45, 0.01), (46, 0.063), (47, 0.021), (48, -0.028), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86687326 <a title="72-lsi-1" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>Author: Vivi Nastase ; Carlo Strapparava</p><p>Abstract: We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in crosslanguage (English-Italian) document categorization. In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically significant, but a large improvement a jump of almost 40 points in F1-score over the raw (vanilla bag-ofwords) representation.</p><p>2 0.74103427 <a title="72-lsi-2" href="./acl-2013-Translating_Italian_connectives_into_Italian_Sign_Language.html">360 acl-2013-Translating Italian connectives into Italian Sign Language</a></p>
<p>Author: Camillo Lugaresi ; Barbara Di Eugenio</p><p>Abstract: We present a corpus analysis of how Italian connectives are translated into LIS, the Italian Sign Language. Since corpus resources are scarce, we propose an alignment method between the syntactic trees of the Italian sentence and of its LIS translation. This method, and clustering applied to its outputs, highlight the different ways a connective can be rendered in LIS: with a corresponding sign, by affecting the location or shape of other signs, or being omitted altogether. We translate these findings into a computational model that will be integrated into the pipeline of an existing Italian-LIS rendering system. Initial experiments to learn the four possible translations with Decision Trees give promising results.</p><p>3 0.71074289 <a title="72-lsi-3" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>Author: Tsutomu Hirao ; Tomoharu Iwata ; Masaaki Nagata</p><p>Abstract: Unsupervised object matching (UOM) is a promising approach to cross-language natural language processing such as bilingual lexicon acquisition, parallel corpus construction, and cross-language text categorization, because it does not require labor-intensive linguistic resources. However, UOM only finds one-to-one correspondences from data sets with the same number of instances in source and target domains, and this prevents us from applying UOM to real-world cross-language natural language processing tasks. To alleviate these limitations, we proposes latent semantic matching, which embeds objects in both source and target language domains into a shared latent topic space. We demonstrate the effectiveness of our method on cross-language text categorization. The results show that our method outperforms conventional unsupervised object matching methods.</p><p>4 0.6311655 <a title="72-lsi-4" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>Author: Yukari Ogura ; Ichiro Kobayashi</p><p>Abstract: In this paper, we propose a method to raise the accuracy of text classification based on latent topics, reconsidering the techniques necessary for good classification for example, to decide important sentences in a document, the sentences with important words are usually regarded as important sentences. In this case, tf.idf is often used to decide important words. On the other hand, we apply the PageRank algorithm to rank important words in each document. Furthermore, before clustering documents, we refine the target documents by representing them as a collection of important sentences in each document. We then classify the documents based on latent information in the documents. As a clustering method, we employ the k-means algorithm and inves– tigate how our proposed method works for good clustering. We conduct experiments with Reuters-21578 corpus under various conditions of important sentence extraction, using latent and surface information for clustering, and have confirmed that our proposed method provides better result among various conditions for clustering.</p><p>5 0.62983239 <a title="72-lsi-5" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>6 0.62415946 <a title="72-lsi-6" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>7 0.59556347 <a title="72-lsi-7" href="./acl-2013-High-quality_Training_Data_Selection_using_Latent_Topics_for_Graph-based_Semi-supervised_Learning.html">182 acl-2013-High-quality Training Data Selection using Latent Topics for Graph-based Semi-supervised Learning</a></p>
<p>8 0.59032089 <a title="72-lsi-8" href="./acl-2013-Context_Vector_Disambiguation_for_Bilingual_Lexicon_Extraction_from_Comparable_Corpora.html">93 acl-2013-Context Vector Disambiguation for Bilingual Lexicon Extraction from Comparable Corpora</a></p>
<p>9 0.56646085 <a title="72-lsi-9" href="./acl-2013-Sign_Language_Lexical_Recognition_With_Propositional_Dynamic_Logic.html">321 acl-2013-Sign Language Lexical Recognition With Propositional Dynamic Logic</a></p>
<p>10 0.56194448 <a title="72-lsi-10" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>11 0.56056768 <a title="72-lsi-11" href="./acl-2013-A_Novel_Classifier_Based_on_Quantum_Computation.html">14 acl-2013-A Novel Classifier Based on Quantum Computation</a></p>
<p>12 0.54940432 <a title="72-lsi-12" href="./acl-2013-Transfer_Learning_Based_Cross-lingual_Knowledge_Extraction_for_Wikipedia.html">356 acl-2013-Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia</a></p>
<p>13 0.53748751 <a title="72-lsi-13" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>14 0.5264166 <a title="72-lsi-14" href="./acl-2013-Dirt_Cheap_Web-Scale_Parallel_Text_from_the_Common_Crawl.html">120 acl-2013-Dirt Cheap Web-Scale Parallel Text from the Common Crawl</a></p>
<p>15 0.51821107 <a title="72-lsi-15" href="./acl-2013-Categorization_of_Turkish_News_Documents_with_Morphological_Analysis.html">78 acl-2013-Categorization of Turkish News Documents with Morphological Analysis</a></p>
<p>16 0.51636791 <a title="72-lsi-16" href="./acl-2013-Context-Dependent_Multilingual_Lexical_Lookup_for_Under-Resourced_Languages.html">92 acl-2013-Context-Dependent Multilingual Lexical Lookup for Under-Resourced Languages</a></p>
<p>17 0.50552922 <a title="72-lsi-17" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>18 0.50543696 <a title="72-lsi-18" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>19 0.50303262 <a title="72-lsi-19" href="./acl-2013-Topic_Modeling_Based_Classification_of_Clinical_Reports.html">351 acl-2013-Topic Modeling Based Classification of Clinical Reports</a></p>
<p>20 0.50032932 <a title="72-lsi-20" href="./acl-2013-An_Information_Theoretic_Approach_to_Bilingual_Word_Clustering.html">47 acl-2013-An Information Theoretic Approach to Bilingual Word Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.035), (6, 0.032), (11, 0.043), (15, 0.021), (24, 0.427), (26, 0.039), (35, 0.058), (42, 0.024), (48, 0.04), (70, 0.032), (88, 0.031), (90, 0.024), (95, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98737121 <a title="72-lda-1" href="./acl-2013-A_Visual_Analytics_System_for_Cluster_Exploration.html">29 acl-2013-A Visual Analytics System for Cluster Exploration</a></p>
<p>Author: Andreas Lamprecht ; Annette Hautli ; Christian Rohrdantz ; Tina Bogel</p><p>Abstract: This paper offers a new way of representing the results of automatic clustering algorithms by employing a Visual Analytics system which maps members of a cluster and their distance to each other onto a twodimensional space. A case study on Urdu complex predicates shows that the system allows for an appropriate investigation of linguistically motivated data. 1 Motivation In recent years, Visual Analytics systems have increasingly been used for the investigation of linguistic phenomena in a number of different areas, starting from literary analysis (Keim and Oelke, 2007) to the cross-linguistic comparison of language features (Mayer et al., 2010a; Mayer et al., 2010b; Rohrdantz et al., 2012a) and lexical semantic change (Rohrdantz et al., 2011; Heylen et al., 2012; Rohrdantz et al., 2012b). Visualization has also found its way into the field of computational linguistics by providing insights into methods such as machine translation (Collins et al., 2007; Albrecht et al., 2009) or discourse parsing (Zhao et al., 2012). One issue in computational linguistics is the interpretability of results coming from machine learning algorithms and the lack of insight they offer on the underlying data. This drawback often prevents theoretical linguists, who work with computational models and need to see patterns on large data sets, from drawing detailed conclusions. The present paper shows that a Visual Analytics system facilitates “analytical reasoning [...] by an interactive visual interface” (Thomas and Cook, 2006) and helps resolving this issue by offering a customizable, in-depth view on the statistically generated result and simultaneously an at-a-glance overview of the overall data set. In particular, we focus on the visual representa- tion of automatically generated clusters, in itself not a novel idea as it has been applied in other fields like the financial sector, biology or geography (Schreck et al., 2009). But as far as the literature is concerned, interactive systems are still less common, particularly in computational linguistics, and they have not been designed for the specific needs of theoretical linguists. This paper offers a method of visually encoding clusters and their internal coherence with an interactive user interface, which allows users to adjust underlying parameters and their views on the data depending on the particular research question. By this, we partly open up the “black box” of machine learning. The linguistic phenomenon under investigation, for which the system has originally been designed, is the varied behavior of nouns in N+V CP complex predicates in Urdu (e.g., memory+do = ‘to remember’) (Mohanan, 1994; Ahmed and Butt, 2011), where, depending on the lexical semantics of the noun, a set of different light verbs is chosen to form a complex predicate. The aim is an automatic detection of the different groups of nouns, based on their light verb distribution. Butt et al. (2012) present a static visualization for the phenomenon, whereas the present paper proposes an interactive system which alleviates some of the previous issues with respect to noise detection, filtering, data interaction and cluster coherence. For this, we proceed as follows: section 2 explains the proposed Visual Analytics system, followed by the linguistic case study in section 3. Section 4 concludes the paper. 2 The system The system requires a plain text file as input, where each line corresponds to one data object.In our case, each line corresponds to one Urdu noun (data object) and contains its unique ID (the name of the noun) and its bigram frequencies with the 109 Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t.he ?c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 109–1 4, four light verbs under investigation, namely kar ‘do’, ho ‘be’, hu ‘become’ and rakH ‘put’ ; an exemplary input file is shown in Figure 1. From a data analysis perspective, we have four- dimensional data objects, where each dimension corresponds to a bigram frequency previously extracted from a corpus. Note that more than four dimensions can be loaded and analyzed, but for the sake of simplicity we focus on the fourdimensional Urdu example for the remainder of this paper. Moreover, it is possible to load files containing absolute bigram frequencies and relative frequencies. When loading absolute frequencies, the program will automatically calculate the relative frequencies as they are the input for the clustering. The absolute frequencies, however, are still available and can be used for further processing (e.g. filtering). Figure 1: preview of appropriate file structures 2.1 Initial opening and processing of a file It is necessary to define a metric distance function between data objects for both clustering and visualization. Thus, each data object is represented through a high dimensional (in our example fourdimensional) numerical vector and we use the Euclidean distance to calculate the distances between pairs of data objects. The smaller the distance between two data objects, the more similar they are. For visualization, the high dimensional data is projected onto the two-dimensional space of a computer screen using a principal component analysis (PCA) algorithm1 . In the 2D projection, the distances between data objects in the highdimensional space, i.e. the dissimilarities of the bigram distributions, are preserved as accurately as possible. However, when projecting a highdimensional data space onto a lower dimension, some distinctions necessarily level out: two data objects may be far apart in the high-dimensional space, but end up closely together in the 2D projection. It is important to bear in mind that the 2D visualization is often quite insightful, but interpre1http://workshop.mkobos.com/201 1/java-pca- transformation-library/ tations have to be verified by interactively investigating the data. The initial clusters are calculated (in the highdimensional data space) using a default k-Means algorithm2 with k being a user-defined parameter. There is also the option of selecting another clustering algorithm, called the Greedy Variance Minimization3 (GVM), and an extension to include further algorithms is under development. 2.2 Configuration & Interaction 2.2.1 The main window The main window in Figure 2 consists of three areas, namely the configuration area (a), the visualization area (b) and the description area (c). The visualization area is mainly built with the piccolo2d library4 and initially shows data objects as colored circles with a variable diameter, where color indicates cluster membership (four clusters in this example). Hovering over a dot displays information on the particular noun, the cluster membership and the light verb distribution in the de- scription area to the right. By using the mouse wheel, the user can zoom in and out of the visualization. A very important feature for the task at hand is the possibility to select multiple data objects for further processing or for filtering, with a list of selected data objects shown in the description area. By right-clicking on these data objects, the user can assign a unique class (and class color) to them. Different clustering methods can be employed using the options item in the menu bar. Another feature of the system is that the user can fade in the cluster centroids (illustrated by a larger dot in the respective cluster color in Figure 2), where the overall feature distribution of the cluster can be examined in a tooltip hovering over the corresponding centroid. 2.2.2 Visually representing data objects To gain further insight into the data distribution based on the 2D projection, the user can choose between several ways to visualize the individual data objects, all of which are shown in Figure 3. The standard visualization type is shown on the left and consists of a circle which encodes cluster membership via color. 2http://java-ml.sourceforge.net/api/0.1.7/ (From the JML library) 3http://www.tomgibara.com/clustering/fast-spatial/ 4http://www.piccolo2d.org/ 110 Figure 2: Overview of the main window of the system, including the configuration area (a), the visualization area (b) and the description area (c). Large circles are cluster centroids. Figure 3: Different visualizations of data points Alternatively, normal glyphs and star glyphs can be displayed. The middle part of Figure 3 shows the data displayed with normal glyphs. In linestarinorthpsiflvtrheinorqsbgnutheviasnemdocwfya,proepfthlpdienaoecsr.nihetloa Titnghve det clockwise around the center according to their occurrence in the input file. This view has the advantage that overall feature dominance in a cluster can be seen at-a-glance. The visualization type on the right in Figure 3 agislnycpaehlxset. dnstHhioe nrset ,oarthngeolyrmlpinhae,l endings are connected, forming a “star”. As in the representation with the glyphs, this makes similar data objects easily recognizable and comparable with each other. 2.2.3 Filtering options Our systems offers options for filtering data ac- cording to different criteria. Filter by means of bigram occurrence By activating the bigram occurrence filtering, it is possible to only show those nouns, which occur in bigrams with a certain selected subset of all features (light verbs) only. This is especially useful when examining possible commonalities. Filter selected words Another opportunity of showing only items of interest is to select and display them separately. The PCA is recalculated for these data objects and the visualization is stretched to the whole area. 111 Filter selected cluster Additionally, the user can visualize a specific cluster of interest. Again, the PCA is recalculated and the visualization stretched to the whole area. The cluster can then be manually fine-tuned and cleaned, for instance by removing wrongly assigned items. 2.2.4 Options to handle overplotting Due to the nature of the data, much overplotting occurs. For example, there are many words, which only occur with one light verb. The PCA assigns the same position to these words and, as a consequence, only the top bigram can be viewed in the visualization. In order to improve visual access to overplotted data objects, several methods that allow for a more differentiated view of the data have been included and are described in the following paragraphs. Change transparency of data objects By modifying the transparency with the given slider, areas with a dense data population can be readily identified, as shown in the following example: Repositioning of data objects To reduce the overplotting in densely populated areas, data objects can be repositioned randomly having a fixed deviation from their initial position. The degree of deviation can be interactively determined by the user employing the corresponding slider: The user has the option to reposition either all data objects or only those that are selected in advance. Frequency filtering If the initial data contains absolute bigram frequencies, the user can filter the visualized words by frequency. For example, many nouns occur only once and therefore have an observed probability of 100% for co-occurring with one of the light verbs. In most cases it is useful to filter such data out. Scaling data objects If the user zooms beyond the maximum zoom factor, the data objects are scaled down. This is especially useful, if data objects are only partly covered by many other objects. In this case, they become fully visible, as shown in the following example: 2.3 Alternative views on the data In order to enable a holistic analysis it is often valuable to provide the user with different views on the data. Consequently, we have integrated the option to explore the data with further standard visualization methods. 2.3.1 Correlation matrix The correlation matrix in Figure 4 shows the correlations between features, which are visualized by circles using the following encoding: The size of a circle represents the correlation strength and the color indicates whether the corresponding features are negatively (white) or positively (black) correlated. Figure 4: example of a correlation matrix 2.3.2 Parallel coordinates The parallel coordinates diagram shows the distribution of the bigram frequencies over the different dimensions (Figure 5). Every noun is represented with a line, and shows, when hovered over, a tooltip with the most important information. To filter the visualized words, the user has the option of displaying previously selected data objects, or s/he can restrict the value range for a feature and show only the items which lie within this range. 2.3.3 Scatter plot matrix To further examine the relation between pairs of features, a scatter plot matrix can be used (Figure 6). The individual scatter plots give further insight into the correlation details of pairs of features. 112 Figure 5: Parallel coordinates diagram Figure 6: Example showing a scatter plot matrix. 3 Case study In principle, the Visual Analytics system presented above can be used for any kind of cluster visualization, but the built-in options and add-ons are particularly designed for the type of work that linguists tend to be interested in: on the one hand, the user wants to get a quick overview of the overall patterns in the phenomenon, but on the same time, the system needs to allow for an in-depth data inspection. Both is given in the system: The overall cluster result shown in Figure 2 depicts the coherence of clusters and therefore the overall pattern of the data set. The different glyph visualizations in Figure 3 illustrate the properties of each cluster. Single data points can be inspected in the description area. The randomization of overplotted data points helps to see concentrated cluster patterns where light verbs behave very similarly in different noun+verb complex predicates. The biggest advantage of the system lies in the ability for interaction: Figure 7 shows an example of the visualization used in Butt et al. (2012), the input being the same text file as shown in Figure 1. In this system, the relative frequencies of each noun with each light verb is correlated with color saturation the more saturated the color to the right of the noun, the higher the relative frequency of the light verb occurring with it. The number of the cluster (here, 3) and the respective nouns (e.g. kAm ‘work’) is shown to the left. The user does — not get information on the coherence of the cluster, nor does the visualization show prototypical cluster patterns. Figure 7: Cluster visualization in Butt et al. (2012) Moreover, the system in Figure 7 only has a limited set of interaction choices, with the consequence that the user is not able to adjust the underlying data set, e.g. by filtering out noise. However, Butt et al. (2012) report that the Urdu data is indeed very noisy and requires a manual cleaning of the data set before the actual clustering. In the system presented here, the user simply marks conspicuous regions in the visualization panel and removes the respective data points from the original data set. Other filtering mechanisms, e.g. the removal of low frequency items which occur due to data sparsity issues, can be removed from the overall data set by adjusting the parameters. A linguistically-relevant improvement lies in the display of cluster centroids, in other words the typical noun + light verb distribution of a cluster. This is particularly helpful when the linguist wants to pick out prototypical examples for the cluster in order to stipulate generalizations over the other cluster members. 113 4 Conclusion In this paper, we present a novel visual analytics system that helps to automatically analyze bigrams extracted from corpora. The main purpose is to enable a more informed and steered cluster analysis than currently possible with standard methods. This includes rich options for interaction, e.g. display configuration or data manipulation. Initially, the approach was motivated by a concrete research problem, but has much wider applicability as any kind of high-dimensional numerical data objects can be loaded and analyzed. However, the system still requires some basic understanding about the algorithms applied for clustering and projection in order to prevent the user to draw wrong conclusions based on artifacts. Bearing this potential pitfall in mind when performing the analysis, the system enables a much more insightful and informed analysis than standard noninteractive methods. In the future, we aim to conduct user experiments in order to learn more about how the functionality and usability could be further enhanced. Acknowledgments This work was partially funded by the German Research Foundation (DFG) under grant BU 1806/7-1 “Visual Analysis of Language Change and Use Patterns” and the German Fed- eral Ministry of Education and Research (BMBF) under grant 01461246 “VisArgue” under research grant. References Tafseer Ahmed and Miriam Butt. 2011. Discovering Semantic Classes for Urdu N-V Complex Predicates. In Proceedings of the international Conference on Computational Semantics (IWCS 2011), pages 305–309. Joshua Albrecht, Rebecca Hwa, and G. Elisabeta Marai. 2009. The Chinese Room: Visualization and Interaction to Understand and Correct Ambiguous Machine Translation. Comput. Graph. Forum, 28(3): 1047–1054. Miriam Butt, Tina B ¨ogel, Annette Hautli, Sebastian Sulger, and Tafseer Ahmed. 2012. Identifying Urdu Complex Predication via Bigram Extraction. In In Proceedings of COLING 2012, Technical Papers, pages 409 424, Mumbai, India. Christopher Collins, M. Sheelagh T. Carpendale, and Gerald Penn. 2007. Visualization of Uncertainty in Lattices to Support Decision-Making. In EuroVis 2007, pages 5 1–58. Eurographics Association. Kris Heylen, Dirk Speelman, and Dirk Geeraerts. 2012. Looking at word meaning. An interactive visualization of Semantic Vector Spaces for Dutch – synsets. In Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 16–24. Daniel A. Keim and Daniela Oelke. 2007. Literature Fingerprinting: A New Method for Visual Literary Analysis. In IEEE VAST 2007, pages 115–122. IEEE. Thomas Mayer, Christian Rohrdantz, Miriam Butt, Frans Plank, and Daniel A. Keim. 2010a. Visualizing Vowel Harmony. Linguistic Issues in Language Technology, 4(Issue 2): 1–33, December. Thomas Mayer, Christian Rohrdantz, Frans Plank, Peter Bak, Miriam Butt, and Daniel A. Keim. 2010b. Consonant Co-Occurrence in Stems across Languages: Automatic Analysis and Visualization of a Phonotactic Constraint. In Proceedings of the 2010 Workshop on NLP andLinguistics: Finding the Common Ground, pages 70–78, Uppsala, Sweden, July. Association for Computational Linguistics. Tara Mohanan. 1994. Argument Structure in Hindi. Stanford: CSLI Publications. Christian Rohrdantz, Annette Hautli, Thomas Mayer, Miriam Butt, Frans Plank, and Daniel A. Keim. 2011. Towards Tracking Semantic Change by Visual Analytics. In ACL 2011 (Short Papers), pages 305–3 10, Portland, Oregon, USA, June. Association for Computational Linguistics. Christian Rohrdantz, Michael Hund, Thomas Mayer, Bernhard W ¨alchli, and Daniel A. Keim. 2012a. The World’s Languages Explorer: Visual Analysis of Language Features in Genealogical and Areal Contexts. Computer Graphics Forum, 3 1(3):935–944. Christian Rohrdantz, Andreas Niekler, Annette Hautli, Miriam Butt, and Daniel A. Keim. 2012b. Lexical Semantics and Distribution of Suffixes - A Visual Analysis. In Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 7–15, April. Tobias Schreck, J ¨urgen Bernard, Tatiana von Landesberger, and J o¨rn Kohlhammer. 2009. Visual cluster analysis of trajectory data with interactive kohonen maps. Information Visualization, 8(1): 14–29. James J. Thomas and Kristin A. Cook. 2006. A Visual Analytics Agenda. IEEE Computer Graphics and Applications, 26(1): 10–13. Jian Zhao, Fanny Chevalier, Christopher Collins, and Ravin Balakrishnan. 2012. Facilitating Discourse Analysis with Interactive Visualization. IEEE Trans. Vis. Comput. Graph., 18(12):2639–2648. 114</p><p>2 0.96989775 <a title="72-lda-2" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>3 0.96188039 <a title="72-lda-3" href="./acl-2013-ParaQuery%3A_Making_Sense_of_Paraphrase_Collections.html">271 acl-2013-ParaQuery: Making Sense of Paraphrase Collections</a></p>
<p>Author: Lili Kotlerman ; Nitin Madnani ; Aoife Cahill</p><p>Abstract: Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections. We present ParaQuery, a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection, analyze its utility for a particular domain, and compare it to other popular lexical similarity resources all within a single interface.</p><p>4 0.96017534 <a title="72-lda-4" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>Author: Hua He ; Denilson Barbosa ; Grzegorz Kondrak</p><p>Abstract: Speaker identification is the task of at- tributing utterances to characters in a literary narrative. It is challenging to auto- mate because the speakers of the majority ofutterances are not explicitly identified in novels. In this paper, we present a supervised machine learning approach for the task that incorporates several novel features. The experimental results show that our method is more accurate and general than previous approaches to the problem.</p><p>5 0.94642895 <a title="72-lda-5" href="./acl-2013-Does_Korean_defeat_phonotactic_word_segmentation%3F.html">128 acl-2013-Does Korean defeat phonotactic word segmentation?</a></p>
<p>Author: Robert Daland ; Kie Zuraw</p><p>Abstract: Computational models of infant word segmentation have not been tested on a wide range of languages. This paper applies a phonotactic segmentation model to Korean. In contrast to the undersegmentation pattern previously found in English and Russian, the model exhibited more oversegmentation errors and more errors overall. Despite the high error rate, analysis suggested that lexical acquisition might not be problematic, provided that infants attend only to frequently segmented items. 1</p><p>6 0.9439317 <a title="72-lda-6" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>same-paper 7 0.9104867 <a title="72-lda-7" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>8 0.86608231 <a title="72-lda-8" href="./acl-2013-Mining_Opinion_Words_and_Opinion_Targets_in_a_Two-Stage_Framework.html">244 acl-2013-Mining Opinion Words and Opinion Targets in a Two-Stage Framework</a></p>
<p>9 0.73045695 <a title="72-lda-9" href="./acl-2013-PhonMatrix%3A_Visualizing_co-occurrence_constraints_of_sounds.html">279 acl-2013-PhonMatrix: Visualizing co-occurrence constraints of sounds</a></p>
<p>10 0.69468862 <a title="72-lda-10" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>11 0.66269308 <a title="72-lda-11" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<p>12 0.65461743 <a title="72-lda-12" href="./acl-2013-Evaluating_Text_Segmentation_using_Boundary_Edit_Distance.html">140 acl-2013-Evaluating Text Segmentation using Boundary Edit Distance</a></p>
<p>13 0.63598013 <a title="72-lda-13" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>14 0.63380277 <a title="72-lda-14" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>15 0.63253373 <a title="72-lda-15" href="./acl-2013-Lightly_Supervised_Learning_of_Procedural_Dialog_Systems.html">230 acl-2013-Lightly Supervised Learning of Procedural Dialog Systems</a></p>
<p>16 0.62127614 <a title="72-lda-16" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>17 0.61873513 <a title="72-lda-17" href="./acl-2013-ICARUS_-_An_Extensible_Graphical_Search_Tool_for_Dependency_Treebanks.html">183 acl-2013-ICARUS - An Extensible Graphical Search Tool for Dependency Treebanks</a></p>
<p>18 0.61528885 <a title="72-lda-18" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>19 0.60999042 <a title="72-lda-19" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>20 0.60498697 <a title="72-lda-20" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
