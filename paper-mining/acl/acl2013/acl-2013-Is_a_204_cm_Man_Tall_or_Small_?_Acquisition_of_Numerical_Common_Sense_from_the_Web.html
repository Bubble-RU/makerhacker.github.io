<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>202 acl-2013-Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-202" href="#">acl2013-202</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>202 acl-2013-Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web</h1>
<br/><p>Source: <a title="acl-2013-202-pdf" href="http://aclweb.org/anthology//P/P13/P13-1038.pdf">pdf</a></p><p>Author: Katsuma Narisawa ; Yotaro Watanabe ; Junta Mizuno ; Naoaki Okazaki ; Kentaro Inui</p><p>Abstract: This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical common sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numerical expressions and their context from the Web. One approach estimates the distribution ofnumbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distri- bution. Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. Experimental results demonstrate the effectiveness of both approaches.</p><p>Reference: <a title="acl-2013-202-reference" href="../acl2013_reference/acl-2013-Is_a_204_cm_Man_Tall_or_Small_%3F_Acquisition_of_Numerical_Common_Sense_from_the_Web_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 t  Abstract This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e. [sent-5, score-0.861]
</p><p>2 We first discuss the necessity of numerical common sense in solving textual entailment problems. [sent-10, score-1.066]
</p><p>3 We explore two approaches for acquiring numerical common sense. [sent-11, score-0.912]
</p><p>4 Both approaches start with extracting numerical expressions and their context from the Web. [sent-12, score-1.045]
</p><p>5 Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. [sent-14, score-0.984]
</p><p>6 Given this background, we focus on solving one of the basic phenomena in RTE: semantic inference related to numerical expressions. [sent-25, score-0.885]
</p><p>7 The specific problem we address is acquisition of numerical common sense. [sent-26, score-0.885]
</p><p>8 For example, (1) t : Before long, 3b people will face a water shortage in  the world. [sent-27, score-0.26]
</p><p>9 Although recognizing the entailment relation between t and h is frustratingly difficult, we assume this inference is decomposable into three phases: 3b people face a water shortage. [sent-29, score-0.299]
</p><p>10 In the first phase, it is necessary to recognize 3b as a numerical expression and to resolve the expression 3b into the exact amount 3,000,000,000. [sent-33, score-1.103]
</p><p>11 In this paper, we address the first and second phases of inference as an initial step towards semantic processing with numerical expressions. [sent-35, score-0.825]
</p><p>12 We examine instances in existing RTE corpora, categorize them into groups in terms of the necessary semantic inferences, and discuss the impact of this study for solving RTE problems with numerical expressions. [sent-38, score-0.825]
</p><p>13 We describe a method of normalizing numerical expressions referring to the same amount in text into a unified semantic representation. [sent-40, score-1.007]
</p><p>14 We present approaches for aggregating numerical common sense from examples of numerical expressions and for judging whether a given amount is large, small, or normal. [sent-42, score-1.874]
</p><p>15 2  Related work  Surprisingly, NLP research has paid little attention to semantic processing of numerical expressions. [sent-49, score-0.825]
</p><p>16 Several studies deal with numerical expressions in the context of information extraction (Bakalov et al. [sent-55, score-1.045]
</p><p>17 , 3b people), but rather treat numerical expressions as strings. [sent-67, score-0.974]
</p><p>18 Given a query, their approach retrieves documents relevant to the query and identifies the quantities of numerical expressions in the retrieved documents. [sent-72, score-1.094]
</p><p>19 In question answering, to help “sanity check” answers with numerical values that were 1http : / /www . [sent-76, score-0.825]
</p><p>20 Some recent studies delve deeper into the semantic interpretation of numerical expressions. [sent-98, score-0.825]
</p><p>21 Davidov and Rappoport (2010) presented a method for the extraction from the Web and approximation of numerical object attributes such as height and weight. [sent-103, score-0.868]
</p><p>22 Given an object-attribute pair, the study expands the object into a set of comparable objects and then approximates the numerical values even when no exact value can be found in a text. [sent-104, score-0.859]
</p><p>23 , “Object is * [unit] tall”), focusing on a specific set of numerical attributes (e. [sent-108, score-0.825]
</p><p>24 Recently, the RTE community has started to pay some attention to the appropriate processing of numerical expressions. [sent-114, score-0.825]
</p><p>25 Iftene (2010) presented an approach for matching numerical ranges expressed by a set of phrases (e. [sent-115, score-0.825]
</p><p>26 However, these studies do not nec-  essarily focus on semantic processing of numerical expressions; thus, these studies do not normalize units of numerical expressions nor make inferences with numerical common sense. [sent-120, score-2.707]
</p><p>27 They argued the importance of aligning numerical quantities and performing numerical reasoning in RTE. [sent-123, score-1.704]
</p><p>28 These studies provided a closer look at the phenomena involved in RTE, but they did not propose a solution for handling numerical expressions. [sent-127, score-0.885]
</p><p>29 3  Investigation of textual-entailment pairs with numerical expressions  In this section, we investigate textual entailment (TE) pairs in existing corpora in order to study the core phenomena that establish an entailment relation. [sent-128, score-1.298]
</p><p>30 We manually selected sentence pairs in which one or both of the sentences contained a numerical expression. [sent-137, score-0.825]
</p><p>31 Here, we define the term numerical expression as an expression containing a number or quantity represented by a numeral and a unit. [sent-138, score-1.172]
</p><p>32 For example, 3 kilometers is a numerical expression with the numeral 3 and the unit kilometer. [sent-139, score-1.071]
</p><p>33 Note that intensity of 4 is not a numerical expression because intensity is not a unit. [sent-140, score-0.964]
</p><p>34 Note that we ignored T-H pairs in which numerical expressions were unnecessary to prove the entailment relation (e. [sent-143, score-1.072]
</p><p>35 Out of 371 pairs, we identified 114 pairs in which numerical expressions played a central role in the entailment relation. [sent-146, score-1.072]
</p><p>36 We can infer an entailment relation in this category by aligning two numerical expressions, e. [sent-149, score-0.923]
</p><p>37 cal reasoning, interpreting the amount (number, unit, and range) in a numerical expression. [sent-154, score-0.854]
</p><p>38 The second largest category requires common sense about numerical amounts. [sent-157, score-0.9]
</p><p>39 We believe that this study forms the basis for investigating other phenomena of numerical expressions in the future. [sent-164, score-1.034]
</p><p>40 4  Collecting numerical expressions from the Web  In this paper, we explore two approaches to acquiring numerical common sense. [sent-165, score-1.886]
</p><p>41 Both approaches start with extracting numerical expressions and  their context from the Web. [sent-166, score-1.045]
</p><p>42 We define a context as the verb and its arguments that appear around a numerical expression. [sent-167, score-0.896]
</p><p>43 For instance, the context of3bpeople in the sentence 3b people face a water shortage is “face” and “water shortage. [sent-168, score-0.331]
</p><p>44 ” In order to extract and aggregate numerical expressions in various documents, we converted the numerical expressions into semantic representations (to be described in Section 4. [sent-169, score-1.948]
</p><p>45 The first approach for acquiring numerical common sense estimates the distribution of numbers that co-occur within a context, and examines whether a given value is large, small, or normal based on that distribution (to be described in Section 5. [sent-172, score-1.13]
</p><p>46 The second approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical ex384 CategoryDefinitionExample#  Numerical matchingsdAi folinfgesrnien in cg Tes n aiun d umne Hrit, ic c ra oln gse i,xd pe tr cei. [sent-174, score-0.984]
</p><p>47 In this study, we acquired numerical common sense from a collection of 8 billion sentences in 100 million Japanese Web pages (Shinzato et al. [sent-221, score-0.938]
</p><p>48 1 Extracting and normalizing numerical expressions The first step for collecting numerical expressions is to recognize when a numerical expression is mentioned and then to normalize it into a semantic representation. [sent-226, score-2.945]
</p><p>49 This is the most fundamental  StringOperation  Tkgantobirsglmo(euigtspa)rn3lem:yoapA(sle)nx sae tm- umpnoilted:‘foignef’r ;n:um’ ‘las(upimbrtelgo/auimpsl’toyn-d)viafluer:d1,i0ct o,n0ary  step in numerical reasoning and has a number of applications. [sent-227, score-0.853]
</p><p>50 For example, this step handles cases of numerical matching, as in Table 1. [sent-228, score-0.825]
</p><p>51 The semantic representation of a numerical expression consists of three fields: the value or range of the real number(s)5, the unit (a string), and the optional modifiers. [sent-229, score-1.053]
</p><p>52 Table 2 shows some examples of numerical expressions and their semantic representations. [sent-230, score-0.974]
</p><p>53 When a numerical expression is accompanied by a modifier such as over, about, or more than, we updated the value and modifier fields appropriately. [sent-236, score-1.116]
</p><p>54 385  We developed an extractor and a normalizer for Japanese numerical expressions6. [sent-240, score-0.852]
</p><p>55 Find numbers in the text by using regular expressions and convert the non-Arabic numbers into their corresponding Arabic numbers. [sent-243, score-0.273]
</p><p>56 We activated the modifier ‘large’ when a numerical expression occurred with the Japanese word mo, which roughly corresponds to as many as, as large as, or as heavy as in English10. [sent-258, score-1.05]
</p><p>57 Similarly, we activated the modifier ‘small’ when a numerical expression occurred with the word shika, which roughly corresponds to as little as, as small as, or as light . [sent-259, score-1.065]
</p><p>58 10In Japanese, we can use the word mo with a numerical expression to state that the amount is ‘large’ regardless of  how large it is (e. [sent-270, score-1.027]
</p><p>59 2  Extraction of context  The next step in acquiring numerical common sense is to capture the context of numerical expressions. [sent-292, score-1.918]
</p><p>60 The context of a numerical expression should provide sufficient information to determine what it measures. [sent-294, score-1.035]
</p><p>61 For example, given the sentence, “He gave $300 to  a friend at the bank,” it would be better if we could generalize the context to someone gives money to a friend for the numerical expression $300. [sent-295, score-1.224]
</p><p>62 For this reason, we employ a simple rule to capture the context of numerical expressions: we represent the context with the verb that governs the numerical expression and its typed arguments. [sent-297, score-1.954]
</p><p>63 Figure 1 illustrates the procedure for extracting the context of a numerical expression12. [sent-298, score-0.896]
</p><p>64 1recognizes $300 as a numerical expression, then normalizes it into a semantic representation. [sent-300, score-0.825]
</p><p>65 Because the numerical expression is a dependent ofthe verb gave, we extract the verb and its arguments (except for the numerical expression itself) as the context. [sent-301, score-1.928]
</p><p>66 5  Acquiring numerical common sense  In this section, we present two approaches for ac-  quiring numerical common sense from a collection of numerical expressions and their contexts. [sent-303, score-2.774]
</p><p>67 Both approaches start with collecting the numbers (in semantic representation) and contexts of numerical expressions from a large number of sentences (Shinzato et al. [sent-304, score-1.063]
</p><p>68 When a context and a value are given for a prediction (hereinafter called the query context and query value, respectively), these approaches judge whether the query value is large, small, or normal. [sent-307, score-0.408]
</p><p>69 1 Distribution-based approach Given a query context and query value, this approach retrieves numbers associated with the query context and draws a distribution of normalized numbers. [sent-309, score-0.43]
</p><p>70 2  Clue-based approach  This approach utilizes textual clues with which a  speaker explicitly expresses his or her judgment about the amount of a numerical expression. [sent-320, score-0.978]
</p><p>71 The numerator of Equation 2 counts the number of numerical expressions that support the judgment that x is large14, and its denominator counts the total number of numerical expressions with large as a modifier. [sent-342, score-2.032]
</p><p>72 1 Normalizing numerical expressions We evaluated the method that we described in Section 4. [sent-350, score-0.974]
</p><p>73 In order to prepare a gold-standard data set, we obtained 1,041 sentences by randomly sampling about 1% of the sentences containing numbers (Arabic digits and/or Chinese numerical characters) in a Japanese Web corpus (100 million pages) (Shinzato et al. [sent-352, score-0.925]
</p><p>74 For every numerical expression in these sentences, we manually determined a tuple of the normalized value, unit, and modifier. [sent-354, score-0.964]
</p><p>75 We obtained 329 numerical expressions from the 1,041 sentences. [sent-356, score-0.974]
</p><p>76 For example, the proposed method could not identify 1Ghz as a numerical expression because the unit dictionary did not register Ghz but GHz. [sent-363, score-1.019]
</p><p>77 For example, the proposed method identified Seven Hills as a numerical expression although it denotes a location name. [sent-366, score-0.964]
</p><p>78 In order to reduce false positives, it may be necessary to utilize broader contexts when locating numerical expressions; this could be done by using, for example, a named entity recognizer. [sent-367, score-0.861]
</p><p>79 However, these errors do not have a large effect on the estimation of the distribution of the numerical values that occur with specific named entities and idiomatic phrases. [sent-369, score-0.852]
</p><p>80 Moreover, as explained in  Section 5, we draw distributions for fine-grained contexts of numerical expressions. [sent-370, score-0.825]
</p><p>81 For these reasons, we think that the current performance is sufficient for acquiring numerical common sense. [sent-371, score-0.912]
</p><p>82 1 Preparing an evaluation set We built a gold-standard data set for numerical common sense. [sent-375, score-0.861]
</p><p>83 We asked three human judges to annotate every numerical expression with one of six labels, small, relatively small, normal, relatively large, large, and unsure. [sent-379, score-0.964]
</p><p>84 The label relatively small could be applied to a numerical expression when the judge felt that the amount was rather small (below the normal) but hesitated to label it small. [sent-380, score-1.094]
</p><p>85 We gave the following criteria for labeling an item as unsure: when the judgment was highly dependent on the context; when the sentence was incompre-  hensible; and when it was a non-numerical expressions (false positives of the method are discussed in Section 4. [sent-382, score-0.238]
</p><p>86 0%) Table 4: Inter-annotator agreement [# extrac7on]  [# extrac7on]  (clue:based)  (distribu7on:based)  210 "130"cdl4uis0et"r:ib au1s57ed0o"n(sl:ambr1ag6sel0)d" 170"8 190"2 10"52430" 0 0 [cm]  Figure 2: Distributions of numbers with large and small modifiers for the context human ’s height. [sent-390, score-0.231]
</p><p>87 For the evaluation of numerical expressions in the data set, we used those for which at least two annotators assigned the same label. [sent-391, score-0.974]
</p><p>88 After removing the unsure instances, we obtained 640 numerical expressions (20 small, 35 relatively small, 152 normal, 263 relatively large, and 170 large) as the evaluation set. [sent-392, score-1.001]
</p><p>89 2 Results The proposed method extracted about 23 million pairs of numerical expressions and their context from the corpus (with 100 million Web pages). [sent-395, score-1.121]
</p><p>90 These distributions are quite reasonable as common-sense knowledge: we can interpret that numbers under 150 cm are perceived as small and those above 180 cm as  large. [sent-398, score-0.244]
</p><p>91 We should extract context as 7  small  normal  have broken up with their boyfriends recently. [sent-416, score-0.227]
</p><p>92 We present translations of the  sentences,  which  were  origi-  and accuracy (Acc) of the acquisition of numerical common sense. [sent-422, score-0.885]
</p><p>93 ” Human judges label normal to the numerical expression two reasons, but the method predicts small. [sent-446, score-1.07]
</p><p>94 Our approach represents the context of a numerical expression with the verb that governs the numerical expression and its typed arguments. [sent-450, score-2.022]
</p><p>95 These are similar to the scope-ambiguity problem such as encoun389  tered with negation and quantification; it is difficult to model the scope when a numerical expres-  sion refers to a situation. [sent-453, score-0.825]
</p><p>96 7  Conclusions  We proposed novel approaches for acquiring numerical common sense from a collection of texts. [sent-460, score-0.951]
</p><p>97 The approaches collect numerical expressions and their contexts from the Web, and acquire numerical common sense by considering the distributions of normalized numbers and textual clues such as mo (as many as) and shika (only, as few as). [sent-461, score-2.129]
</p><p>98 We believe that acquisition of numerical common sense is an important step towards a deeper understanding of inferences with numbers. [sent-464, score-0.971]
</p><p>99 Finally, we are planning to address the ‘third phase’ of the example explained in Section 1: associating many people face a water shortage with a serious water shortage. [sent-468, score-0.372]
</p><p>100 Extraction and approximation of numerical attributes from the web. [sent-510, score-0.825]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('numerical', 0.825), ('expressions', 0.149), ('expression', 0.139), ('water', 0.112), ('japanese', 0.101), ('entailment', 0.098), ('rte', 0.096), ('normal', 0.083), ('shinzato', 0.076), ('prep', 0.075), ('context', 0.071), ('cm', 0.07), ('textual', 0.068), ('query', 0.066), ('numbers', 0.062), ('rite', 0.062), ('odani', 0.061), ('shika', 0.061), ('phenomena', 0.06), ('modifier', 0.059), ('shortage', 0.059), ('friend', 0.058), ('judgment', 0.057), ('unit', 0.055), ('acquiring', 0.051), ('inferences', 0.047), ('fontoura', 0.046), ('lenient', 0.046), ('suma', 0.046), ('people', 0.045), ('quantity', 0.044), ('face', 0.044), ('height', 0.043), ('small', 0.042), ('money', 0.041), ('cyc', 0.04), ('yoshida', 0.04), ('sense', 0.039), ('million', 0.038), ('rv', 0.037), ('aramaki', 0.037), ('cabrio', 0.037), ('arithmetic', 0.037), ('common', 0.036), ('false', 0.036), ('mo', 0.036), ('kat', 0.035), ('value', 0.034), ('prager', 0.033), ('sammons', 0.033), ('davidov', 0.033), ('normalizing', 0.033), ('gave', 0.032), ('boyfriends', 0.031), ('kilograms', 0.031), ('lobue', 0.031), ('lottery', 0.031), ('piquant', 0.031), ('sentenced', 0.031), ('socrates', 0.031), ('toledo', 0.031), ('tomohide', 0.031), ('tsuboi', 0.031), ('modifiers', 0.029), ('consensus', 0.029), ('precede', 0.029), ('interpreting', 0.029), ('dagan', 0.029), ('retrieves', 0.028), ('reasoning', 0.028), ('clues', 0.028), ('large', 0.027), ('jst', 0.027), ('normalizer', 0.027), ('imebank', 0.027), ('imeml', 0.027), ('shibata', 0.027), ('unsure', 0.027), ('ohoku', 0.027), ('soumen', 0.027), ('bakalov', 0.027), ('iftene', 0.027), ('kilometers', 0.027), ('tall', 0.027), ('nsubj', 0.026), ('quantities', 0.026), ('web', 0.026), ('hiroshi', 0.025), ('turtle', 0.025), ('numeral', 0.025), ('bernardo', 0.025), ('says', 0.025), ('phase', 0.024), ('banerjee', 0.024), ('acquisition', 0.024), ('pl', 0.024), ('governs', 0.023), ('registered', 0.023), ('label', 0.023), ('physical', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="202-tfidf-1" href="./acl-2013-Is_a_204_cm_Man_Tall_or_Small_%3F_Acquisition_of_Numerical_Common_Sense_from_the_Web.html">202 acl-2013-Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web</a></p>
<p>Author: Katsuma Narisawa ; Yotaro Watanabe ; Junta Mizuno ; Naoaki Okazaki ; Kentaro Inui</p><p>Abstract: This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical common sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numerical expressions and their context from the Web. One approach estimates the distribution ofnumbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distri- bution. Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. Experimental results demonstrate the effectiveness of both approaches.</p><p>2 0.16259769 <a title="202-tfidf-2" href="./acl-2013-Modeling_Human_Inference_Process_for_Textual_Entailment_Recognition.html">245 acl-2013-Modeling Human Inference Process for Textual Entailment Recognition</a></p>
<p>Author: Hen-Hsen Huang ; Kai-Chun Chang ; Hsin-Hsi Chen</p><p>Abstract: This paper aims at understanding what human think in textual entailment (TE) recognition process and modeling their thinking process to deal with this problem. We first analyze a labeled RTE-5 test set and find that the negative entailment phenomena are very effective features for TE recognition. Then, a method is proposed to extract this kind of phenomena from text-hypothesis pairs automatically. We evaluate the performance of using the negative entailment phenomena on both the English RTE-5 dataset and Chinese NTCIR-9 RITE dataset, and conclude the same findings.</p><p>3 0.098050259 <a title="202-tfidf-3" href="./acl-2013-Recognizing_Partial_Textual_Entailment.html">297 acl-2013-Recognizing Partial Textual Entailment</a></p>
<p>Author: Omer Levy ; Torsten Zesch ; Ido Dagan ; Iryna Gurevych</p><p>Abstract: Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. Indeed, our results show that these methods are useful for rec- ognizing partial entailment. We also provide a preliminary assessment of how partial entailment may be used for recognizing (complete) textual entailment.</p><p>4 0.089101441 <a title="202-tfidf-4" href="./acl-2013-Building_Japanese_Textual_Entailment_Specialized_Data_Sets_for_Inference_of_Basic_Sentence_Relations.html">75 acl-2013-Building Japanese Textual Entailment Specialized Data Sets for Inference of Basic Sentence Relations</a></p>
<p>Author: Kimi Kaneko ; Yusuke Miyao ; Daisuke Bekki</p><p>Abstract: This paper proposes a methodology for generating specialized Japanese data sets for textual entailment, which consists of pairs decomposed into basic sentence relations. We experimented with our methodology over a number of pairs taken from the RITE-2 data set. We compared our methodology with existing studies in terms of agreement, frequencies and times, and we evaluated its validity by investigating recognition accuracy.</p><p>5 0.072843306 <a title="202-tfidf-5" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>Author: David Kauchak</p><p>Abstract: In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams.</p><p>6 0.058287282 <a title="202-tfidf-6" href="./acl-2013-Paraphrasing_Adaptation_for_Web_Search_Ranking.html">273 acl-2013-Paraphrasing Adaptation for Web Search Ranking</a></p>
<p>7 0.058164511 <a title="202-tfidf-7" href="./acl-2013-Accurate_Word_Segmentation_using_Transliteration_and_Language_Model_Projection.html">34 acl-2013-Accurate Word Segmentation using Transliteration and Language Model Projection</a></p>
<p>8 0.054501809 <a title="202-tfidf-8" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>9 0.048806839 <a title="202-tfidf-9" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<p>10 0.047943018 <a title="202-tfidf-10" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>11 0.046896569 <a title="202-tfidf-11" href="./acl-2013-Bi-directional_Inter-dependencies_of_Subjective_Expressions_and_Targets_and_their_Value_for_a_Joint_Model.html">67 acl-2013-Bi-directional Inter-dependencies of Subjective Expressions and Targets and their Value for a Joint Model</a></p>
<p>12 0.046599317 <a title="202-tfidf-12" href="./acl-2013-Integrating_Multiple_Dependency_Corpora_for_Inducing_Wide-coverage_Japanese_CCG_Resources.html">199 acl-2013-Integrating Multiple Dependency Corpora for Inducing Wide-coverage Japanese CCG Resources</a></p>
<p>13 0.046193816 <a title="202-tfidf-13" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>14 0.045994051 <a title="202-tfidf-14" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>15 0.045670003 <a title="202-tfidf-15" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>16 0.045039367 <a title="202-tfidf-16" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>17 0.044734556 <a title="202-tfidf-17" href="./acl-2013-Question_Analysis_for_Polish_Question_Answering.html">290 acl-2013-Question Analysis for Polish Question Answering</a></p>
<p>18 0.044483308 <a title="202-tfidf-18" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>19 0.043539476 <a title="202-tfidf-19" href="./acl-2013-A_Visual_Analytics_System_for_Cluster_Exploration.html">29 acl-2013-A Visual Analytics System for Cluster Exploration</a></p>
<p>20 0.042721126 <a title="202-tfidf-20" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, 0.046), (2, -0.015), (3, -0.062), (4, -0.007), (5, 0.004), (6, -0.03), (7, -0.035), (8, 0.022), (9, 0.006), (10, -0.03), (11, 0.014), (12, -0.007), (13, 0.012), (14, -0.022), (15, -0.057), (16, 0.024), (17, 0.029), (18, -0.002), (19, -0.011), (20, 0.039), (21, -0.043), (22, -0.013), (23, 0.069), (24, -0.01), (25, 0.113), (26, -0.095), (27, 0.043), (28, 0.043), (29, -0.113), (30, -0.024), (31, 0.051), (32, -0.049), (33, -0.064), (34, -0.032), (35, -0.065), (36, 0.003), (37, -0.056), (38, -0.092), (39, -0.055), (40, -0.004), (41, -0.052), (42, 0.03), (43, 0.071), (44, -0.042), (45, 0.055), (46, 0.045), (47, 0.077), (48, -0.005), (49, 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93406343 <a title="202-lsi-1" href="./acl-2013-Is_a_204_cm_Man_Tall_or_Small_%3F_Acquisition_of_Numerical_Common_Sense_from_the_Web.html">202 acl-2013-Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web</a></p>
<p>Author: Katsuma Narisawa ; Yotaro Watanabe ; Junta Mizuno ; Naoaki Okazaki ; Kentaro Inui</p><p>Abstract: This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical common sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numerical expressions and their context from the Web. One approach estimates the distribution ofnumbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distri- bution. Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. Experimental results demonstrate the effectiveness of both approaches.</p><p>2 0.86255699 <a title="202-lsi-2" href="./acl-2013-Building_Japanese_Textual_Entailment_Specialized_Data_Sets_for_Inference_of_Basic_Sentence_Relations.html">75 acl-2013-Building Japanese Textual Entailment Specialized Data Sets for Inference of Basic Sentence Relations</a></p>
<p>Author: Kimi Kaneko ; Yusuke Miyao ; Daisuke Bekki</p><p>Abstract: This paper proposes a methodology for generating specialized Japanese data sets for textual entailment, which consists of pairs decomposed into basic sentence relations. We experimented with our methodology over a number of pairs taken from the RITE-2 data set. We compared our methodology with existing studies in terms of agreement, frequencies and times, and we evaluated its validity by investigating recognition accuracy.</p><p>3 0.83198982 <a title="202-lsi-3" href="./acl-2013-Modeling_Human_Inference_Process_for_Textual_Entailment_Recognition.html">245 acl-2013-Modeling Human Inference Process for Textual Entailment Recognition</a></p>
<p>Author: Hen-Hsen Huang ; Kai-Chun Chang ; Hsin-Hsi Chen</p><p>Abstract: This paper aims at understanding what human think in textual entailment (TE) recognition process and modeling their thinking process to deal with this problem. We first analyze a labeled RTE-5 test set and find that the negative entailment phenomena are very effective features for TE recognition. Then, a method is proposed to extract this kind of phenomena from text-hypothesis pairs automatically. We evaluate the performance of using the negative entailment phenomena on both the English RTE-5 dataset and Chinese NTCIR-9 RITE dataset, and conclude the same findings.</p><p>4 0.82906455 <a title="202-lsi-4" href="./acl-2013-Recognizing_Partial_Textual_Entailment.html">297 acl-2013-Recognizing Partial Textual Entailment</a></p>
<p>Author: Omer Levy ; Torsten Zesch ; Ido Dagan ; Iryna Gurevych</p><p>Abstract: Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to this setting. Indeed, our results show that these methods are useful for rec- ognizing partial entailment. We also provide a preliminary assessment of how partial entailment may be used for recognizing (complete) textual entailment.</p><p>5 0.56852669 <a title="202-lsi-5" href="./acl-2013-PLIS%3A_a_Probabilistic_Lexical_Inference_System.html">269 acl-2013-PLIS: a Probabilistic Lexical Inference System</a></p>
<p>Author: Eyal Shnarch ; Erel Segal-haLevi ; Jacob Goldberger ; Ido Dagan</p><p>Abstract: This paper presents PLIS, an open source Probabilistic Lexical Inference System which combines two functionalities: (i) a tool for integrating lexical inference knowledge from diverse resources, and (ii) a framework for scoring textual inferences based on the integrated knowledge. We provide PLIS with two probabilistic implementation of this framework. PLIS is available for download and developers of text processing applications can use it as an off-the-shelf component for injecting lexical knowledge into their applications. PLIS is easily configurable, components can be extended or replaced with user generated ones to enable system customization and further research. PLIS includes an online interactive viewer, which is a powerful tool for investigating lexical inference processes. 1 Introduction and background Semantic Inference is the process by which machines perform reasoning over natural language texts. A semantic inference system is expected to be able to infer the meaning of one text from the meaning of another, identify parts of texts which convey a target meaning, and manipulate text units in order to deduce new meanings. Semantic inference is needed for many Natural Language Processing (NLP) applications. For instance, a Question Answering (QA) system may encounter the following question and candidate answer (Example 1): Q: which explorer discovered the New World? A: Christopher Columbus revealed America. As there are no overlapping words between the two sentences, to identify that A holds an answer for Q, background world knowledge is needed to link Christopher Columbus with explorer and America with New World. Linguistic knowledge is also needed to identify that reveal and discover refer to the same concept. Knowledge is needed in order to bridge the gap between text fragments, which may be dissimilar on their surface form but share a common meaning. For the purpose of semantic inference, such knowledge can be derived from various resources (e.g. WordNet (Fellbaum, 1998) and others, detailed in Section 2.1) in a form which we denote as inference links (often called inference/entailment rules), each is an ordered pair of elements in which the first implies the meaning of the second. For instance, the link ship→vessel can be derived from tshtaen hypernym rkel sahtiiopn→ ovfe Wsseolr cdNanet b. Other applications can benefit from utilizing inference links to identify similarity between language expressions. In Information Retrieval, the user’s information need may be expressed in relevant documents differently than it is expressed in the query. Summarization systems should identify text snippets which convey the same meaning. Our work addresses a generic, application in- dependent, setting of lexical inference. We therefore adopt the terminology of Textual Entailment (Dagan et al., 2006), a generic paradigm for applied semantic inference which captures inference needs of many NLP applications in a common underlying task: given two textual fragments, termed hypothesis (H) and text (T), the task is to recognize whether T implies the meaning of H, denoted T→H. For instance, in a QA application, H reprTe→seHnts. Fthoer question, a innd a T Q a c aanpdpilidcaattei answer. pInthis setting, T is likely to hold an answer for the question if it entails the question. It is challenging to properly extract the needed inference knowledge from available resources, and to effectively utilize it within the inference process. The integration of resources, each has its own format, is technically complex and the quality 97 ProceedingSsof oiaf, th Beu 5lg1asrtia A,n Anuuaglu Mst 4ee-9tin 2g0 o1f3. th ?ec A20ss1o3ci Aastisoonci faotrio Cno fomrp Cuotamtipountaalti Loinnaglu Lisitnigcsu,is patigcess 97–102, Figure 1: PLIS schema - a text-hypothesis pair is processed by the Lexical Integrator which uses a set of lexical resources to extract inference chains which connect the two. The Lexical Inference component provides probability estimations for the validity of each level of the process. ofthe resulting inference links is often unknown in advance and varies considerably. For coping with this challenge we developed PLIS, a Probabilistic Lexical Inference System1 . PLIS, illustrated in Fig 1, has two main modules: the Lexical Integra- tor (Section 2) accepts a set of lexical resources and a text-hypothesis pair, and finds all the lexical inference relations between any pair of text term ti and hypothesis term hj, based on the available lexical relations found in the resources (and their combination). The Lexical Inference module (Section 3) provides validity scores for these relations. These term-level scores are used to estimate the sentence-level likelihood that the meaning of the hypothesis can be inferred from the text, thus making PLIS a complete lexical inference system. Lexical inference systems do not look into the structure of texts but rather consider them as bag ofterms (words or multi-word expressions). These systems are easy to implement, fast to run, practical across different genres and languages, while maintaining a competitive level of performance. PLIS can be used as a stand-alone efficient inference system or as the lexical component of any NLP application. PLIS is a flexible system, allowing users to choose the set of knowledge resources as well as the model by which inference 1The complete software package is available at http:// www.cs.biu.ac.il/nlp/downloads/PLIS.html and an online interactive viewer is available for examination at http://irsrv2. cs.biu.ac.il/nlp-net/PLIS.html. is done. PLIS can be easily extended with new knowledge resources and new inference models. It comes with a set of ready-to-use plug-ins for many common lexical resources (Section 2.1) as well as two implementation of the scoring framework. These implementations, described in (Shnarch et al., 2011; Shnarch et al., 2012), provide probability estimations for inference. PLIS has an interactive online viewer (Section 4) which provides a visualization of the entire inference process, and is very helpful for analysing lexical inference models and lexical resources usability. 2 Lexical integrator The input for the lexical integrator is a set of lexical resources and a pair of text T and hypothesis H. The lexical integrator extracts lexical inference links from the various lexical resources to connect each text term ti ∈ T with each hypothesis term hj ∈ H2. A lexical i∈nfTer wenicthe elianckh hinydpicoathteess a semantic∈ rHelation between two terms. It could be a directional relation (Columbus→navigator) or a bai ddiirreeccttiioonnaall one (car ←→ automobile). dSirinecceti knowledge resources vary lien) their representation methods, the lexical integrator wraps each lexical resource in a common plug-in interface which encapsulates resource’s inner representation method and exposes its knowledge as a list of inference links. The implemented plug-ins that come with PLIS are described in Section 2.1. Adding a new lexical resource and integrating it with the others only demands the implementation of the plug-in interface. As the knowledge needed to connect a pair of terms, ti and hj, may be scattered across few resources, the lexical integrator combines inference links into lexical inference chains to deduce new pieces of knowledge, such as Columbus −r −e −so −u −rc −e →2 −r −e −so −u −rc −e →1 navigator explorer. Therefore, the only assumption −t −he − l−e −x →ica elx integrator makes, regarding its input lexical resources, is that the inferential lexical relations they provide are transitive. The lexical integrator generates lexical infer- ence chains by expanding the text and hypothesis terms with inference links. These links lead to new terms (e.g. navigator in the above chain example and t0 in Fig 1) which can be further expanded, as all inference links are transitive. A transitivity 2Where iand j run from 1 to the length of the text and hypothesis respectively. 98 limit is set by the user to determine the maximal length for inference chains. The lexical integrator uses a graph-based representation for the inference chains, as illustrates in Fig 1. A node holds the lemma, part-of-speech and sense of a single term. The sense is the ordinal number of WordNet sense. Whenever we do not know the sense of a term we implement the most frequent sense heuristic.3 An edge represents an inference link and is labeled with the semantic relation of this link (e.g. cytokine→protein is larbeellaetdio wni othf tt hheis sW linokrd (Nee.gt .re clayttiookni hypernym). 2.1 Available plug-ins for lexical resources We have implemented plug-ins for the follow- ing resources: the English lexicon WordNet (Fellbaum, 1998)(based on either JWI, JWNL or extJWNL java APIs4), CatVar (Habash and Dorr, 2003), a categorial variations database, Wikipedia-based resource (Shnarch et al., 2009), which applies several extraction methods to derive inference links from the text and structure of Wikipedia, VerbOcean (Chklovski and Pantel, 2004), a knowledge base of fine-grained semantic relations between verbs, Lin’s distributional similarity thesaurus (Lin, 1998), and DIRECT (Kotlerman et al., 2010), a directional distributional similarity thesaurus geared for lexical inference. To summarize, the lexical integrator finds all possible inference chains (of a predefined length), resulting from any combination of inference links extracted from lexical resources, which link any t, h pair of a given text-hypothesis. Developers can use this tool to save the hassle of interfacing with the different lexical knowledge resources, and spare the labor of combining their knowledge via inference chains. The lexical inference model, described next, provides a mean to decide whether a given hypothesis is inferred from a given text, based on weighing the lexical inference chains extracted by the lexical integrator. 3 Lexical inference There are many ways to implement an inference model which identifies inference relations between texts. A simple model may consider the 3This disambiguation policy was better than considering all senses of an ambiguous term in preliminary experiments. However, it is a matter of changing a variable in the configuration of PLIS to switch between these two policies. 4http://wordnet.princeton.edu/wordnet/related-projects/ number of hypothesis terms for which inference chains, originated from text terms, were found. In PLIS, the inference model is a plug-in, similar to the lexical knowledge resources, and can be easily replaced to change the inference logic. We provide PLIS with two implemented baseline lexical inference models which are mathematically based. These are two Probabilistic Lexical Models (PLMs), HN-PLM and M-PLM which are described in (Shnarch et al., 2011; Shnarch et al., 2012) respectively. A PLM provides probability estimations for the three parts of the inference process (as shown in Fig 1): the validity probability of each inference chain (i.e. the probability for a valid inference relation between its endpoint terms) P(ti → hj), the probability of each hypothesis term to →b e i hnferred by the entire text P(T → hj) (term-level probability), eanntdir teh tee probability o hf the entire hypothesis to be inferred by the text P(T → H) (sentencelteov eble probability). HN-PLM describes a generative process by which the hypothesis is generated from the text. Its parameters are the reliability level of each of the resources it utilizes (that is, the prior probability that applying an arbitrary inference link derived from each resource corresponds to a valid inference). For learning these parameters HN-PLM applies a schema of the EM algorithm (Dempster et al., 1977). Its performance on the recognizing textual entailment task, RTE (Bentivogli et al., 2009; Bentivogli et al., 2010), are in line with the state of the art inference systems, including complex systems which perform syntactic analysis. This model is improved by M-PLM, which deduces sentence-level probability from term-level probabilities by a Markovian process. PLIS with this model was used for a passage retrieval for a question answering task (Wang et al., 2007), and outperformed state of the art inference systems. Both PLMs model the following prominent aspects of the lexical inference phenomenon: (i) considering the different reliability levels of the input knowledge resources, (ii) reducing inference chain probability as its length increases, and (iii) increasing term-level probability as we have more inference chains which suggest that the hypothesis term is inferred by the text. Both PLMs only need sentence-level annotations from which they derive term-level inference probabilities. To summarize, the lexical inference module 99 ?(? → ?) Figure 2: PLIS interactive viewer with Example 1 demonstrates knowledge integration of multiple inference chains and resource combination (additional explanations, which are not part of the demo, are provided in orange). provides the setting for interfacing with the lexical integrator. Additionally, the module provides the framework for probabilistic inference models which estimate term-level probabilities and integrate them into a sentence-level inference decision, while implementing prominent aspects of lexical inference. The user can choose to apply another inference logic, not necessarily probabilistic, by plugging a different lexical inference model into the provided inference infrastructure. 4 The PLIS interactive system PLIS comes with an online interactive viewer5 in which the user sets the parameters of PLIS, inserts a text-hypothesis pair and gets a visualization of the entire inference process. This is a powerful tool for investigating knowledge integration and lexical inference models. Fig 2 presents a screenshot of the processing of Example 1. On the right side, the user configures the system by selecting knowledge resources, adjusting their configuration, setting the transitivity limit, and choosing the lexical inference model to be applied by PLIS. After inserting a text and a hypothesis to the appropriate text boxes, the user clicks on the infer button and PLIS generates all lexical inference chains, of length up to the transitivity limit, that connect text terms with hypothesis terms, as available from the combination of the selected input re5http://irsrv2.cs.biu.ac.il/nlp-net/PLIS.html sources. Each inference chain is presented in a line between the text and hypothesis. PLIS also displays the probability estimations for all inference levels; the probability of each chain is presented at the end of its line. For each hypothesis term, term-level probability, which weighs all inference chains found for it, is given below the dashed line. The overall sentence-level probability integrates the probabilities of all hypothesis terms and is displayed in the box at the bottom right corner. Next, we detail the inference process of Example 1, as presented in Fig 2. In this QA example, the probability of the candidate answer (set as the text) to be relevant for the given question (the hypothesis) is estimated. When utilizing only two knowledge resources (WordNet and Wikipedia), PLIS is able to recognize that explorer is inferred by Christopher Columbus and that New World is inferred by America. Each one of these pairs has two independent inference chains, numbered 1–4, as evidence for its inference relation. Both inference chains 1 and 3 include a single inference link, each derived from a different relation of the Wikipedia-based resource. The inference model assigns a higher probability for chain 1since the BeComp relation is much more reliable than the Link relation. This comparison illustrates the ability of the inference model to learn how to differ knowledge resources by their reliability. Comparing the probability assigned by the in100 ference model for inference chain 2 with the probabilities assigned for chains 1 and 3, reveals the sophisticated way by which the inference model integrates lexical knowledge. Inference chain 2 is longer than chain 1, therefore its probability is lower. However, the inference model assigns chain 2 a higher probability than chain 3, even though the latter is shorter, since the model is sensitive enough to consider the difference in reliability levels between the two highly reliable hypernym relations (from WordNet) of chain 2 and the less reliable Link relation (from Wikipedia) of chain 3. Another aspect of knowledge integration is exemplified in Fig 2 by the three circled probabilities. The inference model takes into consideration the multiple pieces of evidence for the inference of New World (inference chains 3 and 4, whose probabilities are circled). This results in a termlevel probability estimation for New World (the third circled probability) which is higher than the probabilities of each chain separately. The third term of the hypothesis, discover, remains uncovered by the text as no inference chain was found for it. Therefore, the sentence-level inference probability is very low, 37%. In order to identify that the hypothesis is indeed inferred from the text, the inference model should be provided with indications for the inference of discover. To that end, the user may increase the transitivity limit in hope that longer inference chains provide the needed information. In addition, the user can examine other knowledge resources in search for the missing inference link. In this example, it is enough to add VerbOcean to the input of PLIS to expose two inference chains which connect reveal with discover by combining an inference link from WordNet and another one from VerbOcean. With this additional information, the sentence-level probability increases to 76%. This is a typical scenario of utilizing PLIS, either via the interactive system or via the software, for analyzing the usability of the different knowledge resources and their combination. A feature of the interactive system, which is useful for lexical resources analysis, is that each term in a chain is clickable and links to another screen which presents all the terms that are inferred from it and those from which it is inferred. Additionally, the interactive system communicates with a server which runs PLIS, in a fullduplex WebSocket connection6. This mode of operation is publicly available and provides a method for utilizing PLIS, without having to install it or the lexical resources it uses. Finally, since PLIS is a lexical system it can easily be adjusted to other languages. One only needs to replace the basic lexical text processing tools and plug in knowledge resources in the target language. If PLIS is provided with bilingual resources,7 it can operate also as a cross-lingual inference system (Negri et al., 2012). For instance, the text in Fig 3 is given in English, while the hypothesis is written in Spanish (given as a list of lemma:part-of-speech). The left side of the figure depicts a cross-lingual inference process in which the only lexical knowledge resource used is a man- ually built English-Spanish dictionary. As can be seen, two Spanish terms, jugador and casa remain uncovered since the dictionary alone cannot connect them to any of the English terms in the text. As illustrated in the right side of Fig 3, PLIS enables the combination of the bilingual dictionary with monolingual resources to produce cross-lingual inference chains, such as footballer−h −y −p −er−n y −m →player− −m −a −nu − →aljugador. Such inferenc−e − c−h −a −in − →s hpalavey trh− e− capability otro. overcome monolingual language variability (the first link in this chain) as well as to provide cross-lingual translation (the second link). 5 Conclusions To utilize PLIS one should gather lexical resources, obtain sentence-level annotations and train the inference model. Annotations are available in common data sets for task such as QA, Information Retrieval (queries are hypotheses and snippets are texts) and Student Response Analysis (reference answers are the hypotheses that should be inferred by the student answers). For developers of NLP applications, PLIS offers a ready-to-use lexical knowledge integrator which can interface with many common lexical knowledge resources and constructs lexical inference chains which combine the knowledge in them. A developer who wants to overcome lexical language variability, or to incorporate background knowledge, can utilize PLIS to inject lex6We used the socket.io implementation. 7A bilingual resource holds inference links which connect terms in different languages (e.g. an English-Spanish dictionary can provide the inference link explorer→explorador). 101 Figure 3 : PLIS as a cross-lingual inference system. Left: the process with a single manual bilingual resource. Right: PLIS composes cross-lingual inference chains to increase hypothesis coverage and increase sentence-level inference probability. ical knowledge into any text understanding application. PLIS can be used as a lightweight inference system or as the lexical component of larger, more complex inference systems. Additionally, PLIS provides scores for infer- ence chains and determines the way to combine them in order to recognize sentence-level inference. PLIS comes with two probabilistic lexical inference models which achieved competitive performance levels in the tasks of recognizing textual entailment and passage retrieval for QA. All aspects of PLIS are configurable. The user can easily switch between the built-in lexical resources, inference models and even languages, or extend the system with additional lexical resources and new inference models. Acknowledgments The authors thank Eden Erez for his help with the interactive viewer and Miquel Espl a` Gomis for the bilingual dictionaries. This work was partially supported by the European Community’s 7th Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT) and the Israel Science Foundation grant 880/12. References Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth PASCAL recognizing textual entailment challenge. In Proc. of TAC. Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2010. The sixth PASCAL recognizing textual entailment challenge. In Proc. of TAC. Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In Proc. of EMNLP. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Lecture Notes in Computer Science, volume 3944, pages 177–190. A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the royal statistical society, series [B], 39(1): 1–38. Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts. Nizar Habash and Bonnie Dorr. 2003. A categorial variation database for English. In Proc. of NAACL. Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389. Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proc. of COLOING-ACL. Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa Bentivogli, and Danilo Giampiccolo. 2012. Semeval-2012 task 8: Cross-lingual textual entailment for content synchronization. In Proc. of SemEval. Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Extracting lexical reference rules from Wikipedia. In Proc. of ACL. Eyal Shnarch, Jacob Goldberger, and Ido Dagan. 2011. Towards a probabilistic model for lexical entailment. In Proc. of the TextInfer Workshop. Eyal Shnarch, Ido Dagan, and Jacob Goldberger. 2012. A probabilistic lexical model for ranking textual inferences. In Proc. of *SEM. Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? A quasisynchronous grammar for QA. In Proc. of EMNLP. 102</p><p>6 0.43540281 <a title="202-lsi-6" href="./acl-2013-Exploiting_Qualitative_Information_from_Automatic_Word_Alignment_for_Cross-lingual_NLP_Tasks.html">145 acl-2013-Exploiting Qualitative Information from Automatic Word Alignment for Cross-lingual NLP Tasks</a></p>
<p>7 0.43115979 <a title="202-lsi-7" href="./acl-2013-Sign_Language_Lexical_Recognition_With_Propositional_Dynamic_Logic.html">321 acl-2013-Sign Language Lexical Recognition With Propositional Dynamic Logic</a></p>
<p>8 0.42807499 <a title="202-lsi-8" href="./acl-2013-Understanding_Tables_in_Context_Using_Standard_NLP_Toolkits.html">365 acl-2013-Understanding Tables in Context Using Standard NLP Toolkits</a></p>
<p>9 0.42127943 <a title="202-lsi-9" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>10 0.41955572 <a title="202-lsi-10" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>11 0.4051713 <a title="202-lsi-11" href="./acl-2013-Machine_Translation_Detection_from_Monolingual_Web-Text.html">235 acl-2013-Machine Translation Detection from Monolingual Web-Text</a></p>
<p>12 0.40228838 <a title="202-lsi-12" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>13 0.38242003 <a title="202-lsi-13" href="./acl-2013-Typesetting_for_Improved_Readability_using_Lexical_and_Syntactic_Information.html">364 acl-2013-Typesetting for Improved Readability using Lexical and Syntactic Information</a></p>
<p>14 0.38001713 <a title="202-lsi-14" href="./acl-2013-Unsupervised_joke_generation_from_big_data.html">371 acl-2013-Unsupervised joke generation from big data</a></p>
<p>15 0.3783581 <a title="202-lsi-15" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>16 0.37740374 <a title="202-lsi-16" href="./acl-2013-Identifying_English_and_Hungarian_Light_Verb_Constructions%3A_A_Contrastive_Approach.html">186 acl-2013-Identifying English and Hungarian Light Verb Constructions: A Contrastive Approach</a></p>
<p>17 0.37021336 <a title="202-lsi-17" href="./acl-2013-Aid_is_Out_There%3A_Looking_for_Help_from_Tweets_during_a_Large_Scale_Disaster.html">42 acl-2013-Aid is Out There: Looking for Help from Tweets during a Large Scale Disaster</a></p>
<p>18 0.369344 <a title="202-lsi-18" href="./acl-2013-Post-Retrieval_Clustering_Using_Third-Order_Similarity_Measures.html">281 acl-2013-Post-Retrieval Clustering Using Third-Order Similarity Measures</a></p>
<p>19 0.36836576 <a title="202-lsi-19" href="./acl-2013-Why-Question_Answering_using_Intra-_and_Inter-Sentential_Causal_Relations.html">387 acl-2013-Why-Question Answering using Intra- and Inter-Sentential Causal Relations</a></p>
<p>20 0.36826259 <a title="202-lsi-20" href="./acl-2013-Psycholinguistically_Motivated_Computational_Models_on_the_Organization_and_Processing_of_Morphologically_Complex_Words.html">286 acl-2013-Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.049), (2, 0.014), (6, 0.022), (11, 0.117), (15, 0.016), (24, 0.054), (26, 0.041), (28, 0.021), (35, 0.074), (42, 0.062), (48, 0.043), (64, 0.012), (70, 0.044), (83, 0.203), (88, 0.031), (90, 0.035), (95, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83510053 <a title="202-lda-1" href="./acl-2013-Is_a_204_cm_Man_Tall_or_Small_%3F_Acquisition_of_Numerical_Common_Sense_from_the_Web.html">202 acl-2013-Is a 204 cm Man Tall or Small ? Acquisition of Numerical Common Sense from the Web</a></p>
<p>Author: Katsuma Narisawa ; Yotaro Watanabe ; Junta Mizuno ; Naoaki Okazaki ; Kentaro Inui</p><p>Abstract: This paper presents novel methods for modeling numerical common sense: the ability to infer whether a given number (e.g., three billion) is large, small, or normal for a given context (e.g., number of people facing a water shortage). We first discuss the necessity of numerical common sense in solving textual entailment problems. We explore two approaches for acquiring numerical common sense. Both approaches start with extracting numerical expressions and their context from the Web. One approach estimates the distribution ofnumbers co-occurring within a context and examines whether a given value is large, small, or normal, based on the distri- bution. Another approach utilizes textual patterns with which speakers explicitly expresses their judgment about the value of a numerical expression. Experimental results demonstrate the effectiveness of both approaches.</p><p>2 0.79827917 <a title="202-lda-2" href="./acl-2013-Conditional_Random_Fields_for_Responsive_Surface_Realisation_using_Global_Features.html">90 acl-2013-Conditional Random Fields for Responsive Surface Realisation using Global Features</a></p>
<p>Author: Nina Dethlefs ; Helen Hastie ; Heriberto Cuayahuitl ; Oliver Lemon</p><p>Abstract: Surface realisers in spoken dialogue systems need to be more responsive than conventional surface realisers. They need to be sensitive to the utterance context as well as robust to partial or changing generator inputs. We formulate surface realisation as a sequence labelling task and combine the use of conditional random fields (CRFs) with semantic trees. Due to their extended notion of context, CRFs are able to take the global utterance context into account and are less constrained by local features than other realisers. This leads to more natural and less repetitive surface realisation. It also allows generation from partial and modified inputs and is therefore applicable to incremental surface realisation. Results from a human rating study confirm that users are sensitive to this extended notion of context and assign ratings that are significantly higher (up to 14%) than those for taking only local context into account.</p><p>3 0.73083085 <a title="202-lda-3" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>Author: Olivier Ferret</p><p>Abstract: Distributional thesauri are now widely used in a large number of Natural Language Processing tasks. However, they are far from containing only interesting semantic relations. As a consequence, improving such thesaurus is an important issue that is mainly tackled indirectly through the improvement of semantic similarity measures. In this article, we propose a more direct approach focusing on the identification of the neighbors of a thesaurus entry that are not semantically linked to this entry. This identification relies on a discriminative classifier trained from unsupervised selected examples for building a distributional model of the entry in texts. Its bad neighbors are found by applying this classifier to a representative set of occurrences of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies.</p><p>4 0.70042753 <a title="202-lda-4" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>Author: Bishan Yang ; Claire Cardie</p><p>Abstract: This paper addresses the task of finegrained opinion extraction the identification of opinion-related entities: the opinion expressions, the opinion holders, and the targets of the opinions, and the relations between opinion expressions and their targets and holders. Most existing approaches tackle the extraction of opinion entities and opinion relations in a pipelined manner, where the interdependencies among different extraction stages are not captured. We propose a joint inference model that leverages knowledge from predictors that optimize subtasks – of opinion extraction, and seeks a globally optimal solution. Experimental results demonstrate that our joint inference approach significantly outperforms traditional pipeline methods and baselines that tackle subtasks in isolation for the problem of opinion extraction.</p><p>5 0.67474163 <a title="202-lda-5" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>Author: Jinho D. Choi ; Andrew McCallum</p><p>Abstract: We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</p><p>6 0.66750038 <a title="202-lda-6" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>7 0.66663152 <a title="202-lda-7" href="./acl-2013-Modeling_Human_Inference_Process_for_Textual_Entailment_Recognition.html">245 acl-2013-Modeling Human Inference Process for Textual Entailment Recognition</a></p>
<p>8 0.66471058 <a title="202-lda-8" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>9 0.66441077 <a title="202-lda-9" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>10 0.66247571 <a title="202-lda-10" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<p>11 0.6607489 <a title="202-lda-11" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>12 0.659473 <a title="202-lda-12" href="./acl-2013-Fast_and_Adaptive_Online_Training_of_Feature-Rich_Translation_Models.html">156 acl-2013-Fast and Adaptive Online Training of Feature-Rich Translation Models</a></p>
<p>13 0.6594637 <a title="202-lda-13" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>14 0.65805697 <a title="202-lda-14" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>15 0.65352368 <a title="202-lda-15" href="./acl-2013-Discriminative_Learning_with_Natural_Annotations%3A_Word_Segmentation_as_a_Case_Study.html">123 acl-2013-Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study</a></p>
<p>16 0.6534794 <a title="202-lda-16" href="./acl-2013-Extracting_bilingual_terminologies_from_comparable_corpora.html">154 acl-2013-Extracting bilingual terminologies from comparable corpora</a></p>
<p>17 0.65324557 <a title="202-lda-17" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>18 0.65248883 <a title="202-lda-18" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>19 0.65181404 <a title="202-lda-19" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>20 0.65181333 <a title="202-lda-20" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
