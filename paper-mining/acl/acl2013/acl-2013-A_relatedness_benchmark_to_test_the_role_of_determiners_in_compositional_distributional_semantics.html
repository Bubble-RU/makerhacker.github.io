<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-32" href="#">acl2013-32</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</h1>
<br/><p>Source: <a title="acl-2013-32-pdf" href="http://aclweb.org/anthology//P/P13/P13-2010.pdf">pdf</a></p><p>Author: Raffaella Bernardi ; Georgiana Dinu ; Marco Marelli ; Marco Baroni</p><p>Abstract: Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</p><p>Reference: <a title="acl-2013-32-reference" href="../acl2013_reference/acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A relatedness benchmark to test the role of determiners in compositional distributional semantics Raffaella Bernardi and Georgiana Dinu and Marco Marelli and Marco Baroni Center for Mind/Brain Sciences (University of Trento, Italy) first . [sent-1, score-0.916]
</p><p>2 it  Abstract Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. [sent-3, score-0.283]
</p><p>3 We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). [sent-4, score-0.819]
</p><p>4 We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set. [sent-5, score-0.242]
</p><p>5 1 Introduction Distributional semantics models (DSMs) approximate meaning with vectors that record the dis-  tributional occurrence patterns of words in corpora. [sent-6, score-0.238]
</p><p>6 DSMs have been effectively applied to increasingly more sophisticated semantic tasks in linguistics, artificial intelligence and cognitive science, and they have been recently extended to capture the meaning of phrases and sentences via compositional mechanisms. [sent-7, score-0.316]
</p><p>7 However, scaling up to larger constituents poses the issue of how to handle grammatical words, such as determiners, prepositions, or auxiliaries, that lack rich conceptual content, and operate instead as the logical “glue” holding sentences together. [sent-8, score-0.182]
</p><p>8 In typical DSMs, grammatical words are treated as “stop words” to be discarded, or at best used as context features in the representation of content words. [sent-9, score-0.167]
</p><p>9 Similarly, current compositional DSMs (cDSMs) focus almost entirely on phrases made of two or more content words (e. [sent-10, score-0.297]
</p><p>10 As these examples suggest, however, as soon as we set our sight on modeling phrases and sentences, grammatical words are hard to avoid. [sent-13, score-0.193]
</p><p>11 Stripping off grammatical words has more serious consequences than making you sound like the Lord of the Jungle. [sent-14, score-0.117]
</p><p>12 (2013), that the logical framework of language should be left to other devices than distributional semantics, and the latter should be limited to similarity scoring, still ignoring grammatical elements is going to dramatically distort the very similarity scores (c)DSMs should provide. [sent-18, score-0.352]
</p><p>13 We focus here on how cDSMs handle determiners and the phrases they form with nouns (deter-  miner phrases, or DPs). [sent-20, score-0.499]
</p><p>14 Moreover, determiner-noun phrases are, in superficial syntactic terms, similar to the adjective-noun phrases that have already been extensively studied from a cDSM perspective by Baroni and Zampar1Some linguists refer to what we call DPs as noun phrases or NPs. [sent-22, score-0.304]
</p><p>15 We say DPs simply to emphasize our focus on determiners. [sent-23, score-0.027]
</p><p>16 Thus, we can straightforwardly extend the methods already proposed for adjective-noun phrases to DPs. [sent-27, score-0.076]
</p><p>17 We introduce a new task, a similarity-based challenge, where we consider nouns that are strongly conceptually related to certain DPs and test whether cDSMs can pick the most appropriate related DP (e. [sent-28, score-0.125]
</p><p>18 , monarchy is more related to one ruler than many rulers). [sent-30, score-0.028]
</p><p>19 2 We make our new dataset publicly available, and we hope that it will stimulate further work on the distributional semantics of grammatical elements. [sent-31, score-0.39]
</p><p>20 3 2  Composition models  Interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010), who proposed three simple but effective composition models. [sent-32, score-0.343]
</p><p>21 In these models, the composed vectors are obtained through componentwise operations on the constituent vectors. [sent-33, score-0.171]
</p><p>22 Given input vectors u and v, the multiplicative model (mult) returns a composed vector p with: pi = uivi. [sent-34, score-0.202]
</p><p>23 In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: p = αu + βv, where α and β are two  scalars. [sent-35, score-0.118]
</p><p>24 Finally, in the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. [sent-36, score-0.194]
</p><p>25 Following this, the parallel vector is dilated by a factor λ before re-combining. [sent-37, score-0.059]
</p><p>26 In this approach, the two vectors to be added are pre-multiplied by weight matrices estimated from corpus-extracted examples: p = Au + Bv. [sent-42, score-0.19]
</p><p>27 (2010) take inspiration from formal semantics to characterize composition in terms of function application. [sent-44, score-0.217]
</p><p>28 The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. [sent-45, score-0.181]
</p><p>29 Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a −  +  2Baroni et al. [sent-46, score-0.067]
</p><p>30 (2012), like us, study determiner phrases with distributional methods, but they do not model them compositionally. [sent-47, score-0.474]
</p><p>31 functor (such as the adjective) is represented by a matrix U to be multiplied with the argument vector v (e. [sent-52, score-0.084]
</p><p>32 Adjective matrices are estimated from corpus-extracted examples of noun vectors and corresponding output adjective-noun phrase vectors, similarly to Guevara’s approach. [sent-55, score-0.266]
</p><p>33 4 3  The noun-DP relatedness benchmark  Paraphrasing a single word with a phrase is a natural task for models of compositionality (Turney, 2012; Zanzotto et al. [sent-56, score-0.202]
</p><p>34 , 2010) and determiners sometimes play a crucial role in defining the meaning of a noun. [sent-57, score-0.431]
</p><p>35 For example a trilogy is composed of three works, an assemblage includes several things and an orchestra is made of many musicians. [sent-58, score-0.048]
</p><p>36 In other words, we can set up an experiment in which having an effective representation of the determiner is crucial in order to obtain the correct result. [sent-60, score-0.259]
</p><p>37 Using regular expressions over WordNet glosses (Fellbaum, 1998) and complementing them with definitions from various online dictionaries, we constructed a list of more than 200 nouns that are strongly conceptually related to a specific DP. [sent-61, score-0.124]
</p><p>38 After the materials were checked by all authors, two native speakers took the multiple-choice test. [sent-64, score-0.029]
</p><p>39 The final set,  4Other approaches to composition in DSMs have been recently proposed by Socher et al. [sent-66, score-0.144]
</p><p>40 We leave their empirical evaluation on DPs to further work, in the first case because it is not trivial to adapt their complex architecture to our setting; in the other because it is not clear how Turney would extend his approach to represent DPs. [sent-68, score-0.033]
</p><p>41 4  Setup  Our semantic space provides distributional representations  of determiners,  nouns and DPs. [sent-71, score-0.302]
</p><p>42 We  considered a set of 50 determiners that include all those in our benchmark fying determiners  and range from quanti-  (every, some. [sent-72, score-0.833]
</p><p>43 ) and low nu-  merals (one to four), to multi-word units analyzed as single determiners  in the literature, such as a  few, all that, too much. [sent-75, score-0.359]
</p><p>44 We picked the 20K most  frequent nouns in our source corpus considering singular and plural forms as separate words, since number clearly plays an important role in DP semantics. [sent-76, score-0.064]
</p><p>45 Finally, for each of the target determiners we added to the space the 2K most frequent DPs containing that determiner and a target noun. [sent-77, score-0.664]
</p><p>46 We use a bag-of-words approach, counting co-occurrence with all context words in the same sentence with a target item. [sent-80, score-0.038]
</p><p>47 We tuned a number of parameters on the independent MEN word-relatedness benchmark (Bruni et al. [sent-81, score-0.087]
</p><p>48 This led us to pick the top 20K most frequent content word lemmas as context items, Pointwise Mutual Information as weighting scheme, and dimensionality reduction by Non-negative Matrix Factorization. [sent-83, score-0.077]
</p><p>49 Except for the parameter-free mult method, parameters of the composition methods are estimated by minimizing the average Euclidean distance between the model-generated and corpus-  consider. [sent-84, score-0.253]
</p><p>50 6  extracted vectors of the 20K DPs we For the lexfunc model, we assume that the determiner is the functor and the noun is the argument, 5wacky . [sent-85, score-0.612]
</p><p>51 uk 6All vectors are normalized to unit length before composition. [sent-92, score-0.123]
</p><p>52 Note that the objective function used in estimation minimizes the distance between model-generated and corpusextracted vectors. [sent-93, score-0.028]
</p><p>53 6  Table 2: Percentage accuracy of composition methods on the relatedness benchmark and estimate separate matrices representing each determiner using the 2K DPs in the semantic space that contain that determiner. [sent-104, score-0.611]
</p><p>54 Similarly to the classic TOEFL synonym detection challenge (Landauer and Dumais, 1997), our models tackle the relatedness task by measuring cosines between each target noun and the candidate answers and returning the item with the highest cosine. [sent-106, score-0.285]
</p><p>55 5  Results  Table 2 reports the accuracy results (mean ranks of correct answers confirm the same trend). [sent-107, score-0.033]
</p><p>56 All models except mult and determiner outperform the trivial random guessing baseline, although they are all well below the 100% accuracy of the humans who took our test. [sent-108, score-0.4]
</p><p>57 For the mult method we observe a very strong bias for choosing a single word as answer (>60% of the times), which in the test set is always incorrect. [sent-109, score-0.109]
</p><p>58 This leads to its accuracy being below the chance level. [sent-110, score-0.073]
</p><p>59 The determiner-only baseline (using the vector of the component determiner as surrogate for the DP) fails because D vectors tend to be far from N vectors, thus the N foil is often preferred to the correct response (that is represented, for this baseline, by its D). [sent-112, score-0.745]
</p><p>60 In the noun-only baseline (use the vector of the component noun as surrogate for the DP), 55  the correct response is identical to the same-N and N foils, thus forcing a random choice between these. [sent-113, score-0.147]
</p><p>61 The observed DP vectors extracted directly from the corpus compete with the top compositional methods, but do not surpass them. [sent-115, score-0.294]
</p><p>62 7 The lexfunc method is the best compositional model, indicating that its added flexibility in modeling composition pays off empirically. [sent-116, score-0.446]
</p><p>63 The fulladd model is not as good, but also performs well. [sent-117, score-0.105]
</p><p>64 The wadd and especially dilation models perform relatively well, but they are penalized by the fact that they assign more weight to the noun vectors,  making the right answer dangerously similar to the same-N and N foils. [sent-118, score-0.294]
</p><p>65 Focusing on those determiners appearing in at least 4 correct answers, they range from those where lexfunc performance was very significantly above chance (p<0. [sent-120, score-0.563]
</p><p>66 001 of equal or higher chance performance): too few, all, four, too much, less, several; to those on which performance was still significant but less impressively so (0. [sent-121, score-0.073]
</p><p>67 05): several, no, various, most, two, too many, many, one; to those where performance was not significantly better than chance at the 0. [sent-123, score-0.073]
</p><p>68 Given that, on the one hand, performance is not constant across determiners, and on the other no obvious groupings can account for their performance difference (compare the excellent lexfunc performance on four to the lousy one on three! [sent-125, score-0.131]
</p><p>69 ), future research should explore the contextual properties of specific determiners that make them more or less amenable to be captured  by compositional DSMs. [sent-126, score-0.53]
</p><p>70 6  Conclusion  DSMs, even when applied to phrases, are typically seen as models of content word meaning. [sent-127, score-0.05]
</p><p>71 However, to scale up compositionally beyond the simplest constructions, cDSMs must deal with grammatical terms such as determiners. [sent-128, score-0.117]
</p><p>72 The most important take-home message is that distributional representations are rich enough to encode information about determiners, achieving performance well above chance on the new benchmark. [sent-131, score-0.284]
</p><p>73 Theoretical considerations would lead one to  expect a “functional” approach to determiner representations along the lines of Baroni and Zamparelli (2010) and Coecke et al. [sent-132, score-0.271]
</p><p>74 (2010) to outperform those approaches that combine vectors separately representing determiners and nouns. [sent-133, score-0.482]
</p><p>75 This prediction was largely borne out in the results, although the additive models, and particularly fulladd, were competitive rivals. [sent-134, score-0.065]
</p><p>76 We attempted to capture the distributional semantics of DPs using a fairly standard, “vanilla” semantic space characterized by latent dimensions that summarize patterns of co-occurrence with content word contexts. [sent-135, score-0.35]
</p><p>77 Considering the sort of semantic space we used (which we took to be a reasonable starting point because of its effectiveness in a standard lexical task), it is actually surpris-  ing that we obtained the significant results we obtained. [sent-137, score-0.056]
</p><p>78 Thus, a top priority in future work is to explore different contextual features, such as adverbs and grammatical terms, that might carry information that is more directly relevant to the semantics of determiners. [sent-138, score-0.19]
</p><p>79 Another important line of research pertains to improving composition methods: Although the best model, at 40% accuracy, is well above chance, we are still far from the 100% performance of humans. [sent-139, score-0.144]
</p><p>80 Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. [sent-147, score-0.068]
</p><p>81 Mathematical foundations for a compositional distributional model of meaning. [sent-159, score-0.34]
</p><p>82 A formal approach to linking logical form and vectorspace lexical semantics. [sent-167, score-0.038]
</p><p>83 Experimental support for a categorical compositional distributional model of meaning. [sent-176, score-0.34]
</p><p>84 A regression model of adjective-noun compositionality in distributional semantics. [sent-180, score-0.227]
</p><p>85 A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. [sent-184, score-0.027]
</p><p>86 Domain and function: A dualspace model of semantic relations and compositions. [sent-204, score-0.027]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('determiners', 0.359), ('dps', 0.342), ('foil', 0.322), ('determiner', 0.229), ('dsms', 0.188), ('dp', 0.183), ('cdsms', 0.171), ('compositional', 0.171), ('distributional', 0.169), ('composition', 0.144), ('lexfunc', 0.131), ('vectors', 0.123), ('grammatical', 0.117), ('baroni', 0.114), ('wives', 0.114), ('mult', 0.109), ('dilation', 0.105), ('fulladd', 0.105), ('foils', 0.097), ('goods', 0.09), ('benchmark', 0.087), ('rats', 0.085), ('wadd', 0.085), ('phrases', 0.076), ('noun', 0.076), ('chance', 0.073), ('semantics', 0.073), ('mitchell', 0.072), ('coecke', 0.071), ('guevara', 0.071), ('zanzotto', 0.071), ('matrices', 0.067), ('grefenstette', 0.065), ('sadrzadeh', 0.065), ('cdsm', 0.064), ('nouns', 0.064), ('marco', 0.06), ('compositionality', 0.058), ('home', 0.058), ('relatedness', 0.057), ('bernardi', 0.057), ('garrette', 0.057), ('raffaella', 0.057), ('socher', 0.053), ('classic', 0.053), ('functor', 0.053), ('content', 0.05), ('bruni', 0.049), ('composed', 0.048), ('zamparelli', 0.047), ('jeff', 0.045), ('mehrnoosh', 0.045), ('unitn', 0.043), ('representations', 0.042), ('meaning', 0.042), ('constructions', 0.041), ('adjective', 0.041), ('surrogate', 0.04), ('turney', 0.039), ('additive', 0.039), ('target', 0.038), ('logical', 0.038), ('kill', 0.036), ('mirella', 0.035), ('conceptually', 0.034), ('answers', 0.033), ('lapata', 0.033), ('trivial', 0.033), ('publicly', 0.031), ('vector', 0.031), ('characterized', 0.031), ('landauer', 0.031), ('crucial', 0.03), ('took', 0.029), ('cosines', 0.028), ('aor', 0.028), ('bunt', 0.028), ('corpusextracted', 0.028), ('dangerously', 0.028), ('dilated', 0.028), ('distort', 0.028), ('fever', 0.028), ('fying', 0.028), ('homes', 0.028), ('infection', 0.028), ('khanh', 0.028), ('monarchy', 0.028), ('skeletal', 0.028), ('skyrocketed', 0.028), ('viu', 0.028), ('paraphrasing', 0.028), ('conceptual', 0.027), ('semantic', 0.027), ('say', 0.027), ('pick', 0.027), ('cimec', 0.026), ('dsm', 0.026), ('borne', 0.026), ('complementing', 0.026), ('elia', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="32-tfidf-1" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>Author: Raffaella Bernardi ; Georgiana Dinu ; Marco Marelli ; Marco Baroni</p><p>Abstract: Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</p><p>2 0.35565442 <a title="32-tfidf-2" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>Author: Angeliki Lazaridou ; Marco Marelli ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</p><p>3 0.2158868 <a title="32-tfidf-3" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>Author: Georgiana Dinu ; Nghia The Pham ; Marco Baroni</p><p>Abstract: We introduce DISSECT, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. The toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure.</p><p>4 0.19107214 <a title="32-tfidf-4" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>Author: Kartik Goyal ; Sujay Kumar Jauhar ; Huiying Li ; Mrinmaya Sachan ; Shashank Srivastava ; Eduard Hovy</p><p>Abstract: In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics pre- viously employed in the literature.</p><p>5 0.1563077 <a title="32-tfidf-5" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>Author: Aurelie Herbelot ; Mohan Ganesalingam</p><p>Abstract: Some words are more contentful than others: for instance, make is intuitively more general than produce and fifteen is more ‘precise’ than a group. In this paper, we propose to measure the ‘semantic content’ of lexical items, as modelled by distributional representations. We investigate the hypothesis that semantic content can be computed using the KullbackLeibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL diver- gence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions.</p><p>6 0.15249784 <a title="32-tfidf-6" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>7 0.1045532 <a title="32-tfidf-7" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>8 0.10073154 <a title="32-tfidf-8" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>9 0.095628418 <a title="32-tfidf-9" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>10 0.085186318 <a title="32-tfidf-10" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>11 0.082448587 <a title="32-tfidf-11" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>12 0.075096779 <a title="32-tfidf-12" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>13 0.069537528 <a title="32-tfidf-13" href="./acl-2013-Building_and_Evaluating_a_Distributional_Memory_for_Croatian.html">76 acl-2013-Building and Evaluating a Distributional Memory for Croatian</a></p>
<p>14 0.068770058 <a title="32-tfidf-14" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>15 0.068516344 <a title="32-tfidf-15" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>16 0.068181612 <a title="32-tfidf-16" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>17 0.067775242 <a title="32-tfidf-17" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>18 0.067710146 <a title="32-tfidf-18" href="./acl-2013-A_Two_Level_Model_for_Context_Sensitive_Inference_Rules.html">27 acl-2013-A Two Level Model for Context Sensitive Inference Rules</a></p>
<p>19 0.065537624 <a title="32-tfidf-19" href="./acl-2013-Using_Lexical_Expansion_to_Learn_Inference_Rules_from_Sparse_Data.html">376 acl-2013-Using Lexical Expansion to Learn Inference Rules from Sparse Data</a></p>
<p>20 0.065326862 <a title="32-tfidf-20" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.161), (1, 0.037), (2, 0.023), (3, -0.138), (4, -0.109), (5, -0.087), (6, -0.015), (7, 0.023), (8, -0.028), (9, 0.084), (10, -0.088), (11, -0.008), (12, 0.229), (13, -0.13), (14, -0.053), (15, 0.109), (16, -0.109), (17, -0.19), (18, -0.029), (19, -0.02), (20, -0.021), (21, 0.034), (22, 0.034), (23, -0.036), (24, -0.088), (25, -0.006), (26, -0.008), (27, 0.079), (28, -0.022), (29, 0.079), (30, 0.011), (31, 0.103), (32, 0.065), (33, -0.035), (34, 0.03), (35, 0.08), (36, 0.079), (37, 0.068), (38, 0.042), (39, 0.078), (40, 0.101), (41, -0.192), (42, 0.047), (43, 0.017), (44, 0.029), (45, 0.094), (46, -0.039), (47, -0.059), (48, 0.042), (49, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93811834 <a title="32-lsi-1" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>Author: Raffaella Bernardi ; Georgiana Dinu ; Marco Marelli ; Marco Baroni</p><p>Abstract: Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</p><p>2 0.91555959 <a title="32-lsi-2" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>Author: Georgiana Dinu ; Nghia The Pham ; Marco Baroni</p><p>Abstract: We introduce DISSECT, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. The toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure.</p><p>3 0.83968931 <a title="32-lsi-3" href="./acl-2013-Compositional-ly_Derived_Representations_of_Morphologically_Complex_Words_in_Distributional_Semantics.html">87 acl-2013-Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics</a></p>
<p>Author: Angeliki Lazaridou ; Marco Marelli ; Roberto Zamparelli ; Marco Baroni</p><p>Abstract: Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</p><p>4 0.70894641 <a title="32-lsi-4" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>Author: Aurelie Herbelot ; Mohan Ganesalingam</p><p>Abstract: Some words are more contentful than others: for instance, make is intuitively more general than produce and fifteen is more ‘precise’ than a group. In this paper, we propose to measure the ‘semantic content’ of lexical items, as modelled by distributional representations. We investigate the hypothesis that semantic content can be computed using the KullbackLeibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL diver- gence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions.</p><p>5 0.68350458 <a title="32-lsi-5" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<p>Author: Kartik Goyal ; Sujay Kumar Jauhar ; Huiying Li ; Mrinmaya Sachan ; Shashank Srivastava ; Eduard Hovy</p><p>Abstract: In this paper we present a novel approach to modelling distributional semantics that represents meaning as distributions over relations in syntactic neighborhoods. We argue that our model approximates meaning in compositional configurations more effectively than standard distributional vectors or bag-of-words models. We test our hypothesis on the problem of judging event coreferentiality, which involves compositional interactions in the predicate-argument structure of sentences, and demonstrate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics pre- viously employed in the literature.</p><p>6 0.57786602 <a title="32-lsi-6" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>7 0.55892253 <a title="32-lsi-7" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>8 0.53397053 <a title="32-lsi-8" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>9 0.51036668 <a title="32-lsi-9" href="./acl-2013-Building_and_Evaluating_a_Distributional_Memory_for_Croatian.html">76 acl-2013-Building and Evaluating a Distributional Memory for Croatian</a></p>
<p>10 0.49826717 <a title="32-lsi-10" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>11 0.47888514 <a title="32-lsi-11" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>12 0.44249007 <a title="32-lsi-12" href="./acl-2013-Semantic_Parsing_with_Combinatory_Categorial_Grammars.html">313 acl-2013-Semantic Parsing with Combinatory Categorial Grammars</a></p>
<p>13 0.41878152 <a title="32-lsi-13" href="./acl-2013-Psycholinguistically_Motivated_Computational_Models_on_the_Organization_and_Processing_of_Morphologically_Complex_Words.html">286 acl-2013-Psycholinguistically Motivated Computational Models on the Organization and Processing of Morphologically Complex Words</a></p>
<p>14 0.41646689 <a title="32-lsi-14" href="./acl-2013-Robust_Automated_Natural_Language_Processing_with_Multiword_Expressions_and_Collocations.html">302 acl-2013-Robust Automated Natural Language Processing with Multiword Expressions and Collocations</a></p>
<p>15 0.39349595 <a title="32-lsi-15" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>16 0.37539119 <a title="32-lsi-16" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>17 0.35340315 <a title="32-lsi-17" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>18 0.34937975 <a title="32-lsi-18" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>19 0.34432271 <a title="32-lsi-19" href="./acl-2013-Leveraging_Domain-Independent_Information_in_Semantic_Parsing.html">228 acl-2013-Leveraging Domain-Independent Information in Semantic Parsing</a></p>
<p>20 0.34283254 <a title="32-lsi-20" href="./acl-2013-Variable_Bit_Quantisation_for_LSH.html">381 acl-2013-Variable Bit Quantisation for LSH</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.043), (11, 0.041), (14, 0.011), (15, 0.012), (24, 0.024), (26, 0.041), (35, 0.513), (42, 0.028), (48, 0.104), (70, 0.039), (88, 0.015), (95, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97845376 <a title="32-lda-1" href="./acl-2013-A_relatedness_benchmark_to_test_the_role_of_determiners_in_compositional_distributional_semantics.html">32 acl-2013-A relatedness benchmark to test the role of determiners in compositional distributional semantics</a></p>
<p>Author: Raffaella Bernardi ; Georgiana Dinu ; Marco Marelli ; Marco Baroni</p><p>Abstract: Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set.</p><p>2 0.97687435 <a title="32-lda-2" href="./acl-2013-Are_Semantically_Coherent_Topic_Models_Useful_for_Ad_Hoc_Information_Retrieval%3F.html">55 acl-2013-Are Semantically Coherent Topic Models Useful for Ad Hoc Information Retrieval?</a></p>
<p>Author: Romain Deveaud ; Eric SanJuan ; Patrice Bellot</p><p>Abstract: The current topic modeling approaches for Information Retrieval do not allow to explicitly model query-oriented latent topics. More, the semantic coherence of the topics has never been considered in this field. We propose a model-based feedback approach that learns Latent Dirichlet Allocation topic models on the top-ranked pseudo-relevant feedback, and we measure the semantic coherence of those topics. We perform a first experimental evaluation using two major TREC test collections. Results show that retrieval perfor- mances tend to be better when using topics with higher semantic coherence.</p><p>3 0.97118902 <a title="32-lda-3" href="./acl-2013-Patient_Experience_in_Online_Support_Forums%3A_Modeling_Interpersonal_Interactions_and_Medication_Use.html">278 acl-2013-Patient Experience in Online Support Forums: Modeling Interpersonal Interactions and Medication Use</a></p>
<p>Author: Annie Chen</p><p>Abstract: Though there has been substantial research concerning the extraction of information from clinical notes, to date there has been less work concerning the extraction of useful information from patient-generated content. Using a dataset comprised of online support group discussion content, this paper investigates two dimensions that may be important in the extraction of patient-generated experiences from text; significant individuals/groups and medication use. With regard to the former, the paper describes an approach involving the pairing of important figures (e.g. family, husbands, doctors, etc.) and affect, and suggests possible applications of such techniques to research concerning online social support, as well as integration into search interfaces for patients. Additionally, the paper demonstrates the extraction of side effects and sentiment at different phases in patient medication use, e.g. adoption, current use, discontinuation and switching, and demonstrates the utility of such an application for drug safety monitoring in online discussion forums. 1</p><p>4 0.97102666 <a title="32-lda-4" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>Author: Ndapandula Nakashole ; Tomasz Tylenda ; Gerhard Weikum</p><p>Abstract: Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and semantically typing newly emerging out-ofKB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE. Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowdsourced user studies, show our method performing significantly better than prior work.</p><p>5 0.97029132 <a title="32-lda-5" href="./acl-2013-Building_and_Evaluating_a_Distributional_Memory_for_Croatian.html">76 acl-2013-Building and Evaluating a Distributional Memory for Croatian</a></p>
<p>Author: Jan Snajder ; Sebastian Pado ; Zeljko Agic</p><p>Abstract: We report on the first structured distributional semantic model for Croatian, DM.HR. It is constructed after the model of the English Distributional Memory (Baroni and Lenci, 2010), from a dependencyparsed Croatian web corpus, and covers about 2M lemmas. We give details on the linguistic processing and the design principles. An evaluation shows state-of-theart performance on a semantic similarity task with particularly good performance on nouns. The resource is freely available.</p><p>6 0.95628911 <a title="32-lda-6" href="./acl-2013-Semantic_Neighborhoods_as_Hypergraphs.html">311 acl-2013-Semantic Neighborhoods as Hypergraphs</a></p>
<p>7 0.95443404 <a title="32-lda-7" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>8 0.94815153 <a title="32-lda-8" href="./acl-2013-Discriminative_Approach_to_Fill-in-the-Blank_Quiz_Generation_for_Language_Learners.html">122 acl-2013-Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners</a></p>
<p>9 0.81979841 <a title="32-lda-9" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>10 0.8128916 <a title="32-lda-10" href="./acl-2013-Derivational_Smoothing_for_Syntactic_Distributional_Semantics.html">113 acl-2013-Derivational Smoothing for Syntactic Distributional Semantics</a></p>
<p>11 0.80933565 <a title="32-lda-11" href="./acl-2013-Automatic_Coupling_of_Answer_Extraction_and_Information_Retrieval.html">60 acl-2013-Automatic Coupling of Answer Extraction and Information Retrieval</a></p>
<p>12 0.78550017 <a title="32-lda-12" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>13 0.78089488 <a title="32-lda-13" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>14 0.77106476 <a title="32-lda-14" href="./acl-2013-DISSECT_-_DIStributional_SEmantics_Composition_Toolkit.html">103 acl-2013-DISSECT - DIStributional SEmantics Composition Toolkit</a></p>
<p>15 0.76020259 <a title="32-lda-15" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>16 0.76014882 <a title="32-lda-16" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>17 0.74954152 <a title="32-lda-17" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>18 0.7463156 <a title="32-lda-18" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>19 0.74491477 <a title="32-lda-19" href="./acl-2013-Linggle%3A_a_Web-scale_Linguistic_Search_Engine_for_Words_in_Context.html">231 acl-2013-Linggle: a Web-scale Linguistic Search Engine for Words in Context</a></p>
<p>20 0.74202347 <a title="32-lda-20" href="./acl-2013-A_Structured_Distributional_Semantic_Model_for_Event_Co-reference.html">22 acl-2013-A Structured Distributional Semantic Model for Event Co-reference</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
