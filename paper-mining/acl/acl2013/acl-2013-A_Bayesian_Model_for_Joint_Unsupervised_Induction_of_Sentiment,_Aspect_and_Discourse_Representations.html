<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-2" href="#">acl2013-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</h1>
<br/><p>Source: <a title="acl-2013-2-pdf" href="http://aclweb.org/anthology//P/P13/P13-1160.pdf">pdf</a></p><p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>Reference: <a title="acl-2013-2-reference" href="../acl2013_reference/acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. [sent-7, score-2.417]
</p><p>2 We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. [sent-8, score-2.394]
</p><p>3 The quantitative analysis that we conducted indicated that the integration of a discourse model increased the  prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure. [sent-9, score-1.261]
</p><p>4 In the review domain, this is done in aspectbased sentiment analysis which aims at identifying text fragments in which opinions are expressed about ratable aspects of products, such as ‘room quality’ or ‘service quality’ . [sent-12, score-0.595]
</p><p>5 However, these local lexical representations by themselves are often not sufficient to infer a sentiment or aspect for a fragment of text. [sent-17, score-0.74]
</p><p>6 The same observations can be made about transitions between aspects: changes in aspect are often clearly marked by discourse connectives. [sent-28, score-0.933]
</p><p>7 Importantly, some of these cues are not discourse connectives in the strict linguistic sense and are specific to the review domain (e. [sent-29, score-0.803]
</p><p>8 Ac s2s0o1ci3a Atiosnso fcoirat Cioonm foprut Caotimonpaulta Lti nognuails Lti cnsg,u piasgteics 1630–1639, count for these discourse phenomena and cannot rely solely on local lexical information. [sent-36, score-0.588]
</p><p>9 Consequently, there has recently been an increased interest in models that leverage content and discourse structure in sentiment analysis tasks. [sent-38, score-1.072]
</p><p>10 However, discourse-level information is typically incorporated in a pipeline architecture, either in the form of sentiment polarity shifters (Polanyi and Zaenen, 2006; Naka-  gawa et al. [sent-39, score-0.752]
</p><p>11 , 2010) that operate on the lexical level or by using discourse relations (Taboada et al. [sent-40, score-0.676]
</p><p>12 , 2011) that comply with discourse theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988). [sent-42, score-0.564]
</p><p>13 First, they require additional resources, such as lists of polarity shifters or discourse connectives which signal specific relations. [sent-44, score-0.888]
</p><p>14 Second, relying on a generic discourse analysis step that is carried out before sentiment analysis may introduce additional noise and lead to error propagation. [sent-46, score-1.072]
</p><p>15 Furthermore, these techniques will not necessarily be able to induce discourse relations informative for the sentiment analysis domain (Voll and Taboada, 2007). [sent-47, score-1.216]
</p><p>16 An alternative approach is to define a taskspecific scheme of discourse relations (Somasundaran et al. [sent-48, score-0.64]
</p><p>17 This previous work showed that task-specific discourse relations are helpful in predicting sentiment, however, in doing so they relied on gold-standard discourse annotation at test  time rather than predicting it automatically or inducing it jointly with sentiment polarity. [sent-50, score-1.874]
</p><p>18 We take a different approach and induce discourse and sentiment information jointly in an unsupervised (or weakly supervised) manner. [sent-51, score-1.184]
</p><p>19 This has the advantage of not having to pre-specify a mapping from discourse cues to discourse relations; our model induces this automatically, which makes it portable to new domains and languages. [sent-52, score-1.276]
</p><p>20 Joint induction of discourse and sentiment structure also has the added benefit that the model is able to learn exactly those aspects of discourse structure that are relevant for sentiment analysis. [sent-53, score-2.252]
</p><p>21 We start with a relatively standard joint model of sentiment and topic, which can be regarded as a cross-breed between the JST model (Lin and He, 2009) and the ASUM model (Jo and Oh, 2011), changeably as well as sentiment levels and opinion polarity. [sent-54, score-1.187]
</p><p>22 not aspect-specific) opinion polarity labels to induce topics and sentiment on the subsentential level. [sent-58, score-0.839]
</p><p>23 In order to test our hypothesis  that discourse information is beneficial, we add a discourse modeling component. [sent-59, score-1.157]
</p><p>24 Note that in modeling discourse we do not exploit any kind of supervision. [sent-60, score-0.593]
</p><p>25 To the best of our knowledge, unsupervised joint induction of discourse structure, sentiment and topic information has not been considered before, particularly not in the context of the aspect-based sentiment analysis task. [sent-62, score-1.743]
</p><p>26 Importantly, our method for discourse modeling is a general method which can be integrated in virtually any LDA-style model of aspect and sentiment. [sent-63, score-0.854]
</p><p>27 2  Modeling Discourse Structure  Discourse cues typically do not directly indicate sentiment polarity (or aspect). [sent-64, score-0.769]
</p><p>28 , between adjacent clauses or, from a discourse-theoretic point of view, between adjacent elementary discourse units (EDUs). [sent-68, score-0.616]
</p><p>29 To  model these changes we need a strong linguistic signal, for example, in the form of discourse connectives or other discourse cues. [sent-69, score-1.233]
</p><p>30 This is certainly true for most of the traditional discourse relation cues (particularly connectives). [sent-71, score-0.734]
</p><p>31 Changes in polarity or aspect are often correlated with specific discourse relations, such as ‘contrast’. [sent-72, score-0.938]
</p><p>32 However, not all relations are relevant and there is no one-to-one correspondence between relations and sentiment changes. [sent-73, score-0.66]
</p><p>33 3 Furthermore, if a discourse relation signals a change, it is typically ambiguous whether this change occurs with the polarity (example 1) or the aspect (the room was nice but the breakfast was even better) or both (the room was nice but the breakfast was awful). [sent-74, score-1.321]
</p><p>34 ANltaSmameedif erentD poeslacrriitpyt,io sanme aspect  SaAmltAeAlt sdamif e preonlatr pitoyl,a dri tfyfe arnendt as apsepec t Table 1: Discourse relations  generic discourse relations; instead, inspired by the work of Somasundaran et al. [sent-76, score-0.872]
</p><p>35 Note that we do not have a discourse relation SameSame since we do not expect to have strong linguistic evidence which states that an EDU contains the same sentiment information as the previous one. [sent-78, score-1.123]
</p><p>36 4 However, we assume that the sentiment and topic flow is fairly smooth in general. [sent-79, score-0.601]
</p><p>37 In other words, for two adjacent EDUs not connected by any of the above three relations, the prior probability of staying at the same topic and sentiment level is higher than picking a new topic and sentiment level (i. [sent-80, score-1.3]
</p><p>38 3  Model  In this section we describe our Bayesian model, first the discourse-agnostic model and then an extension needed to encode discourse information. [sent-84, score-0.62]
</p><p>39 The formal generative story is presented in Figure 1: the red fragments correspond to the discourse modeling component. [sent-85, score-0.593]
</p><p>40 1 Discourse-agnostic model In our approach we make an assumption that all the words in an EDU correspond to the same topic and sentiment level. [sent-88, score-0.63]
</p><p>41 , K} and every ls,en fotirm eevnetr polarity zlev ∈el y ∈ {−1, 0, +1}, we s steanrtt by drawing a unigram language ,m+o1de}l, 4The typical connective in this situation would be and which is highly ambiguous and can signal several traditional discourse relations. [sent-97, score-0.84]
</p><p>42 Similarly, for every topic z and every overall sentiment polarity yˆ, we draw a distribution ψ yˆ,z over opinion polarity in this topic z. [sent-100, score-1.104]
</p><p>43 Intuitively, one would expect the sentiment of an aspect to more often agree with the overall sentiment yˆ than not. [sent-101, score-1.248]
</p><p>44 Using these “heavy-diagonal” priors is crucial, as this is the way to ensure that the overall sentiment level is tied to the aspectspecific sentiment level. [sent-106, score-1.103]
</p><p>45 Otherwise, sentiment levels will be specific to individual aspects (e. [sent-107, score-0.578]
</p><p>46 , the  ”+1” sentiment for one topic may correspond to a ”-1” sentiment for another one). [sent-109, score-1.109]
</p><p>47 Without this property we would not be able to encode soft constraints imposed by the discourse relations. [sent-110, score-0.618]
</p><p>48 Then, we choose a sentiment level yd,s for the considered EDU from the categorical distribution ψ ˆyd,zd,s, conditioned on the aspect zd,s, as well as on the global sentiment of the document ˆyd. [sent-116, score-1.317]
</p><p>49 This model can be seen as a variant of a state-of-  the-art model for jointly inducing sentiment and aspect at the sentence level (Jo and Oh, 2011), or, more precisely, as its combination with the JST model (Lin and He, 2009), adapted to the specifics of our setting. [sent-118, score-0.927]
</p><p>50 Both these models have been shown to perform well on sentiment and topic prediction tasks, outperforming earlier models, such as the TSM model (Mei et al. [sent-119, score-0.63]
</p><p>51 2 Discourse-informed model In order to integrate discourse information into the discourse-agnostic model, we need to define a set of extra parameters and random variables. [sent-123, score-0.593]
</p><p>52 Drawing model parameters First, at the corpus level, we draw a distribution ϕ˜ over four discourse relations: three relations as defined in Table 1 and an additional dummy relation 4 to indicate that there is no relation between two adjacent EDUs (NoRelation). [sent-124, score-0.866]
</p><p>53 These parameters encode the intuition that most pairs of EDUs do not  exhibit a discourse relation relevant for the task (i. [sent-126, score-0.642]
</p><p>54 This distribution encodes our beliefs about sentiment transitions between EDUs s and s + 1related through c. [sent-130, score-0.619]
</p><p>55 For example, the distribution ψ˜SameAlt,+1 would assign higher probability mass to the positive sentiment polarity (+1) than to the other 2 sentiment levels (0, -1). [sent-131, score-1.218]
</p><p>56 These scalars are hand-coded and define soft constraints that discourse relations impose on the local flow of sentiment and aspects. [sent-138, score-1.175]
</p><p>57 The parameter is a language model over discourse cues w˜ , which are not restricted to unigrams but can generate phrases of arbitrary (and variable) size. [sent-139, score-0.712]
</p><p>58 Drawing documents As pointed out above, the content generation is broken into two steps, where first we draw the discourse cue w˜ d,s from φ˜c and then we generate the remaining words. [sent-146, score-0.627]
</p><p>59 The second difference at the data generation step (Figure 1) is in the way the aspect and sentiment labels are drawn. [sent-147, score-0.764]
</p><p>60 The PoE model seems to be more appropriate here than a mixture model, as we do not want the discourse transition to overpower the sentiment-topic model. [sent-150, score-0.629]
</p><p>61 The variables that need to be inferred are the topic assignments z, the sentiment assignments y, the discourse relations c and the discourse cue w˜ (or, more precisely, its length) and are all sampled jointly (for each EDU) since we expect them to be highly dependent. [sent-154, score-1.868]
</p><p>62 We assume that zd,s and yd,s are drawn twice; once from the document specific distribution and once from the discourse transition distributions. [sent-163, score-0.633]
</p><p>63 5  Experiments  To the best of our knowledge, this is the first work that aims at evaluating directly the joint information of the sentiment and aspect assignment at the sub-sentential level of full reviews; most existing studies either focus on indirect evaluation of the produced models (e. [sent-165, score-0.776]
</p><p>64 , classifying the overall sentiment of sentences (Titov and McDonald, 2008a; Brody and Elhadad, 2010) or even reviews (Naka-  gawa et al. [sent-167, score-0.574]
</p><p>65 The annotators were presented with the whole review partitioned in EDUs and were asked to annotate every EDU with the aspect and sentiment (i. [sent-175, score-0.784]
</p><p>66 The inter-annotator agreement (IAA), as measured in terms of Cohen’s kappa score, was 66% for the aspect labeling, 70% for the sentiment annotation and 61% for the joint task of sentiment and aspect annotation. [sent-181, score-1.48]
</p><p>67 1 Direct clustering evaluation Our labels encoding aspect and sentiment level can be regarded as clusters. [sent-217, score-0.8]
</p><p>68 We divide the dataset in two subsets; one containing all EDUs starting with a discourse cue (“marked”) and one containing the remaining EDUs (“unmarked”). [sent-238, score-0.617]
</p><p>69 We hypothesize that the effect ofthe discourse-aware model should be stronger on the first subset, since the presence of the connective indicates the possibility of a discourse relation with the previous EDU. [sent-239, score-0.68]
</p><p>70 The set of discourse connectives is taken from the Penn Discourse Treebank (Prasad et al. [sent-240, score-0.64]
</p><p>71 Table 5 presents a subset of “marked” EDUs for which trying to assign the sentiment and aspect out of context (i. [sent-242, score-0.74]
</p><p>72 Although the performance over the “unmarked” example is the same for the two models, this is not the case for the “marked” instances where the discourse-informed model leverages the discourse signal and achieves better performance. [sent-251, score-0.631]
</p><p>73 This behavior agrees with our initial hypothesis, and suggests that our discourse representation, though application-specific, relies in part on the information encoded in linguistically-defined discourse cues. [sent-252, score-1.152]
</p><p>74 Nevertheless, this clearly suggests that the discourse-informed model is in fact capable of exploiting discourse signal. [sent-257, score-0.593]
</p><p>75 2  Qualitative analysis  To investigate the quality of the induced discourse structure, we present the most frequent discourse cues extracted for every discourse relation. [sent-259, score-1.854]
</p><p>76 Table 6 presents a selection of cues that best explain the discourse relation they have been associated with. [sent-260, score-0.734]
</p><p>77 A general observation is that among the cues there are not only “traditional” discourse connectives like even though, although, and, but also cues that are discriminative for the specific application. [sent-261, score-0.878]
</p><p>78 Cues for the relation AltSame  also include  Table 6: Induced cues from the discourse relations phrases that contain some anaphoric expressions, which might refer to previous mentions of an aspect in the discourse (i. [sent-265, score-1.633]
</p><p>79 3 Features in supervised learning As an additional experiment to demonstrate informative of the output of the two models, we design a supervised learning task of predicting sentiment and topic of EDUs. [sent-274, score-0.676]
</p><p>80 predicting both sentiment and topic, only sentiment and only topic for all EDUs, as well as predicting sentiment and topic for the “marked” dataset. [sent-279, score-1.808]
</p><p>81 Finally, the fact that the results for the complete set of EDUs are higher than the ones for the “marked” dataset clearly suggests that the latter constitute a hard case for sentiment analysis, in which exploiting discourse signal proves to be beneficial. [sent-295, score-1.136]
</p><p>82 Sentiment analysis has not been an exception to this and discourse has been used in order to enforce constraints on the assignment of polarity labels at several granularity levels, ranging from the lexical level (Polanyi and Zaenen, 2006) to the review level (Taboada et al. [sent-298, score-0.846]
</p><p>83 (201 1) defined a recurrent neural network model, which, in essence, learns those polarity shifters relying on sentence-level sentiment labels. [sent-304, score-0.718]
</p><p>84 Though successful, this model is unlikely to capture intra-sentence non-local phenomena such as effect of discourse connectives, unless it is provided with syntactic information as an input. [sent-305, score-0.593]
</p><p>85 However, this has been achieved by either using a subset of relations that can be found in discourse theories (Zhou et al. [sent-308, score-0.64]
</p><p>86 , 2008) the output of discourse parsers (Soricut and Marcu, 2003). [sent-311, score-0.564]
</p><p>87 This work was extended by Trivedi and Eisenstein (2013), where discourse connectors are used as features for modeling subjectivity transitions. [sent-313, score-0.619]
</p><p>88 Similarly to our set-up, discourse relations enforce constraints on sentiment polarity of associated sentiment expressions. [sent-316, score-1.798]
</p><p>89 (2009) show that gold-standard discourse information encoded in this way provides a useful signal for prediction of sentiment, but they leave automatic discourse relation prediction for future work. [sent-318, score-1.241]
</p><p>90 They use an integer linear programming framework to enforce agreement between classifiers and soft constraints provided by discourse annotations. [sent-319, score-0.591]
</p><p>91 This contrasts with our work; we do not rely on expert discourse annotation, but rather induce both discourse relations and cues jointly with aspect and sentiment. [sent-320, score-1.633]
</p><p>92 7  Conclusions and Future Work  In this work, we showed that by jointly inducing discourse information in the form of discourse cues, we can achieve better predictions for aspectspecific sentiment polarity. [sent-321, score-1.751]
</p><p>93 Our contribution con-  sists in proposing a general way of how discourse information can be integrated in any LDA-style discourse-agnostic model of aspect and sentiment. [sent-322, score-0.825]
</p><p>94 In the future, we aim at modeling more flexible sets of discourse relations and automatically inducing discourse segmentation relevant to the task. [sent-323, score-1.29]
</p><p>95 Aspect and sentiment unification model for online review analysis. [sent-370, score-0.581]
</p><p>96 Supervised and unsupervised methods in employing discourse relations for improving opinion polarity classification. [sent-419, score-0.873]
</p><p>97 Sentence level discourse parsing using syntactic and lexical information. [sent-423, score-0.6]
</p><p>98 Extracting sentiment as a function of discourse structure and topicality. [sent-427, score-1.072]
</p><p>99 A joint model of text and aspect ratings for sentiment summariza-  1638tion. [sent-440, score-0.769]
</p><p>100 Unsupervised discovery of discourse relations for eliminating intra-sentence polarity ambiguities. [sent-464, score-0.782]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('discourse', 0.564), ('sentiment', 0.508), ('edus', 0.251), ('aspect', 0.232), ('polarity', 0.142), ('cues', 0.119), ('topic', 0.093), ('norelation', 0.086), ('room', 0.081), ('edu', 0.081), ('taboada', 0.079), ('transitions', 0.078), ('relations', 0.076), ('connectives', 0.076), ('somasundaran', 0.072), ('shifters', 0.068), ('voll', 0.068), ('titov', 0.061), ('poe', 0.06), ('drawing', 0.06), ('marked', 0.059), ('opinion', 0.057), ('hotel', 0.056), ('polanyi', 0.056), ('unmarked', 0.056), ('relation', 0.051), ('aspectspecific', 0.051), ('kimberly', 0.051), ('samealt', 0.051), ('sentasp', 0.051), ('predicting', 0.049), ('nice', 0.048), ('maite', 0.045), ('review', 0.044), ('induced', 0.043), ('aspects', 0.043), ('consequently', 0.042), ('induce', 0.042), ('zaenen', 0.039), ('eisenstein', 0.038), ('jo', 0.038), ('signal', 0.038), ('breakfast', 0.037), ('induction', 0.036), ('jointly', 0.036), ('topics', 0.036), ('connective', 0.036), ('draw', 0.036), ('transition', 0.036), ('level', 0.036), ('altalt', 0.034), ('altsame', 0.034), ('aridou', 0.034), ('discourseagnostic', 0.034), ('gawa', 0.034), ('iki', 0.034), ('samesame', 0.034), ('trivedi', 0.034), ('unsupervised', 0.034), ('qualitative', 0.034), ('distribution', 0.033), ('reviews', 0.032), ('proportional', 0.032), ('mei', 0.031), ('ganu', 0.03), ('jst', 0.03), ('sadamitsu', 0.03), ('subsentential', 0.03), ('snyder', 0.03), ('segmentation', 0.029), ('modeling', 0.029), ('model', 0.029), ('ange', 0.028), ('rooms', 0.028), ('inducing', 0.028), ('anaphoric', 0.027), ('cue', 0.027), ('oh', 0.027), ('encode', 0.027), ('soft', 0.027), ('levels', 0.027), ('ivan', 0.026), ('informative', 0.026), ('connectors', 0.026), ('parametrized', 0.026), ('webber', 0.026), ('dirichlet', 0.026), ('dataset', 0.026), ('adjacent', 0.026), ('asher', 0.025), ('facilities', 0.025), ('swapna', 0.025), ('collapsed', 0.025), ('bayesian', 0.024), ('labels', 0.024), ('favor', 0.024), ('noemie', 0.024), ('encoded', 0.024), ('solely', 0.024), ('fox', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="2-tfidf-1" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>2 0.38866159 <a title="2-tfidf-2" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>3 0.35214826 <a title="2-tfidf-3" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>4 0.33844483 <a title="2-tfidf-4" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>Author: Hongliang Yu ; Zhi-Hong Deng ; Shiyingxue Li</p><p>Abstract: Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications. Most existing researches exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words.</p><p>5 0.32363597 <a title="2-tfidf-5" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>6 0.23294853 <a title="2-tfidf-6" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>7 0.22949742 <a title="2-tfidf-7" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>8 0.2217544 <a title="2-tfidf-8" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>9 0.20759715 <a title="2-tfidf-9" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<p>10 0.20561202 <a title="2-tfidf-10" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>11 0.20475903 <a title="2-tfidf-11" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>12 0.20166692 <a title="2-tfidf-12" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<p>13 0.20049952 <a title="2-tfidf-13" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>14 0.17823702 <a title="2-tfidf-14" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>15 0.17326279 <a title="2-tfidf-15" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<p>16 0.16936848 <a title="2-tfidf-16" href="./acl-2013-Generating_Recommendation_Dialogs_by_Extracting_Information_from_User_Reviews.html">168 acl-2013-Generating Recommendation Dialogs by Extracting Information from User Reviews</a></p>
<p>17 0.16897275 <a title="2-tfidf-17" href="./acl-2013-Probabilistic_Sense_Sentiment_Similarity_through_Hidden_Emotions.html">284 acl-2013-Probabilistic Sense Sentiment Similarity through Hidden Emotions</a></p>
<p>18 0.15738352 <a title="2-tfidf-18" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>19 0.15649478 <a title="2-tfidf-19" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>20 0.15242675 <a title="2-tfidf-20" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.231), (1, 0.346), (2, -0.09), (3, 0.273), (4, -0.141), (5, -0.026), (6, 0.062), (7, 0.068), (8, -0.025), (9, 0.234), (10, 0.309), (11, -0.008), (12, -0.099), (13, 0.076), (14, 0.046), (15, -0.074), (16, 0.082), (17, -0.168), (18, -0.15), (19, -0.155), (20, -0.003), (21, 0.12), (22, 0.005), (23, -0.082), (24, -0.009), (25, -0.038), (26, 0.05), (27, 0.076), (28, 0.012), (29, 0.003), (30, 0.079), (31, 0.022), (32, -0.013), (33, -0.064), (34, 0.017), (35, -0.002), (36, -0.012), (37, -0.003), (38, -0.05), (39, 0.007), (40, -0.0), (41, -0.055), (42, -0.037), (43, -0.045), (44, 0.009), (45, 0.017), (46, -0.042), (47, -0.033), (48, -0.016), (49, 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98252499 <a title="2-lsi-1" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>2 0.72784913 <a title="2-lsi-2" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>3 0.71016616 <a title="2-lsi-3" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>4 0.69155097 <a title="2-lsi-4" href="./acl-2013-Identifying_Sentiment_Words_Using_an_Optimization-based_Model_without_Seed_Words.html">188 acl-2013-Identifying Sentiment Words Using an Optimization-based Model without Seed Words</a></p>
<p>Author: Hongliang Yu ; Zhi-Hong Deng ; Shiyingxue Li</p><p>Abstract: Sentiment Word Identification (SWI) is a basic technique in many sentiment analysis applications. Most existing researches exploit seed words, and lead to low robustness. In this paper, we propose a novel optimization-based model for SWI. Unlike previous approaches, our model exploits the sentiment labels of documents instead of seed words. Several experiments on real datasets show that WEED is effective and outperforms the state-of-the-art methods with seed words.</p><p>5 0.68765831 <a title="2-lsi-5" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>6 0.65588671 <a title="2-lsi-6" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<p>7 0.65280265 <a title="2-lsi-7" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>8 0.65020502 <a title="2-lsi-8" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>9 0.5940001 <a title="2-lsi-9" href="./acl-2013-Exploring_Sentiment_in_Social_Media%3A_Bootstrapping_Subjectivity_Clues_from_Multilingual_Twitter_Streams.html">148 acl-2013-Exploring Sentiment in Social Media: Bootstrapping Subjectivity Clues from Multilingual Twitter Streams</a></p>
<p>10 0.58509499 <a title="2-lsi-10" href="./acl-2013-Dual_Training_and_Dual_Prediction_for_Polarity_Classification.html">131 acl-2013-Dual Training and Dual Prediction for Polarity Classification</a></p>
<p>11 0.54492319 <a title="2-lsi-11" href="./acl-2013-LABR%3A_A_Large_Scale_Arabic_Book_Reviews_Dataset.html">211 acl-2013-LABR: A Large Scale Arabic Book Reviews Dataset</a></p>
<p>12 0.54265177 <a title="2-lsi-12" href="./acl-2013-Connotation_Lexicon%3A_A_Dash_of_Sentiment_Beneath_the_Surface_Meaning.html">91 acl-2013-Connotation Lexicon: A Dash of Sentiment Beneath the Surface Meaning</a></p>
<p>13 0.53847945 <a title="2-lsi-13" href="./acl-2013-Utterance-Level_Multimodal_Sentiment_Analysis.html">379 acl-2013-Utterance-Level Multimodal Sentiment Analysis</a></p>
<p>14 0.50731337 <a title="2-lsi-14" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>15 0.49309537 <a title="2-lsi-15" href="./acl-2013-Probabilistic_Sense_Sentiment_Similarity_through_Hidden_Emotions.html">284 acl-2013-Probabilistic Sense Sentiment Similarity through Hidden Emotions</a></p>
<p>16 0.47021547 <a title="2-lsi-16" href="./acl-2013-The_Haves_and_the_Have-Nots%3A_Leveraging_Unlabelled_Corpora_for_Sentiment_Analysis.html">345 acl-2013-The Haves and the Have-Nots: Leveraging Unlabelled Corpora for Sentiment Analysis</a></p>
<p>17 0.45060265 <a title="2-lsi-17" href="./acl-2013-Generating_Recommendation_Dialogs_by_Extracting_Information_from_User_Reviews.html">168 acl-2013-Generating Recommendation Dialogs by Extracting Information from User Reviews</a></p>
<p>18 0.44424585 <a title="2-lsi-18" href="./acl-2013-An_annotated_corpus_of_quoted_opinions_in_news_articles.html">49 acl-2013-An annotated corpus of quoted opinions in news articles</a></p>
<p>19 0.44047832 <a title="2-lsi-19" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>20 0.43256456 <a title="2-lsi-20" href="./acl-2013-Detecting_Event-Related_Links_and_Sentiments_from_Social_Media_Texts.html">115 acl-2013-Detecting Event-Related Links and Sentiments from Social Media Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.051), (4, 0.012), (6, 0.029), (11, 0.059), (15, 0.03), (24, 0.105), (26, 0.102), (29, 0.018), (35, 0.084), (42, 0.04), (48, 0.048), (70, 0.042), (88, 0.066), (89, 0.132), (90, 0.025), (95, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88604224 <a title="2-lda-1" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>2 0.83161455 <a title="2-lda-2" href="./acl-2013-Measuring_semantic_content_in_distributional_vectors.html">238 acl-2013-Measuring semantic content in distributional vectors</a></p>
<p>Author: Aurelie Herbelot ; Mohan Ganesalingam</p><p>Abstract: Some words are more contentful than others: for instance, make is intuitively more general than produce and fifteen is more ‘precise’ than a group. In this paper, we propose to measure the ‘semantic content’ of lexical items, as modelled by distributional representations. We investigate the hypothesis that semantic content can be computed using the KullbackLeibler (KL) divergence, an informationtheoretic measure of the relative entropy of two distributions. In a task focusing on retrieving the correct ordering of hyponym-hypernym pairs, the KL diver- gence achieves close to 80% precision but does not outperform a simpler (linguistically unmotivated) frequency measure. We suggest that this result illustrates the rather ‘intensional’ aspect of distributions.</p><p>3 0.81669861 <a title="2-lda-3" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>Author: Christian Scheible ; Hinrich Schutze</p><p>Abstract: A number of different notions, including subjectivity, have been proposed for distinguishing parts of documents that convey sentiment from those that do not. We propose a new concept, sentiment relevance, to make this distinction and argue that it better reflects the requirements of sentiment analysis systems. We demonstrate experimentally that sentiment relevance and subjectivity are related, but different. Since no large amount of labeled training data for our new notion of sentiment relevance is available, we investigate two semi-supervised methods for creating sentiment relevance classifiers: a distant supervision approach that leverages structured information about the domain of the reviews; and transfer learning on feature representations based on lexical taxonomies that enables knowledge transfer. We show that both methods learn sentiment relevance classifiers that perform well.</p><p>4 0.79412425 <a title="2-lda-4" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>Author: Matt Post ; Shane Bergsma</p><p>Abstract: Syntactic features are useful for many text classification tasks. Among these, tree kernels (Collins and Duffy, 2001) have been perhaps the most robust and effective syntactic tool, appealing for their empirical success, but also because they do not require an answer to the difficult question of which tree features to use for a given task. We compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly avail- able tools) , we suggest they should always be included as baseline comparisons in tree kernel method evaluations.</p><p>5 0.79316062 <a title="2-lda-5" href="./acl-2013-Crawling_microblogging_services_to_gather_language-classified_URLs._Workflow_and_case_study.html">95 acl-2013-Crawling microblogging services to gather language-classified URLs. Workflow and case study</a></p>
<p>Author: Adrien Barbaresi</p><p>Abstract: We present a way to extract links from messages published on microblogging platforms and we classify them according to the language and possible relevance of their target in order to build a text corpus. Three platforms are taken into consideration: FriendFeed, identi.ca and Reddit, as they account for a relative diversity of user profiles and more importantly user languages. In order to explore them, we introduce a traversal algorithm based on user pages. As we target lesser-known languages, we try to focus on non-English posts by filtering out English text. Using mature open-source software from the NLP research field, a spell checker (as- pell) and a language identification system (langid .py), our case study and our benchmarks give an insight into the linguistic structure of the considered services.</p><p>6 0.78614116 <a title="2-lda-6" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>7 0.78495467 <a title="2-lda-7" href="./acl-2013-Using_Conceptual_Class_Attributes_to_Characterize_Social_Media_Users.html">373 acl-2013-Using Conceptual Class Attributes to Characterize Social Media Users</a></p>
<p>8 0.78395617 <a title="2-lda-8" href="./acl-2013-Linking_Tweets_to_News%3A_A_Framework_to_Enrich_Short_Text_Data_in_Social_Media.html">233 acl-2013-Linking Tweets to News: A Framework to Enrich Short Text Data in Social Media</a></p>
<p>9 0.78372049 <a title="2-lda-9" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>10 0.7834664 <a title="2-lda-10" href="./acl-2013-Identifying_Opinion_Subgroups_in_Arabic_Online_Discussions.html">187 acl-2013-Identifying Opinion Subgroups in Arabic Online Discussions</a></p>
<p>11 0.7828759 <a title="2-lda-11" href="./acl-2013-Detecting_Turnarounds_in_Sentiment_Analysis%3A_Thwarting.html">117 acl-2013-Detecting Turnarounds in Sentiment Analysis: Thwarting</a></p>
<p>12 0.7786907 <a title="2-lda-12" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>13 0.7780757 <a title="2-lda-13" href="./acl-2013-Multilingual_Affect_Polarity_and_Valence_Prediction_in_Metaphor-Rich_Texts.html">253 acl-2013-Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich Texts</a></p>
<p>14 0.77739155 <a title="2-lda-14" href="./acl-2013-Semantic_Frames_to_Predict_Stock_Price_Movement.html">310 acl-2013-Semantic Frames to Predict Stock Price Movement</a></p>
<p>15 0.77260786 <a title="2-lda-15" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>16 0.77241343 <a title="2-lda-16" href="./acl-2013-Text_Classification_from_Positive_and_Unlabeled_Data_using_Misclassified_Data_Correction.html">342 acl-2013-Text Classification from Positive and Unlabeled Data using Misclassified Data Correction</a></p>
<p>17 0.77214319 <a title="2-lda-17" href="./acl-2013-Exploiting_Topic_based_Twitter_Sentiment_for_Stock_Prediction.html">147 acl-2013-Exploiting Topic based Twitter Sentiment for Stock Prediction</a></p>
<p>18 0.77189314 <a title="2-lda-18" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>19 0.77111322 <a title="2-lda-19" href="./acl-2013-Co-regularizing_character-based_and_word-based_models_for_semi-supervised_Chinese_word_segmentation.html">82 acl-2013-Co-regularizing character-based and word-based models for semi-supervised Chinese word segmentation</a></p>
<p>20 0.77075315 <a title="2-lda-20" href="./acl-2013-PhonMatrix%3A_Visualizing_co-occurrence_constraints_of_sounds.html">279 acl-2013-PhonMatrix: Visualizing co-occurrence constraints of sounds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
