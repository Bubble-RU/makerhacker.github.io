<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-388" href="#">acl2013-388</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</h1>
<br/><p>Source: <a title="acl-2013-388-pdf" href="http://aclweb.org/anthology//P/P13/P13-1017.pdf">pdf</a></p><p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>Reference: <a title="acl-2013-388-reference" href="../acl2013_reference/acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn  Abstract In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al. [sent-4, score-0.374]
</p><p>2 , 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. [sent-7, score-0.898]
</p><p>3 Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score. [sent-9, score-0.336]
</p><p>4 1 Introduction Recent years research communities have seen a strong resurgent interest in modeling with deep (multi-layer) neural networks. [sent-10, score-0.42]
</p><p>5 The unsupervised pretraining trains the network one layer at a time, and helps to guide the parameters of the layer towards better regions in parameter space (Bengio, 2009). [sent-19, score-0.752]
</p><p>6 , 2012) proposed context-dependent neural network with large vocabulary, which achieved 16. [sent-26, score-0.545]
</p><p>7 Word embedding is usually first learned from huge amount of monolingual texts, and then fine-tuned with taskspecific objectives. [sent-31, score-0.261]
</p><p>8 Inspired by successful previous works, we propose a new DNN-based word alignment method, which exploits contextual and semantic similarities between words. [sent-36, score-0.299]
</p><p>9 Figure 1: Two examples of word alignment  the English word “mammoth” is not, so it is very hard to align them correctly. [sent-53, score-0.384]
</p><p>10 As we mentioned in the last paragraph, word embedding (trained with huge monolingual texts) has the ability to map a word into a vector space, in which, similar words are near each other. [sent-55, score-0.373]
</p><p>11 In the rest of this paper, related work about DNN and word alignment are first reviewed in  Section 2, followed by a brief introduction of DNN in Section 3. [sent-60, score-0.299]
</p><p>12 We then introduce the details of leveraging DNN for word alignment, including the details of our network structure in Section 4 and the training method in Section 5. [sent-61, score-0.308]
</p><p>13 , 2006) proposed to use multi-layer neural network for language modeling task. [sent-75, score-0.545]
</p><p>14 (Niehues and Waibel, 2012) shows that machine translation results can be improved by combining neural language model with n-gram traditional language. [sent-78, score-0.429]
</p><p>15 , 2012) improves translation quality of n-gram translation model by using a bilingual neural language model. [sent-80, score-0.574]
</p><p>16 , 2012) learns a context-free cross-lingual word embeddings to facilitate cross-lingual information retrieval. [sent-82, score-0.35]
</p><p>17 Word embeddings often implicitly encode syntactic or semantic knowledge of the words. [sent-93, score-0.294]
</p><p>18 Assuming a finite sized vocabulary V , word embeddings form a (L |V | )-dimension embedding mbeadtdriixn WV, mwh ae (rLe ×L iVs a pre-determined  ×  embedding length; mapping words to embeddings is done by simply looking up their respective columns in the embedding matrix WV. [sent-94, score-1.15]
</p><p>19 The lookup process is called a lookup layer LT , which is usually the first layer after the input layer in neural network. [sent-95, score-1.099]
</p><p>20 If input must be of variable length, convolution layer and max layer can be used, (Collobert et al. [sent-101, score-0.402]
</p><p>21 Multi-layer neural networks are trained with the standard back propagation algorithm (LeCun, 1985). [sent-103, score-0.362]
</p><p>22 , 1998) have been developed to train better neural networks. [sent-107, score-0.351]
</p><p>23 Besides that, neural network training also involves some hyperparameters such as learning rate, the number of hidden layers. [sent-108, score-0.649]
</p><p>24 4 DNN for word alignment Our DNN word alignment model extends classic HMM word alignment model (Vogel et al. [sent-110, score-1.118]
</p><p>25 Given a sentence pair (e, f), HMM word alignment takes the following form:  P(a,e|f) =Y|e|Plex(ei|fai)Pd(ai− ai−1)  (4)  iY= Y1  where Plex is the lexical translation probability and Pd is the jump distance distortion probability. [sent-112, score-0.63]
</p><p>26 One straightforward way to integrate DNN into HMM is to use neural network to compute the emission (lexical translation) probability Plex. [sent-113, score-0.545]
</p><p>27 Such approach requires a softmax layer in the neural network to normalize over all words in source vocabulary. [sent-114, score-0.775]
</p><p>28 Hence we give up the probabilistic interpretation and resort to a nonprobabilistic, discriminative view:  sNN(a|e,f) =Y|e|tlex(ei,fai|e,f)td(ai,ai−1|e,f) Yi=1  (5) where tlex is a lexical translation score computed by neural network, and td is a distortion score. [sent-116, score-0.745]
</p><p>29 In the classic HMM word alignment model, context is not considered in the lexical translation probability. [sent-117, score-0.554]
</p><p>30 a Idn V contrast, our model does not maintain a separate translation score parameters for every  source-target word pair, but computes tlex through a multi-layer network, which naturally handles contexts on both sides without explosive growth of number of parameters. [sent-120, score-0.363]
</p><p>31 Figure 2 shows the neural network we used to compute context dependent lexical translation score tlex. [sent-123, score-0.653]
</p><p>32 For word pair (ei, fj), we take fixed length windows surrounding both ei and fj as input: (ei−s2w, . [sent-124, score-0.307]
</p><p>33 For the distortion td, we could use a lexicalized distortion model: td(ai,  ai−1  |e, f) = td(ai  − ai−1  |window(fa )) (7)  which can be computed by a neural network similar to the one used to compute lexical translation scores. [sent-132, score-0.907]
</p><p>34 If we map jump distance (ai ai−1) to B buckets, we can change the length o af the output layer to B, where each dimension in the output stands for a different bucket of jump distances. [sent-133, score-0.389]
</p><p>35 But we found in our initial experiments on small scale data, lexicalized distortion does not produce better alignment over the simple jumpdistance based model. [sent-134, score-0.389]
</p><p>36 So we drop the lexicalized distortion and reverse to the simple version: −  td(ai,  ai−1  |e, f) = td(ai  −  ai−1)  (8)  Vocabulary V of our alignment model consists of a source vocabulary Ve and a target vocabulary Vf. [sent-135, score-0.52]
</p><p>37 To decode our model, the lexical translation scores are computed for each source-target word pair in the sentence pair, which requires going through the neural network (|e| |f|) times; after that, tthhee forward-backward algorithm can a bfeused to find the viterbi path as in the classic HMM model. [sent-139, score-0.856]
</p><p>38 The majority of tunable parameters in our model resides in the lookup table LT, which is a (L ( |Ve | + |Vf |))-dimension matrix. [sent-140, score-0.24]
</p><p>39 In fact, discriminative word alignment can model contexts by deploying arbitrary features (Moore, 2005). [sent-144, score-0.408]
</p><p>40 Different from previous discriminative word alignment, our model does not use manually engineered features, but learn “features” automatically from raw words by the neural net-  work. [sent-145, score-0.451]
</p><p>41 , 1996) use a maximum entropy model to model the bag-of-words context for word alignment, but their model treats each word as a distinct feature, which can not leverage the similarity between words as our model. [sent-147, score-0.261]
</p><p>42 , 2011) can be adapted to train 1In practice, the number of non-zero parameters in classic HMM model would be much smaller, as many words do not co-occur in bilingual sentence pairs. [sent-149, score-0.36]
</p><p>43 In our experiments, the number of non-zero parameters in classic HMM model is about 328 millions, while the NN model only has about 4 millions. [sent-150, score-0.293]
</p><p>44 169  our model from raw sentence pairs, they are too computational demanding as the lexical translation probabilities must be computed from neural networks. [sent-151, score-0.429]
</p><p>45 As we do  not have a large manually word aligned corpus, we use traditional word alignment models such as HMM and IBM model 4 to generate word alignment on a large parallel corpus. [sent-153, score-0.691]
</p><p>46 We obtain bidirectional alignment by running the usual growdiag-final heuristics (Koehn et al. [sent-154, score-0.243]
</p><p>47 , 2012), where training data for neural network model is generated by forced decoding with traditional Gaussian mixture models. [sent-157, score-0.611]
</p><p>48 Tunable parameters in neural network alignment model include: word embeddings in lookup table LT, parameters Wl, bl for linear transformations in the hidden layers of the neural network, and distortion parameters sd of jump distance. [sent-158, score-2.32]
</p><p>49 One nuance here is that the gold alignment after grow-diag-final contains many-to-many links, which cannot be generated by any path. [sent-161, score-0.243]
</p><p>50 Our solution is that for each source word alignment multiple target, we randomly choose one link among all candidates as the golden link. [sent-162, score-0.299]
</p><p>51 Because our multi-layer neural network is inherently non-linear and is non-convex, directly training against the above criteria is unlikely to yield good results. [sent-163, score-0.637]
</p><p>52 1 Pre-training initial word embedding with monolingual data Most parameters reside in the word embeddings. [sent-166, score-0.413]
</p><p>53 To get a good initial value, the usual approach is to pre-train the embeddings on a large monolingual corpus. [sent-167, score-0.37]
</p><p>54 , 2011) and train word embeddings for source  and target languages from their monolingual corpus respectively. [sent-169, score-0.455]
</p><p>55 We set word embedding length to 20, window size to 5, and the length of the only hidden layer to 40. [sent-171, score-0.655]
</p><p>56 Note that embedding for null word in either Ve and Vf cannot be trained from monolingual corpus, and we simply leave them at the initial value untouched. [sent-177, score-0.327]
</p><p>57 Word embeddings from monolingual corpus learn strong syntactic knowledge of each word, which is not always desirable for word alignment between some language pairs like English and Chinese. [sent-178, score-0.669]
</p><p>58 For example, many Chinese words can act as a verb, noun and adjective without any change, while their English counter parts are distinct words with quite different word embeddings due to their different syntactic roles. [sent-179, score-0.35]
</p><p>59 Thus we  have to modify the word embeddings in subsequent steps according to bilingual data. [sent-180, score-0.425]
</p><p>60 2  Training neural network based on local criteria Training the network against the sentence level criteria Eq. [sent-182, score-0.894]
</p><p>61 This training criteria essentially means our model suffers loss unless it gives correct word pairs a higher score than random pairs from the same sentence pair with some margin. [sent-186, score-0.277]
</p><p>62 We initialize the lookup table with embeddings obtained from monolingual training, and randomly initialize all Wl and bl in linear layers to [-0. [sent-187, score-0.717]
</p><p>63 We randomly cy-  cle through all sentence pairs in training data; for each correct word pair (including null alignment), we generate a positive example, and generate two negative examples by randomly corrupting either 170  side of the pair with another word in the sentence pair. [sent-191, score-0.259]
</p><p>64 To make our model concrete, there are still hyper-parameters to be determined: the window size sw and tw, the length of each hidden layer Ll. [sent-196, score-0.48]
</p><p>65 3 Training distortion parameters We fix neural network parameters obtained from the last step, and tune the distortion parameters sd with respect to the sentence level loss using  standard stochastic gradient descent. [sent-199, score-1.194]
</p><p>66 4  Tuning neural network based on sentence level criteria  Up-to-now, parameters in the lexical translation neural network have not been trained against the sentence level criteria Eq. [sent-205, score-1.358]
</p><p>67 We could achieve this by re-using the same online training method used to train distortion parameters, except that we now fix the distortion parameters and let the loss back-propagate through the neural networks. [sent-207, score-0.798]
</p><p>68 This tuning is quite slow, and it did not improve alignment on an initial small scale experiment; so,  we skip this step in all subsequent experiment in this work. [sent-209, score-0.243]
</p><p>69 6  Experiments and Results  We conduct our experiment on Chinese-to-English word alignment task. [sent-210, score-0.299]
</p><p>70 We use the manually aligned Chinese-English alignment corpus (Haghighi et al. [sent-211, score-0.243]
</p><p>71 The monolingual corpus to pre-train word embeddings are also crawled from web, which amounts to about 1. [sent-216, score-0.426]
</p><p>72 We train our proposed model from results of classic HMM and IBM model 4 separately. [sent-221, score-0.25]
</p><p>73 3 Alignment Result It can be seen from Table 1, the proposed model consistently outperforms its corresponding baseline whether it is trained from alignment of classic HMM or IBM model 4. [sent-225, score-0.464]
</p><p>74 In future we would like to explore whether our method can improve other word alignment models. [sent-234, score-0.299]
</p><p>75 Despite different alignment scores, we do not obtain significant difference in translation performance. [sent-237, score-0.313]
</p><p>76 307 for models trained from IBM-4 and NN alignment results. [sent-241, score-0.243]
</p><p>77 The result is not surprising considering our parallel corpus is quite large, and similar observations have been made in previous work as (DeNero and Macherey, 2011) that better alignment quality does not necessarily lead to better end-to-end result. [sent-242, score-0.243]
</p><p>78 By analyzing the results, we found out that for both baseline and our model, a large part of missing alignment links involves stop words like English words “the”, “a”, “it” and Chinese words “de”. [sent-247, score-0.243]
</p><p>79 Stop words are inherently hard to align, which often requires grammatical judgment unavailable to our models; as they are  also extremely frequent, our model fully learns their alignment patterns of the baseline models, including errors. [sent-248, score-0.28]
</p><p>80 In our model, different person names have very similar word embeddings on both English side and Chinese side, due to monolingual pre-training; what is more, different person names often appear in similar contexts. [sent-252, score-0.598]
</p><p>81 As our model considers both word embeddings and contexts, it learns that English person names should be aligned to Chinese person names, which corrects errors of baseline models and leads to better precision. [sent-253, score-0.51]
</p><p>82 2 Effect of context To examine how context contribute to alignment quality, we re-train our model with different window size, all from result of IBM model 4. [sent-256, score-0.495]
</p><p>83 74  1  3  5  7  9  11 13  Figure 3: Effect of different window sizes on word alignment F-score. [sent-264, score-0.401]
</p><p>84 With larger window size, our model is able to produce more accurate translation scores based on more contexts, which leads to better alignment despite the simpler distortions. [sent-270, score-0.452]
</p><p>85 Two hidden layers outperform one hidden layer, while three hidden layers do not bring further improvement. [sent-273, score-0.489]
</p><p>86 3 Effect of number of hidden layers Our neural network contains two hidden layers besides the lookup layer. [sent-276, score-1.046]
</p><p>87 For 1-hidden-layer setting, we set the hidden layer length to 120; and for 3-hidden-layer setting, we set hidden layer lengths to 120, 100, 10 respectively. [sent-279, score-0.586]
</p><p>88 As can be seen from Table 3, 2hidden-layer outperforms the 1-hidden-layer setting, while another hidden layer does not bring 172  Table 2: Nearest neighbors of several words according to their embedding distance. [sent-280, score-0.485]
</p><p>89 LM shows neighbors of word embeddings  trained by monolingual  language model method;  WA shows neighbors  of word  embeddings trained by our word alignment model. [sent-281, score-1.224]
</p><p>90 Due to time constraint, we have not tuned the hyper-parameters such as length of hidden layers in 1 and 3-hidden-layer settings, nor have we tested settings with more hidden-layers. [sent-283, score-0.273]
</p><p>91 While this is true for relatively frequent nouns such as “lab” and “labs”, rarer nouns still remain near their monolingual embeddings as they are only modified a few times during the bilingual training. [sent-293, score-0.445]
</p><p>92 7  Conclusion  In this paper, we explores applying deep neural network for word alignment task. [sent-295, score-0.942]
</p><p>93 Our model integrates a multi-layer neural network into an HMM-like framework, where context dependent lexical translation score is computed by neural network, and distortion is modeled by a simple jump-distance scheme. [sent-296, score-1.158]
</p><p>94 Our model is discriminatively trained on bilingual corpus, while huge monolingual data is used to pre-train wordembeddings. [sent-297, score-0.258]
</p><p>95 Experiments on large-scale Chineseto-English task show that the proposed method produces better word alignment results, compared with both classic HMM model and IBM model 4. [sent-298, score-0.52]
</p><p>96 Secondly, we want to explore the possibility of unsupervised training of our neural word alignment model, without reliance of alignment result of other models. [sent-300, score-0.893]
</p><p>97 Neurocomputing: Algorithms, architectures and applications, chapter probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. [sent-329, score-0.259]
</p><p>98 Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. [sent-344, score-0.46]
</p><p>99 Advances in neural information processing systems, 20: 1185–1 192. [sent-397, score-0.322]
</p><p>100 Parsing natural scenes and natural language with recursive neural networks. [sent-422, score-0.36]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dnn', 0.349), ('neural', 0.322), ('embeddings', 0.294), ('alignment', 0.243), ('network', 0.223), ('layer', 0.201), ('hmm', 0.195), ('embedding', 0.153), ('classic', 0.147), ('distortion', 0.146), ('layers', 0.132), ('collobert', 0.122), ('dahl', 0.11), ('bengio', 0.109), ('window', 0.102), ('deep', 0.098), ('tlex', 0.092), ('lookup', 0.087), ('ibm', 0.086), ('zl', 0.085), ('ai', 0.084), ('mammoth', 0.083), ('yibula', 0.083), ('td', 0.079), ('jump', 0.077), ('monolingual', 0.076), ('hidden', 0.075), ('bilingual', 0.075), ('parameters', 0.072), ('translation', 0.07), ('vf', 0.07), ('yann', 0.064), ('yoshua', 0.064), ('criteria', 0.063), ('htanh', 0.062), ('juda', 0.062), ('krizhevsky', 0.062), ('fj', 0.062), ('ei', 0.06), ('plex', 0.058), ('surrounding', 0.057), ('socher', 0.057), ('word', 0.056), ('neighbors', 0.056), ('hyperbolic', 0.055), ('lecun', 0.055), ('pretraining', 0.055), ('lt', 0.055), ('loss', 0.054), ('chinese', 0.053), ('fl', 0.051), ('hinton', 0.05), ('bl', 0.05), ('names', 0.049), ('fai', 0.048), ('kavukcuoglu', 0.048), ('imagenet', 0.048), ('ve', 0.048), ('vocabulary', 0.047), ('stochastic', 0.047), ('tunable', 0.044), ('null', 0.042), ('boureau', 0.042), ('nongmin', 0.042), ('seide', 0.042), ('itg', 0.041), ('networks', 0.04), ('sd', 0.04), ('della', 0.039), ('initialize', 0.039), ('discriminatively', 0.038), ('optimizer', 0.038), ('wl', 0.038), ('pair', 0.038), ('context', 0.038), ('recursive', 0.038), ('person', 0.037), ('distortions', 0.037), ('rbm', 0.037), ('model', 0.037), ('architectures', 0.036), ('activation', 0.036), ('contexts', 0.036), ('discriminative', 0.036), ('nearest', 0.035), ('length', 0.034), ('denero', 0.034), ('huge', 0.032), ('settings', 0.032), ('niehues', 0.032), ('koray', 0.032), ('convolutional', 0.032), ('boltzmann', 0.032), ('vogel', 0.031), ('sw', 0.031), ('shujie', 0.029), ('softmax', 0.029), ('training', 0.029), ('align', 0.029), ('train', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000013 <a title="388-tfidf-1" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>2 0.37220952 <a title="388-tfidf-2" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>Author: Igor Labutov ; Hod Lipson</p><p>Abstract: We present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task. Recently, with an increase in computing resources, it became possible to learn rich word embeddings from massive amounts of unlabeled data. However, some methods take days or weeks to learn good embeddings, and some are notoriously difficult to train. We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with re- spect to several baselines, and observe that the approach is most useful when the training set is sufficiently small.</p><p>3 0.29992512 <a title="388-tfidf-3" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>Author: lemao liu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Most statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks.</p><p>4 0.19534577 <a title="388-tfidf-4" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>Author: Zhengyan He ; Shujie Liu ; Mu Li ; Ming Zhou ; Longkai Zhang ; Houfeng Wang</p><p>Abstract: We propose a novel entity disambiguation model, based on Deep Neural Network (DNN). Instead of utilizing simple similarity measures and their disjoint combinations, our method directly optimizes document and entity representations for a given similarity measure. Stacked Denoising Auto-encoders are first employed to learn an initial document representation in an unsupervised pre-training stage. A supervised fine-tuning stage follows to optimize the representation towards the similarity measure. Experiment results show that our method achieves state-of-the-art performance on two public datasets without any manually designed features, even beating complex collective approaches.</p><p>5 0.1839762 <a title="388-tfidf-5" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>Author: Tiberiu Boros ; Radu Ion ; Dan Tufis</p><p>Abstract: Radu Ion Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy radu@ racai . ro Dan Tufi? Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy tufi s @ racai . ro Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, ?????? ?????? ??????? ??????, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. 1</p><p>6 0.1833403 <a title="388-tfidf-6" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>7 0.1721358 <a title="388-tfidf-7" href="./acl-2013-Adaptation_Data_Selection_using_Neural_Language_Models%3A_Experiments_in_Machine_Translation.html">35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</a></p>
<p>8 0.16224593 <a title="388-tfidf-8" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>9 0.15241854 <a title="388-tfidf-9" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>10 0.14641367 <a title="388-tfidf-10" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>11 0.14201215 <a title="388-tfidf-11" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>12 0.13906401 <a title="388-tfidf-12" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>13 0.13685043 <a title="388-tfidf-13" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>14 0.13522601 <a title="388-tfidf-14" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>15 0.13246627 <a title="388-tfidf-15" href="./acl-2013-The_Role_of_Syntax_in_Vector_Space_Models_of_Compositional_Semantics.html">347 acl-2013-The Role of Syntax in Vector Space Models of Compositional Semantics</a></p>
<p>16 0.1315164 <a title="388-tfidf-16" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>17 0.1273853 <a title="388-tfidf-17" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>18 0.12695554 <a title="388-tfidf-18" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>19 0.10738189 <a title="388-tfidf-19" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>20 0.10735064 <a title="388-tfidf-20" href="./acl-2013-Distortion_Model_Considering_Rich_Context_for_Statistical_Machine_Translation.html">125 acl-2013-Distortion Model Considering Rich Context for Statistical Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.255), (1, -0.105), (2, 0.121), (3, 0.067), (4, 0.018), (5, -0.031), (6, -0.042), (7, -0.024), (8, -0.072), (9, 0.083), (10, 0.009), (11, -0.231), (12, 0.114), (13, -0.242), (14, 0.008), (15, 0.099), (16, -0.056), (17, 0.017), (18, -0.016), (19, -0.338), (20, 0.013), (21, -0.096), (22, -0.162), (23, -0.055), (24, 0.039), (25, -0.062), (26, 0.084), (27, -0.087), (28, 0.138), (29, 0.016), (30, -0.241), (31, -0.021), (32, -0.01), (33, -0.066), (34, 0.013), (35, -0.043), (36, -0.043), (37, -0.096), (38, 0.066), (39, -0.04), (40, 0.013), (41, 0.035), (42, 0.054), (43, 0.047), (44, 0.009), (45, -0.112), (46, 0.039), (47, -0.033), (48, -0.085), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93163627 <a title="388-lsi-1" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>2 0.75249332 <a title="388-lsi-2" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>Author: Igor Labutov ; Hod Lipson</p><p>Abstract: We present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task. Recently, with an increase in computing resources, it became possible to learn rich word embeddings from massive amounts of unlabeled data. However, some methods take days or weeks to learn good embeddings, and some are notoriously difficult to train. We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with re- spect to several baselines, and observe that the approach is most useful when the training set is sufficiently small.</p><p>3 0.72389543 <a title="388-lsi-3" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>Author: Tiberiu Boros ; Radu Ion ; Dan Tufis</p><p>Abstract: Radu Ion Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy radu@ racai . ro Dan Tufi? Research Institute for ?????????? ???????????? ?????? Dr?????????? Romanian Academy tufi s @ racai . ro Networks (Marques and Lopes, 1996) and Conditional Random Fields (CRF) (Lafferty et Standard methods for part-of-speech tagging suffer from data sparseness when used on highly inflectional languages (which require large lexical tagset inventories). For this reason, a number of alternative methods have been proposed over the years. One of the most successful methods used for this task, ?????? ?????? ??????? ??????, 1999), exploits a reduced set of tags derived by removing several recoverable features from the lexicon morpho-syntactic descriptions. A second phase is aimed at recovering the full set of morpho-syntactic features. In this paper we present an alternative method to Tiered Tagging, based on local optimizations with Neural Networks and we show how, by properly encoding the input sequence in a general Neural Network architecture, we achieve results similar to the Tiered Tagging methodology, significantly faster and without requiring extensive linguistic knowledge as implied by the previously mentioned method. 1</p><p>4 0.71350074 <a title="388-lsi-4" href="./acl-2013-Adaptation_Data_Selection_using_Neural_Language_Models%3A_Experiments_in_Machine_Translation.html">35 acl-2013-Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation</a></p>
<p>Author: Kevin Duh ; Graham Neubig ; Katsuhito Sudoh ; Hajime Tsukada</p><p>Abstract: Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling un- known word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams.</p><p>5 0.70059603 <a title="388-lsi-5" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>Author: lemao liu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Most statistical machine translation (SMT) systems are modeled using a loglinear framework. Although the log-linear model achieves success in SMT, it still suffers from some limitations: (1) the features are required to be linear with respect to the model itself; (2) features cannot be further interpreted to reach their potential. A neural network is a reasonable method to address these pitfalls. However, modeling SMT with a neural network is not trivial, especially when taking the decoding efficiency into consideration. In this paper, we propose a variant of a neural network, i.e. additive neural networks, for SMT to go beyond the log-linear translation model. In addition, word embedding is employed as the input to the neural network, which encodes each word as a feature vector. Our model outperforms the log-linear translation models with/without embedding features on Chinese-to-English and Japanese-to-English translation tasks.</p><p>6 0.67956871 <a title="388-lsi-6" href="./acl-2013-Combination_of_Recurrent_Neural_Networks_and_Factored_Language_Models_for_Code-Switching_Language_Modeling.html">84 acl-2013-Combination of Recurrent Neural Networks and Factored Language Models for Code-Switching Language Modeling</a></p>
<p>7 0.57088858 <a title="388-lsi-7" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>8 0.56396008 <a title="388-lsi-8" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>9 0.5634923 <a title="388-lsi-9" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>10 0.55803454 <a title="388-lsi-10" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>11 0.54731226 <a title="388-lsi-11" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>12 0.50026846 <a title="388-lsi-12" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>13 0.5 <a title="388-lsi-13" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>14 0.48614866 <a title="388-lsi-14" href="./acl-2013-The_mathematics_of_language_learning.html">349 acl-2013-The mathematics of language learning</a></p>
<p>15 0.47505251 <a title="388-lsi-15" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>16 0.45746645 <a title="388-lsi-16" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>17 0.43789354 <a title="388-lsi-17" href="./acl-2013-Advancements_in_Reordering_Models_for_Statistical_Machine_Translation.html">40 acl-2013-Advancements in Reordering Models for Statistical Machine Translation</a></p>
<p>18 0.42578727 <a title="388-lsi-18" href="./acl-2013-Scalable_Modified_Kneser-Ney_Language_Model_Estimation.html">308 acl-2013-Scalable Modified Kneser-Ney Language Model Estimation</a></p>
<p>19 0.41610557 <a title="388-lsi-19" href="./acl-2013-Word_surprisal_predicts_N400_amplitude_during_reading.html">390 acl-2013-Word surprisal predicts N400 amplitude during reading</a></p>
<p>20 0.4148654 <a title="388-lsi-20" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.053), (6, 0.08), (11, 0.056), (15, 0.01), (24, 0.042), (26, 0.04), (28, 0.013), (35, 0.085), (42, 0.077), (48, 0.054), (67, 0.226), (70, 0.036), (88, 0.022), (90, 0.028), (95, 0.081), (97, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.84053463 <a title="388-lda-1" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>Author: Nan Yang ; Shujie Liu ; Mu Li ; Ming Zhou ; Nenghai Yu</p><p>Abstract: In this paper, we explore a novel bilingual word alignment approach based on DNN (Deep Neural Network), which has been proven to be very effective in various machine learning tasks (Collobert et al., 2011). We describe in detail how we adapt and extend the CD-DNNHMM (Dahl et al., 2012) method introduced in speech recognition to the HMMbased word alignment model, in which bilingual word embedding is discriminatively learnt to capture lexical translation information, and surrounding words are leveraged to model context information in bilingual sentences. While being capable to model the rich bilingual correspondence, our method generates a very compact model with much fewer parameters. Experiments on a large scale EnglishChinese word alignment task show that the proposed method outperforms the HMM and IBM model 4 baselines by 2 points in F-score.</p><p>2 0.82510674 <a title="388-lda-2" href="./acl-2013-Enriching_Entity_Translation_Discovery_using_Selective_Temporality.html">138 acl-2013-Enriching Entity Translation Discovery using Selective Temporality</a></p>
<p>Author: Gae-won You ; Young-rok Cha ; Jinhan Kim ; Seung-won Hwang</p><p>Abstract: This paper studies named entity translation and proposes “selective temporality” as a new feature, as using temporal features may be harmful for translating “atemporal” entities. Our key contribution is building an automatic classifier to distinguish temporal and atemporal entities then align them in separate procedures to boost translation accuracy by 6. 1%.</p><p>3 0.81017607 <a title="388-lda-3" href="./acl-2013-Deepfix%3A_Statistical_Post-editing_of_Statistical_Machine_Translation_Using_Deep_Syntactic_Analysis.html">110 acl-2013-Deepfix: Statistical Post-editing of Statistical Machine Translation Using Deep Syntactic Analysis</a></p>
<p>Author: Rudolf Rosa ; David Marecek ; Ales Tamchyna</p><p>Abstract: Deepfix is a statistical post-editing system for improving the quality of statistical machine translation outputs. It attempts to correct errors in verb-noun valency using deep syntactic analysis and a simple probabilistic model of valency. On the English-to-Czech translation pair, we show that statistical post-editing of statistical machine translation leads to an improvement of the translation quality when helped by deep linguistic knowledge.</p><p>4 0.80884445 <a title="388-lda-4" href="./acl-2013-Crowdsourcing_Interaction_Logs_to_Understand_Text_Reuse_from_the_Web.html">100 acl-2013-Crowdsourcing Interaction Logs to Understand Text Reuse from the Web</a></p>
<p>Author: Martin Potthast ; Matthias Hagen ; Michael Volske ; Benno Stein</p><p>Abstract: unkown-abstract</p><p>5 0.75292361 <a title="388-lda-5" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>Author: Sebastian Martschat</p><p>Abstract: We present an unsupervised model for coreference resolution that casts the problem as a clustering task in a directed labeled weighted multigraph. The model outperforms most systems participating in the English track of the CoNLL’ 12 shared task.</p><p>6 0.73155206 <a title="388-lda-6" href="./acl-2013-Scaling_Semi-supervised_Naive_Bayes_with_Feature_Marginals.html">309 acl-2013-Scaling Semi-supervised Naive Bayes with Feature Marginals</a></p>
<p>7 0.68648148 <a title="388-lda-7" href="./acl-2013-Additive_Neural_Networks_for_Statistical_Machine_Translation.html">38 acl-2013-Additive Neural Networks for Statistical Machine Translation</a></p>
<p>8 0.6693756 <a title="388-lda-8" href="./acl-2013-Large_tagset_labeling_using_Feed_Forward_Neural_Networks._Case_study_on_Romanian_Language.html">216 acl-2013-Large tagset labeling using Feed Forward Neural Networks. Case study on Romanian Language</a></p>
<p>9 0.6660955 <a title="388-lda-9" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>10 0.6587283 <a title="388-lda-10" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>11 0.64436591 <a title="388-lda-11" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>12 0.64350992 <a title="388-lda-12" href="./acl-2013-Re-embedding_words.html">294 acl-2013-Re-embedding words</a></p>
<p>13 0.63753641 <a title="388-lda-13" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>14 0.6367197 <a title="388-lda-14" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>15 0.63667929 <a title="388-lda-15" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>16 0.63653433 <a title="388-lda-16" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>17 0.63032728 <a title="388-lda-17" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>18 0.62902707 <a title="388-lda-18" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>19 0.62768805 <a title="388-lda-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.62727982 <a title="388-lda-20" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
