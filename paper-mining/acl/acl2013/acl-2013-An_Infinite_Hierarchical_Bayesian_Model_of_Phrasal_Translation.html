<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-46" href="#">acl2013-46</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</h1>
<br/><p>Source: <a title="acl-2013-46-pdf" href="http://aclweb.org/anthology//P/P13/P13-1077.pdf">pdf</a></p><p>Author: Trevor Cohn ; Gholamreza Haffari</p><p>Abstract: Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.</p><p>Reference: <a title="acl-2013-46-reference" href="../acl2013_reference/acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk Abstract Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. [sent-4, score-0.457]
</p><p>2 This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. [sent-5, score-0.177]
</p><p>3 This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. [sent-6, score-0.58]
</p><p>4 Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. [sent-7, score-0.227]
</p><p>5 , 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. [sent-10, score-0.132]
</p><p>6 , 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. [sent-13, score-0.269]
</p><p>7 ,  1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from Gholamreza Haffari Faculty of Information Technology Monash University Clayton, Australia re z a@monash . [sent-16, score-0.15]
</p><p>8 edu which phrasal translation units are extracted using a heuristic. [sent-17, score-0.264]
</p><p>9 This paper develops a phrase-based translation model which aims to address the above short-  comings of the phrase-based translation pipeline. [sent-23, score-0.264]
</p><p>10 Specifically, we formulate translation using inverse transduction grammar (ITG), and seek to learn an ITG from parallel corpora. [sent-24, score-0.418]
</p><p>11 The novelty of our approach is that we develop a Bayesian prior over the grammar, such that a nonterminal becomes a ‘cache’ learning each production and its complete yield, which in turn is recursively composed of its child constituents. [sent-25, score-0.232]
</p><p>12 This is closely related to adaptor grammars (Johnson et al. [sent-26, score-0.185]
</p><p>13 Our model learns translations of entire sentences while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. [sent-28, score-0.227]
</p><p>14 The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. [sent-29, score-0.133]
</p><p>15 We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling  a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). [sent-30, score-0.267]
</p><p>16 ’s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. [sent-36, score-0.162]
</p><p>17 Moreover our approach results in consistent translation improvements across a number of translation tasks compared to Neubig et al. [sent-39, score-0.264]
</p><p>18 2  Related Work  Inversion transduction grammar (or ITG) (Wu, 1997) is a well studied synchronous grammar formalism. [sent-41, score-0.418]
</p><p>19 Terminal productions of the form X → e/f generate a waol rpdr idnu tcwtioo languages, ramnd X nonterminal productions allow phrasal movement in the translation process. [sent-42, score-0.433]
</p><p>20 Our paper fits into the recent line of work forjointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al. [sent-58, score-0.135]
</p><p>21 Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al. [sent-65, score-0.142]
</p><p>22 This work was inspired by adaptor grammars  (Johnson et al. [sent-69, score-0.185]
</p><p>23 , 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. [sent-70, score-0.206]
</p><p>24 The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. [sent-71, score-0.223]
</p><p>25 Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the top-level and base distributions are explicitly linked together. [sent-73, score-0.146]
</p><p>26 As mentioned above, ours is not the first work attempting to generalise adaptor grammars for machine translation; (Neubig et al. [sent-74, score-0.185]
</p><p>27 Our approach improves upon theirs in terms of the model and inference, and critically, this is borne out in our experiments where we show uniform improvements in translation quality over a baseline system, as compared to their almost entirely negative results. [sent-76, score-0.132]
</p><p>28 We believe that their approach had a number of flaws: For inference they use a beam-search, which may  speed up processing but means that they are no longer sampling from the true distribution, nor a distribution with the same support as the posterior. [sent-77, score-0.279]
</p><p>29 Finally our approach models separately the three different types of ITG production (mono-  tone, swap and lexical emission), allowing for a richer parameterisation which the model exploits by learning different hyper-parameter values. [sent-84, score-0.335]
</p><p>30 3  Model  The generative process of the model follows that of ITG with the following simple grammar X → [X X] | hX Xi X → e/f | e/⊥ | ⊥/f , where [·] denotes monotone ordering and h·i denotes a swap ionte one language. [sent-85, score-0.57]
</p><p>31 Tghuaisg corresponds otol a si dme-ple generative story, with each stage being a nonterminal rewrite starting with X and terminating when there are no frontier non-terminals. [sent-87, score-0.14]
</p><p>32 A popular variant is a phrasal ITG, where the leaves of the ITG tree are phrase-pairs and the training seeks to learn a segmentation ofthe source and target which yields good phrases. [sent-88, score-0.172]
</p><p>33 Our approach improves over the phrasal model by recursively generating complete phrases. [sent-90, score-0.219]
</p><p>34 f{omro r = mono (a) draw the complete subtree expansion, t = X → [. [sent-95, score-0.325]
</p><p>35 for r = swap (a) draw the complete subtree expansion, t = X → h. [sent-99, score-0.46]
</p><p>36 for r = emit (a) draw a pair of strings, (e, f) ∼ E (b) sderta wt = pXai → e/f Note t(hba)t we split Xth →e problem of drawing a tree into two steps: first choosing the top-level rule type and then drawing a rule of that type. [sent-104, score-0.544]
</p><p>37 This gives us greater control than simply drawing a tree of any type from one distribution, due to our parameterisation of the priors over the model parameters TM, TS and E. [sent-105, score-0.235]
</p><p>38 3 We use Pitman-Yor Process priors for the TM and TS parameters TM  ∼  PYP(aM, bM, P1(·|r = mono))  TS  ∼  PYP(aS, bS, P1(· |r = swap))  where P1(t1, t2 |r) is a distribution over a pair of trees (the left an|dr) right dcihsiltdrirbeunt oofn a mveorn aot poanire or swap production). [sent-110, score-0.42]
</p><p>39 set t = X → [t1 t2] or t = X → ht1 t2i depending on r This generative process is mutually recursive: P2 makes draws from P1 and P1 makes draws from P2. [sent-114, score-0.146]
</p><p>40 The recursion is terminated when the rule type r = emit is drawn. [sent-115, score-0.194]
</p><p>41 3We also experimented with using word translation probabilities from IBM model 1, based on the prior used by Levenberg et al. [sent-118, score-0.186]
</p><p>42 In our case we can consider the process of drawing a tree from P2 as a customer entering a restaurant and choosing where to sit, from an infinite set of tables. [sent-123, score-0.365]
</p><p>43 The seating decision is based on the number of other customers at each table, such that popular tables are more likely to be joined than unpopular or empty ones. [sent-124, score-0.158]
</p><p>44 If the customer chooses an occupied table, the identity of the tree is then set to be the same as for the other customers also seated there. [sent-125, score-0.246]
</p><p>45 For empty tables the tree must be sampled from the base distribution P1. [sent-126, score-0.179]
</p><p>46 In the standard CRF  analogy, this leads to another customer entering the restaurant one step up in the hierarchy, and this process can be chained many times. [sent-127, score-0.171]
</p><p>47 In our case, however, every new table leads to new customers reentering the original restaurant these correspond to the left and right child trees of a monotone or swap rule. [sent-128, score-0.631]
</p><p>48 , a draw from P2) under the model is P2(t) = P(r)P2(t|r)  (1)  where r is the rule type, one of mono, swap or emit. [sent-133, score-0.426]
</p><p>49 The distribution over types, P(r), is defined as  P(r) =nnrTT,,−−++ b bTT31 where nT,− are the counts over rules of types. [sent-134, score-0.153]
</p><p>50 Fo(rt r = mono or r = swap rules, it is defined as  P2(t|r) =nt−,nr−r−+ K bt−,rrar+Knr−r−a+r+ br brP1(t1,t2|r), (2) where nt−,r is the count for tree t in the other training sentences, Kt−,r is the table count for t and nr− 4The conditioning omitted for clarity. [sent-136, score-0.527]
</p><p>51 erivation we still need to define P1, which is formulated as  P1(t1, t2) = P2(t1)P2(t2|t1)  ,  where the conditioning ofthe second recursive call to P2 reflects that the counts n− and K−may be affected by the first draw from P2. [sent-140, score-0.178]
</p><p>52 5 We construct an approximating ITG following the technique used for sampling trees from monolingual tree-substitution grammars (Cohn et al. [sent-146, score-0.331]
</p><p>53 , during inside inference –  –  we recover P2 as shown in (2). [sent-151, score-0.148]
</p><p>54 The full grammar transform for inside inference is shown in Table 1. [sent-152, score-0.254]
</p><p>55 The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs (Johnson et al. [sent-153, score-0.324]
</p><p>56 This involves first constructing the inside lattice using the productions in Table 1, and then performing a top-down sampling pass. [sent-156, score-0.319]
</p><p>57 The function sig(t) returns a unique identifier for the complete tree t, and the function yield(t) returns the pair of terminal strings from the yield of t. [sent-160, score-0.229]
</p><p>58 6 Accepted samples then replace the old tree (otherwise the old tree is retained) and the model counts are incremented. [sent-162, score-0.21]
</p><p>59 Tlihseh corpora )s,ta atisntdics A orafb tihc→eseE translation tasks are summarised in Table 2. [sent-165, score-0.132]
</p><p>60 The UR-EN corpus comes from NIST 2009 translation evaluation. [sent-166, score-0.132]
</p><p>61 9 Sampler configuration Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method  (Blunsom et al. [sent-186, score-0.208]
</p><p>62 This algorithm represents the translation of a sentence as a large SCFG rule, which it then factorises into lower rank SCFG rules, a process akin to rule binarisation commonly used in SCFG decoding. [sent-188, score-0.203]
</p><p>63 After each full sampling iteration, we resample all the hyper-parameters using slice-sampling, with the following priors: a ∼ Beta(1, 1), b ∼ Gamma(10, 0. [sent-191, score-0.162]
</p><p>64 Figure 1 sh∼ows the posterbio ∼r probability improves gwuriteh 1ea schho wfusll t sampling iterations. [sent-193, score-0.162]
</p><p>65 The sampling was repeated for 5 independent runs, and we present results where we combine the outputs of these runs. [sent-196, score-0.162]
</p><p>66 The time complexity of our inference algorithm is  O(n6),  which can be prohibitive for large scale  machine translation tasks. [sent-198, score-0.208]
</p><p>67 plexity consider  by constraining  We reduce the com-  the inside inference  only derivations  to  which are compatible  9Hence the BLEU scores we get for the baselines may appear lower than what reported in the literature. [sent-199, score-0.148]
</p><p>68 10Using the factorised alignments directly in a translation system resulted in a slight loss in BLEU versus using the unfactorised alignments. [sent-200, score-0.288]
</p><p>69 784  0  100  200  300  400  500  iteration  Figure 1: Training progress on the UR-EN corpus, showing the posterior probability improving with each full sampling iteration. [sent-202, score-0.228]
</p><p>70 average sentence length  Figure 2: The runtime cost of bottom-up inside inference and top-down sampling as a function of sentence length (UR-EN), with time shown on a logarithmic scale. [sent-204, score-0.31]
</p><p>71 Full ITG inference is shown  ×  with red circles, and restricted inference using the intersection constraints with blue triangles. [sent-205, score-0.152]
</p><p>72 11 Figure 2 shows the sampling time with respect to the average sentence length, showing that our alignment-constrained sampling algorithm is better than the unconstrained algorithm with empirical complexity of n4. [sent-208, score-0.324]
</p><p>73 Presumably other means of inference may be more efficient, such as Gibbs sampling (Levenberg et al. [sent-210, score-0.238]
</p><p>74 , 2012) or auxiliary variable sampling (Blunsom and Cohn, 2010); we leave these extensions to future work. [sent-211, score-0.162]
</p><p>75 , 2011), we evaluate our model by using its output word alignments to construct a phrase table. [sent-215, score-0.142]
</p><p>76 This alignment is used as input to the rule factorisation algorithm, producing the ITG trees with which we initialise our sampler. [sent-218, score-0.229]
</p><p>77 14 In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. [sent-221, score-0.132]
</p><p>78 The baselines are GIZA++ alignments and those generated by the pialign (Neubig et al. [sent-242, score-0.22]
</p><p>79 rule frequency  Figure 3: Fraction of rules with a given frequency, using a single sample grammar (UR-EN). [sent-244, score-0.251]
</p><p>80 1 Results Table 3 shows the BLEU scores for the three translation tasks UR/AR/FA→EN based on our method against tshkes UbaRse/AliRne/Fs. [sent-246, score-0.132]
</p><p>81 We believe this type of Monte Carlo model averaging should be considered in general when sampling techniques are employed for grammatical inference, e. [sent-250, score-0.162]
</p><p>82 Figure 3 shows the fraction of rules with a given frequency for each of the three rule types. [sent-258, score-0.145]
</p><p>83 As expected, there is a higher tendency to reuse high-frequency emissions (or single-word translation) compared to other rule types, which are the basic building blocks to compose larger rules (or phrases). [sent-260, score-0.145]
</p><p>84 Table 4 lists the high frequency monotone and swap rules in the learned grammar. [sent-261, score-0.538]
</p><p>85 We observe the high frequency swap rules capture reordering in verb clusters, preposition-noun inversions and adjective-noun reordering. [sent-262, score-0.362]
</p><p>86 Similar patterns are seen in the monotone rules, along with some common canned phrases. [sent-263, score-0.223]
</p><p>87 Note that “in Iraq” appears twice, once as an inversion in UR-EN and another time in monotone order for AR-EN. [sent-264, score-0.272]
</p><p>88 There is very little spread in the inferred  values, suggesting the sampling chains may have converged. [sent-267, score-0.162]
</p><p>89 Furthermore, there is a large difference between the learned hyper-parameters for the monotone rules versus the swap rules. [sent-268, score-0.538]
</p><p>90 Table 5:  phrase pairs in the top-100 high frequency phrase pairs specific our method vs that of pialign for FA-EN and AR-EN translation tasks. [sent-2508, score-0.33]
</p><p>91 If the number of observed monotone and swap rules were equal,  then there would be a higher chance in reusing the monotone rules. [sent-2511, score-0.714]
</p><p>92 However, the number of observed monotone and swap rules are not equal, as plotted in Figure 4. [sent-2512, score-0.538]
</p><p>93 Conclusions We have presented a novel method for learning a phrase-based model of translation directly from parallel data which we have framed as learn-  ing an inverse transduction grammar (ITG) using a recursive Bayesian prior. [sent-2519, score-0.491]
</p><p>94 This has led to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. [sent-2520, score-0.227]
</p><p>95 We have presented a Metropolis-Hastings sampling algorithm for blocked inference in our non-parametric ITG. [sent-2521, score-0.238]
</p><p>96 05 650 506 0 am and as  bm and bs  be  bt  (a)  0. [sent-2534, score-0.153]
</p><p>97 8 40 1760 monotone  swap  (b)  Figure 4: (a) Posterior over the hyper-parameters, aM, aS, bM, bS, bE, bT, measured for UR-EN using samples 400–500 for 3 independent sampling chains, and the intersection constraints. [sent-2539, score-0.626]
</p><p>98 (b) Posterior over the number of monotone and swap rules in the resultant grammars. [sent-2540, score-0.538]
</p><p>99 The distribution for emission rules was also peaked about 147k rules. [sent-2541, score-0.164]
</p><p>100 Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. [sent-3796, score-0.344]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('itg', 0.401), ('swap', 0.288), ('neubig', 0.186), ('monotone', 0.176), ('sampling', 0.162), ('mono', 0.153), ('translation', 0.132), ('transduction', 0.132), ('emit', 0.123), ('levenberg', 0.118), ('pialign', 0.118), ('adaptor', 0.117), ('blunsom', 0.115), ('grammar', 0.106), ('bayesian', 0.105), ('alignments', 0.102), ('inversion', 0.096), ('terminating', 0.095), ('tree', 0.086), ('phrasal', 0.086), ('productions', 0.085), ('ts', 0.083), ('scfg', 0.08), ('sig', 0.078), ('inference', 0.076), ('rules', 0.074), ('cohn', 0.074), ('synchronous', 0.074), ('phil', 0.073), ('draws', 0.073), ('recursive', 0.073), ('inside', 0.072), ('recursively', 0.072), ('tm', 0.071), ('rule', 0.071), ('giza', 0.07), ('monte', 0.069), ('grammars', 0.068), ('draw', 0.067), ('posterior', 0.066), ('monash', 0.066), ('scfgs', 0.066), ('drawing', 0.063), ('sampler', 0.063), ('restaurant', 0.063), ('complete', 0.061), ('denero', 0.059), ('cherry', 0.059), ('trevor', 0.058), ('bm', 0.056), ('johnson', 0.055), ('prior', 0.054), ('customer', 0.054), ('nt', 0.054), ('entering', 0.054), ('factorisation', 0.054), ('factorised', 0.054), ('pilevar', 0.054), ('seated', 0.054), ('seating', 0.054), ('customers', 0.052), ('tables', 0.052), ('alignment', 0.052), ('trees', 0.052), ('bs', 0.049), ('emission', 0.049), ('approximating', 0.049), ('bt', 0.048), ('parallel', 0.048), ('carlo', 0.048), ('parameterisation', 0.047), ('canned', 0.047), ('phrasepairs', 0.047), ('pyp', 0.047), ('tehran', 0.047), ('units', 0.046), ('modelling', 0.045), ('infinite', 0.045), ('nonterminal', 0.045), ('translations', 0.044), ('terminal', 0.044), ('subtree', 0.044), ('iid', 0.044), ('inducing', 0.043), ('decomposition', 0.042), ('marcu', 0.042), ('farsi', 0.041), ('pauls', 0.041), ('distribution', 0.041), ('bleu', 0.041), ('phrases', 0.041), ('phrase', 0.04), ('priors', 0.039), ('rewrites', 0.039), ('yield', 0.038), ('sharon', 0.038), ('counts', 0.038), ('koehn', 0.038), ('substructures', 0.037), ('urdu', 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="46-tfidf-1" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>Author: Trevor Cohn ; Gholamreza Haffari</p><p>Abstract: Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.</p><p>2 0.34259322 <a title="46-tfidf-2" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>Author: Conghui Zhu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.</p><p>3 0.1947455 <a title="46-tfidf-3" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>Author: Yang Feng ; Trevor Cohn</p><p>Abstract: Most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phraseinternal translation and reordering. However phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. We propose a new model to address this imbalance, based on a word-based Markov model of translation which generates target translations left-to-right. Our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU.</p><p>4 0.18082234 <a title="46-tfidf-4" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>Author: Sujith Ravi</p><p>Abstract: In this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as side-information used for estimating translation models. In order to perform fast, efficient Bayesian inference in this framework, we then derive a hash sampling strategy that is inspired by the work of Ahmed et al. (2012). The new translation hash sampler enables us to scale elegantly to complex models (for the first time) and large vocab- ulary/corpora sizes. We show empirical results on the OPUS data—our method yields the best BLEU scores compared to existing approaches, while achieving significant computational speedups (several orders faster). We also report for the first time—BLEU score results for a largescale MT task using only non-parallel data (EMEA corpus).</p><p>5 0.16586111 <a title="46-tfidf-5" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>Author: Graham Neubig</p><p>Abstract: In this paper we describe Travatar, a forest-to-string machine translation (MT) engine based on tree transducers. It provides an open-source C++ implementation for the entire forest-to-string MT pipeline, including rule extraction, tuning, decoding, and evaluation. There are a number of options for model training, and tuning includes advanced options such as hypergraph MERT, and training of sparse features through online learning. The training pipeline is modeled after that of the popular Moses decoder, so users familiar with Moses should be able to get started quickly. We perform a validation experiment of the decoder on EnglishJapanese machine translation, and find that it is possible to achieve greater accuracy than translation using phrase-based and hierarchical-phrase-based translation. As auxiliary results, we also compare different syntactic parsers and alignment techniques that we tested in the process of developing the decoder. Travatar is available under the LGPL at http : / /phont ron . com/t ravat ar</p><p>6 0.15433933 <a title="46-tfidf-6" href="./acl-2013-The_effect_of_non-tightness_on_Bayesian_estimation_of_PCFGs.html">348 acl-2013-The effect of non-tightness on Bayesian estimation of PCFGs</a></p>
<p>7 0.14743499 <a title="46-tfidf-7" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>8 0.13907175 <a title="46-tfidf-8" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>9 0.13130337 <a title="46-tfidf-9" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>10 0.12972029 <a title="46-tfidf-10" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>11 0.12650184 <a title="46-tfidf-11" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>12 0.12346756 <a title="46-tfidf-12" href="./acl-2013-Semantic_Roles_for_String_to_Tree_Machine_Translation.html">314 acl-2013-Semantic Roles for String to Tree Machine Translation</a></p>
<p>13 0.120777 <a title="46-tfidf-13" href="./acl-2013-Cut_the_noise%3A_Mutually_reinforcing_reordering_and_alignments_for_improved_machine_translation.html">101 acl-2013-Cut the noise: Mutually reinforcing reordering and alignments for improved machine translation</a></p>
<p>14 0.11632863 <a title="46-tfidf-14" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>15 0.11410104 <a title="46-tfidf-15" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>16 0.10904408 <a title="46-tfidf-16" href="./acl-2013-Integrating_Translation_Memory_into_Phrase-Based_Machine_Translation_during_Decoding.html">201 acl-2013-Integrating Translation Memory into Phrase-Based Machine Translation during Decoding</a></p>
<p>17 0.10738189 <a title="46-tfidf-17" href="./acl-2013-Word_Alignment_Modeling_with_Context_Dependent_Deep_Neural_Network.html">388 acl-2013-Word Alignment Modeling with Context Dependent Deep Neural Network</a></p>
<p>18 0.10631884 <a title="46-tfidf-18" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>19 0.10526869 <a title="46-tfidf-19" href="./acl-2013-Bilingual_Data_Cleaning_for_SMT_using_Graph-based_Random_Walk.html">68 acl-2013-Bilingual Data Cleaning for SMT using Graph-based Random Walk</a></p>
<p>20 0.10266726 <a title="46-tfidf-20" href="./acl-2013-Integrating_Phrase-based_Reordering_Features_into_a_Chart-based_Decoder_for_Machine_Translation.html">200 acl-2013-Integrating Phrase-based Reordering Features into a Chart-based Decoder for Machine Translation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.28), (1, -0.176), (2, 0.144), (3, 0.093), (4, -0.07), (5, 0.042), (6, 0.05), (7, 0.006), (8, -0.051), (9, -0.016), (10, 0.029), (11, -0.036), (12, 0.045), (13, -0.052), (14, -0.026), (15, -0.112), (16, 0.083), (17, 0.075), (18, 0.012), (19, -0.039), (20, -0.01), (21, -0.015), (22, 0.055), (23, 0.071), (24, -0.032), (25, -0.061), (26, -0.033), (27, 0.007), (28, 0.075), (29, 0.039), (30, 0.071), (31, 0.024), (32, -0.008), (33, -0.033), (34, 0.023), (35, 0.09), (36, 0.039), (37, -0.036), (38, -0.036), (39, 0.016), (40, 0.001), (41, 0.0), (42, -0.059), (43, 0.013), (44, 0.068), (45, -0.011), (46, -0.1), (47, 0.06), (48, -0.034), (49, 0.13)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9441551 <a title="46-lsi-1" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>Author: Trevor Cohn ; Gholamreza Haffari</p><p>Abstract: Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.</p><p>2 0.76117611 <a title="46-lsi-2" href="./acl-2013-The_effect_of_non-tightness_on_Bayesian_estimation_of_PCFGs.html">348 acl-2013-The effect of non-tightness on Bayesian estimation of PCFGs</a></p>
<p>Author: Shay B. Cohen ; Mark Johnson</p><p>Abstract: Probabilistic context-free grammars have the unusual property of not always defining tight distributions (i.e., the sum of the “probabilities” of the trees the grammar generates can be less than one). This paper reviews how this non-tightness can arise and discusses its impact on Bayesian estimation of PCFGs. We begin by presenting the notion of “almost everywhere tight grammars” and show that linear CFGs follow it. We then propose three different ways of reinterpreting non-tight PCFGs to make them tight, show that the Bayesian estimators in Johnson et al. (2007) are correct under one of them, and provide MCMC samplers for the other two. We conclude with a discussion of the impact of tightness empirically.</p><p>3 0.73858041 <a title="46-lsi-3" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>Author: Wenduan Xu ; Yue Zhang ; Philip Williams ; Philipp Koehn</p><p>Abstract: We present a context-sensitive chart pruning method for CKY-style MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.</p><p>4 0.72890657 <a title="46-lsi-4" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>Author: Conghui Zhu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.</p><p>5 0.72152972 <a title="46-lsi-5" href="./acl-2013-A_Markov_Model_of_Machine_Translation_using_Non-parametric_Bayesian_Inference.html">10 acl-2013-A Markov Model of Machine Translation using Non-parametric Bayesian Inference</a></p>
<p>Author: Yang Feng ; Trevor Cohn</p><p>Abstract: Most modern machine translation systems use phrase pairs as translation units, allowing for accurate modelling of phraseinternal translation and reordering. However phrase-based approaches are much less able to model sentence level effects between different phrase-pairs. We propose a new model to address this imbalance, based on a word-based Markov model of translation which generates target translations left-to-right. Our model encodes word and phrase level phenomena by conditioning translation decisions on previous decisions and uses a hierarchical Pitman-Yor Process prior to provide dynamic adaptive smoothing. This mechanism implicitly supports not only traditional phrase pairs, but also gapping phrases which are non-consecutive in the source. Our experiments on Chinese to English and Arabic to English translation show consistent improvements over competitive baselines, of up to +3.4 BLEU.</p><p>6 0.71610695 <a title="46-lsi-6" href="./acl-2013-Travatar%3A_A_Forest-to-String_Machine_Translation_Engine_based_on_Tree_Transducers.html">361 acl-2013-Travatar: A Forest-to-String Machine Translation Engine based on Tree Transducers</a></p>
<p>7 0.71249151 <a title="46-lsi-7" href="./acl-2013-Shallow_Local_Multi-Bottom-up_Tree_Transducers_in_Statistical_Machine_Translation.html">320 acl-2013-Shallow Local Multi-Bottom-up Tree Transducers in Statistical Machine Translation</a></p>
<p>8 0.71247989 <a title="46-lsi-8" href="./acl-2013-Scalable_Decipherment_for_Machine_Translation_via_Hash_Sampling.html">307 acl-2013-Scalable Decipherment for Machine Translation via Hash Sampling</a></p>
<p>9 0.69699669 <a title="46-lsi-9" href="./acl-2013-A_Novel_Graph-based_Compact_Representation_of_Word_Alignment.html">15 acl-2013-A Novel Graph-based Compact Representation of Word Alignment</a></p>
<p>10 0.66551131 <a title="46-lsi-10" href="./acl-2013-Exact_Maximum_Inference_for_the_Fertility_Hidden_Markov_Model.html">143 acl-2013-Exact Maximum Inference for the Fertility Hidden Markov Model</a></p>
<p>11 0.65538949 <a title="46-lsi-11" href="./acl-2013-A_Multi-Domain_Translation_Model_Framework_for_Statistical_Machine_Translation.html">11 acl-2013-A Multi-Domain Translation Model Framework for Statistical Machine Translation</a></p>
<p>12 0.65499675 <a title="46-lsi-12" href="./acl-2013-Semantic_Parsing_as_Machine_Translation.html">312 acl-2013-Semantic Parsing as Machine Translation</a></p>
<p>13 0.65400422 <a title="46-lsi-13" href="./acl-2013-Training_Nondeficient_Variants_of_IBM-3_and_IBM-4_for_Word_Alignment.html">354 acl-2013-Training Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment</a></p>
<p>14 0.63925844 <a title="46-lsi-14" href="./acl-2013-Stacking_for_Statistical_Machine_Translation.html">328 acl-2013-Stacking for Statistical Machine Translation</a></p>
<p>15 0.63851982 <a title="46-lsi-15" href="./acl-2013-General_binarization_for_parsing_and_translation.html">165 acl-2013-General binarization for parsing and translation</a></p>
<p>16 0.62993896 <a title="46-lsi-16" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>17 0.62932861 <a title="46-lsi-17" href="./acl-2013-Can_Markov_Models_Over_Minimal_Translation_Units_Help_Phrase-Based_SMT%3F.html">77 acl-2013-Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</a></p>
<p>18 0.61400473 <a title="46-lsi-18" href="./acl-2013-A_Tightly-coupled_Unsupervised_Clustering_and_Bilingual_Alignment_Model_for_Transliteration.html">25 acl-2013-A Tightly-coupled Unsupervised Clustering and Bilingual Alignment Model for Transliteration</a></p>
<p>19 0.61236608 <a title="46-lsi-19" href="./acl-2013-Learning_a_Phrase-based_Translation_Model_from_Monolingual_Data_with_Application_to_Domain_Adaptation.html">223 acl-2013-Learning a Phrase-based Translation Model from Monolingual Data with Application to Domain Adaptation</a></p>
<p>20 0.60093576 <a title="46-lsi-20" href="./acl-2013-Stem_Translation_with_Affix-Based_Rule_Selection_for_Agglutinative_Languages.html">330 acl-2013-Stem Translation with Affix-Based Rule Selection for Agglutinative Languages</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.065), (6, 0.04), (9, 0.153), (11, 0.06), (24, 0.039), (26, 0.059), (28, 0.012), (35, 0.118), (42, 0.062), (48, 0.049), (70, 0.079), (77, 0.033), (88, 0.031), (90, 0.056), (95, 0.061)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8910768 <a title="46-lda-1" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>Author: Lu Wang ; Claire Cardie</p><p>Abstract: We address the challenge of generating natural language abstractive summaries for spoken meetings in a domain-independent fashion. We apply Multiple-Sequence Alignment to induce abstract generation templates that can be used for different domains. An Overgenerateand-Rank strategy is utilized to produce and rank candidate abstracts. Experiments using in-domain and out-of-domain training on disparate corpora show that our system uniformly outperforms state-of-the-art supervised extract-based approaches. In addition, human judges rate our system summaries significantly higher than compared systems in fluency and overall quality.</p><p>same-paper 2 0.8712765 <a title="46-lda-2" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>Author: Trevor Cohn ; Gholamreza Haffari</p><p>Abstract: Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.</p><p>3 0.78100991 <a title="46-lda-3" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>Author: Wen-tau Yih ; Ming-Wei Chang ; Christopher Meek ; Andrzej Pastusiak</p><p>Abstract: In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.</p><p>4 0.77329975 <a title="46-lda-4" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>Author: Richard Socher ; John Bauer ; Christopher D. Manning ; Ng Andrew Y.</p><p>Abstract: Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.</p><p>5 0.76988763 <a title="46-lda-5" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>Author: Wei Xu ; Raphael Hoffmann ; Le Zhao ; Ralph Grishman</p><p>Abstract: Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus. However, previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases. To tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features. We adapt the information retrieval technique of pseudo- relevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments.</p><p>6 0.76946241 <a title="46-lda-6" href="./acl-2013-Generalizing_Image_Captions_for_Image-Text_Parallel_Corpus.html">167 acl-2013-Generalizing Image Captions for Image-Text Parallel Corpus</a></p>
<p>7 0.76875114 <a title="46-lda-7" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>8 0.76805723 <a title="46-lda-8" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>9 0.76782316 <a title="46-lda-9" href="./acl-2013-A_Context_Free_TAG_Variant.html">4 acl-2013-A Context Free TAG Variant</a></p>
<p>10 0.76666534 <a title="46-lda-10" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>11 0.76626849 <a title="46-lda-11" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>12 0.76387036 <a title="46-lda-12" href="./acl-2013-Models_of_Semantic_Representation_with_Visual_Attributes.html">249 acl-2013-Models of Semantic Representation with Visual Attributes</a></p>
<p>13 0.76351535 <a title="46-lda-13" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>14 0.76350105 <a title="46-lda-14" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>15 0.76294714 <a title="46-lda-15" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>16 0.76095492 <a title="46-lda-16" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>17 0.76049381 <a title="46-lda-17" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>18 0.76049286 <a title="46-lda-18" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>19 0.75906074 <a title="46-lda-19" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>20 0.75902849 <a title="46-lda-20" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
