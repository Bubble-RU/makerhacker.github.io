<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-331" href="#">acl2013-331</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</h1>
<br/><p>Source: <a title="acl-2013-331-pdf" href="http://aclweb.org/anthology//P/P13/P13-1028.pdf">pdf</a></p><p>Author: David Marecek ; Milan Straka</p><p>Abstract: Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direction), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Dependency Model with Valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks.</p><p>Reference: <a title="acl-2013-331-reference" href="../acl2013_reference/acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 c z  Abstract Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. [sent-4, score-0.222]
</p><p>2 In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direction), which is obtained from a large raw corpus using the reducibility principle. [sent-5, score-0.323]
</p><p>3 1 Introduction  The task of unsupervised dependency parsing (which strongly relates to the grammar induction task) has become popular in the last decade, and its quality has been greatly increasing during this period. [sent-7, score-0.356]
</p><p>4 1 Current attachment scores of state-of-the-art unsupervised parsers are higher than 50% for many languages (Spitkovsky et al. [sent-9, score-0.153]
</p><p>5 Moreover, 1The adjacent-word baseline is a dependency tree in which each word is attached to the previous (or the following) word. [sent-12, score-0.2]
</p><p>6 supervised parsers always only simulate the treebanks they were trained on, whereas unsupervised parsers have an ability to be fitted to different particular applications. [sent-15, score-0.199]
</p><p>7 Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS  tags marking the verbs, which renders them useless when unsupervised POS tags are employed. [sent-21, score-0.168]
</p><p>8 The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV. [sent-22, score-0.237]
</p><p>9 The estimation is done using the reducibility principle introduced in (Mare ˇcek and Zˇabokrtsk y´, 2012). [sent-23, score-0.228]
</p><p>10 The reducibility principle postulates that if a word (or a sequence of words) can be removed from a sentence without violating its grammatical correctness, it is a leaf (or a subtree) in its dependency structure. [sent-24, score-0.378]
</p><p>11 For the purposes of this paper, we assume the following hypothesis: If a sequence of words can be removed from 2The Pstop probability may be conditioned by additional parameters, such as adjacency adj or fringe word cf, which will be described in Section 4. [sent-25, score-0.37]
</p><p>12 Our hypothesis is a generalization of the original hypothesis since it allows a reducible sequence to form several adjacent subtrees. [sent-31, score-0.402]
</p><p>13 Sequences of reducible words are marked by thick lines below the sentence. [sent-34, score-0.364]
</p><p>14 Therefore, we can deduce that the Pstop probability for such word is high both for the left and for the right direction. [sent-37, score-0.145]
</p><p>15 The phrase “for further discussions ” is reducible as well and we can deduce that the Pstop of its first word ( “for”) in the left direction is high since it cannot have any left children. [sent-38, score-0.543]
</p><p>16 Similarly, the word “discussions”, which is the last word in this sequence, cannot have any right children and we can estimate that its right Pstop probability is high. [sent-40, score-0.294]
</p><p>17 The most difficult task in this approach is to automatically recognize reducible sequences. [sent-42, score-0.364]
</p><p>18 2  Related Work  Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recognizing dependency relations. [sent-48, score-0.301]
</p><p>19 We have directly utilized the aforementioned criteria for dependency relations in unsupervised dependency parsing in our previous paper (Mare ˇcek and Zˇabokrtsk y´, 2012). [sent-57, score-0.354]
</p><p>20 Our depen-  dency model contained a submodel which directly prioritized subtrees that form reducible sequences of POS tags. [sent-58, score-0.442]
</p><p>21 Reducibility scores of given POS tag sequences were estimated using a large corpus of Wikipedia articles. [sent-59, score-0.174]
</p><p>22 The weakness of this approach was the fact that longer sequences of POS tags are very sparse and no reducibility scores could be estimated for them. [sent-60, score-0.367]
</p><p>23 In this paper, we avoid this shortcoming by estimating the STOP probabilities for individual POS tags only. [sent-61, score-0.167]
</p><p>24 Another task related to reducibility is sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008), which was used for text summarization. [sent-62, score-0.182]
</p><p>25 Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. [sent-65, score-0.235]
</p><p>26 Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. [sent-70, score-0.158]
</p><p>27 282  Other approaches to unsupervised dependency parsing were described e. [sent-76, score-0.235]
</p><p>28 ” is reducible since the same sentence without this bigram, i. [sent-91, score-0.364]
</p><p>29 It is apparent that only very few reducible sequences can be found by this procedure. [sent-96, score-0.442]
</p><p>30 If we use a corpus containing about 10,000 sentences, it is possible that we found no reducible sequences at all. [sent-97, score-0.442]
</p><p>31 However, we managed to find a sufficient amount of reducible sequences in corpora containing millions of sentences, see Section 6. [sent-98, score-0.442]
</p><p>32 2  Computing the STOP-probability estimations  Recall our hypothesis from Section 1: If a sequence of words is reducible, no word outside the sequence can depend on any word in the sequence. [sent-101, score-0.124]
</p><p>33 Or, in terms of dependency structure: A reducible sequence consists of one or more adjacent subtrees. [sent-102, score-0.521]
</p><p>34 This means that the first word of a reducible sequence does not have any left children and, similarly, the last word in a reducible sequence does  tocMhnaelmyrotipn5ve9tFriopaulornicWatsodretalhdwi sCawuyspefrsioxtkmeahn t,dhmienatFgsriLen,l taechlitnam onaugmnheisnrSilsven acNndo arsdwtena. [sent-103, score-1.006]
</p><p>35 ae1Aufs5xmr7tthnbadnu Figure 2: Example of reducible sequences words found in a large corpus. [sent-114, score-0.442]
</p><p>36 Hereinafter, Psetsotp (ch, dir) denotes the STOPprobability we want to estimate from a large corpus; ch is the head’s POS tag and dir is the direction in which the STOP probability is estimated. [sent-117, score-0.766]
</p><p>37 If ch is very often in the first position of reducible sequences, Psetsotp (ch, left) will be high. [sent-118, score-0.628]
</p><p>38 Similarly, if ch is often in the last position of reducible sequences, Psetsotp (ch, right) will be high. [sent-119, score-0.665]
</p><p>39 For each POS tag ch in the given corpus, we first compute its left and right “raw” score Sstop (ch, left) and Sstop (ch, right) as the relative number of times a word with POS tag ch was in the first (or last) position in a reducible sequence found in the corpus. [sent-120, score-1.094]
</p><p>40 It may happen that for many POS tags there  are no reducible sequences found. [sent-134, score-0.488]
</p><p>41 Since reducible sequences found are very sparse, the values of Sstop (ch, dir) scores are very small. [sent-137, score-0.442]
</p><p>42 To convert them to estimated probabilities Psetsotp (ch, dir), we need a smoothing that fulfills the following properties: (1) Psetsotp is a probability and therefore its value must be between 0 and 1. [sent-138, score-0.232]
</p><p>43 The number of stop decisions is 2W since they come after generating the last children in both the directions. [sent-140, score-0.378]
</p><p>44 Then, for each head, all its left children are generated, then the left STOP, then all its right children, and then the right STOP. [sent-149, score-0.304]
</p><p>45 When deciding whether to generate another child in the direction dir or the STOP symbol, we use the Psdtmopv (STOP|ch, dir, adj , cf) model. [sent-151, score-0.626]
</p><p>46 The new child cd in the direction dir is generated according to the Pchoose (cd|ch, dir) model. [sent-152, score-0.589]
</p><p>47 Our Psdtmopv depends on the head POS tag ch, direction dir, adjacency adj, and fringe POS tag cf (described below). [sent-155, score-0.442]
</p><p>48 That is, Psdtmopv (ch, dir, adj = 1, cf) decides whether the word ch has any children in the direction dir at all, whereas Psdtmopv (h, dir, adj = 0, cf) decides whether another child will be generated next to the already generated one. [sent-157, score-1.135]
</p><p>49 This distinction is of crucial importance for us: although we know how to estimate the STOP probabilities for adj = 1  from large data, we do not know anything about the STOP probabilities for adj = 0. [sent-158, score-0.444]
</p><p>50 The last factor cf, called fringe, is the POS tag of the previously generated sibling in the current direction dir. [sent-159, score-0.153]
</p><p>51 If there is no such sibling (in case adj = 1), the head ch is used as the fringe cf. [sent-160, score-0.519]
</p><p>52 Similarly, count−(ch, dir) is the number of times something has been attached to ch in the direction dir. [sent-166, score-0.387]
</p><p>53 3| The STOP probability is computed in a similar way:  Psdtmopv (STOP|ch, dir, adj, cf) = =αs23+ count−(STOP,ch,dir,adj,cf) + count−(ch, dir, adj , cf) αs  where count−(STOP, ch, dir, adj , cf) is the number of times a head ch had the last child cf in the direction dir in the history. [sent-168, score-1.325]
</p><p>54 Therefore, we introduce a new model  Psdtompv+est,  in which the probability based on the previously generated data is linearly combined with the probability estimates based on large corpora (Section 3). [sent-170, score-0.141]
</p><p>55 The definition of the for adj = 0 equals Finally, we obtain the probability of the whole generated treebank as a product over the trees:  Psdtmopv+est  Ptreebank  = Y  Ptree(T). [sent-172, score-0.225]
</p><p>56 Unlike in (Mare ˇcek and Zˇabokrtsk y´, 2012), where edges were sampled individually, we sample whole trees from all possibilities on a given sentence using dynamic programming. [sent-178, score-0.13]
</p><p>57 A random projective dependency tree is assigned to each sentence in the corpus. [sent-180, score-0.188]
</p><p>58 For each sentence, we sample a new dependency tree based on all other trees that are currently in the corpus. [sent-183, score-0.251]
</p><p>59 Parsing: Based on the collected counts, we compute the final dependency trees using the Chu-Liu/Edmonds’ algorithm (1965) for finding maximum directed spanning trees. [sent-190, score-0.175]
</p><p>60 1 Sampling Our goal is to sample a new projective dependency tree T with probability proportional to Ptree (T). [sent-192, score-0.276]
</p><p>61 , probabilities  285  of edges of T are independent. [sent-203, score-0.13]
</p><p>62 There is a standard way to sample using the real Ptree (T) we can use Pt0ree (T) as a proposal distribution in the Metropolis-Hastings algorithm (Hastings, 1970), –  which then produces trees with probabilities proportional to Ptree (T) using acceptance-rejection scheme. [sent-204, score-0.186]
</p><p>63 , wN with probability proportional to Pt0ree (T), we first compute three tables: • ti(g, i,j) for g < ior g > j is the sum of probabilities of any tree on words wi, . [sent-209, score-0.183]
</p><p>64 , wj whose root is a child of wg, but not an outermost child in its direction; • to(g, i,j) is the same, but the tree is the outetrmost child of wg; • fo(g, i,j) for g < i or g > j is the sum of probabilities of any forest on words wi, . [sent-212, score-0.437]
</p><p>65 , wj, such that all the trees are children of wg and are the outermost children of wg in their direction. [sent-215, score-0.417]
</p><p>66 At first, we sample the root r proportionally to the probability of a tree with the root r, which is a product of the probability of left children of r and right children of r. [sent-219, score-0.606]
</p><p>67 The probability of left children of r is either Ps0top (STOP|r, left) if r has no children, or Ps0top (¬STOP|r, left)fo(r, 1, r 1) otherwise; the probability oPf right tc)hfild(rre,n1 ,isr analogous. [sent-220, score-0.312]
</p><p>68 We sample the first left child range l1proportionally either to to(r, 1, r− 1) if l1 = 1, or to ti(r, l1, r 1)fo(r, 1, l1 1) if l1 > 1. [sent-222, score-0.167]
</p><p>69 Then we sample −the 1 )sefcond left −chi 1ld) range l2 proportionally either to to(r, 1, l1 1) if l2 = 1, or to ti(r, l2, l1 1)fo(r, 1, l2 1) if l2 > 1, and so on, while− t 1h)efre are any le 1f)t children. [sent-223, score-0.13]
</p><p>70 2 Parsing Beginning the 500th iteration, we start collecting counts of individual dependency edges during the remaining iterations. [sent-230, score-0.156]
</p><p>71 After each iteration is finished (all the trees in the corpus are re-sampled), we increment the counter of all directed pairs of nodes which are connected by a dependency edge in the current trees. [sent-231, score-0.175]
</p><p>72 Even if we average the strictly projective dependency trees, some non-projective edges may appear in the result. [sent-234, score-0.186]
</p><p>73 As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees. [sent-240, score-0.155]
</p><p>74 In case a punctuation node was not a leaf, its children are attached to the parent of the removed node. [sent-241, score-0.197]
</p><p>75 We employed the TnT tagger (Brants, 2000) which was trained on the re286  tokens and number of reducible sequences found in them. [sent-247, score-0.442]
</p><p>76 However, it is sufficient for obtaining usable stop probability estimates. [sent-250, score-0.276]
</p><p>77 To evaluate the quality of our estimations, we compare them with Pstbtop, the stop probabilities computed directly on the evaluation treebanks. [sent-253, score-0.358]
</p><p>78 The y-axis shows the stop probabilities estimated on Wikipedia by our algorithm, while the x-axis shows the stop probabilities computed on the evaluation CoNLL data. [sent-256, score-0.737]
</p><p>79 Ideally, the computed and estimated stop probabilities should be the same, i. [sent-257, score-0.419]
</p><p>80 Our  method correctly recognizes that adverbs RB and adjectives JJ are often leaves (their stop probabilities in both directions are very high). [sent-261, score-0.318]
</p><p>81 Nouns (NN, NNS) are somewhere in the middle, the stop probabilities for proper nouns (NNP) are estimated higher, which is correct since they have much less modifiers then the common nouns NN. [sent-263, score-0.379]
</p><p>82 Their estimated stop probability is not very high (about 0. [sent-265, score-0.337]
</p><p>83 The stop probabilities of prepositions (IN) are also very well recognized. [sent-272, score-0.346]
</p><p>84 Our estimation assigns them the stop probability about 0. [sent-275, score-0.322]
</p><p>85 In order to see the impact of using the estimated stop probabilities (using model we provide results for classical DMV (using model Psdtmopv) as well. [sent-285, score-0.379]
</p><p>86 The addition of estimated stop probabilities based on large corpora improves the parsing  Psdtmopv+est),  accuracy on 15 out of 20 treebanks. [sent-288, score-0.419]
</p><p>87 In many cases, the improvement is substantial, which means that the estimated stop probabilities forced the model to completely rebuild the structures. [sent-289, score-0.379]
</p><p>88 Unfortunately, there are also negative examples, such as Hungarian, where the addition of the estimated stop probabilities decreases the attachment score from 60. [sent-293, score-0.429]
</p><p>89 This is probably caused by not very good estimates of the right-stop probability (see the last graph in Figure 3). [sent-295, score-0.127]
</p><p>90 Nevertheless, the estimated stop probabilities increase the average score over all the treebanks by more than 12% and therefore prove its usefulness. [sent-296, score-0.448]
</p><p>91 To see the theoretical upper bound of our model performance, we replaced the Psetsotp estimates by the Pstbtop estimates computed from the evaluation treebanks and run the same inference algorithm with the same setting. [sent-306, score-0.187]
</p><p>92 This shows a huge space in which the estimation of STOP probabilities could be further improved. [sent-308, score-0.139]
</p><p>93 DMV model using standard Psdtmopv probability is compared with DMV with Psdtompv+est, which incorporates STOP estimations based on reducibility principle. [sent-316, score-0.281]
</p><p>94 In future work, we would like to focus on unsupervised parsing without gold POS tags (see e. [sent-320, score-0.162]
</p><p>95 Unsupervised induction of tree substitution grammars for dependency parsing. [sent-345, score-0.198]
</p><p>96 Turning the pipeline into a loop: Iterated unsupervised dependency parsing and PoS induction. [sent-359, score-0.235]
</p><p>97 Improving unsupervised dependency parsing with richer contexts and smoothing. [sent-408, score-0.235]
</p><p>98 Corpusbased induction of syntactic structure: models of dependency and constituency. [sent-414, score-0.159]
</p><p>99 From ranked words to dependency trees: two-stage unsupervised non-projective dependency parsing. [sent-473, score-0.314]
</p><p>100 From baby steps to leapfrog: how ”less is more” in unsupervised dependency parsing. [sent-479, score-0.195]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reducible', 0.364), ('dir', 0.335), ('psdtmopv', 0.281), ('ch', 0.264), ('stop', 0.225), ('psetsotp', 0.215), ('reducibility', 0.182), ('dmv', 0.175), ('ptree', 0.165), ('spitkovsky', 0.157), ('pstop', 0.149), ('cf', 0.135), ('adj', 0.129), ('dependency', 0.119), ('children', 0.116), ('pchoose', 0.116), ('sstop', 0.116), ('mare', 0.114), ('abokrtsk', 0.111), ('probabilities', 0.093), ('cd', 0.092), ('fringe', 0.083), ('child', 0.081), ('est', 0.081), ('direction', 0.081), ('sequences', 0.078), ('unsupervised', 0.076), ('cek', 0.076), ('pos', 0.072), ('treebanks', 0.069), ('psdtompv', 0.066), ('pstbtop', 0.066), ('conll', 0.063), ('estimated', 0.061), ('trees', 0.056), ('probability', 0.051), ('alshawi', 0.051), ('attachment', 0.05), ('bisk', 0.05), ('ptreebank', 0.05), ('left', 0.049), ('estimations', 0.048), ('wg', 0.048), ('hiyan', 0.048), ('valence', 0.047), ('estimation', 0.046), ('stroudsburg', 0.046), ('cohn', 0.046), ('tags', 0.046), ('treebank', 0.045), ('right', 0.045), ('blunsom', 0.044), ('proportionally', 0.044), ('grammar', 0.044), ('head', 0.043), ('attached', 0.042), ('valentin', 0.042), ('rasooli', 0.04), ('parsing', 0.04), ('computed', 0.04), ('induction', 0.04), ('tree', 0.039), ('removed', 0.039), ('estimates', 0.039), ('sequence', 0.038), ('edges', 0.037), ('last', 0.037), ('sample', 0.037), ('pa', 0.036), ('tag', 0.035), ('zden', 0.035), ('wikipedia', 0.034), ('gilks', 0.033), ('headden', 0.033), ('lillehammer', 0.033), ('lopatkov', 0.033), ('outermost', 0.033), ('count', 0.031), ('adjacency', 0.03), ('projective', 0.03), ('root', 0.029), ('majli', 0.029), ('gerdes', 0.029), ('sampling', 0.029), ('estimating', 0.028), ('prepositions', 0.028), ('ubler', 0.028), ('christodoulopoulos', 0.027), ('faili', 0.027), ('fulfills', 0.027), ('weekend', 0.027), ('govern', 0.027), ('parsers', 0.027), ('shay', 0.025), ('chu', 0.025), ('ek', 0.025), ('raw', 0.025), ('cohen', 0.024), ('tnt', 0.024), ('exchangeable', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="331-tfidf-1" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<p>Author: David Marecek ; Milan Straka</p><p>Abstract: Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direction), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Dependency Model with Valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks.</p><p>2 0.12590259 <a title="331-tfidf-2" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>3 0.11419034 <a title="331-tfidf-3" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>Author: Ryan McDonald ; Joakim Nivre ; Yvonne Quirmbach-Brundage ; Yoav Goldberg ; Dipanjan Das ; Kuzman Ganchev ; Keith Hall ; Slav Petrov ; Hao Zhang ; Oscar Tackstrom ; Claudia Bedini ; Nuria Bertomeu Castello ; Jungmee Lee</p><p>Abstract: We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English, Swedish, Spanish, French and Korean. To show the usefulness of such a resource, we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before. This ‘universal’ treebank is made freely available in order to facilitate research on multilingual dependency parsing.1</p><p>4 0.11040635 <a title="331-tfidf-4" href="./acl-2013-Coordination_Structures_in_Dependency_Treebanks.html">94 acl-2013-Coordination Structures in Dependency Treebanks</a></p>
<p>Author: Martin Popel ; David Marecek ; Jan StÄłpanek ; Daniel Zeman ; ZdÄłnÄłk Zabokrtsky</p><p>Abstract: Paratactic syntactic structures are notoriously difficult to represent in dependency formalisms. This has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending problem in dependency analysis of natural languages. This paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical observations on convertibility between selected styles of representations are shown too.</p><p>5 0.090630636 <a title="331-tfidf-5" href="./acl-2013-Transition-based_Dependency_Parsing_with_Selectional_Branching.html">358 acl-2013-Transition-based Dependency Parsing with Selectional Branching</a></p>
<p>Author: Jinho D. Choi ; Andrew McCallum</p><p>Abstract: We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transitionbased parser that uses beam search.</p><p>6 0.085858613 <a title="331-tfidf-6" href="./acl-2013-Chinese_Parsing_Exploiting_Characters.html">80 acl-2013-Chinese Parsing Exploiting Characters</a></p>
<p>7 0.084792547 <a title="331-tfidf-7" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>8 0.081591763 <a title="331-tfidf-8" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>9 0.080906577 <a title="331-tfidf-9" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>10 0.078816772 <a title="331-tfidf-10" href="./acl-2013-Cross-lingual_Transfer_of_Semantic_Role_Labeling_Models.html">98 acl-2013-Cross-lingual Transfer of Semantic Role Labeling Models</a></p>
<p>11 0.075783469 <a title="331-tfidf-11" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>12 0.074298836 <a title="331-tfidf-12" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>13 0.073935032 <a title="331-tfidf-13" href="./acl-2013-A_Shift-Reduce_Parsing_Algorithm_for_Phrase-based_String-to-Dependency_Translation.html">19 acl-2013-A Shift-Reduce Parsing Algorithm for Phrase-based String-to-Dependency Translation</a></p>
<p>14 0.071686536 <a title="331-tfidf-14" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>15 0.071456589 <a title="331-tfidf-15" href="./acl-2013-Arguments_and_Modifiers_from_the_Learner%27s_Perspective.html">57 acl-2013-Arguments and Modifiers from the Learner's Perspective</a></p>
<p>16 0.070602499 <a title="331-tfidf-16" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>17 0.069649443 <a title="331-tfidf-17" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>18 0.069474883 <a title="331-tfidf-18" href="./acl-2013-Simpler_unsupervised_POS_tagging_with_bilingual_projections.html">323 acl-2013-Simpler unsupervised POS tagging with bilingual projections</a></p>
<p>19 0.06854748 <a title="331-tfidf-19" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>20 0.067037098 <a title="331-tfidf-20" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, -0.073), (2, -0.121), (3, 0.004), (4, -0.077), (5, -0.038), (6, 0.036), (7, 0.011), (8, 0.008), (9, -0.102), (10, 0.021), (11, -0.021), (12, -0.013), (13, 0.016), (14, -0.056), (15, -0.017), (16, -0.015), (17, 0.002), (18, 0.003), (19, -0.009), (20, -0.007), (21, 0.017), (22, 0.044), (23, -0.036), (24, -0.038), (25, 0.002), (26, -0.023), (27, -0.014), (28, -0.004), (29, -0.029), (30, 0.008), (31, -0.038), (32, 0.004), (33, 0.007), (34, 0.062), (35, -0.018), (36, 0.038), (37, -0.09), (38, -0.04), (39, -0.044), (40, 0.015), (41, -0.039), (42, -0.069), (43, -0.052), (44, -0.003), (45, -0.045), (46, -0.033), (47, 0.043), (48, -0.075), (49, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93350351 <a title="331-lsi-1" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<p>Author: David Marecek ; Milan Straka</p><p>Abstract: Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direction), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Dependency Model with Valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks.</p><p>2 0.8244161 <a title="331-lsi-2" href="./acl-2013-Coordination_Structures_in_Dependency_Treebanks.html">94 acl-2013-Coordination Structures in Dependency Treebanks</a></p>
<p>Author: Martin Popel ; David Marecek ; Jan StÄłpanek ; Daniel Zeman ; ZdÄłnÄłk Zabokrtsky</p><p>Abstract: Paratactic syntactic structures are notoriously difficult to represent in dependency formalisms. This has painful consequences such as high frequency of parsing errors related to coordination. In other words, coordination is a pending problem in dependency analysis of natural languages. This paper tries to shed some light on this area by bringing a systematizing view of various formal means developed for encoding coordination structures. We introduce a novel taxonomy of such approaches and apply it to treebanks across a typologically diverse range of 26 languages. In addition, empirical observations on convertibility between selected styles of representations are shown too.</p><p>3 0.75861448 <a title="331-lsi-3" href="./acl-2013-Universal_Dependency_Annotation_for_Multilingual_Parsing.html">368 acl-2013-Universal Dependency Annotation for Multilingual Parsing</a></p>
<p>Author: Ryan McDonald ; Joakim Nivre ; Yvonne Quirmbach-Brundage ; Yoav Goldberg ; Dipanjan Das ; Kuzman Ganchev ; Keith Hall ; Slav Petrov ; Hao Zhang ; Oscar Tackstrom ; Claudia Bedini ; Nuria Bertomeu Castello ; Jungmee Lee</p><p>Abstract: We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English, Swedish, Spanish, French and Korean. To show the usefulness of such a resource, we present a case study of crosslingual transfer parsing with more reliable evaluation than has been possible before. This ‘universal’ treebank is made freely available in order to facilitate research on multilingual dependency parsing.1</p><p>4 0.73877901 <a title="331-lsi-4" href="./acl-2013-Joint_Inference_for_Heterogeneous_Dependency_Parsing.html">208 acl-2013-Joint Inference for Heterogeneous Dependency Parsing</a></p>
<p>Author: Guangyou Zhou ; Jun Zhao</p><p>Abstract: This paper is concerned with the problem of heterogeneous dependency parsing. In this paper, we present a novel joint inference scheme, which is able to leverage the consensus information between heterogeneous treebanks in the parsing phase. Different from stacked learning methods (Nivre and McDonald, 2008; Martins et al., 2008), which process the dependency parsing in a pipelined way (e.g., a second level uses the first level outputs), in our method, multiple dependency parsing models are coordinated to exchange consensus information. We conduct experiments on Chinese Dependency Treebank (CDT) and Penn Chinese Treebank (CTB), experimental results show that joint infer- ence can bring significant improvements to all state-of-the-art dependency parsers.</p><p>5 0.71020472 <a title="331-lsi-5" href="./acl-2013-Survey_on_parsing_three_dependency_representations_for_English.html">335 acl-2013-Survey on parsing three dependency representations for English</a></p>
<p>Author: Angelina Ivanova ; Stephan Oepen ; Lilja vrelid</p><p>Abstract: In this paper we focus on practical issues of data representation for dependency parsing. We carry out an experimental comparison of (a) three syntactic dependency schemes; (b) three data-driven dependency parsers; and (c) the influence of two different approaches to lexical category disambiguation (aka tagging) prior to parsing. Comparing parsing accuracies in various setups, we study the interactions of these three aspects and analyze which configurations are easier to learn for a dependency parser.</p><p>6 0.67266393 <a title="331-lsi-6" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<p>7 0.65727276 <a title="331-lsi-7" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>8 0.62520975 <a title="331-lsi-8" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>9 0.61892742 <a title="331-lsi-9" href="./acl-2013-Turning_on_the_Turbo%3A_Fast_Third-Order_Non-Projective_Turbo_Parsers.html">362 acl-2013-Turning on the Turbo: Fast Third-Order Non-Projective Turbo Parsers</a></p>
<p>10 0.61223507 <a title="331-lsi-10" href="./acl-2013-Iterative_Transformation_of_Annotation_Guidelines_for_Constituency_Parsing.html">204 acl-2013-Iterative Transformation of Annotation Guidelines for Constituency Parsing</a></p>
<p>11 0.60720176 <a title="331-lsi-11" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>12 0.59841532 <a title="331-lsi-12" href="./acl-2013-Exploring_Word_Order_Universals%3A_a_Probabilistic_Graphical_Model_Approach.html">149 acl-2013-Exploring Word Order Universals: a Probabilistic Graphical Model Approach</a></p>
<p>13 0.59227335 <a title="331-lsi-13" href="./acl-2013-Enhanced_and_Portable_Dependency_Projection_Algorithms_Using_Interlinear_Glossed_Text.html">136 acl-2013-Enhanced and Portable Dependency Projection Algorithms Using Interlinear Glossed Text</a></p>
<p>14 0.58837593 <a title="331-lsi-14" href="./acl-2013-The_effect_of_non-tightness_on_Bayesian_estimation_of_PCFGs.html">348 acl-2013-The effect of non-tightness on Bayesian estimation of PCFGs</a></p>
<p>15 0.58704889 <a title="331-lsi-15" href="./acl-2013-Dependency_Parser_Adaptation_with_Subtrees_from_Auto-Parsed_Target_Domain_Data.html">112 acl-2013-Dependency Parser Adaptation with Subtrees from Auto-Parsed Target Domain Data</a></p>
<p>16 0.58294159 <a title="331-lsi-16" href="./acl-2013-ParGramBank%3A_The_ParGram_Parallel_Treebank.html">270 acl-2013-ParGramBank: The ParGram Parallel Treebank</a></p>
<p>17 0.58200228 <a title="331-lsi-17" href="./acl-2013-A_Transition-Based_Dependency_Parser_Using_a_Dynamic_Parsing_Strategy.html">26 acl-2013-A Transition-Based Dependency Parser Using a Dynamic Parsing Strategy</a></p>
<p>18 0.57764506 <a title="331-lsi-18" href="./acl-2013-Fast_and_Accurate_Shift-Reduce_Constituent_Parsing.html">155 acl-2013-Fast and Accurate Shift-Reduce Constituent Parsing</a></p>
<p>19 0.57751852 <a title="331-lsi-19" href="./acl-2013-Punctuation_Prediction_with_Transition-based_Parsing.html">288 acl-2013-Punctuation Prediction with Transition-based Parsing</a></p>
<p>20 0.55864137 <a title="331-lsi-20" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.084), (6, 0.022), (11, 0.065), (24, 0.022), (26, 0.053), (29, 0.011), (35, 0.07), (40, 0.02), (42, 0.063), (48, 0.056), (65, 0.237), (67, 0.018), (70, 0.057), (88, 0.049), (90, 0.029), (95, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82762134 <a title="331-lda-1" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<p>Author: Eric T. Nalisnick ; Henry S. Baird</p><p>Abstract: We present an automatic method for analyzing sentiment dynamics between characters in plays. This literary format’s structured dialogue allows us to make assumptions about who is participating in a conversation. Once we have an idea of who a character is speaking to, the sentiment in his or her speech can be attributed accordingly, allowing us to generate lists of a character’s enemies and allies as well as pinpoint scenes critical to a character’s emotional development. Results of experiments on Shakespeare’s plays are presented along with discussion of how this work can be extended to unstructured texts (i.e. novels).</p><p>same-paper 2 0.80177796 <a title="331-lda-2" href="./acl-2013-Stop-probability_estimates_computed_on_a_large_corpus_improve_Unsupervised_Dependency_Parsing.html">331 acl-2013-Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing</a></p>
<p>Author: David Marecek ; Milan Straka</p><p>Abstract: Even though the quality of unsupervised dependency parsers grows, they often fail in recognition of very basic dependencies. In this paper, we exploit a prior knowledge of STOP-probabilities (whether a given word has any children in a given direction), which is obtained from a large raw corpus using the reducibility principle. By incorporating this knowledge into Dependency Model with Valence, we managed to considerably outperform the state-of-theart results in terms of average attachment score over 20 treebanks from CoNLL 2006 and 2007 shared tasks.</p><p>3 0.596187 <a title="331-lda-3" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<p>Author: Kai Liu ; Yajuan Lu ; Wenbin Jiang ; Qun Liu</p><p>Abstract: This paper describes a novel strategy for automatic induction of a monolingual dependency grammar under the guidance of bilingually-projected dependency. By moderately leveraging the dependency information projected from the parsed counterpart language, and simultaneously mining the underlying syntactic structure of the language considered, it effectively integrates the advantages of bilingual projection and unsupervised induction, so as to induce a monolingual grammar much better than previous models only using bilingual projection or unsupervised induction. We induced dependency gram- mar for five different languages under the guidance of dependency information projected from the parsed English translation, experiments show that the bilinguallyguided method achieves a significant improvement of 28.5% over the unsupervised baseline and 3.0% over the best projection baseline on average.</p><p>4 0.58909315 <a title="331-lda-4" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>Author: Richard Socher ; John Bauer ; Christopher D. Manning ; Ng Andrew Y.</p><p>Abstract: Natural language parsing has typically been done with small sets of discrete categories such as NP and VP, but this representation does not capture the full syntactic nor semantic richness of linguistic phrases, and attempts to improve on this by lexicalizing phrases or splitting categories only partly address the problem at the cost of huge feature spaces and sparseness. Instead, we introduce a Compositional Vector Grammar (CVG), which combines PCFGs with a syntactically untied recursive neural network that learns syntactico-semantic, compositional vector representations. The CVG improves the PCFG of the Stanford Parser by 3.8% to obtain an F1 score of 90.4%. It is fast to train and implemented approximately as an efficient reranker it is about 20% faster than the current Stanford factored parser. The CVG learns a soft notion of head words and improves performance on the types of ambiguities that require semantic information such as PP attachments.</p><p>5 0.58886522 <a title="331-lda-5" href="./acl-2013-Nonconvex_Global_Optimization_for_Latent-Variable_Models.html">260 acl-2013-Nonconvex Global Optimization for Latent-Variable Models</a></p>
<p>Author: Matthew R. Gormley ; Jason Eisner</p><p>Abstract: Many models in NLP involve latent variables, such as unknown parses, tags, or alignments. Finding the optimal model parameters is then usually a difficult nonconvex optimization problem. The usual practice is to settle for local optimization methods such as EM or gradient ascent. We explore how one might instead search for a global optimum in parameter space, using branch-and-bound. Our method would eventually find the global maximum (up to a user-specified ?) if run for long enough, but at any point can return a suboptimal solution together with an upper bound on the global maximum. As an illustrative case, we study a generative model for dependency parsing. We search for the maximum-likelihood model parameters and corpus parse, subject to posterior constraints. We show how to formulate this as a mixed integer quadratic programming problem with nonlinear constraints. We use the Reformulation Linearization Technique to produce convex relaxations during branch-and-bound. Although these techniques do not yet provide a practical solution to our instance of this NP-hard problem, they sometimes find better solutions than Viterbi EM with random restarts, in the same time.</p><p>6 0.58611113 <a title="331-lda-6" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>7 0.58453131 <a title="331-lda-7" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<p>8 0.58389503 <a title="331-lda-8" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>9 0.58166307 <a title="331-lda-9" href="./acl-2013-Unsupervised_Consonant-Vowel_Prediction_over_Hundreds_of_Languages.html">369 acl-2013-Unsupervised Consonant-Vowel Prediction over Hundreds of Languages</a></p>
<p>10 0.58138013 <a title="331-lda-10" href="./acl-2013-The_Effect_of_Higher-Order_Dependency_Features_in_Discriminative_Phrase-Structure_Parsing.html">343 acl-2013-The Effect of Higher-Order Dependency Features in Discriminative Phrase-Structure Parsing</a></p>
<p>11 0.5812676 <a title="331-lda-11" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>12 0.58088571 <a title="331-lda-12" href="./acl-2013-Multigraph_Clustering_for_Unsupervised_Coreference_Resolution.html">252 acl-2013-Multigraph Clustering for Unsupervised Coreference Resolution</a></p>
<p>13 0.57965052 <a title="331-lda-13" href="./acl-2013-Graph-based_Semi-Supervised_Model_for_Joint_Chinese_Word_Segmentation_and_Part-of-Speech_Tagging.html">173 acl-2013-Graph-based Semi-Supervised Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</a></p>
<p>14 0.57944089 <a title="331-lda-14" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>15 0.57697701 <a title="331-lda-15" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>16 0.57581979 <a title="331-lda-16" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>17 0.57532799 <a title="331-lda-17" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>18 0.5742873 <a title="331-lda-18" href="./acl-2013-Graph_Propagation_for_Paraphrasing_Out-of-Vocabulary_Words_in_Statistical_Machine_Translation.html">174 acl-2013-Graph Propagation for Paraphrasing Out-of-Vocabulary Words in Statistical Machine Translation</a></p>
<p>19 0.57353508 <a title="331-lda-19" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>20 0.57325518 <a title="331-lda-20" href="./acl-2013-Easy-First_POS_Tagging_and_Dependency_Parsing_with_Beam_Search.html">132 acl-2013-Easy-First POS Tagging and Dependency Parsing with Beam Search</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
