<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-352" href="#">acl2013-352</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</h1>
<br/><p>Source: <a title="acl-2013-352-pdf" href="http://aclweb.org/anthology//P/P13/P13-2141.pdf">pdf</a></p><p>Author: Xingxing Zhang ; Jianwen Zhang ; Junyu Zeng ; Jun Yan ; Zheng Chen ; Zhifang Sui</p><p>Abstract: Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.</p><p>Reference: <a title="acl-2013-352-reference" href="../acl2013_reference/acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Abstract Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. [sent-6, score-0.379]
</p><p>2 In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. [sent-8, score-0.799]
</p><p>3 By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach. [sent-10, score-0.344]
</p><p>4 They are composed of relational facts often represented in the form of a triplet, ( S rcEnt ity Re l i at on D stEnt ity ) , such as “(Bill Gates, BornIn, Seattle)”. [sent-12, score-0.334]
</p><p>5 An important task is to enrich such KBs by extracting more facts from text. [sent-13, score-0.219]
</p><p>6 Specifically, this paper focuses on extracting facts for existing relations. [sent-14, score-0.219]
</p><p>7 However, it is difficult to handle large scale corpus due to the high cost of labeling. [sent-19, score-0.042]
</p><p>8 Recently an approach called distant supervision (DS) (Mintz et al. [sent-20, score-0.221]
</p><p>9 It treats the extraction problem as classifying  ,  ∗ The contact author. [sent-22, score-0.084]
</p><p>10 Then an existing fact in a KB can be used as a labeled example whose label is the relation name. [sent-24, score-0.229]
</p><p>11 Then the features of all the sentences (from a given text corpus) containing the entity pair are merged as the feature of the example. [sent-25, score-0.393]
</p><p>12 They argue that DS introduces a lot of noise into the training data by merging the features of all the sentences containing the same entity pair, because a sentence containing the entity pair of a relation may not talk about the relation. [sent-32, score-1.042]
</p><p>13 (201 1) introduce hidden variables to indicate whether a sentence is noise and try to infer them from the data. [sent-35, score-0.062]
</p><p>14 (2012) design a generative model to identify noise patterns. [sent-37, score-0.062]
</p><p>15 In this paper, we point out and analyze some critical factors in DS which have great impact on the accuracy but has not been touched or well handled before. [sent-39, score-0.213]
</p><p>16 First, each relation has its own schema definition, i. [sent-40, score-0.234]
</p><p>17 , the source entity and the destination entity should be of valid types, which is overlooked in DS. [sent-42, score-1.096]
</p><p>18 Therefore, we propose a component of entity type detection to check it. [sent-43, score-0.575]
</p><p>19 Second, DS introduces many false negative examples into the training set and we propose a new method to construct negative training examples. [sent-44, score-0.329]
</p><p>20 Third, we find it is difficult for a single classifier to achieve high accuracy and hence we train multiple classifiers and ensemble them. [sent-45, score-0.271]
</p><p>21 We also notice that Nguyen and Moschitti (201 1a) and Nguyen and Moschitti (201 1b) utilize external information such as more facts from Yago and labeled sentences from ACE to improve the 810  Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t. [sent-46, score-0.283]
</p><p>22 2  Critical Factors Affecting the Accuracy  DS has four steps: (1) Detect candidate entity pairs in the corpus. [sent-50, score-0.432]
</p><p>23 Among these steps, we find the following three critical factors have great impact on the accuracy (see Section 4 for the experimental results). [sent-54, score-0.184]
</p><p>24 In DS, a sentence with a candidate entity pair a sentence with two candidate entities is noisy. [sent-56, score-0.646]
</p><p>25 First, the schema of each relation in the KB requires that the source and destination entities should be of valid types, e. [sent-57, score-0.719]
</p><p>26 , the source and destination entity of the relation “DirectorOfFilm” should be of the types “Director” and “Film” respectively. [sent-59, score-0.903]
</p><p>27 If the two entities in a sentence are not of the valid types, the sentence is noisy. [sent-60, score-0.18]
</p><p>28 Second, the sentence may not talk about the relation even when the two entities  are of the valid types. [sent-61, score-0.379]
</p><p>29 , 2012) do not distinguish the two types of noise but directly infer the overall noise from the data. [sent-65, score-0.166]
</p><p>30 We argue that the first type ofnoise is very difficult to be inferred just from the noisy relational labels. [sent-66, score-0.119]
</p><p>31 Instead, we decouple the two types of noise, and utilize external labeled data, i. [sent-67, score-0.106]
</p><p>32 , the Wikipedia anchor links, to train an entity type detection module to handle the first type of noise. [sent-69, score-0.714]
</p><p>33 We notice that when Ling and Weld (2012) studied a fine-grained NER method, they applied the method to relation extraction by adding the recognized entity tags to the features. [sent-70, score-0.708]
</p><p>34 We worry that the contribution of the entity type features may be drowned when many other features are used. [sent-71, score-0.423]
</p><p>35 DS treats the relation extraction as a multi-class classification task. [sent-75, score-0.283]
</p><p>36 For a relation, it implies that the facts of all the other relations together with the “Other” class  are negative examples. [sent-76, score-0.376]
</p><p>37 This introduces many false negative examples into the training data. [sent-77, score-0.228]
</p><p>38 First, many relations are not exclusive with each other, e. [sent-78, score-0.056]
</p><p>39 Second, in DS, the “Other” class is composed of all the candidate entity pairs not existed in the KB, which actually contains many positive facts of non-Other relations because the KB is not complete. [sent-81, score-0.707]
</p><p>40 Therefore we use a different way to construct negative training examples. [sent-82, score-0.101]
</p><p>41 The features used in DS are very sparse and many examples do not contain any features. [sent-84, score-0.052]
</p><p>42 However we find it is difficult for a single classifier on all the features to achieve high accuracy and hence we divide the features into different categories and train a separate classifier for each category and then ensemble them finally. [sent-86, score-0.26]
</p><p>43 3  Accurate Distant Supervision (ADS)  Different from DS, we treat the extraction problem as N binary classification problems, one for each relation. [sent-87, score-0.053]
</p><p>44 In step (1), when detecting candidate entity pairs in sentences, we use our entity type detection module (Section 3. [sent-89, score-0.94]
</p><p>45 1) to filter out the sentences where the entity pair is of invalid entity types. [sent-90, score-0.75]
</p><p>46 In step (2), we use our new method to construct negative examples (Section 3. [sent-91, score-0.153]
</p><p>47 In step (3), we employ more features and design an ensemble classifier (Section 3. [sent-93, score-0.189]
</p><p>48 In step (4), we train N binary classifiers separately. [sent-95, score-0.083]
</p><p>49 1 Entity Type Detection We divide the entity type detection into two steps. [sent-97, score-0.508]
</p><p>50 The first step, called boundary detection, is to detect phrases as candidate entities. [sent-98, score-0.167]
</p><p>51 The second step, called named entity disambiguation, maps a detected candidate entity to some entity types, e. [sent-99, score-1.146]
</p><p>52 Note that an entity might be mapped to multiple types. [sent-102, score-0.357]
</p><p>53 Boundary Detection Two ways are used for boundary detection. [sent-104, score-0.055]
</p><p>54 First, for each relation, from the training set of facts, we get two dictionaries (one for source entities and one for destination entities). [sent-105, score-0.408]
</p><p>55 The two dictionaries are used to detect the source and destination entities. [sent-106, score-0.342]
</p><p>56 We first find the compatible NER tags for an entity type in the KB. [sent-108, score-0.51]
</p><p>57 For example, 811  for the type “FilmDirector”, the compatible NER tag of Standford NER is “Person”. [sent-109, score-0.111]
</p><p>58 To do this, for each entity type in the KB, we match all the entities of that type (in the training set) back to the training corpus and get the probability Ptag(ti) of each NER tag (including the “NULL” tag meaning not recognized as a named entity) recognized by the NER tool. [sent-110, score-0.706]
</p><p>59 nd Ifid thaete eetnatiitnieeds recognized by NER tool will be discarded. [sent-115, score-0.057]
</p><p>60 Named Entity Disambiguation (NED) With a candidate entity obtained by the boundary detection, we need a NED component to assign some entity types to it. [sent-116, score-0.917]
</p><p>61 To obtain such a NED, we leverage the anchor text in Wikipedia to generate training data and train a NED component. [sent-117, score-0.098]
</p><p>62 The referred Freebase entity and the types of an anchor link in Wikipedia can be obtained from Freebase. [sent-118, score-0.456]
</p><p>63 2 Negative Examples Construction Treating the problem as a multi-class classification implies introducing many false negative examples for a relation; therefore, we handle each relation with a separate binary classifier. [sent-123, score-0.438]
</p><p>64 However, a KB only tells us which entity pairs belong to a relation, i. [sent-124, score-0.357]
</p><p>65 But we also need negative examples to train a binary classifier. [sent-127, score-0.194]
</p><p>66 To reduce the number of false negative examples, we propose a new method to construct negative examples by utilizing the 1-to-1/1-to-n/n-to-1/n-to-n property of a relation. [sent-128, score-0.298]
</p><p>67 1-to-1/n-to-1/1-to-n Relation A 1-to-1 or n-to1 relation is a functional relation: for a relation r, for each valid source entity e1, there is only one unique destination entity e2 such that (e1, e2) ∈ r. [sent-129, score-1.494]
</p><p>68 Inequality (2) says the proportion of source entities which have exactly one counterpart destination entity should be greater than a given threshold. [sent-133, score-0.8]
</p><p>69 Inequality (3) says the average number of destination entities of a source entity should be less than the threshold. [sent-134, score-0.8]
</p><p>70 To check whether r is a 1-to-n relation, we simply swap the source and destination entities of the relation and check whether the reversed relation is a n-to-1 relation by the above two inequalities. [sent-135, score-1.242]
</p><p>71 We approximately categorize a n-to-n relation to n-to-1 or 1-to-n by checking which one it is closer to. [sent-149, score-0.199]
</p><p>72 This is done by computing the following two values αsrc and  r is treated as a 1-to-n relation if αsrc > and as a 1-to-n relation otherwise. [sent-150, score-0.398]
</p><p>73 yp(a4ir)  (e1, e2) not in the relation r of the KB, we first determine whether it is 1-to-n or n-to-1 using the above method. [sent-154, score-0.199]
</p><p>74 If r is 1-to-1/n-to-1 and e1 exists in some fact of r as the source entity, then (e1, e2) is a negative example as it violates the 1-to-1/n-to-1 constraint. [sent-155, score-0.149]
</p><p>75 If r is 1-to-n, the judgement is similar and just simply swap the source and destination entities of the relation. [sent-156, score-0.448]
</p><p>76 , 2011): Trigger Words (the words on the dependency path except stop words) and Entity  String (source entity and destination entity). [sent-161, score-0.614]
</p><p>77 We find that without considering the reversed order of entity pairs in a sentence, the precision can be higher, but the recall decreases. [sent-165, score-0.516]
</p><p>78 For example, for the entity pair ⟨Ventura Pons, Actrius⟩, we only ocorn thsied eenr sentences ewnittuhr ath Peo right octrrdiuesr (e. [sent-166, score-0.393]
</p><p>79 For each relation, we train four classifiers: C1 (without considering reversed order), C2 (considering reversed order), C1more (without considering reversed order and employ more feature) and C2more (considering reversed order and employ more feature). [sent-170, score-0.687]
</p><p>80 We then ensemble the four classifiers by averaging the probabilities of predictions: P(y|x) 4  =  P1+ P2+ P14more+ P2more  (5)  Experiments  4. [sent-171, score-0.161]
</p><p>81 1 Dataset and Configurations We aimed to extract facts of the 92 most frequent relations in Freebase 2009. [sent-172, score-0.275]
</p><p>82 The facts of each relation were equally split to two parts for training and testing. [sent-173, score-0.418]
</p><p>83 During the NED phrase, there are 94 unique entity types (they are also relations in Freebase) for the source and destination entities. [sent-175, score-0.76]
</p><p>84 Note that some entity types contain too few entities and they are discarded. [sent-176, score-0.502]
</p><p>85 , 2008) as classifiers for NED  and relation extraction and the solver is L2LR. [sent-182, score-0.294]
</p><p>86 We evaluate the performance on the half hold-on facts for testing. [sent-185, score-0.219]
</p><p>87 We compared performance of the n = 50, 000 best extracted relation instances of each method and the Precision-Recall (PR) curves are in Figure 1 and  Recall  Figure 1: Performance of different methods. [sent-186, score-0.199]
</p><p>88 For a candidate fact without any entity existing in Freebase, we are not able to judge whether it is correct. [sent-189, score-0.432]
</p><p>89 Thus we only evaluate the candidate facts that at least one entity occurs as the source or destination entity in the test fact set. [sent-190, score-1.313]
</p><p>90 , 2012), we selected the top 50 ranked (according to their classification probabilities) relation facts of the 15 largest relations. [sent-200, score-0.418]
</p><p>91 3 Contribution of Each Component In Figure 2, with the entity type detection (ETD),  the performance is better than the original DS method (OrigDS). [sent-212, score-0.508]
</p><p>92 ETD is also better than adding the entity types of the pair to the feature vector (DS Figer)2 as in (Ling and Weld, 2012). [sent-214, score-0.435]
</p><p>93 If we also employ the negative example construction strategy in Section 3. [sent-215, score-0.14]
</p><p>94 By adding more features (More) and employing the ensemble learning (Ensemble(ADS)) to ETD+Neg, the performance is further improved. [sent-217, score-0.119]
</p><p>95 We find some factors are crucially important, including valid entity type detection, negative training examples construction and ensembles. [sent-219, score-0.706]
</p><p>96 2We use Figer (Ling and Weld, 2012) to detect entity types Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. [sent-237, score-0.436]
</p><p>97 Incorporating non-local information into information extraction systems by gibbs sampling. [sent-243, score-0.053]
</p><p>98 Knowledge-based weak supervision for information extraction of overlapping relations. [sent-249, score-0.16]
</p><p>99 End-to-end relation extraction using distant supervision from external semantic repositories. [sent-267, score-0.507]
</p><p>100 Reducing wrong labels in distant supervision for relation extraction. [sent-285, score-0.42]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('entity', 0.357), ('ds', 0.282), ('takamatsu', 0.263), ('destination', 0.257), ('kb', 0.243), ('ned', 0.23), ('facts', 0.219), ('relation', 0.199), ('ner', 0.158), ('freebase', 0.138), ('etd', 0.132), ('reversed', 0.125), ('ensemble', 0.119), ('distant', 0.114), ('hoffmann', 0.108), ('supervision', 0.107), ('entities', 0.103), ('negative', 0.101), ('filmdirector', 0.099), ('pons', 0.099), ('ventura', 0.099), ('ling', 0.085), ('detection', 0.085), ('riedel', 0.081), ('valid', 0.077), ('candidate', 0.075), ('nguyen', 0.067), ('stags', 0.066), ('type', 0.066), ('mintz', 0.063), ('noise', 0.062), ('wikipedia', 0.06), ('weld', 0.059), ('multir', 0.058), ('figer', 0.058), ('standford', 0.058), ('recognized', 0.057), ('anchor', 0.057), ('relations', 0.056), ('critical', 0.055), ('boundary', 0.055), ('yao', 0.054), ('src', 0.054), ('factors', 0.053), ('relational', 0.053), ('extraction', 0.053), ('examples', 0.052), ('ads', 0.048), ('source', 0.048), ('yago', 0.046), ('compatible', 0.045), ('maltparser', 0.045), ('limin', 0.045), ('false', 0.044), ('inequality', 0.043), ('types', 0.042), ('tags', 0.042), ('bunescu', 0.042), ('classifiers', 0.042), ('handle', 0.042), ('train', 0.041), ('banko', 0.041), ('null', 0.04), ('carlson', 0.04), ('swap', 0.04), ('employ', 0.039), ('neg', 0.039), ('accuracy', 0.038), ('great', 0.038), ('liblinear', 0.037), ('detect', 0.037), ('pair', 0.036), ('check', 0.036), ('finkel', 0.036), ('moschitti', 0.036), ('says', 0.035), ('schema', 0.035), ('mention', 0.034), ('ia', 0.034), ('considering', 0.034), ('external', 0.034), ('articles', 0.034), ('treats', 0.031), ('ity', 0.031), ('fan', 0.031), ('component', 0.031), ('classifier', 0.031), ('introduces', 0.031), ('ti', 0.031), ('labeled', 0.03), ('person', 0.029), ('shubin', 0.029), ('stent', 0.029), ('jianwen', 0.029), ('shingo', 0.029), ('kbs', 0.029), ('openie', 0.029), ('triplet', 0.029), ('touched', 0.029), ('dst', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="352-tfidf-1" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>Author: Xingxing Zhang ; Jianwen Zhang ; Junyu Zeng ; Jun Yan ; Zheng Chen ; Zhifang Sui</p><p>Abstract: Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.</p><p>2 0.31174791 <a title="352-tfidf-2" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>Author: Wei Xu ; Raphael Hoffmann ; Le Zhao ; Ralph Grishman</p><p>Abstract: Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus. However, previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases. To tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features. We adapt the information retrieval technique of pseudo- relevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments.</p><p>3 0.23249333 <a title="352-tfidf-3" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>Author: Mohamed Amir Yosef ; Sandro Bauer ; Johannes Hoffart ; Marc Spaniol ; Gerhard Weikum</p><p>Abstract: Recent research has shown progress in achieving high-quality, very fine-grained type classification in hierarchical taxonomies. Within such a multi-level type hierarchy with several hundreds of types at different levels, many entities naturally belong to multiple types. In order to achieve high-precision in type classification, current approaches are either limited to certain domains or require time consuming multistage computations. As a consequence, existing systems are incapable of performing ad-hoc type classification on arbitrary input texts. In this demo, we present a novel Webbased tool that is able to perform domain independent entity type classification under real time conditions. Thanks to its efficient implementation and compacted feature representation, the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations. Our system offers an online interface where natural-language text can be inserted, which returns semantic type labels for entity mentions. Further more, the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy.</p><p>4 0.23169358 <a title="352-tfidf-4" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>Author: Ndapandula Nakashole ; Tomasz Tylenda ; Gerhard Weikum</p><p>Abstract: Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and semantically typing newly emerging out-ofKB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE. Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowdsourced user studies, show our method performing significantly better than prior work.</p><p>5 0.16260947 <a title="352-tfidf-5" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>Author: Xiaohua Liu ; Yitong Li ; Haocheng Wu ; Ming Zhou ; Furu Wei ; Yi Lu</p><p>Abstract: We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.</p><p>6 0.15524088 <a title="352-tfidf-6" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>7 0.12978448 <a title="352-tfidf-7" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>8 0.11872172 <a title="352-tfidf-8" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>9 0.11406294 <a title="352-tfidf-9" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>10 0.11116827 <a title="352-tfidf-10" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>11 0.10819866 <a title="352-tfidf-11" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>12 0.10816932 <a title="352-tfidf-12" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>13 0.10581379 <a title="352-tfidf-13" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>14 0.10038092 <a title="352-tfidf-14" href="./acl-2013-Understanding_Tables_in_Context_Using_Standard_NLP_Toolkits.html">365 acl-2013-Understanding Tables in Context Using Standard NLP Toolkits</a></p>
<p>15 0.093962073 <a title="352-tfidf-15" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>16 0.088123344 <a title="352-tfidf-16" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>17 0.086837523 <a title="352-tfidf-17" href="./acl-2013-Enriching_Entity_Translation_Discovery_using_Selective_Temporality.html">138 acl-2013-Enriching Entity Translation Discovery using Selective Temporality</a></p>
<p>18 0.085110538 <a title="352-tfidf-18" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>19 0.084538408 <a title="352-tfidf-19" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>20 0.078788802 <a title="352-tfidf-20" href="./acl-2013-FudanNLP%3A_A_Toolkit_for_Chinese_Natural_Language_Processing.html">164 acl-2013-FudanNLP: A Toolkit for Chinese Natural Language Processing</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.198), (1, 0.073), (2, -0.037), (3, -0.082), (4, 0.09), (5, 0.203), (6, -0.073), (7, -0.026), (8, 0.071), (9, -0.003), (10, 0.04), (11, -0.148), (12, -0.086), (13, -0.014), (14, -0.021), (15, 0.049), (16, -0.025), (17, 0.033), (18, -0.162), (19, -0.012), (20, -0.052), (21, 0.045), (22, -0.003), (23, 0.301), (24, 0.051), (25, 0.056), (26, 0.071), (27, -0.109), (28, -0.148), (29, 0.082), (30, 0.065), (31, -0.053), (32, -0.002), (33, 0.004), (34, -0.181), (35, 0.008), (36, 0.031), (37, -0.09), (38, -0.064), (39, 0.006), (40, 0.083), (41, -0.042), (42, -0.046), (43, 0.061), (44, 0.001), (45, -0.025), (46, -0.094), (47, -0.004), (48, -0.029), (49, 0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96888852 <a title="352-lsi-1" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>Author: Xingxing Zhang ; Jianwen Zhang ; Junyu Zeng ; Jun Yan ; Zheng Chen ; Zhifang Sui</p><p>Abstract: Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.</p><p>2 0.88624322 <a title="352-lsi-2" href="./acl-2013-Fine-grained_Semantic_Typing_of_Emerging_Entities.html">160 acl-2013-Fine-grained Semantic Typing of Emerging Entities</a></p>
<p>Author: Ndapandula Nakashole ; Tomasz Tylenda ; Gerhard Weikum</p><p>Abstract: Methods for information extraction (IE) and knowledge base (KB) construction have been intensively studied. However, a largely under-explored case is tapping into highly dynamic sources like news streams and social media, where new entities are continuously emerging. In this paper, we present a method for discovering and semantically typing newly emerging out-ofKB entities, thus improving the freshness and recall of ontology-based IE and improving the precision and semantic rigor of open IE. Our method is based on a probabilistic model that feeds weights into integer linear programs that leverage type signatures of relational phrases and type correlation or disjointness constraints. Our experimental evaluation, based on crowdsourced user studies, show our method performing significantly better than prior work.</p><p>3 0.85156024 <a title="352-lsi-3" href="./acl-2013-HYENA-live%3A_Fine-Grained_Online_Entity_Type_Classification_from_Natural-language_Text.html">179 acl-2013-HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text</a></p>
<p>Author: Mohamed Amir Yosef ; Sandro Bauer ; Johannes Hoffart ; Marc Spaniol ; Gerhard Weikum</p><p>Abstract: Recent research has shown progress in achieving high-quality, very fine-grained type classification in hierarchical taxonomies. Within such a multi-level type hierarchy with several hundreds of types at different levels, many entities naturally belong to multiple types. In order to achieve high-precision in type classification, current approaches are either limited to certain domains or require time consuming multistage computations. As a consequence, existing systems are incapable of performing ad-hoc type classification on arbitrary input texts. In this demo, we present a novel Webbased tool that is able to perform domain independent entity type classification under real time conditions. Thanks to its efficient implementation and compacted feature representation, the system is able to process text inputs on-the-fly while still achieving equally high precision as leading state-ofthe-art implementations. Our system offers an online interface where natural-language text can be inserted, which returns semantic type labels for entity mentions. Further more, the user interface allows users to explore the assigned types by visualizing and navigating along the type-hierarchy.</p><p>4 0.78332949 <a title="352-lsi-4" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>Author: Wei Xu ; Raphael Hoffmann ; Le Zhao ; Ralph Grishman</p><p>Abstract: Distant supervision has attracted recent interest for training information extraction systems because it does not require any human annotation but rather employs existing knowledge bases to heuristically label a training corpus. However, previous work has failed to address the problem of false negative training examples mislabeled due to the incompleteness of knowledge bases. To tackle this problem, we propose a simple yet novel framework that combines a passage retrieval model using coarse features into a state-of-the-art relation extractor using multi-instance learning with fine features. We adapt the information retrieval technique of pseudo- relevance feedback to expand knowledge bases, assuming entity pairs in top-ranked passages are more likely to express a relation. Our proposed technique significantly improves the quality of distantly supervised relation extraction, boosting recall from 47.7% to 61.2% with a consistently high level of precision of around 93% in the experiments.</p><p>5 0.72570008 <a title="352-lsi-5" href="./acl-2013-Entity_Linking_for_Tweets.html">139 acl-2013-Entity Linking for Tweets</a></p>
<p>Author: Xiaohua Liu ; Yitong Li ; Haocheng Wu ; Ming Zhou ; Furu Wei ; Yi Lu</p><p>Abstract: We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry. Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations. To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions. Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary. We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method.</p><p>6 0.64340138 <a title="352-lsi-6" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>7 0.62747979 <a title="352-lsi-7" href="./acl-2013-Learning_Entity_Representation_for_Entity_Disambiguation.html">219 acl-2013-Learning Entity Representation for Entity Disambiguation</a></p>
<p>8 0.56566411 <a title="352-lsi-8" href="./acl-2013-Understanding_Tables_in_Context_Using_Standard_NLP_Toolkits.html">365 acl-2013-Understanding Tables in Context Using Standard NLP Toolkits</a></p>
<p>9 0.55803156 <a title="352-lsi-9" href="./acl-2013-Enriching_Entity_Translation_Discovery_using_Selective_Temporality.html">138 acl-2013-Enriching Entity Translation Discovery using Selective Temporality</a></p>
<p>10 0.55511272 <a title="352-lsi-10" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>11 0.52388906 <a title="352-lsi-11" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>12 0.52301079 <a title="352-lsi-12" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>13 0.51922131 <a title="352-lsi-13" href="./acl-2013-Named_Entity_Recognition_using_Cross-lingual_Resources%3A_Arabic_as_an_Example.html">256 acl-2013-Named Entity Recognition using Cross-lingual Resources: Arabic as an Example</a></p>
<p>14 0.50486648 <a title="352-lsi-14" href="./acl-2013-Bootstrapping_Entity_Translation_on_Weakly_Comparable_Corpora.html">71 acl-2013-Bootstrapping Entity Translation on Weakly Comparable Corpora</a></p>
<p>15 0.4741374 <a title="352-lsi-15" href="./acl-2013-Text-Driven_Toponym_Resolution_using_Indirect_Supervision.html">340 acl-2013-Text-Driven Toponym Resolution using Indirect Supervision</a></p>
<p>16 0.45879814 <a title="352-lsi-16" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>17 0.45860374 <a title="352-lsi-17" href="./acl-2013-Transfer_Learning_Based_Cross-lingual_Knowledge_Extraction_for_Wikipedia.html">356 acl-2013-Transfer Learning Based Cross-lingual Knowledge Extraction for Wikipedia</a></p>
<p>18 0.43640897 <a title="352-lsi-18" href="./acl-2013-The_Impact_of_Topic_Bias_on_Quality_Flaw_Prediction_in_Wikipedia.html">346 acl-2013-The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia</a></p>
<p>19 0.43001309 <a title="352-lsi-19" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>20 0.41717505 <a title="352-lsi-20" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.042), (6, 0.052), (11, 0.072), (21, 0.289), (24, 0.038), (26, 0.06), (35, 0.12), (42, 0.039), (48, 0.05), (70, 0.039), (71, 0.011), (88, 0.021), (90, 0.028), (95, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85967284 <a title="352-lda-1" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>Author: Ben King ; Rahul Jha ; Dragomir Radev ; Robert Mankoff</p><p>Abstract: In this paper, we study the problem of automatically annotating the factoids present in collective discourse. Factoids are information units that are shared between instances of collective discourse and may have many different ways ofbeing realized in words. Our approach divides this problem into two steps, using a graph-based approach for each step: (1) factoid discovery, finding groups of words that correspond to the same factoid, and (2) factoid assignment, using these groups of words to mark collective discourse units that contain the respective factoids. We study this on two novel data sets: the New Yorker caption contest data set, and the crossword clues data set.</p><p>same-paper 2 0.78608394 <a title="352-lda-2" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>Author: Xingxing Zhang ; Jianwen Zhang ; Junyu Zeng ; Jun Yan ; Zheng Chen ; Zhifang Sui</p><p>Abstract: Distant supervision (DS) is an appealing learning method which learns from existing relational facts to extract more from a text corpus. However, the accuracy is still not satisfying. In this paper, we point out and analyze some critical factors in DS which have great impact on accuracy, including valid entity type detection, negative training examples construction and ensembles. We propose an approach to handle these factors. By experimenting on Wikipedia articles to extract the facts in Freebase (the top 92 relations), we show the impact of these three factors on the accuracy of DS and the remarkable improvement led by the proposed approach.</p><p>3 0.75596082 <a title="352-lda-3" href="./acl-2013-Predicting_and_Eliciting_Addressee%27s_Emotion_in_Online_Dialogue.html">282 acl-2013-Predicting and Eliciting Addressee's Emotion in Online Dialogue</a></p>
<p>Author: Takayuki Hasegawa ; Nobuhiro Kaji ; Naoki Yoshinaga ; Masashi Toyoda</p><p>Abstract: While there have been many attempts to estimate the emotion of an addresser from her/his utterance, few studies have explored how her/his utterance affects the emotion of the addressee. This has motivated us to investigate two novel tasks: predicting the emotion of the addressee and generating a response that elicits a specific emotion in the addressee’s mind. We target Japanese Twitter posts as a source of dialogue data and automatically build training data for learning the predictors and generators. The feasibility of our approaches is assessed by using 1099 utterance-response pairs that are built by . five human workers.</p><p>4 0.73917955 <a title="352-lda-4" href="./acl-2013-Grounded_Language_Learning_from_Video_Described_with_Sentences.html">175 acl-2013-Grounded Language Learning from Video Described with Sentences</a></p>
<p>Author: Haonan Yu ; Jeffrey Mark Siskind</p><p>Abstract: We present a method that learns representations for word meanings from short video clips paired with sentences. Unlike prior work on learning language from symbolic input, our input consists of video of people interacting with multiple complex objects in outdoor environments. Unlike prior computer-vision approaches that learn from videos with verb labels or images with noun labels, our labels are sentences containing nouns, verbs, prepositions, adjectives, and adverbs. The correspondence between words and concepts in the video is learned in an unsupervised fashion, even when the video depicts si- multaneous events described by multiple sentences or when different aspects of a single event are described with multiple sentences. The learned word meanings can be subsequently used to automatically generate description of new video.</p><p>5 0.66769624 <a title="352-lda-5" href="./acl-2013-Part-of-Speech_Induction_in_Dependency_Trees_for_Statistical_Machine_Translation.html">276 acl-2013-Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation</a></p>
<p>Author: Akihiro Tamura ; Taro Watanabe ; Eiichiro Sumita ; Hiroya Takamura ; Manabu Okumura</p><p>Abstract: This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forest- to-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model.</p><p>6 0.61846548 <a title="352-lda-6" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>7 0.54693311 <a title="352-lda-7" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>8 0.54692698 <a title="352-lda-8" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>9 0.54546338 <a title="352-lda-9" href="./acl-2013-Identifying_Bad_Semantic_Neighbors_for_Improving_Distributional_Thesauri.html">185 acl-2013-Identifying Bad Semantic Neighbors for Improving Distributional Thesauri</a></p>
<p>10 0.54416597 <a title="352-lda-10" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>11 0.54398823 <a title="352-lda-11" href="./acl-2013-Text_Classification_based_on_the_Latent_Topics_of_Important_Sentences_extracted_by_the_PageRank_Algorithm.html">341 acl-2013-Text Classification based on the Latent Topics of Important Sentences extracted by the PageRank Algorithm</a></p>
<p>12 0.54322582 <a title="352-lda-12" href="./acl-2013-Automated_Collocation_Suggestion_for_Japanese_Second_Language_Learners.html">58 acl-2013-Automated Collocation Suggestion for Japanese Second Language Learners</a></p>
<p>13 0.54192346 <a title="352-lda-13" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>14 0.54160106 <a title="352-lda-14" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>15 0.54101795 <a title="352-lda-15" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>16 0.54095435 <a title="352-lda-16" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>17 0.54073584 <a title="352-lda-17" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>18 0.54037499 <a title="352-lda-18" href="./acl-2013-Word_Association_Profiles_and_their_Use_for_Automated_Scoring_of_Essays.html">389 acl-2013-Word Association Profiles and their Use for Automated Scoring of Essays</a></p>
<p>19 0.54029888 <a title="352-lda-19" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>20 0.54023099 <a title="352-lda-20" href="./acl-2013-Discovering_User_Interactions_in_Ideological_Discussions.html">121 acl-2013-Discovering User Interactions in Ideological Discussions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
