<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-291" href="#">acl2013-291</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</h1>
<br/><p>Source: <a title="acl-2013-291-pdf" href="http://aclweb.org/anthology//P/P13/P13-1171.pdf">pdf</a></p><p>Author: Wen-tau Yih ; Ming-Wei Chang ; Christopher Meek ; Andrzej Pastusiak</p><p>Abstract: In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.</p><p>Reference: <a title="acl-2013-291-reference" href="../acl2013_reference/acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract In this paper, we study the answer sentence selection problem for question answering. [sent-2, score-0.708]
</p><p>2 1 Introduction Open-domain question answering (QA), which fulfills a user’s information need by outputting direct answers to natural language queries, is a challenging but important problem (Etzioni, 2011). [sent-7, score-0.457]
</p><p>3 State-of-the-art QA systems often implement a complicated pipeline architecture, consisting of question analysis, document or passage retrieval, answer selection and verification (Ferrucci, 2012; Moldovan et al. [sent-8, score-0.691]
</p><p>4 In this paper, we focus on one of the key subtasks answer sentence selection. [sent-10, score-0.369]
</p><p>5 Given a question and a set of candidate sentences, the task is to choose the correct sentence that contains the exact answer and can sufficiently support the answer choice. [sent-11, score-0.985]
</p><p>6 For instance, although both of the following sentences contain the answer “Jack Lemmon” to the question “Who won the best actor Oscar in 1973? [sent-12, score-0.659]
</p><p>7 One of the benefits of answer sentence selection is that the output can be provided directly to the user. [sent-16, score-0.448]
</p><p>8 Answer sentence selection can be naturally reduced to a semantic text matching problem. [sent-18, score-0.389]
</p><p>9 Conceptually, we would like to measure how close the question and sentence can be matched semantically. [sent-19, score-0.325]
</p><p>10 Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al. [sent-20, score-0.257]
</p><p>11 In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. [sent-28, score-0.295]
</p><p>12 We formulate answer selection as a semantic matching problem with a latent word-alignment structure as in (Chang et al. [sent-33, score-0.765]
</p><p>13 First, by incorporating the abundant information from a variety of lexical semantic models, the answer selection system can be enhanced substantially, regardless of the choice of learning algorithms and settings. [sent-36, score-0.724]
</p><p>14 Compared to the previous work, our latent alignment model improves the result on a benchmark dataset by a wide margin the mean average precision (MAP) and mean reciprocal rank (MRR) scores are increased by 25. [sent-37, score-0.233]
</p><p>15 Second, while the latent alignment model performs better than unstructured models, the difference diminishes after adding the enhanced lexical semantics information. [sent-40, score-0.641]
</p><p>16 This may suggest that compared to introducing complex structured constraints, incorporating shallow semantic information is both more effective and computationally inexpensive in improving the performance, at least for the specific word alignment model tested in this work. [sent-41, score-0.356]
</p><p>17 3 defines the problem of answer sentence selection, –  along with the high-level description of our solution. [sent-46, score-0.369]
</p><p>18 The enhanced lexical semantic models and the learning frameworks we explore are presented in Sec. [sent-47, score-0.281]
</p><p>19 2  Related Work  While the task of question answering has a long history dated back to the dawn of artificial intelligence, early systems like STUDENT (Winograd, 1977) and LUNAR (Woods, 1973) are typically designed to demonstrate natural language understanding for a small and specific domain. [sent-54, score-0.36]
</p><p>20 The Text REtrieval Conference (TREC) Question Answering Track was arguably the first largescale evaluation of open-domain question answering (Voorhees and Tice, 2000). [sent-55, score-0.36]
</p><p>21 Given a factoid question along with a collection of documents, a system is required to return the exact answer, along with the document that sup-  ports the answer. [sent-57, score-0.26]
</p><p>22 TV quiz show provides another open-domain question answering setting, in which IBM’s Watson system famously beat the two highest ranked players (Ferrucci, 2012). [sent-59, score-0.36]
</p><p>23 Questions in this game are presented in a statement form and the system needs to identify the true question and to give the exact answer. [sent-60, score-0.26]
</p><p>24 A short sentence or paragraph to justify the answer is not required in either TREC-QA or Jeopardy! [sent-61, score-0.369]
</p><p>25 As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. [sent-62, score-0.462]
</p><p>26 Limiting the scope of an answer to a sentence is first highlighted by Wang et al. [sent-63, score-0.369]
</p><p>27 (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. [sent-64, score-0.369]
</p><p>28 (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. [sent-66, score-0.372]
</p><p>29 This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. [sent-68, score-0.233]
</p><p>30 Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. [sent-71, score-0.407]
</p><p>31 Although lexical semantic information derived from WordNet has been used in some of these approaches, the research has mainly focused on modeling the mapping between the syntactic structures of questions and sentences, produced from syntactic analysis. [sent-72, score-0.303]
</p><p>32 The potential improvement from enhanced lexical semantic models seems to have been deliberately overlooked. [sent-73, score-0.281]
</p><p>33 1 3  Problem Definition  We consider the answer selection problem in a supervised learning setting. [sent-74, score-0.383]
</p><p>34 Figure 1: An example pair of question and answer sentence, adapted from (Harabagiu and Moldovan, 2001). [sent-78, score-0.611]
</p><p>35 ,  ,  ,  {(yi1 si1), (yi1 si2), · · · (yin, sin)}, where yij = 1{ iyndicates that sent)e,n·c·e· sij is a co)r}r,ec wt answer to question qi, and 0 otherwise. [sent-80, score-0.564]
</p><p>36 Using this labeled data, our goal is to learn a probabilistic classifier to predict the label of a new, unseen pair of question and sentence. [sent-81, score-0.307]
</p><p>37 Fundamentally, what the classifier predicts is whether the sentence “matches” the question semantically. [sent-82, score-0.325]
</p><p>38 Without representing the question and sentence in logic or syntactic trees, we take a word-alignment view for solving this problem. [sent-84, score-0.325]
</p><p>39 With this alignment structure, features like the degree of mapping or whether all the content words in the question can be mapped to some words in the sentence can be extracted and help improve the classifier. [sent-88, score-0.52]
</p><p>40 In this work, we focus our study on leveraging the lowlevel semantic cues from recently proposed lexical semantic models. [sent-94, score-0.265]
</p><p>41 As will be shown in our experiments, such information not only improves a latent structure learning method, but also makes a simple bipartite matching approach extremely strong. [sent-95, score-0.286]
</p><p>42 2 4  Lexical Semantic Models  In this section, we introduce the lexical semantic models we adopt for solving the semantic matching problem in answer selection. [sent-96, score-0.718]
</p><p>43 To go beyond the simple, limited surface-form matching, we aim to pair words that are semantically related, specifically measured by models of word relations including synonymy/antonymy, hypernymy/hyponymy (the Is-A relation) and general semantic word similarity. [sent-97, score-0.294]
</p><p>44 In order to estimate the degree of synonymy, we leverage a recently proposed polarity-inducing latent semantic analysis (PILSA) model (Yih et al. [sent-104, score-0.291]
</p><p>45 2 Hypernymy and Hyponymy The Class-Inclusion or Is-A relation is commonly  observed between words in questions and answer sentences. [sent-118, score-0.397]
</p><p>46 For example, to correctly answer the question “What color is Saturn? [sent-119, score-0.605]
</p><p>47 Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation. [sent-150, score-0.253]
</p><p>48 5  Learning QA Matching Models  In this section, we investigate the effectiveness of various learning models for matching questions and sentences, including the bag-of-words setting 5http : / /www . [sent-175, score-0.283]
</p><p>49 1 Bag-of-Words Model The bag-of-words model treats each question and  sentence as an unstructured bag of words. [sent-180, score-0.412]
</p><p>50 When comparing a question with a sentence, the model first matches each word in the question to each word in the sentence. [sent-181, score-0.628]
</p><p>51 Formally, let x = (q, s) be a pair of question q and sentence s. [sent-184, score-0.372]
</p><p>52 In order to leverage the latent structured information, we adapt a recently proposed framework of learning constrained latent representations (LCLR) (Chang et al. [sent-201, score-0.324]
</p><p>53 In this answer selection task, x = (q, s) represents a pair of question q and candidate sentence s. [sent-206, score-0.807]
</p><p>54 3, h refers to the latent alignment between q and s. [sent-208, score-0.233]
</p><p>55 (3) is: candidate sentence s correctly answers question q if and only if the decision can be supported by the best alignment h. [sent-210, score-0.521]
</p><p>56 We then show the positive impact of adding information of word relations from various lexical semantics models, with some discussion on the limitation of the word-matching approach. [sent-220, score-0.238]
</p><p>57 1 Data & Evaluation Metrics The answer selection dataset we used was originally created by Wang et al. [sent-222, score-0.383]
</p><p>58 ” In average, each question is associated with approximately 33 answer candidate sentences. [sent-225, score-0.616]
</p><p>59 A pair of question and sentence is judged positive if the sentence contains the exact answer key and can provide sufficient context as supporting evidence. [sent-226, score-0.741]
</p><p>60 The task is treated as a sentence ranking problem for each question and thus evaluated in Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), using the official TREC evaluation program. [sent-229, score-0.325]
</p><p>61 (2007) presented a generative probabilistic model based on a Quasi-synchronous Grammar formulation and was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. [sent-235, score-0.233]
</p><p>62 In contrast, Heilman and 6Each word in the question needs to be linked to a word in the sentence. [sent-236, score-0.368]
</p><p>63 Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. [sent-259, score-0.407]
</p><p>64 All these systems incorporated lexical semantics features derived from WordNet and named entity features. [sent-261, score-0.246]
</p><p>65 The second one, word count, is to count how many words in the question that also occur in the answer sentence, after removing stopwords7,  and lowering the case. [sent-264, score-0.658]
</p><p>66 Finally, the last baseline method, weighted word count, is basically the same as identical word matching, but the count is re-weighted using the IDF value of the question word. [sent-265, score-0.408]
</p><p>67 8 In addition, weighting the question words with their IDF values further improves the results. [sent-270, score-0.26]
</p><p>68 all the features are based on the properties of the pair of a word from the question and a word from the candidate sentence. [sent-277, score-0.467]
</p><p>69 Features used in the experiments can be categorized into six types: identical word matching (I), lemma matching (L), WordNet (WN), enhanced Lexical Semantics (LS), Named Entity matching (NE) and Answer type checking (Ans). [sent-279, score-0.654]
</p><p>70 Inspired by the weighted word count baseline, all features except (Ans) are weighted by the IDF value of the question word. [sent-280, score-0.354]
</p><p>71 Staring from the our baseline model, weighted word count, the identical word matching (I) feature checks whether the pair of words are the same. [sent-282, score-0.304]
</p><p>72 Instead of checking the surface form of the word, lemma matching (L) verifies whether the two words have the same lemma form. [sent-283, score-0.231]
</p><p>73 Alternatively, the enhanced lexical semantics (LS) fea-  tures apply the models described in Sec. [sent-285, score-0.253]
</p><p>74 4 to the word pair and use their estimated degree of synonymy, antonymy, hyponymy and semantic relatedness as features. [sent-286, score-0.385]
</p><p>75 Named entity matching (NE) checks whether two words are individually part of some named entities with the same type. [sent-287, score-0.254]
</p><p>76 Finally, when the question word is the WH-word, we check if the paired word belongs to some phrase that has the correct answer type using simple rules, such as “Who should link to a word that is part of a named entity of type Person. [sent-288, score-0.831]
</p><p>77 For the structured-output setting, we used the framework of learning constrained latent representa-  tion (LCLR) and required that each question word needed to be mapped to a word in the sentence. [sent-294, score-0.505]
</p><p>78 Feature groups are identical word matching (I), lemma matching (L), WordNet (WN) and enhanced Lexical Semantics (LS). [sent-304, score-0.505]
</p><p>79 First, while incorporating more information of the word pairs in general helps, it is clear that mapping words beyond surface-form matching with the help of WordNet (Line #3 vs. [sent-309, score-0.304]
</p><p>80 Overall, by simply incorporating more in-  formation on word relations, we gain approximately 10 points in both MAP and MRR compared to surface-form matching (Line #4 vs. [sent-313, score-0.263]
</p><p>81 However, adding more information like named entity matching and answer type verification does not seem to help much (Line #5 vs. [sent-315, score-0.606]
</p><p>82 4  Limitation of Word Matching Models  Although we have demonstrated the benefits of leveraging various lexical semantic models to help find the association between words, the problem of question answering is nevertheless far from solved using the word-based approach. [sent-322, score-0.529]
</p><p>83 While the first two can be improved by, say, using a better named entity tagger, incorporating other knowledge bases and building a question classifier, how to solve the third problem is tricky. [sent-324, score-0.425]
</p><p>84 This is a correct answer sentence because “winning a best actor Oscar” implies that the role Gordon Gekko is the main character. [sent-327, score-0.464]
</p><p>85 It is hard to believe that a pure word-matching model would be able to solve this type of “inferential question answering” problem. [sent-328, score-0.26]
</p><p>86 7  Conclusions  In this paper, we present an experimental study on solving the answer selection problem using en-  hanced lexical semantic models. [sent-329, score-0.552]
</p><p>87 Following the word-alignment paradigm, we find that the rich lexical semantic information improves the models consistently in the unstructured bag-of-words setting and also in the framework of learning latent structures. [sent-330, score-0.434]
</p><p>88 Another interesting finding we have is that while the latent structured model, LCLR, performs better than the other two unstructured models, the difference diminishes after more information, including the enhanced lexical semantic knowledge and answer type verification, has been incorporated. [sent-331, score-0.927]
</p><p>89 This may suggest that adding shallow semantic information is more effective than introducing complex structured constraints, at least for the specific word alignment model we experimented with in this work. [sent-332, score-0.296]
</p><p>90 First, although we focus on improving TREC-style open-domain question answering in this work, we would like to apply the proposed technology to other QA scenarios, such  as community-based QA (CQA). [sent-334, score-0.36]
</p><p>91 For instance, the sentence matching technique can help map a given question to some questions in an existing CQA database (e. [sent-335, score-0.616]
</p><p>92 Moreover, the answer sentence selection scheme could also be useful in extracting the most related sentences from the answer text to form a summary answer. [sent-339, score-0.752]
</p><p>93 Second, because the task of answer sentence selection is very similar to paraphrase detection (Dolan et al. [sent-340, score-0.448]
</p><p>94 , 2006), we would like to investigate whether systems for these tasks can be improved by incorporating enhanced lexical semantic knowledge as well. [sent-342, score-0.341]
</p><p>95 Finally, we would like to improve our system for the answer sentence selection task and for question answering in general. [sent-343, score-0.808]
</p><p>96 4, we plan to use logic-like semantic representations of questions and sentences, and explore the role of lexical semantics for handling questions that require inference. [sent-346, score-0.423]
</p><p>97 Performance issues and error analysis in an open-domain question answering system. [sent-492, score-0.36]
</p><p>98 COGEX: A semantically and contextually enriched logic prover for question answering. [sent-500, score-0.26]
</p><p>99 A word at a time: computing word relatedness using temporal semantic analysis. [sent-527, score-0.273]
</p><p>100 Probabilistic treeedit models with structured latent variables for textual entailment and question answering. [sent-605, score-0.447]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('answer', 0.304), ('lclr', 0.277), ('question', 0.26), ('yih', 0.192), ('probase', 0.158), ('matching', 0.149), ('synonymy', 0.14), ('latent', 0.137), ('moldovan', 0.131), ('enhanced', 0.112), ('harabagiu', 0.105), ('qa', 0.101), ('answering', 0.1), ('alignment', 0.096), ('semantic', 0.096), ('actor', 0.095), ('questions', 0.093), ('wang', 0.093), ('unstructured', 0.087), ('bdt', 0.083), ('gekko', 0.083), ('lemmon', 0.083), ('meek', 0.083), ('heilman', 0.083), ('selection', 0.079), ('wordnet', 0.079), ('trec', 0.076), ('vsms', 0.073), ('lexical', 0.073), ('relatedness', 0.069), ('semantics', 0.068), ('mrr', 0.068), ('antonymy', 0.068), ('diminishes', 0.068), ('blanco', 0.068), ('sentence', 0.065), ('oscar', 0.065), ('fastest', 0.064), ('punyakanok', 0.061), ('hyponymy', 0.061), ('idf', 0.06), ('incorporating', 0.06), ('similarity', 0.059), ('jack', 0.058), ('jeopardy', 0.058), ('degree', 0.058), ('smith', 0.057), ('car', 0.056), ('antonyms', 0.056), ('ans', 0.056), ('jaguar', 0.055), ('moonlight', 0.055), ('pilsa', 0.055), ('saturn', 0.055), ('sonata', 0.055), ('wq', 0.055), ('zhila', 0.055), ('relational', 0.055), ('synonyms', 0.055), ('entity', 0.054), ('gordon', 0.054), ('gabrilovich', 0.054), ('reisinger', 0.054), ('dean', 0.054), ('word', 0.054), ('trees', 0.053), ('candidate', 0.052), ('named', 0.051), ('taxonomy', 0.051), ('recurrent', 0.05), ('structured', 0.05), ('map', 0.049), ('rink', 0.049), ('vq', 0.049), ('lunar', 0.049), ('felzenszwalb', 0.049), ('zweig', 0.049), ('outputting', 0.049), ('verification', 0.048), ('answers', 0.048), ('pair', 0.047), ('tv', 0.046), ('rnnlm', 0.045), ('radinsky', 0.045), ('robertson', 0.045), ('roth', 0.044), ('relations', 0.043), ('morante', 0.043), ('bilotti', 0.043), ('tree', 0.042), ('semeval', 0.042), ('projection', 0.042), ('mapping', 0.041), ('setting', 0.041), ('lemma', 0.041), ('color', 0.041), ('mikolov', 0.04), ('voorhees', 0.04), ('bollacker', 0.04), ('count', 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="291-tfidf-1" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>Author: Wen-tau Yih ; Ming-Wei Chang ; Christopher Meek ; Andrzej Pastusiak</p><p>Abstract: In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.</p><p>2 0.30132335 <a title="291-tfidf-2" href="./acl-2013-Automatic_Coupling_of_Answer_Extraction_and_Information_Retrieval.html">60 acl-2013-Automatic Coupling of Answer Extraction and Information Retrieval</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Peter Clark</p><p>Abstract: Information Retrieval (IR) and Answer Extraction are often designed as isolated or loosely connected components in Question Answering (QA), with repeated overengineering on IR, and not necessarily performance gain for QA. We propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured IR model. Our method is very quick to implement, and significantly improves IR for QA (measured in Mean Average Precision and Mean Reciprocal Rank) by 10%-20% against an uncoupled retrieval baseline in both document and passage retrieval, which further leads to a downstream 20% improvement in QA F1.</p><p>3 0.21180305 <a title="291-tfidf-3" href="./acl-2013-Minimum_Bayes_Risk_based_Answer_Re-ranking_for_Question_Answering.html">241 acl-2013-Minimum Bayes Risk based Answer Re-ranking for Question Answering</a></p>
<p>Author: Nan Duan</p><p>Abstract: This paper presents two minimum Bayes risk (MBR) based Answer Re-ranking (MBRAR) approaches for the question answering (QA) task. The first approach re-ranks single QA system’s outputs by using a traditional MBR model, by measuring correlations between answer candidates; while the second approach reranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model. Evaluations are performed on factoid questions selected from two different domains: Jeopardy! and Web, and significant improvements are achieved on all data sets.</p><p>4 0.20962633 <a title="291-tfidf-4" href="./acl-2013-Question_Analysis_for_Polish_Question_Answering.html">290 acl-2013-Question Analysis for Polish Question Answering</a></p>
<p>Author: Piotr Przybyla</p><p>Abstract: This study is devoted to the problem of question analysis for a Polish question answering system. The goal of the question analysis is to determine its general structure, type of an expected answer and create a search query for finding relevant documents in a textual knowledge base. The paper contains an overview of available solutions of these problems, description of their implementation and presents an evaluation based on a set of 1137 questions from a Polish quiz TV show. The results help to understand how an environment of a Slavonic language affects the performance of methods created for English.</p><p>5 0.2025854 <a title="291-tfidf-5" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>Author: Guangyou Zhou ; Fang Liu ; Yang Liu ; Shizhu He ; Jun Zhao</p><p>Abstract: Community question answering (CQA) has become an increasingly popular research topic. In this paper, we focus on the problem of question retrieval. Question retrieval in CQA can automatically find the most relevant and recent questions that have been solved by other users. However, the word ambiguity and word mismatch problems bring about new challenges for question retrieval in CQA. State-of-the-art approaches address these issues by implicitly expanding the queried questions with additional words or phrases using monolingual translation models. While useful, the effectiveness of these models is highly dependent on the availability of quality parallel monolingual corpora (e.g., question-answer pairs) in the absence of which they are troubled by noise issue. In this work, we propose an alternative way to address the word ambiguity and word mismatch problems by taking advantage of potentially rich semantic information drawn from other languages. Our proposed method employs statistical machine translation to improve question retrieval and enriches the question representation with the translated words from other languages via matrix factorization. Experiments conducted on a real CQA data show that our proposed approach is promising.</p><p>6 0.19620982 <a title="291-tfidf-6" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>7 0.19258192 <a title="291-tfidf-7" href="./acl-2013-Deceptive_Answer_Prediction_with_User_Preference_Graph.html">107 acl-2013-Deceptive Answer Prediction with User Preference Graph</a></p>
<p>8 0.17914751 <a title="291-tfidf-8" href="./acl-2013-Question_Classification_Transfer.html">292 acl-2013-Question Classification Transfer</a></p>
<p>9 0.17617041 <a title="291-tfidf-9" href="./acl-2013-Latent_Semantic_Tensor_Indexing_for_Community-based_Question_Answering.html">218 acl-2013-Latent Semantic Tensor Indexing for Community-based Question Answering</a></p>
<p>10 0.16933776 <a title="291-tfidf-10" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>11 0.13895838 <a title="291-tfidf-11" href="./acl-2013-Align%2C_Disambiguate_and_Walk%3A_A_Unified_Approach_for_Measuring_Semantic_Similarity.html">43 acl-2013-Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity</a></p>
<p>12 0.12934071 <a title="291-tfidf-12" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>13 0.12629102 <a title="291-tfidf-13" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>14 0.12011782 <a title="291-tfidf-14" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>15 0.11887038 <a title="291-tfidf-15" href="./acl-2013-Latent_Semantic_Matching%3A_Application_to_Cross-language_Text_Categorization_without_Alignment_Information.html">217 acl-2013-Latent Semantic Matching: Application to Cross-language Text Categorization without Alignment Information</a></p>
<p>16 0.11439791 <a title="291-tfidf-16" href="./acl-2013-SEMILAR%3A_The_Semantic_Similarity_Toolkit.html">304 acl-2013-SEMILAR: The Semantic Similarity Toolkit</a></p>
<p>17 0.11245526 <a title="291-tfidf-17" href="./acl-2013-PAL%3A_A_Chatterbot_System_for_Answering_Domain-specific_Questions.html">266 acl-2013-PAL: A Chatterbot System for Answering Domain-specific Questions</a></p>
<p>18 0.10936445 <a title="291-tfidf-18" href="./acl-2013-Why-Question_Answering_using_Intra-_and_Inter-Sentential_Causal_Relations.html">387 acl-2013-Why-Question Answering using Intra- and Inter-Sentential Causal Relations</a></p>
<p>19 0.10880362 <a title="291-tfidf-19" href="./acl-2013-A_New_Set_of_Norms_for_Semantic_Relatedness_Measures.html">12 acl-2013-A New Set of Norms for Semantic Relatedness Measures</a></p>
<p>20 0.10494874 <a title="291-tfidf-20" href="./acl-2013-SPred%3A_Large-scale_Harvesting_of_Semantic_Predicates.html">306 acl-2013-SPred: Large-scale Harvesting of Semantic Predicates</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.318), (1, 0.078), (2, 0.023), (3, -0.2), (4, 0.045), (5, 0.044), (6, -0.052), (7, -0.357), (8, 0.117), (9, 0.013), (10, 0.121), (11, -0.08), (12, 0.042), (13, -0.084), (14, 0.08), (15, 0.085), (16, 0.06), (17, -0.041), (18, 0.05), (19, 0.022), (20, 0.063), (21, 0.023), (22, 0.029), (23, -0.106), (24, -0.055), (25, 0.043), (26, -0.002), (27, -0.022), (28, 0.001), (29, -0.023), (30, 0.005), (31, 0.016), (32, 0.023), (33, -0.024), (34, 0.041), (35, -0.017), (36, 0.029), (37, -0.013), (38, 0.014), (39, 0.044), (40, 0.0), (41, 0.034), (42, -0.022), (43, 0.012), (44, -0.025), (45, 0.04), (46, -0.026), (47, -0.028), (48, -0.02), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95850015 <a title="291-lsi-1" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>Author: Wen-tau Yih ; Ming-Wei Chang ; Christopher Meek ; Andrzej Pastusiak</p><p>Abstract: In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.</p><p>2 0.88282144 <a title="291-lsi-2" href="./acl-2013-Latent_Semantic_Tensor_Indexing_for_Community-based_Question_Answering.html">218 acl-2013-Latent Semantic Tensor Indexing for Community-based Question Answering</a></p>
<p>Author: Xipeng Qiu ; Le Tian ; Xuanjing Huang</p><p>Abstract: Retrieving similar questions is very important in community-based question answering(CQA) . In this paper, we propose a unified question retrieval model based on latent semantic indexing with tensor analysis, which can capture word associations among different parts of CQA triples simultaneously. Thus, our method can reduce lexical chasm of question retrieval with the help of the information of question content and answer parts. The experimental result shows that our method outperforms the traditional methods.</p><p>3 0.88219714 <a title="291-lsi-3" href="./acl-2013-Automatic_Coupling_of_Answer_Extraction_and_Information_Retrieval.html">60 acl-2013-Automatic Coupling of Answer Extraction and Information Retrieval</a></p>
<p>Author: Xuchen Yao ; Benjamin Van Durme ; Peter Clark</p><p>Abstract: Information Retrieval (IR) and Answer Extraction are often designed as isolated or loosely connected components in Question Answering (QA), with repeated overengineering on IR, and not necessarily performance gain for QA. We propose to tightly integrate them by coupling automatically learned features for answer extraction to a shallow-structured IR model. Our method is very quick to implement, and significantly improves IR for QA (measured in Mean Average Precision and Mean Reciprocal Rank) by 10%-20% against an uncoupled retrieval baseline in both document and passage retrieval, which further leads to a downstream 20% improvement in QA F1.</p><p>4 0.85312784 <a title="291-lsi-4" href="./acl-2013-PAL%3A_A_Chatterbot_System_for_Answering_Domain-specific_Questions.html">266 acl-2013-PAL: A Chatterbot System for Answering Domain-specific Questions</a></p>
<p>Author: Yuanchao Liu ; Ming Liu ; Xiaolong Wang ; Limin Wang ; Jingjing Li</p><p>Abstract: In this paper, we propose PAL, a prototype chatterbot for answering non-obstructive psychological domain-specific questions. This system focuses on providing primary suggestions or helping people relieve pressure by extracting knowledge from online forums, based on which the chatterbot system is constructed. The strategies used by PAL, including semantic-extension-based question matching, solution management with personal information consideration, and XML-based knowledge pattern construction, are described and discussed. We also conduct a primary test for the feasibility of our system.</p><p>5 0.84130204 <a title="291-lsi-5" href="./acl-2013-Minimum_Bayes_Risk_based_Answer_Re-ranking_for_Question_Answering.html">241 acl-2013-Minimum Bayes Risk based Answer Re-ranking for Question Answering</a></p>
<p>Author: Nan Duan</p><p>Abstract: This paper presents two minimum Bayes risk (MBR) based Answer Re-ranking (MBRAR) approaches for the question answering (QA) task. The first approach re-ranks single QA system’s outputs by using a traditional MBR model, by measuring correlations between answer candidates; while the second approach reranks the combined outputs of multiple QA systems with heterogenous answer extraction components by using a mixture model-based MBR model. Evaluations are performed on factoid questions selected from two different domains: Jeopardy! and Web, and significant improvements are achieved on all data sets.</p><p>6 0.82645708 <a title="291-lsi-6" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>7 0.81114739 <a title="291-lsi-7" href="./acl-2013-Question_Analysis_for_Polish_Question_Answering.html">290 acl-2013-Question Analysis for Polish Question Answering</a></p>
<p>8 0.77961904 <a title="291-lsi-8" href="./acl-2013-Question_Classification_Transfer.html">292 acl-2013-Question Classification Transfer</a></p>
<p>9 0.77717656 <a title="291-lsi-9" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>10 0.71873707 <a title="291-lsi-10" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>11 0.71221685 <a title="291-lsi-11" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<p>12 0.69662654 <a title="291-lsi-12" href="./acl-2013-Deceptive_Answer_Prediction_with_User_Preference_Graph.html">107 acl-2013-Deceptive Answer Prediction with User Preference Graph</a></p>
<p>13 0.68009824 <a title="291-lsi-13" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>14 0.64709723 <a title="291-lsi-14" href="./acl-2013-Offspring_from_Reproduction_Problems%3A_What_Replication_Failure_Teaches_Us.html">262 acl-2013-Offspring from Reproduction Problems: What Replication Failure Teaches Us</a></p>
<p>15 0.64705628 <a title="291-lsi-15" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>16 0.62123007 <a title="291-lsi-16" href="./acl-2013-Why-Question_Answering_using_Intra-_and_Inter-Sentential_Causal_Relations.html">387 acl-2013-Why-Question Answering using Intra- and Inter-Sentential Causal Relations</a></p>
<p>17 0.57668078 <a title="291-lsi-17" href="./acl-2013-Grounded_Unsupervised_Semantic_Parsing.html">176 acl-2013-Grounded Unsupervised Semantic Parsing</a></p>
<p>18 0.55744761 <a title="291-lsi-18" href="./acl-2013-A_corpus-based_evaluation_method_for_Distributional_Semantic_Models.html">31 acl-2013-A corpus-based evaluation method for Distributional Semantic Models</a></p>
<p>19 0.52914673 <a title="291-lsi-19" href="./acl-2013-Learning_Semantic_Textual_Similarity_with_Structural_Representations.html">222 acl-2013-Learning Semantic Textual Similarity with Structural Representations</a></p>
<p>20 0.52677709 <a title="291-lsi-20" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.08), (6, 0.047), (11, 0.088), (15, 0.015), (24, 0.036), (26, 0.041), (28, 0.01), (35, 0.127), (42, 0.036), (48, 0.054), (64, 0.016), (70, 0.081), (77, 0.138), (88, 0.038), (90, 0.027), (95, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91714936 <a title="291-lda-1" href="./acl-2013-Mining_Informal_Language_from_Chinese_Microtext%3A_Joint_Word_Recognition_and_Segmentation.html">243 acl-2013-Mining Informal Language from Chinese Microtext: Joint Word Recognition and Segmentation</a></p>
<p>Author: Aobo Wang ; Min-Yen Kan</p><p>Abstract: We address the problem of informal word recognition in Chinese microblogs. A key problem is the lack of word delimiters in Chinese. We exploit this reliance as an opportunity: recognizing the relation between informal word recognition and Chinese word segmentation, we propose to model the two tasks jointly. Our joint inference method significantly outperforms baseline systems that conduct the tasks individually or sequentially.</p><p>same-paper 2 0.8853274 <a title="291-lda-2" href="./acl-2013-Question_Answering_Using_Enhanced_Lexical_Semantic_Models.html">291 acl-2013-Question Answering Using Enhanced Lexical Semantic Models</a></p>
<p>Author: Wen-tau Yih ; Ming-Wei Chang ; Christopher Meek ; Andrzej Pastusiak</p><p>Abstract: In this paper, we study the answer sentence selection problem for question answering. Unlike previous work, which primarily leverages syntactic analysis through dependency tree matching, we focus on improving the performance using models of lexical semantic resources. Experiments show that our systems can be consistently and significantly improved with rich lexical semantic information, regardless of the choice of learning algorithms. When evaluated on a benchmark dataset, the MAP and MRR scores are increased by 8 to 10 points, compared to one of our baseline systems using only surface-form matching. Moreover, our best system also outperforms pervious work that makes use of the dependency tree structure by a wide margin.</p><p>3 0.84740752 <a title="291-lda-3" href="./acl-2013-Hierarchical_Phrase_Table_Combination_for_Machine_Translation.html">181 acl-2013-Hierarchical Phrase Table Combination for Machine Translation</a></p>
<p>Author: Conghui Zhu ; Taro Watanabe ; Eiichiro Sumita ; Tiejun Zhao</p><p>Abstract: Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data. With the growth of the available data across different domains, it is computationally demanding to perform batch training every time when new data comes. In face of the problem, we propose an efficient phrase table combination method. In particular, we train a Bayesian phrasal inversion transduction grammars for each domain separately. The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman-Yor process. The performance measured by BLEU is at least as comparable to the traditional batch training method. Furthermore, each phrase table is trained separately in each domain, and while computational overhead is significantly reduced by training them in parallel.</p><p>4 0.83505166 <a title="291-lda-4" href="./acl-2013-An_Infinite_Hierarchical_Bayesian_Model_of_Phrasal_Translation.html">46 acl-2013-An Infinite Hierarchical Bayesian Model of Phrasal Translation</a></p>
<p>Author: Trevor Cohn ; Gholamreza Haffari</p><p>Abstract: Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.</p><p>5 0.82844692 <a title="291-lda-5" href="./acl-2013-Paraphrase-Driven_Learning_for_Open_Question_Answering.html">272 acl-2013-Paraphrase-Driven Learning for Open Question Answering</a></p>
<p>Author: Anthony Fader ; Luke Zettlemoyer ; Oren Etzioni</p><p>Abstract: We study question answering as a machine learning problem, and induce a function that maps open-domain questions to queries over a database of web extractions. Given a large, community-authored, question-paraphrase corpus, we demonstrate that it is possible to learn a semantic lexicon and linear ranking function without manually annotating questions. Our approach automatically generalizes a seed lexicon and includes a scalable, parallelized perceptron parameter estimation scheme. Experiments show that our approach more than quadruples the recall of the seed lexicon, with only an 8% loss in precision.</p><p>6 0.82751739 <a title="291-lda-6" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>7 0.82588774 <a title="291-lda-7" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>8 0.81930721 <a title="291-lda-8" href="./acl-2013-A_Random_Walk_Approach_to_Selectional_Preferences_Based_on_Preference_Ranking_and_Propagation.html">17 acl-2013-A Random Walk Approach to Selectional Preferences Based on Preference Ranking and Propagation</a></p>
<p>9 0.81441551 <a title="291-lda-9" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>10 0.81400698 <a title="291-lda-10" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>11 0.81317031 <a title="291-lda-11" href="./acl-2013-Models_of_Semantic_Representation_with_Visual_Attributes.html">249 acl-2013-Models of Semantic Representation with Visual Attributes</a></p>
<p>12 0.81089813 <a title="291-lda-12" href="./acl-2013-Parsing_with_Compositional_Vector_Grammars.html">275 acl-2013-Parsing with Compositional Vector Grammars</a></p>
<p>13 0.81057662 <a title="291-lda-13" href="./acl-2013-Feature-Based_Selection_of_Dependency_Paths_in_Ad_Hoc_Information_Retrieval.html">158 acl-2013-Feature-Based Selection of Dependency Paths in Ad Hoc Information Retrieval</a></p>
<p>14 0.80914479 <a title="291-lda-14" href="./acl-2013-Learning_to_Extract_International_Relations_from_Political_Context.html">224 acl-2013-Learning to Extract International Relations from Political Context</a></p>
<p>15 0.80825096 <a title="291-lda-15" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>16 0.80824316 <a title="291-lda-16" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>17 0.80820405 <a title="291-lda-17" href="./acl-2013-Language-Independent_Discriminative_Parsing_of_Temporal_Expressions.html">212 acl-2013-Language-Independent Discriminative Parsing of Temporal Expressions</a></p>
<p>18 0.80809104 <a title="291-lda-18" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>19 0.80788225 <a title="291-lda-19" href="./acl-2013-Statistical_Machine_Translation_Improves_Question_Retrieval_in_Community_Question_Answering_via_Matrix_Factorization.html">329 acl-2013-Statistical Machine Translation Improves Question Retrieval in Community Question Answering via Matrix Factorization</a></p>
<p>20 0.80785978 <a title="291-lda-20" href="./acl-2013-Multimodal_DBN_for_Predicting_High-Quality_Answers_in_cQA_portals.html">254 acl-2013-Multimodal DBN for Predicting High-Quality Answers in cQA portals</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
