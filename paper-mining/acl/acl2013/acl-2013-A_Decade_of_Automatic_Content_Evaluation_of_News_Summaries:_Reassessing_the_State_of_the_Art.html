<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-5" href="#">acl2013-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</h1>
<br/><p>Source: <a title="acl-2013-5-pdf" href="http://aclweb.org/anthology//P/P13/P13-2024.pdf">pdf</a></p><p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>Reference: <a title="acl-2013-5-reference" href="../acl2013_reference/acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 gov  Abstract How good are automatic content metrics for news summary evaluation? [sent-8, score-0.506]
</p><p>2 Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. [sent-9, score-0.49]
</p><p>3 Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. [sent-10, score-0.354]
</p><p>4 Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies,  are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. [sent-11, score-0.575]
</p><p>5 We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction. [sent-12, score-0.381]
</p><p>6 1 Introduction ROUGE (Lin, 2004) is a suite of automatic evaluations for summarization and was introduced a decade ago as a reasonable substitute for costly and slow human evaluation. [sent-13, score-0.318]
</p><p>7 The scores it produces are based on n-gram or syntactic overlap between an automatic summary and a set of human reference summaries. [sent-14, score-0.354]
</p><p>8 However, the field does not have a good grasp of which of the many evaluation scores is most accurate in replicating human judgements. [sent-15, score-0.125]
</p><p>9 edu  ent researchers choose to publish different variants of scores. [sent-18, score-0.169]
</p><p>10 In this paper we reassess the strengths of ROUGE variants using the data from four years of Text Analysis Conference (TAC) evaluations, 2008 to 2011. [sent-19, score-0.234]
</p><p>11 To assess the performance of the automatic evaluations, we focus on determining statistical significance1 between systems, where the gold-standard comes from comparing the systems using manual pyramid and responsiveness evaluations. [sent-20, score-0.525]
</p><p>12 Instead, we report on the accuracy of decisions on pairs of systems, as well as the precision and recall of identifying pairs of systems which exhibit statistically significant differences in content selection performance. [sent-22, score-0.447]
</p><p>13 2  Background  During 2008–201 1, automatic summarization systems at TAC were required to create 100-word summaries. [sent-23, score-0.252]
</p><p>14 Each year there were two multidocument summarization sub-tasks, the initial summary and the update summary, usually referred to as task A and task B, respectively. [sent-24, score-0.456]
</p><p>15 The test inputs in each consisted of about 10 documents and the type of summary varied between query-focused and guided. [sent-25, score-0.273]
</p><p>16 There are between 44 and 48 test inputs on which systems are compared for each task. [sent-26, score-0.141]
</p><p>17 In 2008 and 2009, task A was to produce a 1For the purpose of this study, we define a difference as significant when the test statistic attains a value correspond-  13in1g to a p-value less than 0. [sent-27, score-0.067]
</p><p>18 c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioinngauli Lsitnicgsu,i psatgices 131–136, query-focused summary in response to a user information need stated both as a brief statement and a paragraph-long description of the information the user seeks to find. [sent-31, score-0.264]
</p><p>19 In 2010 and 2011 task A was “guided summarization”, where the test inputs came from a small set of predefined domains. [sent-32, score-0.091]
</p><p>20 The writers of the reference summaries for evaluation were given similar instructions. [sent-35, score-0.265]
</p><p>21 In all four years, task B was to produce an update summary for each of the inputs given in task A (query-focused or guided). [sent-36, score-0.344]
</p><p>22 The task was to generate an update summary aimed at a user who has already read all documents in the inputs for task A. [sent-38, score-0.341]
</p><p>23 The two manual evaluation approaches used in TAC 2008–201 1 are modified pyramid (Nenkova et al. [sent-39, score-0.282]
</p><p>24 The pyramid method requires several reference summaries for each input. [sent-41, score-0.356]
</p><p>25 These are manually analyzed to discover content units based on meaning rather than specific wording. [sent-42, score-0.106]
</p><p>26 Each content unit is as-  signed a weight equal to the number of reference summaries that included that content unit. [sent-43, score-0.361]
</p><p>27 The modified pyramid score is defined as the sum of weights of the content units in the summary normalized by the weight of an ideally informative summary which expresses n content units, where n is equal to the average of content units in the reference summaries. [sent-44, score-0.836]
</p><p>28 Assessors are presented with a statement of the user’s information need and the summary they need to evaluate. [sent-46, score-0.21]
</p><p>29 Then they rate how well they think the summary responds to the information need contained in the topic statement. [sent-47, score-0.182]
</p><p>30 For each sub-task during 2008–201 1, we analyze the performance of only the top 30 systems, which roughly corresponds to the systems that performed better than or around the median according to each manual metric. [sent-49, score-0.153]
</p><p>31 Table 1 gives the number of significant differences among the top 30 partici-  pating systems. [sent-50, score-0.161]
</p><p>32 We keep only the best performing systems for the analysis because we are interested in studying how well automatic evaluation metrics can correctly compare very good systems. [sent-51, score-0.348]
</p><p>33 ROUGE-n measures the n-gram recall of the evaluated summary compared to the available reference summaries. [sent-58, score-0.325]
</p><p>34 ROUGE-L is the ratio of the number of words in the longest common sub-  sequence between the reference and the evaluated summary and the number of words in the reference. [sent-59, score-0.237]
</p><p>35 ROUGE-SU4 is a combination of skip bigrams and unigrams, where the skip bigrams are formed for all words that appear in the text with no more than four intervening words in between. [sent-62, score-0.155]
</p><p>36 ROUGE-BE-HM computes recall of dependency syntactic relations between the summary and the reference. [sent-63, score-0.225]
</p><p>37 To evaluate how well an automatic evaluation metric reproduces human judgments, we use prediction accuracy similar to Owczarzak et al. [sent-64, score-0.283]
</p><p>38 For each pair of systems in each subtask, we compare the results of two Wilcoxon signedrank tests, one using the manual evaluation scores for each system and one using the automatic evaluation scores for each system (Rankel et al. [sent-66, score-0.406]
</p><p>39 2 The accuracy then is simply the percent agreement between the results of these two tests. [sent-68, score-0.152]
</p><p>40 There is considerable variation in system performance for different inputs  (Nenkova and Louis, 2008) and paired  13o2f the input. [sent-72, score-0.091]
</p><p>41 tests remove  the effect  all eight tasks in 2008-201 1, with and (without) significance. [sent-73, score-0.101]
</p><p>42 As can be seen in Table 1, the manual evaluation metrics often did not show many significant differences between systems. [sent-74, score-0.46]
</p><p>43 3 Thus, it is clear that the percent agreement will be high for an approach for automatic evaluation that always predicts zero significant differences. [sent-75, score-0.241]
</p><p>44 As traditionally done when dealing which such skewed distributions of classes, we also examine the precision and recall with respect to finding significant differences of several ROUGE variants, to better assess the quality of their prediction. [sent-76, score-0.236]
</p><p>45 To identify a measure that is strong at both predicting significant and non-significant differences we compute balanced accuracy, the mean of the accuracy of  predicting significant differences and the accuracy of predicting no significant  difference. [sent-77, score-0.795]
</p><p>46 4  Each of these four measures for judging the performance of ROUGE variants has direct intuitive interpretation, unlike other opaque measures such as correlation  coefficients  and F-measure  which  have formal definitions which do not readily yield to intuitive understanding. [sent-78, score-0.345]
</p><p>47 Recent work has shown that this is unlikely to be the case because the collection of summaries from several systems indicates better what content is important than the single best summary (Louis and Nenkova, 2013). [sent-81, score-0.468]
</p><p>48 The short summary length for which the summarizers are compared may also contribute to the fact that there are few significant difference. [sent-82, score-0.249]
</p><p>49 In early NIST evaluations manual evaluations could not distinguish automatic and human summaries based on summaries of length 50 and 100 words and there were more significant differences between systems for 200-word summaries than for 100-word summaries (Nenkova, 2005). [sent-83, score-1.213]
</p><p>50 Balanced accuracy weighs all errors as equally bad and all correct prediction as equally good (von Neumann and Morgenstern, 1953). [sent-85, score-0.152]
</p><p>51 Few prior studies have taken statistical significance into account during the assessment of automatic metrics for evaluation. [sent-86, score-0.436]
</p><p>52 For this reason we first briefly discuss ROUGE accuracy without taking significance into account. [sent-87, score-0.208]
</p><p>53 In this special case, agreement simply means that the automatic and manual evaluations agree on which of two systems is better, based on each system’s average score for all test inputs for a given task. [sent-88, score-0.396]
</p><p>54 It is very rare that the average scores of two systems are equal, so there is always a better system in each pair, and random prediction would have 50% accuracy. [sent-89, score-0.126]
</p><p>55 Many papers do not report the significance of differences in ROUGE scores (for the ROUGE variant of their choice), but simply claim that their system X with higher average ROUGE score than system Y is better than system Y . [sent-90, score-0.259]
</p><p>56 Table 2 lists the average accuracy with significance taken into account and then in parentheses, accuracy without taking significance into account. [sent-91, score-0.483]
</p><p>57 The data demonstrate that the best accuracy of the eight ROUGE metrics is a meager 64% for responsiveness when significance is not taken into account. [sent-92, score-0.609]
</p><p>58 So the conclusion about the relative merit of systems would  be different from that based on manual evaluation in one out of three comparisons. [sent-93, score-0.197]
</p><p>59 However, the best accuracy rises to 73% when significance is taken into account; an incorrect conclusion will be drawn in one out of four comparisons. [sent-94, score-0.274]
</p><p>60 (2012), where ROUGE-2 was shown to have accuracy of 81% for responsiveness and 89% for pyramid. [sent-98, score-0.259]
</p><p>61 This illustrates that our automatic metrics are not as good at discriminating systems near the top. [sent-100, score-0.304]
</p><p>62 These findings give strong support for the idea of requiring authors to report the significance of the difference between their summarization system and the chosen baseline; the conclusions about relative merits of the system would be more similar to those one would draw from manual evaluation. [sent-101, score-0.353]
</p><p>63 In addition to accuracy, Table 2 gives precision,  recall and balanced accuracy for each of the eight ROUGE measures when significance is taken into account. [sent-102, score-0.478]
</p><p>64 This means that it reports many significant differences, most of which do not exist according to the manual evaluations. [sent-104, score-0.17]
</p><p>65 Balanced accuracy helps us identify which ROUGE variants are most accurate in finding statistical significance and correctly predicting that two systems are not significantly different. [sent-105, score-0.468]
</p><p>66 For the pyramid evaluation, the variants with best balanced accuracy (66%) are ROUGE-3 and ROUGE-BE, with ROUGE-4 just a percent lower at 65%. [sent-106, score-0.557]
</p><p>67 For responsiveness the configuration is similar, with ROUGE-3 and ROUGE-4 tied for best (60%), and ROUGE-BE just a percent lower. [sent-107, score-0.229]
</p><p>68 Based on our results however, they are much more likely to accurately reproduce conclusions that  would have been drawn from manual evaluation of top-performing systems. [sent-109, score-0.175]
</p><p>69 4  Multiple hypothesis tests to combine ROUGE variants  We now consider a method to combine multiple evaluation scores in order to obtain a stronger ensemble metric. [sent-110, score-0.346]
</p><p>70 The idea of combining ROUGE variants has been explored in the prior literature. [sent-111, score-0.169]
</p><p>71 (2012) applied the “heterogeneity principle” and combined ROUGE scores to improve the precision relative to a human evaluation metric. [sent-116, score-0.124]
</p><p>72 Their results demonstrate that a consensus among ROUGE scores can predict more accurately if an improvement in a human evaluation metric will be achieved. [sent-117, score-0.171]
</p><p>73 Along the lines of these investigations, we examine the performance of a simple combination of variants: Call the difference between two sys-  tems significant only when all the variants in the combination indicate significance. [sent-118, score-0.31]
</p><p>74 anced Accuracy of each ROUGE combination on TAC 2008-2010 pyramid. [sent-120, score-0.065]
</p><p>75 The performance of these combinations for reproducing the decisions in TAC 2008-2010 based on the pyramid5 evaluation are given in Table 3. [sent-122, score-0.096]
</p><p>76 The best balanced accuracy (76%) is for the combination of all four variants. [sent-123, score-0.259]
</p><p>77 As more variants are combined, precision increases but recalls drops. [sent-124, score-0.201]
</p><p>78 Here we show how the submitted AESOP metrics compare to the best ROUGE variants that we have established so far. [sent-128, score-0.367]
</p><p>79 We report the results on 2011 only, because even when the same team participated in more than one year, the metrics submitted were different and the 2011 results represent the best effort of these teams. [sent-129, score-0.198]
</p><p>80 However, as we saw in Table 1, in 2011there were very few significant differences between the top summarization systems. [sent-130, score-0.294]
</p><p>81 In this sense the tasks that year represent a challenging dataset for testing automatic evaluations. [sent-131, score-0.114]
</p><p>82 The results for the best AESOP systems (according to one or more measures), and the corresponding results for the ROUGE combinations  are shown in Table 4. [sent-132, score-0.102]
</p><p>83 6 The combination metrics achieve the highest accuracy by generally predicting correctly when there are no significant differences between the systems. [sent-141, score-0.482]
</p><p>84 In addition, for 20082010, where far more differences between systems occur, the results of Table 3 show the combination metrics outperformed use of a single metric and are competitive with the best metrics of AESOP 2011. [sent-142, score-0.536]
</p><p>85 Thus, the combination metrics have the ability to discriminate under both conditions giving good prediction of human evaluation. [sent-143, score-0.25]
</p><p>86 6  Conclusion  We have tested the best-known automatic evaluation metrics (ROUGE) on several years of TAC data and compared their performance with recently developed AESOP metrics. [sent-148, score-0.3]
</p><p>87 We discovered that some of the rarely used variants of ROUGE perform surprisingly well, and that by combining different ROUGEs together, one can create an evaluation metric that is extremely competitive with metrics submitted to the latest AESOP task. [sent-149, score-0.462]
</p><p>88 Our results were reported in terms of several different measures, and in each case, compared how well the automatic metric predicted significant differences found in manual evaluation. [sent-150, score-0.384]
</p><p>89 We believe strongly that developers should include statistical significance when reporting differences in ROUGE scores of theirs and other systems, as this improves the accuracy and credibility of their results. [sent-151, score-0.417]
</p><p>90 Significant improvement in multiple ROUGE scores is a significantly stronger indicator that the developers have made a noteworthy improvement in text summarization. [sent-152, score-0.114]
</p><p>91 Systems that report significant improvement using a com-  bination of ROUGE-BE (or its improved version BEwT-E) in conjunction with ROUGE-1, 2, and 4, are more likely to give rise to summaries that humans would judge as significantly better. [sent-153, score-0.233]
</p><p>92 Acknowledgments The authors would like to thank Ed Hovy who raised the question “How well do automatic metrics perform when comparing top systems? [sent-154, score-0.221]
</p><p>93 The heterogeneity principle in evaluation measures for automatic summarization. [sent-159, score-0.213]
</p><p>94 Mind  the gap: Dangers of divorcing evaluations of summary content from linguistic quality. [sent-165, score-0.335]
</p><p>95 Using graph based mapping of co-occurring words and closeness centrality score for summarization evaluation. [sent-203, score-0.133]
</p><p>96 Automatically assessing machine summary content without a gold standard. [sent-212, score-0.282]
</p><p>97 The pyramid method: Incorporating human content selection variation in summarization evaluation. [sent-222, score-0.338]
</p><p>98 An assessment of the accuracy of automatic evaluation in summarization. [sent-231, score-0.235]
</p><p>99 Better metrics to automatically predict the quality of a text summary. [sent-245, score-0.152]
</p><p>100 More accurate tests for the statistical significance of result differences. [sent-261, score-0.173]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rouge', 0.53), ('aesop', 0.282), ('rankel', 0.194), ('conroy', 0.19), ('summary', 0.182), ('tac', 0.181), ('variants', 0.169), ('responsiveness', 0.168), ('summaries', 0.166), ('metrics', 0.152), ('pyramid', 0.135), ('summarization', 0.133), ('nenkova', 0.12), ('significance', 0.117), ('giannakopoulos', 0.111), ('manual', 0.103), ('balanced', 0.101), ('ani', 0.101), ('differences', 0.094), ('accuracy', 0.091), ('inputs', 0.091), ('evaluations', 0.083), ('owczarzak', 0.072), ('content', 0.07), ('automatic', 0.069), ('significant', 0.067), ('dang', 0.065), ('hoa', 0.063), ('srinathan', 0.063), ('vouros', 0.063), ('percent', 0.061), ('wilcoxon', 0.058), ('louis', 0.058), ('tests', 0.056), ('heterogeneity', 0.055), ('neumann', 0.055), ('vangelis', 0.055), ('kumar', 0.055), ('reference', 0.055), ('multidocument', 0.055), ('trang', 0.055), ('combinations', 0.052), ('metric', 0.051), ('tslp', 0.051), ('systems', 0.05), ('hovy', 0.049), ('amig', 0.048), ('scores', 0.048), ('submitted', 0.046), ('eight', 0.045), ('year', 0.045), ('measures', 0.045), ('skip', 0.044), ('tratz', 0.044), ('evaluation', 0.044), ('recall', 0.043), ('kennedy', 0.042), ('update', 0.041), ('predicting', 0.041), ('dianne', 0.041), ('annie', 0.04), ('developers', 0.037), ('combination', 0.037), ('john', 0.037), ('princeton', 0.037), ('taken', 0.036), ('units', 0.036), ('judith', 0.036), ('von', 0.036), ('years', 0.035), ('stan', 0.035), ('george', 0.034), ('exhibited', 0.033), ('decade', 0.033), ('investigations', 0.033), ('good', 0.033), ('precision', 0.032), ('account', 0.031), ('assessment', 0.031), ('four', 0.03), ('assessing', 0.03), ('guided', 0.03), ('reporting', 0.03), ('stronger', 0.029), ('statement', 0.028), ('coefficients', 0.028), ('anced', 0.028), ('copeck', 0.028), ('dangers', 0.028), ('endangered', 0.028), ('kannan', 0.028), ('karkaletsis', 0.028), ('kazantseva', 0.028), ('niraj', 0.028), ('opaque', 0.028), ('referees', 0.028), ('super', 0.028), ('accurately', 0.028), ('prediction', 0.028), ('user', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="5-tfidf-1" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>2 0.21723567 <a title="5-tfidf-2" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>Author: Rebecca J. Passonneau ; Emily Chen ; Weiwei Guo ; Dolores Perin</p><p>Abstract: The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students’ summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics.</p><p>3 0.18199691 <a title="5-tfidf-3" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.</p><p>4 0.16671543 <a title="5-tfidf-4" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>5 0.1621491 <a title="5-tfidf-5" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>Author: Lu Wang ; Hema Raghavan ; Vittorio Castelli ; Radu Florian ; Claire Cardie</p><p>Abstract: We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. ,</p><p>6 0.14599527 <a title="5-tfidf-6" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>7 0.12910302 <a title="5-tfidf-7" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>8 0.12901531 <a title="5-tfidf-8" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>9 0.12514363 <a title="5-tfidf-9" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>10 0.12055562 <a title="5-tfidf-10" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>11 0.11617365 <a title="5-tfidf-11" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>12 0.092324205 <a title="5-tfidf-12" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>13 0.083600685 <a title="5-tfidf-13" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>14 0.067511886 <a title="5-tfidf-14" href="./acl-2013-Improving_machine_translation_by_training_against_an_automatic_semantic_frame_based_evaluation_metric.html">195 acl-2013-Improving machine translation by training against an automatic semantic frame based evaluation metric</a></p>
<p>15 0.066904888 <a title="5-tfidf-15" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>16 0.063496657 <a title="5-tfidf-16" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>17 0.062750511 <a title="5-tfidf-17" href="./acl-2013-A_New_Syntactic_Metric_for_Evaluation_of_Machine_Translation.html">13 acl-2013-A New Syntactic Metric for Evaluation of Machine Translation</a></p>
<p>18 0.057234373 <a title="5-tfidf-18" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>19 0.056840651 <a title="5-tfidf-19" href="./acl-2013-Evaluating_Text_Segmentation_using_Boundary_Edit_Distance.html">140 acl-2013-Evaluating Text Segmentation using Boundary Edit Distance</a></p>
<p>20 0.05225008 <a title="5-tfidf-20" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.049), (2, 0.015), (3, -0.077), (4, 0.03), (5, -0.004), (6, 0.121), (7, -0.01), (8, -0.162), (9, -0.065), (10, -0.06), (11, 0.078), (12, -0.191), (13, 0.005), (14, -0.141), (15, 0.179), (16, 0.14), (17, -0.171), (18, 0.008), (19, 0.061), (20, -0.002), (21, -0.095), (22, 0.007), (23, -0.034), (24, -0.004), (25, -0.038), (26, -0.001), (27, 0.016), (28, 0.042), (29, 0.008), (30, -0.025), (31, 0.007), (32, 0.093), (33, -0.065), (34, -0.018), (35, -0.01), (36, -0.08), (37, 0.035), (38, -0.033), (39, -0.025), (40, 0.017), (41, 0.037), (42, -0.019), (43, -0.033), (44, 0.021), (45, -0.023), (46, -0.008), (47, -0.027), (48, 0.095), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94422203 <a title="5-lsi-1" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>2 0.88092732 <a title="5-lsi-2" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>Author: Jackie Chi Kit Cheung ; Gerald Penn</p><p>Abstract: In automatic summarization, centrality is the notion that a summary should contain the core parts of the source text. Current systems use centrality, along with redundancy avoidance and some sentence compression, to produce mostly extractive summaries. In this paper, we investigate how summarization can advance past this paradigm towards robust abstraction by making greater use of the domain of the source text. We conduct a series of studies comparing human-written model summaries to system summaries at the semantic level of caseframes. We show that model summaries (1) are more abstractive and make use of more sentence aggregation, (2) do not contain as many topical caseframes as system summaries, and (3) cannot be reconstructed solely from the source text, but can be if texts from in-domain documents are added. These results suggest that substantial improvements are unlikely to result from better optimizing centrality-based criteria, but rather more domain knowledge is needed.</p><p>3 0.83872753 <a title="5-lsi-3" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>Author: Rebecca J. Passonneau ; Emily Chen ; Weiwei Guo ; Dolores Perin</p><p>Abstract: The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students’ summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics.</p><p>4 0.82126045 <a title="5-lsi-4" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>Author: Chen Li ; Xian Qian ; Yang Liu</p><p>Abstract: In this paper, we propose a bigram based supervised method for extractive document summarization in the integer linear programming (ILP) framework. For each bigram, a regression model is used to estimate its frequency in the reference summary. The regression model uses a variety ofindicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary. During testing, the sentence selection problem is formulated as an ILP problem to maximize the bigram gains. We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets, and performs competitively compared to the best results in the TAC evaluations. We also conducted various analysis to show the impact of bigram selection, weight estimation, and ILP setup.</p><p>5 0.79694718 <a title="5-lsi-5" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>Author: Anirban Dasgupta ; Ravi Kumar ; Sujith Ravi</p><p>Abstract: We propose a new optimization framework for summarization by generalizing the submodular framework of (Lin and Bilmes, 2011). In our framework the summarization desideratum is expressed as a sum of a submodular function and a nonsubmodular function, which we call dispersion; the latter uses inter-sentence dissimilarities in different ways in order to ensure non-redundancy of the summary. We consider three natural dispersion functions and show that a greedy algorithm can obtain an approximately optimal summary in all three cases. We conduct experiments on two corpora—DUC 2004 and user comments on news articles—and show that the performance of our algorithm outperforms those that rely only on submodularity.</p><p>6 0.73410439 <a title="5-lsi-6" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>7 0.73060405 <a title="5-lsi-7" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>8 0.66819537 <a title="5-lsi-8" href="./acl-2013-Fast_and_Robust_Compressive_Summarization_with_Dual_Decomposition_and_Multi-Task_Learning.html">157 acl-2013-Fast and Robust Compressive Summarization with Dual Decomposition and Multi-Task Learning</a></p>
<p>9 0.6423108 <a title="5-lsi-9" href="./acl-2013-Evolutionary_Hierarchical_Dirichlet_Process_for_Timeline_Summarization.html">142 acl-2013-Evolutionary Hierarchical Dirichlet Process for Timeline Summarization</a></p>
<p>10 0.6349771 <a title="5-lsi-10" href="./acl-2013-Subtree_Extractive_Summarization_via_Submodular_Maximization.html">332 acl-2013-Subtree Extractive Summarization via Submodular Maximization</a></p>
<p>11 0.58525538 <a title="5-lsi-11" href="./acl-2013-Sequential_Summarization%3A_A_New_Application_for_Timely_Updated_Twitter_Trending_Topics.html">319 acl-2013-Sequential Summarization: A New Application for Timely Updated Twitter Trending Topics</a></p>
<p>12 0.48929498 <a title="5-lsi-12" href="./acl-2013-Learning_to_Order_Natural_Language_Texts.html">225 acl-2013-Learning to Order Natural Language Texts</a></p>
<p>13 0.47934383 <a title="5-lsi-13" href="./acl-2013-Using_Integer_Linear_Programming_in_Concept-to-Text_Generation_to_Produce_More_Compact_Texts.html">375 acl-2013-Using Integer Linear Programming in Concept-to-Text Generation to Produce More Compact Texts</a></p>
<p>14 0.47639588 <a title="5-lsi-14" href="./acl-2013-Probabilistic_Domain_Modelling_With_Contextualized_Distributional_Semantic_Vectors.html">283 acl-2013-Probabilistic Domain Modelling With Contextualized Distributional Semantic Vectors</a></p>
<p>15 0.47366485 <a title="5-lsi-15" href="./acl-2013-A_System_for_Summarizing_Scientific_Topics_Starting_from_Keywords.html">23 acl-2013-A System for Summarizing Scientific Topics Starting from Keywords</a></p>
<p>16 0.44517258 <a title="5-lsi-16" href="./acl-2013-HEADY%3A_News_headline_abstraction_through_event_pattern_clustering.html">178 acl-2013-HEADY: News headline abstraction through event pattern clustering</a></p>
<p>17 0.41347414 <a title="5-lsi-17" href="./acl-2013-English-to-Russian_MT_evaluation_campaign.html">135 acl-2013-English-to-Russian MT evaluation campaign</a></p>
<p>18 0.41239014 <a title="5-lsi-18" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>19 0.4022395 <a title="5-lsi-19" href="./acl-2013-A_Statistical_NLG_Framework_for_Aggregated_Planning_and_Realization.html">21 acl-2013-A Statistical NLG Framework for Aggregated Planning and Realization</a></p>
<p>20 0.38036835 <a title="5-lsi-20" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.062), (6, 0.085), (11, 0.05), (15, 0.022), (24, 0.041), (26, 0.047), (35, 0.068), (42, 0.055), (48, 0.036), (60, 0.17), (61, 0.011), (70, 0.058), (88, 0.041), (90, 0.05), (95, 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.88921613 <a title="5-lda-1" href="./acl-2013-Automatic_detection_of_deception_in_child-produced_speech_using_syntactic_complexity_features.html">63 acl-2013-Automatic detection of deception in child-produced speech using syntactic complexity features</a></p>
<p>Author: Maria Yancheva ; Frank Rudzicz</p><p>Abstract: It is important that the testimony of children be admissible in court, especially given allegations of abuse. Unfortunately, children can be misled by interrogators or might offer false information, with dire consequences. In this work, we evaluate various parameterizations of five classifiers (including support vector machines, neural networks, and random forests) in deciphering truth from lies given transcripts of interviews with 198 victims of abuse between the ages of 4 and 7. These evaluations are performed using a novel set of syntactic features, including measures of complexity. Our results show that sentence length, the mean number of clauses per utterance, and the StajnerMitkov measure of complexity are highly informative syntactic features, that classification accuracy varies greatly by the age of the speaker, and that accuracy up to 91.7% can be achieved by support vector machines given a sufficient amount of data.</p><p>same-paper 2 0.827245 <a title="5-lda-2" href="./acl-2013-A_Decade_of_Automatic_Content_Evaluation_of_News_Summaries%3A_Reassessing_the_State_of_the_Art.html">5 acl-2013-A Decade of Automatic Content Evaluation of News Summaries: Reassessing the State of the Art</a></p>
<p>Author: Peter A. Rankel ; John M. Conroy ; Hoa Trang Dang ; Ani Nenkova</p><p>Abstract: How good are automatic content metrics for news summary evaluation? Here we provide a detailed answer to this question, with a particular focus on assessing the ability of automatic evaluations to identify statistically significant differences present in manual evaluation of content. Using four years of data from the Text Analysis Conference, we analyze the performance of eight ROUGE variants in terms of accuracy, precision and recall in finding significantly different systems. Our experiments show that some of the neglected variants of ROUGE, based on higher order n-grams and syntactic dependencies, are most accurate across the years; the commonly used ROUGE-1 scores find too many significant differences between systems which manual evaluation would deem comparable. We also test combinations ofROUGE variants and find that they considerably improve the accuracy of automatic prediction.</p><p>3 0.80619931 <a title="5-lda-3" href="./acl-2013-PARMA%3A_A_Predicate_Argument_Aligner.html">267 acl-2013-PARMA: A Predicate Argument Aligner</a></p>
<p>Author: Travis Wolfe ; Benjamin Van Durme ; Mark Dredze ; Nicholas Andrews ; Charley Beller ; Chris Callison-Burch ; Jay DeYoung ; Justin Snyder ; Jonathan Weese ; Tan Xu ; Xuchen Yao</p><p>Abstract: We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17% F1.</p><p>4 0.73124927 <a title="5-lda-4" href="./acl-2013-Adapting_Discriminative_Reranking_to_Grounded_Language_Learning.html">36 acl-2013-Adapting Discriminative Reranking to Grounded Language Learning</a></p>
<p>Author: Joohyun Kim ; Raymond Mooney</p><p>Abstract: We adapt discriminative reranking to improve the performance of grounded language acquisition, specifically the task of learning to follow navigation instructions from observation. Unlike conventional reranking used in syntactic and semantic parsing, gold-standard reference trees are not naturally available in a grounded setting. Instead, we show how the weak supervision of response feedback (e.g. successful task completion) can be used as an alternative, experimentally demonstrating that its performance is comparable to training on gold-standard parse trees.</p><p>5 0.73035246 <a title="5-lda-5" href="./acl-2013-Automated_Pyramid_Scoring_of_Summaries_using_Distributional_Semantics.html">59 acl-2013-Automated Pyramid Scoring of Summaries using Distributional Semantics</a></p>
<p>Author: Rebecca J. Passonneau ; Emily Chen ; Weiwei Guo ; Dolores Perin</p><p>Abstract: The pyramid method for content evaluation of automated summarizers produces scores that are shown to correlate well with manual scores used in educational assessment of students’ summaries. This motivates the development of a more accurate automated method to compute pyramid scores. Of three methods tested here, the one that performs best relies on latent semantics.</p><p>6 0.72613031 <a title="5-lda-6" href="./acl-2013-Models_of_Translation_Competitions.html">250 acl-2013-Models of Translation Competitions</a></p>
<p>7 0.72286361 <a title="5-lda-7" href="./acl-2013-Joint_Word_Alignment_and_Bilingual_Named_Entity_Recognition_Using_Dual_Decomposition.html">210 acl-2013-Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition</a></p>
<p>8 0.72160971 <a title="5-lda-8" href="./acl-2013-Summarization_Through_Submodularity_and_Dispersion.html">333 acl-2013-Summarization Through Submodularity and Dispersion</a></p>
<p>9 0.72012061 <a title="5-lda-9" href="./acl-2013-A_Sentence_Compression_Based_Framework_to_Query-Focused_Multi-Document_Summarization.html">18 acl-2013-A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization</a></p>
<p>10 0.71946734 <a title="5-lda-10" href="./acl-2013-Collective_Annotation_of_Linguistic_Resources%3A_Basic_Principles_and_a_Formal_Model.html">83 acl-2013-Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model</a></p>
<p>11 0.7182914 <a title="5-lda-11" href="./acl-2013-Non-Monotonic_Sentence_Alignment_via_Semisupervised_Learning.html">259 acl-2013-Non-Monotonic Sentence Alignment via Semisupervised Learning</a></p>
<p>12 0.71812868 <a title="5-lda-12" href="./acl-2013-Modeling_Thesis_Clarity_in_Student_Essays.html">246 acl-2013-Modeling Thesis Clarity in Student Essays</a></p>
<p>13 0.71557081 <a title="5-lda-13" href="./acl-2013-Modelling_Annotator_Bias_with_Multi-task_Gaussian_Processes%3A_An_Application_to_Machine_Translation_Quality_Estimation.html">248 acl-2013-Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation</a></p>
<p>14 0.71278673 <a title="5-lda-14" href="./acl-2013-Towards_Robust_Abstractive_Multi-Document_Summarization%3A_A_Caseframe_Analysis_of_Centrality_and_Domain.html">353 acl-2013-Towards Robust Abstractive Multi-Document Summarization: A Caseframe Analysis of Centrality and Domain</a></p>
<p>15 0.70989913 <a title="5-lda-15" href="./acl-2013-Embedding_Semantic_Similarity_in_Tree_Kernels_for_Domain_Adaptation_of_Relation_Extraction.html">134 acl-2013-Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction</a></p>
<p>16 0.70717275 <a title="5-lda-16" href="./acl-2013-Learning_to_Prune%3A_Context-Sensitive_Pruning_for_Syntactic_MT.html">226 acl-2013-Learning to Prune: Context-Sensitive Pruning for Syntactic MT</a></p>
<p>17 0.70693302 <a title="5-lda-17" href="./acl-2013-Domain-Independent_Abstract_Generation_for_Focused_Meeting_Summarization.html">129 acl-2013-Domain-Independent Abstract Generation for Focused Meeting Summarization</a></p>
<p>18 0.70684743 <a title="5-lda-18" href="./acl-2013-A_Lightweight_and_High_Performance_Monolingual_Word_Aligner.html">9 acl-2013-A Lightweight and High Performance Monolingual Word Aligner</a></p>
<p>19 0.7058987 <a title="5-lda-19" href="./acl-2013-Annotating_named_entities_in_clinical_text_by_combining_pre-annotation_and_active_learning.html">52 acl-2013-Annotating named entities in clinical text by combining pre-annotation and active learning</a></p>
<p>20 0.70560873 <a title="5-lda-20" href="./acl-2013-Bilingually-Guided_Monolingual_Dependency_Grammar_Induction.html">70 acl-2013-Bilingually-Guided Monolingual Dependency Grammar Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
