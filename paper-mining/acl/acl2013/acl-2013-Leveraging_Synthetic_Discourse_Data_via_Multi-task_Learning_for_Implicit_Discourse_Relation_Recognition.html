<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</title>
</head>

<body>
<p><a title="acl" href="../acl_home.html">acl</a> <a title="acl-2013" href="../home/acl2013_home.html">acl2013</a> <a title="acl-2013-229" href="#">acl2013-229</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</h1>
<br/><p>Source: <a title="acl-2013-229-pdf" href="http://aclweb.org/anthology//P/P13/P13-1047.pdf">pdf</a></p><p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>Reference: <a title="acl-2013-229-reference" href="../acl2013_reference/acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 cn 0 2 0 Abstract To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. [sent-8, score-2.658]
</p><p>2 In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data  for implicit discourse relation recognition. [sent-12, score-1.465]
</p><p>3 Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5. [sent-13, score-0.975]
</p><p>4 1 Introduction The task of implicit discourse relation recognition is to identify the type of discourse relation (a. [sent-15, score-1.801]
</p><p>5 rhetorical relation) hold between two spans of text, where there is no discourse connective (a. [sent-18, score-0.795]
</p><p>6 , but, and) in context to explicitly mark their discourse relation (e. [sent-23, score-0.667]
</p><p>7 First, without discourse connective in text, the task is quite difficult in itself. [sent-30, score-0.811]
</p><p>8 Second, implicit discourse rela-  tion is quite frequent in text. [sent-31, score-0.974]
</p><p>9 Therefore, the task of implicit discourse relation recognition is the key to improving end-to-end discourse parser performance. [sent-36, score-1.668]
</p><p>10 Thus the resulting sentences were used as synthetic training examples for implicit discourse relation recognition. [sent-42, score-1.438]
</p><p>11 Since there is ambiguity of a word or  phrase serving for discourse connective (i. [sent-43, score-0.805]
</p><p>12 , the ambiguity between discourse and non-discourse usage or the ambiguity between two or more discourse relations if the word or phrase is used as a discourse connective), the synthetic implicit data would contain a lot of noises. [sent-45, score-2.539]
</p><p>13 , 2008), recent studies performed implicit discourse relation recognition on natural (i. [sent-48, score-1.103]
</p><p>14 (Sporleder and Lascarides, 2008) conducted a study of the pattern-based approach presented by (Marcu and Echihabi, 2002) and showed that the model built on synthetical implicit data has not generalize well on natural implicit data. [sent-54, score-0.928]
</p><p>15 That is, discourse connectives can be inserted between or removed from two sentences without changing the semantic relations between them in some cases. [sent-66, score-0.688]
</p><p>16 To label implicit discourse relation, annotators inserted connective which can best express the relation between sentences without any redundancy1 . [sent-68, score-1.326]
</p><p>17 We see that there should be some linguistical similarities between explicit  and implicit discourse examples. [sent-69, score-1.043]
</p><p>18 Therefore, the first question arises: can we exploit this kind of linguistic similarity between explicit and implicit discourse examples to improve implicit discourse relation recognition? [sent-70, score-2.144]
</p><p>19 In this paper, we propose a multi-task learning based method to improve the performance of implicit discourse relation recognition (as main task) with the help of relevant auxiliary tasks. [sent-71, score-1.367]
</p><p>20 Specifically, the main task is to recognize the implicit discourse relations based on genuine implicit discourse data and the auxiliary task is to recognize the implicit discourse relations based on synthetic implicit discourse data. [sent-72, score-4.616]
</p><p>21 That means, the model can learn from synthetic implicit data while it would not bring unnecessary noise from synthetic implicit data. [sent-74, score-1.541]
</p><p>22 Although (Sporleder and Lascarides, 2008) did not mention, we speculate that another possible reason for the reported worse performance may  result from noises in synthetic implicit discourse data. [sent-75, score-1.289]
</p><p>23 2002) and (Sporleder and Lascarides, 2008), and (2) manually annotated explicit data with the removal of explicit discourse connectives. [sent-77, score-0.767]
</p><p>24 Therefore, the second question to address in this work is: whether synthetic implicit discourse data generated from explicit discourse data source (i. [sent-79, score-1.944]
</p><p>25 To answer this question, we will make a comparison of synthetic discourse data generated from two corpora, i. [sent-84, score-0.886]
</p><p>26 , the BILLIP corpus and the explicit discourse data annotated in PDTB. [sent-86, score-0.66]
</p><p>27 Section 2 reviews related work on implicit discourse relation classification and multi-task learning. [sent-88, score-1.122]
</p><p>28 Section 3 presents our proposed multi-task learning method for implicit discourse relation classification. [sent-89, score-1.113]
</p><p>29 1 Unsupervised approaches Due to the lack of benchmark data for implicit discourse relation analysis, earlier work used unlabeled data to generate synthetic implicit discourse data. [sent-96, score-2.445]
</p><p>30 They first used unambiguous pattern to extract explicit discourse examples from raw corpus. [sent-100, score-0.66]
</p><p>31 Then they generated synthetic implicit dis-  course data by removing explicit discourse connectives from sentences extracted. [sent-101, score-1.492]
</p><p>32 In their work, they collected word pairs from synthetic data set as features and used machine learning method to classify implicit discourse relation. [sent-102, score-1.332]
</p><p>33 , 2006) showed that the use of phrasal patterns as additional features can help a word-pair based system for discourse relation prediction on a Japanese corpus. [sent-105, score-0.69]
</p><p>34 (Soricut and Marcu, 2003) parsed the discourse structures of sentences on RST Bank data set (Carlson et al. [sent-111, score-0.549]
</p><p>35 , 2006) presented a study of discourse relation disambiguation on GraphBank (Wolf et al. [sent-114, score-0.667]
</p><p>36 , 2010) conducted discourse relation study on PDTB (Prasad et al. [sent-119, score-0.667]
</p><p>37 3 Semi-supervised approaches Research work in this category exploited both labeled and unlabeled data for discourse relation prediction. [sent-123, score-0.727]
</p><p>38 , 2011) introduced a semi-supervised work using structure  learning method for discourse relation classification, which is quite relevant to our work. [sent-127, score-0.732]
</p><p>39 However, they performed discourse relation classification on both explicit and implicit data. [sent-128, score-1.213]
</p><p>40 The predicted connective was then used as a feature to classify the implicit relation. [sent-133, score-0.642]
</p><p>41 1 Motivation The idea of using multi-task learning for implicit discourse relation classification is motivated by the observations that we have made on implicit discourse relation. [sent-145, score-2.102]
</p><p>42 On one hand, since building a hand-annotated implicit discourse relation corpus is costly and time consuming, most previous work attempted to use synthetic implicit discourse examples as training data. [sent-146, score-2.39]
</p><p>43 However, (Sporleder and Lascarides, 2008) found that the model trained on synthetic implicit data has not performed as well as expected in natural implicit data. [sent-147, score-1.188]
</p><p>44 They stated that the reason is linguistic dissimilarity between explicit and implicit discourse data. [sent-148, score-1.07]
</p><p>45 This indicates that straightly using synthetic implicit data as training data may not be helpful. [sent-149, score-0.803]
</p><p>46 On the other hand, as shown in Section 1, we observe that in some cases explicit discourse relation and implicit discourse relation can express the same meaning with or without a discourse connective. [sent-150, score-2.394]
</p><p>47 If it is true, the synthetic implicit relations are expected to be helpful for implicit discourse relation classification. [sent-152, score-1.913]
</p><p>48 To solve it, we propose a multi-task learning method for implicit discourse relation classi478  fication, where the classification model seeks the shared part through jointly learning main task and multiple auxiliary tasks. [sent-154, score-1.45]
</p><p>49 Previous work (Marcu and Echihabi, 2002) and (Sporleder and Lascarides, 2008) adopted predefined pattern-based approach to generate synthetic labeled data, where each predefined pattern has one discourse relation label. [sent-181, score-1.05]
</p><p>50 In contrast, we adopt an automatic approach to generate synthetic labeled data, where each discourse connective between two texts serves as their relation label. [sent-182, score-1.307]
</p><p>51 The reason lies in the very strong connection between discourse connectives and discourse relations. [sent-183, score-1.149]
</p><p>52 , 2008) proved that using only connective itself, the accuracy of explicit discourse relation classification is over 93%. [sent-186, score-1.019]
</p><p>53 To build the mapping between discourse connective and discourse relation, for each connective, we count the times it appears in each relation and regard the relation in which it appears most  frequently as its most relevant relation. [sent-187, score-1.573]
</p><p>54 Based on this mapping between connective and relation, we extract the synthetic labeled data containing the connective as training data for auxiliary tasks. [sent-188, score-1.015]
</p><p>55 sco Tuhresen “waen de”x raancdt remove this connective “and” from sentences to generate synthetic implicit data. [sent-193, score-1.0]
</p><p>56 1 Data sets for main and auxiliary tasks To examine whether there is a difference in synthetic implicit data generated from unannotated  and annotated corpus, we use two corpora. [sent-196, score-1.054]
</p><p>57 , the explicit discourse relations in PDTB, denoted as exp. [sent-199, score-0.714]
</p><p>58 , 2008) is the largest handannotated corpus of discourse relation so far. [sent-207, score-0.667]
</p><p>59 The sense label of discourse relations is hierarchically with three levels, i. [sent-209, score-0.607]
</p><p>60 Both explicit and implicit discourse relations are labeled in PDTB. [sent-220, score-1.141]
</p><p>61 In our experiment, the im-  plicit discourse relations are used in the main task and for evaluation. [sent-221, score-0.684]
</p><p>62 While the explicit discourse relations are used in the auxiliary task. [sent-222, score-0.873]
</p><p>63 , 2010), the implicit relations in sections 2-20 are used as training data for the main task (denoted as imp) and the implicit relations in sections 21-22 are for evaluation. [sent-226, score-1.104]
</p><p>64 In comparison with the synthetic labeled data generated from the explicit relations in PDTB, the synthetic labeled data from BLLIP contains more noise. [sent-238, score-0.918]
</p><p>65 This is because the former data is manually annotated whether a word serves as discourse connective or not, while the latter does not manually disambiguate two types of ambiguity, i. [sent-239, score-0.84]
</p><p>66 , whether a word serves as discourse connective or not, and the type of discourse relation if it is a discourse connective. [sent-241, score-1.974]
</p><p>67 3 Classifiers used multi-task learning We extract the above linguistically informed features from two synthetic implicit data sets (i. [sent-267, score-0.813]
</p><p>68 , BLLIP and exp) to learn the auxiliary classifier and from the natural implicit data set (i. [sent-269, score-0.608]
</p><p>69 Under the single task learning, various combinations of exp and BLLIP data are incorporated with imp data for the implicit discourse relation classification task. [sent-281, score-1.554]
</p><p>70 We hypothesize that synthetical implicit data  would contribute to the main task, i. [sent-282, score-0.556]
</p><p>71 , imp) are used to create main task and the synthetical implicit data (exp or BLLIP) are used to create auxiliary tasks for the purpose of optimizing the objective functions of main task. [sent-287, score-0.83]
</p><p>72 If the hypothesis is correct, the performance of main task would be improved by auxiliary tasks created from synthetical implicit data. [sent-288, score-0.769]
</p><p>73 , imp) data are used for main task training while different combinations of synthetical implicit examples (exp and BLLIP) are used for auxiliary task training. [sent-291, score-0.809]
</p><p>74 2  Results  Table 2 summarizes the experimental results under single and multi-task learning on the top level of  four PDTB relations with respect to different combinations of synthetic implicit data. [sent-295, score-0.929]
</p><p>75 , imp) and three multitask learning systems (imp:exp+BLLIP indicates that imp is used for main task and the combination of exp and BLLIP are for auxiliary task). [sent-349, score-0.685]
</p><p>76 We can see that the multi-task learning systems perform consistently better than the single task learning systems for the prediction of implicit discourse relations. [sent-355, score-1.079]
</p><p>77 It indicates that using synthetic implicit data as auxiliary task greatly improves the performance of the main task. [sent-372, score-1.04]
</p><p>78 In contrast to the performance of multi-task learning, the performance of the best single task learning system has been achieved on natural implicit discourse data alone. [sent-374, score-1.043]
</p><p>79 It indicates that under single task learning, directly adding synthetic implicit data to increase the number of training data cannot be helpful to implicit discourse relation classification. [sent-376, score-1.936]
</p><p>80 The possible reasons result from (1) the different nature of implicit and explicit discourse data in linguistics and (2) the noise brought from synthetic implicit data. [sent-377, score-1.829]
</p><p>81 Based on the above analysis, we state that it is the way of utilizing synthetic implicit data that is important for implicit discourse relation classification. [sent-378, score-1.855]
</p><p>82 Although all three multi-task learning systems outperformed single task learning systems, we find that the two synthetic implicit data sets have not been shown a universally consistent performance on four top level PDTB relations. [sent-379, score-0.915]
</p><p>83 , the performance of the two synthetic implicit data sets  alone and their combination are comparable to each other and there is no significant difference between them. [sent-382, score-0.77]
</p><p>84 This is contrary to our original expectation that exp data which has been manually annotated for discourse connective disambiguation should outperform BLLIP which contains a lot of noise. [sent-386, score-0.912]
</p><p>85 4 Ambiguity Analysis Although our experiments show that synthetic implicit data can help implicit discourse relation classification under multi-task learning framework, the overall performance is still quite low (44. [sent-390, score-1.942]
</p><p>86 1 Ambiguity of implicit relation Without explicit discourse connective, the implicit discourse relation instance can be understood in two or more different ways. [sent-395, score-2.261]
</p><p>87 Given the example E2 in PDTB, the PDTB annotators explain it as Contingency or Expansion relation and manually insert corresponding implicit connective for one thing or because to express its relation. [sent-396, score-0.827]
</p><p>88 Instantiation (wsj 01 18)  Thus the ambiguity of implicit discourse relations makes this task difficult in itself. [sent-400, score-1.103]
</p><p>89 2 Ambiguity of discourse connectives As we mentioned before, even given an explicit discourse connective in text, its discourse relation still can be explained in two or more differ-  ent ways. [sent-403, score-2.131]
</p><p>90 Reason (wsj 0003) In PDTB, “since” appears 184 times in explicit discourse relations. [sent-412, score-0.625]
</p><p>91 Thus the ambiguity of discourse connectives is another source which has brought noise to data  when we generate synthetical implicit discourse relation. [sent-417, score-1.743]
</p><p>92 6  Conclusions  In this paper, we present a multi-task learning method to improve implicit discourse relation classification by leveraging synthetic implicit discourse data. [sent-418, score-2.439]
</p><p>93 Results on PDTB show that under the framework of multi-task learning, using synthetic discourse data as auxiliary task significantly improves the performance of main task. [sent-419, score-1.138]
</p><p>94 This indicates that it is the way of utilizing synthetic discourse examples that is important for implicit discourse relation classification. [sent-431, score-1.99]
</p><p>95 A semi-supervised approach to improve classification of infrequent discourse relations using feature vector extension. [sent-525, score-0.644]
</p><p>96 Recognizing implicit discourse relations in the penn discourse  treebank. [sent-556, score-1.578]
</p><p>97 Automatic sense prediction for implicit discourse relations in  text. [sent-597, score-1.048]
</p><p>98 Sentence level discourse parsing using syntactic and lexical information. [sent-619, score-0.559]
</p><p>99 Kernel based discourse relation recognition with temporal ordering information. [sent-652, score-0.712]
</p><p>100 The discourse graphbank: A database of texts annotated with coherence relations. [sent-680, score-0.554]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('discourse', 0.534), ('implicit', 0.418), ('synthetic', 0.337), ('imp', 0.251), ('bllip', 0.236), ('pdtb', 0.23), ('connective', 0.224), ('auxiliary', 0.175), ('relation', 0.133), ('exp', 0.103), ('sporleder', 0.092), ('explicit', 0.091), ('lascarides', 0.09), ('connectives', 0.081), ('pitler', 0.078), ('synthetical', 0.077), ('ando', 0.073), ('relations', 0.073), ('aso', 0.068), ('mtl', 0.057), ('ebxlp', 0.051), ('mxlp', 0.051), ('echihabi', 0.047), ('contingency', 0.047), ('ambiguity', 0.047), ('main', 0.046), ('marcu', 0.045), ('negated', 0.042), ('hernault', 0.039), ('rhetorical', 0.037), ('classification', 0.037), ('argyriou', 0.034), ('multitask', 0.033), ('prasad', 0.031), ('pontil', 0.031), ('task', 0.031), ('evgeniou', 0.029), ('learning', 0.028), ('expansion', 0.027), ('temporal', 0.027), ('genuine', 0.027), ('dissimilarity', 0.027), ('shanghai', 0.026), ('graphbank', 0.026), ('labeled', 0.025), ('regularization', 0.025), ('level', 0.025), ('niu', 0.024), ('wolf', 0.023), ('bonilla', 0.023), ('prediction', 0.023), ('quite', 0.022), ('tasks', 0.022), ('ip', 0.021), ('predictors', 0.021), ('generate', 0.021), ('unannotated', 0.021), ('saito', 0.021), ('cimiano', 0.021), ('annotated', 0.02), ('unlabeled', 0.02), ('wsj', 0.02), ('wellner', 0.02), ('shortage', 0.02), ('verberne', 0.02), ('shared', 0.02), ('thing', 0.019), ('raw', 0.019), ('penn', 0.019), ('bollegala', 0.019), ('recognition', 0.018), ('indicates', 0.018), ('adopt', 0.018), ('zhou', 0.018), ('risk', 0.017), ('baidu', 0.017), ('single', 0.017), ('express', 0.017), ('lan', 0.017), ('top', 0.016), ('denoted', 0.016), ('noise', 0.016), ('manually', 0.016), ('removing', 0.016), ('examples', 0.016), ('specifically', 0.015), ('dissimilar', 0.015), ('carlson', 0.015), ('modal', 0.015), ('sigdial', 0.015), ('alternating', 0.015), ('data', 0.015), ('minimize', 0.015), ('levin', 0.015), ('relevant', 0.015), ('serves', 0.015), ('sections', 0.015), ('instances', 0.015), ('linguistically', 0.015), ('summarizes', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="229-tfidf-1" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>2 0.45529482 <a title="229-tfidf-2" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>Author: Or Biran ; Kathleen McKeown</p><p>Abstract: We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation.</p><p>3 0.35214826 <a title="229-tfidf-3" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>4 0.30954111 <a title="229-tfidf-4" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>5 0.18791287 <a title="229-tfidf-5" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>Author: Egoitz Laparra ; German Rigau</p><p>Abstract: This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling. The system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data.</p><p>6 0.15724391 <a title="229-tfidf-6" href="./acl-2013-What_causes_a_causal_relation%3F_Detecting_Causal_Triggers_in_Biomedical_Scientific_Discourse.html">386 acl-2013-What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse</a></p>
<p>7 0.11853765 <a title="229-tfidf-7" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>8 0.097024657 <a title="229-tfidf-8" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>9 0.093567438 <a title="229-tfidf-9" href="./acl-2013-Argument_Inference_from_Relevant_Event_Mentions_in_Chinese_Argument_Extraction.html">56 acl-2013-Argument Inference from Relevant Event Mentions in Chinese Argument Extraction</a></p>
<p>10 0.079164445 <a title="229-tfidf-10" href="./acl-2013-Generating_Synthetic_Comparable_Questions_for_News_Articles.html">169 acl-2013-Generating Synthetic Comparable Questions for News Articles</a></p>
<p>11 0.075424604 <a title="229-tfidf-11" href="./acl-2013-Temporal_Signals_Help_Label_Temporal_Relations.html">339 acl-2013-Temporal Signals Help Label Temporal Relations</a></p>
<p>12 0.071696945 <a title="229-tfidf-12" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>13 0.069120921 <a title="229-tfidf-13" href="./acl-2013-Explicit_and_Implicit_Syntactic_Features_for_Text_Classification.html">144 acl-2013-Explicit and Implicit Syntactic Features for Text Classification</a></p>
<p>14 0.063814871 <a title="229-tfidf-14" href="./acl-2013-Joint_Inference_for_Fine-grained_Opinion_Extraction.html">207 acl-2013-Joint Inference for Fine-grained Opinion Extraction</a></p>
<p>15 0.062046096 <a title="229-tfidf-15" href="./acl-2013-Sentiment_Relevance.html">318 acl-2013-Sentiment Relevance</a></p>
<p>16 0.060103495 <a title="229-tfidf-16" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>17 0.055551872 <a title="229-tfidf-17" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>18 0.054033563 <a title="229-tfidf-18" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>19 0.046991497 <a title="229-tfidf-19" href="./acl-2013-Towards_Accurate_Distant_Supervision_for_Relational_Facts_Extraction.html">352 acl-2013-Towards Accurate Distant Supervision for Relational Facts Extraction</a></p>
<p>20 0.044765614 <a title="229-tfidf-20" href="./acl-2013-Automatic_Term_Ambiguity_Detection.html">62 acl-2013-Automatic Term Ambiguity Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/acl2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, 0.089), (2, -0.075), (3, -0.009), (4, -0.08), (5, 0.13), (6, -0.008), (7, 0.04), (8, -0.002), (9, 0.207), (10, 0.211), (11, 0.025), (12, -0.148), (13, 0.131), (14, -0.016), (15, -0.149), (16, 0.128), (17, -0.216), (18, -0.23), (19, -0.315), (20, 0.094), (21, 0.183), (22, 0.072), (23, -0.141), (24, -0.017), (25, 0.042), (26, 0.131), (27, 0.071), (28, 0.047), (29, 0.016), (30, 0.083), (31, 0.02), (32, 0.052), (33, 0.056), (34, 0.033), (35, -0.023), (36, -0.008), (37, -0.061), (38, 0.004), (39, 0.105), (40, -0.036), (41, -0.102), (42, 0.002), (43, 0.018), (44, 0.038), (45, -0.044), (46, 0.026), (47, -0.025), (48, 0.07), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97547585 <a title="229-lsi-1" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>2 0.87140781 <a title="229-lsi-2" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<p>Author: Or Biran ; Kathleen McKeown</p><p>Abstract: We present a reformulation of the word pair features typically used for the task of disambiguating implicit relations in the Penn Discourse Treebank. Our word pair features achieve significantly higher performance than the previous formulation when evaluated without additional features. In addition, we present results for a full system using additional features which achieves close to state of the art performance without resorting to gold syntactic parses or to context outside the relation.</p><p>3 0.82172322 <a title="229-lsi-3" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>Author: Shafiq Joty ; Giuseppe Carenini ; Raymond Ng ; Yashar Mehdad</p><p>Abstract: We propose a novel approach for developing a two-stage document-level discourse parser. Our parser builds a discourse tree by applying an optimal parsing algorithm to probabilities inferred from two Conditional Random Fields: one for intrasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin.</p><p>4 0.69016737 <a title="229-lsi-4" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>Author: Angeliki Lazaridou ; Ivan Titov ; Caroline Sporleder</p><p>Abstract: We propose a joint model for unsupervised induction of sentiment, aspect and discourse information and show that by incorporating a notion of latent discourse relations in the model, we improve the prediction accuracy for aspect and sentiment polarity on the sub-sentential level. We deviate from the traditional view of discourse, as we induce types of discourse relations and associated discourse cues relevant to the considered opinion analysis task; consequently, the induced discourse relations play the role of opinion and aspect shifters. The quantitative analysis that we conducted indicated that the integration of a discourse model increased the prediction accuracy results with respect to the discourse-agnostic approach and the qualitative analysis suggests that the induced representations encode a meaningful discourse structure.</p><p>5 0.53023756 <a title="229-lsi-5" href="./acl-2013-What_causes_a_causal_relation%3F_Detecting_Causal_Triggers_in_Biomedical_Scientific_Discourse.html">386 acl-2013-What causes a causal relation? Detecting Causal Triggers in Biomedical Scientific Discourse</a></p>
<p>Author: Claudiu Mihaila ; Sophia Ananiadou</p><p>Abstract: Current domain-specific information extraction systems represent an important resource for biomedical researchers, who need to process vaster amounts of knowledge in short times. Automatic discourse causality recognition can further improve their workload by suggesting possible causal connections and aiding in the curation of pathway models. We here describe an approach to the automatic identification of discourse causality triggers in the biomedical domain using machine learning. We create several baselines and experiment with various parameter settings for three algorithms, i.e., Conditional Random Fields (CRF), Support Vector Machines (SVM) and Random Forests (RF). Also, we evaluate the impact of lexical, syntactic and semantic features on each of the algorithms and look at er- rors. The best performance of 79.35% F-score is achieved by CRFs when using all three feature types.</p><p>6 0.47215074 <a title="229-lsi-6" href="./acl-2013-Plurality%2C_Negation%2C_and_Quantification%3ATowards_Comprehensive_Quantifier_Scope_Disambiguation.html">280 acl-2013-Plurality, Negation, and Quantification:Towards Comprehensive Quantifier Scope Disambiguation</a></p>
<p>7 0.46701524 <a title="229-lsi-7" href="./acl-2013-A_Novel_Translation_Framework_Based_on_Rhetorical_Structure_Theory.html">16 acl-2013-A Novel Translation Framework Based on Rhetorical Structure Theory</a></p>
<p>8 0.46571913 <a title="229-lsi-8" href="./acl-2013-ImpAr%3A_A_Deterministic_Algorithm_for_Implicit_Semantic_Role_Labelling.html">189 acl-2013-ImpAr: A Deterministic Algorithm for Implicit Semantic Role Labelling</a></p>
<p>9 0.38078323 <a title="229-lsi-9" href="./acl-2013-Random_Walk_Factoid_Annotation_for_Collective_Discourse.html">293 acl-2013-Random Walk Factoid Annotation for Collective Discourse</a></p>
<p>10 0.37980852 <a title="229-lsi-10" href="./acl-2013-Recognizing_Rare_Social_Phenomena_in_Conversation%3A_Empowerment_Detection_in_Support_Group_Chatrooms.html">298 acl-2013-Recognizing Rare Social Phenomena in Conversation: Empowerment Detection in Support Group Chatrooms</a></p>
<p>11 0.36413601 <a title="229-lsi-11" href="./acl-2013-Mining_Equivalent_Relations_from_Linked_Data.html">242 acl-2013-Mining Equivalent Relations from Linked Data</a></p>
<p>12 0.347848 <a title="229-lsi-12" href="./acl-2013-Temporal_Signals_Help_Label_Temporal_Relations.html">339 acl-2013-Temporal Signals Help Label Temporal Relations</a></p>
<p>13 0.32649967 <a title="229-lsi-13" href="./acl-2013-Why-Question_Answering_using_Intra-_and_Inter-Sentential_Causal_Relations.html">387 acl-2013-Why-Question Answering using Intra- and Inter-Sentential Causal Relations</a></p>
<p>14 0.32581657 <a title="229-lsi-14" href="./acl-2013-Automatic_Interpretation_of_the_English_Possessive.html">61 acl-2013-Automatic Interpretation of the English Possessive</a></p>
<p>15 0.30152956 <a title="229-lsi-15" href="./acl-2013-Universal_Conceptual_Cognitive_Annotation_%28UCCA%29.html">367 acl-2013-Universal Conceptual Cognitive Annotation (UCCA)</a></p>
<p>16 0.29094598 <a title="229-lsi-16" href="./acl-2013-Filling_Knowledge_Base_Gaps_for_Distant_Supervision_of_Relation_Extraction.html">159 acl-2013-Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction</a></p>
<p>17 0.28152278 <a title="229-lsi-17" href="./acl-2013-A_Unified_Morpho-Syntactic_Scheme_of_Stanford_Dependencies.html">28 acl-2013-A Unified Morpho-Syntactic Scheme of Stanford Dependencies</a></p>
<p>18 0.27629611 <a title="229-lsi-18" href="./acl-2013-Large-scale_Semantic_Parsing_via_Schema_Matching_and_Lexicon_Extension.html">215 acl-2013-Large-scale Semantic Parsing via Schema Matching and Lexicon Extension</a></p>
<p>19 0.27526104 <a title="229-lsi-19" href="./acl-2013-Graph-based_Local_Coherence_Modeling.html">172 acl-2013-Graph-based Local Coherence Modeling</a></p>
<p>20 0.27334443 <a title="229-lsi-20" href="./acl-2013-Understanding_Tables_in_Context_Using_Standard_NLP_Toolkits.html">365 acl-2013-Understanding Tables in Context Using Standard NLP Toolkits</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/acl2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.031), (6, 0.021), (11, 0.067), (24, 0.418), (26, 0.036), (28, 0.012), (35, 0.051), (42, 0.026), (48, 0.038), (70, 0.031), (88, 0.083), (90, 0.024), (95, 0.046), (99, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9735418 <a title="229-lda-1" href="./acl-2013-A_Visual_Analytics_System_for_Cluster_Exploration.html">29 acl-2013-A Visual Analytics System for Cluster Exploration</a></p>
<p>Author: Andreas Lamprecht ; Annette Hautli ; Christian Rohrdantz ; Tina Bogel</p><p>Abstract: This paper offers a new way of representing the results of automatic clustering algorithms by employing a Visual Analytics system which maps members of a cluster and their distance to each other onto a twodimensional space. A case study on Urdu complex predicates shows that the system allows for an appropriate investigation of linguistically motivated data. 1 Motivation In recent years, Visual Analytics systems have increasingly been used for the investigation of linguistic phenomena in a number of different areas, starting from literary analysis (Keim and Oelke, 2007) to the cross-linguistic comparison of language features (Mayer et al., 2010a; Mayer et al., 2010b; Rohrdantz et al., 2012a) and lexical semantic change (Rohrdantz et al., 2011; Heylen et al., 2012; Rohrdantz et al., 2012b). Visualization has also found its way into the field of computational linguistics by providing insights into methods such as machine translation (Collins et al., 2007; Albrecht et al., 2009) or discourse parsing (Zhao et al., 2012). One issue in computational linguistics is the interpretability of results coming from machine learning algorithms and the lack of insight they offer on the underlying data. This drawback often prevents theoretical linguists, who work with computational models and need to see patterns on large data sets, from drawing detailed conclusions. The present paper shows that a Visual Analytics system facilitates “analytical reasoning [...] by an interactive visual interface” (Thomas and Cook, 2006) and helps resolving this issue by offering a customizable, in-depth view on the statistically generated result and simultaneously an at-a-glance overview of the overall data set. In particular, we focus on the visual representa- tion of automatically generated clusters, in itself not a novel idea as it has been applied in other fields like the financial sector, biology or geography (Schreck et al., 2009). But as far as the literature is concerned, interactive systems are still less common, particularly in computational linguistics, and they have not been designed for the specific needs of theoretical linguists. This paper offers a method of visually encoding clusters and their internal coherence with an interactive user interface, which allows users to adjust underlying parameters and their views on the data depending on the particular research question. By this, we partly open up the “black box” of machine learning. The linguistic phenomenon under investigation, for which the system has originally been designed, is the varied behavior of nouns in N+V CP complex predicates in Urdu (e.g., memory+do = ‘to remember’) (Mohanan, 1994; Ahmed and Butt, 2011), where, depending on the lexical semantics of the noun, a set of different light verbs is chosen to form a complex predicate. The aim is an automatic detection of the different groups of nouns, based on their light verb distribution. Butt et al. (2012) present a static visualization for the phenomenon, whereas the present paper proposes an interactive system which alleviates some of the previous issues with respect to noise detection, filtering, data interaction and cluster coherence. For this, we proceed as follows: section 2 explains the proposed Visual Analytics system, followed by the linguistic case study in section 3. Section 4 concludes the paper. 2 The system The system requires a plain text file as input, where each line corresponds to one data object.In our case, each line corresponds to one Urdu noun (data object) and contains its unique ID (the name of the noun) and its bigram frequencies with the 109 Proce dingSsof oifa, th Beu 5l1gsarti Aan,An u aglu Mste 4e-ti9n2g 0 o1f3 t.he ?c A2s0s1o3ci Aatsiosonc fioartio Cno fmorpu Ctoamtiopnuatalt Lioin gauli Lsitnicgsu,i psatgices 109–1 4, four light verbs under investigation, namely kar ‘do’, ho ‘be’, hu ‘become’ and rakH ‘put’ ; an exemplary input file is shown in Figure 1. From a data analysis perspective, we have four- dimensional data objects, where each dimension corresponds to a bigram frequency previously extracted from a corpus. Note that more than four dimensions can be loaded and analyzed, but for the sake of simplicity we focus on the fourdimensional Urdu example for the remainder of this paper. Moreover, it is possible to load files containing absolute bigram frequencies and relative frequencies. When loading absolute frequencies, the program will automatically calculate the relative frequencies as they are the input for the clustering. The absolute frequencies, however, are still available and can be used for further processing (e.g. filtering). Figure 1: preview of appropriate file structures 2.1 Initial opening and processing of a file It is necessary to define a metric distance function between data objects for both clustering and visualization. Thus, each data object is represented through a high dimensional (in our example fourdimensional) numerical vector and we use the Euclidean distance to calculate the distances between pairs of data objects. The smaller the distance between two data objects, the more similar they are. For visualization, the high dimensional data is projected onto the two-dimensional space of a computer screen using a principal component analysis (PCA) algorithm1 . In the 2D projection, the distances between data objects in the highdimensional space, i.e. the dissimilarities of the bigram distributions, are preserved as accurately as possible. However, when projecting a highdimensional data space onto a lower dimension, some distinctions necessarily level out: two data objects may be far apart in the high-dimensional space, but end up closely together in the 2D projection. It is important to bear in mind that the 2D visualization is often quite insightful, but interpre1http://workshop.mkobos.com/201 1/java-pca- transformation-library/ tations have to be verified by interactively investigating the data. The initial clusters are calculated (in the highdimensional data space) using a default k-Means algorithm2 with k being a user-defined parameter. There is also the option of selecting another clustering algorithm, called the Greedy Variance Minimization3 (GVM), and an extension to include further algorithms is under development. 2.2 Configuration & Interaction 2.2.1 The main window The main window in Figure 2 consists of three areas, namely the configuration area (a), the visualization area (b) and the description area (c). The visualization area is mainly built with the piccolo2d library4 and initially shows data objects as colored circles with a variable diameter, where color indicates cluster membership (four clusters in this example). Hovering over a dot displays information on the particular noun, the cluster membership and the light verb distribution in the de- scription area to the right. By using the mouse wheel, the user can zoom in and out of the visualization. A very important feature for the task at hand is the possibility to select multiple data objects for further processing or for filtering, with a list of selected data objects shown in the description area. By right-clicking on these data objects, the user can assign a unique class (and class color) to them. Different clustering methods can be employed using the options item in the menu bar. Another feature of the system is that the user can fade in the cluster centroids (illustrated by a larger dot in the respective cluster color in Figure 2), where the overall feature distribution of the cluster can be examined in a tooltip hovering over the corresponding centroid. 2.2.2 Visually representing data objects To gain further insight into the data distribution based on the 2D projection, the user can choose between several ways to visualize the individual data objects, all of which are shown in Figure 3. The standard visualization type is shown on the left and consists of a circle which encodes cluster membership via color. 2http://java-ml.sourceforge.net/api/0.1.7/ (From the JML library) 3http://www.tomgibara.com/clustering/fast-spatial/ 4http://www.piccolo2d.org/ 110 Figure 2: Overview of the main window of the system, including the configuration area (a), the visualization area (b) and the description area (c). Large circles are cluster centroids. Figure 3: Different visualizations of data points Alternatively, normal glyphs and star glyphs can be displayed. The middle part of Figure 3 shows the data displayed with normal glyphs. In linestarinorthpsiflvtrheinorqsbgnutheviasnemdocwfya,proepfthlpdienaoecsr.nihetloa Titnghve det clockwise around the center according to their occurrence in the input file. This view has the advantage that overall feature dominance in a cluster can be seen at-a-glance. The visualization type on the right in Figure 3 agislnycpaehlxset. dnstHhioe nrset ,oarthngeolyrmlpinhae,l endings are connected, forming a “star”. As in the representation with the glyphs, this makes similar data objects easily recognizable and comparable with each other. 2.2.3 Filtering options Our systems offers options for filtering data ac- cording to different criteria. Filter by means of bigram occurrence By activating the bigram occurrence filtering, it is possible to only show those nouns, which occur in bigrams with a certain selected subset of all features (light verbs) only. This is especially useful when examining possible commonalities. Filter selected words Another opportunity of showing only items of interest is to select and display them separately. The PCA is recalculated for these data objects and the visualization is stretched to the whole area. 111 Filter selected cluster Additionally, the user can visualize a specific cluster of interest. Again, the PCA is recalculated and the visualization stretched to the whole area. The cluster can then be manually fine-tuned and cleaned, for instance by removing wrongly assigned items. 2.2.4 Options to handle overplotting Due to the nature of the data, much overplotting occurs. For example, there are many words, which only occur with one light verb. The PCA assigns the same position to these words and, as a consequence, only the top bigram can be viewed in the visualization. In order to improve visual access to overplotted data objects, several methods that allow for a more differentiated view of the data have been included and are described in the following paragraphs. Change transparency of data objects By modifying the transparency with the given slider, areas with a dense data population can be readily identified, as shown in the following example: Repositioning of data objects To reduce the overplotting in densely populated areas, data objects can be repositioned randomly having a fixed deviation from their initial position. The degree of deviation can be interactively determined by the user employing the corresponding slider: The user has the option to reposition either all data objects or only those that are selected in advance. Frequency filtering If the initial data contains absolute bigram frequencies, the user can filter the visualized words by frequency. For example, many nouns occur only once and therefore have an observed probability of 100% for co-occurring with one of the light verbs. In most cases it is useful to filter such data out. Scaling data objects If the user zooms beyond the maximum zoom factor, the data objects are scaled down. This is especially useful, if data objects are only partly covered by many other objects. In this case, they become fully visible, as shown in the following example: 2.3 Alternative views on the data In order to enable a holistic analysis it is often valuable to provide the user with different views on the data. Consequently, we have integrated the option to explore the data with further standard visualization methods. 2.3.1 Correlation matrix The correlation matrix in Figure 4 shows the correlations between features, which are visualized by circles using the following encoding: The size of a circle represents the correlation strength and the color indicates whether the corresponding features are negatively (white) or positively (black) correlated. Figure 4: example of a correlation matrix 2.3.2 Parallel coordinates The parallel coordinates diagram shows the distribution of the bigram frequencies over the different dimensions (Figure 5). Every noun is represented with a line, and shows, when hovered over, a tooltip with the most important information. To filter the visualized words, the user has the option of displaying previously selected data objects, or s/he can restrict the value range for a feature and show only the items which lie within this range. 2.3.3 Scatter plot matrix To further examine the relation between pairs of features, a scatter plot matrix can be used (Figure 6). The individual scatter plots give further insight into the correlation details of pairs of features. 112 Figure 5: Parallel coordinates diagram Figure 6: Example showing a scatter plot matrix. 3 Case study In principle, the Visual Analytics system presented above can be used for any kind of cluster visualization, but the built-in options and add-ons are particularly designed for the type of work that linguists tend to be interested in: on the one hand, the user wants to get a quick overview of the overall patterns in the phenomenon, but on the same time, the system needs to allow for an in-depth data inspection. Both is given in the system: The overall cluster result shown in Figure 2 depicts the coherence of clusters and therefore the overall pattern of the data set. The different glyph visualizations in Figure 3 illustrate the properties of each cluster. Single data points can be inspected in the description area. The randomization of overplotted data points helps to see concentrated cluster patterns where light verbs behave very similarly in different noun+verb complex predicates. The biggest advantage of the system lies in the ability for interaction: Figure 7 shows an example of the visualization used in Butt et al. (2012), the input being the same text file as shown in Figure 1. In this system, the relative frequencies of each noun with each light verb is correlated with color saturation the more saturated the color to the right of the noun, the higher the relative frequency of the light verb occurring with it. The number of the cluster (here, 3) and the respective nouns (e.g. kAm ‘work’) is shown to the left. The user does — not get information on the coherence of the cluster, nor does the visualization show prototypical cluster patterns. Figure 7: Cluster visualization in Butt et al. (2012) Moreover, the system in Figure 7 only has a limited set of interaction choices, with the consequence that the user is not able to adjust the underlying data set, e.g. by filtering out noise. However, Butt et al. (2012) report that the Urdu data is indeed very noisy and requires a manual cleaning of the data set before the actual clustering. In the system presented here, the user simply marks conspicuous regions in the visualization panel and removes the respective data points from the original data set. Other filtering mechanisms, e.g. the removal of low frequency items which occur due to data sparsity issues, can be removed from the overall data set by adjusting the parameters. A linguistically-relevant improvement lies in the display of cluster centroids, in other words the typical noun + light verb distribution of a cluster. This is particularly helpful when the linguist wants to pick out prototypical examples for the cluster in order to stipulate generalizations over the other cluster members. 113 4 Conclusion In this paper, we present a novel visual analytics system that helps to automatically analyze bigrams extracted from corpora. The main purpose is to enable a more informed and steered cluster analysis than currently possible with standard methods. This includes rich options for interaction, e.g. display configuration or data manipulation. Initially, the approach was motivated by a concrete research problem, but has much wider applicability as any kind of high-dimensional numerical data objects can be loaded and analyzed. However, the system still requires some basic understanding about the algorithms applied for clustering and projection in order to prevent the user to draw wrong conclusions based on artifacts. Bearing this potential pitfall in mind when performing the analysis, the system enables a much more insightful and informed analysis than standard noninteractive methods. In the future, we aim to conduct user experiments in order to learn more about how the functionality and usability could be further enhanced. Acknowledgments This work was partially funded by the German Research Foundation (DFG) under grant BU 1806/7-1 “Visual Analysis of Language Change and Use Patterns” and the German Fed- eral Ministry of Education and Research (BMBF) under grant 01461246 “VisArgue” under research grant. References Tafseer Ahmed and Miriam Butt. 2011. Discovering Semantic Classes for Urdu N-V Complex Predicates. In Proceedings of the international Conference on Computational Semantics (IWCS 2011), pages 305–309. Joshua Albrecht, Rebecca Hwa, and G. Elisabeta Marai. 2009. The Chinese Room: Visualization and Interaction to Understand and Correct Ambiguous Machine Translation. Comput. Graph. Forum, 28(3): 1047–1054. Miriam Butt, Tina B ¨ogel, Annette Hautli, Sebastian Sulger, and Tafseer Ahmed. 2012. Identifying Urdu Complex Predication via Bigram Extraction. In In Proceedings of COLING 2012, Technical Papers, pages 409 424, Mumbai, India. Christopher Collins, M. Sheelagh T. Carpendale, and Gerald Penn. 2007. Visualization of Uncertainty in Lattices to Support Decision-Making. In EuroVis 2007, pages 5 1–58. Eurographics Association. Kris Heylen, Dirk Speelman, and Dirk Geeraerts. 2012. Looking at word meaning. An interactive visualization of Semantic Vector Spaces for Dutch – synsets. In Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 16–24. Daniel A. Keim and Daniela Oelke. 2007. Literature Fingerprinting: A New Method for Visual Literary Analysis. In IEEE VAST 2007, pages 115–122. IEEE. Thomas Mayer, Christian Rohrdantz, Miriam Butt, Frans Plank, and Daniel A. Keim. 2010a. Visualizing Vowel Harmony. Linguistic Issues in Language Technology, 4(Issue 2): 1–33, December. Thomas Mayer, Christian Rohrdantz, Frans Plank, Peter Bak, Miriam Butt, and Daniel A. Keim. 2010b. Consonant Co-Occurrence in Stems across Languages: Automatic Analysis and Visualization of a Phonotactic Constraint. In Proceedings of the 2010 Workshop on NLP andLinguistics: Finding the Common Ground, pages 70–78, Uppsala, Sweden, July. Association for Computational Linguistics. Tara Mohanan. 1994. Argument Structure in Hindi. Stanford: CSLI Publications. Christian Rohrdantz, Annette Hautli, Thomas Mayer, Miriam Butt, Frans Plank, and Daniel A. Keim. 2011. Towards Tracking Semantic Change by Visual Analytics. In ACL 2011 (Short Papers), pages 305–3 10, Portland, Oregon, USA, June. Association for Computational Linguistics. Christian Rohrdantz, Michael Hund, Thomas Mayer, Bernhard W ¨alchli, and Daniel A. Keim. 2012a. The World’s Languages Explorer: Visual Analysis of Language Features in Genealogical and Areal Contexts. Computer Graphics Forum, 3 1(3):935–944. Christian Rohrdantz, Andreas Niekler, Annette Hautli, Miriam Butt, and Daniel A. Keim. 2012b. Lexical Semantics and Distribution of Suffixes - A Visual Analysis. In Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 7–15, April. Tobias Schreck, J ¨urgen Bernard, Tatiana von Landesberger, and J o¨rn Kohlhammer. 2009. Visual cluster analysis of trajectory data with interactive kohonen maps. Information Visualization, 8(1): 14–29. James J. Thomas and Kristin A. Cook. 2006. A Visual Analytics Agenda. IEEE Computer Graphics and Applications, 26(1): 10–13. Jian Zhao, Fanny Chevalier, Christopher Collins, and Ravin Balakrishnan. 2012. Facilitating Discourse Analysis with Interactive Visualization. IEEE Trans. Vis. Comput. Graph., 18(12):2639–2648. 114</p><p>2 0.9508003 <a title="229-lda-2" href="./acl-2013-ParaQuery%3A_Making_Sense_of_Paraphrase_Collections.html">271 acl-2013-ParaQuery: Making Sense of Paraphrase Collections</a></p>
<p>Author: Lili Kotlerman ; Nitin Madnani ; Aoife Cahill</p><p>Abstract: Pivoting on bilingual parallel corpora is a popular approach for paraphrase acquisition. Although such pivoted paraphrase collections have been successfully used to improve the performance of several different NLP applications, it is still difficult to get an intrinsic estimate of the quality and coverage of the paraphrases contained in these collections. We present ParaQuery, a tool that helps a user interactively explore and characterize a given pivoted paraphrase collection, analyze its utility for a particular domain, and compare it to other popular lexical similarity resources all within a single interface.</p><p>3 0.95056552 <a title="229-lda-3" href="./acl-2013-Building_Comparable_Corpora_Based_on_Bilingual_LDA_Model.html">74 acl-2013-Building Comparable Corpora Based on Bilingual LDA Model</a></p>
<p>Author: Zede Zhu ; Miao Li ; Lei Chen ; Zhenxin Yang</p><p>Abstract: Comparable corpora are important basic resources in cross-language information processing. However, the existing methods of building comparable corpora, which use intertranslate words and relative features, cannot evaluate the topical relation between document pairs. This paper adopts the bilingual LDA model to predict the topical structures of the documents and proposes three algorithms of document similarity in different languages. Experiments show that the novel method can obtain similar documents with consistent top- ics own better adaptability and stability performance.</p><p>4 0.94011223 <a title="229-lda-4" href="./acl-2013-Identification_of_Speakers_in_Novels.html">184 acl-2013-Identification of Speakers in Novels</a></p>
<p>Author: Hua He ; Denilson Barbosa ; Grzegorz Kondrak</p><p>Abstract: Speaker identification is the task of at- tributing utterances to characters in a literary narrative. It is challenging to auto- mate because the speakers of the majority ofutterances are not explicitly identified in novels. In this paper, we present a supervised machine learning approach for the task that incorporates several novel features. The experimental results show that our method is more accurate and general than previous approaches to the problem.</p><p>same-paper 5 0.93878436 <a title="229-lda-5" href="./acl-2013-Leveraging_Synthetic_Discourse_Data_via_Multi-task_Learning_for_Implicit_Discourse_Relation_Recognition.html">229 acl-2013-Leveraging Synthetic Discourse Data via Multi-task Learning for Implicit Discourse Relation Recognition</a></p>
<p>Author: Man Lan ; Yu Xu ; Zhengyu Niu</p><p>Abstract: To overcome the shortage of labeled data for implicit discourse relation recognition, previous works attempted to automatically generate training data by removing explicit discourse connectives from sentences and then built models on these synthetic implicit examples. However, a previous study (Sporleder and Lascarides, 2008) showed that models trained on these synthetic data do not generalize very well to natural (i.e. genuine) implicit discourse data. In this work we revisit this issue and present a multi-task learning based system which can effectively use synthetic data for implicit discourse relation recognition. Results on PDTB data show that under the multi-task learning framework our models with the use of the prediction of explicit discourse connectives as auxiliary learning tasks, can achieve an averaged F1 improvement of 5.86% over baseline models.</p><p>6 0.93501973 <a title="229-lda-6" href="./acl-2013-Does_Korean_defeat_phonotactic_word_segmentation%3F.html">128 acl-2013-Does Korean defeat phonotactic word segmentation?</a></p>
<p>7 0.87645775 <a title="229-lda-7" href="./acl-2013-Bridging_Languages_through_Etymology%3A_The_case_of_cross_language_text_categorization.html">72 acl-2013-Bridging Languages through Etymology: The case of cross language text categorization</a></p>
<p>8 0.82369167 <a title="229-lda-8" href="./acl-2013-Mining_Opinion_Words_and_Opinion_Targets_in_a_Two-Stage_Framework.html">244 acl-2013-Mining Opinion Words and Opinion Targets in a Two-Stage Framework</a></p>
<p>9 0.72092539 <a title="229-lda-9" href="./acl-2013-PhonMatrix%3A_Visualizing_co-occurrence_constraints_of_sounds.html">279 acl-2013-PhonMatrix: Visualizing co-occurrence constraints of sounds</a></p>
<p>10 0.65745503 <a title="229-lda-10" href="./acl-2013-Using_Supervised_Bigram-based_ILP_for_Extractive_Summarization.html">377 acl-2013-Using Supervised Bigram-based ILP for Extractive Summarization</a></p>
<p>11 0.65445048 <a title="229-lda-11" href="./acl-2013-Character-to-Character_Sentiment_Analysis_in_Shakespeare%27s_Plays.html">79 acl-2013-Character-to-Character Sentiment Analysis in Shakespeare's Plays</a></p>
<p>12 0.63193351 <a title="229-lda-12" href="./acl-2013-Evaluating_Text_Segmentation_using_Boundary_Edit_Distance.html">140 acl-2013-Evaluating Text Segmentation using Boundary Edit Distance</a></p>
<p>13 0.61876231 <a title="229-lda-13" href="./acl-2013-A_Bayesian_Model_for_Joint_Unsupervised_Induction_of_Sentiment%2C_Aspect_and_Discourse_Representations.html">2 acl-2013-A Bayesian Model for Joint Unsupervised Induction of Sentiment, Aspect and Discourse Representations</a></p>
<p>14 0.61478901 <a title="229-lda-14" href="./acl-2013-Lightly_Supervised_Learning_of_Procedural_Dialog_Systems.html">230 acl-2013-Lightly Supervised Learning of Procedural Dialog Systems</a></p>
<p>15 0.6068027 <a title="229-lda-15" href="./acl-2013-ICARUS_-_An_Extensible_Graphical_Search_Tool_for_Dependency_Treebanks.html">183 acl-2013-ICARUS - An Extensible Graphical Search Tool for Dependency Treebanks</a></p>
<p>16 0.60177147 <a title="229-lda-16" href="./acl-2013-Combining_Intra-_and_Multi-sentential_Rhetorical_Parsing_for_Document-level_Discourse_Analysis.html">85 acl-2013-Combining Intra- and Multi-sentential Rhetorical Parsing for Document-level Discourse Analysis</a></p>
<p>17 0.60094029 <a title="229-lda-17" href="./acl-2013-Improving_Text_Simplification_Language_Modeling_Using_Unsimplified_Text_Data.html">194 acl-2013-Improving Text Simplification Language Modeling Using Unsimplified Text Data</a></p>
<p>18 0.60024375 <a title="229-lda-18" href="./acl-2013-Crowd_Prefers_the_Middle_Path%3A_A_New_IAA_Metric_for_Crowdsourcing_Reveals_Turker_Biases_in_Query_Segmentation.html">99 acl-2013-Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing Reveals Turker Biases in Query Segmentation</a></p>
<p>19 0.59416252 <a title="229-lda-19" href="./acl-2013-Broadcast_News_Story_Segmentation_Using_Manifold_Learning_on_Latent_Topic_Distributions.html">73 acl-2013-Broadcast News Story Segmentation Using Manifold Learning on Latent Topic Distributions</a></p>
<p>20 0.58887553 <a title="229-lda-20" href="./acl-2013-Aggregated_Word_Pair_Features_for_Implicit_Discourse_Relation_Disambiguation.html">41 acl-2013-Aggregated Word Pair Features for Implicit Discourse Relation Disambiguation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
